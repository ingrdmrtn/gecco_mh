def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM mixture with capacity- and age-modulated WM arbitration and one-shot WM learning.
    
    Idea:
    - Decisions are a mixture of incremental RL and a fast Working Memory (WM) system.
    - WM contribution is stronger when set size is small relative to an internal capacity K,
      and is reduced for older adults.
    - WM stores the most recent outcome for the chosen action in a state (one-shot learning),
      with a fast learning rate and mild decay toward a uniform prior.
    
    Parameters (model_parameters):
    - lr: float in [0,1], RL learning rate.
    - wm_weight: float in [0,1], baseline arbitration weight for WM.
    - softmax_beta: float >= 0, inverse temperature for RL softmax (internally scaled).
    - wm_alpha: float in [0,1], WM update strength (one-shot learning toward observed outcome).
    - K: float > 0, effective WM capacity (in number of state-action mappings).
    - age_old_factor: float in [0,1], multiplicative reduction of WM weight for older adults (>=45).
    
    Inputs:
    - states: array of state indices (0..set_size-1 for each block).
    - actions: array of chosen actions (0,1,2).
    - rewards: array of rewards (0 or 1).
    - blocks: array of block indices.
    - set_sizes: array of set size for each trial (3 or 6).
    - age: array-like, first element is participant age (years).
    - model_parameters: list or array of the 6 parameters above.
    
    Returns:
    - Negative log-likelihood of the observed choices under the model.
    """
    lr, wm_weight, softmax_beta, wm_alpha, K, age_old_factor = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    age_val = age[0]
    is_old = 1.0 if age_val >= 45 else 0.0

    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])
            nS_now = float(block_set_sizes[t])

            # RL policy for observed action a
            Q_s = q[s, :]
            # p(a) = exp(beta*Q[a]) / sum exp(beta*Q)
            # stable form using difference:
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM policy: softmax over WM values
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # Arbitration: WM weight depends on set size vs capacity and age
            # Set-size factor (0..1): saturates when set size <= K
            set_factor = min(1.0, max(0.0, K / max(nS_now, 1.0)))
            # Age factor: reduce WM weight if older
            age_factor = (1.0 - is_old) + is_old * max(0.0, min(1.0, age_old_factor))
            wm_w_eff = max(0.0, min(1.0, wm_weight * set_factor * age_factor))

            p_total = wm_w_eff * p_wm + (1.0 - wm_w_eff) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: partial overwrite toward observed outcome, and mild decay toward uniform
            # First decay current state's WM row slightly toward uniform prior
            w[s, :] = (1.0 - wm_alpha) * w[s, :] + wm_alpha * w_0[s, :]
            # Then set the chosen action closer to the observed outcome (one-shot style)
            w[s, a] = (1.0 - wm_alpha) * w[s, a] + wm_alpha * r

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM mixture with asymmetric RL learning rates and arbitration modulated by RL uncertainty,
    set size, and age. WM uses fast one-shot updates with decay.

    Idea:
    - RL has separate learning rates for positive vs. negative prediction errors (lr_pos, lr_neg).
    - WM learns one-shot and decays toward uniform; its influence is reduced when RL is confident
      (low entropy), when set size is large, and for older adults.
    - Arbitration weight: wm_weight_eff = wm_weight_base * set_size_factor * age_factor * (1 - H_rl),
      where H_rl is the entropy of the RL softmax policy in the current state.

    Parameters (model_parameters):
    - lr_pos: float in [0,1], RL learning rate for positive prediction errors.
    - lr_neg: float in [0,1], RL learning rate for negative prediction errors.
    - softmax_beta: float >= 0, inverse temperature for RL softmax (internally scaled).
    - wm_weight_base: float in [0,1], baseline WM arbitration weight.
    - wm_alpha: float in [0,1], WM update strength/decay toward outcomes (and uniform).
    - age_old_factor: float in [0,1], multiplicative reduction of WM weight for older adults (>=45).

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age: as described above.
    - model_parameters: list/array of the 6 parameters above.

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr_pos, lr_neg, softmax_beta, wm_weight_base, wm_alpha, age_old_factor = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    age_val = age[0]
    is_old = 1.0 if age_val >= 45 else 0.0

    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])
            nS_now = float(block_set_sizes[t])

            Q_s = q[s, :]
            # RL action probabilities
            logits_rl = softmax_beta * (Q_s - np.max(Q_s))
            pi_rl = np.exp(logits_rl)
            pi_rl /= np.sum(pi_rl)
            p_rl = max(pi_rl[a], eps)

            # RL policy entropy (0..log nA), normalize to 0..1 by dividing by log(nA)
            H_rl = -np.sum(pi_rl * np.log(np.maximum(pi_rl, eps)))
            H_rl_norm = H_rl / np.log(3.0)

            # WM policy
            W_s = w[s, :]
            logits_wm = softmax_beta_wm * (W_s - np.max(W_s))
            pi_wm = np.exp(logits_wm)
            pi_wm /= np.sum(pi_wm)
            p_wm = max(pi_wm[a], eps)

            # Arbitration: downweight WM when set size is large, when RL is confident, and if older
            set_factor = min(1.0, 3.0 / max(nS_now, 1.0))  # 1 for set size 3, ~0.5 for 6
            age_factor = (1.0 - is_old) + is_old * max(0.0, min(1.0, age_old_factor))
            wm_w_eff = wm_weight_base * set_factor * age_factor * (1.0 - H_rl_norm)
            wm_w_eff = max(0.0, min(1.0, wm_w_eff))

            p_total = wm_w_eff * p_wm + (1.0 - wm_w_eff) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update with asymmetric learning rates
            pe = r - Q_s[a]
            lr_use = lr_pos if pe >= 0.0 else lr_neg
            q[s, a] += lr_use * pe

            # WM update: decay toward uniform for current state, then move chosen action toward outcome
            w[s, :] = (1.0 - wm_alpha) * w[s, :] + wm_alpha * w_0[s, :]
            w[s, a] = (1.0 - wm_alpha) * w[s, a] + wm_alpha * r

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM mixture with spacing-sensitive WM decay and perseveration bias.
    
    Idea:
    - WM decays as a function of the number of intervening trials since a state was last seen,
      which harms WM more in larger set sizes; older adults also show stronger decay and reduced WM weight.
    - A perseveration bias adds value to repeating the last action taken in the same state.
    - Arbitration remains a mixture of WM and RL, with age- and set-size-modulated WM weight.

    Parameters (model_parameters):
    - lr: float in [0,1], RL learning rate.
    - softmax_beta: float >= 0, inverse temperature for RL softmax (internally scaled).
    - wm_alpha: float in [0,1], WM learning/update strength.
    - wm_weight: float in [0,1], baseline WM arbitration weight.
    - wm_decay: float in [0,1], per-trial WM retention factor (applied per gap since last visit).
    - persev: float >= 0, perseveration strength added to last chosen action in a state.

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age: as described above.
    - model_parameters: list/array of the 6 parameters above.

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, softmax_beta, wm_alpha, wm_weight, wm_decay, persev = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    age_val = age[0]
    is_old = 1.0 if age_val >= 45 else 0.0

    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track last seen trial index and last chosen action per state
        last_seen = -1 * np.ones(nS, dtype=int)
        last_action = -1 * np.ones(nS, dtype=int)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])
            nS_now = float(block_set_sizes[t])

            # Apply spacing-sensitive decay to WM for current state based on gap
            if last_seen[s] >= 0:
                gap = t - last_seen[s]
                # Older adults decay more strongly: effective decay factor reduced
                age_decay_mult = 0.9 if is_old > 0.5 else 1.0
                decay_factor = (wm_decay * age_decay_mult) ** max(gap, 1)
                w[s, :] = w_0[s, :] + decay_factor * (w[s, :] - w_0[s, :])

            # Add perseveration bias to both RL and WM action values for policy computation
            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()
            if last_action[s] >= 0:
                la = int(last_action[s])
                Q_s[la] += persev
                W_s[la] += persev

            # RL policy probability for observed action
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM policy probability for observed action
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # Arbitration: reduce WM influence with larger set sizes and for older adults
            set_factor = min(1.0, 3.0 / max(nS_now, 1.0))  # 1 for 3, ~0.5 for 6
            age_factor = 0.7 if is_old > 0.5 else 1.0       # older adults reduced WM weight
            wm_w_eff = max(0.0, min(1.0, wm_weight * set_factor * age_factor))

            p_total = wm_w_eff * p_wm + (1.0 - wm_w_eff) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM update: decay toward uniform and incorporate observed outcome one-shot
            w[s, :] = (1.0 - wm_alpha) * w[s, :] + wm_alpha * w_0[s, :]
            w[s, a] = (1.0 - wm_alpha) * w[s, a] + wm_alpha * r

            # Update traces
            last_seen[s] = t
            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p