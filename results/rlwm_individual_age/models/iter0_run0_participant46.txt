Below are three standalone cognitive models that follow the requested RL+WM template. Each function:

- Accepts: states, actions, rewards, blocks, set_sizes, age, model_parameters
- Returns: negative log-likelihood of observed choices
- Uses age group (older vs younger) and set size to modulate WM/RL contributions
- Fills in the WM policy and WM updating in the template structure
- Uses all parameters meaningfully
- Uses at most 6 parameters per model

Note: Assumes numpy as np (and any other needed standard packages) are already imported in the environment.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM mixture with capacity-limited working memory and age-related capacity penalty.
    - RL: standard delta rule with softmax policy.
    - WM: one-shot learning with decay toward uniform. If rewarded, store near one-hot for chosen action;
      if not rewarded, suppress chosen action slightly.
    - Mixture: WM contribution scales with an effective capacity term that depends on set size and age group.

    Parameters (tuple):
      lr:                RL learning rate (also used as WM learning strength)
      wm_weight:         Base WM mixture weight in [0,1]
      softmax_beta:      RL inverse temperature (will be scaled by 10 internally)
      K:                 WM capacity (in "items" units)
      decay:             WM decay toward uniform per trial (0=none, 1=full reset)
      age_penalty:       Capacity penalty applied if participant is older (>=45): K_eff = K - age_penalty

    Returns:
      Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, K, decay, age_penalty = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    age_val = age[0]
    older = 1.0 if age_val >= 45 else 0.0

    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Effective WM capacity after age penalty
        K_eff = max(0.0, K - age_penalty * older)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            Q_center = Q_s[a]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_center)))

            # WM policy: softmax over WM weights
            # First, apply decay of WM towards uniform for this state
            w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]

            W_s = w[s, :]
            W_center = W_s[a]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_center)))

            # Load- and age-dependent WM weight
            load_factor = 0.0 if nS <= 0 else min(1.0, K_eff / float(nS))
            eta = np.clip(wm_weight * load_factor, 0.0, 1.0)

            p_total = eta * p_wm + (1.0 - eta) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: reward-driven one-shot storage and mild punishment
            if r > 0.5:
                # Move WM for this state toward a one-hot on chosen action
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - lr) * w[s, :] + lr * target
            else:
                # Mild suppression of the chosen action if not rewarded
                w[s, a] = max(eps, w[s, a] * (1.0 - lr))
                # Renormalize probabilities to sum to 1
                w_sum = np.sum(w[s, :])
                if w_sum > 0:
                    w[s, :] /= w_sum
                else:
                    w[s, :] = w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM arbitration with uncertainty- and load-sensitive WM, and age-reduced WM precision.
    - RL: standard delta rule with softmax.
    - WM: associative weights with decay to uniform; precise (high beta) but precision drops with set size and age.
    - Arbitration: WM weight increases when RL policy is uncertain (high entropy) and decreases with larger set size.
      Age reduces both WM precision and WM arbitration weight.

    Parameters (tuple):
      lr:                RL learning rate (also used as WM learning strength)
      wm_weight:         Base WM mixture weight in [0,1]
      softmax_beta:      RL inverse temperature (scaled by 10 internally)
      wm_decay:          WM decay toward uniform per trial (0=none, 1=full reset)
      wm_beta_scale:     Scales WM inverse temperature (precision); >1 sharper, <1 noisier
      age_bias:          Age penalty factor: reduces WM precision and arbitration if older

    Returns:
      Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wm_decay, wm_beta_scale, age_bias = model_parameters
    softmax_beta *= 10.0
    base_wm_beta = 50.0
    age_val = age[0]
    older = 1.0 if age_val >= 45 else 0.0

    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL softmax policy (also compute full policy for entropy)
            Q_s = q[s, :]
            Q_center = Q_s[a]
            # Chosen-action prob
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_center)))
            # Full RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - np.max(Q_s))))
            pi_rl = np.exp(softmax_beta * (Q_s - np.max(Q_s))) / max(denom_rl, eps)
            # RL uncertainty (normalized entropy)
            H = -np.sum(pi_rl * np.log(np.clip(pi_rl, eps, 1.0)))
            H_norm = H / np.log(nA)

            # WM decay toward uniform, then policy
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            W_s = w[s, :]
            # WM precision decreases with set size and age
            wm_beta_eff = base_wm_beta * max(eps, wm_beta_scale) * (3.0 / float(nS)) * np.exp(-age_bias * older)
            W_center = W_s[a]
            p_wm = 1.0 / np.sum(np.exp(wm_beta_eff * (W_s - W_center)))

            # Arbitration: increase WM when RL uncertain; decrease with load; age reduces WM usage
            load_penalty = max(0.0, (nS - 3) / 3.0)  # 0 at 3-back, up to 1 at 6
            eta_raw = wm_weight + (H_norm - load_penalty) - age_bias * older
            eta = np.clip(eta_raw, 0.0, 1.0)

            p_total = eta * p_wm + (1.0 - eta) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: reward strengthens chosen association; no-reward weakens it slightly
            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - lr) * w[s, :] + lr * target
            else:
                w[s, a] = max(eps, w[s, a] * (1.0 - 0.5 * lr))
                # Renormalize
                w_sum = np.sum(w[s, :])
                if w_sum > 0:
                    w[s, :] /= w_sum
                else:
                    w[s, :] = w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with valence-asymmetric learning + capacity-free WM cache with load- and age-dependent recall.
    - RL: separate learning rates for positive vs. negative prediction errors; softmax policy.
    - WM: a simple cache that stores the last rewarded action per state as a one-hot vector; slight decay to uniform.
      WM recall probability scales with set size and is reduced by age.
    - Mixture: On each trial, mixture weight equals the WM recall probability; otherwise relies on RL.

    Parameters (tuple):
      lr_pos:            RL learning rate for positive prediction errors
      lr_neg:            RL learning rate for negative prediction errors
      wm_weight:         Base WM recall/mixture weight in [0,1]
      softmax_beta:      RL inverse temperature (scaled by 10 internally)
      gamma:             Load sensitivity for WM recall: recall ~ (3/set_size)^gamma
      age_effect:        Reduces WM recall and increases WM decay when older

    Returns:
      Negative log-likelihood of observed choices.
    """
    lr_pos, lr_neg, wm_weight, softmax_beta, gamma, age_effect = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    age_val = age[0]
    older = 1.0 if age_val >= 45 else 0.0

    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Age-dependent WM decay (older => more decay)
        wm_decay = np.clip(0.1 + 0.3 * age_effect * older, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            Q_center = Q_s[a]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_center)))

            # WM decay then policy (cache-like: one-hot when remembered)
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            W_s = w[s, :]
            W_center = W_s[a]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_center)))

            # WM recall probability: depends on load and age
            load_term = (3.0 / float(nS)) ** max(0.0, gamma)
            recall = np.clip(wm_weight * load_term * (1.0 - np.clip(age_effect * older, 0.0, 0.9)), 0.0, 1.0)

            p_total = recall * p_wm + (1.0 - recall) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update with valence asymmetry
            pe = r - Q_s[a]
            if pe >= 0.0:
                q[s, a] += lr_pos * pe
            else:
                q[s, a] += lr_neg * pe

            # WM cache update: store last rewarded action as one-hot
            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = target  # cache overwrite on success
            # else: keep decayed value (already applied at start of trial)

        blocks_log_p += log_p

    return -blocks_log_p