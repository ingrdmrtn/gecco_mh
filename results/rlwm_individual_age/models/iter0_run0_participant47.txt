def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM mixture with age- and capacity-modulated WM contribution.

    Core idea:
    - Decisions arise from a mixture of reinforcement learning (RL) and working memory (WM).
    - WM is precise but capacity-limited and subject to decay; it contributes more in low set size.
    - Older adults (age >= 45) have reduced effective WM contribution and capacity.
    - RL updates are standard delta-rule.
    - The mixture weight adapts to set size via a capacity parameter.

    Parameters (model_parameters):
    - lr: scalar in (0,1). RL learning rate for Q-values.
    - wm_weight_base: scalar in [0,1]. Base weight on WM policy.
    - softmax_beta: scalar > 0. Inverse temperature for RL policy (will be scaled internally).
    - wm_decay: scalar in [0,1]. WM decay rate toward uniform; larger = faster decay/forgetting.
    - k_capacity: scalar in [1,6]. Effective WM capacity in number of state-action associations.

    Inputs:
    - states: int array, trial-wise state indices.
    - actions: int array, trial-wise chosen actions (0..2).
    - rewards: int array, trial-wise rewards (0/1).
    - blocks: int array, trial-wise block index.
    - set_sizes: int array, trial-wise set size for the block (3 or 6).
    - age: array-like with a single value, participant age.

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight_base, softmax_beta, wm_decay, k_capacity = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # near-deterministic WM readout

    age_val = age[0]
    is_older = 1 if age_val >= 45 else 0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL policy probability for chosen action a
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WORKING MEMORY policy
            # WM readout: softmax over W_s (near-deterministic)
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm

            # Effective WM mixture weight: reduced for older adults and when set size exceeds capacity
            # Capacity factor: full WM if nS <= k_capacity; otherwise scales down as k_capacity/nS
            cap_factor = min(1.0, max(0.0, k_capacity) / max(1.0, nS))
            age_factor = 0.65 if is_older else 1.0  # older adults rely less on WM
            wm_weight_eff = np.clip(wm_weight_base * age_factor * cap_factor, 0.0, 1.0)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update
            # Decay toward uniform first
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # If rewarded, commit a one-shot memory to the chosen action (overwrite-like)
            if r > 0.5:
                # Make chosen action dominant; renormalize
                w[s, :] = (1.0 - wm_decay) * w[s, :]
                w[s, a] += wm_decay
                # Ensure normalization and non-negativity
                w[s, :] = np.maximum(w[s, :], 1e-12)
                w[s, :] /= np.sum(w[s, :])
            else:
                # If not rewarded, just leave the decayed distribution (already normalized up to numerical errors)
                w[s, :] = np.maximum(w[s, :], 1e-12)
                w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM with perseveration and age-dependent exploration/interference.

    Core idea:
    - Choices are a mixture of RL and WM.
    - WM experiences interference that grows with set size; older adults experience stronger interference.
    - RL includes action perseveration (stickiness) within each state.
    - Older adults show reduced inverse temperature (more exploration), especially at larger set sizes.

    Parameters (model_parameters):
    - lr: scalar in (0,1). RL learning rate.
    - wm_weight_base: scalar in [0,1]. Base WM weight.
    - softmax_beta: scalar > 0. Base inverse temperature for RL (scaled internally).
    - wm_decay: scalar in [0,1]. WM decay rate and interference driver.
    - stickiness: scalar in [0, +]. Additive bias for repeating last action in a state.
    - age_explore: scalar in [0,1]. Strength of age-related exploration (reduces beta for older adults at higher set sizes).

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age: as specified.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight_base, softmax_beta, wm_decay, stickiness, age_explore = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    age_val = age[0]
    is_older = 1 if age_val >= 45 else 0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        prev_action = -1 * np.ones(nS, dtype=int)  # per-state previous action

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # Age- and load-dependent inverse temperature for RL
            # Reduce beta more for older adults and larger set sizes
            load_factor = max(0.0, (nS - 3.0) / 3.0)  # 0 for 3-set, 1 for 6-set
            beta_eff = softmax_beta * (1.0 - is_older * age_explore * load_factor)
            beta_eff = max(1e-3, beta_eff)

            # Perseveration bias for last action in this state
            bias = np.zeros(3)
            if prev_action[s] >= 0:
                bias[prev_action[s]] += stickiness

            Q_eff = Q_s + bias

            # RL policy probability for chosen action a using Q_eff
            p_rl = 1.0 / np.sum(np.exp(beta_eff * (Q_eff - Q_eff[a])))

            # WM policy: near-deterministic readout
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # WM weight reduced by set size interference; stronger reduction for older adults
            # Sigmoid over set size with an age shift (older -> more interference)
            # For nS=3 => higher c; for nS=6 => lower c
            shift = 0.5 if is_older else 0.0
            c = 1.0 / (1.0 + np.exp((nS - 4.0) + shift))
            wm_weight_eff = np.clip(wm_weight_base * c, 0.0, 1.0)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update with interference/decay and reward-sensitive boost
            # First decay toward uniform (captures interference)
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            # Reward boosts the chosen association more strongly
            boost = wm_decay * (0.5 + 0.5 * r)  # = wm_decay*0.5 if r=0, = wm_decay*1.0 if r=1
            w[s, a] += boost
            # Renormalize and ensure non-negativity
            w[s, :] = np.maximum(w[s, :], 1e-12)
            w[s, :] /= np.sum(w[s, :])

            # Update perseveration memory
            prev_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM with uncertainty-based arbitration and asymmetric learning rates.

    Core idea:
    - Arbitration between RL and WM is dynamic: when WM memory is sharp and RL policy is uncertain
      (high entropy), the model relies more on WM; otherwise, more on RL.
    - WM decays to uniform, strengthened by reward; includes lateral competition implicitly via normalization.
    - RL learning is asymmetric: separate learning rates for rewards vs. non-rewards.
    - Older adults have reduced WM reliance under high load (nS=6).

    Parameters (model_parameters):
    - lr_pos: scalar in (0,1). RL learning rate for rewards (r=1).
    - lr_neg: scalar in (0,1). RL learning rate for non-rewards (r=0).
    - wm_weight_base: scalar in [0,1]. Baseline WM weight in arbitration.
    - softmax_beta: scalar > 0. Inverse temperature for RL action selection (scaled internally).
    - wm_decay: scalar in [0,1]. WM decay rate toward uniform.
    - arbitration_slope: scalar > 0. Sensitivity of the arbitrator to WM sharpness vs RL entropy.

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age: as specified.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr_pos, lr_neg, wm_weight_base, softmax_beta, wm_decay, arbitration_slope = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    age_val = age[0]
    is_older = 1 if age_val >= 45 else 0

    def softmax_probs(vec, beta):
        z = vec - np.max(vec)
        ez = np.exp(beta * z)
        p = ez / np.sum(ez)
        return p

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL policy and its entropy (uncertainty)
            pvec_rl = softmax_probs(Q_s, softmax_beta)
            H_rl = -np.sum(pvec_rl * np.log(np.maximum(pvec_rl, 1e-12)))
            p_rl = pvec_rl[a]

            # WM policy probability
            pvec_wm = softmax_probs(W_s, softmax_beta_wm)
            p_wm = pvec_wm[a]

            # WM sharpness measure: gap between top-1 and top-2 WM preferences
            sorted_w = np.sort(W_s)
            wm_gap = (sorted_w[-1] - sorted_w[-2]) if nA >= 2 else sorted_w[-1]

            # Age- and load-modulated arbitration: more WM when wm_gap > H_rl
            base = wm_weight_base
            arb_input = wm_gap - H_rl
            wm_weight = 1.0 / (1.0 + np.exp(-arbitration_slope * arb_input))

            # Apply baseline and age/load penalty to WM reliance
            wm_weight *= base
            if is_older and nS > 3:
                wm_weight *= 0.7  # older adults rely less on WM in high load

            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            lr_use = lr_pos if r > 0.5 else lr_neg
            delta = r - Q_s[a]
            q[s, a] += lr_use * delta

            # WM update: decay toward uniform, then reward-driven strengthening of chosen action
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            if r > 0.5:
                w[s, a] += wm_decay  # strengthen the chosen item
            # Normalize and ensure positivity
            w[s, :] = np.maximum(w[s, :], 1e-12)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p