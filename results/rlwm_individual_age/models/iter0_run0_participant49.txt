Below are three standalone cognitive models that implement RL–WM arbitration with age- and set-size-sensitive mechanisms. Each function follows the block-wise template, mixes an RL softmax policy with a WM softmax policy, and returns the negative log-likelihood of the observed choices.

Note: These functions assume numpy as np is already imported.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL–WM mixture with capacity-limited WM and age-dependent capacity reduction.

    Idea:
    - RL system: standard delta-rule with softmax.
    - WM system: item-specific, nearly deterministic retrieval that decays toward uniform.
    - Arbitration: WM weight scaled by effective capacity K_eff relative to set size.
      Older adults (age >= 45) have reduced effective capacity.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - wm_weight: baseline WM mixture weight (0..1)
    - softmax_beta: RL inverse temperature (rescaled internally)
    - wm_decay: WM decay toward uniform per access (0..1)
    - K_base: baseline WM capacity for younger
    - K_old_drop: capacity drop applied if older (>=45)

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wm_decay, K_base, K_old_drop = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # very deterministic WM
    is_old = 1.0 if age[0] >= 45 else 0.0

    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM value tables
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Effective WM capacity and mixture gating by set size
        K_eff = max(0.0, K_base - is_old * K_old_drop)  # age reduces capacity
        cap_gate = min(1.0, K_eff / max(1.0, nS))       # scale WM contribution by capacity vs set size
        gate = np.clip(wm_weight * cap_gate, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy: softmax probability of chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax on WM values, nearly deterministic
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture policy
            p_total = gate * p_wm + (1.0 - gate) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: decay toward uniform, then store if rewarded
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            if r > 0.5:
                # Store strong item-specific association for the rewarded action
                w[s, :] = w_0[s, :]
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL–WM mixture with age-dependent RL learning rate and set-size-driven WM interference.

    Idea:
    - RL: delta rule with age-specific learning rate (older learn more slowly).
    - WM: near one-shot storage on reward, decays to uniform.
    - Arbitration: WM weight decays exponentially with set size (interference),
      independent baseline wm_weight.
    - Age affects RL learning rate directly (older group uses lr_old).

    Parameters (model_parameters):
    - lr_young: RL learning rate for younger (<45)
    - lr_old: RL learning rate for older (>=45)
    - wm_weight: baseline WM mixture weight (0..1)
    - softmax_beta: RL inverse temperature (rescaled internally)
    - interference: WM interference per added item (>=0), higher = less WM at larger set sizes

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr_young, lr_old, wm_weight, softmax_beta, interference = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    is_old = 1.0 if age[0] >= 45 else 0.0
    lr = lr_old if is_old > 0.5 else lr_young

    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size driven WM weight (more interference at larger set sizes)
        # wm_effective = wm_weight * exp(-interference * (nS - 1))
        wm_effective = wm_weight * np.exp(-interference * max(0, nS - 1))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = wm_effective * p_wm + (1.0 - wm_effective) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: simple recency store with decay toward uniform
            # Decay rate tied to interference (more items -> more decay)
            wm_decay = 1.0 - np.exp(-interference * max(1, nS))  # in [0,1)
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            if r > 0.5:
                # Item-specific binding on reward
                w[s, :] = w_0[s, :]
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL–WM arbitration with logistic gating based on set size and age, plus WM forgetting.

    Idea:
    - RL: delta rule with single learning rate.
    - WM: decays toward uniform; reinforced when rewarded.
    - Arbitration: WM weight computed by a logistic gate that increases for small set sizes
      and for younger age; older age adds a negative bias to WM reliance.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - wm_weight_base: baseline intercept for WM gate (logit space)
    - setsize_gain: positive value increases WM reliance when set size is small (3 vs 6)
    - age_bias_old: negative value reduces WM reliance if older (>=45) in the gate
    - softmax_beta: RL inverse temperature (rescaled internally)
    - wm_decay: WM decay toward uniform per access (0..1)

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight_base, setsize_gain, age_bias_old, softmax_beta, wm_decay = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    is_old = 1.0 if age[0] >= 45 else 0.0

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Logistic gating favors WM for smaller set sizes; age bias reduces WM in older adults
        # gate = sigmoid(wm_weight_base + setsize_gain*(3 - nS) + age_bias_old*is_old)
        gate = sigmoid(wm_weight_base + setsize_gain * (3 - nS) + age_bias_old * is_old)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = gate * p_wm + (1.0 - gate) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update with fixed decay, reinforce on reward
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            if r > 0.5:
                w[s, :] = w_0[s, :]
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p