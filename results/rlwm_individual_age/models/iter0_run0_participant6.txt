def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM mixture with capacity- and age-modulated WM, plus forgetting and lapse.

    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..nS-1 within each block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of int
        Binary feedback on each trial (0/1).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size on each trial (3 or 6).
    age : array-like of float
        Participant age; age[0] is used to determine age group (<45 vs >=45).
    model_parameters : sequence of 6 floats
        lr : RL learning rate (also used as WM learning rate)
        wm_weight : Baseline WM mixture weight (will be modulated by load and age)
        softmax_beta : Inverse temperature for RL softmax (scaled by 10 internally)
        capacity_K : WM capacity parameter (in "items")
        wm_decay : Per-trial decay toward uniform for WM store (0=no decay; 1=full reset)
        lapse : Lapse probability mixing in uniform random choice

    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    lr, wm_weight, softmax_beta, capacity_K, wm_decay, lapse = model_parameters

    # Follow the template scaling/conventions
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic WM by default; we will still use this constant
    age = age[0]
    is_older = 1 if age >= 45 else 0

    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM value stores
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy probability for the chosen action
            Q_s = q[s, :]
            Q_s_center = Q_s - Q_s[a]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s_center)))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # Decay WM store slightly toward uniform (global decay each trial)
            w = (1.0 - wm_decay) * w + wm_decay * w_0

            # Compute WM policy for current state
            W_s = w[s, :]
            # Deterministic WM softmax (fixed high beta per template)
            W_s_center = W_s - W_s[a]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s_center)))

            # Effective WM mixture weight depends on capacity vs set size and age group
            load_factor = min(1.0, max(0.0, capacity_K / float(nS)))  # 0..1
            age_factor = 0.75 if is_older == 1 else 1.0              # older adults rely a bit less on WM
            wm_weight_eff = np.clip(wm_weight * load_factor * age_factor, 0.0, 1.0)

            wm_weight = wm_weight_eff  # use the effective weight in the template mixture line

            # Mixture with a small lapse (uniform choice)
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = (1.0 - lapse) * p_total + lapse * (1.0 / nA)
            p_total = np.clip(p_total, eps, 1.0)

            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Win-based one-shot storage: move WM toward a one-hot code for the rewarded action
            # Use the same lr as a "binding strength" for WM
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - lr) * w[s, :] + lr * one_hot
            else:
                # If incorrect, gently nudge away from the chosen action toward uniform
                w[s, a] = (1.0 - lr) * w[s, a] + lr * (1.0 / nA)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RLâ€“WM arbitration by uncertainty with age bias and action perseveration.

    Arbitration weight is computed from relative uncertainty (entropy) of RL vs WM
    policies, biased by age group. WM is updated rapidly on correct trials and
    decays toward uniform. RL has a single learning rate but we use separate
    learning for positive vs negative outcomes via lr and (1-lr).

    Parameters
    ----------
    states : array-like of int
        State index per trial.
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of int
        Binary reward per trial (0/1).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (3 or 6).
    age : array-like of float
        Participant age; age[0] used to determine age group (<45 vs >=45).
    model_parameters : sequence of 6 floats
        lr : Base RL learning rate (used for positive prediction errors; negative uses 1-lr)
        wm_weight : Baseline arbitration bias toward WM (used as an intercept term)
        softmax_beta : Inverse temperature for RL softmax (scaled by 10 internally)
        age_bias_wm : Additive bias toward WM if younger and away if older (enters arbitration)
        perseveration : Action stickiness added to last chosen action within a state
        wm_noise : Inverse precision factor for WM policy (higher -> noisier WM)

    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    lr, wm_weight, softmax_beta, age_bias_wm, perseveration, wm_noise = model_parameters

    softmax_beta *= 10
    softmax_beta_wm = 50  # baseline; we will reduce by wm_noise in WM policy
    age = age[0]
    is_older = 1 if age >= 45 else 0

    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track previous action per state for perseveration
        prev_action = -1 * np.ones(nS, dtype=int)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy with perseveration bias on the last action in this state
            Q_s = q[s, :].copy()
            if prev_action[s] >= 0:
                Q_s[prev_action[s]] += perseveration

            # Full RL softmax probabilities for entropy/arbitration
            Qs_shift = Q_s - np.max(Q_s)
            exp_rl = np.exp(softmax_beta * Qs_shift)
            probs_rl = exp_rl / np.sum(exp_rl)
            p_rl = np.clip(probs_rl[a], eps, 1.0)

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # Apply a mild global decay toward uniform
            decay = 0.05  # small fixed decay per trial
            w = (1.0 - decay) * w + decay * w_0

            # WM softmax with reduced precision if wm_noise is high
            W_s = w[s, :].copy()
            if prev_action[s] >= 0:
                # allow WM to also reflect stickiness slightly
                W_s[prev_action[s]] += 0.25 * perseveration
            W_shift = W_s - np.max(W_s)
            beta_wm_eff = softmax_beta_wm / (1.0 + wm_noise)
            exp_wm = np.exp(beta_wm_eff * W_shift)
            probs_wm = exp_wm / np.sum(exp_wm)
            p_wm = np.clip(probs_wm[a], eps, 1.0)

            # Compute arbitration weight from relative uncertainty (lower entropy => higher weight)
            H_rl = -np.sum(probs_rl * np.log(np.clip(probs_rl, eps, 1.0)))
            H_wm = -np.sum(probs_wm * np.log(np.clip(probs_wm, eps, 1.0)))

            # Age bias: younger favor WM; older favor RL (via age_bias_wm term)
            age_term = (1.0 - is_older) - is_older  # +1 if younger, -1 if older
            intercept = wm_weight  # baseline bias toward WM
            # Higher RL entropy than WM => rely more on WM, and vice versa
            arb_input = intercept + (H_rl - H_wm) + age_bias_wm * age_term
            wm_weight_eff = 1.0 / (1.0 + np.exp(-arb_input))
            wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

            wm_weight = wm_weight_eff  # use effective arbitration in the template mixture

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update with asymmetry: lr+ for rewards, lr- = (1-lr) for non-rewards
            delta = r - q[s][a]
            if delta >= 0:
                q[s][a] += lr * delta
            else:
                q[s][a] += (1.0 - lr) * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Rapid binding on correct, suppression on incorrect
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - lr) * w[s, :] + lr * one_hot
            else:
                # reduce weight on the chosen action; redistribute a bit to others
                w[s, a] = (1.0 - lr) * w[s, a] + lr * (1.0 / nA)

            # Update perseveration tracker
            prev_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Load-specific RL learning rates with capacity-limited WM and action stickiness.

    RL uses separate learning rates for small vs large set sizes. WM contribution
    depends on capacity relative to set size and age group (effective capacity boost
    if younger, reduction if older). WM precision (beta) is parameterized. A motor
    bias (stickiness) favors repeating the last action within a state.

    Parameters
    ----------
    states : array-like of int
        State index per trial.
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of int
        Binary reward per trial (0/1).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (3 or 6).
    age : array-like of float
        Participant age; age[0] used to determine age group (<45 vs >=45).
    model_parameters : sequence of 6 floats
        lr_small : RL learning rate used in set size 3 blocks
        lr_large : RL learning rate used in set size 6 blocks
        softmax_beta : Inverse temperature for RL softmax (scaled by 10 internally)
        capacity_K : WM capacity parameter (in "items")
        beta_wm_base : Base WM precision scaling factor (multiplies the template WM beta)
        motor_bias_scale : Strength of action stickiness toward last action in state

    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    lr_small, lr_large, softmax_beta, capacity_K, beta_wm_base, motor_bias_scale = model_parameters

    softmax_beta *= 10
    softmax_beta_wm = 50  # base; we will scale by beta_wm_base and age
    age = age[0]
    is_older = 1 if age >= 45 else 0

    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track previous action per state for motor bias
        prev_action = -1 * np.ones(nS, dtype=int)

        # Choose RL learning rate based on set size
        lr = lr_small if nS <= 3 else lr_large

        # Effective WM precision depends on age
        age_prec_factor = 0.8 if is_older == 1 else 1.2
        beta_wm_eff_global = softmax_beta_wm * np.clip(beta_wm_base * age_prec_factor, 0.1, 5.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy with motor bias toward repeating last action in this state
            Q_s = q[s, :].copy()
            if prev_action[s] >= 0:
                Q_s[prev_action[s]] += motor_bias_scale

            Qs_center = Q_s - Q_s[a]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * Qs_center))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # Capacity- and load-dependent WM weight per trial
            cap_eff = capacity_K * (1.1 if is_older == 0 else 0.9)
            load_factor = np.clip(cap_eff / float(nS), 0.0, 1.0)

            # State-wise WM softmax with effective precision and motor bias
            W_s = w[s, :].copy()
            if prev_action[s] >= 0:
                W_s[prev_action[s]] += 0.5 * motor_bias_scale
            Ws_center = W_s - W_s[a]
            # Use the global effective WM precision
            p_wm = 1.0 / np.sum(np.exp(beta_wm_eff_global * Ws_center))

            # Effective WM mixture weight: stronger in small sets and if younger
            base_wm = 0.5  # implicit baseline before template mixture param
            wm_weight_eff = np.clip(base_wm * load_factor, 0.0, 1.0)
            wm_weight = wm_weight_eff  # use effective mixture in the template line

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s][a]
            q[s][a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Trial-level interference/forgetting increases with load
            decay = 1.0 - load_factor  # more decay at larger set sizes
            w = (1.0 - decay) * w + decay * w_0

            # WM update: push toward one-hot on correct, mild redistribution on incorrect
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - lr) * w[s, :] + lr * one_hot
            else:
                w[s, a] = (1.0 - lr) * w[s, a] + lr * (1.0 / nA)

            # Update last action for motor bias
            prev_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p