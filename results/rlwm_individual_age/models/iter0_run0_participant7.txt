def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM mixture with capacity-limited WM and age-sensitive gating.

    Idea:
    - Decisions arise from a mixture of incremental RL and a working-memory (WM) store.
    - WM encodes stimulus-action mappings when rewarded, but decays toward uniform over time.
    - WM contribution is scaled by set size via an effective capacity K that differs by age group.
      Younger: higher K; Older: lower K; effective WM weight scales as min(1, K/nS).
    - RL uses a Rescorla-Wagner update.
    
    Parameters (model_parameters):
    - lr: RL learning rate in [0,1]
    - wm_weight: base mixing weight for WM contribution in [0,1]
    - softmax_beta: inverse temperature for RL policy (scaled internally by 10)
    - wm_decay: decay strength moving WM weights toward uniform each visit, in [0,1]
    - wm_noise: encoding noise on rewarded WM update (0=perfect one-hot; 1=uniform)
    
    Inputs:
    - states: array of state indices per trial (0..nS-1 inside a block)
    - actions: array of chosen actions per trial (0..2)
    - rewards: array of binary rewards per trial (0/1)
    - blocks: array of block indices per trial
    - set_sizes: array of set sizes per trial (3 or 6)
    - age: array-like with a single numeric age, age[0]
    
    Returns:
    - Negative log-likelihood of observed actions under the model.
    """
    lr, wm_weight, softmax_beta, wm_decay, wm_noise = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # near-deterministic WM policy
    age_val = age[0]
    is_older = 1 if age_val >= 45 else 0

    # Age-dependent WM capacity (no extra parameter cost):
    # Younger have higher WM capacity than older.
    K_young, K_old = 4.0, 2.5
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM stores
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Compute effective WM weight for this block via capacity and age
        K = K_old if is_older else K_young
        cap_scale = min(1.0, K / float(nS))
        wm_weight_eff_block = np.clip(wm_weight * cap_scale, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability of chosen action a
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl

            # WM policy probability of chosen action a
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm

            # Mixture
            p_total = wm_weight_eff_block * p_wm + (1.0 - wm_weight_eff_block) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # WM encoding on reward: bias toward the chosen action with some noise
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                encoded = (1.0 - wm_noise) * one_hot + wm_noise * (1.0 / nA)
                w[s, :] = encoded

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM mixture with asymmetric RL learning rates, perseveration bias, and age-sensitive WM capacity.

    Idea:
    - RL updates have separate learning rates for positive and negative prediction errors.
    - WM encodes rewarded mappings with decay; its contribution scales with set size and age (capacity-limited).
    - A perseveration bias favors repeating the last action taken in a state, more pronounced in older adults.
    - Age moderates exploration: older adults show lower effective beta.

    Parameters (model_parameters):
    - lr_pos: RL learning rate for positive PE in [0,1]
    - lr_neg: RL learning rate for negative PE in [0,1]
    - wm_weight: base WM mixture weight in [0,1]
    - softmax_beta: inverse temperature baseline (scaled internally by 10)
    - wm_decay: WM decay toward uniform per visit in [0,1]
    - perseveration: bias added to last chosen action logits (>=0)

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age: arrays as specified
    - age: array-like with a single numeric age, age[0]

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr_pos, lr_neg, wm_weight, softmax_beta, wm_decay, perseveration = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    age_val = age[0]
    is_older = 1 if age_val >= 45 else 0

    # Age-sensitive WM capacity and exploration
    K_young, K_old = 4.0, 2.0
    beta_age_scale = 0.8 if is_older else 1.0  # older: slightly lower beta (more noise)
    persev_age_boost = 1.25 if is_older else 1.0

    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        last_action = -1 * np.ones(nS, dtype=int)

        # Block-level WM mixture weight via capacity
        K = K_old if is_older else K_young
        cap_scale = min(1.0, K / float(nS))
        wm_weight_eff_block = np.clip(wm_weight * cap_scale, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # Add perseveration bias to Q logits for policy computation
            bias_vec = np.zeros(nA)
            if last_action[s] >= 0:
                bias_vec[last_action[s]] = perseveration * persev_age_boost

            Q_logits = Q_s + bias_vec
            beta_eff = softmax_beta * beta_age_scale

            # RL policy prob for chosen action with perseveration-adjusted logits
            denom_rl = np.sum(np.exp(beta_eff * (Q_logits - Q_logits[a])))
            p_rl = 1.0 / denom_rl

            # WM policy
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm

            # Mixture
            p_total = wm_weight_eff_block * p_wm + (1.0 - wm_weight_eff_block) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            pe = r - q[s, a]
            eta = lr_pos if pe >= 0.0 else lr_neg
            q[s, a] += eta * pe

            # WM decay toward uniform
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # WM encoding on reward
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                # reuse wm_decay as small smoothing; sharper than RL perseveration
                encoded = (1.0 - 0.0) * one_hot + 0.0 * (1.0 / nA)
                w[s, :] = encoded

            # Update perseveration memory
            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with forgetting + dynamic WM gating by state-specific confidence, modulated by set size and age.

    Idea:
    - RL: standard Rescorla-Wagner with a small forgetting toward uniform values.
    - WM: stores recently rewarded mappings; decays toward uniform each visit.
    - WM mixture weight is state-specific and dynamic: grows with recent rewards (confidence) and
      is scaled by capacity (depends on set size and age).
    - Age: older adults have lower WM capacity and higher RL forgetting.

    Parameters (model_parameters):
    - lr: RL learning rate in [0,1]
    - softmax_beta: inverse temperature for RL (scaled internally by 10)
    - wm_weight: base WM weight in [0,1]
    - wm_decay: WM decay toward uniform per visit in [0,1]
    - rl_forget: RL forgetting rate toward uniform per visit in [0,1]
    - wm_noise: encoding noise for WM on reward in [0,1]

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age: arrays as specified
    - age: array-like with a single numeric age, age[0]

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr, softmax_beta, wm_weight, wm_decay, rl_forget, wm_noise = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    age_val = age[0]
    is_older = 1 if age_val >= 45 else 0

    # Age effects
    K_young, K_old = 4.0, 2.0
    rl_forget_age = rl_forget * (1.25 if is_older else 1.0)  # older: more forgetting

    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # State-specific WM confidence (0..1), starts low
        c = np.zeros(nS)

        # Capacity scaling by set size and age
        K = K_old if is_older else K_young
        cap_scale = min(1.0, K / float(nS))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl

            # WM policy
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm

            # Dynamic, state-specific WM weight: grows with confidence c[s]
            wm_weight_state = np.clip(wm_weight * cap_scale * c[s], 0.0, 1.0)

            p_total = wm_weight_state * p_wm + (1.0 - wm_weight_state) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with forgetting toward uniform
            pe = r - q[s, a]
            q[s, a] += lr * pe
            # forgetting only on the visited state's action values toward uniform
            q[s, :] = (1.0 - rl_forget_age) * q[s, :] + rl_forget_age * (1.0 / nA)

            # WM decay toward uniform
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # WM encoding on reward with noise
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                encoded = (1.0 - wm_noise) * one_hot + wm_noise * (1.0 / nA)
                w[s, :] = encoded

            # Update WM confidence: tracks recent rewards at the state
            # Using wm_decay as smoothing rate
            c[s] = (1.0 - wm_decay) * c[s] + wm_decay * r

        blocks_log_p += log_p

    return -blocks_log_p