def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM mixture with capacity scaling by set size and age group.

    The model mixes a reinforcement-learning (RL) policy with a working-memory (WM) policy.
    The effective contribution of WM is down-weighted when set size exceeds WM capacity, and
    WM capacity depends on age group.

    Parameters
    ----------
    states : np.ndarray of int
        State index at each trial (0..nS-1 within a block).
    actions : np.ndarray of int
        Observed chosen action (0..nA-1).
    rewards : np.ndarray of int
        Binary feedback (0 or 1).
    blocks : np.ndarray of int
        Block index per trial.
    set_sizes : np.ndarray of int
        Set size (3 or 6) per trial.
    age : np.ndarray of float
        Participant age as a length-1 array; used to determine age group (<45 = younger).
    model_parameters : iterable of float
        [lr, wm_weight, softmax_beta, K_young, K_old, wm_decay]
        - lr: RL learning rate (0..1).
        - wm_weight: base mixture weight for WM (0..1).
        - softmax_beta: inverse temperature for RL policy (scaled internally).
        - K_young: WM capacity estimate for younger participants.
        - K_old: WM capacity estimate for older participants.
        - wm_decay: WM decay/learning rate controlling both decay and overwrite strength (0..1).

    Returns
    -------
    nll : float
        Negative log-likelihood of the observed choices.
    """
    lr, wm_weight, softmax_beta, K_young, K_old, wm_decay = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    age = age[0]
    is_younger = 1.0 if age < 45 else 0.0
    K = K_young if is_younger == 1.0 else K_old

    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))   # RL Q-values
        w = (1 / nA) * np.ones((nS, nA))   # WM "Q"-values
        w_0 = (1 / nA) * np.ones((nS, nA)) # WM baseline

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy probability of chosen action
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax over WM weights
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Effective WM weight scales with capacity relative to set size and age
            # capacity factor c in [0,1]
            c = min(1.0, max(0.0, K / float(nS)))
            wm_weight_eff = np.clip(wm_weight * c, 0.0, 1.0)

            # Mixture policy
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: decay towards baseline, then strengthen chosen action if rewarded
            # decay row s toward w_0
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            if r > 0.5:
                # strengthen chosen action for this state
                w[s, :] = (1.0 - wm_decay) * w[s, :]
                w[s, a] += wm_decay

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM arbitration by uncertainty (entropy), with age bias and stickiness.

    The model mixes RL and WM policies using a dynamic weight based on each system's
    uncertainty (entropy). WM contribution is down-weighted at larger set sizes and for
    older adults (age bias), while a choice stickiness term biases RL toward repeating
    the previous action in a state.

    Parameters
    ----------
    states : np.ndarray of int
    actions : np.ndarray of int
    rewards : np.ndarray of int (0/1)
    blocks : np.ndarray of int
    set_sizes : np.ndarray of int
    age : np.ndarray of float (length 1)
    model_parameters : iterable of float
        [lr, wm_weight_base, softmax_beta, wm_decay, age_bias, stickiness]
        - lr: RL learning rate (0..1)
        - wm_weight_base: base WM mixture weight (0..1)
        - softmax_beta: RL inverse temperature (scaled internally)
        - wm_decay: WM decay/update rate (0..1)
        - age_bias: multiplicative reduction in WM weight for older adults (>=0)
        - stickiness: bias added to previous action in RL softmax (>0 favors perseveration)

    Returns
    -------
    nll : float
        Negative log-likelihood.
    """
    lr, wm_weight_base, softmax_beta, wm_decay, age_bias, stickiness = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    age = age[0]
    is_older = 1.0 if age >= 45 else 0.0

    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))   # RL Q-values
        w = (1 / nA) * np.ones((nS, nA))   # WM weights
        w_0 = (1 / nA) * np.ones((nS, nA)) # WM baseline
        prev_choice = -1 * np.ones(nS, dtype=int)  # for stickiness within state

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy with stickiness bias for previous action in this state
            Q_s = q[s, :].copy()
            if prev_choice[s] >= 0:
                Q_s[prev_choice[s]] += stickiness
            # compute p(a) under softmax without explicitly normalizing vector
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy softmax
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Entropy-based arbitration:
            # compute full softmax distributions to get entropies
            rl_logits = softmax_beta * (Q_s - np.max(Q_s))
            prl_all = np.exp(rl_logits)
            prl_all = prl_all / np.sum(prl_all)
            wm_logits = softmax_beta_wm * (W_s - np.max(W_s))
            pwm_all = np.exp(wm_logits)
            pwm_all = pwm_all / np.sum(pwm_all)
            # normalized entropies in [0,1]
            H_rl = -np.sum(prl_all * (np.log(prl_all + eps))) / np.log(nA)
            H_wm = -np.sum(pwm_all * (np.log(pwm_all + eps))) / np.log(nA)

            # base WM weight penalized by set size and age group
            # larger set size => smaller WM influence; older => additional reduction
            base = wm_weight_base * (3.0 / float(nS)) * (1.0 - is_older * np.clip(age_bias, 0.0, 1.0))
            base = np.clip(base, 0.0, 1.0)

            # Allocate more weight to the system with lower entropy
            inv_unc_rl = (1.0 - H_rl)
            inv_unc_wm = (1.0 - H_wm)
            denom = inv_unc_rl + inv_unc_wm + eps
            arb = (inv_unc_wm / denom)
            wm_weight_eff = np.clip(base * arb + (1.0 - base) * 0.0 + base * 0.0, 0.0, 1.0)
            # Note: arb in [0,1]; base scales overall WM contribution

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM update: decay toward baseline; strengthen chosen action if rewarded
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            if r > 0.5:
                w[s, :] = (1.0 - wm_decay) * w[s, :]
                w[s, a] += wm_decay

            # update prev choice for stickiness
            prev_choice[s] = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with forgetting + capacity-limited WM slots (state gating by capacity and age).

    WM can store up to K states in the current block; when a rewarded association is
    observed for a state, that state is admitted to WM if capacity allows (FIFO eviction).
    If the probed state is not in WM, the WM policy is uniform and RL drives behavior.
    RL includes a forgetting term toward uniform.

    Parameters
    ----------
    states : np.ndarray of int
    actions : np.ndarray of int
    rewards : np.ndarray of int (0/1)
    blocks : np.ndarray of int
    set_sizes : np.ndarray of int
    age : np.ndarray of float (length 1)
    model_parameters : iterable of float
        [lr, wm_weight, softmax_beta, K_young, K_old, rl_forget]
        - lr: RL learning rate (0..1)
        - wm_weight: base WM mixture weight (0..1)
        - softmax_beta: RL inverse temperature (scaled internally)
        - K_young: WM capacity for younger adults
        - K_old: WM capacity for older adults
        - rl_forget: RL forgetting rate toward uniform (0..1)

    Returns
    -------
    nll : float
        Negative log-likelihood.
    """
    lr, wm_weight, softmax_beta, K_young, K_old, rl_forget = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    age = age[0]
    is_younger = 1.0 if age < 45 else 0.0
    K_base = K_young if is_younger == 1.0 else K_old

    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))   # RL Q-values
        w = (1 / nA) * np.ones((nS, nA))   # WM values per state
        w_0 = (1 / nA) * np.ones((nS, nA)) # WM baseline

        # WM capacity for this block and participant; cap by set size
        K = int(np.clip(np.floor(K_base), 0, nS))
        wm_states = []  # FIFO list of states currently in WM

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL forgetting on the current state toward uniform
            q[s, :] = (1.0 - rl_forget) * q[s, :] + rl_forget * (1.0 / nA)

            # RL choice prob
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # Determine if current state is represented in WM
            s_in_wm = (s in wm_states)
            if s_in_wm:
                W_s = w[s, :]
                p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
                wm_weight_eff = np.clip(wm_weight * (K / float(nS)), 0.0, 1.0)
            else:
                # WM provides no information (uniform); mixture reduces to RL
                p_wm = 1.0 / nA
                wm_weight_eff = 0.0  # no WM contribution for states outside capacity

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM update and capacity management
            if r > 0.5:
                # admit state to WM if necessary
                if not s_in_wm and K > 0:
                    if len(wm_states) >= K:
                        # evict oldest
                        evicted = wm_states.pop(0)
                        # optional: reset evicted state's WM values to baseline
                        w[evicted, :] = w_0[evicted, :].copy()
                    wm_states.append(s)
                # write one-hot preference for rewarded action
                w[s, :] = w_0[s, :].copy()
                w[s, a] = 1.0
            else:
                # for non-reward, gently decay WM toward baseline for this state
                w[s, :] = 0.5 * w[s, :] + 0.5 * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p