def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + decaying working-memory precision bias, with age-dependent WM precision.

    Brief:
    - Standard model-free RL updates Q-values with learning rate alpha.
    - In addition, a fast working-memory (WM) trace stores the last rewarded action for each state,
      with a strength that decays geometrically by lambda each trial.
    - The WM trace adds an additive bias to action logits whose magnitude (precision) depends on age group.
      Younger vs. older participants can have different WM precisions.
    - Policy is a softmax over (beta * Q + wm_bias).

    Parameters
    ----------
    states : array-like of int
        State index at each trial (0-based within a block).
    actions : array-like of int
        Chosen action at each trial. Valid actions are {0,1,2}.
    rewards : array-like of int
        Observed rewards in {0,1}. Non-binary values will be treated as invalid trials (uniform likelihood).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (3 or 6), constant within a block. Used to determine number of states per block.
    age : array-like (length 1)
        Participant age. Age group threshold: <45 is younger, >=45 is older. Determines WM precision.
    model_parameters : tuple/list of floats
        (alpha, beta, wm_precision_young, wm_precision_old, wm_decay_lambda)
        - alpha: RL learning rate (0-1).
        - beta: inverse temperature for RL softmax (>=0).
        - wm_precision_young: additive logit boost for the memorized action (younger group).
        - wm_precision_old: additive logit boost for the memorized action (older group).
        - wm_decay_lambda: per-trial decay of WM strength (0-1).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, wm_prec_y, wm_prec_o, lam = model_parameters
    age_val = age[0]
    wm_prec = wm_prec_y if age_val < 45 else wm_prec_o

    states = np.asarray(states)
    actions = np.asarray(actions)
    rewards = np.asarray(rewards)
    blocks = np.asarray(blocks)
    set_sizes = np.asarray(set_sizes)

    nll = 0.0
    nA = 3

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        if len(block_states) == 0:
            continue

        # Infer number of states in block from observed states (robust to missing)
        nS = int(np.max(block_states) + 1)
        Q = np.zeros((nS, nA))

        # WM: for each state, store a preferred action and its current strength
        wm_action = -1 * np.ones(nS, dtype=int)
        wm_strength = np.zeros(nS)

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            # Handle invalid trials robustly
            if (a < 0) or (a >= nA) or (s < 0) or (s >= nS) or (r < 0) or (r > 1):
                nll -= np.log(1.0 / nA)
                # Decay WM even on invalid trials to maintain temporal dynamics
                wm_strength *= lam
                continue

            # Build logits from RL and WM bias
            logits = beta * Q[s, :].copy()

            # WM bias: add a precision-weighted one-hot for memorized action, scaled by current strength
            if wm_action[s] >= 0 and wm_strength[s] > 0.0:
                logits[wm_action[s]] += wm_prec * wm_strength[s]

            # Softmax
            logits -= np.max(logits)
            exp_logits = np.exp(logits)
            p_vec = exp_logits / np.sum(exp_logits)
            p = np.clip(p_vec[a], 1e-12, 1.0)
            nll -= np.log(p)

            # RL update
            td = r - Q[s, a]
            Q[s, a] += alpha * td

            # WM update: decay global strength, then refresh on successful reward
            wm_strength *= lam
            if r == 1:
                wm_action[s] = a
                wm_strength[s] = 1.0  # reset to full strength for the rewarded association

    return nll


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Asymmetric RL learning rates shaped by set size and age, with a small global lapse.

    Brief:
    - Model-free RL with separate positive and negative learning rates derived from a base alpha and
      an asymmetry term that scales with set size and age group.
    - The asymmetry increases with set size (higher cognitive load), and is modulated by age:
      older vs younger groups can show different asymmetry via an age_shift parameter.
    - Softmax policy with inverse temperature beta.
    - Global lapse mixes the softmax policy with uniform responding.

    Parameters
    ----------
    states : array-like of int
        State index at each trial (0-based within a block).
    actions : array-like of int
        Chosen action at each trial. Valid actions are {0,1,2}.
    rewards : array-like of int
        Observed rewards in {0,1}. Non-binary values treated as invalid (uniform likelihood).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (3 or 6), constant within a block. Drives asymmetry scaling.
    age : array-like (length 1)
        Participant age. Age group (<45 vs >=45) modulates asymmetry via age_shift.
    model_parameters : tuple/list of floats
        (alpha_base, asym_strength, beta, lapse, age_shift)
        - alpha_base: base learning rate used for both valences before asymmetry.
        - asym_strength: magnitude of valence asymmetry induced by load (>=0).
        - beta: inverse temperature for softmax (>=0).
        - lapse: lapse probability mixing with uniform responding (0-1).
        - age_shift: multiplicative modifier of asymmetry for older vs younger.
          Effective asymmetry = asym_strength * (1 + sign_age * age_shift),
          where sign_age = -1 for younger, +1 for older.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha_base, asym_strength, beta, lapse, age_shift = model_parameters
    age_val = age[0]
    sign_age = -1.0 if age_val < 45 else 1.0
    asym_age = asym_strength * (1.0 + sign_age * age_shift)

    states = np.asarray(states)
    actions = np.asarray(actions)
    rewards = np.asarray(rewards)
    blocks = np.asarray(blocks)
    set_sizes = np.asarray(set_sizes)

    nll = 0.0
    nA = 3

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        if len(block_states) == 0:
            continue

        nS = int(np.max(block_states) + 1)
        Q = np.zeros((nS, nA))

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            if (a < 0) or (a >= nA) or (s < 0) or (s >= nS) or (r < 0) or (r > 1):
                nll -= np.log(1.0 / nA)
                continue

            ss = int(block_set_sizes[t])
            # Load-dependent asymmetry scaling: 0 at ss=3, up to ~1 at ss=6
            load_scale = max(0.0, (ss - 3.0) / 3.0)
            asym_eff = asym_age * load_scale
            asym_eff = np.clip(asym_eff, -0.99, 0.99)  # keep effective learning rates positive

            alpha_pos = alpha_base * (1.0 + asym_eff)
            alpha_neg = alpha_base * (1.0 - asym_eff)

            # Policy
            logits = beta * Q[s, :]
            logits -= np.max(logits)
            exp_logits = np.exp(logits)
            p_vec = exp_logits / np.sum(exp_logits)
            p_soft = np.clip(p_vec[a], 1e-12, 1.0)
            p = (1.0 - lapse) * p_soft + lapse * (1.0 / nA)
            p = np.clip(p, 1e-12, 1.0)
            nll -= np.log(p)

            # Update with valence-asymmetric learning
            td = r - Q[s, a]
            if td >= 0:
                Q[s, a] += alpha_pos * td
            else:
                Q[s, a] += alpha_neg * td

    return nll


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with directed exploration via uncertainty bonus (UCB-style), with age- and load-modulated bonus.

    Brief:
    - Standard RL learning of Q-values with learning rate alpha.
    - A directed exploration bonus encourages sampling uncertain actions using an uncertainty proxy
      u(s,a) = 1 / sqrt(N(s,a) + 1).
    - The weight of the bonus increases with set size (higher load) and is modulated by age group
      via an additive shift on the base weight.
    - Policy is softmax over (beta * (Q + bonus_weight * u)).

    Parameters
    ----------
    states : array-like of int
        State index at each trial (0-based within a block).
    actions : array-like of int
        Chosen action at each trial. Valid actions are {0,1,2}.
    rewards : array-like of int
        Observed rewards in {0,1}. Non-binary values treated as invalid (uniform likelihood).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (3 or 6), constant within a block. Modulates exploration bonus.
    age : array-like (length 1)
        Participant age. Age group (<45 vs >=45) modulates the bonus via age_bonus_shift.
    model_parameters : tuple/list of floats
        (alpha, beta, bonus_weight_base, bonus_size_slope, age_bonus_shift)
        - alpha: RL learning rate (0-1).
        - beta: inverse temperature for softmax (>=0).
        - bonus_weight_base: base weight for the uncertainty bonus (>=0).
        - bonus_size_slope: how much the bonus increases per unit of (set_size - 3).
        - age_bonus_shift: additive shift to the bonus weight for older (>=45) vs younger (<45):
            bonus_weight = base + bonus_size_slope*(ss-3) + sign_age*age_bonus_shift
            where sign_age = -1 for younger, +1 for older.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, w_base, w_slope, age_shift = model_parameters
    age_val = age[0]
    sign_age = -1.0 if age_val < 45 else 1.0

    states = np.asarray(states)
    actions = np.asarray(actions)
    rewards = np.asarray(rewards)
    blocks = np.asarray(blocks)
    set_sizes = np.asarray(set_sizes)

    nll = 0.0
    nA = 3

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        if len(block_states) == 0:
            continue

        nS = int(np.max(block_states) + 1)
        Q = np.zeros((nS, nA))
        N = np.zeros((nS, nA))  # visit counts for uncertainty

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            if (a < 0) or (a >= nA) or (s < 0) or (s >= nS) or (r < 0) or (r > 1):
                nll -= np.log(1.0 / nA)
                continue

            ss = int(block_set_sizes[t])
            # Compute uncertainty bonus per action
            u = 1.0 / np.sqrt(N[s, :] + 1.0)

            # Weight of bonus depends on set size and age
            w = w_base + w_slope * (ss - 3.0) + sign_age * age_shift
            w = max(0.0, w)  # keep non-negative to ensure it's a bonus

            logits = beta * (Q[s, :] + w * u)
            logits -= np.max(logits)
            exp_logits = np.exp(logits)
            p_vec = exp_logits / np.sum(exp_logits)
            p = np.clip(p_vec[a], 1e-12, 1.0)
            nll -= np.log(p)

            # Update counts and Q-values
            N[s, a] += 1.0
            td = r - Q[s, a]
            Q[s, a] += alpha * td

    return nll