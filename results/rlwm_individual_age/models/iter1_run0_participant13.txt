def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + Episodic cache mixture with age- and load-dependent cache gating.

    The model blends a slow RL system with a fast, one-shot episodic cache that stores
    the last rewarded action for each state. The contribution of the cache to choice
    is modulated by age group (younger vs older) and by set size (interference).

    Parameters
    ----------
    states : array-like of int
        State indices on each trial (0..nS-1 within block).
    actions : array-like of int
        Chosen actions on each trial (0..nA-1).
    rewards : array-like of int
        Binary rewards (0 or 1) observed each trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size (3 or 6) for the block of each trial.
    age : array-like of float
        Participant age; age[0] is used to define age group (younger <45, older >=45).
    model_parameters : tuple/list of floats
        (alpha, beta, cache_strength, cache_decay, age_cache_bonus, interference)
        - alpha: RL learning rate (squashed 0..1)
        - beta: inverse temperature for RL softmax (scaled internally)
        - cache_strength: baseline gate favoring episodic cache (logit space)
        - cache_decay: per-trial decay of cached evidence (squashed to 0..1)
        - age_cache_bonus: additive gate boost for younger group (older gets 0)
        - interference: reduction of cache gate per unit increase in set size

    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    alpha, beta, cache_strength, cache_decay, age_cache_bonus, interference = model_parameters

    # Squash/scale parameters
    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    alpha = sigmoid(alpha)
    beta = sigmoid(beta) * 12.0
    cache_decay = sigmoid(cache_decay)  # 0..1
    # cache_strength, age_cache_bonus, interference are used linearly in the gate's logit

    age_val = float(age[0])
    younger = 1 if age_val < 45 else 0

    nll = 0.0
    eps = 1e-12
    nA = 3

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx]
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_set_sizes = set_sizes[idx]
        nS = int(block_set_sizes[0])

        # RL values
        Q = np.zeros((nS, nA))
        # Episodic cache: confidence for each action; starts neutral (all zeros)
        E = np.zeros((nS, nA))

        # Compute gate toward cache this block (age and load dependent)
        size_term = interference * max(0, nS - 3)
        gate_logit = cache_strength + age_cache_bonus * younger - size_term
        gate = sigmoid(gate_logit)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # RL policy
            q_s = Q[s, :].copy()
            q_s -= np.max(q_s)
            pi_rl = np.exp(beta * q_s)
            pi_rl = pi_rl / (np.sum(pi_rl) + eps)

            # Episodic cache policy: a sharp softmax over cached evidence
            e_s = E[s, :].copy()
            e_s -= np.max(e_s)
            pi_cache = np.exp(15.0 * e_s)  # high inverse temperature for cache
            pi_cache = pi_cache / (np.sum(pi_cache) + eps)

            # Mixture policy
            pi = gate * pi_cache + (1.0 - gate) * pi_rl

            p = max(pi[a], eps)
            nll -= np.log(p)

            # RL update
            delta = r - Q[s, a]
            Q[s, a] += alpha * delta

            # Episodic cache update with decay and one-shot encoding
            # Global decay of cached evidence in this state
            E[s, :] *= (1.0 - cache_decay)
            # If rewarded, store a clean "episode" for the chosen action
            if r > 0.5:
                # Clear competing actions (winner-take-all memory)
                E[s, :] = 0.0
                E[s, a] = 1.0
            else:
                # If not rewarded, slightly suppress the chosen action evidence
                # to avoid reinforcing incorrect episodes
                E[s, a] = max(0.0, E[s, a] - cache_decay)

    return nll


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with uncertainty-driven exploration (UCB-like) modulated by age and set size.

    The model learns Q-values via delta rule and augments choices with an
    uncertainty bonus derived from state-action visit counts (Bayesian intuition).
    Younger participants are modeled to use a stronger uncertainty bonus; higher
    set sizes dampen the strength of the bonus.

    Parameters
    ----------
    states : array-like of int
        State indices on each trial (0..nS-1 within block).
    actions : array-like of int
        Chosen actions on each trial (0..nA-1).
    rewards : array-like of int
        Binary rewards (0 or 1) observed each trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size (3 or 6) for the block of each trial.
    age : array-like of float
        Participant age; age[0] is used to define age group (younger <45, older >=45).
    model_parameters : tuple/list of floats
        (alpha, beta, prior_strength, bonus_weight, age_explore_shift, size_penalty)
        - alpha: learning rate (squashed 0..1)
        - beta: inverse temperature for softmax (scaled internally)
        - prior_strength: pseudo-count added to visits (squashed to >=0 via softplus)
        - bonus_weight: baseline weight of uncertainty bonus
        - age_explore_shift: additive shift to bonus weight for younger group (older=0)
        - size_penalty: reduction of bonus weight per unit increase in set size

    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    alpha, beta, prior_strength, bonus_weight, age_explore_shift, size_penalty = model_parameters

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))
    def softplus(x):
        return np.log1p(np.exp(x))

    alpha = sigmoid(alpha)
    beta = sigmoid(beta) * 12.0
    prior_strength = softplus(prior_strength)  # >= 0

    age_val = float(age[0])
    younger = 1 if age_val < 45 else 0

    nll = 0.0
    eps = 1e-12
    nA = 3

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx]
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_set_sizes = set_sizes[idx]
        nS = int(block_set_sizes[0])

        Q = np.zeros((nS, nA))
        # Visit counts for uncertainty
        N = np.zeros((nS, nA))

        # Effective uncertainty bonus weight for this block
        load_term = size_penalty * max(0, nS - 3)
        w_bonus = bonus_weight + age_explore_shift * younger - load_term

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Uncertainty estimate: higher when counts are low
            U = 1.0 / (prior_strength + N[s, :] + 1.0)  # simple inverse-visit bonus

            # Augmented preferences
            prefs = Q[s, :] + w_bonus * U
            prefs -= np.max(prefs)
            exp_p = np.exp(beta * prefs)
            pi = exp_p / (np.sum(exp_p) + eps)

            p = max(pi[a], eps)
            nll -= np.log(p)

            # RL update
            delta = r - Q[s, a]
            Q[s, a] += alpha * delta

            # Update counts after observing the action
            N[s, a] += 1.0

    return nll


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Hybrid RL + Win-Stay/Lose-Shift (WSLS) with age- and load-dependent arbitration and lapse.

    The policy mixes a standard RL softmax with a WSLS heuristic applied within state.
    The arbitration weight on WSLS increases for younger participants and decreases
    with set size. A small lapse ensures robustness.

    Parameters
    ----------
    states : array-like of int
        State indices on each trial (0..nS-1 within block).
    actions : array-like of int
        Chosen actions on each trial (0..nA-1).
    rewards : array-like of int
        Binary rewards (0 or 1) observed each trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size (3 or 6) for the block of each trial.
    age : array-like of float
        Participant age; age[0] is used to define age group (younger <45, older >=45).
    model_parameters : tuple/list of floats
        (alpha, beta, wsls_base, wsls_age_shift, wsls_size_shift, epsilon)
        - alpha: RL learning rate (squashed 0..1)
        - beta: RL inverse temperature (scaled internally)
        - wsls_base: baseline logit for WSLS weight in arbitration
        - wsls_age_shift: additive logit shift for younger group (older=0)
        - wsls_size_shift: reduction in WSLS logit per unit increase in set size
        - epsilon: lapse probability mixed with uniform (squashed to 0..0.3)

    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    alpha, beta, wsls_base, wsls_age_shift, wsls_size_shift, epsilon = model_parameters

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    alpha = sigmoid(alpha)
    beta = sigmoid(beta) * 12.0
    epsilon = sigmoid(epsilon) * 0.3  # cap lapse

    age_val = float(age[0])
    younger = 1 if age_val < 45 else 0

    nll = 0.0
    eps = 1e-12
    nA = 3

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx]
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_set_sizes = set_sizes[idx]
        nS = int(block_set_sizes[0])

        Q = np.zeros((nS, nA))
        last_action = -np.ones(nS, dtype=int)
        last_reward = np.zeros(nS)

        # Arbitration weight toward WSLS for this block
        size_term = wsls_size_shift * max(0, nS - 3)
        wsls_logit = wsls_base + wsls_age_shift * younger - size_term
        w_wsls = sigmoid(wsls_logit)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # RL policy
            prefs = Q[s, :].copy()
            prefs -= np.max(prefs)
            exp_p = np.exp(beta * prefs)
            pi_rl = exp_p / (np.sum(exp_p) + eps)

            # WSLS policy within state
            if last_action[s] == -1:
                pi_wsls = np.ones(nA) / nA
            else:
                pi_wsls = np.zeros(nA)
                if last_reward[s] >= 0.5:
                    # Win: repeat last action with prob 1 (deterministic WS)
                    pi_wsls[last_action[s]] = 1.0
                else:
                    # Lose: choose uniformly among the other actions (LS)
                    others = [aa for aa in range(nA) if aa != last_action[s]]
                    for aa in others:
                        pi_wsls[aa] = 1.0 / (nA - 1)

            # Mix WSLS and RL, then add lapse
            pi = w_wsls * pi_wsls + (1.0 - w_wsls) * pi_rl
            pi = (1.0 - epsilon) * pi + epsilon * (1.0 / nA)

            p = max(pi[a], eps)
            nll -= np.log(p)

            # RL update
            delta = r - Q[s, a]
            Q[s, a] += alpha * delta

            # Update WSLS memory
            last_action[s] = a
            last_reward[s] = r

    return nll