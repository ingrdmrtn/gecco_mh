def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited Working Memory (WM) mixture with age- and set-size-dependent WM effectiveness,
    WM decay, and a lapse (epsilon) component.

    Mechanism:
    - Model-free RL maintains Q-values per state-action and learns from reward prediction error.
    - A simple WM store keeps the last rewarded association per state with strength that decays each trial.
      When reward is received, WM for that state is reset to a "one-hot" map favoring the rewarded action.
    - Arbitration: choice probability is a mixture of WM policy and RL policy.
      The WM weight is reduced by larger set sizes (interference) and is higher in younger adults.
      WM traces decay faster with larger set sizes and for older adults.
    - Lapse: with small probability, choices are uniform random.

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..set_size-1).
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of {0,1}
        Outcome per trial.
    blocks : array-like of int
        Block index per trial; learning resets per block.
    set_sizes : array-like of int
        Set size per trial (constant within a block; either 3 or 6).
    age : array-like or scalar
        Participant age; younger <45, older >=45 (used to modulate WM capacity/decay).
    model_parameters : iterable of 5 floats
        alpha : RL learning rate (0..1).
        beta : RL softmax inverse temperature (>0).
        wm_capacity : baseline WM contribution weight at set size 3 for a young adult (0..1).
        wm_decay : per-trial WM decay factor (0..1).
        lapse : lapse probability (0..0.2 typically).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta, wm_capacity, wm_decay, lapse = model_parameters
    age_val = age[0] if hasattr(age, "__len__") else age
    is_young = 1.0 if age_val < 45 else 0.0
    is_old = 1.0 - is_young

    nA = 3
    total_logp = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        b_states = states[mask]
        b_actions = actions[mask]
        b_rewards = rewards[mask]
        b_set_sizes = set_sizes[mask]

        nS = int(b_set_sizes[0])

        # Initialize RL Q-values and WM strengths
        Q = (1.0 / nA) * np.ones((nS, nA))
        WM = np.zeros((nS, nA))  # 0 means no memory; on reward we write a one-hot

        # Effective WM decay increases with set size and is worse for older adults
        wm_decay_eff = wm_decay * (1.0 + 0.5 * max(0, nS - 3))
        wm_decay_eff = np.clip(wm_decay_eff * (0.8 * is_young + 1.2 * is_old), 0.0, 1.0)

        # Mixture weight: reduced by set size, boosted in youth
        base_w = np.clip(wm_capacity, 0.0, 1.0)
        base_w *= (1.2 * is_young + 0.8 * is_old)
        mix_w = base_w / (1.0 + 0.5 * max(0, nS - 3))
        mix_w = np.clip(mix_w, 0.0, 1.0)

        # WM policy sharpness (increases with wm_capacity)
        beta_wm = 5.0 + 10.0 * np.clip(wm_capacity, 0.0, 1.0)

        for t in range(len(b_states)):
            s = int(b_states[t])
            a = int(b_actions[t])
            r = float(b_rewards[t])

            # Decay WM globally each trial toward zero (no memory)
            WM *= (1.0 - wm_decay_eff)

            # Compute RL policy
            q_s = Q[s, :]
            pref_rl = beta * (q_s - np.max(q_s))
            p_rl_vec = np.exp(pref_rl)
            p_rl_vec /= np.sum(p_rl_vec)

            # Compute WM policy (softmax over WM strengths)
            wm_s = WM[s, :]
            pref_wm = beta_wm * (wm_s - np.max(wm_s))
            p_wm_vec = np.exp(pref_wm)
            p_wm_vec_sum = np.sum(p_wm_vec)
            if p_wm_vec_sum <= 0.0 or not np.isfinite(p_wm_vec_sum):
                p_wm_vec = np.ones(nA) / nA
            else:
                p_wm_vec /= p_wm_vec_sum

            # Mixture with lapse
            p_choice = (1.0 - lapse) * (mix_w * p_wm_vec[a] + (1.0 - mix_w) * p_rl_vec[a]) + lapse * (1.0 / nA)
            p_choice = max(p_choice, 1e-12)
            total_logp += np.log(p_choice)

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # WM update: on reward, store a one-hot association for that state
            if r > 0.5:
                WM[s, :] = 0.0
                WM[s, a] = 1.0

    return -total_logp


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with set-size and age modulated effective learning rate and exploration, plus a reward-contingent
    win-stay bias.

    Mechanism:
    - Model-free RL with a block-constant effective learning rate that increases with set size (more weight
      to recent outcomes under higher load) and is slightly higher for younger adults.
    - Inverse temperature (beta) is reduced as set size increases, with an age-specific exploration factor.
      Younger adults can explore more strongly as load increases (if age_explore > 0).
    - Win-stay bias: add a bonus to the action that was last chosen in that state if that last choice was rewarded.
      The strength of this bonus is a free parameter.

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..set_size-1).
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of {0,1}
        Outcome per trial.
    blocks : array-like of int
        Block index per trial; learning resets per block.
    set_sizes : array-like of int
        Set size per trial (constant within a block).
    age : array-like or scalar
        Participant age; younger <45, older >=45 (used to modulate alpha and beta).
    model_parameters : iterable of 5 floats
        alpha0 : base learning rate (0..1).
        beta_base : base inverse temperature (>0).
        kappa_ss : multiplicative slope for learning rate vs set size (>=0).
        age_explore : exploration modulation with age (>=0 increases exploration more in youth).
        ws_bonus : win-stay preference bonus added to the last rewarded action in a state (>=0).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha0, beta_base, kappa_ss, age_explore, ws_bonus = model_parameters
    age_val = age[0] if hasattr(age, "__len__") else age
    is_young = 1.0 if age_val < 45 else 0.0
    is_old = 1.0 - is_young

    nA = 3
    total_logp = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        b_states = states[mask]
        b_actions = actions[mask]
        b_rewards = rewards[mask]
        b_set_sizes = set_sizes[mask]
        nS = int(b_set_sizes[0])

        # Effective parameters for the whole block
        alpha_eff = alpha0 * (1.0 + kappa_ss * max(0, nS - 3))
        alpha_eff *= (1.2 * is_young + 0.9 * is_old)
        alpha_eff = np.clip(alpha_eff, 0.0, 1.0)

        # Exploration increases with set size; age_explore modulates this more for youth
        explore_scale = 1.0 + age_explore * (1.2 * is_young + 0.8 * is_old) * (float(nS) / 3.0 - 1.0)
        beta_eff = beta_base / max(1e-6, explore_scale)

        Q = (1.0 / nA) * np.ones((nS, nA))
        last_action = -1 * np.ones(nS, dtype=int)
        last_reward = -1 * np.ones(nS, dtype=int)  # -1 unknown, 0/1 as observed

        for t in range(len(b_states)):
            s = int(b_states[t])
            a = int(b_actions[t])
            r = int(b_rewards[t])

            # Preferences from Q
            pref = Q[s, :].copy()

            # Win-stay bonus if last choice in this state was rewarded
            if last_action[s] >= 0 and last_reward[s] == 1:
                pref[last_action[s]] += ws_bonus

            # Softmax
            z = beta_eff * (pref - np.max(pref))
            exp_z = np.exp(z)
            p_vec = exp_z / np.sum(exp_z)
            p_choice = max(p_vec[a], 1e-12)
            total_logp += np.log(p_choice)

            # RL update
            pe = float(r) - Q[s, a]
            Q[s, a] += alpha_eff * pe

            # Book-keeping for win-stay
            last_action[s] = a
            last_reward[s] = r

    return -total_logp


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Dual-system arbitration: Model-based (Bayesian associative map) + Model-free RL, with arbitration
    driven by posterior entropy, set size, and age.

    Mechanism:
    - Model-based (MB) system maintains Dirichlet counts over actions for each state. Positive feedback
      increases the chosen action's count; negative feedback distributes evidence to the alternative actions.
      Young adults integrate negative feedback more strongly if age_bias > 0 (and vice versa for older).
    - Model-free (MF) RL learns Q-values via prediction errors.
    - Arbitration: the weight on MB policy is a logistic function of a base weight, reduced by posterior
      entropy and by larger set sizes. Age biases the arbitration toward MB (young) or MF (old).
    - Action probabilities are a mixture of MB posterior and MF softmax.

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..set_size-1).
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of {0,1}
        Outcome per trial.
    blocks : array-like of int
        Block index per trial; learning resets per block.
    set_sizes : array-like of int
        Set size per trial (constant within a block).
    age : array-like or scalar
        Participant age; younger <45, older >=45 (affects MB evidence use and arbitration).
    model_parameters : iterable of 5 floats
        alpha : MF learning rate (0..1).
        beta : MF softmax inverse temperature (>0).
        mb_weight_base : baseline arbitration bias toward MB (logit scale suggested, but used via logistic here).
        entropy_slope : strength of entropy-based reduction in MB weight (>=0).
        age_bias : scales age effect on both MB evidence use (neg-feedback) and arbitration (>=0).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta, mb_weight_base, entropy_slope, age_bias = model_parameters
    age_val = age[0] if hasattr(age, "__len__") else age
    is_young = 1.0 if age_val < 45 else 0.0
    is_old = 1.0 - is_young

    nA = 3
    total_logp = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        b_states = states[mask]
        b_actions = actions[mask]
        b_rewards = rewards[mask]
        b_set_sizes = set_sizes[mask]
        nS = int(b_set_sizes[0])

        # Initialize MF and MB
        Q = (1.0 / nA) * np.ones((nS, nA))
        counts = np.ones((nS, nA))  # Dirichlet(1,1,1) prior

        for t in range(len(b_states)):
            s = int(b_states[t])
            a = int(b_actions[t])
            r = int(b_rewards[t])

            # Model-based posterior over actions
            mb_post = counts[s, :] / np.sum(counts[s, :])

            # Entropy of MB posterior (normalized)
            with np.errstate(divide='ignore', invalid='ignore'):
                log_mb = np.log(np.clip(mb_post, 1e-12, 1.0))
            H = -np.sum(mb_post * log_mb)
            H_norm = H / np.log(nA)

            # Age term biases arbitration toward MB for younger (if age_bias>0)
            age_term = age_bias * (is_young - is_old)

            # Set-size penalty to MB arbitration (more items -> lower MB reliability)
            ss_term = -0.5 * max(0, nS - 3)

            # Logistic arbitration weight on MB
            x = mb_weight_base + age_term + ss_term - entropy_slope * H_norm
            w_mb = 1.0 / (1.0 + np.exp(-x))
            w_mb = np.clip(w_mb, 0.0, 1.0)

            # MF softmax policy
            q_s = Q[s, :]
            z = beta * (q_s - np.max(q_s))
            exp_z = np.exp(z)
            p_mf = exp_z / np.sum(exp_z)

            # Mixture policy
            p = w_mb * mb_post + (1.0 - w_mb) * p_mf
            p_choice = max(p[a], 1e-12)
            total_logp += np.log(p_choice)

            # Updates
            # MF update
            pe = float(r) - Q[s, a]
            Q[s, a] += alpha * pe

            # MB counts update:
            if r == 1:
                # Positive evidence for chosen action; slightly stronger in the group favored by age_bias
                pos_scale = 1.0 + 0.1 * age_bias * (is_young - is_old)
                counts[s, a] += pos_scale
            else:
                # Negative evidence: distribute to alternatives; age_bias scales how strongly neg evidence is used
                neg_scale = 0.5 * (1.0 + 0.2 * age_bias * (is_young - is_old))
                for aa in range(nA):
                    if aa != a:
                        counts[s, aa] += neg_scale

    return -total_logp