def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + Uncertainty-Bonus (UCB) with an age- and load-limited episodic cache.

    The model combines:
    - Model-free RL (Q-learning) with an uncertainty-directed exploration bonus (UCB-style).
    - An episodic cache that stores the most recently rewarded action per state, but is capacity-limited.
      The cache contribution to choice is weighted by how many items fit into capacity; capacity is reduced for
      older adults and by larger set sizes.

    Parameters
    ----------
    states : 1D array-like of int
        State index on each trial within its block.
    actions : 1D array-like of int
        Observed chosen action on each trial (0..2).
    rewards : 1D array-like of {0,1}
        Feedback received after each choice.
    blocks : 1D array-like of int
        Block index for each trial.
    set_sizes : 1D array-like of int
        The number of distinct states in the current block (3 or 6).
    age : 1D array-like (length 1)
        Participant age; used to define age group (younger <45, older >=45).
    model_parameters : iterable of length 6
        [alpha, beta, bonus, cache_strength, age_bias, decay]
        - alpha: RL learning rate (0..1).
        - beta: inverse temperature for softmax; internally scaled by 10.
        - bonus: magnitude of directed exploration bonus added as bonus/sqrt(N) to each action.
        - cache_strength: baseline arbitration weight of the episodic cache when capacity is sufficient.
        - age_bias: modifies effective cache capacity for age groups:
                    effective_capacity = base_capacity * (1 + age_bias*(0.5 - age_group)).
                    Positive values boost younger capacity; negative hurt younger, etc.
        - decay: per-trial forgetting of Q toward uniform, also applied to counts (to keep uncertainty alive).
                 Implemented per trial as: Q <- (1-decay)*Q + decay*(1/nA).

    Returns
    -------
    float
        Negative log-likelihood of the observed actions.
    """
    alpha, beta, bonus, cache_strength, age_bias, decay = model_parameters
    beta = beta * 10.0
    nA = 3
    age_val = age[0]
    age_group = 1 if age_val >= 45 else 0  # 1=older, 0=younger

    total_logp = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        b_states = states[idx]
        b_actions = actions[idx]
        b_rewards = rewards[idx]
        b_set_sizes = set_sizes[idx]
        nS = int(b_set_sizes[0])

        # RL values and visitation counts
        Q = np.ones((nS, nA)) / nA
        N = np.ones((nS, nA))  # start with 1 to avoid div by zero; decays over time

        # Episodic cache: -1 = empty; otherwise store action index [0..2]
        cache = -1 * np.ones(nS, dtype=int)

        # Effective episodic capacity (age- and load-scaled)
        base_capacity = 4.0  # nominal capacity units
        cap_age = base_capacity * (1.0 + age_bias * (0.5 - age_group))
        cap_age = max(0.0, cap_age)
        # Weight the cache contribution by how much of the set can fit in capacity
        # Capacity weight depends on set size in each block.
        capacity_weight = cache_strength * min(1.0, cap_age / max(1, nS))

        for t in range(len(b_states)):
            s = int(b_states[t])
            a = int(b_actions[t])
            r = float(b_rewards[t])

            # RL with exploration bonus from uncertainty
            inv_sqrtN = 1.0 / np.sqrt(np.maximum(N[s, :], 1e-6))
            ucb = Q[s, :] + bonus * inv_sqrtN

            rl_logits = beta * ucb
            rl_logits = rl_logits - np.max(rl_logits)
            rl_probs = np.exp(rl_logits)
            rl_probs = rl_probs / np.sum(rl_probs)

            # Episodic cache policy: if cached action exists, probability mass on that action
            if cache[s] >= 0:
                cache_probs = np.ones(nA) * (0.0)
                cache_probs[cache[s]] = 1.0
            else:
                cache_probs = np.ones(nA) / nA

            # Arbitration: capacity-limited weight to cache
            w_cache = np.clip(capacity_weight, 0.0, 1.0)
            p = w_cache * cache_probs + (1.0 - w_cache) * rl_probs

            p_a = np.clip(p[a], 1e-12, 1.0)
            total_logp += np.log(p_a)

            # Learning updates
            # RL: delta rule
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe
            # Per-trial decay toward uniform (load-agnostic, already handled by capacity weight)
            if decay > 0.0:
                Q[s, :] = (1.0 - decay) * Q[s, :] + decay * (1.0 / nA)
                N[s, :] = (1.0 - decay) * N[s, :] + decay * 1.0  # avoid going to zero

            # Increment visits
            N[s, a] += 1.0

            # Update episodic cache only on rewarded outcomes
            if r > 0.5:
                cache[s] = a

    return -float(total_logp)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Actor-Critic with age- and load-modulated temperature and a load-increasing lapse policy.

    The model uses:
    - Policy (actor) preferences updated by the advantage (r - V).
    - State value (critic) updated by TD learning.
    - Inverse temperature increases for younger adults and decreases with set size.
    - Lapse rate increases with set size; older adults are more lapse-prone if beta_age < 0 (via lower inverse temp).

    Parameters
    ----------
    states : 1D array-like of int
        State index on each trial within its block.
    actions : 1D array-like of int
        Observed chosen action on each trial (0..2).
    rewards : 1D array-like of {0,1}
        Feedback received after each choice.
    blocks : 1D array-like of int
        Block index for each trial.
    set_sizes : 1D array-like of int
        The number of distinct states in the current block (3 or 6).
    age : 1D array-like (length 1)
        Participant age; used to define age group (younger <45, older >=45).
    model_parameters : iterable of length 6
        [alpha_actor, alpha_critic, beta0, beta_age, lapse0, lapse_size]
        - alpha_actor: learning rate for policy preferences.
        - alpha_critic: learning rate for state value.
        - beta0: baseline inverse temperature; scaled by 10 internally.
        - beta_age: additive age-group modulation of inverse temperature:
                    beta_eff = 10*beta0 + beta_age*(0.5 - age_group).
        - lapse0: baseline lapse probability (0..1).
        - lapse_size: increase in lapse with set size, implemented per block as:
                      lapse = clip(lapse0 + lapse_size * ((nS-3)/3), 0, 0.5).

    Returns
    -------
    float
        Negative log-likelihood of the observed actions.
    """
    alpha_actor, alpha_critic, beta0, beta_age, lapse0, lapse_size = model_parameters
    nA = 3
    age_val = age[0]
    age_group = 1 if age_val >= 45 else 0  # 1=older, 0=younger

    total_logp = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        b_states = states[idx]
        b_actions = actions[idx]
        b_rewards = rewards[idx]
        b_set_sizes = set_sizes[idx]
        nS = int(b_set_sizes[0])

        # Initialize per-block policy preferences and value
        P = np.zeros((nS, nA))  # actor preferences
        V = np.zeros(nS)        # critic value

        # Block-level parameters
        beta_eff = 10.0 * beta0 + beta_age * (0.5 - age_group) - 2.0 * ((nS - 3) / 3.0)
        # keep a lower bound to avoid flat probabilities when negative
        beta_eff = max(0.0, beta_eff)

        lapse = lapse0 + lapse_size * ((nS - 3) / 3.0)
        lapse = np.clip(lapse, 0.0, 0.5)

        for t in range(len(b_states)):
            s = int(b_states[t])
            a = int(b_actions[t])
            r = float(b_rewards[t])

            logits = beta_eff * P[s, :]
            logits = logits - np.max(logits)
            pol = np.exp(logits)
            pol = pol / np.sum(pol)

            final_pol = (1.0 - lapse) * pol + lapse * (1.0 / nA)

            p_a = np.clip(final_pol[a], 1e-12, 1.0)
            total_logp += np.log(p_a)

            # Advantage-based updates
            adv = r - V[s]
            # Actor update: gradient ascent on log-policy with advantage
            onehot = np.zeros(nA)
            onehot[a] = 1.0
            P[s, :] += alpha_actor * adv * (onehot - pol)

            # Critic update
            V[s] += alpha_critic * adv

    return -float(total_logp)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Mixture of RL and rule-like one-to-one mapping memory with interference, age modulation, and action bias.

    The model blends:
    - RL (Q-learning) providing a graded softmax policy.
    - Rule memory: a one-to-one mapping cache per state learned on rewarded trials with probability wm_phi.
      Memory retrieval suffers interference that increases with set size and is worse for older adults.
    - Action bias: a small prior bias toward action = state mod 3 to capture heuristic responding.

    Parameters
    ----------
    states : 1D array-like of int
        State index on each trial within its block.
    actions : 1D array-like of int
        Observed chosen action on each trial (0..2).
    rewards : 1D array-like of {0,1}
        Feedback received after each choice.
    blocks : 1D array-like of int
        Block index for each trial.
    set_sizes : 1D array-like of int
        The number of distinct states in the current block (3 or 6).
    age : 1D array-like (length 1)
        Participant age; used to define age group (younger <45, older >=45).
    model_parameters : iterable of length 6
        [alpha, beta, wm_phi, interference, age_shift, bias_strength]
        - alpha: RL learning rate (0..1).
        - beta: inverse temperature for RL softmax; internally scaled by 10.
        - wm_phi: probability to store/update rule memory on rewarded trials; also used as base retrieval strength.
        - interference: increases memory failure with set size: retrieval_prob = wm_phi * exp(-interference*(nS-3)).
        - age_shift: age penalty (if positive) for older adults on memory retrieval:
                     retrieval_prob *= exp(-age_shift*age_group).
        - bias_strength: transforms the heuristic bias into logits added to both RL and memory policies.

    Returns
    -------
    float
        Negative log-likelihood of the observed actions.
    """
    alpha, beta, wm_phi, interference, age_shift, bias_strength = model_parameters
    beta = beta * 10.0
    nA = 3
    age_val = age[0]
    age_group = 1 if age_val >= 45 else 0  # 1=older, 0=younger

    total_logp = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        b_states = states[idx]
        b_actions = actions[idx]
        b_rewards = rewards[idx]
        b_set_sizes = set_sizes[idx]
        nS = int(b_set_sizes[0])

        # RL values
        Q = np.ones((nS, nA)) / nA

        # Rule memory: stores best action per state (-1 if unknown)
        M = -1 * np.ones(nS, dtype=int)

        # Compute retrieval strength for this block (affected by load and age)
        retrieval_prob = wm_phi * np.exp(-interference * max(0, nS - 3))
        retrieval_prob *= np.exp(-age_shift * age_group)
        retrieval_prob = np.clip(retrieval_prob, 0.0, 1.0)

        for t in range(len(b_states)):
            s = int(b_states[t])
            a = int(b_actions[t])
            r = float(b_rewards[t])

            # Heuristic bias toward action == state mod 3
            bias_logits = np.zeros(nA)
            bias_logits[s % nA] = bias_strength

            # RL policy
            rl_logits = beta * Q[s, :] + bias_logits
            rl_logits = rl_logits - np.max(rl_logits)
            rl_probs = np.exp(rl_logits)
            rl_probs = rl_probs / np.sum(rl_probs)

            # Memory policy: deterministic if retrieved; otherwise uniform; also add bias
            if (M[s] >= 0) and (np.random.rand() < retrieval_prob):
                mem_probs = np.zeros(nA)
                mem_probs[M[s]] = 1.0
            else:
                mem_probs = np.ones(nA) / nA

            # Apply the same bias via softmax-like transform on memory side
            mem_logits = np.log(np.clip(mem_probs, 1e-12, 1.0)) + bias_logits
            mem_logits = mem_logits - np.max(mem_logits)
            mem_probs_biased = np.exp(mem_logits)
            mem_probs_biased = mem_probs_biased / np.sum(mem_probs_biased)

            # Arbitration: retrieval_prob is used as the mixing weight on this trial
            w = retrieval_prob
            p = w * mem_probs_biased + (1.0 - w) * rl_probs

            p_a = np.clip(p[a], 1e-12, 1.0)
            total_logp += np.log(p_a)

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # Memory update on rewarded trials with probability wm_phi
            if (r > 0.5) and (np.random.rand() < wm_phi):
                M[s] = a

    return -float(total_logp)