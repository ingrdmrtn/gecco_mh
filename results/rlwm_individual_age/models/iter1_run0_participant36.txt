def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Model 1: RL with interference-based forgetting and action perseveration, age-modulated exploration.
    
    Mechanism:
      - Model-free Q-learning within blocks.
      - Global interference/forgetting increases with set size (more items -> more decay).
      - Action perseveration (stickiness) biases toward repeating the last action taken in the same state.
      - Older adults explore more (lower effective beta) via an age-modulated exploration parameter.
    
    Parameters
    ----------
    states : array-like, shape (T,)
        State index on each trial within a block (0..nS-1).
    actions : array-like, shape (T,)
        Chosen action on each trial (0..2).
    rewards : array-like, shape (T,)
        Binary reward on each trial (0/1).
    blocks : array-like, shape (T,)
        Block index for each trial. States reset between blocks.
    set_sizes : array-like, shape (T,)
        Set size for the active block on each trial (3 or 6 here).
    age : array-like or scalar
        Participant age; used for age group (older >= 45).
    model_parameters : array-like, length 5
        alpha         : RL learning rate in [0,1]
        beta          : Base softmax inverse temperature (>0), scaled internally
        k_forget      : Base interference/forgetting rate per trial in [0,1]
        stickiness    : Perseveration weight added to the last action in the current state (real, >=0 recommended)
        age_explore   : Age exploration scaling; reduces beta if older (>=0)
                        Effective beta = beta * exp(-age_explore * is_older)
    
    Returns
    -------
    nll : float
        Negative log-likelihood of the observed choices under the model.
    """
    nA = 3
    eps = 1e-12

    alpha, beta, k_forget, stickiness, age_explore = model_parameters
    beta = 5.0 * max(beta, eps)

    age_val = age[0] if hasattr(age, "__len__") and not np.isscalar(age) else age
    is_older = 1.0 if age_val >= 45 else 0.0
    beta_eff_age = beta * np.exp(-max(0.0, age_explore) * is_older)

    total_logp = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx].astype(int)
        block_actions = actions[idx].astype(int)
        block_rewards = rewards[idx].astype(float)
        nS = int(set_sizes[idx][0])

        # Initialize Q-values and last-action memory per state
        Q = np.zeros((nS, nA))
        last_action_state = -1 * np.ones(nS, dtype=int)

        # Interference scaling increases with set size and more for older adults
        # Scale k_forget by nS/6 and an age factor
        inter_scale = (nS / 6.0) * (1.0 + 0.5 * is_older)
        k_decay = np.clip(k_forget * inter_scale, 0.0, 1.0)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Apply global interference/forgetting to all Q-values
            if k_decay > 0:
                Q *= (1.0 - k_decay)

            # Build preference vector: value + stickiness toward last action in this state
            prefs = Q[s, :].copy()
            if last_action_state[s] >= 0 and stickiness != 0:
                prefs[last_action_state[s]] += stickiness

            # Softmax policy with age-modulated beta
            prefs_stable = prefs - np.max(prefs)
            p = np.exp(beta_eff_age * prefs_stable)
            p = p / np.sum(p + eps)
            p = np.clip(p, eps, 1.0)
            p = p / np.sum(p)

            total_logp += np.log(p[a])

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # Update last action memory
            last_action_state[s] = a

    return -float(total_logp)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Model 2: Uncertainty-scaled exploration with lapse and age modulation.
    
    Mechanism:
      - Model-free RL with a dynamic inverse temperature that increases with state visit count.
      - Older adults show reduced sensitivity gain from experience (lower beta scaling with visits).
      - Includes a lapse rate mixing with uniform choice; larger set sizes increase lapse.
    
    Parameters
    ----------
    states : array-like, shape (T,)
        State index on each trial within a block (0..nS-1).
    actions : array-like, shape (T,)
        Chosen action on each trial (0..2).
    rewards : array-like, shape (T,)
        Binary reward on each trial (0/1).
    blocks : array-like, shape (T,)
        Block index for each trial. States reset between blocks.
    set_sizes : array-like, shape (T,)
        Set size for the active block on each trial (3 or 6 here).
    age : array-like or scalar
        Participant age; used for age group (older >= 45).
    model_parameters : array-like, length 5
        alpha          : RL learning rate in [0,1]
        beta_base      : Base inverse temperature (>0), scaled internally
        visit_gain     : Gain scaling for inverse temperature as a function of visit count (>=0)
        age_dampen     : Amount by which visit-based gain is dampened if older (>=0)
                         Effective beta multiplier = 1 + visit_gain * f(visits) * (1 - age_dampen*is_older)
        lapse_base     : Base lapse probability in [0,1]; scaled by set size as lapse = lapse_base*(nS/6)
    
    Returns
    -------
    nll : float
        Negative log-likelihood of the observed choices under the model.
    """
    nA = 3
    eps = 1e-12

    alpha, beta_base, visit_gain, age_dampen, lapse_base = model_parameters
    beta_base = 5.0 * max(beta_base, eps)
    visit_gain = max(0.0, visit_gain)
    age_dampen = max(0.0, age_dampen)
    lapse_base = np.clip(lapse_base, 0.0, 1.0)

    age_val = age[0] if hasattr(age, "__len__") and not np.isscalar(age) else age
    is_older = 1.0 if age_val >= 45 else 0.0

    total_logp = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx].astype(int)
        block_actions = actions[idx].astype(int)
        block_rewards = rewards[idx].astype(float)
        nS = int(set_sizes[idx][0])

        Q = np.zeros((nS, nA))
        visits = np.zeros(nS, dtype=float)

        # Lapse increases with set size
        lapse = np.clip(lapse_base * (nS / 6.0), 0.0, 0.5)  # kept moderate to avoid numerical issues

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Compute visit-based beta multiplier (more visits -> more certainty -> higher beta)
            v = visits[s]
            # Smooth saturating function of visits: v/(v+1) in [0,1)
            certainty = v / (v + 1.0) if v >= 0 else 0.0
            gain_mult = 1.0 + visit_gain * certainty * (1.0 - age_dampen * is_older)

            beta_eff = beta_base * max(gain_mult, eps)

            q_s = Q[s, :] - np.max(Q[s, :])
            prl = np.exp(beta_eff * q_s)
            prl = prl / np.sum(prl + eps)

            # Mixture with lapse (uniform)
            p = (1.0 - lapse) * prl + lapse * (np.ones(nA) / nA)
            p = np.clip(p, eps, 1.0)
            p = p / np.sum(p)

            total_logp += np.log(p[a])

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # Update visits after observing outcome
            visits[s] += 1.0

    return -float(total_logp)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Model 3: RL + gated episodic WM with logistic gating by set size and age, and stochastic WM retention.
    
    Mechanism:
      - Choices arise from a mixture of model-free RL and a one-shot episodic WM for rewarded mappings.
      - WM gating weight is a logistic function of (capacity_theta - set size) with an age shift,
        giving lower WM reliance for larger sets and in older adults.
      - WM retention is probabilistic: stored mappings can decay across time since last seen.
      - When WM has a stored action for the current state, it proposes that action with high precision.
    
    Parameters
    ----------
    states : array-like, shape (T,)
        State index on each trial within a block (0..nS-1).
    actions : array-like, shape (T,)
        Chosen action on each trial (0..2).
    rewards : array-like, shape (T,)
        Binary reward on each trial (0/1).
    blocks : array-like, shape (T,)
        Block index for each trial. States reset between blocks.
    set_sizes : array-like, shape (T,)
        Set size for the active block on each trial (3 or 6 here).
    age : array-like or scalar
        Participant age; used for age group (older >= 45).
    model_parameters : array-like, length 6
        alpha          : RL learning rate in [0,1]
        beta           : RL inverse temperature (>0), scaled internally
        capacity_theta : WM gating threshold (in set-size units). Higher -> more WM use.
        age_shift      : Additive shift applied to gating if older (>=0 reduces WM gating when positive)
        decay_prob     : Probability of WM forgetting per trial since last seen in [0,1]
        wm_beta        : WM precision when a memory exists (>0), scaled internally
    
    Returns
    -------
    nll : float
        Negative log-likelihood of the observed choices under the model.
    """
    nA = 3
    eps = 1e-12

    alpha, beta, capacity_theta, age_shift, decay_prob, wm_beta = model_parameters
    beta = 5.0 * max(beta, eps)
    wm_beta = 10.0 * max(wm_beta, eps)
    decay_prob = np.clip(decay_prob, 0.0, 1.0)

    age_val = age[0] if hasattr(age, "__len__") and not np.isscalar(age) else age
    is_older = 1.0 if age_val >= 45 else 0.0

    total_logp = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx].astype(int)
        block_actions = actions[idx].astype(int)
        block_rewards = rewards[idx].astype(float)
        nS = int(set_sizes[idx][0])

        Q = np.zeros((nS, nA))

        # WM store: for each state, either has an action memory or not; we keep a confidence vector
        WM = np.zeros((nS, nA))
        last_seen = -np.ones(nS, dtype=int)

        # Compute block-specific WM gating weight using logistic transform
        # gate_raw = capacity_theta - nS - age_shift*is_older
        gate_raw = (capacity_theta - nS) - (abs(age_shift) * is_older)
        w_wm = 1.0 / (1.0 + np.exp(-gate_raw))  # in (0,1)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Apply WM decay based on time since last seen for current state
            if last_seen[s] >= 0:
                dt = t - last_seen[s]
                if dt > 0 and decay_prob > 0:
                    # Each step, memory decays multiplicatively by (1 - decay_prob)
                    WM[s, :] *= (1.0 - decay_prob) ** dt
            last_seen[s] = t

            # RL policy
            q_s = Q[s, :] - np.max(Q[s, :])
            p_rl = np.exp(beta * q_s)
            p_rl = p_rl / np.sum(p_rl + eps)

            # WM policy: if memory is present, it is a peaked distribution toward the stored action
            w_s = WM[s, :]
            if np.sum(w_s) > eps:
                wm_pref = w_s - np.max(w_s)
                p_wm = np.exp(wm_beta * wm_pref)
                p_wm = p_wm / np.sum(p_wm + eps)
            else:
                # No memory -> uniform WM proposal
                p_wm = np.ones(nA) / nA

            # Mixture
            p = w_wm * p_wm + (1.0 - w_wm) * p_rl
            p = np.clip(p, eps, 1.0)
            p = p / np.sum(p)

            total_logp += np.log(p[a])

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # WM update: store or strengthen only on rewarded trials; otherwise slight decay
            if r >= 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                # Move WM toward a one-hot memory for the rewarded action
                WM[s, :] = 0.5 * WM[s, :] + 0.5 * target
                # Boost the stored action to ensure it stands out
                WM[s, a] = max(WM[s, a], 1.0)
            else:
                # Mild decay on unrewarded outcome (already decayed via time)
                WM[s, :] *= (1.0 - 0.25 * decay_prob)

    return -float(total_logp)