def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with eligibility traces and WM lapse; age reduces WM contribution and increases lapse under load.

    Idea:
    - RL uses a per-state eligibility trace (lambda) so that recent choices in a state get larger updates.
    - WM stores rewarded S-A associations; retrieval is noisy (wm_noise) and is down-weighted by set size and age.
    - Arbitration weight (eta) decreases with larger set sizes and for older adults.

    Parameters (tuple): (lr, wm_weight, softmax_beta, lam, wm_noise)
        lr: RL learning rate (0..1).
        wm_weight: Base WM arbitration weight (0..1).
        softmax_beta: RL inverse temperature; internally scaled by 10.
        lam: Eligibility trace decay (0..1). Higher => more credit to recent actions within a state.
        wm_noise: WM lapse/noise mixing with uniform (0..1). Higher => less precise WM policy.

    Returns:
        Negative log-likelihood of the observed choices.
    Age use:
        - Older (>=45): down-weight WM arbitration (age_factor=0.85) and stronger set-size reduction of WM.
    """
    lr, wm_weight, softmax_beta, lam, wm_noise = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    age_val = age[0]
    age_factor = 0.85 if age_val >= 45 else 1.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        # per-state eligibility traces
        E = np.zeros((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]
            nS_curr = int(block_set_sizes[t])

            Q_s = q[s, :]
            # RL policy: probability of chosen action
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy (noisy retrieval)
            W_s = w[s, :]
            denom_wm_clean = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm_clean = 1.0 / max(denom_wm_clean, 1e-12)
            p_wm = (1.0 - wm_noise) * p_wm_clean + wm_noise * (1.0 / nA)

            # Arbitration: set-size and age reduce WM contribution
            size_term = 3.0 / float(nS_curr)  # =1 for set size 3, =0.5 for 6
            eta = wm_weight * size_term * age_factor
            eta = min(max(eta, 0.0), 1.0)

            p_total = eta * p_wm + (1.0 - eta) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update with eligibility trace within the current state
            delta = r - q[s, a]
            # decay eligibilities for this state and set chosen action's trace
            E[s, :] *= lam
            E[s, a] += 1.0
            q[s, :] += lr * delta * E[s, :]

            # WM updating: rewarded associations are stored; otherwise slight decay
            if r > 0:
                # move WM row toward a one-hot on chosen action; incorporate wm_noise in representation
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                one_hot = (1.0 - wm_noise) * one_hot + wm_noise * (1.0 / nA)
                w[s, :] = one_hot
            else:
                # decay toward uniform baseline when not rewarded
                w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with entropy-based arbitration; age reduces WM engagement and boosts decay.

    Idea:
    - Arbitration weight is higher when WM policy is confident (low entropy) relative to RL policy.
    - Set-size reduces WM weight; older adults have lower WM weight and faster WM decay.
    - RL uses standard delta rule; WM decays to baseline each trial and is reinforced by rewards.

    Parameters (tuple): (lr, wm_weight, softmax_beta, entropy_temp, wm_decay)
        lr: RL learning rate (0..1).
        wm_weight: Base arbitration weight for WM (0..1).
        softmax_beta: RL inverse temperature (scaled by 10 internally).
        entropy_temp: Sensitivity of arbitration to entropy difference (can be negative/positive).
        wm_decay: WM decay rate toward uniform baseline (0..1).

    Returns:
        Negative log-likelihood of the observed choices.
    Age use:
        - Older (>=45): WM arbitration attenuated (0.8x) and WM decay increased (+0.1 capped at 1).
    """
    lr, wm_weight, softmax_beta, entropy_temp, wm_decay = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    age_val = age[0]
    age_wm_scale = 0.8 if age_val >= 45 else 1.0
    wm_decay_eff = min(1.0, wm_decay + (0.1 if age_val >= 45 else 0.0))

    def entropy(p):
        p = np.clip(p, 1e-12, 1.0)
        return -np.sum(p * np.log(p))

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]
            nS_curr = int(block_set_sizes[t])

            Q_s = q[s, :]
            # RL full softmax distribution and chosen probability
            rl_logits = softmax_beta * Q_s
            rl_logit_shift = rl_logits - np.max(rl_logits)
            rl_probs = np.exp(rl_logit_shift) / np.sum(np.exp(rl_logit_shift))
            p_rl = rl_probs[a]

            W_s = w[s, :]
            wm_logits = softmax_beta_wm * W_s
            wm_logit_shift = wm_logits - np.max(wm_logits)
            wm_probs = np.exp(wm_logit_shift) / np.sum(np.exp(wm_logit_shift))
            p_wm = wm_probs[a]

            # Entropy-based arbitration, reduced by set size and age
            H_rl = entropy(rl_probs)
            H_wm = entropy(wm_probs)
            # higher weight when WM is more certain than RL
            arb_drive = wm_weight + entropy_temp * (H_rl - H_wm)
            eta = 1.0 / (1.0 + np.exp(-arb_drive))  # sigmoid in [0,1]
            eta *= (3.0 / float(nS_curr))           # reduce under higher load
            eta *= age_wm_scale
            eta = min(max(eta, 0.0), 1.0)

            p_total = eta * p_wm + (1.0 - eta) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM decay toward baseline each trial
            w[s, :] = (1.0 - wm_decay_eff) * w[s, :] + wm_decay_eff * w_0[s, :]
            # Reward-driven Hebbian increment then renormalize
            if r > 0:
                w[s, a] += (1.0 - wm_decay_eff)  # strengthen chosen action representation
                # renormalize row to form a probability vector
                row_sum = np.sum(w[s, :])
                if row_sum > 0:
                    w[s, :] /= row_sum
            else:
                # mild suppression of the chosen action on errors
                w[s, a] *= (1.0 - 0.1)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Capacity-limited WM with LRU selection + RL forgetting; age reduces WM capacity.

    Idea:
    - WM can maintain only C effective states per block (LRU policy). If the current state is in WM,
      WM policy is near-deterministic; otherwise WM contributes little.
    - RL has a forgetting term that drifts Q-values toward uniform upon each visit (capturing load/aging).
    - Arbitration depends on whether the state is in WM; also reduced by set size and age.

    Parameters (tuple): (lr, wm_weight, softmax_beta, C, forget_rl, wm_refresh)
        lr: RL learning rate (0..1).
        wm_weight: Base WM weight when state is in WM (0..1).
        softmax_beta: RL inverse temperature (scaled by 10 internally).
        C: WM capacity in number of states (can be fractional; effective capacity is scaled by age).
        forget_rl: RL forgetting rate toward uniform on each visit (0..1).
        wm_refresh: WM decay toward baseline on each visit (0..1).

    Returns:
        Negative log-likelihood of the observed choices.
    Age use:
        - Older (>=45): Effective capacity reduced (x0.8), lowering the number of states WM can hold.
    """
    lr, wm_weight, softmax_beta, C, forget_rl, wm_refresh = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    age_val = age[0]
    cap_scale = 0.8 if age_val >= 45 else 1.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # LRU bookkeeping: last-visit timestamp per state
        last_visit = -1 * np.ones(nS, dtype=int)
        # effective capacity (at least 1, at most nS)
        K_eff = int(np.clip(np.round(C * cap_scale), 1, nS))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]
            nS_curr = int(block_set_sizes[t])

            # Determine current WM set via top-K most recently visited states
            last_visit[s] = t  # update recency for current state before computing set
            # compute current WM membership
            # states sorted by recency (descending); take top K_eff
            order = np.argsort(-last_visit)
            wm_states = set(order[:K_eff])
            in_wm = (s in wm_states)

            Q_s = q[s, :]
            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Arbitration: strong WM weight if state is in WM, otherwise diminished
            base_eta = wm_weight if in_wm else 0.2 * wm_weight
            # reduce with set size (load)
            base_eta *= (3.0 / float(nS_curr))
            base_eta = min(max(base_eta, 0.0), 1.0)

            p_total = base_eta * p_wm + (1.0 - base_eta) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update with forgetting toward uniform on each visit
            q[s, :] = (1.0 - forget_rl) * q[s, :] + forget_rl * (1.0 / nA)
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM decay/refresh and storage if rewarded
            # decay toward baseline each visit
            w[s, :] = (1.0 - wm_refresh) * w[s, :] + wm_refresh * w_0[s, :]
            # If rewarded, store near one-hot for this state (helps only if in WM set)
            if r > 0:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p