def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited, decaying working memory with age-modulated capacity and lapse.

    Idea:
      - Two systems: model-free RL and a capacity-limited WM cache that stores the last rewarded action per state.
      - WM item strength decays across trials; if an item is strong and within capacity, it guides choice.
      - Effective WM capacity decreases with larger set size and in older adults.
      - Final policy mixes WM-directed choice and RL softmax, with a small lapse rate.

    Parameters (model_parameters):
      - alpha: float in (0,1), RL learning rate
      - beta: float > 0, base softmax inverse temperature (rescaled internally by 10)
      - K_cap: float >= 0, nominal WM capacity (in number of items; interacts with set size)
      - phi_decay: float in [0,1), per-trial WM decay rate within a block (higher = faster forgetting)
      - lapse: float in [0,0.2], stimulus-independent lapse probability; uniform choice on lapse
      - age_wm_shift: float in [0,1], increases effective set-size load for older adults (reducing capacity utility)

    Inputs:
      - states, actions, rewards, blocks, set_sizes: 1D integer arrays aligned by trial
      - age: array-like with a single number; age >= 45 => older group
      - model_parameters: sequence of six floats as described

    Returns:
      - Negative log-likelihood of observed choices under the model (float)
    """
    alpha, beta, K_cap, phi_decay, lapse, age_wm_shift = model_parameters
    beta_eff = max(1e-6, beta * 10.0)
    age_val = age[0]
    is_older = 1.0 if age_val >= 45 else 0.0

    nA = 3
    neg_loglik = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)
        nS = int(block_set_sizes[0])

        # RL Q-values
        Q = np.zeros((nS, nA))

        # WM cache: last rewarded action and its strength (0..1) per state
        wm_action = -1 * np.ones(nS, dtype=int)  # -1 means empty
        wm_strength = np.zeros(nS, dtype=float)

        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]
            nS_now = float(block_set_sizes[t])

            # Decay WM strengths globally each trial within block
            wm_strength = wm_strength * (1.0 - np.clip(phi_decay, 0.0, 0.999))

            # Effective WM availability for this state:
            # Base capacity utility ~ K / setsize, reduced by age_wm_shift for older
            denom_age = nS_now * (1.0 + is_older * np.clip(age_wm_shift, 0.0, 2.0))
            cap_util = K_cap / max(1.0, denom_age)
            cap_util = np.clip(cap_util, 0.0, 1.0)

            # State-specific availability includes memory strength
            p_wm_use = cap_util * np.clip(wm_strength[s], 0.0, 1.0)

            # RL softmax probabilities
            q_s = Q[s, :].copy()
            q_s -= np.max(q_s)
            p_rl = np.exp(beta_eff * q_s)
            p_rl /= np.sum(p_rl)

            # WM policy: if we have a stored action, choose it deterministically
            if wm_action[s] >= 0:
                p_wm = np.zeros(nA)
                p_wm[wm_action[s]] = 1.0
            else:
                p_wm = np.ones(nA) / nA

            # Mixture with lapse
            mix = (1.0 - lapse) * (p_wm_use * p_wm + (1.0 - p_wm_use) * p_rl) + lapse * (np.ones(nA) / nA)

            pa = max(1e-12, mix[a])
            neg_loglik -= np.log(pa)

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # WM update:
            # - If rewarded, store/refresh this action with full strength.
            # - If not rewarded, do not overwrite a stored correct item; allow decay to remove it.
            if r > 0.5:
                wm_action[s] = a
                wm_strength[s] = 1.0

    return float(neg_loglik)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Q-learning with UCB-like uncertainty bonus, age- and set-sizeâ€“dependent forgetting, and lapse.

    Idea:
      - Model-free Q-learning with action-specific visitation counts to quantify uncertainty.
      - Add an exploration bonus (UCB) that is larger for infrequently tried actions, and scales up with set size.
      - Older adults show stronger exploration bonus and stronger value forgetting.
      - Softmax with inverse temperature scaled by set size; small lapse.

    Parameters (model_parameters):
      - alpha: float in (0,1), learning rate
      - beta: float > 0, base inverse temperature (rescaled by 10 internally)
      - eta_bonus: float >= 0, magnitude of UCB exploration bonus
      - rho_forget: float in [0,1), per-trial forgetting rate on Q-values within a block
      - lapse: float in [0,0.2], stimulus-independent lapse probability
      - age_explore: float in [0,1], scales both exploration bonus and forgetting for older adults

    Inputs:
      - states, actions, rewards, blocks, set_sizes: 1D arrays of equal length
      - age: array-like with a single number; age >= 45 => older group
      - model_parameters: six floats as above

    Returns:
      - Negative log-likelihood of observed choices (float)
    """
    alpha, beta, eta_bonus, rho_forget, lapse, age_explore = model_parameters
    beta_base = max(1e-6, beta * 10.0)
    age_val = age[0]
    is_older = 1.0 if age_val >= 45 else 0.0

    nA = 3
    neg_loglik = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)
        nS = int(block_set_sizes[0])

        Q = np.zeros((nS, nA))
        N = np.zeros((nS, nA))  # visitation counts per state-action

        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]
            nS_now = float(block_set_sizes[t])

            # Apply forgetting (stronger with larger set size and for older adults)
            forget_scale = (nS_now / 3.0) * (1.0 + is_older * np.clip(age_explore, 0.0, 2.0))
            rho_eff = np.clip(rho_forget * forget_scale, 0.0, 0.95)
            Q *= (1.0 - rho_eff)

            # UCB bonus, larger with set size and in older adults
            bonus_scale = (1.0 + max(0.0, (nS_now - 3.0) / 3.0)) * (1.0 + is_older * np.clip(age_explore, 0.0, 2.0))
            bonus = eta_bonus * bonus_scale / np.sqrt(N[s, :] + 1.0)

            # Softmax temperature decreases with set size uncertainty
            beta_eff = beta_base / (1.0 + max(0.0, (nS_now - 3.0) / 3.0))
            beta_eff = max(1e-6, beta_eff)

            pref = Q[s, :] + bonus
            pref = pref - np.max(pref)
            p_soft = np.exp(beta_eff * pref)
            p_soft = p_soft / np.sum(p_soft)

            p_final = (1.0 - lapse) * p_soft + lapse * (np.ones(nA) / nA)

            pa = max(1e-12, p_final[a])
            neg_loglik -= np.log(pa)

            # Update counts and Q-learning
            N[s, a] += 1.0
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

    return float(neg_loglik)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Bayesian action-belief (Beta-Bernoulli) with state misbinding that increases with set size and age.

    Idea:
      - For each state-action, maintain a Beta(prior, prior) posterior over reward probability.
      - Choice uses softmax over posterior means for the intended state.
      - With some probability (misbinding), the agent consults another state's beliefs by mistake
        (e.g., binding error), more likely at larger set sizes and in older adults.
      - Final choice probability is a mixture of intended-state policy and average over other states' policies.
      - Includes a small lapse probability.

    Parameters (model_parameters):
      - beta: float > 0, inverse temperature for softmax over posterior means (rescaled by 10)
      - prior: float > 0, symmetric Beta prior strength per action
      - lapse: float in [0,0.2], stimulus-independent lapse rate
      - mis0: float, baseline logit of misbinding probability
      - mis_age: float >= 0, additive effect on misbinding logit for older adults
      - mis_set: float >= 0, additive effect on misbinding logit per unit set-size load ((nS-3)/3)

    Inputs:
      - states, actions, rewards, blocks, set_sizes: 1D arrays aligned by trial
      - age: array-like with a single number; age >= 45 => older group
      - model_parameters: six floats as described

    Returns:
      - Negative log-likelihood of observed choices (float)
    """
    beta, prior, lapse, mis0, mis_age, mis_set = model_parameters
    beta_eff = max(1e-6, beta * 10.0)
    prior = max(1e-6, prior)
    age_val = age[0]
    is_older = 1.0 if age_val >= 45 else 0.0

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    nA = 3
    neg_loglik = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)
        nS = int(block_set_sizes[0])

        # Success and failure counts per (state, action)
        succ = np.zeros((nS, nA))
        fail = np.zeros((nS, nA))

        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]
            nS_now = float(block_set_sizes[t])

            # Posterior means for the intended state
            ev_s = (succ[s, :] + prior) / (succ[s, :] + fail[s, :] + 2.0 * prior)

            # Softmax for intended state
            pref_s = ev_s - np.max(ev_s)
            p_s = np.exp(beta_eff * pref_s)
            p_s = p_s / np.sum(p_s)

            # Build misbinding mixture over other states (uniform mixture over all other states if any)
            if nS > 1:
                p_mix_others = np.zeros(nA)
                count_others = 0
                for s2 in range(nS):
                    if s2 == s:
                        continue
                    ev_s2 = (succ[s2, :] + prior) / (succ[s2, :] + fail[s2, :] + 2.0 * prior)
                    pref_s2 = ev_s2 - np.max(ev_s2)
                    ps2 = np.exp(beta_eff * pref_s2)
                    ps2 = ps2 / np.sum(ps2)
                    p_mix_others += ps2
                    count_others += 1
                p_mix_others /= max(1, count_others)
            else:
                p_mix_others = np.ones(nA) / nA

            # Misbinding probability (logit-linear in age and set size)
            set_load = max(0.0, (nS_now - 3.0) / 3.0)
            logit_mis = mis0 + is_older * np.clip(mis_age, 0.0, 5.0) + set_load * np.clip(mis_set, 0.0, 5.0)
            p_mis = np.clip(sigmoid(logit_mis), 0.0, 1.0)

            # Lapse mixture
            p_final = (1.0 - lapse) * ((1.0 - p_mis) * p_s + p_mis * p_mix_others) + lapse * (np.ones(nA) / nA)

            pa = max(1e-12, p_final[a])
            neg_loglik -= np.log(pa)

            # Update Beta posteriors for the chosen action in the intended state
            if r > 0.5:
                succ[s, a] += 1.0
            else:
                fail[s, a] += 1.0

    return float(neg_loglik)