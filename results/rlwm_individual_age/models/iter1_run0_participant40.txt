def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited, decaying working memory (WM) with age- and load-modulated availability.

    The agent blends RL with a simple slot-like WM that stores recently rewarded stimulus-action
    associations. WM has limited capacity and decays over time; both capacity and decay are
    modulated by age and set size (load). Older adults have reduced effective capacity and faster decay.
    Choices are a convex mixture of RL softmax and WM policy, with a global lapse.

    Parameters
    ----------
    states : np.ndarray of int
        State index at each trial (0..nS-1 within a block).
    actions : np.ndarray of int
        Chosen action at each trial (0..2). If action < 0 it is treated as a lapse/missed response.
    rewards : np.ndarray of float
        Feedback at each trial. Values in {0,1}; values < 0 indicate invalid/missing feedback (trial ignored for updating).
    blocks : np.ndarray of int
        Block id per trial.
    set_sizes : np.ndarray of int
        Set size per trial (3 or 6); used to modulate WM availability and decay.
    age : np.ndarray of float
        Array containing participant age (use age[0]); age>=45 considered older group.
    model_parameters : sequence of float
        [alpha, beta, capacity, wm_decay, epsilon]
        - alpha: RL learning rate.
        - beta: inverse temperature for softmax over Q-values.
        - capacity: baseline WM capacity in number of items (effective capacity is age-modulated).
        - wm_decay: baseline per-trial WM decay rate (0..1) of stored associations.
        - epsilon: lapse probability (uniform choice).

    Returns
    -------
    float
        Negative log-likelihood of the observed actions under the model.
    """
    alpha, beta, capacity, wm_decay, epsilon = model_parameters
    age_val = float(age[0])
    is_older = age_val >= 45.0

    total_logp = 0.0
    nA = 3
    eps = 1e-12

    for b in np.unique(blocks):
        idx = (blocks == b)
        b_states = states[idx]
        b_actions = actions[idx]
        b_rewards = rewards[idx]
        b_set_sizes = set_sizes[idx]
        nS = int(b_set_sizes[0])

        # RL values
        Q = (1.0 / nA) * np.ones((nS, nA))

        # WM store: for each state, store an action and a strength in [0,1].
        wm_action = -1 * np.ones(nS, dtype=int)   # stored action; -1 means empty
        wm_strength = np.zeros(nS)                # confidence/strength of stored association

        # Age effects: reduce capacity, increase decay for older adults
        cap_age_scale = 0.7 if is_older else 1.0
        decay_age_scale = 1.3 if is_older else 1.0
        eff_capacity_base = max(0.0, capacity) * cap_age_scale
        base_decay = np.clip(wm_decay, 0.0, 1.0) * decay_age_scale

        for t in range(len(b_states)):
            s = int(b_states[t])
            a = int(b_actions[t])
            r = float(b_rewards[t])
            ss = int(b_set_sizes[t])

            # Load effects: when set size exceeds capacity, WM reliability drops:
            # compute an availability weight as a smooth function of capacity vs. set size.
            # Here we use eff_capacity relative to current set size.
            eff_capacity = eff_capacity_base
            load_ratio = ss / max(1.0, eff_capacity)  # >1 means overload
            wm_availability = 1.0 / (1.0 + max(0.0, load_ratio - 1.0))  # in (0,1]; lower when overloaded

            # WM decay per trial increases with load
            decay_load_scale = 1.0 + 0.25 * max(0, ss - 3)
            eff_decay = np.clip(base_decay * decay_load_scale, 0.0, 1.0)

            # Construct RL softmax policy
            Qs = Q[s, :]
            Qs_centered = Qs - np.max(Qs)
            p_rl = np.exp(beta * Qs_centered)
            p_rl = p_rl / np.sum(p_rl)

            # Construct WM policy for this state
            if wm_action[s] >= 0 and wm_strength[s] > 0.0:
                pa = np.full(nA, (1.0 - wm_strength[s]) / (nA - 1))
                pa[wm_action[s]] = wm_strength[s]
                p_wm = pa
            else:
                p_wm = np.ones(nA) / nA

            # Combine with availability and lapse
            p_mix = wm_availability * p_wm + (1.0 - wm_availability) * p_rl
            p = (1.0 - epsilon) * p_mix + epsilon * (1.0 / nA)

            if a >= 0 and a < nA:
                pa_taken = float(np.clip(p[a], eps, 1.0))
                total_logp += np.log(pa_taken)
            else:
                # Lapse/invalid response
                total_logp += np.log(1.0 / nA)

            # Value learning and WM maintenance only if valid action
            if a >= 0 and a < nA:
                # RL update
                if r >= 0.0:
                    delta = r - Q[s, a]
                    Q[s, a] += alpha * delta

                # WM decay
                wm_strength[s] = (1.0 - eff_decay) * wm_strength[s]

                # WM encoding: if rewarded, store or reinforce; if not, allow decay to continue
                if r >= 0.0 and r > 0.0:
                    wm_action[s] = a
                    # Boost toward 1 with a simple increment that respects decay-adjusted space
                    wm_strength[s] = wm_strength[s] + (1.0 - wm_strength[s]) * 0.7
                # If unrewarded, do not overwrite with wrong action; rely on decay.

    return -float(total_logp)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with dual learning rates and age/load-modulated forgetting and temperature.

    The agent updates Q-values with separate learning rates for positive and negative outcomes.
    Q-values also forget (decay toward the uniform baseline) at a rate that increases with set size
    and more strongly for older adults. Decision temperature (inverse temperature) is reduced under
    higher load, especially for older adults, reflecting noisier decisions under cognitive load.

    Parameters
    ----------
    states : np.ndarray of int
        State index at each trial (0..nS-1 within a block).
    actions : np.ndarray of int
        Chosen action at each trial (0..2). If action < 0 it is treated as a lapse/missed response.
    rewards : np.ndarray of float
        Feedback at each trial. Values in {0,1}; values < 0 indicate invalid/missing feedback (trial ignored for updating).
    blocks : np.ndarray of int
        Block id per trial.
    set_sizes : np.ndarray of int
        Set size per trial (3 or 6); used to modulate forgetting and choice temperature.
    age : np.ndarray of float
        Array containing participant age (use age[0]); age>=45 considered older group.
    model_parameters : sequence of float
        [alpha_pos, alpha_neg, beta_base, forget, epsilon]
        - alpha_pos: learning rate for positive prediction errors (reward=1).
        - alpha_neg: learning rate for nonreward (reward=0).
        - beta_base: baseline inverse temperature for softmax.
        - forget: baseline forgetting rate (0..1) toward uniform values.
        - epsilon: lapse probability (uniform choice).

    Returns
    -------
    float
        Negative log-likelihood of the observed actions under the model.
    """
    alpha_pos, alpha_neg, beta_base, forget, epsilon = model_parameters
    age_val = float(age[0])
    is_older = age_val >= 45.0

    total_logp = 0.0
    nA = 3
    eps = 1e-12

    for b in np.unique(blocks):
        idx = (blocks == b)
        b_states = states[idx]
        b_actions = actions[idx]
        b_rewards = rewards[idx]
        b_set_sizes = set_sizes[idx]
        nS = int(b_set_sizes[0])

        # Initialize Q-values to uniform
        Q = (1.0 / nA) * np.ones((nS, nA))

        for t in range(len(b_states)):
            s = int(b_states[t])
            a = int(b_actions[t])
            r = float(b_rewards[t])
            ss = int(b_set_sizes[t])

            # Age/load-modulated effective forgetting and temperature
            # Forgetting increases with load and more for older adults
            load_factor = 1.0 + 0.4 * max(0, ss - 3)
            age_forget_scale = 1.4 if is_older else 1.0
            eff_forget = np.clip(forget * load_factor * age_forget_scale, 0.0, 1.0)

            # Temperature (beta) decreases under load, especially for older adults
            age_temp_scale = 1.0 / (1.0 + (0.6 if is_older else 0.3) * max(0, ss - 3))
            beta_eff = max(1e-6, beta_base * age_temp_scale)

            # Apply forgetting toward uniform baseline on the current state
            Q[s, :] = (1.0 - eff_forget) * Q[s, :] + eff_forget * (1.0 / nA)

            # Softmax policy
            Qs = Q[s, :]
            Qs_centered = Qs - np.max(Qs)
            p_soft = np.exp(beta_eff * Qs_centered)
            p_soft = p_soft / np.sum(p_soft)
            p = (1.0 - epsilon) * p_soft + epsilon * (1.0 / nA)

            if a >= 0 and a < nA:
                pa_taken = float(np.clip(p[a], eps, 1.0))
                total_logp += np.log(pa_taken)
            else:
                total_logp += np.log(1.0 / nA)

            # Learning update if valid action and valid reward
            if a >= 0 and a < nA and r >= 0.0:
                pe = r - Q[s, a]
                if r > 0.0:
                    Q[s, a] += alpha_pos * pe
                else:
                    Q[s, a] += alpha_neg * pe

    return -float(total_logp)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Bayesian WM-RL arbitration with age/load-modulated control.

    The agent maintains:
      - RL Q-values (model-free).
      - A Bayesian WM store: for each state-action, Beta(a,b) posterior over success probability,
        updated from observed rewards when that action is chosen. The posterior mean forms a WM policy.
    Arbitration weight between WM and RL depends on WM confidence, penalized by set size and age.
    Older adults place less weight on WM, especially under higher load. Choices include a global lapse.

    Parameters
    ----------
    states : np.ndarray of int
        State index at each trial (0..nS-1 within a block).
    actions : np.ndarray of int
        Chosen action at each trial (0..2). If action < 0 it is treated as a lapse/missed response.
    rewards : np.ndarray of float
        Feedback at each trial. Values in {0,1}; values < 0 indicate invalid/missing feedback (trial ignored for updating).
    blocks : np.ndarray of int
        Block id per trial.
    set_sizes : np.ndarray of int
        Set size per trial (3 or 6); used to modulate arbitration.
    age : np.ndarray of float
        Array containing participant age (use age[0]); age>=45 considered older group.
    model_parameters : sequence of float
        [alpha, beta, arbit_bias, conf_temp, epsilon]
        - alpha: RL learning rate.
        - beta: inverse temperature for softmax over Q-values.
        - arbit_bias: baseline bias toward WM (positive) vs RL (negative) before confidence/load adjustments.
        - conf_temp: sensitivity of arbitration to WM confidence (higher -> more WM when confident).
        - epsilon: lapse probability (uniform choice).

    Returns
    -------
    float
        Negative log-likelihood of the observed actions under the model.
    """
    alpha, beta, arbit_bias, conf_temp, epsilon = model_parameters
    age_val = float(age[0])
    is_older = age_val >= 45.0

    total_logp = 0.0
    nA = 3
    eps = 1e-12

    for b in np.unique(blocks):
        idx = (blocks == b)
        b_states = states[idx]
        b_actions = actions[idx]
        b_rewards = rewards[idx]
        b_set_sizes = set_sizes[idx]
        nS = int(b_set_sizes[0])

        # RL values
        Q = (1.0 / nA) * np.ones((nS, nA))

        # WM Bayesian parameters: Beta(a,b) per (s,a); start with uniform prior Beta(1,1)
        A = np.ones((nS, nA))  # successes + 1
        B = np.ones((nS, nA))  # failures + 1

        for t in range(len(b_states)):
            s = int(b_states[t])
            a = int(b_actions[t])
            r = float(b_rewards[t])
            ss = int(b_set_sizes[t])

            # WM posterior means as a policy proxy
            wm_means = A[s, :] / (A[s, :] + B[s, :])
            if np.allclose(np.sum(wm_means), 0.0):
                p_wm = np.ones(nA) / nA
            else:
                # Normalize to a proper distribution
                p_wm = wm_means / np.sum(wm_means)

            # RL softmax
            Qs = Q[s, :]
            Qs_centered = Qs - np.max(Qs)
            p_rl = np.exp(beta * Qs_centered)
            p_rl = p_rl / np.sum(p_rl)

            # WM confidence: separation between best and average (0..1)
            best = np.max(wm_means)
            conf = best - np.mean(wm_means)

            # Age/load-modulated arbitration
            load_penalty = (0.6 if is_older else 0.3) * max(0, ss - 3)
            logits = arbit_bias + conf_temp * conf - load_penalty
            w_wm = 1.0 / (1.0 + np.exp(-logits))  # sigmoid

            p_mix = w_wm * p_wm + (1.0 - w_wm) * p_rl
            p = (1.0 - epsilon) * p_mix + epsilon * (1.0 / nA)

            if a >= 0 and a < nA:
                pa_taken = float(np.clip(p[a], eps, 1.0))
                total_logp += np.log(pa_taken)
            else:
                total_logp += np.log(1.0 / nA)

            # Updates
            if a >= 0 and a < nA:
                # Update Bayesian WM store only with valid reward
                if r >= 0.0:
                    if r > 0.0:
                        A[s, a] += 1.0
                    else:
                        B[s, a] += 1.0
                # RL update
                if r >= 0.0:
                    delta = r - Q[s, a]
                    Q[s, a] += alpha * delta

    return -float(total_logp)