Below are three standalone cognitive models that implement an RL+WM mixture for this task, with explicit set-size and age effects. Each returns the negative log-likelihood of the observed choices.

Note: Assumes numpy as np is already imported. All parameters are used and limited to a maximum of 6 per model.

----------------------------------------
def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM mixture with age- and set-size-dependent WM weighting and WM decay/overwrite.

    Concept:
    - Choices result from a mixture of RL and WM policies.
    - WM contributes more at small set sizes (3) and less at large set sizes (6).
    - Older adults (age >= 45) have lower effective WM weight and faster decay.
    - WM stores rewarded action as a one-shot overwrite, and decays toward uniform otherwise.

    Parameters (tuple/list):
    - lr: float in (0,1], RL learning rate
    - wm_weight_base: float in [0,1], baseline WM mixture weight
    - softmax_beta: float > 0, RL inverse temperature (will be scaled internally)
    - wm_decay: float in [0,1], per-trial WM decay toward uniform
    - wm_eta: float in [0,1], WM overwrite strength when reward is received
    - age_wm_penalty: float in [0,1], multiplicative penalty applied to WM weight for older adults

    Returns:
    - neg_log_likelihood: float
    """
    lr, wm_weight_base, softmax_beta, wm_decay, wm_eta, age_wm_penalty = model_parameters

    # Scale RL beta higher as in template spirit
    softmax_beta *= 10.0
    # WM softmax: generally more deterministic, but we let age degrade it slightly
    softmax_beta_wm_base = 50.0

    # Determine age group: 1 for older (>=45), 0 for younger
    is_older = 1 if age[0] >= 45 else 0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_idx = (blocks == b)
        block_actions = actions[block_idx]
        block_rewards = rewards[block_idx]
        block_states = states[block_idx]
        block_set_sizes = set_sizes[block_idx]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM value tables
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability of chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy:
            # Age reduces WM determinism slightly; older => lower beta_wm
            beta_wm = softmax_beta_wm_base * (1.0 - 0.3 * is_older)
            beta_wm = max(1e-3, beta_wm)
            p_wm = 1.0 / np.sum(np.exp(beta_wm * (W_s - W_s[a])))

            # Mixture weight: decrease with set size and with older age
            # Set-size scaling: 3/nS ensures higher weight at set size=3 and lower at 6
            set_size_scale = 3.0 / nS
            wm_weight = wm_weight_base * set_size_scale
            if is_older:
                wm_weight *= (1.0 - age_wm_penalty)
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            # Total choice probability
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform each trial (state-specific)
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # WM overwrite when reward is received: store chosen action strongly
            if r > 0.0:
                # shift probability mass toward chosen action by wm_eta
                w[s, :] = (1.0 - wm_eta) * w[s, :]
                w[s, a] += wm_eta

            # Renormalize to avoid drift
            w[s, :] = np.maximum(w[s, :], 1e-8)
            w[s, :] = w[s, :] / np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


----------------------------------------
def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM with capacity-limited WM and perseveration bias, age-adjusted capacity.

    Concept:
    - WM has a capacity limit K: it is effective only for up to K states in a block.
      When set size > K, WM weight is scaled down.
    - Older adults (age >= 45) have an effectively smaller capacity (K reduced).
    - Includes perseveration: bias to repeat the last action in the same state.
    - Choices are a mixture of RL and WM, modulated by capacity scaling and perseveration.

    Parameters:
    - lr: float in (0,1], RL learning rate
    - softmax_beta: float > 0, RL inverse temperature (scaled internally)
    - wm_weight_base: float in [0,1], base WM mixture weight when under capacity
    - K_wm: float >= 1, effective WM capacity (in number of states)
    - persev_weight: float in [0,1], strength of perseveration bias added to mixture
    - age_k_drop: float in [0, K_wm], reduction of K for older adults

    Returns:
    - neg_log_likelihood: float
    """
    lr, softmax_beta, wm_weight_base, K_wm, persev_weight, age_k_drop = model_parameters

    softmax_beta *= 10.0
    beta_wm = 50.0

    is_older = 1 if age[0] >= 45 else 0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_idx = (blocks == b)
        block_actions = actions[block_idx]
        block_rewards = rewards[block_idx]
        block_states = states[block_idx]
        block_set_sizes = set_sizes[block_idx]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track last action per state for perseveration
        last_action = -1 * np.ones(nS, dtype=int)

        # Effective WM capacity for this participant
        K_eff = max(1.0, K_wm - is_older * age_k_drop)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(beta_wm * (W_s - W_s[a])))

            # Capacity scaling: if set size exceeds K_eff, scale down WM weight smoothly
            # Using min(1, K_eff/nS)
            cap_scale = min(1.0, K_eff / float(nS))
            wm_weight = wm_weight_base * cap_scale
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            # Perseveration policy: probability mass to repeating last action in this state
            if last_action[s] == -1:
                p_persev = 1.0 / nA  # no history yet
            else:
                # Probability of chosen action under perseveration bias:
                # assign high mass to last_action, uniform to others
                p_same = 0.95
                p_vec = np.ones(nA) * ((1.0 - p_same) / (nA - 1))
                p_vec[last_action[s]] = p_same
                # Extract prob of the chosen action a
                p_persev = p_vec[a]

            # Combine policies: perseveration adds an additional mixture component
            # Final mixture: wm_weight for WM, (1 - wm_weight - persev_weight) for RL, persev_weight for perseveration
            # Ensure weights sum to 1 and nonnegative
            mix_rl = max(0.0, 1.0 - wm_weight - persev_weight)
            total = wm_weight + persev_weight + mix_rl
            if total <= 0:
                mix_rl = 1.0  # fallback
                wm_weight = 0.0
                persev_weight = 0.0

            p_total = wm_weight * p_wm + persev_weight * p_persev + mix_rl * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: decay toward uniform each visit; store rewarded action strongly
            # We use a simple one-shot store: move half the mass to the chosen action on reward
            w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]
            if r > 0.0:
                w[s, :] *= 0.5
                w[s, a] += 0.5
            # Renormalize
            w[s, :] = np.maximum(w[s, :], 1e-8)
            w[s, :] = w[s, :] / np.sum(w[s, :])

            # Update perseveration memory
            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p


----------------------------------------
def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with asymmetric learning rates + WM mixture + lapse, with age effects on lapse and WM weighting.

    Concept:
    - RL uses separate learning rates for positive and negative prediction errors.
    - WM mixes in more at small set sizes and less at large set sizes.
    - Includes a stimulus-independent lapse probability; older adults have higher lapse and reduced WM weight.
    - WM is stored on reward and decays otherwise.

    Parameters:
    - lr_pos: float in (0,1], RL learning rate for positive PE
    - lr_neg: float in (0,1], RL learning rate for negative PE
    - softmax_beta: float > 0, RL inverse temperature (scaled internally)
    - wm_weight_base: float in [0,1], baseline WM mixture weight at small set size
    - lapse_base: float in [0,0.2], baseline lapse probability
    - age_lapse_increase: float in [0,0.2], added lapse for older adults

    Returns:
    - neg_log_likelihood: float
    """
    lr_pos, lr_neg, softmax_beta, wm_weight_base, lapse_base, age_lapse_increase = model_parameters

    softmax_beta *= 10.0
    beta_wm = 50.0

    is_older = 1 if age[0] >= 45 else 0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_idx = (blocks == b)
        block_actions = actions[block_idx]
        block_rewards = rewards[block_idx]
        block_states = states[block_idx]
        block_set_sizes = set_sizes[block_idx]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(beta_wm * (W_s - W_s[a])))

            # Set-size scaled WM weight; older adults get reduced WM contribution
            set_size_scale = 3.0 / nS
            wm_weight = wm_weight_base * set_size_scale
            if is_older:
                wm_weight *= 0.7  # reduce WM reliance in older adults
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            # Lapse probability; older adults have higher lapse
            lapse = lapse_base + is_older * age_lapse_increase
            lapse = np.clip(lapse, 0.0, 0.4)

            # Mixture without lapse
            p_mix = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            # Lapse policy = uniform over actions => probability of chosen action is 1/nA
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            pe = r - Q_s[a]
            if pe >= 0:
                q[s, a] += lr_pos * pe
            else:
                q[s, a] += lr_neg * pe

            # WM update: mild decay and reward-based strengthening
            # Decay slightly stronger at larger set sizes
            decay = 0.1 + 0.2 * (nS - 3) / 3.0  # 0.1 at set size 3, 0.3 at set size 6
            w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]
            if r > 0.0:
                # shift 0.6 of mass to chosen action
                w[s, :] *= 0.4
                w[s, a] += 0.6
            # Renormalize
            w[s, :] = np.maximum(w[s, :], 1e-8)
            w[s, :] = w[s, :] / np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p