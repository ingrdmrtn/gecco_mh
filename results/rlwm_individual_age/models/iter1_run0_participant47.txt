def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited Working Memory (WM) mixture with age- and set-size-modulated WM efficacy and lapse.

    This model mixes a model-free RL system with a one-shot working-memory system that can
    store the correct action for a state after a rewarded trial. The WM system has a limited
    capacity that degrades with larger set size and with older age. WM items also decay over
    time within a block. A small lapse rate introduces uniform random responding.

    Policy: p(a|s) = (1 - lapse) * [ w_WM * p_WM(a|s) + (1 - w_WM) * softmax_beta(Q_s) ] + lapse/3

    WM mechanics:
      - On a rewarded trial, the chosen action is stored for that state with probability p_store.
      - Stored WM associations decay per trial by a factor dependent on set size.
      - WM retrieval probability equals the current WM strength for the stored action; non-stored actions get 0.

    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..nS-1 within block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of float
        Reward on each trial (0 or 1).
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size for each trial's block (3 or 6).
    age : array-like or scalar
        Participant age; used to determine older group (>=45).
    model_parameters : tuple/list
        (alpha, beta, wm_base, age_capacity_slope, lapse)
        - alpha: RL learning rate (0..1).
        - beta: inverse temperature for RL softmax (>0).
        - wm_base: baseline WM weight/capacity proxy (real).
        - age_capacity_slope: reduction in WM with older age (>=0).
        - lapse: probability of uniform random choice (0..1).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, wm_base, age_capacity_slope, lapse = model_parameters
    age_scalar = float(age[0] if hasattr(age, "__len__") else age)
    older = 1.0 if age_scalar >= 45 else 0.0

    nll = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        # Model-free Q
        Q = (1.0 / nA) * np.ones((nS, nA))

        # Working memory store: for each state, store a preferred action and its current strength
        wm_action = -1 * np.ones(nS, dtype=int)      # -1 means nothing stored
        wm_strength = np.zeros(nS)                   # [0,1] strength

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])
            ss = int(block_set_sizes[t])

            # WM weight/effectiveness: logistic transform of base with penalties for age and set size
            # Higher set size reduces WM; older reduces WM via age_capacity_slope.
            # Map to [0,1] via sigmoid.
            wm_eff = 1.0 / (1.0 + np.exp(-(wm_base - age_capacity_slope * older - 0.6 * (ss - 3))))
            wm_eff = np.clip(wm_eff, 0.0, 1.0)

            # WM retrieval: probability mass on the stored action equals its strength; others 0.
            p_wm = np.zeros(nA)
            if wm_action[s] >= 0:
                p_wm[wm_action[s]] = np.clip(wm_strength[s], 0.0, 1.0)

            # RL softmax policy
            q_s = Q[s, :]
            logits = beta * (q_s - np.max(q_s))
            expv = np.exp(logits)
            p_rl = expv / np.sum(expv)

            # Mixture with lapse
            p_mix = wm_eff * p_wm + (1.0 - wm_eff) * p_rl
            p_mix = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)

            p_choice = max(p_mix[a], 1e-12)
            log_p += np.log(p_choice)

            # Updates
            # RL update
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # WM decay depends on set size (more decay for larger set)
            decay = np.clip(0.10 + 0.15 * (ss - 3), 0.0, 0.9)
            wm_strength[s] *= (1.0 - decay)

            # WM store on rewarded trials with probability p_store (depends on same factors as wm_eff)
            p_store = wm_eff  # use same effective capacity as probability to encode
            if r > 0.5:
                # Stochastic storage approximated by expected update to keep differentiable for likelihood:
                # update strength toward 1 for chosen action proportional to p_store.
                # If different action was stored, replace it by chosen action with probability p_store.
                replace_prob = p_store
                # Expected replacement:
                wm_action[s] = a if (wm_action[s] == -1 or replace_prob >= 0.5) else wm_action[s]
                # Strength moves toward 1 by p_store
                wm_strength[s] += p_store * (1.0 - wm_strength[s])

        nll -= log_p

    return nll


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Beta-Bernoulli (Bayesian) value estimates with set-size-dependent forgetting and global stickiness.
    Age modulates temperature (older -> more noise).

    For each state-action pair, the agent tracks successes and failures (s,f) as Beta(s+1, f+1).
    The expected reward is m = s / (s + f). A forgetting process shrinks counts each trial
    to capture bounded memory, with stronger forgetting at larger set sizes. Policy includes
    a global last-action stickiness that biases repeating the previous action irrespective of state.

    Parameters
    ----------
    states : array-like of int
        State index on each trial.
    actions : array-like of int
        Chosen action (0..2).
    rewards : array-like of float
        Reward (0 or 1).
    blocks : array-like of int
        Block index.
    set_sizes : array-like of int
        Set size per trial's block (3 or 6).
    age : array-like or scalar
        Participant age; older group is age >= 45, used to reduce inverse temperature.
    model_parameters : tuple/list
        (beta_base, kappa_stick, eta, forget_base, age_temp_drop)
        - beta_base: base inverse temperature (>0).
        - kappa_stick: global last-action stickiness weight (real).
        - eta: learning step added to counts per trial (>0).
        - forget_base: baseline forgetting rate per trial (0..1).
        - age_temp_drop: proportional drop in beta for older group (>=0).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    beta_base, kappa_stick, eta, forget_base, age_temp_drop = model_parameters
    age_scalar = float(age[0] if hasattr(age, "__len__") else age)
    older = 1.0 if age_scalar >= 45 else 0.0

    nll = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        succ = np.ones((nS, nA))  # start with Beta(1,1)
        fail = np.ones((nS, nA))

        last_action = None

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])
            ss = int(block_set_sizes[t])

            # Set-size-dependent forgetting
            # Larger sets -> more forgetting
            forget = np.clip(forget_base + 0.15 * (ss - 3), 0.0, 0.99)
            succ[s, :] *= (1.0 - forget)
            fail[s, :] *= (1.0 - forget)

            # Expected value under Beta posterior
            denom = succ[s, :] + fail[s, :]
            m = succ[s, :] / np.maximum(denom, 1e-12)

            # Age-modulated inverse temperature
            beta = beta_base * (1.0 - age_temp_drop * older)
            beta = max(beta, 1e-6)

            # Global stickiness (bias to repeat last action)
            stick = np.zeros(nA)
            if last_action is not None:
                stick[last_action] = 1.0

            logits = beta * (m - np.max(m)) + kappa_stick * stick
            expv = np.exp(logits)
            p = expv / np.sum(expv)

            p_choice = max(p[a], 1e-12)
            log_p += np.log(p_choice)

            # Update counts for chosen action
            succ[s, a] += eta * r
            fail[s, a] += eta * (1.0 - r)

            # Update last action
            last_action = a

        nll -= log_p

    return nll


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Q-learning with uncertainty bonus (UCB-like) and meta-control over exploration.
    Age and set size modulate both the exploration bonus and the effective temperature.
    Includes slow value forgetting to capture limited memory.

    Decision variable: DV(a) = beta_eff * Q(s,a) + gamma_eff * U(s,a)
      - Q updated via standard RL with learning rate alpha.
      - U(s,a) = 1 / sqrt(N(s,a) + 1), where N is visit count for (s,a).
      - beta_eff decreases with age and with larger set sizes (harder condition -> more noise).
      - gamma_eff increases with age and with larger set sizes (older + larger sets -> more exploration).

    Parameters
    ----------
    states : array-like of int
        State index on each trial.
    actions : array-like of int
        Chosen action (0..2).
    rewards : array-like of float
        Reward (0 or 1).
    blocks : array-like of int
        Block index.
    set_sizes : array-like of int
        Set size per trial's block.
    age : array-like or scalar
        Participant age; used to modulate beta and exploration bonus via older group indicator.
    model_parameters : tuple/list
        (alpha, beta0, gamma_unc, age_explore_bias, tau_forget)
        - alpha: RL learning rate (0..1).
        - beta0: base inverse temperature (>0).
        - gamma_unc: baseline uncertainty bonus weight (>=0).
        - age_explore_bias: scales both beta reduction and bonus increase for older group (>=0).
        - tau_forget: value forgetting rate per trial (0..1), increases with set size.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta0, gamma_unc, age_explore_bias, tau_forget = model_parameters
    age_scalar = float(age[0] if hasattr(age, "__len__") else age)
    older = 1.0 if age_scalar >= 45 else 0.0

    nll = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        Q = (1.0 / nA) * np.ones((nS, nA))
        N = np.zeros((nS, nA))  # visit counts for uncertainty

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])
            ss = int(block_set_sizes[t])

            # Effective temperature and uncertainty bonus modulation
            # Larger set size -> lower beta, higher uncertainty weighting.
            beta_eff = beta0 / (1.0 + 0.4 * (ss - 3)) / (1.0 + age_explore_bias * older)
            beta_eff = max(beta_eff, 1e-6)
            gamma_eff = gamma_unc * (1.0 + 0.5 * (ss - 3)) * (1.0 + age_explore_bias * older)

            # Uncertainty bonus
            U = 1.0 / np.sqrt(N[s, :] + 1.0)

            dv = beta_eff * Q[s, :] + gamma_eff * U
            dv = dv - np.max(dv)
            expv = np.exp(dv)
            p = expv / np.sum(expv)

            p_choice = max(p[a], 1e-12)
            log_p += np.log(p_choice)

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # Forgetting toward neutral values (here, toward the mean reward 0.5 proxy via shrink to 0)
            # Implement as multiplicative decay per trial; stronger for larger set sizes.
            forget = np.clip(tau_forget * (1.0 + 0.3 * (ss - 3)), 0.0, 1.0)
            Q[s, :] = (1.0 - forget) * Q[s, :] + forget * (0.0)  # shrink toward 0 baseline

            # Visit count update
            N[s, a] += 1.0

        nll -= log_p

    return nll