def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Leaky RL with uncertainty-driven exploration bonus, perseveration, and age-modulated temperature.

    Mechanism:
    - Model-free Q-learning (single alpha) with leaky integration toward a neutral baseline.
    - A perseveration (choice kernel) term biases repeating the previous action in the same block.
    - An uncertainty bonus (entropy of current Q for that state) promotes exploration; bonus is
      stronger when load is lower (set size 3 vs 6).
    - Older age reduces effective inverse temperature (more noise) via a multiplicative drop.

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..nS-1 within block).
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of float/int
        Reward per trial (0 or 1).
    blocks : array-like of int
        Block ID per trial; learning resets per block.
    set_sizes : array-like of int
        Set size per trial (3 or 6).
    age : array-like or scalar
        Participant age; older (>=45) yields a temperature drop factor.
    model_parameters : sequence of 6 floats
        alpha         : RL learning rate in [0,1]
        beta_base     : Base softmax inverse temperature (scaled by 10 internally)
        kappa         : Perseveration strength added to last chosen action
        leak          : Q-value leak toward uniform (0..1 per trial)
        age_temp_drop : Multiplicative drop on beta if older (>=45), in [0,1]
        u_bonus       : Weight of uncertainty (entropy) exploration bonus

    Returns
    -------
    nll : float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta_base, kappa, leak, age_temp_drop, u_bonus = model_parameters
    # Determine age group
    try:
        age_val = float(age[0])
    except Exception:
        age_val = float(age)
    is_older = 1.0 if age_val >= 45 else 0.0

    beta = beta_base * 10.0
    # Apply an age-based reduction in inverse temperature
    beta = beta * (1.0 - age_temp_drop * is_older)

    eps = 1e-12
    total_log_p = 0.0
    nA = 3

    unique_blocks = np.unique(blocks)
    for b in unique_blocks:
        idx = (blocks == b)
        block_actions = np.asarray(actions)[idx]
        block_rewards = np.asarray(rewards)[idx]
        block_states = np.asarray(states)[idx]
        block_set_sizes = np.asarray(set_sizes)[idx]

        nS = int(block_set_sizes[0])
        # Initialize Q to uniform and choice kernel to zero
        Q = (1.0 / nA) * np.ones((nS, nA))
        choice_kernel = np.zeros(nA)  # across actions, shared within state

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Compute uncertainty (entropy) of Q for this state
            # Softmax over Q for a pseudo-policy to measure uncertainty
            q_logits = Q[s, :] - np.max(Q[s, :])
            p_q = np.exp(q_logits) / (np.sum(np.exp(q_logits)) + eps)
            entropy = -np.sum(p_q * np.log(p_q + eps))

            # Load scaling: more bonus in small sets (3) than large (6)
            load_scale = 3.0 / float(nS)  # 1.0 for 3, 0.5 for 6

            # Action preferences: Q + perseveration + uncertainty bonus (same added to all actions)
            # A uniform uncertainty bonus would cancel out in softmax; instead, bias toward less-explored by
            # subtracting p_q (favoring lower-prob actions). Use (mean - p_q) scaled by entropy.
            mean_p = 1.0 / nA
            uncertainty_bias = u_bonus * load_scale * entropy * (mean_p - p_q)

            prefs = Q[s, :] + kappa * choice_kernel + uncertainty_bias
            prefs = prefs - np.max(prefs)
            p_vec = np.exp(beta * prefs)
            p_vec = p_vec / (np.sum(p_vec) + eps)
            p_a = p_vec[a]

            total_log_p += np.log(max(p_a, eps))

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # Leaky integration toward uniform baseline
            Q[s, :] = (1.0 - leak) * Q[s, :] + leak * (1.0 / nA)

            # Update choice kernel (perseveration): increment chosen action, decay others
            choice_kernel = 0.9 * choice_kernel
            choice_kernel[a] += 1.0

    return -float(total_log_p)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with probabilistic one-shot caching (item memory), retrieval noise, and age-modulated lapses.

    Mechanism:
    - Standard RL learns gradually.
    - When a rewarded trial occurs, with probability p_cache (which is lower at higher set sizes),
      the model caches the correct state-action mapping in a fast item memory.
    - If a state is cached, a retrieval policy places most mass on the cached action with
      retrieval_noise spreading to non-cached actions.
    - Final policy mixes RL and cache depending on cache presence; additionally, a lapse process
      injects uniform random choice, with lapse increased for older adults.

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..nS-1 within block).
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of float/int
        Reward per trial (0 or 1).
    blocks : array-like of int
        Block ID per trial; learning resets per block.
    set_sizes : array-like of int
        Set size per trial (3 or 6).
    age : array-like or scalar
        Participant age; older (>=45) increases lapse probability.
    model_parameters : sequence of 6 floats
        alpha          : RL learning rate in [0,1]
        beta           : RL softmax inverse temperature (scaled by 10 internally)
        p_cache_base   : Base probability of caching on a rewarded trial, in [0,1]
        retrieval_noise: Noise in cache retrieval; probability mass spread to other actions in [0,1]
        lapse_base     : Base lapse probability in [0,0.5)
        age_lapse_slope: Additional lapse if older (>=45), in [0,0.5)

    Returns
    -------
    nll : float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, p_cache_base, retrieval_noise, lapse_base, age_lapse_slope = model_parameters
    beta = beta * 10.0

    try:
        age_val = float(age[0])
    except Exception:
        age_val = float(age)
    is_older = 1.0 if age_val >= 45 else 0.0

    nA = 3
    eps = 1e-12
    total_log_p = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = np.asarray(actions)[idx]
        block_rewards = np.asarray(rewards)[idx]
        block_states = np.asarray(states)[idx]
        block_set_sizes = np.asarray(set_sizes)[idx]
        nS = int(block_set_sizes[0])

        # RL values
        Q = (1.0 / nA) * np.ones((nS, nA))
        # Cache: -1 indicates no cache; otherwise store cached action index
        cache = -1 * np.ones(nS, dtype=int)

        # Effective lapse with age
        lapse = lapse_base + age_lapse_slope * is_older
        lapse = min(max(lapse, 0.0), 0.49)

        # Effective caching probability decreases with larger set size
        p_cache_eff = p_cache_base * (3.0 / float(nS))
        p_cache_eff = min(max(p_cache_eff, 0.0), 1.0)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # RL policy
            logits = beta * (Q[s, :] - np.max(Q[s, :]))
            p_rl = np.exp(logits)
            p_rl = p_rl / (np.sum(p_rl) + eps)

            # Cache policy if present
            if cache[s] >= 0:
                cached_a = int(cache[s])
                p_cache_vec = np.ones(nA) * (retrieval_noise / (nA - 1.0))
                p_cache_vec[cached_a] = 1.0 - retrieval_noise
            else:
                p_cache_vec = np.ones(nA) * (1.0 / nA)

            # Mixture: if cached, weight more toward cache; if not, it's all RL
            has_cache = 1.0 if cache[s] >= 0 else 0.0
            mix_w = has_cache  # 1 if cached else 0
            p_mix = mix_w * p_cache_vec + (1.0 - mix_w) * p_rl

            # Lapse
            p_final = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            p_a = p_final[a]
            total_log_p += np.log(max(p_a, eps))

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # Update cache stochastically upon reward
            if r > 0.5:
                # Bernoulli draw via thresholding deterministic proxy: use state-dependent fraction
                # Here we emulate probabilistic caching without RNG by assigning cache deterministically
                # whenever effective probability exceeds a rotating threshold derived from trial index.
                # To keep it deterministic and still use p_cache_eff, we compare p_cache_eff against
                # a cyclic pseudo-threshold based on t.
                threshold = ((t + 1) % 10) / 10.0
                if p_cache_eff >= threshold:
                    cache[s] = a

    return -float(total_log_p)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Meta-control between WM-like recent-outcome policy and RL, with asymmetric learning and stickiness.
    Gating depends on load (set size) and age; older age reduces WM gating and/or increases reliance on RL.

    Mechanism:
    - RL with separate learning rates for positive and negative prediction errors.
    - Choice stickiness (choice kernel) added to action preferences.
    - A WM-like table W stores the most recent outcome for the chosen action in a state.
      The WM policy favors the most recently rewarded action in that state via softmax.
    - A logistic gate selects how much to rely on WM vs RL; the gate increases at low load,
      and is reduced by older age.

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..nS-1 within block).
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of float/int
        Reward per trial (0 or 1).
    blocks : array-like of int
        Block ID per trial; learning resets per block.
    set_sizes : array-like of int
        Set size per trial (3 or 6).
    age : array-like or scalar
        Participant age; older (>=45) reduces WM gate.
    model_parameters : sequence of 6 floats
        alpha_pos      : Positive PE learning rate
        alpha_neg      : Negative PE learning rate
        beta           : Inverse temperature for both systems (scaled by 10 internally)
        stickiness     : Additive bias for repeating the previous action
        gate_intercept : Intercept of WM gate logistic
        gamma_load     : Sensitivity of WM gate to load (scaled by 3 - set_size)

    Returns
    -------
    nll : float
        Negative log-likelihood of observed actions.
    """
    alpha_pos, alpha_neg, beta, stickiness, gate_intercept, gamma_load = model_parameters
    beta = beta * 10.0

    try:
        age_val = float(age[0])
    except Exception:
        age_val = float(age)
    is_older = 1.0 if age_val >= 45 else 0.0

    nA = 3
    eps = 1e-12
    total_log_p = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = np.asarray(actions)[idx]
        block_rewards = np.asarray(rewards)[idx]
        block_states = np.asarray(states)[idx]
        block_set_sizes = np.asarray(set_sizes)[idx]
        nS = int(block_set_sizes[0])

        Q = (1.0 / nA) * np.ones((nS, nA))
        W = np.zeros((nS, nA))  # last outcome memory (0..1)
        last_action = -1  # for stickiness

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # RL policy
            prefs_rl = Q[s, :].copy()
            if last_action >= 0:
                prefs_rl[last_action] += stickiness
            prefs_rl -= np.max(prefs_rl)
            p_rl = np.exp(beta * prefs_rl)
            p_rl = p_rl / (np.sum(p_rl) + eps)

            # WM policy based on W
            prefs_wm = W[s, :].copy()
            if last_action >= 0:
                prefs_wm[last_action] += stickiness
            prefs_wm -= np.max(prefs_wm)
            p_wm = np.exp(beta * prefs_wm)
            p_wm = p_wm / (np.sum(p_wm) + eps)

            # Gate depends on load and age:
            # gate = sigmoid(gate_intercept + gamma_load*(3 - set_size) + age_term)
            # age_term reduces gate if older; scale by proportion of older years above 45
            load_term = gamma_load * (3.0 - float(nS))  # 0 for 3, -3 for 6 if gamma_load>0
            age_term = -0.5 * is_older  # fixed reduction for older adults
            z = gate_intercept + load_term + age_term
            gate = 1.0 / (1.0 + np.exp(-z))
            gate = min(max(gate, 0.0), 1.0)

            p_mix = gate * p_wm + (1.0 - gate) * p_rl
            p_a = p_mix[a]
            total_log_p += np.log(max(p_a, eps))

            # RL update with asymmetric learning rates
            pe = r - Q[s, a]
            if pe >= 0:
                Q[s, a] += alpha_pos * pe
            else:
                Q[s, a] += alpha_neg * pe

            # WM update: store latest outcome for chosen action; slight decay for others
            W[s, :] *= 0.9
            W[s, a] = r

            last_action = a

    return -float(total_log_p)