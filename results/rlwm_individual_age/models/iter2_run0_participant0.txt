def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with state-specific choice-kernel arbitration modulated by age and load.

    The model blends a standard RL policy with a state-specific choice kernel (CK) that captures
    recent choice tendencies. The CK acts like a short-term memory of recency/preference that
    decays via learning toward the last chosen action in each state. The arbitration weight for
    CK is modulated by set size (reduced under higher load) and by age group (younger vs older).

    Parameters
    ----------
    states : array-like of int
        State indices per trial (0..set_size-1 within block).
    actions : array-like of int
        Chosen action indices (0..2).
    rewards : array-like of {0,1}
        Feedback per trial.
    blocks : array-like of int
        Block indices; learning and CK reset per block.
    set_sizes : array-like of int
        Set size (3 or 6) per trial.
    age : array-like of number
        Participant age; age>=45 considered older, age<45 younger.
    model_parameters : sequence of float
        [alpha, beta, eta_ck, w_ck_base, age_load_bias]
          - alpha: RL learning rate (0..1).
          - beta: inverse temperature (>0) applied to both RL and CK softmax.
          - eta_ck: CK learning rate (0..1); higher -> CK follows recent choices more.
          - w_ck_base: base mixture weight for CK component (0..1) before modulations.
          - age_load_bias: linear bias scaling the CK weight by age group and load;
                           positive values increase CK in younger and decrease in older,
                           and reduce CK under larger set sizes.

    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    alpha, beta, eta_ck, w_ck_base, age_load_bias = model_parameters
    # Ensure scalar age
    age_val = float(age[0]) if hasattr(age, "__len__") else float(age)
    is_older = age_val >= 45

    eps = 1e-12
    nll = 0.0

    # Helper transforms without importing
    def clip01(x):
        return max(0.0, min(1.0, x))

    def sigmoid(x):
        # numerically stable simple sigmoid
        if x >= 0:
            z = np.exp(-x)
            return 1.0 / (1.0 + z)
        else:
            z = np.exp(x)
            return z / (1.0 + z)

    # Constrain base weight to [0,1] via sigmoid transform of input (which may be given in 0..1 already)
    w_base = clip01(w_ck_base)

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask]
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL Q-values and CK preferences per state
        Q = (1.0 / nA) * np.ones((nS, nA))
        CK = (1.0 / nA) * np.ones((nS, nA))

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            nS_t = int(block_set_sizes[t])

            # RL policy
            Q_s = Q[s, :]
            Q_s_shift = Q_s - np.max(Q_s)
            expQ = np.exp(beta * Q_s_shift)
            p_rl_vec = expQ / np.sum(expQ)
            p_rl = max(p_rl_vec[a], eps)

            # CK policy (softmax over CK state preferences)
            CK_s = CK[s, :]
            CK_s_shift = CK_s - np.max(CK_s)
            expCK = np.exp(beta * CK_s_shift)
            p_ck_vec = expCK / np.sum(expCK)
            p_ck = max(p_ck_vec[a], eps)

            # Arbitration weight:
            # - Lower under higher load (factor 3/nS_t)
            # - Age modulation: +age_load_bias for younger, -age_load_bias for older
            load_factor = 3.0 / float(max(1, nS_t))
            age_sign = 1.0 if not is_older else -1.0
            w_mod = w_base + age_sign * age_load_bias * load_factor
            w_mod = clip01(w_mod)

            # Mixture policy
            p = w_mod * p_ck + (1.0 - w_mod) * p_rl
            p = max(p, eps)
            nll -= np.log(p)

            # RL update
            delta = r - Q[s, a]
            Q[s, a] += alpha * delta

            # CK update (state-specific choice recency)
            # Move CK[s] toward a one-hot on chosen action
            target = np.zeros(nA)
            target[a] = 1.0
            CK[s, :] = (1.0 - eta_ck) * CK[s, :] + eta_ck * target

    return nll


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with adaptive learning rate driven by policy uncertainty, load, and age meta-bias.

    The model uses a single RL system but adapts its per-trial learning rate as a function of:
      - current policy uncertainty (higher uncertainty -> higher learning),
      - cognitive load (larger set sizes reduce effective learning),
      - age group meta-bias (younger vs older shift in adaptive gain).
    No explicit WM module; all effects are captured via dynamic learning-rate modulation.

    Parameters
    ----------
    states : array-like of int
        State indices per trial (0..set_size-1 within block).
    actions : array-like of int
        Chosen action indices (0..2).
    rewards : array-like of {0,1}
        Feedback per trial.
    blocks : array-like of int
        Block indices; learning resets per block.
    set_sizes : array-like of int
        Set size (3 or 6) per trial.
    age : array-like of number
        Participant age; age>=45 considered older, age<45 younger.
    model_parameters : sequence of float
        [alpha_base, beta, kappa_unc, load_penalty, age_meta]
          - alpha_base: baseline RL learning rate (0..1).
          - beta: inverse temperature for softmax (>0).
          - kappa_unc: gain linking policy uncertainty and PE magnitude to learning-rate boosts (>0).
          - load_penalty: scales reduction of learning when set size increases from 3 to 6 (>0).
          - age_meta: signed bias added to the adaptive learning-rate drive;
                      positive increases learning in younger and decreases in older.

    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    alpha_base, beta, kappa_unc, load_penalty, age_meta = model_parameters
    age_val = float(age[0]) if hasattr(age, "__len__") else float(age)
    is_older = age_val >= 45

    eps = 1e-12
    nll = 0.0
    log_nA = np.log(3.0)

    def sigmoid(x):
        if x >= 0:
            z = np.exp(-x)
            return 1.0 / (1.0 + z)
        else:
            z = np.exp(x)
            return z / (1.0 + z)

    # Convert base learning rate to unconstrained logit for stable modulation, then back via sigmoid
    def logit(p):
        p = min(max(p, eps), 1.0 - eps)
        return np.log(p) - np.log(1.0 - p)

    base_logit = logit(alpha_base)

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask]
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])
        Q = (1.0 / nA) * np.ones((nS, nA))

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            nS_t = int(block_set_sizes[t])

            # Compute RL policy
            Q_s = Q[s, :]
            Q_s_shift = Q_s - np.max(Q_s)
            expQ = np.exp(beta * Q_s_shift)
            p_vec = expQ / np.sum(expQ)
            p = max(p_vec[a], eps)
            nll -= np.log(p)

            # Uncertainty (normalized entropy) and PE magnitude
            entropy = -np.sum(p_vec * np.log(np.maximum(p_vec, eps)))
            u_norm = entropy / log_nA  # in [0,1]
            pe = r - Q[s, a]
            drive = kappa_unc * (abs(pe) * u_norm)

            # Load penalty: increase when nS is larger
            load_term = -load_penalty * ((nS_t - 3.0) / 3.0)

            # Age meta-bias: positive for younger, negative for older
            age_term = (age_meta if not is_older else -age_meta)

            # Adaptive learning rate (in (0,1))
            alpha_t = sigmoid(base_logit + drive + load_term + age_term)

            # RL update
            Q[s, a] += alpha_t * pe

    return nll


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + Bayesian-like episodic cache with confidence-weighted arbitration by load and age.

    The model mixes RL with an episodic cache that accumulates evidence for correct actions
    per state using decaying action counts (Dirichlet-like). Cache evidence determines a WM-like
    policy, and its confidence (total evidence mass) gate-controls arbitration with RL. Load
    reduces cache influence; age biases the arbitration in favor of the cache for younger and
    against for older.

    Parameters
    ----------
    states : array-like of int
        State indices per trial (0..set_size-1 within block).
    actions : array-like of int
        Chosen action indices (0..2).
    rewards : array-like of {0,1}
        Feedback per trial.
    blocks : array-like of int
        Block indices; learning and cache reset per block.
    set_sizes : array-like of int
        Set size (3 or 6) per trial.
    age : array-like of number
        Participant age; age>=45 considered older, age<45 younger.
    model_parameters : sequence of float
        [alpha, beta, phi_forget, prior_strength, age_conf_bias]
          - alpha: RL learning rate (0..1).
          - beta: inverse temperature (>0) for both RL and cache policies.
          - phi_forget: per-visit forgetting of cache counts for the current state (0..1).
          - prior_strength: increment added to chosen action on reward=1 (>0), shaping cache certainty.
          - age_conf_bias: linear bias added to cache weight for younger (positive) and subtracted for older.

    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    alpha, beta, phi_forget, prior_strength, age_conf_bias = model_parameters
    age_val = float(age[0]) if hasattr(age, "__len__") else float(age)
    is_older = age_val >= 45

    eps = 1e-12
    nll = 0.0

    def clip01(x):
        return max(0.0, min(1.0, x))

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask]
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # RL Q-values and episodic cache counts
        Q = (1.0 / nA) * np.ones((nS, nA))
        C = np.ones((nS, nA))  # start with symmetric weak prior (1 per action)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            nS_t = int(block_set_sizes[t])

            # RL policy
            Q_s = Q[s, :]
            Q_s_shift = Q_s - np.max(Q_s)
            expQ = np.exp(beta * Q_s_shift)
            p_rl_vec = expQ / np.sum(expQ)
            p_rl = max(p_rl_vec[a], eps)

            # Cache policy from counts
            C_s = C[s, :]
            C_s_shift = C_s - np.max(C_s)
            expC = np.exp(beta * C_s_shift)
            p_cache_vec = expC / np.sum(expC)
            p_cache = max(p_cache_vec[a], eps)

            # Confidence from total mass in this state (minus baseline prior)
            total_mass = np.sum(C_s)
            base_mass = nA  # initial symmetric prior mass
            conf = max(0.0, (total_mass - base_mass) / (total_mass + eps))  # in [0,1) increasing with evidence

            # Load and age modulation of cache weight
            load_factor = 3.0 / float(max(1, nS_t))  # reduce under higher load
            age_term = (age_conf_bias if not is_older else -age_conf_bias)
            # Base weight is confidence scaled by load, then shifted by age term and clipped
            w_cache = conf * load_factor + age_term
            w_cache = clip01(w_cache)

            # Mixture policy
            p = w_cache * p_cache + (1.0 - w_cache) * p_rl
            p = max(p, eps)
            nll -= np.log(p)

            # RL update
            delta = r - Q[s, a]
            Q[s, a] += alpha * delta

            # Cache update with forgetting applied to visited state
            # Forgetting toward zero counts, then add evidence
            C[s, :] = (1.0 - phi_forget) * C[s, :]

            if r >= 0.5:
                # Reward confirms action a in state s
                C[s, a] += prior_strength
            else:
                # Mild diffusion toward uniform on negative feedback: share small fraction to non-chosen
                leak = 0.5 * prior_strength
                share = leak / (nA - 1)
                for aa in range(nA):
                    if aa != a:
                        C[s, aa] += share

    return nll