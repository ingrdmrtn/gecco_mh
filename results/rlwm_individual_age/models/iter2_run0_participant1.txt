def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL mixed with evidence-gated episodic mapping memory, modulated by set size and age.

    Idea
    ----
    The agent has two systems:
    - RL system: learns action values per state via prediction errors.
    - Episodic mapping memory: increments evidence for state-action pairs only when rewarded.
      When sufficient evidence accumulates for some action in a state, the agent increasingly
      relies on that action (one-shot mapping). The reliance on memory is gated by:
        - how strong the evidence is relative to alternatives,
        - the set size (higher load reduces gating),
        - the age group (older group reduces gating; here participant is younger).

    Parameters
    ----------
    states : array-like of int
        State index on each trial (within block).
    actions : array-like of int
        Chosen action (0..2).
    rewards : array-like of int/float
        Binary feedback (0/1).
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size per trial's block (3 or 6).
    age : array-like (length 1)
        Participant age in years. Used to determine age group (<45 younger, >=45 older).
    model_parameters : list/tuple of up to 6 numbers:
        - alpha_rl: RL learning rate in [0,1]
        - beta: inverse temperature for RL softmax (>0)
        - evidence_gain: gain scaling from evidence strength to WM usage (>0)
        - age_gate_bias: additional memory gating penalty if older (>=45) (>=0)
        - epsilon_lapse: lapse probability mixing with uniform choice in [0,0.2]
        - wm_conf_boost: boosts evidence effect after consecutive rewards (>=0)

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha_rl, beta, evidence_gain, age_gate_bias, epsilon_lapse, wm_conf_boost = model_parameters
    nA = 3
    beta = max(beta, 1e-6) * 5.0
    epsilon_lapse = np.clip(epsilon_lapse, 0.0, 0.2)
    evidence_gain = max(evidence_gain, 1e-6)
    wm_conf_boost = max(0.0, wm_conf_boost)

    age_group = 1.0 if age[0] >= 45 else 0.0

    total_loglik = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask]
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_set_sizes = set_sizes[mask]

        nS = int(block_set_sizes[0])

        # RL values
        Q = (1.0 / nA) * np.ones((nS, nA))

        # Episodic mapping evidence: counts of rewarded occurrences
        ev = np.zeros((nS, nA))
        # Track consecutive reward streaks per state-action to boost confidence
        streak = np.zeros((nS, nA))

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Load factor: 0 for set size 3, 1 for set size 6
            load = (max(nS, 3) - 3) / 3.0

            # Compute memory evidence strength for current state
            # Use "margin" between best evidence and second best as strength
            ev_s = ev[s, :].copy()
            if np.all(ev_s == 0):
                best_a = -1
                margin = 0.0
            else:
                order = np.argsort(-ev_s)
                best_a = order[0]
                second = ev_s[order[1]] if nA > 1 else 0.0
                margin = ev_s[best_a] - second

            # Confidence boost from consecutive rewards on best action
            conf_boost = 0.0
            if best_a >= 0 and streak[s, best_a] > 0:
                conf_boost = wm_conf_boost * streak[s, best_a]

            # Map evidence to WM gating probability via sigmoid
            # Older group experiences an additional penalty (less WM reliance),
            # and higher load reduces WM influence.
            gate_logit = evidence_gain * (margin + conf_boost) - (load * evidence_gain) - (age_group * age_gate_bias)
            p_wm = 1.0 / (1.0 + np.exp(-gate_logit))

            # RL policy
            logits_rl = beta * Q[s, :]
            logits_rl -= np.max(logits_rl)
            p_rl = np.exp(logits_rl)
            p_rl = p_rl / np.sum(p_rl)

            # WM policy: if we have a best action, concentrate on it; otherwise uniform
            if best_a >= 0 and (ev_s[best_a] > 0):
                p_wm_vec = np.zeros(nA)
                p_wm_vec[best_a] = 1.0
            else:
                p_wm_vec = np.ones(nA) / nA

            # Mixture of WM and RL, with lapse to uniform
            p_mix = (1.0 - epsilon_lapse) * (p_wm * p_wm_vec + (1.0 - p_wm) * p_rl) + epsilon_lapse * (1.0 / nA)
            p_choice = np.clip(p_mix[a], 1e-12, 1.0)
            total_loglik += np.log(p_choice)

            # RL update
            Q[s, a] += alpha_rl * (r - Q[s, a])

            # Evidence update: increment only on reward, reset streak on zero
            if r > 0.5:
                ev[s, a] += 1.0
                streak[s, :] = 0.0
                streak[s, a] += 1.0
            else:
                streak[s, :] = 0.0

    return -float(total_loglik)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Dual learning-rate RL with novelty bonus, modulated by set size and age.

    Idea
    ----
    - The agent uses Q-learning with different learning rates in low vs high set size blocks.
    - An exploration bonus favors less-visited state-action pairs (novelty). The bonus is
      stronger for younger participants and smaller set sizes, reflecting better working
      memory-guided directed exploration in low load and youth.

    Parameters
    ----------
    states : array-like of int
        State index per trial within block.
    actions : array-like of int
        Chosen action (0..2).
    rewards : array-like of int/float
        Binary feedback (0/1).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial's block (3 or 6).
    age : array-like (length 1)
        Participant age; age group (<45 younger; >=45 older) modulates novelty and learning rate.
    model_parameters : list/tuple (max 6):
        - alpha_small: learning rate used when set size = 3
        - alpha6_base: base learning rate for set size = 6 (pre age adjustment)
        - beta: inverse temperature (>0)
        - eta_novelty: base novelty bonus strength (>0)
        - age_nov_mod: multiplicative reduction of novelty and alpha6 for older group in [0,1]

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha_small, alpha6_base, beta, eta_novelty, age_nov_mod = model_parameters
    nA = 3
    beta = max(beta, 1e-6) * 5.0
    eta_novelty = max(0.0, eta_novelty)
    age_group = 1.0 if age[0] >= 45 else 0.0

    total_loglik = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask]
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_set_sizes = set_sizes[mask]

        nS = int(block_set_sizes[0])

        # Initialize Q and visit counts for novelty
        Q = (1.0 / nA) * np.ones((nS, nA))
        visits = np.zeros((nS, nA)) + 1e-6  # avoid divide by zero

        # Determine learning rate and novelty scaling for this block
        if nS <= 3:
            alpha = np.clip(alpha_small, 0.0, 1.0)
            novelty_scale = 1.0  # more effective novelty in small set
        else:
            # Older group reduces alpha and novelty in large set
            alpha = np.clip(alpha6_base * (1.0 - 0.5 * age_group * np.clip(age_nov_mod, 0.0, 1.0)), 0.0, 1.0)
            novelty_scale = 0.75  # novelty somewhat less diagnostic in large sets

        # Age-dependent novelty multiplier (older reduce novelty)
        age_multiplier = (1.0 - age_group * np.clip(age_nov_mod, 0.0, 1.0))

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Novelty bonus decays with visits; bounded to avoid exploding exponentials
            bonus = (eta_novelty * age_multiplier * novelty_scale) / np.sqrt(visits[s, :] + 1.0)

            # Softmax over Q + bonus
            logits = beta * (Q[s, :] + bonus)
            logits -= np.max(logits)
            p = np.exp(logits)
            p = p / np.sum(p)
            p_choice = np.clip(p[a], 1e-12, 1.0)
            total_loglik += np.log(p_choice)

            # Update Q and visits
            Q[s, a] += alpha * (r - Q[s, a])
            visits[s, a] += 1.0

    return -float(total_loglik)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with load- and age-dependent global forgetting and state-specific perseveration.

    Idea
    ----
    - Standard Q-learning for chosen action.
    - After each trial, all Q-values decay toward a uniform prior to model interference/forgetting.
      Decay strength increases with set size (load) and is stronger for older participants.
    - Choices are biased toward repeating the last action within the same state (state-specific stickiness).
    - A small lapse mixes with uniform choice.

    Parameters
    ----------
    states : array-like of int
        State index per trial within block.
    actions : array-like of int
        Chosen action (0..2).
    rewards : array-like of int/float
        Binary feedback (0/1).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial's block (3 or 6).
    age : array-like (length 1)
        Participant age; age group (<45 younger; >=45 older) increases decay.
    model_parameters : list/tuple (max 6):
        - alpha: RL learning rate in [0,1]
        - beta: inverse temperature (>0)
        - decay_base: base forgetting rate per trial in [0,1]
        - age_decay_boost: multiplicative boost of decay if older (>=0)
        - pers_state: state-specific perseveration strength (bias added to last action logits)
        - lapse: lapse probability mixing with uniform in [0,0.2]

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, decay_base, age_decay_boost, pers_state, lapse = model_parameters
    nA = 3
    beta = max(beta, 1e-6) * 5.0
    decay_base = np.clip(decay_base, 0.0, 1.0)
    age_decay_boost = max(0.0, age_decay_boost)
    pers_state = max(0.0, pers_state)
    lapse = np.clip(lapse, 0.0, 0.2)

    age_group = 1.0 if age[0] >= 45 else 0.0

    total_loglik = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask]
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_set_sizes = set_sizes[mask]

        nS = int(block_set_sizes[0])

        Q = (1.0 / nA) * np.ones((nS, nA))
        # Track last action per state for state-specific stickiness
        last_action_state = -1 * np.ones(nS, dtype=int)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Load factor: 0 for 3, 1 for 6
            load = (max(nS, 3) - 3) / 3.0
            # Effective decay this trial
            decay = np.clip(decay_base * (1.0 + age_group * age_decay_boost) * (1.0 + 0.5 * load), 0.0, 1.0)

            # Build logits with state-specific perseveration
            logits = beta * Q[s, :].copy()
            if last_action_state[s] >= 0:
                logits[last_action_state[s]] += pers_state

            logits -= np.max(logits)
            p = np.exp(logits)
            p = p / np.sum(p)
            p = (1.0 - lapse) * p + lapse * (1.0 / nA)

            p_choice = np.clip(p[a], 1e-12, 1.0)
            total_loglik += np.log(p_choice)

            # Q-learning update for chosen action
            Q[s, a] += np.clip(alpha, 0.0, 1.0) * (r - Q[s, a])

            # Global forgetting toward uniform prior for all states and actions
            Q = (1.0 - decay) * Q + decay * (1.0 / nA)

            # Update state-specific last action
            last_action_state[s] = a

    return -float(total_loglik)