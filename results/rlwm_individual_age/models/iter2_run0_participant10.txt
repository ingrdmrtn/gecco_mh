def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + probabilistic capacity-limited working memory (WM) mixture with age-dependent capacity.

    Idea:
    - A standard RL system learns Q-values with a delta rule.
    - In parallel, a WM system stores the last rewarded action for a state when reward=1.
    - The WM system is capacity-limited: on each block, each state's WM entry is available with
      probability proportional to an age-dependent capacity K divided by the block set size.
      When WM is available for the current state, it mixes with RL.
    - Younger and older groups have different WM capacities (K_y vs K_o).

    Parameters (model_parameters):
    - alpha: RL learning rate (0..1).
    - beta: inverse temperature for softmax (>0).
    - K_y: WM capacity parameter for younger adults (>=0).
    - K_o: WM capacity parameter for older adults (>=0).
    - wm_conf: weight scaling the contribution of WM when it is available (0..1).

    Inputs:
    - states: array of state indices per trial (0..set_size-1 within a block).
    - actions: array of chosen actions per trial (0..2).
    - rewards: array of rewards per trial (0 or 1).
    - blocks: array of block identifiers per trial (learning resets at each new block).
    - set_sizes: array with the set size for each trial (3 or 6) within blocks.
    - age: array-like with a single entry: participant age in years.
    - model_parameters: list/tuple with [alpha, beta, K_y, K_o, wm_conf].

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    alpha, beta, K_y, K_o, wm_conf = model_parameters
    age_val = age[0]
    is_older = 1.0 if age_val >= 45 else 0.0
    K_eff_global = (1.0 - is_older) * K_y + is_older * K_o

    nll = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        # RL values
        Q = (1.0 / nA) * np.ones((nS, nA))

        # WM store: last rewarded action for each state (if any)
        wm_action = -1 * np.ones(nS, dtype=int)
        wm_valid = np.zeros(nS, dtype=float)  # 1.0 if a rewarded action is stored, else 0.0

        # Probability that WM is available for a given state in this block:
        # storage/availability scales with capacity relative to set size
        storage_prob = min(1.0, max(0.0, K_eff_global / float(nS)))

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            # RL policy
            logits = beta * (Q[s, :] - np.max(Q[s, :]))
            prl = np.exp(logits)
            prl = prl / np.sum(prl)

            # WM policy: if valid, choose stored action almost deterministically
            if wm_valid[s] > 0.0 and wm_action[s] >= 0:
                eps = 1e-8
                pwm = np.full(nA, eps)
                pwm[wm_action[s]] = 1.0 - (nA - 1) * eps
                wm_available = 1.0
            else:
                pwm = np.ones(nA) / nA
                wm_available = 0.0

            # Effective WM usage weight on this trial
            wm_use = wm_conf * storage_prob * wm_available
            wm_use = max(0.0, min(1.0, wm_use))

            p = wm_use * pwm + (1.0 - wm_use) * prl
            p_a = max(p[a], 1e-12)
            nll -= np.log(p_a)

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # WM update: store action only if rewarded; reward=0 leaves WM unchanged
            if r >= 0.5:
                wm_action[s] = a
                wm_valid[s] = 1.0

    return nll


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Asymmetric RL (different learning rates for rewards and non-rewards) with age- and set-size
    dependent exploration temperature.

    Idea:
    - Two learning rates: alpha_pos for rewarded outcomes, alpha_neg for non-rewarded outcomes.
    - Choice stochasticity (inverse temperature) is reduced under higher set size and for older adults,
      capturing increased exploration under cognitive load and aging.
      Specifically: beta_eff = beta0 / (1 + c_set * ((set_size-3)/3) + c_age * is_older)

    Parameters (model_parameters):
    - alpha_pos: learning rate for rewarded outcomes (0..1).
    - alpha_neg: learning rate for non-rewarded outcomes (0..1).
    - beta0: base inverse temperature (>0).
    - c_set: penalty scaling of inverse temperature due to larger set size (>=0).
    - c_age: penalty scaling of inverse temperature for older adults (>=0).

    Inputs:
    - states: array of state indices per trial.
    - actions: array of chosen actions per trial (0..2).
    - rewards: array of rewards per trial (0 or 1).
    - blocks: array of block identifiers per trial.
    - set_sizes: array of set sizes per trial (3 or 6).
    - age: array-like with a single entry: participant age in years.
    - model_parameters: list/tuple with [alpha_pos, alpha_neg, beta0, c_set, c_age].

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    alpha_pos, alpha_neg, beta0, c_set, c_age = model_parameters
    age_val = age[0]
    is_older = 1.0 if age_val >= 45 else 0.0

    nll = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        Q = (1.0 / nA) * np.ones((nS, nA))

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]
            curr_set = int(block_set_sizes[t])

            # Effective inverse temperature decreases with load and age
            load_term = c_set * ((curr_set - 3.0) / 3.0)
            age_term = c_age * is_older
            denom = 1.0 + max(0.0, load_term) + max(0.0, age_term)
            beta_eff = beta0 / max(1e-6, denom)

            logits = beta_eff * (Q[s, :] - np.max(Q[s, :]))
            p = np.exp(logits)
            p = p / np.sum(p)
            p_a = max(p[a], 1e-12)
            nll -= np.log(p_a)

            # Asymmetric update based on outcome valence
            alpha = alpha_pos if r >= 0.5 else alpha_neg
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

    return nll


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Pearce-Hall associability RL with age- and load-dependent associability decay.

    Idea:
    - Each state has an associability kappa_s that scales the effective learning rate.
    - kappa_s increases with surprise (absolute prediction error) and decays over trials.
    - The decay is stronger under higher set size and in older adults, capturing reduced sustained
      attention/working-memory under load/aging.
    - Effective LR = alpha0 * kappa_s, bounded to [0,1].

    Parameters (model_parameters):
    - alpha0: base learning rate scale (0..1).
    - beta: inverse temperature for softmax (>0).
    - phi: learning rate for associability updates (how strongly surprise increases kappa) (0..1).
    - decay_base: base decay of associability per trial (0..1).
    - age_decay_gain: multiplicative increase of decay in older adults (>=0).

    Inputs:
    - states: array of state indices per trial.
    - actions: array of chosen actions per trial (0..2).
    - rewards: array of rewards per trial (0 or 1).
    - blocks: array of block identifiers per trial.
    - set_sizes: array of set sizes per trial (3 or 6).
    - age: array-like with a single entry: participant age in years.
    - model_parameters: list/tuple with [alpha0, beta, phi, decay_base, age_decay_gain].

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    alpha0, beta, phi, decay_base, age_decay_gain = model_parameters
    age_val = age[0]
    is_older = 1.0 if age_val >= 45 else 0.0

    nll = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        Q = (1.0 / nA) * np.ones((nS, nA))
        kappa = np.ones(nS) * 0.5  # initial associability mid-range

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]
            curr_set = int(block_set_sizes[t])

            # Choice policy
            logits = beta * (Q[s, :] - np.max(Q[s, :]))
            p = np.exp(logits)
            p = p / np.sum(p)
            p_a = max(p[a], 1e-12)
            nll -= np.log(p_a)

            # RL update with associability-scaled learning rate
            pe = r - Q[s, a]
            lr_eff = max(0.0, min(1.0, alpha0 * kappa[s]))
            Q[s, a] += lr_eff * pe

            # Associability update: increase with surprise, decay with age and set size
            d_eff = decay_base * (curr_set / 3.0) * (1.0 + age_decay_gain * is_older)
            d_eff = max(0.0, min(1.0, d_eff))
            kappa[s] = (1.0 - d_eff) * kappa[s] + phi * abs(pe)
            # keep within [0,1]
            kappa[s] = max(0.0, min(1.0, kappa[s]))

    return nll