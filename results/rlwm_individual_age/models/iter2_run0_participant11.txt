def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with Pearce-Hall volatility-driven learning rate and age-/load-modulated temperature.

    The model is purely model-free RL but uses a dynamic, state-action specific learning rate
    that tracks surprise (Pearce-Hall). The inverse temperature is also modulated by cognitive
    load (set size) and by age group.

    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..nS-1 within a block).
    actions : array-like of int
        Chosen action on each trial (0..2). Trials with out-of-range actions are ignored.
    rewards : array-like of float
        Reward on each trial (typically 0 or 1). Trials with rewards not in {0,1} are ignored.
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size (number of states) for each trial/block.
    age : array-like (single value)
        Participant age in years; forms an age group (older >= 45).
    model_parameters : list or tuple of floats
        [alpha0, beta, eta_ph, age_beta_scale]
        - alpha0: initial learning rate for all state-actions (0..1).
        - beta: base inverse temperature for softmax (>0).
        - eta_ph: Pearce-Hall update rate for adaptive learning rate (0..1).
        - age_beta_scale: scales exploration for older vs. younger; larger reduces
          inverse temperature for older and mildly boosts for younger (>=0).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha0, beta, eta_ph, age_beta_scale = model_parameters
    is_old = 1.0 if age[0] >= 45 else 0.0
    eps = 1e-12
    nll = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize Q-values and adaptive learning rates
        Q = np.zeros((nS, nA))
        ALR = np.ones((nS, nA)) * np.clip(alpha0, 0.0, 1.0)

        # Precompute load scaling for exploration: higher set size -> lower beta_eff
        # Also modulate by age group: older adults have reduced beta (more noise),
        # younger get a mild increase in beta (sharper exploitation).
        load_scale = 3.0 / max(1.0, float(nS))  # in (0,3]; =1 for nS=3, =0.5 for nS=6
        # Map to [0.5, 1] roughly: 0.5 + 0.5*load_scale gives 1 for nS=3 and 0.75 for nS=6
        load_beta_factor = 0.5 + 0.5 * min(1.0, load_scale)

        age_factor = (1.0 - age_beta_scale * is_old) * (1.0 + (1.0 - is_old) * 0.3 * age_beta_scale)
        beta_eff = max(eps, beta * load_beta_factor * age_factor)

        for t in range(len(block_states)):
            a = int(block_actions[t])
            r = float(block_rewards[t])
            s = int(block_states[t])

            if not (0 <= s < nS) or not (0 <= a < nA) or (r not in [0.0, 1.0]):
                continue

            q_s = Q[s, :]
            # Stable softmax
            q_cent = q_s - np.max(q_s)
            p = np.exp(beta_eff * q_cent)
            p = p / (np.sum(p) + eps)

            nll -= np.log(max(p[a], eps))

            # Prediction error and Pearce-Hall learning rate update
            pe = r - Q[s, a]
            # Update learning rate for the experienced (s,a)
            ALR[s, a] = (1.0 - eta_ph) * ALR[s, a] + eta_ph * min(1.0, abs(pe))

            # Set-size reduces the effective learning rate (greater load -> smaller alpha)
            size_factor = min(1.0, 3.0 / max(1.0, float(nS)))
            # Mild age effect on plasticity: older slightly lower, younger slightly higher
            age_alpha_factor = (1.0 - 0.2 * is_old) * (1.0 + 0.1 * (1.0 - is_old))
            alpha_eff = np.clip(ALR[s, a] * size_factor * age_alpha_factor, 0.0, 1.0)

            Q[s, a] += alpha_eff * pe

    return nll


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + probabilistic working-memory (WM) gating with decay; arbitration by WM availability.

    For each state, WM attempts a one-shot storage of the last rewarded action with a confidence
    weight that decays over time. A state's probability of being in WM depends on the expected
    capacity (lambda_slots) relative to set size and is reduced by older age. The policy is a
    mixture of WM and RL controlled by the state's current WM reliability.

    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..nS-1 within a block).
    actions : array-like of int
        Chosen action on each trial (0..2). Trials with out-of-range actions are ignored.
    rewards : array-like of float
        Reward on each trial (typically 0 or 1). Trials with rewards not in {0,1} are ignored.
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size (number of states) for each trial/block.
    age : array-like (single value)
        Participant age in years; forms an age group (older >= 45).
    model_parameters : list or tuple of floats
        [alpha_rl, beta, lambda_slots, wm_decay]
        - alpha_rl: RL learning rate (0..1).
        - beta: inverse temperature for RL softmax (>0).
        - lambda_slots: expected number of WM-available states per block (>=0).
        - wm_decay: per-trial decay of WM confidence (0..1), higher means faster decay.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha_rl, beta, lambda_slots, wm_decay = model_parameters
    is_old = 1.0 if age[0] >= 45 else 0.0
    eps = 1e-12
    nll = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # RL Q-values
        Q = np.zeros((nS, nA))

        # WM: for each state, store best action and a confidence weight in [0,1]
        wm_action = -np.ones(nS, dtype=int)  # -1 means unknown
        wm_conf = np.zeros(nS)

        # Probability that a given state is represented in WM at any time
        # p_in scales with lambda_slots / nS, reduced by age, and slightly reduced at higher set sizes
        base_pin = lambda_slots / max(1.0, float(nS))
        # Age reduces WM availability; younger get mild boost
        age_pin = (1.0 - 0.5 * is_old) * (1.0 + 0.1 * (1.0 - is_old))
        # Load penalty: additional reduction for larger nS
        load_pin = min(1.0, 3.0 / max(1.0, float(nS)))
        p_in = np.clip(base_pin * age_pin * load_pin, 0.0, 1.0)

        for t in range(len(block_states)):
            a = int(block_actions[t])
            r = float(block_rewards[t])
            s = int(block_states[t])

            if not (0 <= s < nS) or not (0 <= a < nA) or (r not in [0.0, 1.0]):
                continue

            # Decay WM confidence each visit
            wm_conf[s] *= (1.0 - np.clip(wm_decay, 0.0, 1.0))

            # Compute WM policy for this state
            if wm_action[s] >= 0:
                wm_probs = np.ones(nA) * (1.0 - wm_conf[s]) / (nA - 1)
                wm_probs[wm_action[s]] = wm_conf[s]
            else:
                wm_probs = np.ones(nA) / nA  # unknown in WM -> uniform

            # RL policy
            q_s = Q[s, :]
            q_cent = q_s - np.max(q_s)
            p_rl = np.exp(beta * q_cent)
            p_rl = p_rl / (np.sum(p_rl) + eps)

            # Arbitration: weight equals probability the state is in WM times its reliability
            w = np.clip(p_in * wm_conf[s], 0.0, 1.0)
            p = w * wm_probs + (1.0 - w) * p_rl

            nll -= np.log(max(p[a], eps))

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += np.clip(alpha_rl, 0.0, 1.0) * pe

            # WM storage rule: if rewarded, store chosen action at high confidence; if not rewarded,
            # reduce confidence and do not overwrite unless another action becomes rewarded later.
            if r == 1.0:
                wm_action[s] = a
                # Confidence saturates toward p_in (availability) and recent success
                wm_conf[s] = max(wm_conf[s], 0.5 + 0.5 * p_in)
            else:
                wm_conf[s] *= 0.5  # penalize incorrect association

    return nll


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + Expected-Value-of-Control (EVC) arbitration for WM usage with age- and load-dependent effort.

    The model combines an RL system with a WM system that stores the last rewarded action per state
    with a confidence that decays. An arbitration controller decides how much to rely on WM by
    comparing expected accuracy benefit to an effort cost that increases with set size and age.

    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..nS-1 within a block).
    actions : array-like of int
        Chosen action on each trial (0..2). Trials with out-of-range actions are ignored.
    rewards : array-like of float
        Reward on each trial (typically 0 or 1). Trials with rewards not in {0,1} are ignored.
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size (number of states) for each trial/block.
    age : array-like (single value)
        Participant age in years; forms an age group (older >= 45).
    model_parameters : list or tuple of floats
        [alpha, beta, effort_cost, benefit_sens, age_cost]
        - alpha: RL learning rate (0..1).
        - beta: inverse temperature for RL softmax (>0).
        - effort_cost: baseline subjective cost of engaging WM control (>=0).
        - benefit_sens: sensitivity converting accuracy gain into utility (>=0).
        - age_cost: multiplies WM effort for older adults; younger get slight discount (>=0).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, effort_cost, benefit_sens, age_cost = model_parameters
    is_old = 1.0 if age[0] >= 45 else 0.0
    eps = 1e-12
    nll = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # RL Q-values
        Q = np.zeros((nS, nA))

        # WM memory: last rewarded action with confidence per state
        wm_action = -np.ones(nS, dtype=int)
        wm_conf = np.zeros(nS)

        # Effort scales with set size and age
        load_level = float(nS) / 6.0  # normalized load in [0.5, 1] for nS in {3,6}
        age_eff = (1.0 + age_cost * is_old) * (1.0 - 0.1 * (1.0 - is_old) * age_cost)
        cost_eff = effort_cost * load_level * age_eff

        for t in range(len(block_states)):
            a = int(block_actions[t])
            r = float(block_rewards[t])
            s = int(block_states[t])

            if not (0 <= s < nS) or not (0 <= a < nA) or (r not in [0.0, 1.0]):
                continue

            # Construct WM policy distribution
            if wm_action[s] >= 0:
                # Use confidence to interpolate between a delta on stored action and uniform
                wm_probs = np.ones(nA) * (1.0 - wm_conf[s]) / (nA - 1)
                wm_probs[wm_action[s]] = wm_conf[s]
            else:
                wm_probs = np.ones(nA) / nA

            # RL policy
            q_s = Q[s, :]
            q_cent = q_s - np.max(q_s)
            p_rl = np.exp(beta * q_cent)
            p_rl = p_rl / (np.sum(p_rl) + eps)

            # EVC arbitration:
            # Expected accuracy benefit of WM over RL is the L1 gain toward the most probable action.
            acc_rl = np.max(p_rl)
            acc_wm = np.max(wm_probs)
            benefit = max(0.0, acc_wm - acc_rl)

            utility = benefit_sens * benefit - cost_eff
            # Map utility to weight in [0,1]
            w = 1.0 / (1.0 + np.exp(-5.0 * utility))  # fixed slope to keep params <= 5
            w = np.clip(w, 0.0, 1.0)

            p = w * wm_probs + (1.0 - w) * p_rl
            nll -= np.log(max(p[a], eps))

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += np.clip(alpha, 0.0, 1.0) * pe

            # WM update: reward writes memory with high confidence; nonreward decays confidence
            if r == 1.0:
                wm_action[s] = a
                # Confidence increases but is bounded; slightly higher under low load
                wm_conf[s] = min(1.0, 0.6 + 0.3 * (3.0 / max(1.0, float(nS))) + 0.1 * wm_conf[s])
            else:
                wm_conf[s] *= 0.5

    return nll