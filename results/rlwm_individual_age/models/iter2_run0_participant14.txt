def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Asymmetric RL with set-size- and age-modulated learning rates and action perseveration.

    Mechanism:
    - Model-free Q-learning with separate learning rates for positive vs negative prediction errors (PE).
    - Asymmetry parameter scales the difference between positive and negative learning rates.
    - Effective learning rate is reduced under higher set size (interference) with a load sensitivity that is
      stronger for older adults than younger adults.
    - Action perseveration: bias toward repeating the last action taken in the same state.

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..set_size-1), resets per block.
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of {0,1}
        Outcome per trial.
    blocks : array-like of int
        Block index per trial; learning resets per block.
    set_sizes : array-like of int
        Set size per trial (constant within a block; either 3 or 6).
    age : array-like or scalar
        Participant age; younger < 45, older >= 45.
    model_parameters : iterable of 5 floats
        alpha_base : baseline learning rate (0..1).
        asym : valence asymmetry (-1..1). Positive => faster learning from positive PE, slower from negative PE.
        beta : softmax inverse temperature (>0).
        kappa : perseveration weight; added to the chosen action preference in the same state (>=0).
        load_sens : how strongly set size reduces learning; scaled by age (>=0).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha_base, asym, beta, kappa, load_sens = model_parameters
    age_val = age[0] if hasattr(age, "__len__") else age
    is_young = 1.0 if age_val < 45 else 0.0
    is_old = 1.0 - is_young

    nA = 3
    total_logp = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        b_states = states[mask].astype(int)
        b_actions = actions[mask].astype(int)
        b_rewards = rewards[mask].astype(float)
        b_set_sizes = set_sizes[mask].astype(int)

        nS = int(b_set_sizes[0])

        # Initialize Q and last-action memory for perseveration
        Q = (1.0 / nA) * np.ones((nS, nA))
        last_action = -1 * np.ones(nS, dtype=int)  # -1 means no previous action in this state yet

        # Set-size interference on learning; older adults suffer more
        load_factor = 1.0 + load_sens * max(0, nS - 3) * (0.8 * is_young + 1.2 * is_old)
        alpha_eff_base = alpha_base / load_factor

        # Construct valence-specific learning rates
        alpha_pos = np.clip(alpha_eff_base * (1.0 + asym), 0.0, 1.0)
        alpha_neg = np.clip(alpha_eff_base * (1.0 - asym), 0.0, 1.0)

        for t in range(len(b_states)):
            s = b_states[t]
            a = b_actions[t]
            r = b_rewards[t]

            # RL softmax with perseveration bias for repeating prior action in this state
            pref = beta * (Q[s, :] - np.max(Q[s, :]))
            if last_action[s] >= 0:
                pref[last_action[s]] += kappa
            p_vec = np.exp(pref)
            p_vec /= np.sum(p_vec)

            p_choice = max(p_vec[a], 1e-12)
            total_logp += np.log(p_choice)

            # Update Q with asymmetric learning rate
            pe = r - Q[s, a]
            lr = alpha_pos if pe >= 0.0 else alpha_neg
            Q[s, a] += lr * pe

            # Update perseveration memory
            last_action[s] = a

    return -total_logp


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + slot-limited Working Memory (WM) gate with age- and set-size-dependent availability,
    intrusion errors, and a small lapse.

    Mechanism:
    - Model-free RL maintains Q-values per state-action.
    - WM holds the most recently rewarded action per state (if any).
    - WM availability is probabilistic and limited by the number of slots relative to set size:
        p_avail = min(1, slots_eff / set_size).
      Younger adults have more effective slots; older adults fewer.
    - WM policy: when available and the state has a stored action, choose it with probability (1 - intrusion);
      with probability intrusion, the WM policy picks uniformly among the other actions.
      If no stored action exists, WM policy is uniform.
    - Arbitration by availability: final policy is a mixture
        p = (1 - lapse) * [ p_avail * p_wm + (1 - p_avail) * p_rl ] + lapse * uniform.

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..set_size-1).
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of {0,1}
        Outcome per trial.
    blocks : array-like of int
        Block index per trial; learning resets per block.
    set_sizes : array-like of int
        Set size per trial (constant within a block; either 3 or 6).
    age : array-like or scalar
        Participant age; younger <45, older >=45; modulates effective slots.
    model_parameters : iterable of 5 floats
        alpha : RL learning rate (0..1).
        beta : RL softmax inverse temperature (>0).
        slots_base : WM slots available at baseline set size for a middle adult (>0).
        intrusion : probability that WM retrieves an incorrect action when available (0..1).
        lapse : probability of random choice (0..0.2).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, slots_base, intrusion, lapse = model_parameters
    age_val = age[0] if hasattr(age, "__len__") else age
    is_young = 1.0 if age_val < 45 else 0.0
    is_old = 1.0 - is_young

    nA = 3
    total_logp = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        b_states = states[mask].astype(int)
        b_actions = actions[mask].astype(int)
        b_rewards = rewards[mask].astype(float)
        b_set_sizes = set_sizes[mask].astype(int)

        nS = int(b_set_sizes[0])

        # RL initialization
        Q = (1.0 / nA) * np.ones((nS, nA))

        # WM store: -1 indicates no stored rewarded action yet
        wm_store = -1 * np.ones(nS, dtype=int)

        # Effective slots: more for young, fewer for old
        slots_eff = slots_base * (1.2 * is_young + 0.8 * is_old)
        # Availability probability capped by set size
        p_avail = min(1.0, slots_eff / max(1, nS))

        for t in range(len(b_states)):
            s = b_states[t]
            a = b_actions[t]
            r = b_rewards[t]

            # RL policy
            pref_rl = beta * (Q[s, :] - np.max(Q[s, :]))
            p_rl = np.exp(pref_rl)
            p_rl /= np.sum(p_rl)

            # WM policy
            if wm_store[s] >= 0:
                correct_a = wm_store[s]
                p_wm = np.ones(nA) * (intrusion / (nA - 1))
                p_wm[correct_a] = 1.0 - intrusion
            else:
                p_wm = np.ones(nA) / nA

            # Arbitration: availability-gated mixture + lapse
            p_mix = p_avail * p_wm[a] + (1.0 - p_avail) * p_rl[a]
            p_choice = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            p_choice = max(p_choice, 1e-12)
            total_logp += np.log(p_choice)

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # WM update: store action only upon reward
            if r > 0.5:
                wm_store[s] = a

    return -total_logp


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with directed exploration via UCB-like uncertainty bonus modulated by age and set size.

    Mechanism:
    - Standard Q-learning updates expected values.
    - Choice policy uses a softmax over augmented preferences: Q + exploration_bonus.
    - Exploration bonus is w * age_scale * ss_scale / sqrt(N[s,a]+1), promoting actions tried less often.
      Younger adults receive stronger directed exploration than older adults; larger set size dampens bonus.

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..set_size-1).
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of {0,1}
        Outcome per trial.
    blocks : array-like of int
        Block index per trial; learning resets per block.
    set_sizes : array-like of int
        Set size per trial (constant within a block; either 3 or 6).
    age : array-like or scalar
        Participant age; younger <45, older >=45; modulates exploration.
    model_parameters : iterable of 5 floats
        alpha : RL learning rate (0..1).
        beta : softmax inverse temperature (>0).
        ucb_weight : base weight for uncertainty bonus (>=0).
        uncertainty_decay : reduction of bonus with larger set sizes (0..1).
        age_uncertainty_scale : scales exploration bonus up for young and down for old (>=0).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, ucb_weight, uncertainty_decay, age_uncertainty_scale = model_parameters
    age_val = age[0] if hasattr(age, "__len__") else age
    is_young = 1.0 if age_val < 45 else 0.0
    is_old = 1.0 - is_young

    nA = 3
    total_logp = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        b_states = states[mask].astype(int)
        b_actions = actions[mask].astype(int)
        b_rewards = rewards[mask].astype(float)
        b_set_sizes = set_sizes[mask].astype(int)

        nS = int(b_set_sizes[0])

        # Initialize Q and counts per state-action
        Q = (1.0 / nA) * np.ones((nS, nA))
        N = np.zeros((nS, nA))  # visit counts

        # Set-size scaling of exploration: higher set size reduces bonus
        ss_scale = 1.0 - uncertainty_decay * max(0, nS - 3) / max(1, (6 - 3))
        ss_scale = max(0.0, ss_scale)

        # Age scaling: larger for young, smaller for old
        age_scale = (1.0 + age_uncertainty_scale * is_young) / (1.0 + 0.5 * age_uncertainty_scale * is_old)

        for t in range(len(b_states)):
            s = b_states[t]
            a = b_actions[t]
            r = b_rewards[t]

            # Uncertainty bonus: inverse sqrt of visits
            bonus = ucb_weight * age_scale * ss_scale / np.sqrt(N[s, :] + 1.0)

            # Softmax over augmented values
            pref = beta * ((Q[s, :] + bonus) - np.max(Q[s, :] + bonus))
            p_vec = np.exp(pref)
            p_vec /= np.sum(p_vec)

            p_choice = max(p_vec[a], 1e-12)
            total_logp += np.log(p_choice)

            # Update
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe
            N[s, a] += 1.0

    return -total_logp