def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Associability-modulated Q-learning with age- and load-dependent perseveration.

    The model uses:
    - A per-state associability (Pearceâ€“Hall style) that scales the effective learning rate by recent unsigned
      prediction errors and is further modulated by set size.
    - Choice perseveration (stickiness) that is modulated by age group and set size.
    - A softmax decision rule.

    Parameters
    ----------
    states : 1D array-like of int
        State index on each trial within its block (0..nS-1 inside block).
    actions : 1D array-like of int
        Observed chosen action on each trial (0..2).
    rewards : 1D array-like of {0,1}
        Feedback received after each choice.
    blocks : 1D array-like of int
        Block index for each trial.
    set_sizes : 1D array-like of int
        The number of distinct states in the current block (3 or 6).
    age : 1D array-like (length 1)
        Participant age; used to define age group (younger <45, older >=45).
    model_parameters : iterable of length 6
        [alpha0, kappa, beta0, eta_size, stick_base, stick_age_shift]
        - alpha0: baseline learning-rate gain passed through a sigmoid to keep it in (0,1).
        - kappa: associability update rate toward unsigned PE (0..1).
        - beta0: baseline inverse temperature; scaled by 10 internally.
        - eta_size: set-size modulation of the effective learning rate (via a logistic gate).
        - stick_base: baseline perseveration strength added to the previous action in the same state.
        - stick_age_shift: age-group modulation of perseveration; positive values increase stickiness for younger.

    Returns
    -------
    float
        Negative log-likelihood of the observed actions.
    """
    alpha0, kappa, beta0, eta_size, stick_base, stick_age_shift = model_parameters
    nA = 3
    age_val = age[0]
    age_group = 1 if age_val >= 45 else 0  # 1=older, 0=younger

    total_logp = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        b_states = states[idx].astype(int)
        b_actions = actions[idx].astype(int)
        b_rewards = rewards[idx].astype(float)
        b_set_sizes = set_sizes[idx].astype(int)
        nS = int(b_set_sizes[0])

        Q = np.zeros((nS, nA))
        A = np.ones(nS)  # associability per state
        last_act = -1 * np.ones(nS, dtype=int)  # previous action per state

        # Effective inverse temperature
        beta_eff = 10.0 * beta0
        # Perseveration strength: age- and load-modulated
        load_level = (nS - 3) / 3.0  # 0 for set size 3, 1 for set size 6
        stick_eff = stick_base + stick_age_shift * (0.5 - age_group) - 0.5 * load_level

        # Learning-rate gate as a logistic of alpha0 and set size
        # lr_base in (0,1)
        lr_base = 1.0 / (1.0 + np.exp(-(alpha0 + eta_size * load_level)))

        for t in range(len(b_states)):
            s = b_states[t]
            a = b_actions[t]
            r = b_rewards[t]

            # Policy with perseveration on last action in this state
            logits = beta_eff * Q[s, :].copy()
            if last_act[s] >= 0:
                logits[last_act[s]] += stick_eff
            logits = logits - np.max(logits)
            pol = np.exp(logits)
            pol = pol / np.sum(pol)

            p_a = np.clip(pol[a], 1e-12, 1.0)
            total_logp += np.log(p_a)

            # TD update with associability-scaled learning rate
            pe = r - Q[s, a]
            alpha_eff = np.clip(lr_base * np.clip(A[s], 0.0, 1.0), 0.0, 1.0)
            Q[s, a] += alpha_eff * pe

            # Update associability toward |PE|
            A[s] = (1.0 - kappa) * A[s] + kappa * abs(pe)
            A[s] = np.clip(A[s], 0.0, 1.0)

            # Update last action for this state
            last_act[s] = a

    return -float(total_logp)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Count-based uncertainty bonus with age-modulated directed exploration and load-dependent interference.

    The model uses:
    - Standard Q-learning for values.
    - A directed exploration bonus proportional to epistemic uncertainty U(s,a) = 1/sqrt(N+1),
      where N is the visit count per state-action. The bonus weight is modulated by age group.
    - Load-dependent interference that decays Q-values upon each state visit, simulating memory load.
    - Choice perseveration that weakens under higher load.

    Parameters
    ----------
    states : 1D array-like of int
        State index on each trial within its block (0..nS-1 inside block).
    actions : 1D array-like of int
        Observed chosen action on each trial (0..2).
    rewards : 1D array-like of {0,1}
        Feedback received after each choice.
    blocks : 1D array-like of int
        Block index for each trial.
    set_sizes : 1D array-like of int
        The number of distinct states in the current block (3 or 6).
    age : 1D array-like (length 1)
        Participant age; used to define age group (younger <45, older >=45).
    model_parameters : iterable of length 6
        [alpha, beta0, bonus0, age_bonus_shift, size_forget, perseveration]
        - alpha: learning rate for Q-learning.
        - beta0: baseline inverse temperature; scaled by 10 internally.
        - bonus0: baseline directed-exploration bonus weight.
        - age_bonus_shift: modulation of the bonus by age group; positive favors younger.
                           bonus_eff = bonus0 + age_bonus_shift*(0.5 - age_group).
        - size_forget: amount of Q decay per state-visit that increases with set size.
                       decay_rate = clip(size_forget * load_level, 0, 0.5).
        - perseveration: baseline choice stickiness that weakens with load.

    Returns
    -------
    float
        Negative log-likelihood of the observed actions.
    """
    alpha, beta0, bonus0, age_bonus_shift, size_forget, perseveration = model_parameters
    nA = 3
    age_val = age[0]
    age_group = 1 if age_val >= 45 else 0  # 1=older, 0=younger

    total_logp = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        b_states = states[idx].astype(int)
        b_actions = actions[idx].astype(int)
        b_rewards = rewards[idx].astype(float)
        b_set_sizes = set_sizes[idx].astype(int)
        nS = int(b_set_sizes[0])

        Q = np.zeros((nS, nA))
        N = np.zeros((nS, nA))  # visit counts
        last_act = -1 * np.ones(nS, dtype=int)

        beta_eff = 10.0 * beta0
        load_level = (nS - 3) / 3.0  # 0 or 1
        bonus_eff = bonus0 + age_bonus_shift * (0.5 - age_group)
        pers_eff = perseveration * (1.0 - 0.5 * load_level)
        decay_rate = np.clip(size_forget * load_level, 0.0, 0.5)

        for t in range(len(b_states)):
            s = b_states[t]
            a = b_actions[t]
            r = b_rewards[t]

            # Apply load-dependent decay to Q in visited state (interference)
            Q[s, :] *= (1.0 - decay_rate)

            # Uncertainty bonus
            U = 1.0 / np.sqrt(N[s, :] + 1.0)

            # Policy with directed exploration and perseveration
            logits = beta_eff * Q[s, :] + bonus_eff * U
            if last_act[s] >= 0:
                logits[last_act[s]] += pers_eff
            logits = logits - np.max(logits)
            pol = np.exp(logits)
            pol = pol / np.sum(pol)

            p_a = np.clip(pol[a], 1e-12, 1.0)
            total_logp += np.log(p_a)

            # Update counts and Q
            N[s, a] += 1.0
            Q[s, a] += alpha * (r - Q[s, a])

            last_act[s] = a

    return -float(total_logp)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Dual-process mixture: Q-learning and WSLS heuristic with age- and load-modulated arbitration and RL decay.

    The model uses:
    - A standard Q-learning controller with softmax choice.
    - A win-stay/lose-shift (WSLS) heuristic defined per state:
        If previous trial in the same state was rewarded, repeat that action;
        otherwise, choose uniformly among the two other actions.
      When no previous trial for that state exists, the WSLS policy is uniform.
    - An arbitration weight w between WSLS and RL that depends on age group and set size:
        w = sigmoid(mix0 + mix_age_shift*(0.5 - age_group) - 2*load_level).
    - Load-dependent decay of Q-values upon each state visit to capture interference.
    - A small baseline bias toward action 0 to capture motor/response tendencies,
      whose impact weakens with load.

    Parameters
    ----------
    states : 1D array-like of int
        State index on each trial within its block (0..nS-1 inside block).
    actions : 1D array-like of int
        Observed chosen action on each trial (0..2).
    rewards : 1D array-like of {0,1}
        Feedback received after each choice.
    blocks : 1D array-like of int
        Block index for each trial.
    set_sizes : 1D array-like of int
        The number of distinct states in the current block (3 or 6).
    age : 1D array-like (length 1)
        Participant age; used to define age group (younger <45, older >=45).
    model_parameters : iterable of length 6
        [alpha, beta0, mix0, mix_age_shift, decay_size, bias0]
        - alpha: learning rate for Q-learning.
        - beta0: baseline inverse temperature for RL; scaled by 10 internally.
        - mix0: baseline arbitration bias toward WSLS (logit space).
        - mix_age_shift: age-group modulation of arbitration; positive favors younger.
        - decay_size: magnitude of RL value decay that increases with set size.
        - bias0: baseline bias added to action 0 in RL logits, attenuated by load.

    Returns
    -------
    float
        Negative log-likelihood of the observed actions.
    """
    alpha, beta0, mix0, mix_age_shift, decay_size, bias0 = model_parameters
    nA = 3
    age_val = age[0]
    age_group = 1 if age_val >= 45 else 0  # 1=older, 0=younger

    total_logp = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        b_states = states[idx].astype(int)
        b_actions = actions[idx].astype(int)
        b_rewards = rewards[idx].astype(float)
        b_set_sizes = set_sizes[idx].astype(int)
        nS = int(b_set_sizes[0])

        Q = np.zeros((nS, nA))
        last_act = -1 * np.ones(nS, dtype=int)
        last_rew = np.zeros(nS)  # last reward observed in this state (0/1), meaningful if last_act>=0

        load_level = (nS - 3) / 3.0
        beta_eff = 10.0 * beta0
        # Arbitration weight toward WSLS (0..1)
        mix_logit = mix0 + mix_age_shift * (0.5 - age_group) - 2.0 * load_level
        w_wsls = 1.0 / (1.0 + np.exp(-mix_logit))
        # RL decay per visit
        decay_rate = np.clip(decay_size * load_level, 0.0, 0.5)
        # Bias toward action 0 in RL logits, attenuated by load
        bias_eff = bias0 * (1.0 - 0.5 * load_level)

        for t in range(len(b_states)):
            s = b_states[t]
            a = b_actions[t]
            r = b_rewards[t]

            # Load-dependent decay on RL values for visited state
            Q[s, :] *= (1.0 - decay_rate)

            # RL policy with softmax and action-0 bias
            rl_logits = beta_eff * Q[s, :].copy()
            rl_logits[0] += bias_eff
            rl_logits = rl_logits - np.max(rl_logits)
            rl_pol = np.exp(rl_logits)
            rl_pol = rl_pol / np.sum(rl_pol)

            # WSLS policy for this state
            if last_act[s] >= 0:
                wsls = np.ones(nA) / nA
                if last_rew[s] > 0.5:
                    wsls = np.zeros(nA)
                    wsls[last_act[s]] = 1.0
                else:
                    wsls = np.zeros(nA)
                    others = [i for i in range(nA) if i != last_act[s]]
                    for o in others:
                        wsls[o] = 0.5
            else:
                wsls = np.ones(nA) / nA

            # Mixture policy
            pol = w_wsls * wsls + (1.0 - w_wsls) * rl_pol
            pol = np.clip(pol, 1e-12, 1.0)
            pol = pol / np.sum(pol)

            p_a = np.clip(pol[a], 1e-12, 1.0)
            total_logp += np.log(p_a)

            # RL update
            Q[s, a] += alpha * (r - Q[s, a])

            # Update WSLS memory traces
            last_act[s] = a
            last_rew[s] = r

    return -float(total_logp)