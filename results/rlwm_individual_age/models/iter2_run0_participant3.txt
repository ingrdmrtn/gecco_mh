def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + Bayesian working memory (Dirichlet) with age- and load-dependent decay and mixture control.

    Mechanism:
    - Two systems contribute to choice:
      1) Model-free RL (Q-learning) updated from reward prediction errors.
      2) Bayesian WM: for each state, maintains a Dirichlet over the 3 actions that represent
         the probability that each action is the correct mapping for that state. Counts decay
         with set size and age (older adults decay faster).
    - A mixing weight blends RL and WM policies. The weight depends on a base term and on
      WM "reliability" (how peaked the Dirichlet is), and is attenuated in older adults per parameter.

    Parameters (model_parameters):
    - alpha: RL learning rate (0..1)
    - beta: inverse temperature for RL softmax (>0)
    - decay_dir: base decay multiplier for WM Dirichlet counts per trial (0..1)
    - mix0: base mixture bias towards WM (real; passed through logistic to [0,1])
    - age_gain: scales the effect of being older on both WM decay and mixing (>=0)

    Inputs:
    - states: array of int, state index on each trial (0..set_size-1 within block)
    - actions: array of int, chosen action on each trial (0,1,2). Out-of-range treated as lapses (uniform).
    - rewards: array numeric; <=0 mapped to 0, >0 to 1
    - blocks: array of int, block index for each trial (learning resets between blocks)
    - set_sizes: array of int, set size for each trial (3 or 6)
    - age: array-like with a single element, participant age in years
    - model_parameters: list/tuple of 5 floats [alpha, beta, decay_dir, mix0, age_gain]

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    alpha, beta, decay_dir, mix0, age_gain = model_parameters
    is_older = 1.0 if age[0] >= 45 else 0.0

    nll = 0.0
    eps = 1e-12
    unique_blocks = np.unique(blocks)

    # logistic function for mixing
    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    for b in unique_blocks:
        mask = (blocks == b)
        b_states = states[mask].astype(int)
        b_actions = actions[mask].astype(int)
        b_rewards = np.array([1.0 if r > 0 else 0.0 for r in rewards[mask]])
        b_set_sizes = set_sizes[mask].astype(int)

        nA = 3
        nS = int(b_set_sizes[0])

        # Initialize RL and WM
        Q = np.zeros((nS, nA))
        # Dirichlet counts start symmetric with small prior
        C = np.ones((nS, nA)) * 1.0

        for t in range(len(b_states)):
            s = int(b_states[t])
            a = int(b_actions[t])
            r = float(b_rewards[t])
            load = max(1, int(b_set_sizes[t]))

            # Age- and load-dependent decay of WM counts
            # More load and older age -> more decay per trial
            decay = np.clip(decay_dir * (load / 6.0) * (1.0 + age_gain * is_older), 0.0, 1.0)
            C[s, :] = (1.0 - decay) * C[s, :] + decay * 1.0  # shrink towards symmetric prior

            # RL policy
            q_s = Q[s, :]
            q_s = q_s - np.max(q_s)
            p_rl = np.exp(beta * q_s)
            p_rl = p_rl / (np.sum(p_rl) + eps)

            # WM (Dirichlet-MAP) policy: probability proportional to counts
            wm_counts = C[s, :]
            p_wm = wm_counts / (np.sum(wm_counts) + eps)

            # WM reliability: how peaked the distribution is vs uniform
            # 0 when uniform, 1 when a single action dominates
            peak = np.max(p_wm)
            reliability = (peak - 1.0 / nA) / (1.0 - 1.0 / nA)
            reliability = np.clip(reliability, 0.0, 1.0)

            # Base mixing transformed by logistic; attenuated by older age via age_gain
            base_mix = sigmoid(mix0)
            age_scale = 1.0 / (1.0 + age_gain * is_older)  # older -> smaller WM weight
            wm_weight = np.clip(base_mix * reliability * age_scale, 0.0, 1.0)

            # Final policy
            p = wm_weight * p_wm + (1.0 - wm_weight) * p_rl

            # Likelihood and handling invalid actions (treated as uniform lapses)
            if a < 0 or a >= nA:
                p_a = 1.0 / nA
                nll -= np.log(max(p_a, eps))
            else:
                p_a = p[a]
                nll -= np.log(max(p_a, eps))

            # RL update
            if 0 <= a < nA:
                delta = r - Q[s, a]
                Q[s, a] += alpha * delta

            # WM Dirichlet update: reward strengthens chosen mapping; error distributes mass to others
            if 0 <= a < nA:
                if r > 0.0:
                    C[s, a] += 1.0
                else:
                    # if incorrect, mildly increase alternatives
                    for aa in range(nA):
                        if aa != a:
                            C[s, aa] += 0.5

    return nll


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with load- and age-modulated action stickiness and state-action congruence bias.

    Mechanism:
    - Choices follow a softmax over a linear combination of:
      1) Model-free Q-values.
      2) Action stickiness: a bias to repeat the last action in the block (perseveration),
         amplified by set size and age.
      3) A heuristic "state-action congruence" bias: when actions share an index with the state
         modulo the action-space size (3), participants may "match" indices under load; this bias
         scales with the same stickiness parameter to limit total parameters.

    Parameters (model_parameters):
    - alpha: RL learning rate (0..1)
    - beta: inverse temperature (>0)
    - nu_stick: base strength of perseveration and congruence biases
    - k_load: exponent controlling how strongly load (set size) scales the biases
    - k_age: multiplicative factor (>0) for older group relative to younger (older -> larger biases)

    Inputs:
    - states: array of int, state index on each trial (0..set_size-1 within block)
    - actions: array of int, chosen action on each trial (0,1,2). Out-of-range treated as uniform.
    - rewards: array numeric; <=0 mapped to 0, >0 to 1
    - blocks: array of int, block index (resets Q and last action)
    - set_sizes: array of int, set size for each trial (3 or 6)
    - age: array-like with a single element, participant age in years
    - model_parameters: list/tuple of 5 floats [alpha, beta, nu_stick, k_load, k_age]

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    alpha, beta, nu_stick, k_load, k_age = model_parameters
    is_older = 1.0 if age[0] >= 45 else 0.0

    nll = 0.0
    eps = 1e-12
    unique_blocks = np.unique(blocks)

    for b in unique_blocks:
        mask = (blocks == b)
        b_states = states[mask].astype(int)
        b_actions = actions[mask].astype(int)
        b_rewards = np.array([1.0 if r > 0 else 0.0 for r in rewards[mask]])
        b_set_sizes = set_sizes[mask].astype(int)

        nA = 3
        nS = int(b_set_sizes[0])

        Q = np.zeros((nS, nA))
        last_action = -1  # no previous action at block start

        for t in range(len(b_states)):
            s = int(b_states[t])
            a = int(b_actions[t])
            r = float(b_rewards[t])
            load = max(1, int(b_set_sizes[t]))

            # Load- and age-dependent bias gain
            load_gain = (float(load) / 3.0) ** np.clip(k_load, 0.0, 5.0)
            age_gain = 1.0 + k_age * is_older
            bias_gain = nu_stick * load_gain * age_gain

            # Construct bias vector
            bias = np.zeros(nA)

            # Perseveration: favor repeating last_action
            if 0 <= last_action < nA:
                bias[last_action] += bias_gain

            # Congruence heuristic: match action index to state modulo 3
            congruent_action = s % nA
            bias[congruent_action] += 0.5 * bias_gain  # smaller than stickiness but coupled

            # Softmax over beta*Q + bias
            logits = beta * Q[s, :] + bias
            logits = logits - np.max(logits)
            p = np.exp(logits)
            p = p / (np.sum(p) + eps)

            # Likelihood and update last_action
            if a < 0 or a >= nA:
                p_a = 1.0 / nA
                nll -= np.log(max(p_a, eps))
                last_action = -1  # treat as lapse; reset stickiness anchor
            else:
                p_a = p[a]
                nll -= np.log(max(p_a, eps))
                last_action = a

            # RL update
            if 0 <= a < nA:
                delta = r - Q[s, a]
                Q[s, a] += alpha * delta

    return nll


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Adaptive meta-control RL: learning rate and decision temperature adapt to recent performance,
    with age- and load-dependent forgetting.

    Mechanism:
    - Model-free Q-learning with:
      1) Per-trial global forgetting of Q-values, stronger under higher load and for older adults.
      2) A moving accuracy trace within each block (exponential), which increases inverse temperature
         beta when recent performance is high and decreases it when low (meta-control).
      3) Learning rate is reduced under higher load and for older adults, reflecting reduced effective updating.

    Parameters (model_parameters):
    - alpha0: base learning rate (0..1)
    - beta0: base inverse temperature (>0)
    - k_meta: sensitivity of decision temperature to recent accuracy (real)
    - rho_forget: base forgetting rate per trial (0..1)
    - age_beta_shift: additional reduction of beta for older group (>=0)

    Inputs:
    - states: array of int, state index on each trial (0..set_size-1 within block)
    - actions: array of int, chosen action on each trial (0,1,2). Out-of-range treated as uniform.
    - rewards: array numeric; <=0 mapped to 0, >0 to 1
    - blocks: array of int, block index (resets learning across blocks)
    - set_sizes: array of int, set size for each trial (3 or 6)
    - age: array-like with a single element, participant age in years
    - model_parameters: list/tuple of 5 floats [alpha0, beta0, k_meta, rho_forget, age_beta_shift]

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    alpha0, beta0, k_meta, rho_forget, age_beta_shift = model_parameters
    is_older = 1.0 if age[0] >= 45 else 0.0

    nll = 0.0
    eps = 1e-12
    unique_blocks = np.unique(blocks)

    for b in unique_blocks:
        mask = (blocks == b)
        b_states = states[mask].astype(int)
        b_actions = actions[mask].astype(int)
        b_rewards = np.array([1.0 if r > 0 else 0.0 for r in rewards[mask]])
        b_set_sizes = set_sizes[mask].astype(int)

        nA = 3
        nS = int(b_set_sizes[0])

        Q = np.zeros((nS, nA))
        acc_trace = 0.5  # start neutral
        trace_alpha = 0.3  # fixed smoothing for the moving accuracy (not a free parameter)

        for t in range(len(b_states)):
            s = int(b_states[t])
            a = int(b_actions[t])
            r = float(b_rewards[t])
            load = max(1, int(b_set_sizes[t]))

            # Age- and load-dependent forgetting applied to whole Q-table
            forget_rate = np.clip(rho_forget * (float(load) / 6.0) * (1.0 + 0.5 * is_older), 0.0, 1.0)
            Q *= (1.0 - forget_rate)

            # Effective learning rate decreases with load and age
            alpha_eff = alpha0 / (1.0 + 0.5 * (load - 3.0) + 0.5 * is_older)
            alpha_eff = np.clip(alpha_eff, 0.0, 1.0)

            # Meta-control of beta based on recent accuracy; older adults' beta reduced
            beta_eff = beta0 * np.exp(k_meta * (acc_trace - 0.5)) / (1.0 + age_beta_shift * is_older)
            # Additional small load penalty on beta
            beta_eff = beta_eff / (1.0 + 0.2 * (load - 3.0))
            beta_eff = max(beta_eff, 1e-3)

            # Softmax policy
            q_s = Q[s, :]
            q_s = q_s - np.max(q_s)
            p = np.exp(beta_eff * q_s)
            p = p / (np.sum(p) + eps)

            # Likelihood
            if a < 0 or a >= 3:
                p_a = 1.0 / 3.0
                nll -= np.log(max(p_a, eps))
                correct = 0.0
            else:
                p_a = p[a]
                nll -= np.log(max(p_a, eps))
                correct = r  # in this task reward encodes correctness

            # Q-learning update
            if 0 <= a < 3:
                delta = r - Q[s, a]
                Q[s, a] += alpha_eff * delta

            # Update accuracy trace
            acc_trace = (1.0 - trace_alpha) * acc_trace + trace_alpha * correct

    return nll