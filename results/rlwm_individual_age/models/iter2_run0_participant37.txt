def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Exponential-decay Working Memory with load- and age-modulated arbitration.

    Idea:
    - RL learns Q-values via a standard delta rule.
    - WM stores a near-deterministic action for a state after rewarded feedback, but the WM trace decays
      exponentially with time since last successful storage.
    - Arbitration between WM and RL depends on set size (lower WM influence in larger sets) and age group
      (older adults have reduced WM influence).
    - WM decay rate is controlled by a time constant.

    Parameters (tuple): (lr, wm_weight, softmax_beta, wm_decay_tau, gamma)
        lr: RL learning rate (0..1).
        wm_weight: Baseline WM contribution weight (0..1).
        softmax_beta: RL inverse temperature, internally scaled by x10.
        wm_decay_tau: Time constant for WM exponential decay in trials; larger means slower decay (>0).
        gamma: Load sensitivity exponent; WM weight scales as (3/nS)^gamma.

    Returns:
        Negative log-likelihood of the observed choices.

    Age use:
        - Older (>=45) group has reduced WM influence (multiplied by 0.75).
    """
    lr, wm_weight, softmax_beta, wm_decay_tau, gamma = model_parameters
    softmax_beta *= 10.0  # beta has a higher upper bound
    softmax_beta_wm = 50.0  # very deterministic
    age_val = age[0]
    age_scale = 0.75 if age_val >= 45 else 1.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track time since last reliable WM update per state
        time_since_update = np.full(nS, np.inf)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            # RL policy
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # Apply exponential decay toward uniform based on time since last update
            # d = 1 - exp(-dt / tau)
            dt = time_since_update[s] if np.isfinite(time_since_update[s]) else np.inf
            if np.isfinite(dt):
                d = 1.0 - np.exp(-max(dt, 0.0) / max(wm_decay_tau, 1e-6))
                w[s, :] = (1.0 - d) * w[s, :] + d * w_0[s, :]
            # Compute WM softmax policy
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Arbitration: load- and age-modulated WM influence
            load_scale = (3.0 / float(nS)) ** gamma
            eta = wm_weight * load_scale * age_scale
            eta = min(max(eta, 0.0), 1.0)

            p_total = eta * p_wm + (1.0 - eta) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # If rewarded, store a deterministic mapping and reset decay timer
            if r > 0:
                w[s, :] = 0.0
                w[s, a] = 1.0
                time_since_update[s] = 0.0
            else:
                # No reward: increment time since last WM reinforcement
                if np.isfinite(time_since_update[s]):
                    time_since_update[s] += 1.0
                else:
                    time_since_update[s] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Uncertainty-based arbitration: RL-WM mixture weighted by entropy; asymmetric RL learning.

    Idea:
    - RL learns with asymmetric learning rates for positive and negative outcomes.
    - WM stores rewarded state-action pairs deterministically but partially decays on each visit (persist).
    - Arbitration weight depends on WM confidence vs RL uncertainty:
        * WM confidence ~ sharpness of WM distribution (max prob).
        * RL uncertainty ~ entropy of RL softmax policy.
      A sigmoid over (confidence - uncertainty) determines WM weight.
    - Age modulates both the bias toward RL and the WM precision.

    Parameters (tuple): (lr_pos, wm_weight, softmax_beta, lr_neg, k_arbitration)
        lr_pos: RL learning rate for rewards (0..1).
        wm_weight: Baseline bias toward WM in arbitration (used as sigmoid bias).
        softmax_beta: RL inverse temperature, internally scaled by x10.
        lr_neg: RL learning rate for no-reward (0..1).
        k_arbitration: Sensitivity of arbitration sigmoid to (conf - uncert).

    Returns:
        Negative log-likelihood of observed choices.

    Age use:
        - Older (>=45): reduce WM precision (lower effective WM beta) and bias arbitration toward RL
          by reducing wm_weight bias by 20%.
    """
    lr_pos, wm_weight, softmax_beta, lr_neg, k_arbitration = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    age_val = age[0]

    # Age effects
    wm_beta_scale = 0.8 if age_val >= 45 else 1.0
    bias_scale = 0.8 if age_val >= 45 else 1.0
    softmax_beta_wm *= wm_beta_scale
    wm_bias = np.log((wm_weight * bias_scale) / max(1.0 - wm_weight * bias_scale, 1e-8))  # logit bias

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # WM persistence factor: each visit nudges WM toward uniform a bit
        wm_persist = 0.1  # small, fixed persistence within this model

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            # RL full softmax distribution for entropy and action prob
            logits_rl = softmax_beta * (Q_s - np.max(Q_s))
            exp_rl = np.exp(logits_rl)
            P_rl = exp_rl / np.sum(exp_rl)
            p_rl = max(P_rl[a], 1e-12)

            # WM distribution
            W_s = w[s, :]
            logits_wm = softmax_beta_wm * (W_s - np.max(W_s))
            exp_wm = np.exp(logits_wm)
            P_wm = exp_wm / np.sum(exp_wm)
            p_wm = max(P_wm[a], 1e-12)

            # Uncertainty (RL entropy) and WM confidence (max prob)
            entropy_rl = -np.sum(P_rl * np.log(np.clip(P_rl, 1e-12, 1.0)))
            wm_conf = np.max(P_wm)

            # Sigmoid arbitration on difference between confidence and uncertainty + bias
            x = (wm_conf - entropy_rl) * k_arbitration + wm_bias
            eta = 1.0 / (1.0 + np.exp(-x))
            # Load penalty: reduce WM weight at larger set sizes
            eta *= (3.0 / float(nS))
            eta = min(max(eta, 0.0), 1.0)

            p_total = eta * p_wm + (1.0 - eta) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update with asymmetric learning
            pe = r - Q_s[a]
            lr = lr_pos if pe >= 0.0 else lr_neg
            q[s, a] += lr * pe

            # WM update: small decay toward uniform each visit
            w[s, :] = (1.0 - wm_persist) * w[s, :] + wm_persist * w_0[s, :]
            if r > 0:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Surprise-gated WM arbitration + load-based RL forgetting + action inertia.

    Idea:
    - RL learns Q-values but also exhibits load-dependent forgetting toward uniform (higher in larger sets).
    - WM stores rewarded mappings deterministically and decays slightly when not rewarded.
    - Arbitration between WM and RL is reduced when RL prediction error magnitude is high (surprise),
      and increased when set size is small; age reduces WM reliance and increases inertia.
    - Action inertia biases the RL policy toward repeating the previous action in the block.

    Parameters (tuple): (lr, wm_weight, softmax_beta, inertia, decay_load_slope, surprise_slope)
        lr: RL learning rate (0..1).
        wm_weight: Baseline WM weight (0..1).
        softmax_beta: RL inverse temperature, internally scaled by x10.
        inertia: Strength added to the previous action's logit in RL policy (>=0).
        decay_load_slope: Amount of RL forgetting per visit from set size (0..1); applied as
                          forget = decay_load_slope * (nS-3)/3 each time a state is visited.
        surprise_slope: Sensitivity of arbitration to prediction error magnitude (>=0);
                        larger values reduce WM weight more under surprise.

    Returns:
        Negative log-likelihood of the observed choices.

    Age use:
        - Older (>=45): WM weight reduced by 25%; inertia increased by 25%.
    """
    lr, wm_weight, softmax_beta, inertia, decay_load_slope, surprise_slope = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    age_val = age[0]

    wm_scale = 0.75 if age_val >= 45 else 1.0
    inertia_scale = 1.25 if age_val >= 45 else 1.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        prev_action = None

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]
            nS_curr = int(block_set_sizes[t])

            # Load-dependent RL forgetting toward uniform
            forget = decay_load_slope * max((nS_curr - 3.0) / 3.0, 0.0)
            if forget > 0.0:
                q[s, :] = (1.0 - forget) * q[s, :] + forget * (1.0 / nA)

            # RL policy with inertia bias on logits
            Q_s = q[s, :].copy()
            if prev_action is not None:
                Q_s[prev_action] += inertia * inertia_scale
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Surprise-based arbitration: lower WM weight when |PE| is high
            Q_sa = q[s, a]
            pe = r - Q_sa
            surprise = abs(pe)
            base_eta = wm_weight * (3.0 / float(nS_curr)) * wm_scale
            # Map surprise in [0,1] to a down-weight via 1 / (1 + slope*surprise)
            eta = base_eta / (1.0 + surprise_slope * surprise)
            eta = min(max(eta, 0.0), 1.0)

            p_total = eta * p_wm + (1.0 - eta) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update
            q[s, a] += lr * (r - q[s, a])

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Slight decay toward uniform each visit; reward writes a one-hot memory
            w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]
            if r > 0:
                w[s, :] = 0.0
                w[s, a] = 1.0

            prev_action = a

        blocks_log_p += log_p

    return -blocks_log_p