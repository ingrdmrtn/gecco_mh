def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Model 1: Model-free RL with age-modulated exploration, set-size-scaled learning, and action perseveration.

    Rationale:
      - Participants learn Q-values via a simple delta rule.
      - Larger set sizes slow learning (reduced effective learning rate).
      - Older adults exhibit more exploratory/variable choices via lower effective inverse temperature.
      - Action perseveration adds a bias toward repeating the previous action within a block (choice stickiness).

    Parameters (model_parameters; total 5):
      - alpha_base: float in (0,1), baseline RL learning rate.
      - beta_base: float > 0, baseline inverse temperature (rescaled internally by 10).
      - kappa_stick: float, perseveration weight added to the previously chosen action.
      - age_temp_shift: float >= 0, increases choice noise for older adults by lowering effective beta.
      - lr_setsize_slope: float >= 0, decreases learning rate as set size increases.

    Inputs:
      - states: 1D int array of state indices per trial.
      - actions: 1D int array of chosen actions (0..2) per trial.
      - rewards: 1D float/int array of rewards (0/1) per trial.
      - blocks: 1D int array of block indices per trial.
      - set_sizes: 1D int array of set size (3 or 6) per trial.
      - age: array-like with a single number; age >= 45 -> older group.
      - model_parameters: sequence of five floats as described.

    Returns:
      - Negative log-likelihood of the observed choices under the model (float).
    """
    alpha_base, beta_base, kappa_stick, age_temp_shift, lr_setsize_slope = model_parameters
    age_val = age[0]
    is_older = 1.0 if age_val >= 45 else 0.0

    nA = 3
    neg_loglik = 0.0

    # Effective inverse temperature: older -> lower beta (more noise)
    # Scale beta up by 10 for identifiability, then reduce with age factor.
    beta_eff_base = max(1e-6, beta_base * 10.0) / (1.0 + is_older * max(0.0, age_temp_shift))

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask].astype(int)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_set_sizes = set_sizes[mask].astype(int)

        nS = int(block_set_sizes[0])
        Q = np.zeros((nS, nA), dtype=float)

        # Track last chosen action per state for perseveration
        last_action = -1 * np.ones(nS, dtype=int)

        for t in range(len(block_states)):
            s = block_states[t]
            a = block_actions[t]
            r = block_rewards[t]
            ss = float(block_set_sizes[t])

            # Set-size scaled learning rate: larger set -> smaller alpha
            alpha_eff = alpha_base / (1.0 + max(0.0, lr_setsize_slope) * max(0.0, ss - 3.0))
            alpha_eff = np.clip(alpha_eff, 1e-6, 1.0)

            # Build softmax logits with perseveration bias
            q_s = Q[s, :].copy()

            stick_bias = np.zeros(nA, dtype=float)
            if last_action[s] >= 0:
                stick_bias[last_action[s]] = kappa_stick

            logits = beta_eff_base * q_s + stick_bias
            # Numerically stable softmax
            logits -= np.max(logits)
            p = np.exp(logits)
            p /= np.sum(p)

            pa = max(1e-12, p[a])
            neg_loglik -= np.log(pa)

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += alpha_eff * pe

            # Update last action
            last_action[s] = a

    return float(neg_loglik)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Model 2: RL + episodic success trace with age- and set-size-modulated recall probability.

    Rationale:
      - A model-free RL learner runs continuously.
      - In parallel, an episodic "success trace" stores the most recently rewarded action per state.
      - On each trial, a recall probability determines mixing between episodic policy and RL softmax.
      - Recall probability decreases with set size and is further reduced in older adults.

    Parameters (model_parameters; total 5):
      - alpha: float in (0,1), RL learning rate.
      - beta: float > 0, RL softmax inverse temperature (rescaled internally by 10).
      - p_recall_base: float in [0,1], baseline probability of retrieving episodic success for a state.
      - age_recall_penalty: float >= 0, multiplicative penalty to recall if older (reduces recall).
      - ss_recall_penalty: float >= 0, multiplicative penalty to recall as set size increases.

    Inputs:
      - states: 1D int array of state indices per trial.
      - actions: 1D int array of chosen actions (0..2) per trial.
      - rewards: 1D float/int array of rewards (0/1) per trial.
      - blocks: 1D int array of block indices per trial.
      - set_sizes: 1D int array of set size (3 or 6) per trial.
      - age: array-like with a single number; age >= 45 -> older group.
      - model_parameters: sequence of five floats as described.

    Returns:
      - Negative log-likelihood of observed choices (float).
    """
    alpha, beta, p_recall_base, age_recall_penalty, ss_recall_penalty = model_parameters
    nA = 3
    beta_eff = max(1e-6, beta * 10.0)
    age_val = age[0]
    is_older = 1.0 if age_val >= 45 else 0.0

    neg_loglik = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask].astype(int)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_set_sizes = set_sizes[mask].astype(int)

        nS = int(block_set_sizes[0])

        # RL values
        Q = np.zeros((nS, nA), dtype=float)

        # Episodic success memory: last rewarded action per state; -1 means none yet
        epi_action = -1 * np.ones(nS, dtype=int)

        for t in range(len(block_states)):
            s = block_states[t]
            a = block_actions[t]
            r = block_rewards[t]
            ss = float(block_set_sizes[t])

            # RL policy
            q_s = Q[s, :].copy()
            q_s -= np.max(q_s)
            p_rl = np.exp(beta_eff * q_s)
            p_rl /= np.sum(p_rl)

            # Episodic policy
            if epi_action[s] >= 0:
                p_epi = np.zeros(nA)
                p_epi[epi_action[s]] = 1.0
            else:
                p_epi = np.ones(nA) / nA

            # Recall probability modulated by age and set size
            # Older and larger set sizes reduce recall multiplicatively.
            recall = p_recall_base
            recall *= 1.0 / (1.0 + is_older * max(0.0, age_recall_penalty))
            recall *= 1.0 / (1.0 + max(0.0, ss_recall_penalty) * max(0.0, ss - 3.0))
            recall = np.clip(recall, 0.0, 1.0)

            mix = recall * p_epi + (1.0 - recall) * p_rl

            pa = max(1e-12, mix[a])
            neg_loglik -= np.log(pa)

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # Episodic success trace updates only on reward
            if r > 0.5:
                epi_action[s] = a

    return float(neg_loglik)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Model 3: Bayesian action-category inference with decay, age-modulated prior strength, and set-size forgetting.

    Rationale:
      - For each state, maintain Dirichlet counts over the 3 actions representing belief about the correct action.
      - Choice uses a softmax over the posterior mean action probabilities (Dirichlet mean) with inverse temperature beta.
      - Evidence decays toward the prior within a block (forgetting), stronger decay for larger set sizes.
      - Older adults rely more on a stronger prior (higher concentration).

    Parameters (model_parameters; total 5):
      - beta: float > 0, softmax inverse temperature on posterior means (rescaled internally by 10).
      - prior_conc_base: float > 0, base symmetric Dirichlet prior concentration per action.
      - age_prior_boost: float >= 0, multiplicative boost to prior concentration for older adults.
      - lambda_forget_base: float in [0,1), per-trial decay rate of evidence toward the prior.
      - ss_forget_scale: float >= 0, increases decay with larger set sizes.

    Inputs:
      - states: 1D int array of state indices per trial.
      - actions: 1D int array of chosen actions (0..2) per trial.
      - rewards: 1D float/int array of rewards (0/1) per trial.
      - blocks: 1D int array of block indices per trial.
      - set_sizes: 1D int array of set size (3 or 6) per trial.
      - age: array-like with a single number; age >= 45 -> older group.
      - model_parameters: sequence of five floats as described.

    Returns:
      - Negative log-likelihood of observed choices (float).
    """
    beta, prior_conc_base, age_prior_boost, lambda_forget_base, ss_forget_scale = model_parameters
    beta_eff = max(1e-6, beta * 10.0)
    age_val = age[0]
    is_older = 1.0 if age_val >= 45 else 0.0

    nA = 3
    neg_loglik = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask].astype(int)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_set_sizes = set_sizes[mask].astype(int)

        nS = int(block_set_sizes[0])

        # Age-modulated prior concentration per action
        prior_c = max(1e-6, prior_conc_base) * (1.0 + is_older * max(0.0, age_prior_boost))
        prior = np.ones((nS, nA), dtype=float) * prior_c
        alphas = prior.copy()  # initialize posterior counts to prior

        for t in range(len(block_states)):
            s = block_states[t]
            a = block_actions[t]
            r = block_rewards[t]
            ss = float(block_set_sizes[t])

            # Posterior mean action probabilities
            alpha_s = alphas[s, :]
            mean_p = alpha_s / np.sum(alpha_s)
            # Softmax on posterior means
            logits = beta_eff * (mean_p - np.max(mean_p))
            p = np.exp(logits)
            p /= np.sum(p)

            pa = max(1e-12, p[a])
            neg_loglik -= np.log(pa)

            # Apply forgetting toward the prior before adding new evidence
            # Effective decay increases with set size
            lam = np.clip(lambda_forget_base * (1.0 + max(0.0, ss_forget_scale) * max(0.0, ss - 3.0)), 0.0, 0.999)
            # Decay counts toward prior: alphas := prior + (alphas - prior) * (1 - lam)
            alphas[s, :] = prior[s, :] + (alphas[s, :] - prior[s, :]) * (1.0 - lam)

            # Add new evidence from current outcome:
            # Reward supports chosen action; no-reward slightly supports alternatives.
            if r >= 0.5:
                alphas[s, a] += 1.0
            else:
                # distribute weak negative evidence to alternatives (keeps counts positive)
                add = 0.5
                for a_alt in range(nA):
                    if a_alt != a:
                        alphas[s, a_alt] += add / (nA - 1)

    return float(neg_loglik)