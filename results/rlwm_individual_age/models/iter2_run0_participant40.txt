def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RLâ€“WM mixture with age- and load-modulated WM reliability and decay.

    The agent combines incremental RL with a lightweight working-memory (WM) store that
    keeps track of the most recently reinforced action per state. WM entries decay with
    a rate that increases with set size and in older adults. The arbitration weight between
    WM and RL is driven by the current WM distinctiveness at the queried state and is
    dampened in older adults and under higher set sizes.

    Parameters
    ----------
    states : np.ndarray of int
        State index at each trial (0..nS-1 within a block).
    actions : np.ndarray of int
        Chosen action at each trial (0..2). If action < 0 it is treated as a lapse/missed response (uniform likelihood).
    rewards : np.ndarray of float
        Feedback at each trial. Values in {0,1}; values < 0 indicate invalid/missing feedback (trial ignored for updating).
    blocks : np.ndarray of int
        Block id per trial.
    set_sizes : np.ndarray of int
        Set size per trial (3 or 6), used to modulate WM decay and arbitration.
    age : np.ndarray of float
        Array containing participant age (use age[0]); age>=45 considered older group.
    model_parameters : sequence of float
        [alpha, beta, wm_gain, decay_base, epsilon]
        - alpha: RL learning rate (0..1).
        - beta: inverse temperature for RL softmax.
        - wm_gain: gain controlling arbitration weight sensitivity to WM distinctiveness.
        - decay_base: base WM decay logit; larger -> faster decay; modulated by age and set size.
        - epsilon: lapse probability (action replaced with uniform random with prob epsilon).

    Returns
    -------
    float
        Negative log-likelihood of observed actions under the model.
    """
    alpha, beta, wm_gain, decay_base, epsilon = model_parameters
    age_val = float(age[0])
    is_older = age_val >= 45.0

    nA = 3
    total_logp = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        b_states = states[idx]
        b_actions = actions[idx]
        b_rewards = rewards[idx]
        b_set_sizes = set_sizes[idx]
        nS = int(b_set_sizes[0])

        # RL values initialized to uniform expectation
        Q = (1.0 / nA) * np.ones((nS, nA))

        # WM strengths per state-action (0..1), start at zero (no memory)
        WM = np.zeros((nS, nA))

        for t in range(len(b_states)):
            s = int(b_states[t])
            a = int(b_actions[t])
            r = float(b_rewards[t])
            ss = int(b_set_sizes[t])

            # RL policy
            q_s = Q[s, :]
            q_center = q_s - np.max(q_s)
            p_rl = np.exp(beta * q_center)
            p_rl = p_rl / np.sum(p_rl)

            # WM policy: softmax over WM strengths (acts like confidence-weighted retrieval)
            wm_temp = 5.0  # fixed steepness for WM readout
            wm_s = WM[s, :]
            wm_center = wm_s - np.max(wm_s)
            p_wm = np.exp(wm_temp * wm_center)
            p_wm = p_wm / np.sum(p_wm)

            # Compute WM arbitration weight based on distinctiveness of WM in this state
            # Distinctiveness = max(WM) - mean(WM); pass through sigmoid with wm_gain.
            distinct = float(np.max(wm_s) - np.mean(wm_s))
            w_raw = 1.0 / (1.0 + np.exp(-wm_gain * distinct))
            # Age and load modulation of WM reliability
            age_scale = 0.7 if is_older else 1.0
            load_scale = 1.0 / (1.0 + 0.5 * max(0, ss - 3))  # reduce WM influence for larger sets
            w = w_raw * age_scale * load_scale
            w = np.clip(w, 0.0, 1.0)

            # Mixture with lapse
            p_mix = w * p_wm + (1.0 - w) * p_rl
            p = (1.0 - epsilon) * p_mix + epsilon * (1.0 / nA)

            if a >= 0 and a < nA:
                pa = float(np.clip(p[a], 1e-12, 1.0))
                total_logp += np.log(pa)
            else:
                total_logp += np.log(1.0 / nA)

            # Updates only for valid actions
            if a >= 0 and a < nA:

                # RL update (only if reward is valid)
                if r >= 0:
                    delta = r - Q[s, a]
                    Q[s, a] += alpha * delta

                # WM decay and write on rewarded trials
                # Decay rate is a sigmoid of (decay_base + age + load terms)
                decay_logit = decay_base + (0.6 if is_older else 0.0) + 0.3 * max(0, ss - 3)
                decay_rate = 1.0 / (1.0 + np.exp(-decay_logit))  # in (0,1)
                # Apply decay to current state's WM trace
                WM[s, :] = (1.0 - decay_rate) * WM[s, :]

                # If reward present and positive, store strong WM for chosen action
                if r >= 0 and r > 0.0:
                    WM[s, a] = 1.0

    return -float(total_logp)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with a global choice kernel (perseveration) modulated by age and set size.

    The agent learns Q-values per state-action and also maintains a global choice kernel K over actions
    that captures recent repetition tendencies, irrespective of state. The influence of the choice kernel
    is stronger in older adults and under larger set sizes, reflecting increased reliance on habitual responding
    under cognitive load. Choices are generated by a softmax over Q plus a scaled choice-kernel, with lapse.

    Parameters
    ----------
    states : np.ndarray of int
        State index at each trial (0..nS-1 within a block).
    actions : np.ndarray of int
        Chosen action at each trial (0..2). If action < 0 it is treated as a lapse/missed response (uniform likelihood).
    rewards : np.ndarray of float
        Feedback at each trial. Values in {0,1}; values < 0 indicate invalid/missing feedback (trial ignored for updating).
    blocks : np.ndarray of int
        Block id per trial.
    set_sizes : np.ndarray of int
        Set size per trial (3 or 6), used to modulate perseveration strength.
    age : np.ndarray of float
        Array containing participant age (use age[0]); age>=45 considered older group.
    model_parameters : sequence of float
        [alpha, beta, eta_ck, persev, epsilon]
        - alpha: RL learning rate (0..1).
        - beta: inverse temperature for softmax.
        - eta_ck: learning rate for the choice kernel (how quickly repetition preference updates).
        - persev: base strength of the choice-kernel influence on value.
        - epsilon: lapse probability.

    Returns
    -------
    float
        Negative log-likelihood of the observed actions.
    """
    alpha, beta, eta_ck, persev, epsilon = model_parameters
    age_val = float(age[0])
    is_older = age_val >= 45.0

    nA = 3
    total_logp = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        b_states = states[idx]
        b_actions = actions[idx]
        b_rewards = rewards[idx]
        b_set_sizes = set_sizes[idx]
        nS = int(b_set_sizes[0])

        Q = (1.0 / nA) * np.ones((nS, nA))
        # Global choice kernel across actions
        K = np.zeros(nA)  # centered kernel; we'll re-center after updates

        for t in range(len(b_states)):
            s = int(b_states[t])
            a = int(b_actions[t])
            r = float(b_rewards[t])
            ss = int(b_set_sizes[t])

            # Age- and load-modulated kernel scaling
            age_scale = 1.25 if is_older else 0.95
            load_scale = 1.0 + 0.25 * max(0, ss - 3)
            kappa = persev * age_scale * load_scale

            # Compose decision values: Q plus scaled, centered choice kernel
            K_centered = K - np.mean(K)
            vals = Q[s, :] + kappa * K_centered
            vals = vals - np.max(vals)
            p_soft = np.exp(beta * vals)
            p_soft = p_soft / np.sum(p_soft)
            p = (1.0 - epsilon) * p_soft + epsilon * (1.0 / nA)

            if a >= 0 and a < nA:
                pa = float(np.clip(p[a], 1e-12, 1.0))
                total_logp += np.log(pa)
            else:
                total_logp += np.log(1.0 / nA)

            # Updates for valid actions
            if a >= 0 and a < nA:
                if r >= 0:
                    delta = r - Q[s, a]
                    Q[s, a] += alpha * delta

                # Update global choice kernel towards one-hot of chosen action
                # Exponential recency with rate eta_ck
                K = (1.0 - eta_ck) * K
                K[a] += eta_ck
                # Keep kernel bounded (optional) and centered
                K = K - np.mean(K)

    return -float(total_logp)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Surprise-modulated exploration with age- and load-dependent forgetting.

    The agent learns Q-values per state-action with (i) state-conditional forgetting that is stronger
    for larger set sizes and in older adults, and (ii) a decision temperature that decreases with
    prediction surprise (|delta|), promoting exploration after unexpected outcomes. Lapse is included.

    Parameters
    ----------
    states : np.ndarray of int
        State index at each trial (0..nS-1 within a block).
    actions : np.ndarray of int
        Chosen action at each trial (0..2). If action < 0 it is treated as a lapse/missed response (uniform likelihood).
    rewards : np.ndarray of float
        Feedback at each trial. Values in {0,1}; values < 0 indicate invalid/missing feedback (trial ignored for updating).
    blocks : np.ndarray of int
        Block id per trial.
    set_sizes : np.ndarray of int
        Set size per trial (3 or 6), used to scale forgetting and exploration modulation.
    age : np.ndarray of float
        Array containing participant age (use age[0]); age>=45 considered older group.
    model_parameters : sequence of float
        [alpha, beta0, surprise_gain, decay_base, epsilon]
        - alpha: RL learning rate (0..1).
        - beta0: baseline inverse temperature.
        - surprise_gain: how strongly recent surprise reduces beta (more exploration).
        - decay_base: base forgetting logit; larger means more decay toward uniform; modulated by age and set size.
        - epsilon: lapse probability.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta0, surprise_gain, decay_base, epsilon = model_parameters
    age_val = float(age[0])
    is_older = age_val >= 45.0

    nA = 3
    total_logp = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        b_states = states[idx]
        b_actions = actions[idx]
        b_rewards = rewards[idx]
        b_set_sizes = set_sizes[idx]
        nS = int(b_set_sizes[0])

        Q = (1.0 / nA) * np.ones((nS, nA))
        # Track previous absolute prediction error per state to modulate beta
        prev_abs_pe = np.zeros(nS)

        for t in range(len(b_states)):
            s = int(b_states[t])
            a = int(b_actions[t])
            r = float(b_rewards[t])
            ss = int(b_set_sizes[t])

            # Effective forgetting factor toward uniform for this state
            # Convert logit to rate via sigmoid; increase with age and set size
            forget_logit = decay_base + (0.5 if is_older else 0.0) + 0.25 * max(0, ss - 3)
            forget_rate = 1.0 / (1.0 + np.exp(-forget_logit))  # in (0,1)
            # Apply forgetting to current state's Q before decision (toward uniform baseline)
            Q[s, :] = (1.0 - forget_rate) * Q[s, :] + forget_rate * (1.0 / nA)

            # Surprise-modulated temperature: higher surprise -> lower beta (more exploration)
            beta_age = beta0 * (0.8 if is_older else 1.0)  # older adults lower baseline selectivity
            beta_eff = beta_age / (1.0 + surprise_gain * prev_abs_pe[s])
            beta_eff = max(1e-6, beta_eff)

            q_s = Q[s, :]
            vals = q_s - np.max(q_s)
            p_soft = np.exp(beta_eff * vals)
            p_soft = p_soft / np.sum(p_soft)
            p = (1.0 - epsilon) * p_soft + epsilon * (1.0 / nA)

            if a >= 0 and a < nA:
                pa = float(np.clip(p[a], 1e-12, 1.0))
                total_logp += np.log(pa)
            else:
                total_logp += np.log(1.0 / nA)

            # Update for valid actions
            if a >= 0 and a < nA:
                if r >= 0:
                    delta = r - Q[s, a]
                    Q[s, a] += alpha * delta
                    prev_abs_pe[s] = abs(delta)

    return -float(total_logp)