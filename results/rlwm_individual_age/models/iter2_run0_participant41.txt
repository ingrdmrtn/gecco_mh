def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Model 1: Q-learning with eligibility traces and action stickiness, plus load- and age-dependent
    credit generalization and lapse-driven omissions.

    Core ideas:
    - Model-free RL with eligibility traces propagating prediction errors to recent state-action pairs.
    - Cross-state generalization of eligibility to the same action in other states, scaled by set size and age.
    - A global action stickiness that biases repeating the most recent action in the block.
    - Lapses yield a mixture of random actions and omissions; omission probability increases with age and load.

    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..nS-1 within a block).
    actions : array-like of int
        Observed action on each trial. Valid actions are {0,1,2}; -2 denotes omission.
    rewards : array-like of float/int
        Observed outcome (1 or 0). Negative values indicate invalid trials; they are excluded from learning.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size for each trial's block (3 or 6).
    age : array-like (single-element)
        Participant's chronological age. Age group used: older if >=45.
    model_parameters : list/tuple of length 5
        [alpha, beta, lambda_trace, stickiness, lapse]
        - alpha (0..1): learning rate for Q updates via eligibility traces.
        - beta (>0): inverse temperature for softmax over Q with stickiness bias.
        - lambda_trace (0..1): eligibility decay across time; older adults effectively have lower persistence.
        - stickiness (>=0): additive bias in logits for repeating the last action in the block.
        - lapse (0..1): lapse probability. Split into random responding and omissions, scaled by age and load.

    Returns
    -------
    neg_log_likelihood : float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, lambda_trace, stickiness, lapse = model_parameters
    age_val = age[0]
    older = 1 if age_val >= 45 else 0

    nA = 3
    eps = 1e-12
    neg_log_lik = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask].astype(int)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        nS = int(set_sizes[mask][0])

        # Initialize Q-values and eligibility traces within block
        Q = (1.0 / nA) * np.ones((nS, nA))
        e = np.zeros((nS, nA))

        # Global last action in the block for stickiness (distinct from per-state perseveration)
        last_action_global = -1

        # Effective parameters modulated by load and age
        # Older adults: shallower traces and stronger stickiness under load
        lambda_eff = np.clip(lambda_trace * (0.9 - 0.2 * older) * (1.0 - 0.15 * (nS > 3)), 0.0, 1.0)
        stick_eff = stickiness * (1.0 + 0.25 * older) * (1.0 + 0.15 * (nS > 3))
        beta_eff = max(1e-6, beta * (1.0 - 0.1 * (nS > 3)))  # slightly lower precision under higher load

        # Cross-state generalization of eligibility to the same action:
        # stronger in small set size; older adults generalize less precisely (we reduce generalization)
        gen_base = 3.0 / float(nS)  # 1.0 for nS=3, 0.5 for nS=6
        gen_eff = max(0.0, gen_base * (1.0 - 0.3 * older))

        # Lapse handling: fraction allocated to omission increases with age and load
        p_omit_frac = np.clip(0.25 + 0.25 * older + 0.15 * (nS > 3), 0.0, 0.95)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Build logits: Q softmax with global stickiness bias
            logits = beta_eff * (Q[s] - np.max(Q[s]))
            if last_action_global in [0, 1, 2]:
                logits[last_action_global] += stick_eff

            # Action probabilities from softmax
            probs = np.exp(logits - np.max(logits))
            probs = probs / np.sum(probs)

            # Lapse mixture with omissions
            p_omit = lapse * p_omit_frac
            p_rand = lapse * (1.0 - p_omit_frac)

            if a == -2:
                p_choice = p_omit
            elif 0 <= a < nA:
                p_choice = (1.0 - lapse) * probs[a] + p_rand * (1.0 / nA)
            else:
                p_choice = eps

            neg_log_lik -= np.log(max(p_choice, eps))

            # Learning (skip on omissions or invalid outcomes)
            if (a >= 0) and (r >= 0):
                # Temporal eligibility decay
                e *= lambda_eff

                # Assign eligibility to current state-action
                e[s, a] += 1.0

                # Cross-state generalization to the same action in other states
                if nS > 1:
                    for s2 in range(nS):
                        if s2 != s:
                            e[s2, a] += gen_eff / max(1, (nS - 1))

                # TD error and trace-based update
                delta = r - Q[s, a]
                Q += alpha * delta * e

                # Track global last action
                last_action_global = a

    return neg_log_lik


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Model 2: Bayesian hypothesis tracking with noisy feedback, hazard-based forgetting, and lapses.

    Core ideas:
    - Each state has an unknown correct action (one of 3). The model maintains a posterior over this hypothesis.
    - Feedback is treated as noisy evidence: if chosen action equals the hypothesis, reward is likely; otherwise unlikely.
    - A hazard rate induces partial resets toward a uniform prior (memory instability), which increases with age and load.
    - Choices arise from a softmax over posterior probabilities (beliefs), with lapses contributing random choices and omissions.

    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..nS-1 within a block).
    actions : array-like of int
        Observed action on each trial. Valid actions are {0,1,2}; -2 denotes omission.
    rewards : array-like of float/int
        Observed outcome (1 or 0). Negative values indicate invalid trials; they are excluded from belief updates.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size for each trial's block (3 or 6).
    age : array-like (single-element)
        Participant's chronological age. Age group used: older if >=45.
    model_parameters : list/tuple of length 4
        [beta, epsilon, hazard, lapse]
        - beta (>0): inverse temperature for mapping belief to policy.
        - epsilon (0..0.5): feedback noise (slip/guess). Higher epsilon weakens evidence from outcomes.
        - hazard (0..1): baseline probability of belief reset toward uniform between trials (forgetting/volatility).
        - lapse (0..1): lapse probability split between random actions and omissions.

    Returns
    -------
    neg_log_likelihood : float
        Negative log-likelihood of the observed choices.
    """
    beta, epsilon, hazard, lapse = model_parameters
    age_val = age[0]
    older = 1 if age_val >= 45 else 0

    nA = 3
    eps = 1e-12
    neg_log_lik = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask].astype(int)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        nS = int(set_sizes[mask][0])

        # Beliefs: posterior over "correct action" per state
        B = (1.0 / nA) * np.ones((nS, nA))  # rows sum to 1

        # Effective parameters modulated by age and load
        # Older adults and larger set sizes: higher hazard and noisier feedback interpretation
        hazard_eff = np.clip(hazard * (1.0 + 0.4 * older) * (1.0 + 0.4 * (nS > 3)), 0.0, 1.0)
        epsilon_eff = np.clip(epsilon * (1.0 + 0.3 * older) * (1.0 + 0.3 * (nS > 3)), 1e-6, 0.49)
        beta_eff = max(1e-6, beta * (1.0 - 0.15 * older * (nS > 3)))

        # Lapse split: more omissions with age and load
        p_omit_frac = np.clip(0.2 + 0.25 * older + 0.15 * (nS > 3), 0.0, 0.95)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Policy from beliefs
            logits = beta_eff * (B[s] - np.max(B[s]))
            probs = np.exp(logits - np.max(logits))
            probs = probs / np.sum(probs)

            p_omit = lapse * p_omit_frac
            p_rand = lapse * (1.0 - p_omit_frac)

            if a == -2:
                p_choice = p_omit
            elif 0 <= a < nA:
                p_choice = (1.0 - lapse) * probs[a] + p_rand * (1.0 / nA)
            else:
                p_choice = eps

            neg_log_lik -= np.log(max(p_choice, eps))

            # Belief update for valid trials
            if (a >= 0) and (r >= 0):
                # Likelihood under each hypothesis h in {0,1,2}
                # If hypothesis h equals chosen action a, then P(r=1) = 1-epsilon_eff; else P(r=1)=epsilon_eff
                like = np.full(nA, epsilon_eff)
                like[a] = 1.0 - epsilon_eff
                if r < 0.5:  # r==0
                    like = 1.0 - like

                # Bayesian update and normalization
                post = B[s] * like
                post_sum = np.sum(post)
                if post_sum <= 0:
                    post = (1.0 / nA) * np.ones(nA)
                else:
                    post = post / post_sum

                # Apply hazard-based forgetting toward uniform
                B[s] = (1.0 - hazard_eff) * post + hazard_eff * (np.ones(nA) / nA)

    return neg_log_lik


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Model 3: Misbinding-prone RL with uncertainty decay and reward-based action priming.

    Core ideas:
    - Standard Q-learning for state-action values drives choice via softmax.
    - Credit misbinding: with some probability, the learning update is applied to an incorrect state
      (a source of interference), which increases with age and set size.
    - Slow value decay toward neutral under higher load and in older adults (uncertainty/forgetting).
    - Reward-based action priming: recently rewarded actions receive a transient boost in choice logits.

    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..nS-1 within a block).
    actions : array-like of int
        Observed action on each trial. Valid actions are {0,1,2}; -2 denotes omission.
    rewards : array-like of float/int
        Observed outcome (1 or 0). Negative values indicate invalid trials; they are excluded from learning.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size for each trial's block (3 or 6).
    age : array-like (single-element)
        Participant's chronological age. Age group used: older if >=45.
    model_parameters : list/tuple of length 4
        [alpha, beta, misbind, lapse]
        - alpha (0..1): learning rate for Q updates.
        - beta (>0): inverse temperature for softmax over Q with priming bias.
        - misbind (0..1): baseline probability of mis-assigning credit to an incorrect state.
        - lapse (0..1): lapse probability split between random actions and omissions.

    Returns
    -------
    neg_log_likelihood : float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, misbind, lapse = model_parameters
    age_val = age[0]
    older = 1 if age_val >= 45 else 0

    nA = 3
    eps = 1e-12
    neg_log_lik = 0.0

    rng = np.random.RandomState(0)  # deterministic tie-breaks for misbinding target selection

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask].astype(int)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        nS = int(set_sizes[mask][0])

        # Initialize Q and action priming traces per block
        Q = (1.0 / nA) * np.ones((nS, nA))
        prime = np.zeros(nA)  # action-level priming irrespective of state

        # Effective parameters
        beta_eff = max(1e-6, beta * (1.0 - 0.1 * (nS > 3)))
        # Misbinding increases with age and load
        misbind_eff = np.clip(misbind * (1.0 + 0.5 * older) * (1.0 + 0.5 * (nS > 3)), 0.0, 1.0)
        # Value decay toward neutral (1/nA), capturing uncertainty and forgetting
        decay = 0.02 * (1.0 + 0.5 * older) * (1.0 + (nS - 3) / 3.0)

        # Priming strength scales with age (older may rely more on recency)
        prime_gain = 0.5 * (1.0 + 0.3 * older)
        prime_decay = 0.2 + 0.2 * (nS > 3)

        # Lapse split: more omissions with age and load
        p_omit_frac = np.clip(0.2 + 0.25 * older + 0.15 * (nS > 3), 0.0, 0.95)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Compute logits: Q plus action priming bias
            logits = beta_eff * (Q[s] - np.max(Q[s]))
            logits += prime_gain * (prime - np.max(prime))

            probs = np.exp(logits - np.max(logits))
            probs = probs / np.sum(probs)

            p_omit = lapse * p_omit_frac
            p_rand = lapse * (1.0 - p_omit_frac)

            if a == -2:
                p_choice = p_omit
            elif 0 <= a < nA:
                p_choice = (1.0 - lapse) * probs[a] + p_rand * (1.0 / nA)
            else:
                p_choice = eps

            neg_log_lik -= np.log(max(p_choice, eps))

            # Learning and dynamics
            if (a >= 0) and (r >= 0):
                # TD error at experienced state
                delta = r - Q[s, a]

                # Decide target state for credit assignment (simulate expected misbinding behavior deterministically)
                # Use a soft assignment: expected update = (1 - p_m) to current state + p_m distributed across other states
                p_m = misbind_eff
                # Update current state
                Q[s, a] += alpha * (1.0 - p_m) * delta
                # Distribute misbound credit evenly to other states (if any)
                if nS > 1 and p_m > 0:
                    other_states = [j for j in range(nS) if j != s]
                    share = (alpha * p_m * delta) / len(other_states)
                    for sj in other_states:
                        Q[sj, a] += share

                # Value decay toward neutral baseline to capture uncertainty/forgetting
                Q = (1.0 - decay) * Q + decay * (np.ones_like(Q) / nA)

                # Update action priming: reinforce recently rewarded actions, decay otherwise
                prime *= (1.0 - prime_decay)
                if r > 0.5 and 0 <= a < nA:
                    prime[a] += (1.0 - prime[a]) * 0.5  # push toward 1 on reward

    return neg_log_lik