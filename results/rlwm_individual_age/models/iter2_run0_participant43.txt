def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with gated recall (working-memory) and perseveration, where memory storage probability
    depends on age group and set size. Decisions arise from a unified softmax over RL values
    plus additive biases from WM recall and action perseveration.

    Parameters
    ----------
    states : array-like of int
        State identity on each trial (0-indexed within block).
    actions : array-like of int
        Chosen action on each trial (assumed in {0,1,2}).
    rewards : array-like of float/int
        Reward feedback on each trial (0/1).
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size (number of states in the block) on each trial.
    age : array-like (length 1)
        Participant age; used to define age group (younger <45, older >=45).
    model_parameters : list/tuple of floats (length 5)
        [alpha, beta, p_store_base, decay, perseveration]
        - alpha: RL learning rate (0..1).
        - beta: inverse temperature controlling choice stochasticity (>0).
        - p_store_base: baseline probability to store a rewarded state-action in WM (0..1).
        - decay: per-trial decay of WM confidence when the state is not encountered (0..1).
        - perseveration: weight of repeating the previous action (stickiness; can be negative or positive).

    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    alpha, beta, p_store_base, decay, perseveration = model_parameters
    age_val = age[0]
    # Age factor reduces WM gating/storage in older adults
    age_factor = 1.0 if age_val < 45 else 0.6
    nll = 0.0
    eps = 1e-12
    nA = 3

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nS = int(block_set_sizes[0])
        # RL values
        Q = np.zeros((nS, nA))
        # WM store: for each state, a stored action and a confidence strength
        wm_a = -1 * np.ones(nS, dtype=int)
        wm_c = np.zeros(nS)
        # Track last seen trial index for each state to implement between-encounter decay
        last_seen = -1 * np.ones(nS, dtype=int)

        # Perseveration: track previous action
        prev_action = None

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            nS_t = int(block_set_sizes[t])

            # Apply between-encounter decay for this state since it was last seen
            if last_seen[s] >= 0:
                gap = max(0, t - last_seen[s])
                wm_c[s] *= max(0.0, (1.0 - decay))**gap
            last_seen[s] = t

            # RL softmax preferences
            q_s = Q[s, :]
            q_center = q_s - np.max(q_s)
            pref_rl = beta * q_center

            # WM additive bias toward stored action, scaled by its confidence and gating by set size and age
            base_gate = np.clip(p_store_base, 0.0, 1.0) * age_factor * (3.0 / max(1.0, float(nS_t)))
            base_gate = np.clip(base_gate, 0.0, 1.0)
            wm_bias = np.zeros(nA)
            if wm_a[s] >= 0 and wm_c[s] > 0.0:
                wm_bias[wm_a[s]] += base_gate * wm_c[s] * beta  # convert to same scale as preferences

            # Perseveration bias toward previous action
            pers_bias = np.zeros(nA)
            if prev_action is not None:
                pers_bias[prev_action] += perseveration

            # Final softmax over combined preferences
            logits = pref_rl + wm_bias + pers_bias
            logits -= np.max(logits)
            p_vec = np.exp(logits)
            p_vec /= (np.sum(p_vec) + eps)
            p = p_vec[a]
            nll -= np.log(max(p, eps))

            # Update RL
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # Update WM store: store on rewarded trials with probability gated by set size and age
            # If not rewarded, reduce confidence slightly (forgetting due to error)
            store_p = base_gate
            if r > 0.5:
                # stochastic storage; here we emulate expectation by blending confidence deterministically
                # New confidence moves toward 1 with weight store_p
                wm_a[s] = a
                wm_c[s] = (1.0 - store_p) * wm_c[s] + store_p * 1.0
            else:
                wm_c[s] = max(0.0, wm_c[s] * (1.0 - decay))

            # Update perseveration tracker
            prev_action = a

    return nll


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Capacity-limited Bayesian (Dirichlet) learner with recency-boosted priors.
    The model maintains Dirichlet counts per state and action. Only a limited set of
    currently "in-focus" states (capacity) receive an additional recency-dependent prior boost.
    Capacity is reduced in older adults and diluted with larger set sizes.

    Parameters
    ----------
    states : array-like of int
        State identity on each trial (0-indexed within block).
    actions : array-like of int
        Chosen action on each trial (assumed in {0,1,2}).
    rewards : array-like of float/int
        Reward feedback on each trial (0/1).
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size (number of states in the block) on each trial.
    age : array-like (length 1)
        Participant age; used to define age group (younger <45, older >=45).
    model_parameters : list/tuple of floats (length 4)
        [beta, eta, k_capacity, recency]
        - beta: inverse temperature for softmax over log posterior probabilities.
        - eta: symmetric Dirichlet prior strength per action (>0).
        - k_capacity: baseline number of states that can receive a recency boost.
        - recency: additional pseudo-count boost for in-focus states (>=0).

    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    beta, eta, k_capacity, recency = model_parameters
    age_val = age[0]
    age_factor = 1.0 if age_val < 45 else 0.7  # older adults have reduced effective capacity
    nll = 0.0
    eps = 1e-12
    nA = 3

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nS = int(block_set_sizes[0])

        # Dirichlet counts initialized to eta
        C = eta * np.ones((nS, nA))

        # Maintain an LRU list of states for recency boosting
        # We'll store the last time each state was visited to pick the most recent ones
        last_visit = -1 * np.ones(nS, dtype=int)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            nS_t = int(block_set_sizes[t])

            # Determine how many states are "in-focus" given capacity, set size, and age
            k_eff = max(0, int(np.floor(k_capacity * age_factor)))
            # Reduce impact of capacity when set size grows (dilution)
            dilution = min(1.0, 3.0 / max(1.0, float(nS_t)))
            k_eff = max(0, int(np.floor(k_eff * dilution)))
            # Identify the k_eff most recent states (including current s if applicable)
            # Scores: more recent -> larger t - last_visit[s] small, but we will sort by recency index
            idxs = np.arange(nS)
            # Eligible states are those visited at least once; include current state to ensure it's considered
            last_visit[s] = t if last_visit[s] < 0 else last_visit[s]
            # Select top-k by last_visit value (higher is more recent)
            recent_order = idxs[np.argsort(last_visit)]
            recent_order = recent_order[::-1]  # descending
            in_focus = set(list(recent_order[:k_eff]))

            # Compute posterior counts with recency boost for in-focus states
            counts_s = C[s, :].copy()
            if s in in_focus and k_eff > 0 and recency > 0:
                counts_s = counts_s + recency

            # Posterior action probabilities for the state
            post_s = counts_s / (np.sum(counts_s) + eps)

            # Choice probability via softmax on log posterior (equivalent to powered posterior)
            logits = beta * (np.log(post_s + eps) - np.max(np.log(post_s + eps)))
            p_vec = np.exp(logits)
            p_vec /= (np.sum(p_vec) + eps)
            p = p_vec[a]
            nll -= np.log(max(p, eps))

            # Update counts: reward-consistent Bayesian evidence
            # On reward, add 1 to chosen action; on no reward, add a small non-reward evidence to non-chosen actions
            if r > 0.5:
                C[s, a] += 1.0
            else:
                # distribute a small fraction of evidence to other actions to reflect error-driven exclusion
                leak = 0.5
                for aa in range(nA):
                    if aa != a:
                        C[s, aa] += leak / (nA - 1)

            # Update recency
            last_visit[s] = t

    return nll


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RLâ€“WM arbitration by uncertainty with capacity and lapse.
    RL values produce a softmax policy; WM stores a single action per state with precision gamma.
    Arbitration weight increases with RL uncertainty (entropy) and decreases with load and age.
    Final choice is a mixture of WM and RL augmented with a lapse probability.

    Parameters
    ----------
    states : array-like of int
        State identity on each trial (0-indexed within block).
    actions : array-like of int
        Chosen action on each trial (assumed in {0,1,2}).
    rewards : array-like of float/int
        Reward feedback on each trial (0/1).
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size (number of states in the block) on each trial.
    age : array-like (length 1)
        Participant age; used to define age group (younger <45, older >=45).
    model_parameters : list/tuple of floats (length 5)
        [alpha, beta, gamma, k_capacity, lapse]
        - alpha: RL learning rate (0..1).
        - beta: RL inverse temperature (>0).
        - gamma: WM precision controlling how peaked WM-driven choice is (>0).
        - k_capacity: WM capacity parameter that scales with set size.
        - lapse: probability of choosing uniformly at random (0..1).

    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    alpha, beta, gamma, k_capacity, lapse = model_parameters
    age_val = age[0]
    age_factor = 1.0 if age_val < 45 else 0.65
    nll = 0.0
    eps = 1e-12
    nA = 3

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nS = int(block_set_sizes[0])

        # RL Q-values
        Q = np.zeros((nS, nA))
        # WM store: for each state, stored action (if learned) and confidence (1 after reward, decays otherwise)
        wm_a = -1 * np.ones(nS, dtype=int)
        wm_c = np.zeros(nS)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            nS_t = int(block_set_sizes[t])

            # RL policy
            q_s = Q[s, :]
            q_center = q_s - np.max(q_s)
            p_rl_vec = np.exp(beta * q_center)
            p_rl_vec = p_rl_vec / (np.sum(p_rl_vec) + eps)

            # RL uncertainty (normalized entropy)
            entropy = -np.sum(p_rl_vec * (np.log(p_rl_vec + eps)))
            entropy_norm = entropy / np.log(nA)

            # WM policy for this state based on stored action and precision gamma
            if wm_a[s] >= 0 and wm_c[s] > 0.0:
                logits_wm = np.zeros(nA)
                logits_wm[wm_a[s]] = gamma * wm_c[s]
                logits_wm -= np.max(logits_wm)
                p_wm_vec = np.exp(logits_wm)
                p_wm_vec /= (np.sum(p_wm_vec) + eps)
            else:
                p_wm_vec = np.ones(nA) / nA

            # Capacity-based arbitration baseline decreases with set size and age
            base = age_factor * (k_capacity / max(1.0, float(nS_t)))
            base = np.clip(base, 0.0, 1.0)

            # Arbitration weight increases with RL uncertainty
            w = np.clip(base * entropy_norm, 0.0, 1.0)

            # Mixture with lapse
            p_mix_vec = (1.0 - lapse) * (w * p_wm_vec + (1.0 - w) * p_rl_vec) + lapse * (np.ones(nA) / nA)
            p = p_mix_vec[a]
            nll -= np.log(max(p, eps))

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # WM updates: rewarded trials store strong trace; errors reduce confidence
            if r > 0.5:
                wm_a[s] = a
                wm_c[s] = 1.0
            else:
                wm_c[s] = 0.5 * wm_c[s]  # partial confidence drop on error

    return nll