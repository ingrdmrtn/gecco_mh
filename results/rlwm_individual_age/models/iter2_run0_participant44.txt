def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + recall-based working-memory with interference and aging.
    
    Idea:
    - Two systems: incremental RL and a sparse, recall-based WM store.
    - WM stores the last rewarded action for a state and can recall it with probability
      that depends on (a) load (set size), (b) age group, and (c) time since last reward via decay.
    - If WM recalls, it selects the stored action; otherwise, RL softmax is used.
    
    Parameters (model_parameters):
    - alpha: RL learning rate for Q-values.
    - beta: inverse temperature for RL softmax (internally scaled by 10).
    - wm_base: baseline recall log-odds for WM (higher -> more likely to recall).
    - wm_load_k: reduction in recall log-odds per extra item beyond 3 (load sensitivity).
    - age_wm_penalty: additional reduction in recall log-odds for older adults (>=45).
    - wm_decay: memory decay factor per trial gap (0..1); strength = wm_decay^(trials since last reward).
    
    Inputs:
    - states: array of state indices per trial.
    - actions: array of chosen actions per trial (0,1,2); others treated as uniform and not learned from.
    - rewards: array of rewards per trial (any numeric; typically 0/1).
    - blocks: array of block indices per trial.
    - set_sizes: array of set sizes per trial (3 or 6).
    - age: array-like with a single scalar age in years.
    - model_parameters: sequence of 6 floats [alpha, beta, wm_base, wm_load_k, age_wm_penalty, wm_decay].
    
    Returns:
    - Negative log-likelihood of the observed action sequence.
    """
    alpha, beta, wm_base, wm_load_k, age_wm_penalty, wm_decay = model_parameters
    beta_eff = beta * 10.0
    age_val = float(age[0]) if hasattr(age, "__len__") else float(age)
    is_older = 1.0 if age_val >= 45 else 0.0
    nA = 3
    eps = 1e-12
    nll = 0.0

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nS = int(block_set_sizes[0])
        Q = np.ones((nS, nA)) / nA  # RL values

        # WM store: last rewarded action and time index of storage per state
        wm_action = -np.ones(nS, dtype=int)
        wm_time = -np.ones(nS, dtype=int)  # -1 = never stored

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = block_rewards[t]
            valid_action = (0 <= a < nA)
            if s < 0 or s >= nS:
                nll -= np.log(max(eps, 1.0 / nA))
                continue

            load = float(max(0, int(block_set_sizes[t]) - 3))

            # Compute WM recall strength from decay since last storage in this state
            if wm_time[s] >= 0:
                gap = t - wm_time[s]
                strength = max(0.0, float(wm_decay) ** max(0, int(gap)))
            else:
                strength = 0.0

            # Recall probability combines baseline, load, age, and current strength
            recall_logit = wm_base - wm_load_k * load - age_wm_penalty * is_older
            p_recall_base = sigmoid(recall_logit)
            p_recall = p_recall_base * strength

            # WM policy if recalled and an action is stored
            if wm_action[s] >= 0:
                p_wm = np.zeros(nA)
                p_wm[wm_action[s]] = 1.0
            else:
                p_wm = np.ones(nA) / nA  # no stored item: WM is effectively uninformative

            # RL softmax policy
            logits = beta_eff * (Q[s, :] - np.max(Q[s, :]))
            p_rl = np.exp(logits)
            p_rl = p_rl / np.sum(p_rl)

            # Mixture policy: recall vs RL
            p_vec = p_recall * p_wm + (1.0 - p_recall) * p_rl

            if valid_action:
                nll -= np.log(max(eps, p_vec[a]))
            else:
                nll -= np.log(max(eps, 1.0 / nA))

            # Learning updates only on valid actions
            if valid_action:
                pe = r - Q[s, a]
                Q[s, a] += alpha * pe

                # Update WM store only on rewarded outcomes; refresh time on reward
                if r > 0:
                    wm_action[s] = a
                    wm_time[s] = t

    return nll


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Surprise- and load-adaptive learning with age modulation and lapses (no perseveration).
    
    Idea:
    - RL with a dynamic learning rate that increases with recent surprise (|PE|),
      with set size (higher load -> higher alpha), and for older adults.
    - Action selection via softmax; includes a lapse rate mixing with uniform.
    - Uses a per-state running surprise trace to avoid using future info.
    
    Parameters (model_parameters):
    - alpha0: baseline logit for learning rate (passed through sigmoid).
    - beta: inverse temperature for softmax (internally scaled by 10).
    - a_surprise: weight of running surprise trace on learning-rate logit.
    - a_load: increment in learning-rate logit per extra item beyond 3.
    - a_age: increment in learning-rate logit for older adults (>=45).
    - lapse: probability of random responding (0..1).
    
    Inputs:
    - states: array of state indices per trial.
    - actions: array of chosen actions per trial (0,1,2); others treated as uniform.
    - rewards: array of rewards per trial (numeric).
    - blocks: array of block indices per trial.
    - set_sizes: array of set sizes per trial (3 or 6).
    - age: array-like with a single scalar age in years.
    - model_parameters: sequence of 6 floats [alpha0, beta, a_surprise, a_load, a_age, lapse].
    
    Returns:
    - Negative log-likelihood of the observed action sequence.
    """
    alpha0, beta, a_surprise, a_load, a_age, lapse = model_parameters
    beta_eff = beta * 10.0
    age_val = float(age[0]) if hasattr(age, "__len__") else float(age)
    is_older = 1.0 if age_val >= 45 else 0.0
    nA = 3
    eps = 1e-12
    nll = 0.0

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nS = int(block_set_sizes[0])
        Q = np.ones((nS, nA)) / nA
        # Running surprise trace per state (updated from past PEs)
        surprise_trace = np.zeros(nS)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = block_rewards[t]
            valid_action = (0 <= a < nA)
            if s < 0 or s >= nS:
                nll -= np.log(max(eps, 1.0 / nA))
                continue

            load = float(max(0, int(block_set_sizes[t]) - 3))

            # Learning rate from current (past-based) surprise, load, and age
            alpha_logit = alpha0 + a_surprise * surprise_trace[s] + a_load * load + a_age * is_older
            alpha_eff = sigmoid(alpha_logit)

            # Softmax policy with lapse
            logits = beta_eff * (Q[s, :] - np.max(Q[s, :]))
            p_soft = np.exp(logits); p_soft = p_soft / np.sum(p_soft)
            p_vec = (1.0 - lapse) * p_soft + lapse * (np.ones(nA) / nA)

            if valid_action:
                nll -= np.log(max(eps, p_vec[a]))
            else:
                nll -= np.log(max(eps, 1.0 / nA))

            # Update after observing outcome (valid action only)
            if valid_action:
                pe = r - Q[s, a]
                Q[s, a] += alpha_eff * pe
                # Update surprise trace toward current absolute PE (bounded in [0,1] if rewards in [0,1])
                # Use a simple leaky integrator with the same alpha_eff as a gain on this state
                surprise_trace[s] = (1.0 - alpha_eff) * surprise_trace[s] + alpha_eff * abs(pe)

    return nll


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with state confusability (misbinding) that increases with set size and age, plus lapses.
    
    Idea:
    - Learner updates true-state action values (standard RL).
    - On each decision, with some probability the current state is confused with another state
      in the block. Confusion probability increases with load and in older adults.
    - The probability of the chosen action is a mixture of the policy for the true state and
      the average policy across the other states (integrating over which wrong state is sampled).
    - Includes lapse mixing with uniform.
    
    Parameters (model_parameters):
    - alpha: RL learning rate.
    - beta: inverse temperature for softmax (internally scaled by 10).
    - conf0: baseline logit for state confusion probability.
    - conf_kload: increment in confusion log-odds per extra item beyond 3.
    - conf_age: increment in confusion log-odds for older adults (>=45).
    - lapse: probability of random responding (0..1).
    
    Inputs:
    - states: array of state indices per trial.
    - actions: array of chosen actions per trial (0,1,2); others treated as uniform.
    - rewards: array of rewards per trial (numeric).
    - blocks: array of block indices per trial.
    - set_sizes: array of set sizes per trial (3 or 6).
    - age: array-like with a single scalar age in years.
    - model_parameters: sequence of 6 floats [alpha, beta, conf0, conf_kload, conf_age, lapse].
    
    Returns:
    - Negative log-likelihood of the observed action sequence.
    """
    alpha, beta, conf0, conf_kload, conf_age, lapse = model_parameters
    beta_eff = beta * 10.0
    age_val = float(age[0]) if hasattr(age, "__len__") else float(age)
    is_older = 1.0 if age_val >= 45 else 0.0
    nA = 3
    eps = 1e-12
    nll = 0.0

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nS = int(block_set_sizes[0])
        Q = np.ones((nS, nA)) / nA

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = block_rewards[t]
            valid_action = (0 <= a < nA)
            if s < 0 or s >= nS:
                nll -= np.log(max(eps, 1.0 / nA))
                continue

            load = float(max(0, int(block_set_sizes[t]) - 3))
            conf_logit = conf0 + conf_kload * load + conf_age * is_older
            p_conf = sigmoid(conf_logit)
            p_conf = min(max(p_conf, 0.0), 1.0)

            # Softmax for the true state
            logits_true = beta_eff * (Q[s, :] - np.max(Q[s, :]))
            p_true = np.exp(logits_true); p_true = p_true / np.sum(p_true)

            # Average softmax over other states (if any)
            if nS > 1:
                p_other_sum = np.zeros(nA)
                count = 0
                for s2 in range(nS):
                    if s2 == s:
                        continue
                    logits_o = beta_eff * (Q[s2, :] - np.max(Q[s2, :]))
                    p_o = np.exp(logits_o); p_o = p_o / np.sum(p_o)
                    p_other_sum += p_o
                    count += 1
                p_other_mean = p_other_sum / max(1, count)
            else:
                p_other_mean = np.ones(nA) / nA

            # Mixture over state identity confusion and lapses
            p_dec = (1.0 - lapse) * ((1.0 - p_conf) * p_true + p_conf * p_other_mean) + lapse * (np.ones(nA) / nA)

            if valid_action:
                nll -= np.log(max(eps, p_dec[a]))
            else:
                nll -= np.log(max(eps, 1.0 / nA))

            # RL update on the true state
            if valid_action:
                pe = r - Q[s, a]
                Q[s, a] += alpha * pe

    return nll