def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited working memory (WM) mixture with age- and set-size-dependent WM reliability.

    The model mixes a slow RL system with a fast WM store that encodes the last rewarded action
    for each state. WM strength decays over time and is capacity-limited; both capacity and decay
    are modulated by set size and age group.

    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..set_size-1 within a block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of float
        Reward on each trial (0 or 1).
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size for each trial's block (3 or 6).
    age : array-like or scalar
        Participant age; used to determine age group (older >=45).
    model_parameters : tuple/list
        (alpha, beta, wm_capacity, wm_persist, age_decline)
        - alpha: RL learning rate (0..1).
        - beta: inverse temperature for RL softmax (>0).
        - wm_capacity: baseline WM capacity weight (0..1).
        - wm_persist: WM persistence/retention per trial (0..1); higher = slower decay.
        - age_decline: proportional reduction of WM capacity for older adults (>=0).

    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    alpha, beta, wm_capacity, wm_persist, age_decline = model_parameters
    age_scalar = float(age[0] if hasattr(age, "__len__") else age)
    older = 1.0 if age_scalar >= 45 else 0.0

    unique_blocks = np.unique(blocks)
    nll = 0.0
    nA = 3

    eps = 1e-12

    for b in unique_blocks:
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nS = int(block_set_sizes[0])

        # Initialize RL values and WM store
        Q = (1.0 / nA) * np.ones((nS, nA))
        M = np.zeros((nS, nA))  # WM strengths per action; encodes last rewarded action with decay

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])
            nS_now = int(block_set_sizes[t])

            # 1) RL policy
            logits_rl = beta * (Q[s, :] - np.max(Q[s, :]))
            p_rl_vec = np.exp(logits_rl)
            p_rl_vec = p_rl_vec / np.sum(p_rl_vec)

            # 2) WM policy
            # Decay WM globally each trial, faster decay for larger set sizes and for older adults
            decay_factor = wm_persist ** (1.0 + 0.25 * (nS_now - 3) + 0.25 * older)
            M *= decay_factor

            # WM policy distribution for current state
            wm_mass = np.sum(M[s, :])
            if wm_mass <= eps:
                # No WM information; fall back to uniform WM proposal
                p_wm_vec = np.ones(nA) / nA
                info_strength = 0.0
            else:
                p_wm_vec = (M[s, :] + eps) / (wm_mass + nA * eps)
                # Information strength increases with concentration of memory on a single action
                # Use L1 distance from uniform as a simple proxy
                info_strength = 0.5 * np.sum(np.abs(p_wm_vec - 1.0 / nA))

            # 3) Mixture weight: WM capacity reduced by set size and by age
            cap_eff = wm_capacity * (1.0 - age_decline * older)
            size_penalty = 3.0 / float(nS_now)  # 1.0 for set size 3, 0.5 for 6
            w_mix = np.clip(cap_eff * size_penalty * (0.5 + info_strength), 0.0, 1.0)

            p_vec = w_mix * p_wm_vec + (1.0 - w_mix) * p_rl_vec
            p_choice = max(p_vec[a], eps)
            log_p += np.log(p_choice)

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # WM encoding: if rewarded, store this action strongly for this state
            if r > 0.5:
                M[s, :] *= 0.0
                M[s, a] = 1.0  # one-shot storage that will decay over subsequent trials

        nll -= log_p

    return nll


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Uncertainty-adaptive (Kalman-like) RL with age- and set-size-modulated process noise and lapse.

    Q-values are updated using a Kalman gain derived from current uncertainty. Uncertainty increases
    via process noise that grows with set size and with age (older adults presumed to track more noise).
    Policy uses softmax, mixed with a small lapse to uniform that also increases in older adults.

    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..set_size-1 within a block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of float
        Reward on each trial (0 or 1).
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size for each trial's block (3 or 6).
    age : array-like or scalar
        Participant age; used to determine age group (older >=45).
    model_parameters : tuple/list
        (beta, sigma0, process_noise, age_unc_boost, lapse)
        - beta: inverse temperature (>0).
        - sigma0: initial uncertainty for each state-action (>0).
        - process_noise: baseline process noise per trial (>0).
        - age_unc_boost: multiplicative boost of process noise for older adults (>=0).
        - lapse: baseline lapse probability to uniform (0..1).

    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    beta, sigma0, process_noise, age_unc_boost, lapse = model_parameters
    age_scalar = float(age[0] if hasattr(age, "__len__") else age)
    older = 1.0 if age_scalar >= 45 else 0.0

    unique_blocks = np.unique(blocks)
    nll = 0.0
    nA = 3
    eps = 1e-12

    # Observation/reward variance for Bernoulli outcomes; use a fixed effective variance
    r_var = 0.25

    for b in unique_blocks:
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nS = int(block_set_sizes[0])

        Q = (1.0 / nA) * np.ones((nS, nA))
        U = sigma0 * np.ones((nS, nA))  # Uncertainty (variance proxy)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])
            nS_now = int(block_set_sizes[t])

            # Softmax policy
            logits = beta * (Q[s, :] - np.max(Q[s, :]))
            p_vec = np.exp(logits)
            p_vec = p_vec / np.sum(p_vec)

            # Lapse increases with age
            eps_lapse = min(0.25, max(0.0, lapse) * (1.0 + 0.5 * older))
            p_mix = (1.0 - eps_lapse) * p_vec + eps_lapse * (np.ones(nA) / nA)

            p_choice = max(p_mix[a], eps)
            log_p += np.log(p_choice)

            # Kalman-like update
            # Process noise grows with set size and age
            q_noise = process_noise * (nS_now / 3.0) * (1.0 + age_unc_boost * older)

            # Prediction error
            pe = r - Q[s, a]

            # Kalman gain
            k_gain = U[s, a] / (U[s, a] + r_var + eps)
            Q[s, a] += k_gain * pe

            # Uncertainty update
            U[s, a] = U[s, a] + q_noise - k_gain * U[s, a]
            # Keep uncertainties positive
            U[s, a] = max(U[s, a], eps)

        nll -= log_p

    return nll


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Actorâ€“Critic with eligibility traces; age and set size modulate trace persistence and exploration.

    This model learns a state value (critic) and action preferences (actor). Eligibility traces allow
    credit assignment across time within a block. Older age and larger set sizes reduce trace
    persistence and effective inverse temperature, capturing reduced maintenance and increased exploration.

    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..set_size-1 within a block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of float
        Reward on each trial (0 or 1).
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size for each trial's block (3 or 6).
    age : array-like or scalar
        Participant age; used to determine age group (older >=45).
    model_parameters : tuple/list
        (alpha_actor, alpha_critic, beta, lambda_trace, age_gamma_drop)
        - alpha_actor: learning rate for actor (>0).
        - alpha_critic: learning rate for critic (>0).
        - beta: base inverse temperature (>0).
        - lambda_trace: baseline eligibility trace decay (0..1).
        - age_gamma_drop: proportional reduction of trace persistence for older adults (>=0).

    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    alpha_a, alpha_c, beta, lambda_trace, age_gamma_drop = model_parameters
    age_scalar = float(age[0] if hasattr(age, "__len__") else age)
    older = 1.0 if age_scalar >= 45 else 0.0

    unique_blocks = np.unique(blocks)
    nll = 0.0
    nA = 3
    eps = 1e-12

    for b in unique_blocks:
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nS = int(block_set_sizes[0])

        # Initialize actor preferences and critic values
        H = np.zeros((nS, nA))          # policy preferences
        V = np.zeros(nS)                # state values

        # Eligibility traces
        eH = np.zeros((nS, nA))
        eV = np.zeros(nS)

        log_p = 0.0
        prev_s = None

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])
            nS_now = int(block_set_sizes[t])

            # Effective inverse temperature reduced by set size and age
            beta_eff = beta / (1.0 + 0.3 * (nS_now - 3) + 0.3 * older)

            # Softmax policy from preferences H
            logits = beta_eff * (H[s, :] - np.max(H[s, :]))
            p_vec = np.exp(logits)
            p_vec = p_vec / np.sum(p_vec)
            p_choice = max(p_vec[a], eps)
            log_p += np.log(p_choice)

            # TD error (no discount across trials; per-trial episodes within block)
            delta = r - V[s]

            # Eligibility decay; reduce persistence for older adults and larger set sizes
            lam_eff = np.clip(lambda_trace * (1.0 - age_gamma_drop * older) * (1.0 - 0.15 * (nS_now - 3)), 0.0, 1.0)
            eH *= lam_eff
            eV *= lam_eff

            # Accumulate eligibilities for current state-action
            # For actor, use compatible features with softmax: increment by (1 - p) for chosen action, -p for others
            eH[s, :] += -p_vec
            eH[s, a] += 1.0
            eV[s] += 1.0

            # Update actor and critic
            H += alpha_a * delta * eH
            V += alpha_c * delta * eV

            prev_s = s

        nll -= log_p

    return nll