def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Asymmetric Q-learning with state-specific forgetting and perseveration bias,
    age-adapted perseveration.

    Idea
    ----
    - Q-values learned with separate learning rates for positive and negative outcomes.
    - Working-memory-like forgetting (decay toward 0) occurs on each state visit and scales with set size.
    - Choice policy uses softmax over Q plus a perseveration bias to repeat the last action in the same state.
    - Age modulates the strength of perseveration: younger reduce perseveration; older increase it.

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..set_size-1 within a block).
    actions : array-like of int
        Chosen action per trial (0,1,2). Invalid actions (<0 or >=3) treated as random; no learning applied.
    rewards : array-like of int or float
        Feedback 0/1 expected. Invalid rewards (not 0/1) cause no learning.
    blocks : array-like of int
        Block index per trial. Q and last-action memory reset per block.
    set_sizes : array-like of int
        Set size (3 or 6); used to scale forgetting strength.
    age : array-like of int or float
        Participant age; younger (<45) vs older (>=45) alters perseveration strength.
    model_parameters : tuple/list of floats
        (alpha_pos, alpha_neg, beta, decay_base, persev_gain)
        - alpha_pos: learning rate for r=1 outcomes.
        - alpha_neg: learning rate for r=0 outcomes.
        - beta: inverse temperature for softmax.
        - decay_base: base decay toward 0 applied on each state visit (0..1).
        - persev_gain: age gain factor; perseveration weight is scaled by
              younger: 1/(1+persev_gain); older: (1+persev_gain).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha_pos, alpha_neg, beta, decay_base, persev_gain = model_parameters
    nA = 3
    eps = 1e-12
    age_val = age[0]
    younger = 1 if age_val < 45 else 0

    total_log_p = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nS = int(block_set_sizes[0])

        Q = np.zeros((nS, nA))
        last_action = -np.ones(nS, dtype=int)  # state-specific last chosen action

        # Perseveration scale determined by age (fixed magnitude embedded in weight below)
        base_persev = 1.0
        age_scale = (1.0 / (1.0 + persev_gain)) if younger == 1 else (1.0 + persev_gain)
        persev_weight = base_persev * age_scale

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r_raw = block_rewards[t]
            r_valid = (r_raw == 0) or (r_raw == 1)
            r = 1.0 if r_raw == 1 else 0.0

            set_size = int(block_set_sizes[t])
            # Scale decay with set size: larger set sizes -> stronger forgetting
            decay_eff = decay_base * (1.0 + 0.5 * max(0, set_size - 3))

            # Apply forgetting only to the visited state (others are not rehersed)
            Q[s, :] *= (1.0 - np.clip(decay_eff, 0.0, 1.0))

            # Policy: softmax over Q plus perseveration bias for repeating last action in this state
            bias = np.zeros(nA)
            if last_action[s] >= 0 and last_action[s] < nA:
                bias[last_action[s]] = persev_weight

            logits = beta * Q[s, :] + bias
            logits -= np.max(logits)
            pi = np.exp(logits) / np.sum(np.exp(logits))

            if a < 0 or a >= nA:
                # Invalid action: treat as random response; no update
                total_log_p += np.log(max(1.0 / nA, eps))
                continue

            total_log_p += np.log(max(pi[a], eps))

            # Learning update if reward valid
            if r_valid:
                err = r - Q[s, a]
                eta = alpha_pos if (r > Q[s, a]) else alpha_neg
                Q[s, a] += eta * err

            # Update last action memory
            last_action[s] = a

    return -float(total_log_p)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Uncertainty-adaptive (Kalman) Q-learning with process noise and perseveration,
    age- and set-size-dependent observation noise.

    Idea
    ----
    - Each state-action has a mean value and uncertainty (variance).
    - Trial-wise learning rate is the Kalman gain K = v/(v + R), where R is observation noise.
    - Observation noise increases with set size (harder memory load) and with older age;
      decreases for younger age.
    - Process noise allows values to drift slowly over time.
    - Choice policy uses softmax over current means plus a perseveration bonus.

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..set_size-1 within a block).
    actions : array-like of int
        Chosen action per trial (0,1,2). Invalid actions treated as random; no learning.
    rewards : array-like of int or float
        Feedback 0/1 expected. Invalid rewards (not 0/1) cause no learning.
    blocks : array-like of int
        Block index per trial. Means/variances reset per block.
    set_sizes : array-like of int
        Set size (3 or 6); scales observation noise.
    age : array-like of int or float
        Participant age; younger (<45) vs older (>=45) changes observation noise scaling.
    model_parameters : tuple/list of floats
        (beta, sigma0, tau, persev, age_unc_gain)
        - beta: inverse temperature for softmax.
        - sigma0: initial standard deviation for each Q (prior uncertainty).
        - tau: process noise added each trial to variance (small positive).
        - persev: perseveration bonus added to last chosen action in the same state.
        - age_unc_gain: scales observation noise:
              younger: R *= 1/(1+age_unc_gain)
              older:   R *= (1+age_unc_gain)
          Set size scaling: R *= (1 + 0.5*(set_size-3)).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    beta, sigma0, tau, persev, age_unc_gain = model_parameters
    nA = 3
    eps = 1e-12
    age_val = age[0]
    younger = 1 if age_val < 45 else 0

    total_log_p = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nS = int(block_set_sizes[0])

        m = np.zeros((nS, nA))               # means
        v = (sigma0 ** 2) * np.ones((nS, nA))  # variances
        last_action = -np.ones(nS, dtype=int)

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r_raw = block_rewards[t]
            r_valid = (r_raw == 0) or (r_raw == 1)
            r = 1.0 if r_raw == 1 else 0.0

            set_size = int(block_set_sizes[t])

            # Age and set-size dependent observation noise
            age_factor = (1.0 / (1.0 + age_unc_gain)) if younger == 1 else (1.0 + age_unc_gain)
            ss_factor = (1.0 + 0.5 * max(0, set_size - 3))
            R = 1.0 * age_factor * ss_factor  # observation noise

            # Policy
            bias = np.zeros(nA)
            if last_action[s] >= 0 and last_action[s] < nA:
                bias[last_action[s]] = persev

            logits = beta * m[s, :] + bias
            logits -= np.max(logits)
            pi = np.exp(logits) / np.sum(np.exp(logits))

            if a < 0 or a >= nA:
                total_log_p += np.log(max(1.0 / nA, eps))
                # Variance evolves via process noise for visited state
                v[s, :] = np.maximum(v[s, :] + tau, eps)
                continue

            total_log_p += np.log(max(pi[a], eps))

            if r_valid:
                # Kalman gain on the chosen action
                K = v[s, a] / (v[s, a] + R)
                m[s, a] = m[s, a] + K * (r - m[s, a])
                v[s, a] = (1.0 - K) * v[s, a] + tau
                # Optional small process noise to unchosen actions in state
                for a_unc in range(nA):
                    if a_unc != a:
                        v[s, a_unc] = np.maximum(v[s, a_unc] + tau, eps)
            else:
                # Only variance evolves if reward invalid
                v[s, :] = np.maximum(v[s, :] + tau, eps)

            last_action[s] = a

    return -float(total_log_p)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Bayesian action-value learning with Dirichlet-Beta posteriors, softmax choice, and adaptive lapse,
    modulated by set size and age.

    Idea
    ----
    - For each state-action, maintain a Beta(a,b) posterior for P(reward=1).
    - Choice policy: softmax over posterior means.
    - Lapse rate increases with set size (higher cognitive load) and with older age; decreases for younger age.
      Lapse mixes uniform random responding into the policy.

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..set_size-1 within a block).
    actions : array-like of int
        Chosen action per trial (0,1,2). Invalid actions treated as pure lapse (uniform); no learning.
    rewards : array-like of int or float
        Feedback 0/1 expected. Invalid rewards (not 0/1) cause no learning.
    blocks : array-like of int
        Block index per trial. Posteriors reset per block.
    set_sizes : array-like of int
        Set size (3 or 6); scales lapse.
    age : array-like of int or float
        Participant age; younger (<45) vs older (>=45) alters lapse.
    model_parameters : tuple/list of floats
        (prior_strength, beta, lapse_base, ss_lapse_slope, age_lapse_gain)
        - prior_strength: symmetric Beta prior (alpha0=beta0=prior_strength) for each state-action.
        - beta: inverse temperature for softmax over posterior means.
        - lapse_base: base lapse probability (0..1), will be clamped internally to [0, 0.49].
        - ss_lapse_slope: degree to which lapse increases with set size above 3.
        - age_lapse_gain: age scaling on lapse:
              younger: lapse *= 1/(1+age_lapse_gain)
              older:   lapse *= (1+age_lapse_gain)

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    prior_strength, beta, lapse_base, ss_lapse_slope, age_lapse_gain = model_parameters
    nA = 3
    eps = 1e-12
    age_val = age[0]
    younger = 1 if age_val < 45 else 0

    total_log_p = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nS = int(block_set_sizes[0])

        # Beta parameters for each state-action
        a_param = prior_strength * np.ones((nS, nA))
        b_param = prior_strength * np.ones((nS, nA))

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r_raw = block_rewards[t]
            r_valid = (r_raw == 0) or (r_raw == 1)
            r = 1.0 if r_raw == 1 else 0.0

            set_size = int(block_set_sizes[t])

            # Posterior mean estimates
            mu = a_param[s, :] / np.maximum(a_param[s, :] + b_param[s, :], eps)

            # Softmax policy over posterior means
            logits = beta * (mu - np.max(mu))
            pi = np.exp(logits) / np.sum(np.exp(logits))

            # Adaptive lapse
            ss_scale = 1.0 + ss_lapse_slope * max(0, set_size - 3)
            age_scale = (1.0 / (1.0 + age_lapse_gain)) if younger == 1 else (1.0 + age_lapse_gain)
            lapse = lapse_base * ss_scale * age_scale
            # Clamp to a reasonable range to avoid degenerate policies
            lapse = float(np.clip(lapse, 0.0, 0.49))

            # Mixture with uniform
            mix_pi = (1.0 - lapse) * pi + lapse * (1.0 / nA)

            if a < 0 or a >= nA:
                total_log_p += np.log(max(1.0 / nA, eps))
                continue

            total_log_p += np.log(max(mix_pi[a], eps))

            # Update Beta posterior if reward valid
            if r_valid:
                if r > 0.5:
                    a_param[s, a] += 1.0
                else:
                    b_param[s, a] += 1.0

    return -float(total_log_p)