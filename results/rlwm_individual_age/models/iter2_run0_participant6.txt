def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with age- and load-modulated forgetting and temperature, plus state-wise action stickiness.

    Mechanism
    - Q-learning with a global forgetting factor that grows with set size (load) and is stronger in older adults.
    - Softmax inverse temperature is modulated by age and set size: younger adults become relatively more decisive
      under low load (3) and older adults relatively less decisive under high load (6).
    - Action stickiness adds a bias toward repeating the last action taken in the same state.

    Parameters
    ----------
    states : array-like of int
        State index per trial within each block (0..set_size-1).
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of {0,1}
        Reward outcome per trial.
    blocks : array-like of int
        Block index per trial. Values identify block membership.
    set_sizes : array-like of int
        Set size (3 or 6) per trial, constant within a block.
    age : array-like or scalar
        Participant age. Age group is younger (<45) vs older (>=45).
    model_parameters : tuple/list
        Five parameters:
        - alpha: Learning rate for Q-learning (0..1).
        - beta: Base inverse temperature for softmax (>0).
        - stickiness: Bias added to the last action taken in the same state (can be +/-).
        - forget_base: Base forgetting rate per trial (0..1). Scaled by age and set size.
        - temp_scale_base: Base modulation of temperature by age and set size (can be +/-).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, stickiness, forget_base, temp_scale_base = model_parameters
    age_val = age[0] if hasattr(age, "__len__") else age
    is_younger = 1 if age_val < 45 else 0

    nll = 0.0
    eps = 1e-12
    nA = 3

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nS = int(block_set_sizes[0])
        ss = float(nS)

        # Initialize Q-values per block
        Q = np.zeros((nS, nA))

        # Track last chosen action per state for stickiness
        last_action = -1 * np.ones(nS, dtype=int)

        # Age- and set-size-modulated forgetting factor applied every trial
        # Older adults and larger sets => stronger forgetting
        age_forget_scale = 0.9 if is_younger else 1.2
        forget_rate = forget_base * age_forget_scale * (ss / 3.0)
        forget_rate = min(max(forget_rate, 0.0), 1.0)

        # Age- and set-size-modulated temperature scaling
        # Younger: more decisive at small set size; Older: less decisive at large set size
        age_dir = 1.0 if is_younger else -1.0
        temp_scale = 1.0 + temp_scale_base * (age_dir * (3.0 / ss))
        temp_scale = max(temp_scale, 1e-3)  # keep positive
        beta_eff = beta * temp_scale

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Apply global forgetting
            Q *= (1.0 - forget_rate)

            # Softmax with stickiness on last action in the same state
            logits = beta_eff * Q[s, :].copy()
            if last_action[s] >= 0:
                logits[last_action[s]] += stickiness

            logits -= np.max(logits)
            p = np.exp(logits)
            p /= np.sum(p)

            pa = max(p[a], eps)
            nll -= np.log(pa)

            # Q-learning update
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # Update last action for stickiness
            last_action[s] = a

    return nll


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Hybrid RL + Bayesian associative memory with age-/load-weighted arbitration and slip.

    Mechanism
    - RL system: Q-learning drives graded value expectations.
    - Bayesian WM: For each state, maintain a Dirichlet count over actions that increases when
      rewarded for that state-action; the normalized counts define a WM policy.
    - Arbitration: Mix WM and RL policies using a weight that increases for younger adults and
      for lower set sizes (WM more reliable in low load and in youth).
    - Slip: With some probability, responses are uniformly random (execution noise).

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..set_size-1).
    actions : array-like of int
        Chosen action (0..2).
    rewards : array-like of {0,1}
        Reward feedback.
    blocks : array-like of int
        Block identifiers.
    set_sizes : array-like of int
        Set size per trial.
    age : array-like or scalar
        Participant age; younger (<45) vs older (>=45).
    model_parameters : tuple/list
        Five parameters:
        - alpha: RL learning rate (0..1).
        - beta: RL inverse temperature (>0).
        - phi_base: Base arbitration weight favoring WM (0..1).
        - dirichlet_prior: Prior pseudo-count for WM per action (>0).
        - slip: Execution noise; probability of uniform random response (0..1).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, phi_base, dirichlet_prior, slip = model_parameters
    age_val = age[0] if hasattr(age, "__len__") else age
    is_younger = 1 if age_val < 45 else 0

    nll = 0.0
    eps = 1e-12
    nA = 3

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nS = int(block_set_sizes[0])
        ss = float(nS)

        # Initialize RL values and Bayesian WM counts
        Q = np.zeros((nS, nA))
        counts = dirichlet_prior * np.ones((nS, nA))

        # Arbitration weight: higher for younger and small set size
        age_scale = 1.2 if is_younger else 0.8
        phi = phi_base * age_scale * (3.0 / ss)
        phi = min(max(phi, 0.0), 1.0)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # RL policy
            logits = beta * Q[s, :].copy()
            logits -= np.max(logits)
            p_rl = np.exp(logits)
            p_rl /= np.sum(p_rl)

            # Bayesian WM policy: normalized Dirichlet counts for state
            p_wm = counts[s, :] / np.sum(counts[s, :])

            # Mix policies and add slip
            p_mix = phi * p_wm + (1.0 - phi) * p_rl
            p_final = (1.0 - slip) * p_mix + slip * (1.0 / nA)

            pa = max(p_final[a], eps)
            nll -= np.log(pa)

            # Update RL
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # Update WM counts only on rewarded outcomes for the chosen action
            if r > 0.5:
                counts[s, a] += 1.0

    return nll


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Uncertainty-based meta-control between WM recall and RL, with age and load biases.

    Mechanism
    - RL: Q-learning provides graded choice values.
    - WM recall: remembers the last rewarded action for each state; if none, WM is uninformative (uniform).
    - Arbitration weight w_t is computed via a logistic function of:
        (i) WM success trace for the state (EMA of rewards in that state),
        (ii) RL uncertainty (proxied by 1 / (1 + state visit count)),
        (iii) age bias (younger vs older), and
        (iv) set-size bias (load).
      Higher w_t shifts policy toward WM.
    - Final policy is a convex combination of WM and RL.

    Parameters
    ----------
    states : array-like of int
        State per trial (0..set_size-1).
    actions : array-like of int
        Action per trial (0..2).
    rewards : array-like of {0,1}
        Reward outcome.
    blocks : array-like of int
        Block identifier.
    set_sizes : array-like of int
        Set size per trial.
    age : array-like or scalar
        Participant age; age group used as a signed bias (younger=+1, older=-1).
    model_parameters : tuple/list
        Five parameters:
        - alpha: Learning rate for both Q-learning and WM success trace (0..1).
        - beta: Inverse temperature for RL (>0).
        - k_arbit: Gain on (WM success - RL uncertainty) in the arbitration logistic (can be +/-).
        - bias_age: Additive age bias in the logistic (younger=+bias_age, older=-bias_age).
        - bias_set: Additive set-size bias in the logistic scaled by (3/set_size - 1) (can be +/-).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, k_arbit, bias_age, bias_set = model_parameters
    age_val = age[0] if hasattr(age, "__len__") else age
    is_younger = 1 if age_val < 45 else 0
    age_sign = 1.0 if is_younger else -1.0

    nll = 0.0
    eps = 1e-12
    nA = 3

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nS = int(block_set_sizes[0])
        ss = float(nS)

        # RL values, WM memory (last rewarded action), WM success trace, and state visit counts
        Q = np.zeros((nS, nA))
        last_rew_action = -1 * np.ones(nS, dtype=int)
        wm_success = 0.5 * np.ones(nS)  # EMA of reward per state
        visits = np.zeros(nS)

        # Set-size bias term (negative for larger than 3, positive for 3)
        set_bias_term = bias_set * ((3.0 / ss) - 1.0)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # RL policy
            logits = beta * Q[s, :].copy()
            logits -= np.max(logits)
            p_rl = np.exp(logits)
            p_rl /= np.sum(p_rl)

            # WM policy: deterministic if last rewarded action known, else uniform
            if last_rew_action[s] >= 0:
                p_wm = np.zeros(nA)
                p_wm[last_rew_action[s]] = 1.0
            else:
                p_wm = (1.0 / nA) * np.ones(nA)

            # RL uncertainty via inverse state visit count
            u_rl = 1.0 / (1.0 + max(1.0, visits[s]))  # decreases as visits increase

            # Arbitration weight via logistic
            z = k_arbit * (wm_success[s] - u_rl) + bias_age * age_sign + set_bias_term
            w = 1.0 / (1.0 + np.exp(-z))

            p_final = w * p_wm + (1.0 - w) * p_rl

            pa = max(p_final[a], eps)
            nll -= np.log(pa)

            # Update RL
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # Update WM memory on reward
            if r > 0.5:
                last_rew_action[s] = a

            # Update WM success trace (EMA) and visits
            wm_success[s] = (1.0 - alpha) * wm_success[s] + alpha * r
            visits[s] += 1.0

    return nll