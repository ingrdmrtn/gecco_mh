def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited, decaying working memory (WM) mixture with age-modulated capacity.

    Idea:
    - Two systems contribute to choice:
      (i) Model-free RL (Q-learning).
      (ii) A capacity-limited WM store that keeps the most recent rewarded action per state with a decaying strength.
    - The probability that a rewarded association is successfully stored depends on effective capacity (K_eff),
      which is modulated by age group and set size (p_store = min(1, K_eff / nS)).
    - WM strength decays each trial; arbitration weight equals the current WM strength for that state.
      WM policy is deterministic for the stored action (mixture prevents zero-likelihood).
    - Age group: younger (<45) = +1, older (>=45) = -1, scales capacity.

    Parameters (model_parameters):
    - alpha: (0,1), RL learning rate.
    - beta: >0, inverse temperature for RL softmax (internally scaled).
    - K_base: >0, baseline WM capacity in "slots".
    - K_age_shift: real, additive capacity shift per age group sign (positive => more capacity for younger).
    - wm_decay: (0,1), per-trial decay of WM strength for all stored items.

    Inputs:
    - states, actions, rewards, blocks, set_sizes: arrays of shape (T,)
    - age: array-like containing one numeric value (years).
    - model_parameters: [alpha, beta, K_base, K_age_shift, wm_decay]

    Returns:
    - Negative log-likelihood of observed choices.
    """
    alpha, beta, K_base, K_age_shift, wm_decay = model_parameters
    beta = max(1e-6, beta) * 6.0
    wm_decay = np.clip(wm_decay, 0.0, 1.0)
    age_years = float(age[0])
    age_group = 1.0 if age_years < 45.0 else -1.0

    nA = 3
    total_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask]
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_set_sizes = set_sizes[mask]
        nS = int(block_set_sizes[0])

        # RL value function
        Q = np.zeros((nS, nA))

        # WM store: for each state, a stored action (or -1) and a strength in [0,1]
        stored_action = -1 * np.ones(nS, dtype=int)
        strength = np.zeros(nS)

        # Effective capacity and store probability
        K_eff = max(0.0, K_base + K_age_shift * age_group)
        p_store = min(1.0, K_eff / float(nS))

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # RL policy
            logits = beta * (Q[s, :] - np.max(Q[s, :]))
            pi = np.exp(logits)
            pi = pi / np.sum(pi)

            # WM policy for current state
            if stored_action[s] >= 0:
                p_wm_vec = np.zeros(nA)
                p_wm_vec[stored_action[s]] = 1.0
            else:
                p_wm_vec = np.ones(nA) / nA  # if nothing stored, WM provides uniform (will be down-weighted by strength=0)

            # Arbitration weight is the WM strength of this state
            w = np.clip(strength[s], 0.0, 1.0)
            p_total = w * p_wm_vec[a] + (1.0 - w) * pi[a]
            p_total = max(1e-12, float(p_total))
            total_log_p += np.log(p_total)

            # Learning updates
            Q[s, a] += alpha * (r - Q[s, a])

            # WM decay for all states each trial
            strength *= (1.0 - wm_decay)

            # WM storage on rewarded trials: raise strength toward 1 with probability mass p_store
            if r > 0.0:
                stored_action[s] = a
                # Deterministic expected update toward 1 with rate p_store
                strength[s] = strength[s] + (1.0 - strength[s]) * p_store

    return -total_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    SARSA(λ) with age- and load-modulated eligibility traces and a "last-rewarded" action bonus.

    Idea:
    - On-policy TD learning with eligibility traces (replacing type simplified here).
    - Eligibility decay λ is modulated by:
        age group (younger <45 vs older >=45) and set size via a logistic transform.
      This captures longer effective credit assignment for younger and under lower load.
    - A state-local bonus encourages selecting the last rewarded action in that state.
      This is distinct from perseveration because it only boosts actions that previously yielded reward in that state.

    Parameters (model_parameters):
    - alpha: (0,1), learning rate.
    - beta: >0, inverse temperature for softmax (internally scaled).
    - lam_base: real, baseline logit for λ.
    - lam_age_slope: real, how age group shifts λ (positive -> larger λ for younger).
    - lam_size_slope: real, how set size modifies λ via (3/nS - 1) contrast.
    - kappa_r: >=0, additive bonus to the last rewarded action's logit in current state.

    Inputs:
    - states, actions, rewards, blocks, set_sizes: arrays (T,)
    - age: array-like with one value.
    - model_parameters: [alpha, beta, lam_base, lam_age_slope, lam_size_slope, kappa_r]

    Returns:
    - Negative log-likelihood of observed choices.
    """
    alpha, beta, lam_base, lam_age_slope, lam_size_slope, kappa_r = model_parameters
    beta = max(1e-6, beta) * 6.0
    kappa_r = max(0.0, kappa_r)
    age_years = float(age[0])
    age_group = 1.0 if age_years < 45.0 else -1.0

    nA = 3
    total_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask]
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_set_sizes = set_sizes[mask]
        nS = int(block_set_sizes[0])

        # Q-values and eligibility traces
        Q = np.zeros((nS, nA))
        Z = np.zeros((nS, nA))

        # Last rewarded action per state (-1 if none)
        last_rew_action = -1 * np.ones(nS, dtype=int)

        # λ modulation by age and load using logistic
        size_contrast = (3.0 / float(nS)) - 1.0  # 0 at nS=3, negative at nS=6
        lam_logit = lam_base + lam_age_slope * age_group + lam_size_slope * size_contrast
        lam_eff = 1.0 / (1.0 + np.exp(-lam_logit))  # in (0,1)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Choice policy with bonus for last rewarded action in state s
            bonus = np.zeros(nA)
            if last_rew_action[s] >= 0:
                bonus[last_rew_action[s]] = kappa_r

            logits = beta * (Q[s, :] - np.max(Q[s, :])) + bonus
            pi = np.exp(logits)
            pi = pi / np.sum(pi)

            p = max(1e-12, float(pi[a]))
            total_log_p += np.log(p)

            # Update eligibility traces (replacing style: increment chosen SA; decay all by λ)
            Z *= lam_eff
            Z[s, a] = 1.0  # replacing eligibility for current SA

            # TD error (bandit-like immediate reward; no next-state term)
            delta = r - Q[s, a]

            # Update Q with eligibility traces
            Q += alpha * delta * Z

            # Track last rewarded action for bonus
            if r > 0.0:
                last_rew_action[s] = a

    return -total_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with uncertainty-gated WM retrieval: age and load modulate WM access probability.

    Idea:
    - RL (Q-learning) runs continuously.
    - A simple WM store keeps the most recent rewarded action per state (no graded strength).
    - The arbitration weight for WM on each trial equals a retrieval probability p_ret that depends on:
        base bias (eta0), age group (eta_age), and set size load (eta_load).
      Additionally, p_ret is attenuated when RL is already certain: p_eff = p_ret * (1 - H/ln(nA)),
      where H is the entropy of the RL policy for that state.
    - If the state has no stored action yet, WM weight is 0.

    Parameters (model_parameters):
    - alpha: (0,1), RL learning rate.
    - beta: >0, inverse temperature for RL softmax (internally scaled).
    - eta0: real, baseline WM retrieval bias (logit).
    - eta_age: real, age-group effect on WM retrieval (positive -> more WM for younger).
    - eta_load: real, load effect on WM retrieval via factor (3/nS) (positive -> more WM under smaller sets).

    Inputs:
    - states, actions, rewards, blocks, set_sizes: arrays (T,)
    - age: array-like with one value (years).
    - model_parameters: [alpha, beta, eta0, eta_age, eta_load]

    Returns:
    - Negative log-likelihood of observed choices.
    """
    alpha, beta, eta0, eta_age, eta_load = model_parameters
    beta = max(1e-6, beta) * 6.0
    age_years = float(age[0])
    age_group = 1.0 if age_years < 45.0 else -1.0

    nA = 3
    log_nA = np.log(nA)
    total_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask]
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_set_sizes = set_sizes[mask]
        nS = int(block_set_sizes[0])

        # RL values
        Q = np.zeros((nS, nA))

        # WM: last rewarded action per state (-1 if none)
        wm_action = -1 * np.ones(nS, dtype=int)

        # Retrieval logit components fixed within block (depend on age and load)
        load_factor = 3.0 / float(nS)
        ret_logit = eta0 + eta_age * age_group + eta_load * load_factor
        p_ret = 1.0 / (1.0 + np.exp(-ret_logit))  # in (0,1)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # RL softmax policy
            logits = beta * (Q[s, :] - np.max(Q[s, :]))
            pi = np.exp(logits)
            pi = pi / np.sum(pi)

            # Entropy-based attenuation of WM when RL is already confident
            H = -np.sum(pi * (np.log(pi + 1e-12)))
            conf_factor = 1.0 - H / log_nA  # 0 (uniform) to 1 (deterministic)

            # Effective WM weight only if something is stored for this state
            if wm_action[s] >= 0:
                w = p_ret * conf_factor
                w = np.clip(w, 0.0, 1.0)
                p_wm_vec = np.zeros(nA)
                p_wm_vec[wm_action[s]] = 1.0
                p_total = w * p_wm_vec[a] + (1.0 - w) * pi[a]
            else:
                p_total = pi[a]

            p_total = max(1e-12, float(p_total))
            total_log_p += np.log(p_total)

            # Learning updates
            Q[s, a] += alpha * (r - Q[s, a])

            # WM storage on rewarded trials (write latest correct action)
            if r > 0.0:
                wm_action[s] = a

    return -total_log_p