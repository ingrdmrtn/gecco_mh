def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Q-learning with set-size/age-modulated learning rate, and state-wise perseveration (sticky choice) plus lapses.

    The model assumes:
    - Model-free Q-learning with an effective learning rate that decreases with set size and more strongly for older adults.
    - A perseveration bias that favors repeating the previous action in the same state. This bias is scaled down by set size
      and amplified by age.
    - Lapse (uniform) choice probability that increases with set size and age.

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..nS-1).
    actions : array-like of int
        Observed action (0..2). Out-of-range treated as lapse (uniform likelihood, no learning).
    rewards : array-like of float
        Feedback (0/1 typical). Negative values treated as invalid (uniform likelihood, no learning).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size (3 or 6).
    age : array-like of float
        Participant age; single-element array. Older group: age > 45.
    model_parameters : iterable of 5 floats
        alpha_base : Baseline learning rate (0..1).
        beta       : Inverse temperature (>0) for softmax.
        tau_stick  : Stickiness strength for repeating last action in a state (>=0).
        eps0       : Baseline lapse rate (>=0), further increased by set size and age.
        age_mod    : Age modulation factor (>=0). Scales both stickiness and learning-rate attenuation for older adults.

    Returns
    -------
    neg_log_likelihood : float
        Negative log-likelihood of the observed choices.
    """
    alpha_base, beta, tau_stick, eps0, age_mod = model_parameters
    age_val = age[0]
    is_older = 1.0 if age_val > 45 else 0.0

    total_log_p = 0.0
    nA = 3

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nS = int(block_set_sizes[0])
        Q = np.zeros((nS, nA))
        last_action = -1 * np.ones(nS, dtype=int)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = block_rewards[t]
            ss = float(block_set_sizes[t])

            if not (0 <= s < nS) or not (0 <= a < nA) or r < 0:
                # Invalid trial: treat as uniform lapse, no learning
                total_log_p += np.log(1.0 / nA)
                # Do not update last_action on invalid trials
                continue

            # Age- and set-size-dependent lapse
            eps = eps0 * (ss / 3.0) * (1.0 + 0.5 * is_older)
            eps = min(max(eps, 0.0), 0.5)

            # Stickiness bias: favor repeating last action in the same state
            stick_strength = tau_stick * (1.0 + age_mod * is_older) / ss
            pref = beta * Q[s, :].copy()
            if last_action[s] >= 0 and last_action[s] < nA:
                pref[last_action[s]] += stick_strength

            # Softmax
            m = np.max(pref)
            expv = np.exp(pref - m)
            pi = expv / np.sum(expv)

            # Lapse-mixture choice probability
            pa = (1.0 - eps) * pi[a] + eps * (1.0 / nA)
            pa = np.clip(pa, 1e-12, 1.0)
            total_log_p += np.log(pa)

            # Learning rate decreases with set size, more so for older adults
            alpha_eff = alpha_base / (1.0 + (ss - 3.0) * (1.0 + age_mod * is_older))
            alpha_eff = max(0.0, min(1.0, alpha_eff))

            # Q-learning update
            delta = r - Q[s, a]
            Q[s, a] += alpha_eff * delta

            # Update last action memory
            last_action[s] = a

    return -float(total_log_p)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited working memory with arbitration based on recall probability.

    The policy is a mixture between:
    - A model-free Q-learning softmax policy.
    - A WM policy that recalls the last rewarded action for a state if stored.

    WM capacity K is reduced for larger set sizes and for older adults via an age shift. Arbitration weight is a logistic
    function of the recall probability.

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..nS-1).
    actions : array-like of int
        Observed action (0..2). Out-of-range treated as lapse (uniform likelihood, no learning).
    rewards : array-like of float
        Feedback (0/1 typical). Negative values treated as invalid (uniform likelihood, no learning).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size (3 or 6).
    age : array-like of float
        Participant age; single-element array. Older group: age > 45.
    model_parameters : iterable of 5 floats
        alpha     : Q-learning rate (0..1).
        beta      : Inverse temperature for RL softmax (>0).
        K_base    : Baseline WM capacity in items (>=0).
        age_shift : Capacity penalty applied in older group (>=0).
        arb_temp  : Arbitration steepness (>0). Higher -> more categorical reliance on WM when recall is likely.

    Returns
    -------
    neg_log_likelihood : float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, K_base, age_shift, arb_temp = model_parameters
    age_val = age[0]
    is_older = 1.0 if age_val > 45 else 0.0

    total_log_p = 0.0
    nA = 3

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nS = int(block_set_sizes[0])

        Q = np.zeros((nS, nA))

        # WM store: stores last rewarded action for each state (or -1 if none)
        wm_action = -1 * np.ones(nS, dtype=int)
        wm_conf = np.zeros(nS)  # 1 if last stored action was positively reinforced

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = block_rewards[t]
            ss = float(block_set_sizes[t])

            if not (0 <= s < nS) or not (0 <= a < nA) or r < 0:
                total_log_p += np.log(1.0 / nA)
                continue

            # RL softmax policy
            pref = beta * Q[s, :]
            m = np.max(pref)
            expv = np.exp(pref - m)
            pi_rl = expv / np.sum(expv)

            # WM policy: point mass on stored rewarded action if available; else uniform
            if wm_conf[s] > 0.5 and (0 <= wm_action[s] < nA):
                p_wm = np.zeros(nA)
                p_wm[wm_action[s]] = 1.0
            else:
                p_wm = np.ones(nA) / nA

            # Effective WM capacity and recall probability
            K_eff = max(0.0, K_base - age_shift * is_older)
            p_recall = min(1.0, K_eff / max(1.0, ss))

            # Arbitration weight: logistic transform centered at 0.5 recall
            wm_weight = 1.0 / (1.0 + np.exp(-(p_recall - 0.5) * max(1e-6, arb_temp)))

            # Mixture policy
            p_mix = wm_weight * p_wm + (1.0 - wm_weight) * pi_rl
            pa = np.clip(p_mix[a], 1e-12, 1.0)
            total_log_p += np.log(pa)

            # Learning updates
            delta = r - Q[s, a]
            Q[s, a] += alpha * delta

            # WM update: store action if rewarded, clear confidence otherwise
            if r > 0:
                wm_action[s] = a
                wm_conf[s] = 1.0
            else:
                wm_conf[s] = 0.0  # forgetting of confidence when unrewarded

    return -float(total_log_p)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    SARSA(λ) with eligibility traces, age- and set-size-dependent trace strength and forgetting.

    The model assumes:
    - On-policy temporal-difference learning with eligibility traces.
    - Trace strength λ decreases with set size and more for older adults.
    - Parametric forgetting (value decay toward 0) that increases with set size and age.
    - Choices follow a softmax over Q-values.

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..nS-1).
    actions : array-like of int
        Observed action (0..2). Out-of-range treated as lapse (uniform likelihood, limited decay).
    rewards : array-like of float
        Feedback (0/1 typical). Negative values treated as invalid (uniform likelihood, limited decay).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size (3 or 6).
    age : array-like of float
        Participant age; single-element array. Older group: age > 45.
    model_parameters : iterable of 5 floats
        alpha    : Learning rate (0..1).
        beta     : Inverse temperature (>0).
        lam0     : Baseline trace strength control (real). Passed through sigmoid to (0,1).
        decay0   : Baseline forgetting control (real). Passed through sigmoid to (0,1).
        age_gain : Age sensitivity (>=0) that increases forgetting and reduces λ for older adults.

    Returns
    -------
    neg_log_likelihood : float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, lam0, decay0, age_gain = model_parameters
    age_val = age[0]
    is_older = 1.0 if age_val > 45 else 0.0

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    total_log_p = 0.0
    nA = 3
    gamma = 1.0  # episodic blocks, undiscounted within block

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nS = int(block_set_sizes[0])

        Q = np.zeros((nS, nA))
        E = np.zeros((nS, nA))  # eligibility traces

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = block_rewards[t]
            ss = float(block_set_sizes[t])

            # Compute age- and set-size-dependent parameters
            lam_eff = sigmoid(lam0 - 0.6 * (ss - 3.0) - age_gain * is_older)
            decay_eff = sigmoid(decay0 + 0.6 * (ss - 3.0) + age_gain * is_older)

            # Apply forgetting to Q-values each trial
            Q *= (1.0 - 0.2 * decay_eff)

            # Compute policy for current state
            if 0 <= s < nS:
                pref = beta * Q[s, :]
                m = np.max(pref)
                expv = np.exp(pref - m)
                pi = expv / np.sum(expv)
            else:
                # If state invalid, use uniform
                pi = np.ones(nA) / nA

            # Likelihood and learning handling invalid trials
            invalid = not (0 <= s < nS) or not (0 <= a < nA) or r < 0

            if invalid:
                total_log_p += np.log(1.0 / nA)
                # Decay traces even on invalid trials
                E *= gamma * lam_eff
                continue

            # Log-prob of observed action
            pa = np.clip(pi[a], 1e-12, 1.0)
            total_log_p += np.log(pa)

            # Next step info for SARSA target (if available)
            if t + 1 < len(block_states):
                s_next = int(block_states[t + 1])
                a_next = int(block_actions[t + 1])
                if 0 <= s_next < nS and 0 <= a_next < nA and block_rewards[t + 1] >= 0:
                    Q_next = Q[s_next, a_next]
                else:
                    Q_next = 0.0
            else:
                Q_next = 0.0

            # Eligibility trace update
            E *= gamma * lam_eff
            E[s, a] += 1.0

            # TD error and Q update
            delta = r + gamma * Q_next - Q[s, a]
            Q += alpha * delta * E

    return -float(total_log_p)