def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with decaying working-memory confidence and arbitration by evidence, modulated by load and age.

    Idea
    ----
    - A slow RL system learns Q-values per state via prediction errors.
    - A fast working-memory (WM) confidence map tracks the strength of a specific stateâ†’action binding.
      Confidence decays with cognitive load (set size) and is further penalized in the older group (>=45).
    - Arbitration: the policy is a weighted mixture of RL and WM. The WM weight increases with the
      margin of the best WM action over competitors, and decreases with load and age penalty.
    - Choice is a lapse-mixture of the combined policy and a uniform random policy.

    Parameters
    ----------
    states : array-like of int
        State index on each trial (within each block, states range from 0..nS-1).
    actions : array-like of int
        Chosen action index (0..2).
    rewards : array-like of float
        Binary feedback (0 or 1).
    blocks : array-like of int
        Block index for each trial. States reset across blocks.
    set_sizes : array-like of int
        Set size per block (3 or 6 here).
    age : array-like (length 1)
        Participant age in years. Used to determine age group (<45 younger, >=45 older).
    model_parameters : list/tuple of 6 numbers
        - learn_rate: scalar in [0,1], RL learning rate
        - inv_temp: >0, inverse temperature for the RL softmax
        - wm_conf_gain: >0, gain to increase/decrease WM confidence after feedback
        - decay_base: >=0, base decay of WM confidence per trial; scaled by load and age
        - arb_slope: >0, slope for sigmoid arbitration as a function of WM evidence margin
        - lapse: in [0, 0.2], lapse probability mixing with uniform choice

    Returns
    -------
    float
        Negative log-likelihood of the observed sequence of choices.
    """
    learn_rate, inv_temp, wm_conf_gain, decay_base, arb_slope, lapse = model_parameters
    nA = 3
    beta = max(inv_temp, 1e-6) * 5.0
    lapse = float(np.clip(lapse, 0.0, 0.2))
    wm_conf_gain = max(wm_conf_gain, 1e-6)
    decay_base = max(decay_base, 0.0)
    arb_slope = max(arb_slope, 1e-6)
    age_group = 1.0 if age[0] >= 45 else 0.0

    total_loglik = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask]
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        nS = int(set_sizes[mask][0])

        # RL values and WM confidence map (bounded in [0,1])
        Q = (1.0 / nA) * np.ones((nS, nA))
        C = np.zeros((nS, nA))  # WM confidence, starts at 0 (unknown)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Load factor from set size (0 for 3, 1 for 6)
            load = (max(nS, 3) - 3) / 3.0

            # RL policy
            logits_rl = beta * Q[s, :]
            logits_rl -= np.max(logits_rl)
            p_rl = np.exp(logits_rl)
            p_rl /= np.sum(p_rl)

            # WM policy: softmax over WM confidence for the state
            # Use a temperature tied to confidence gain (sharper with larger gain)
            wm_temp = 5.0 * wm_conf_gain
            wm_logits = wm_temp * C[s, :]
            wm_logits -= np.max(wm_logits)
            p_wm_vec = np.exp(wm_logits)
            p_wm_vec /= np.sum(p_wm_vec)

            # Arbitration weight: sigmoid of WM margin minus penalties for load and age
            c_s = C[s, :]
            # Evidence margin: best minus mean of others
            best_idx = int(np.argmax(c_s))
            margin = c_s[best_idx] - np.mean(np.delete(c_s, best_idx)) if nA > 1 else c_s[best_idx]
            penalty = 1.0 * load + 0.5 * age_group  # additive penalty on the logit
            gate_logit = arb_slope * (margin - penalty)
            p_wm = 1.0 / (1.0 + np.exp(-gate_logit))

            # Lapse-mixture over the combined policy and uniform
            p_mix = p_wm * p_wm_vec + (1.0 - p_wm) * p_rl
            p_choice = (1.0 - lapse) * p_mix[a] + lapse * (1.0 / nA)
            p_choice = float(np.clip(p_choice, 1e-12, 1.0))
            total_loglik += np.log(p_choice)

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += learn_rate * pe

            # WM confidence decay increases with load and age
            decay = decay_base * (1.0 + load) * (1.0 + 0.5 * age_group)
            decay = np.clip(decay, 0.0, 1.0)
            C[s, :] *= (1.0 - decay)

            # WM confidence update: move chosen action toward r (0 or 1) with gain
            # Reward strengthens confidence; no-reward weakens it.
            C[s, a] += wm_conf_gain * (r - C[s, a])
            # Keep within [0,1]
            C[s, a] = np.clip(C[s, a], 0.0, 1.0)

    return -float(total_loglik)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with load- and age-dependent credit spread (imperfect credit assignment), plus lapse.

    Idea
    ----
    - The agent learns Q-values per state via prediction errors but, under higher load and in older age,
      correct credit assignment is degraded: some fraction of the prediction error intended for the chosen
      action "spreads" to non-chosen actions within the same state.
    - This spread applies for both positive and negative prediction errors (thus can inflate or suppress
      competing actions).
    - Choice is made via softmax; a small lapse probability mixes in uniform random choice.

    Parameters
    ----------
    states : array-like of int
        State index on each trial (within block).
    actions : array-like of int
        Chosen action (0..2).
    rewards : array-like of float
        Binary feedback (0/1).
    blocks : array-like of int
        Block index for each trial. Values reset per block.
    set_sizes : array-like of int
        Set size per trial's block (3 or 6).
    age : array-like (length 1)
        Participant age in years. Used to determine age group (<45 younger, >=45 older).
    model_parameters : list/tuple of 6 numbers
        - lr: base learning rate in [0,1] applied to chosen action's PE
        - beta: inverse temperature (>0) for softmax choice
        - credit_spread: in [0,1], baseline fraction of PE spread to non-chosen actions
        - age_spread_boost: >=0, multiplicative increase in spread for older group
        - load_spread_gain: >=0, multiplicative increase in spread with load (from set size)
        - lapse: in [0,0.2], lapse probability mixing with uniform choice

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, beta, credit_spread, age_spread_boost, load_spread_gain, lapse = model_parameters
    nA = 3
    beta = max(beta, 1e-6) * 5.0
    lapse = float(np.clip(lapse, 0.0, 0.2))
    lr = np.clip(lr, 0.0, 1.0)
    credit_spread = np.clip(credit_spread, 0.0, 1.0)
    age_spread_boost = max(age_spread_boost, 0.0)
    load_spread_gain = max(load_spread_gain, 0.0)
    age_group = 1.0 if age[0] >= 45 else 0.0

    total_loglik = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask]
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        nS = int(set_sizes[mask][0])

        Q = (1.0 / nA) * np.ones((nS, nA))

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Load factor (0 for nS=3; 1 for nS=6)
            load = (max(nS, 3) - 3) / 3.0

            # Choice policy
            logits = beta * Q[s, :]
            logits -= np.max(logits)
            p = np.exp(logits)
            p /= np.sum(p)

            p_choice = (1.0 - lapse) * p[a] + lapse * (1.0 / nA)
            p_choice = float(np.clip(p_choice, 1e-12, 1.0))
            total_loglik += np.log(p_choice)

            # Credit assignment with spread
            pe = r - Q[s, a]
            Q[s, a] += lr * pe

            # Compute effective spread fraction under load and age
            spread = credit_spread * (1.0 + age_group * age_spread_boost) * (1.0 + load * load_spread_gain)
            spread = float(np.clip(spread, 0.0, 1.0))

            if spread > 0.0:
                # Distribute a fraction of the same PE to non-chosen actions
                others = [aa for aa in range(nA) if aa != a]
                for aa in others:
                    Q[s, aa] += lr * spread * pe / (nA - 1)

    return -float(total_loglik)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Pearce-Hall associability RL with load- and age-dependent decision noise, plus lapse.

    Idea
    ----
    - Each state maintains an associability (kappa) that tracks recent unsigned prediction error.
      Learning rate is adapted from kappa (higher surprise => higher alpha).
    - Decision noise (inverse temperature) is reduced under higher load and in older age,
      reflecting noisier choices with heavier cognitive demands.
    - Choice is a lapse-mixture of softmax policy and uniform random choice.

    Parameters
    ----------
    states : array-like of int
        State index on each trial (within block).
    actions : array-like of int
        Chosen action (0..2).
    rewards : array-like of float
        Binary feedback (0/1).
    blocks : array-like of int
        Block index for each trial. Values reset per block.
    set_sizes : array-like of int
        Set size per trial's block (3 or 6).
    age : array-like (length 1)
        Participant age in years. Used to determine age group (<45 younger, >=45 older).
    model_parameters : list/tuple of 6 numbers
        - alpha0: base logit for learning rate; mapped via sigmoid to (0,1)
        - eta_ph: Pearce-Hall update rate for associability kappa in (0,1)
        - beta0: base inverse temperature (>0) prior to load/age penalties
        - beta_load_pen: >=0, penalty scaling of beta with set-size load
        - beta_age_pen: >=0, additional penalty on beta for older group
        - lapse: in [0,0.2], lapse probability mixing with uniform choice

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha0, eta_ph, beta0, beta_load_pen, beta_age_pen, lapse = model_parameters
    nA = 3
    # Parameter hygiene
    eta_ph = np.clip(eta_ph, 0.0, 1.0)
    beta_base = max(beta0, 1e-6) * 5.0
    beta_load_pen = max(beta_load_pen, 0.0)
    beta_age_pen = max(beta_age_pen, 0.0)
    lapse = float(np.clip(lapse, 0.0, 0.2))
    age_group = 1.0 if age[0] >= 45 else 0.0

    # Sigmoid mapping helper for learning rate
    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    total_loglik = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask]
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        nS = int(set_sizes[mask][0])

        Q = (1.0 / nA) * np.ones((nS, nA))
        kappa = 0.5 * np.ones(nS)  # associability per state

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Load factor (0 for 3, 1 for 6)
            load = (max(nS, 3) - 3) / 3.0

            # Effective inverse temperature reduced by load and age penalties
            denom = 1.0 + beta_load_pen * load + beta_age_pen * age_group
            beta_eff = beta_base / denom

            # Choice policy
            logits = beta_eff * Q[s, :]
            logits -= np.max(logits)
            p = np.exp(logits)
            p /= np.sum(p)
            p_choice = (1.0 - lapse) * p[a] + lapse * (1.0 / nA)
            p_choice = float(np.clip(p_choice, 1e-12, 1.0))
            total_loglik += np.log(p_choice)

            # RL update with Pearce-Hall associability-derived learning rate
            pe = r - Q[s, a]
            # State-specific learning rate via sigmoid on alpha0 plus centered kappa
            alpha_t = sigmoid(alpha0 + 5.0 * (kappa[s] - 0.5))
            Q[s, a] += alpha_t * pe

            # Update associability for the state from unsigned PE
            kappa[s] = (1.0 - eta_ph) * kappa[s] + eta_ph * abs(pe)
            kappa[s] = float(np.clip(kappa[s], 0.0, 1.0))

    return -float(total_loglik)