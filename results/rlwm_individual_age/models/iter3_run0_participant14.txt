def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Perseveration-augmented RL with age- and load-modulated stickiness.

    Mechanism:
    - Model-free RL learns Q(s,a) via a single learning rate.
    - Action selection uses a softmax over Q plus a "stickiness" bonus for repeating
      the last action previously taken in the same state.
    - Stickiness increases with set size (habit reliance under higher load) and is
      stronger in older adults (age modulation).
    
    Parameters
    ----------
    states : array-like of int
        State index per trial (0..set_size-1).
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of {0,1}
        Outcome per trial.
    blocks : array-like of int
        Block index per trial; learning resets per block.
    set_sizes : array-like of int
        Set size per trial (constant within a block; either 3 or 6).
    age : array-like or scalar
        Participant age; younger <45, older >=45, used to modulate stickiness.
    model_parameters : iterable of 5 floats
        alpha : RL learning rate (0..1).
        beta : Softmax inverse temperature (>0).
        stick_base : Baseline stickiness weight added to last action logits.
        load_sens : Sensitivity of stickiness to set size (>0 increases habit with load).
        age_stick_shift : Additional multiplicative modulation of stickiness for older vs younger
                          (positive values increase stickiness in older, reduce in younger).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, stick_base, load_sens, age_stick_shift = model_parameters
    age_val = age[0] if hasattr(age, "__len__") else age
    is_young = 1.0 if age_val < 45 else 0.0
    is_old = 1.0 - is_young

    nA = 3
    total_logp = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        b_states = states[mask]
        b_actions = actions[mask]
        b_rewards = rewards[mask]
        b_set_sizes = set_sizes[mask]

        nS = int(b_set_sizes[0])

        # Initialize values
        Q = (1.0 / nA) * np.ones((nS, nA))

        # Track last action per state for stickiness; -1 means none yet
        last_action = -1 * np.ones(nS, dtype=int)

        # Compute effective stickiness for this block
        load_factor = 1.0 + load_sens * max(0.0, (nS - 3) / 3.0)
        age_factor = 1.0 + age_stick_shift * (is_old - is_young)
        kappa = stick_base * load_factor * age_factor

        for t in range(len(b_states)):
            s = int(b_states[t])
            a = int(b_actions[t])
            r = float(b_rewards[t])

            # Softmax over Q + stickiness to repeat last action in this state
            logits = beta * (Q[s, :] - np.max(Q[s, :]))
            if last_action[s] >= 0:
                stick_vec = np.zeros(nA)
                stick_vec[last_action[s]] = 1.0
                logits = logits + kappa * (stick_vec - 1.0 / nA)  # centered to avoid uniform bias

            p_vec = np.exp(logits)
            p_vec /= np.sum(p_vec)

            p_choice = max(p_vec[a], 1e-12)
            total_logp += np.log(p_choice)

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # Update last action for the state
            last_action[s] = a

    return -total_logp


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Probabilistic Win-Stay/Lose-Shift (WSLS) with age- and load-modulated tendencies and lapse.

    Mechanism:
    - For each state, the model stores the last encountered action and outcome.
    - If the last outcome in that state was a win, it repeats that action with probability p_ws;
      otherwise, it switches to one of the other two actions with probability p_ls (split equally).
    - Both p_ws and p_ls are reduced by larger set sizes (interference) and are higher in younger adults.
    - A small lapse probability mixes in uniform random choice.

    Parameters
    ----------
    states : array-like of int
        State index per trial.
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of {0,1}
        Outcome per trial.
    blocks : array-like of int
        Block index per trial; memory resets per block.
    set_sizes : array-like of int
        Set size per trial (3 or 6).
    age : array-like or scalar
        Participant age; used to define younger (<45) vs older (>=45) groups.
    model_parameters : iterable of 5 floats
        ws_base : Baseline win-stay probability at set size 3 for a young adult (0..1).
        ls_base : Baseline lose-shift probability at set size 3 for a young adult (0..1).
        load_sens : Load sensitivity reducing both ws and ls when set size increases (>=0).
        age_ws_bonus : Additive bonus to both ws and ls in younger vs older (positive favors younger).
        lapse : Probability of random choice (0..0.2 typical).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    ws_base, ls_base, load_sens, age_ws_bonus, lapse = model_parameters
    age_val = age[0] if hasattr(age, "__len__") else age
    is_young = 1.0 if age_val < 45 else 0.0
    is_old = 1.0 - is_young

    nA = 3
    total_logp = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        b_states = states[mask]
        b_actions = actions[mask]
        b_rewards = rewards[mask]
        b_set_sizes = set_sizes[mask]

        nS = int(b_set_sizes[0])

        last_action = -1 * np.ones(nS, dtype=int)
        last_reward = -1 * np.ones(nS, dtype=int)  # -1 means unseen, 0 loss, 1 win

        # Effective WS/LS parameters for this block
        load_factor = 1.0 / (1.0 + load_sens * max(0.0, (nS - 3) / 3.0))
        age_bonus = age_ws_bonus * (is_young - is_old)
        p_ws = np.clip(ws_base * load_factor + age_bonus, 0.0, 1.0)
        p_ls = np.clip(ls_base * load_factor + age_bonus, 0.0, 1.0)

        for t in range(len(b_states)):
            s = int(b_states[t])
            a = int(b_actions[t])
            r = int(b_rewards[t])

            if last_action[s] == -1 or last_reward[s] == -1:
                # No memory for this state yet: uniform
                p_state = np.ones(nA) / nA
            else:
                if last_reward[s] == 1:
                    # Win: repeat with p_ws, otherwise choose among the other two uniformly
                    p_state = np.ones(nA) * ((1.0 - p_ws) / (nA - 1))
                    p_state[last_action[s]] = p_ws
                else:
                    # Loss: switch with p_ls to either of the other two actions uniformly
                    p_state = np.ones(nA) * (p_ls / (nA - 1))
                    p_state[last_action[s]] = 1.0 - p_ls

            # Lapse mixture
            p_mix = (1.0 - lapse) * p_state + lapse * (np.ones(nA) / nA)
            p_choice = max(p_mix[a], 1e-12)
            total_logp += np.log(p_choice)

            # Update memory for this state
            last_action[s] = a
            last_reward[s] = r

    return -total_logp


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with error-driven adaptive learning rate modulated by set size and age.

    Mechanism:
    - Standard model-free RL over Q(s,a), softmax choice.
    - The learning rate adapts on each trial: higher after errors than after rewards.
      This error sensitivity is attenuated under higher set sizes and in older adults,
      reflecting reduced flexible updating under load/aging.
    - This creates history-dependent learning without extra states or traces.

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..set_size-1).
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of {0,1}
        Outcome per trial.
    blocks : array-like of int
        Block index per trial; learning resets per block.
    set_sizes : array-like of int
        Set size per trial (3 or 6).
    age : array-like or scalar
        Participant age; younger <45, older >=45.
    model_parameters : iterable of 5 floats
        alpha_base : Baseline learning rate (0..1).
        beta : Softmax inverse temperature (>0).
        err_boost : Additional learning rate added on error trials (r=0) before modulation (>=0).
        load_sens : How strongly larger set sizes reduce the effective error boost (>=0).
        age_err_shift : Relative change in error boost for older vs younger
                        (positive -> older get less boost, younger get more).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha_base, beta, err_boost, load_sens, age_err_shift = model_parameters
    age_val = age[0] if hasattr(age, "__len__") else age
    is_young = 1.0 if age_val < 45 else 0.0
    is_old = 1.0 - is_young

    nA = 3
    total_logp = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        b_states = states[mask]
        b_actions = actions[mask]
        b_rewards = rewards[mask]
        b_set_sizes = set_sizes[mask]

        nS = int(b_set_sizes[0])

        Q = (1.0 / nA) * np.ones((nS, nA))

        # Compute modulation terms that will affect error boost in this block
        load_factor = 1.0 / (1.0 + load_sens * max(0.0, (nS - 3) / 3.0))
        age_factor = 1.0 - age_err_shift * (is_old - is_young)  # if positive age_err_shift: reduce for older, increase for younger

        for t in range(len(b_states)):
            s = int(b_states[t])
            a = int(b_actions[t])
            r = float(b_rewards[t])

            # Choice probabilities from softmax
            logits = beta * (Q[s, :] - np.max(Q[s, :]))
            p_vec = np.exp(logits)
            p_vec /= np.sum(p_vec)

            p_choice = max(p_vec[a], 1e-12)
            total_logp += np.log(p_choice)

            # Adaptive learning rate: higher after errors (r=0)
            is_error = 1.0 - r
            alpha_eff = alpha_base + err_boost * is_error * load_factor * age_factor
            alpha_eff = np.clip(alpha_eff, 0.0, 1.0)

            pe = r - Q[s, a]
            Q[s, a] += alpha_eff * pe

    return -total_logp