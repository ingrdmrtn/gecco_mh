def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM mixture with age- and load-dependent WM decay.

    Policy:
    - Mixture of an RL softmax policy and a simple working-memory (WM) policy.
    - WM holds a per-state action distribution that is sharply updated on rewarded outcomes and
      continuously decays toward uniform. WM reliability (mixture weight) decreases with load and, modestly, with age.

    Learning:
    - RL: standard Q-learning with learning rate alpha and softmax inverse temperature beta.
    - WM: if reward=1, the chosen action is written into WM for that state (one-hot); between trials, WM decays toward uniform
      with a decay rate that increases with set size (load) and more for older adults.

    Age and load usage:
    - WM decay rate f_eff = sigmoid(f0 + f_gain * load * age_dir), where load is 0 for set size 3 and 1 for set size 6,
      and age_dir=+1 for older (>=45), -1 for younger (<45). Thus older adults forget faster under higher load.
    - WM mixture weight w_eff = wmix * (1 - load) * (1 - 0.5*is_older), giving younger adults relatively higher WM reliance,
      particularly at low load.

    Parameters (model_parameters; exactly 5 and all are used):
    - alpha: RL learning rate in (0,1).
    - beta: RL/WM softmax inverse temperature (>0).
    - f0: baseline logit of WM decay rate.
    - f_gain: gain on load-by-age effect for WM decay.
    - wmix: baseline WM mixture weight in [0,1] that is further modulated by load and age.

    Inputs:
    - states: array of state indices per trial (int).
    - actions: array of chosen actions per trial (int in {0,1,2}).
    - rewards: array of rewards per trial (0/1).
    - blocks: array of block indices per trial (int).
    - set_sizes: array of set sizes per trial (3 or 6).
    - age: array-like with a single value for participant age (years).
    - model_parameters: list/tuple of [alpha, beta, f0, f_gain, wmix].

    Returns:
    - Negative log-likelihood of observed choices.
    """
    alpha, beta, f0, f_gain, wmix = model_parameters
    age_val = float(age[0])
    is_older = 1.0 if age_val >= 45 else 0.0
    is_younger = 1.0 - is_older
    age_dir = (+1.0 * is_older) + (-1.0 * is_younger)

    nA = 3
    eps = 1e-12
    total_loglik = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx].astype(int)
        block_actions = actions[idx].astype(int)
        block_rewards = rewards[idx].astype(float)
        block_set_sizes = set_sizes[idx].astype(int)

        nS = int(block_set_sizes[0])
        # RL values
        Q = np.zeros((nS, nA))
        # WM distributions initialized to uniform
        W = np.ones((nS, nA)) / nA

        loglik_block = 0.0
        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            nS_t = int(block_set_sizes[t])

            # Load coding: 0 for 3, 1 for 6
            load = (nS_t - 3) / 3.0

            # Effective WM decay (sigmoid in (0,1))
            f_eff = 1.0 / (1.0 + np.exp(-(f0 + f_gain * load * age_dir)))

            # Mixture weight (bounded to [0,1] by construction of factors)
            w_eff = wmix * (1.0 - load) * (1.0 - 0.5 * is_older)
            w_eff = min(max(w_eff, 0.0), 1.0)

            # RL policy
            pref_rl = beta * (Q[s, :] - np.max(Q[s, :]))
            p_rl = np.exp(pref_rl)
            p_rl = p_rl / (np.sum(p_rl) + eps)

            # WM policy (softmax over WM distribution treated as logits scaled by beta)
            # Use a small numerical floor to avoid log of zero
            wm_logits = np.log(np.maximum(W[s, :], eps))
            pref_wm = beta * (wm_logits - np.max(wm_logits))
            p_wm = np.exp(pref_wm)
            p_wm = p_wm / (np.sum(p_wm) + eps)

            # Mixture policy
            p = w_eff * p_wm + (1.0 - w_eff) * p_rl
            pa = max(p[a], eps)
            loglik_block += np.log(pa)

            # RL update
            delta = r - Q[s, a]
            Q[s, a] += alpha * delta

            # WM decay toward uniform across all states
            W = (1.0 - f_eff) * W + f_eff * (1.0 / nA)

            # WM write on rewarded outcome: store chosen action deterministically
            if r > 0.5:
                W[s, :] = 0.0
                W[s, a] = 1.0

        total_loglik += loglik_block

    return -total_loglik


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Uncertainty-gated exploration with dynamic temperature and age/load-modulated choice kernel.

    Policy:
    - Softmax with a dynamic inverse temperature:
        beta_eff = beta0 + beta_gain * (1 - normalized_entropy(Q[s,:])) - load_beta * load * age_dir
      where normalized_entropy is H/ln(nA), load is 0 (set size 3) or 1 (set size 6), and age_dir=+1 for older, -1 for younger.
      Lower entropy (more certainty) increases beta; for older adults, high load further reduces beta (more exploration).
    - Add a state-specific choice kernel K[s,:] that biases recently chosen actions.
      The kernel's influence weight is w_choice = xi * (1 + load_beta * load * age_dir), increasing with load for older and decreasing for younger.

    Learning:
    - RL: standard Q-learning with learning rate alpha.
    - Choice kernel: decays each trial with gamma (slightly lower for older adults) and reinforces the chosen action.

    Parameters (model_parameters; exactly 5 and all are used):
    - alpha: RL learning rate in (0,1).
    - beta0: baseline inverse temperature (>0).
    - beta_gain: sensitivity of beta to certainty (>=0).
    - load_beta: scales load-by-age effects on both beta and choice kernel weight.
    - xi: base weight for the choice kernel contribution to preferences.

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age: arrays as specified.
    - model_parameters: [alpha, beta0, beta_gain, load_beta, xi].

    Returns:
    - Negative log-likelihood of observed choices.
    """
    alpha, beta0, beta_gain, load_beta, xi = model_parameters
    age_val = float(age[0])
    is_older = 1.0 if age_val >= 45 else 0.0
    is_younger = 1.0 - is_older
    age_dir = (+1.0 * is_older) + (-1.0 * is_younger)

    nA = 3
    ln_nA = np.log(nA)
    eps = 1e-12
    total_loglik = 0.0

    # Choice kernel decay: slightly lower persistence for older adults
    gamma_base = 0.75
    gamma = gamma_base - 0.1 * is_older
    gamma = min(max(gamma, 0.0), 0.99)

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx].astype(int)
        block_actions = actions[idx].astype(int)
        block_rewards = rewards[idx].astype(float)
        block_set_sizes = set_sizes[idx].astype(int)

        nS = int(block_set_sizes[0])
        Q = np.zeros((nS, nA))
        K = np.zeros((nS, nA))  # choice kernel

        loglik_block = 0.0
        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            nS_t = int(block_set_sizes[t])

            load = (nS_t - 3) / 3.0

            # Compute normalized entropy of Q[s,:]
            q_pref = Q[s, :] - np.max(Q[s, :])
            pi_q = np.exp(q_pref)
            pi_q = pi_q / (np.sum(pi_q) + eps)
            H = -np.sum(pi_q * np.log(np.maximum(pi_q, eps)))
            Hn = H / ln_nA

            # Dynamic beta and choice kernel weight
            beta_eff = beta0 + beta_gain * (1.0 - Hn) - load_beta * load * age_dir
            beta_eff = max(beta_eff, eps)

            w_choice = xi * (1.0 + load_beta * load * age_dir)

            # Preferences: Q plus choice kernel
            pref = beta_eff * Q[s, :] + w_choice * K[s, :]
            pref = pref - np.max(pref)
            p = np.exp(pref)
            p = p / (np.sum(p) + eps)

            pa = max(p[a], eps)
            loglik_block += np.log(pa)

            # RL update
            delta = r - Q[s, a]
            Q[s, a] += alpha * delta

            # Choice kernel update: decay then reinforce chosen action
            K[s, :] *= gamma
            K[s, a] += 1.0

        total_loglik += loglik_block

    return -total_loglik


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with state-interference (load- and age-dependent) and lapse.

    Policy:
    - Softmax over Q with inverse temperature beta, combined with a stimulus-independent lapse:
        p_final = (1 - eps) * softmax(beta * Q[s,:]) + eps * (1/nA).
      Lapse probability eps increases with load and more strongly for older adults.

    Learning:
    - Q-learning with learning rate alpha.
    - After each update on the visited state s, values partially "diffuse" to other states (interference),
      with diffusion strength psi_eff that increases with load and is stronger for older adults.
      This models between-state interference under higher cognitive load and aging.

    Age and load usage:
    - eps = sigmoid(lapse_base + age_gain * load * age_dir), where load in {0,1}, age_dir=+1 (older) or -1 (younger).
    - psi_eff = sigmoid(psi + age_gain * load * age_dir) - 0.5, then clipped to [0, 0.5] to ensure stability.
      Larger psi_eff means more leakage of the updated state's Q-values to other states.

    Parameters (model_parameters; exactly 5 and all are used):
    - alpha: learning rate in (0,1).
    - beta: inverse temperature (>0).
    - psi: baseline logit controlling interference strength.
    - age_gain: gain for age-by-load effects on both lapse and interference.
    - lapse_base: baseline logit of lapse probability.

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age: arrays as specified.
    - model_parameters: [alpha, beta, psi, age_gain, lapse_base].

    Returns:
    - Negative log-likelihood of observed choices.
    """
    alpha, beta, psi, age_gain, lapse_base = model_parameters
    age_val = float(age[0])
    is_older = 1.0 if age_val >= 45 else 0.0
    is_younger = 1.0 - is_older
    age_dir = (+1.0 * is_older) + (-1.0 * is_younger)

    nA = 3
    eps_small = 1e-12
    total_loglik = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx].astype(int)
        block_actions = actions[idx].astype(int)
        block_rewards = rewards[idx].astype(float)
        block_set_sizes = set_sizes[idx].astype(int)

        nS = int(block_set_sizes[0])
        Q = np.zeros((nS, nA))

        loglik_block = 0.0
        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            nS_t = int(block_set_sizes[t])

            load = (nS_t - 3) / 3.0

            # Lapse probability
            eps_lapse = 1.0 / (1.0 + np.exp(-(lapse_base + age_gain * load * age_dir)))
            eps_lapse = min(max(eps_lapse, 0.0), 0.5)  # cap for numerical stability

            # Softmax policy
            pref = beta * (Q[s, :] - np.max(Q[s, :]))
            p_soft = np.exp(pref)
            p_soft = p_soft / (np.sum(p_soft) + eps_small)

            # Final policy with lapse
            p = (1.0 - eps_lapse) * p_soft + eps_lapse * (1.0 / nA)
            pa = max(p[a], eps_small)
            loglik_block += np.log(pa)

            # RL update on visited state-action
            delta = r - Q[s, a]
            Q[s, a] += alpha * delta

            # Interference: diffuse updated state's Q to other states
            psi_raw = 1.0 / (1.0 + np.exp(-(psi + age_gain * load * age_dir)))  # in (0,1)
            psi_eff = min(max(psi_raw - 0.5, 0.0), 0.5)  # map to [0,0.5]
            if psi_eff > 0.0 and nS > 1:
                for j in range(nS):
                    if j == s:
                        continue
                    Q[j, :] = (1.0 - psi_eff) * Q[j, :] + psi_eff * Q[s, :]

        total_loglik += loglik_block

    return -total_loglik