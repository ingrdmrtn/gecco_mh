def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Model 1: RL + rewarded-episode retrieval with set-size and age-modulated retrieval probability and decay.
    
    Mechanism:
      - Model-free Q-learning within blocks.
      - Parallel "episodic success memory" M that stores action strengths for actions that previously
        yielded reward = 1 in a given state. M decays via interference that grows with set size.
      - On each trial, the decision policy mixes an episodic retrieval policy and the RL policy.
      - The retrieval weight (rho) is higher for smaller set sizes and is reduced in older adults,
        but this is counteracted (or amplified) by an age-specific shift parameter.
    
    Parameters (model_parameters)
    --------------------------------
    alpha : float
        Q-learning rate in [0,1].
    beta : float
        Base inverse temperature (>0); internally scaled for numerical stability.
    decay_base : float
        Base decay rate for episodic memory per trial in [0,1].
    retr_base : float
        Baseline retrieval log-odds; higher -> more reliance on episodic memory.
    age_retr_shift : float
        Additive shift on retrieval log-odds if older (>=45). Positive increases retrieval in older adults.
    ss_retr_slope : float
        Slope by which retrieval log-odds decreases with set size (relative to 3). Positive -> less retrieval as set size grows.
    
    Inputs
    ------
    states : array-like (T,)
        State index per trial (0..nS-1 within a block).
    actions : array-like (T,)
        Chosen action per trial (0..2).
    rewards : array-like (T,)
        Binary rewards (0/1).
    blocks : array-like (T,)
        Block index per trial. Values indicate trials belonging to the same block.
    set_sizes : array-like (T,)
        Set size for each trial in its block (typically 3 or 6).
    age : array-like or scalar
        Participant age; older group defined as age >= 45.
    
    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices under the model.
    """
    nA = 3
    eps = 1e-12

    alpha, beta, decay_base, retr_base, age_retr_shift, ss_retr_slope = model_parameters
    beta = 5.0 * max(beta, eps)

    # Determine age group
    age_val = age[0] if hasattr(age, "__len__") and not np.isscalar(age) else age
    is_older = 1.0 if age_val >= 45 else 0.0

    total_logp = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx].astype(int)
        block_actions = actions[idx].astype(int)
        block_rewards = rewards[idx].astype(float)
        nS = int(set_sizes[idx][0])

        # Initialize RL values and episodic memory
        Q = np.zeros((nS, nA))
        M = np.zeros((nS, nA))  # episodic success strengths

        # Interference-driven decay scaled by set size and age (older -> stronger interference)
        # Map decay_base into an effective per-trial decay via set size
        ss_factor = (nS - 3) / 3.0  # 0 for 3, 1 for 6
        decay = np.clip(decay_base * (1.0 + 0.5 * ss_factor) * (1.0 + 0.25 * is_older), 0.0, 1.0)

        # Retrieval weight (rho) via logistic link, decreasing with set size, shifted by age
        # logit(rho) = retr_base - ss_retr_slope * ss_factor + age_retr_shift * is_older
        logit_rho = retr_base - ss_retr_slope * ss_factor + age_retr_shift * is_older
        rho = 1.0 / (1.0 + np.exp(-logit_rho))  # in (0,1)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Apply global interference/decay to episodic strengths
            if decay > 0:
                M *= (1.0 - decay)

            # Compute RL policy
            q_s = Q[s, :].copy()
            q_s -= np.max(q_s)  # stabilize
            p_rl = np.exp(beta * q_s)
            p_rl = p_rl / (np.sum(p_rl) + eps)

            # Compute episodic retrieval policy: softmax over episodic strengths for current state
            m_s = M[s, :].copy()
            m_s -= np.max(m_s)
            p_ep = np.exp(beta * m_s)
            p_ep = p_ep / (np.sum(p_ep) + eps)

            # Mixture policy
            p = rho * p_ep + (1.0 - rho) * p_rl
            p = np.clip(p, eps, 1.0)
            p /= np.sum(p)

            total_logp += np.log(p[a])

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # Episodic success memory update: reinforce actions that got reward 1
            # We set the rewarded action's strength near 1 and suppress others slightly for that state.
            if r > 0.5:
                # Winner-take-most encoding of the successful action
                M[s, :] *= 0.5  # partial suppression of competitors
                M[s, a] = 1.0   # mark successful action strongly

    return -float(total_logp)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Model 2: Asymmetric learning with uncertainty-adaptive exploration and set-size/age-modulated lapse.
    
    Mechanism:
      - Model-free Q-learning with separate learning rates for positive vs. negative prediction errors.
      - Tracks a running uncertainty signal U[s,a] (filtered squared prediction errors) driving
        adaptive exploration: higher uncertainty -> lower effective beta.
      - Adds a lapse component that increases with set size and with older age, mixing uniform choice.
    
    Parameters (model_parameters)
    --------------------------------
    alpha_pos : float
        Learning rate for positive prediction errors in [0,1].
    alpha_neg : float
        Learning rate for negative prediction errors in [0,1].
    beta : float
        Base inverse temperature (>0); internally scaled.
    lapse_base : float
        Baseline log-odds of lapse; lapse probability = sigmoid(lapse_base + ...).
    age_lapse_boost : float
        Additive boost to lapse log-odds for older adults (>=45).
    ss_unc_slope : float
        Scales the impact of set size on both lapse and uncertainty-driven exploration.
        Positive -> more noise/exploration in larger sets.
    
    Inputs
    ------
    states, actions, rewards, blocks, set_sizes, age : see Model 1.
    
    Returns
    -------
    nll : float
        Negative log-likelihood.
    """
    nA = 3
    eps = 1e-12

    alpha_pos, alpha_neg, beta, lapse_base, age_lapse_boost, ss_unc_slope = model_parameters
    beta = 5.0 * max(beta, eps)

    # Age group
    age_val = age[0] if hasattr(age, "__len__") and not np.isscalar(age) else age
    is_older = 1.0 if age_val >= 45 else 0.0

    total_logp = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx].astype(int)
        block_actions = actions[idx].astype(int)
        block_rewards = rewards[idx].astype(float)
        nS = int(set_sizes[idx][0])

        Q = np.zeros((nS, nA))
        U = 0.1 * np.ones((nS, nA))  # initialize uncertainty

        ss_factor = (nS - 3) / 3.0  # 0 for 3, 1 for 6

        # Lapse probability increases with set size and further with age
        logit_lapse = lapse_base + age_lapse_boost * is_older + 1.0 * ss_unc_slope * ss_factor
        lapse = 1.0 / (1.0 + np.exp(-logit_lapse))
        lapse = np.clip(lapse, 0.0, 0.5)  # cap lapse to keep likelihood meaningful

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Uncertainty-adaptive exploration: reduce beta with larger uncertainty and larger set size
            unc_s = np.maximum(0.0, np.mean(U[s, :]))  # state-level uncertainty
            beta_eff = beta / (1.0 + ss_unc_slope * ss_factor + unc_s)
            beta_eff = max(beta_eff, 1e-3)

            # Softmax policy
            q_s = Q[s, :].copy()
            q_s -= np.max(q_s)
            p_choice = np.exp(beta_eff * q_s)
            p_choice = p_choice / (np.sum(p_choice) + eps)

            # Lapse mixture with uniform
            p = (1.0 - lapse) * p_choice + lapse * (1.0 / nA)
            p = np.clip(p, eps, 1.0)
            p /= np.sum(p)

            total_logp += np.log(p[a])

            # RL update with asymmetry
            pe = r - Q[s, a]
            lr = alpha_pos if pe >= 0.0 else alpha_neg
            Q[s, a] += lr * pe

            # Uncertainty update: exponential moving average of squared PE for the (s,a)
            # Weight recent evidence more in larger sets to reflect noisier environments
            kappa = np.clip(0.2 + 0.3 * ss_factor + 0.1 * is_older, 0.0, 1.0)
            U[s, a] = (1.0 - kappa) * U[s, a] + kappa * (pe * pe)

    return -float(total_logp)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Model 3: Capacity-limited WM vs RL meta-control with age-modulated capacity and WM precision.
    
    Mechanism:
      - RL learner (Q-learning).
      - A simple WM store that encodes the most recently rewarded action for each encountered state.
      - The probability that the current state's association is available in WM depends on an effective
        capacity that scales inversely with set size and is shifted by age.
      - A meta-controller blends WM policy vs. RL policy via a bias parameter favoring WM when available.
      - WM policy uses a separate inverse temperature (wm_beta) to capture retrieval precision.
    
    Parameters (model_parameters)
    --------------------------------
    alpha : float
        Q-learning rate in [0,1].
    beta : float
        Inverse temperature for RL softmax (>0), internally scaled.
    capacity_K : float
        Nominal WM capacity (in items). Determines availability probability as min(1, (capacity_eff / nS)).
    age_capacity_shift : float
        Additive shift to capacity if older (>=45). Positive -> higher effective capacity in older adults.
    wm_beta : float
        Inverse temperature for WM policy (>0), internally scaled.
    meta_bias : float
        Log-odds bias term that increases the blend weight toward WM when the item is available.
        Effective WM blend weight: w = availability * sigmoid(meta_bias).
    
    Inputs
    ------
    states, actions, rewards, blocks, set_sizes, age : see Model 1.
    
    Returns
    -------
    nll : float
        Negative log-likelihood.
    """
    nA = 3
    eps = 1e-12

    alpha, beta, capacity_K, age_capacity_shift, wm_beta, meta_bias = model_parameters
    beta = 5.0 * max(beta, eps)
    wm_beta = 5.0 * max(wm_beta, eps)

    # Age group
    age_val = age[0] if hasattr(age, "__len__") and not np.isscalar(age) else age
    is_older = 1.0 if age_val >= 45 else 0.0

    # Convert meta bias to a probability multiplier (0,1)
    wm_bias = 1.0 / (1.0 + np.exp(-meta_bias))

    total_logp = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx].astype(int)
        block_actions = actions[idx].astype(int)
        block_rewards = rewards[idx].astype(float)
        nS = int(set_sizes[idx][0])

        # RL values
        Q = np.zeros((nS, nA))

        # WM store: for each state, which action is currently cached (or -1 if none)
        wm_cached_action = -1 * np.ones(nS, dtype=int)

        # Effective capacity with age shift (applies only if older)
        capacity_eff = capacity_K + age_capacity_shift * is_older
        capacity_eff = max(0.0, capacity_eff)

        # Probability that a given state's association is in WM
        availability = np.clip(capacity_eff / max(1.0, float(nS)), 0.0, 1.0)

        # Final WM blend weight when available
        w_when_available = np.clip(wm_bias, 0.0, 1.0)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # RL softmax
            q_s = Q[s, :].copy()
            q_s -= np.max(q_s)
            p_rl = np.exp(beta * q_s)
            p_rl = p_rl / (np.sum(p_rl) + eps)

            # WM softmax (if cached) uses degenerate preference peaking at cached action
            if wm_cached_action[s] >= 0:
                prefs = np.zeros(nA)
                prefs[wm_cached_action[s]] = 1.0  # single preferred action
                prefs -= np.max(prefs)
                p_wm = np.exp(wm_beta * prefs)
                p_wm = p_wm / (np.sum(p_wm) + eps)
                w = availability * w_when_available
            else:
                # No cached item: WM contributes chance-level
                p_wm = np.ones(nA) / nA
                w = 0.0  # no WM influence if nothing cached

            # Blend WM and RL
            p = w * p_wm + (1.0 - w) * p_rl
            p = np.clip(p, eps, 1.0)
            p /= np.sum(p)

            total_logp += np.log(p[a])

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # WM encoding rule: if reward is 1, cache the selected action for that state; if 0, do not overwrite
            if r > 0.5:
                wm_cached_action[s] = a

    return -float(total_logp)