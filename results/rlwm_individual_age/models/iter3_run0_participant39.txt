def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Model 1: Model-free RL with count-based novelty bonus, age-modulated exploration bonus, and set-size-scaled forgetting.

    Rationale:
      - Choices follow a softmax over Q-values augmented by a novelty (count-based) exploration bonus.
      - Older adults express reduced benefit from novelty (age reduces effective novelty gain).
      - Larger set sizes increase forgetting (decay of Q toward 0) due to higher cognitive load.

    Parameters (model_parameters; total 5):
      - alpha: float in (0,1), learning rate for Q updates.
      - beta: float > 0, inverse temperature for softmax (rescaled internally by 10).
      - novelty_gain: float >= 0, weight of the count-based novelty bonus.
      - age_bonus_shift: float >= 0, scales down novelty for older adults (multiplicative divisor).
      - rho_forget: float in [0,1), baseline per-trial forgetting rate toward 0, scaled up by set size.

    Inputs:
      - states: 1D int array of state indices per trial (0..set_size-1 within a block).
      - actions: 1D int array of chosen actions (0..2).
      - rewards: 1D float/int array of rewards (0/1).
      - blocks: 1D int array of block indices per trial.
      - set_sizes: 1D int array of set size (3 or 6) per trial.
      - age: array-like with a single number; age >= 45 -> older group.
      - model_parameters: sequence of five floats as described.

    Returns:
      - Negative log-likelihood of the observed choices under the model (float).
    """
    alpha, beta, novelty_gain, age_bonus_shift, rho_forget = model_parameters
    age_val = age[0]
    is_older = 1.0 if age_val >= 45 else 0.0

    nA = 3
    neg_loglik = 0.0
    beta_eff = max(1e-6, beta * 10.0)

    # Age reduces effective novelty bonus (older -> smaller multiplier)
    novelty_age_mult = 1.0 / (1.0 + is_older * max(0.0, age_bonus_shift))

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask].astype(int)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_set_sizes = set_sizes[mask].astype(int)

        nS = int(block_set_sizes[0])
        Q = np.zeros((nS, nA), dtype=float)
        visits = np.zeros((nS, nA), dtype=float)

        for t in range(len(block_states)):
            s = block_states[t]
            a = block_actions[t]
            r = block_rewards[t]
            ss = float(block_set_sizes[t])

            # Set-size-scaled forgetting toward 0
            rho_eff = np.clip(rho_forget * (1.0 + max(0.0, (ss - 3.0) / 3.0)), 0.0, 0.999)
            Q[s, :] *= (1.0 - rho_eff)

            # Count-based bonus decreases with visits; apply to all actions in the current state
            bonus = novelty_gain * novelty_age_mult / np.sqrt(visits[s, :] + 1.0)

            logits = beta_eff * (Q[s, :] + bonus)
            logits -= np.max(logits)
            p = np.exp(logits)
            p_sum = np.sum(p)
            if p_sum <= 0.0 or not np.isfinite(p_sum):
                p = np.ones(nA) / nA
            else:
                p /= p_sum

            pa = max(1e-12, p[a])
            neg_loglik -= np.log(pa)

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += np.clip(alpha, 1e-6, 1.0) * pe

            # Update visit counts
            visits[s, a] += 1.0

    return float(neg_loglik)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Model 2: Actor-Critic with epsilon-greedy exploration modulated by age and set size.

    Rationale:
      - Policy is represented by state-action preferences (actor) and evaluated by a state-value baseline (critic).
      - Choices follow a softmax over preferences, blended with epsilon-greedy noise.
      - Older adults and larger set sizes increase epsilon (more random exploration).
      - Learning of actor and critic via TD error with a baseline.

    Parameters (model_parameters; total 6):
      - alpha_actor: float in (0,1), learning rate for policy preferences.
      - alpha_critic: float in (0,1), learning rate for state values.
      - beta: float > 0, inverse temperature for softmax over preferences (rescaled internally by 10).
      - eps0: float in [0,1), base epsilon for random choice.
      - age_eps_boost: float >= 0, additive boost to epsilon for older adults.
      - ss_eps_boost: float >= 0, additive boost to epsilon per unit increase of set size over 3.

    Inputs:
      - states: 1D int array of state indices per trial (0..set_size-1 within a block).
      - actions: 1D int array of chosen actions (0..2).
      - rewards: 1D float/int array of rewards (0/1).
      - blocks: 1D int array of block indices per trial.
      - set_sizes: 1D int array of set size (3 or 6) per trial.
      - age: array-like with a single number; age >= 45 -> older group.
      - model_parameters: sequence of six floats as described.

    Returns:
      - Negative log-likelihood of the observed choices under the model (float).
    """
    alpha_actor, alpha_critic, beta, eps0, age_eps_boost, ss_eps_boost = model_parameters
    age_val = age[0]
    is_older = 1.0 if age_val >= 45 else 0.0

    nA = 3
    neg_loglik = 0.0
    beta_eff = max(1e-6, beta * 10.0)

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask].astype(int)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_set_sizes = set_sizes[mask].astype(int)

        nS = int(block_set_sizes[0])
        H = np.zeros((nS, nA), dtype=float)   # preferences (actor)
        V = np.zeros(nS, dtype=float)         # state values (critic)

        for t in range(len(block_states)):
            s = block_states[t]
            a = block_actions[t]
            r = block_rewards[t]
            ss = float(block_set_sizes[t])

            # Epsilon depends on age and set size (more random with age and larger sets)
            eps = eps0 + is_older * max(0.0, age_eps_boost) + max(0.0, (ss - 3.0) / 3.0) * max(0.0, ss_eps_boost)
            eps = np.clip(eps, 0.0, 0.5)

            # Softmax over preferences
            logits = beta_eff * H[s, :]
            logits -= np.max(logits)
            p_soft = np.exp(logits)
            p_sum = np.sum(p_soft)
            if p_sum <= 0.0 or not np.isfinite(p_sum):
                p_soft = np.ones(nA) / nA
            else:
                p_soft /= p_sum

            # Epsilon-greedy mixture
            p = (1.0 - eps) * p_soft + eps * (1.0 / nA)

            pa = max(1e-12, p[a])
            neg_loglik -= np.log(pa)

            # TD error with state-value baseline (gamma=0 for bandit-like)
            delta = r - V[s]

            # Critic update
            V[s] += np.clip(alpha_critic, 1e-6, 1.0) * delta

            # Actor update: policy gradient with baseline; update all actions
            one_hot = np.zeros(nA)
            one_hot[a] = 1.0
            grad_logpi = one_hot - p_soft
            H[s, :] += np.clip(alpha_actor, 1e-6, 1.0) * delta * grad_logpi

    return float(neg_loglik)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Model 3: RL with adaptive inverse temperature driven by recent reward rate, age-damped flexibility, and set-size-scaled forgetting.

    Rationale:
      - Q-values are learned via a delta rule.
      - The decision temperature (beta) adapts online: higher after success (more exploitation), lower after failure (more exploration).
      - Older adults adapt beta more slowly (age-damped beta updates).
      - Larger set sizes increase forgetting toward 0 due to greater memory interference.

    Parameters (model_parameters; total 6):
      - alpha: float in (0,1), learning rate for Q updates and reward-rate tracker.
      - beta0: float > 0, initial inverse temperature at block start (rescaled internally by 10).
      - k_beta: float, step size for adapting beta based on reward prediction (signed).
      - age_beta_damp: float >= 0, reduces magnitude of beta adaptation for older adults.
      - rho_forget: float in [0,1), baseline forgetting rate toward 0 for Q-values.
      - ss_forget_boost: float >= 0, extra forgetting per unit increase of set size over 3.

    Inputs:
      - states: 1D int array of state indices per trial (0..set_size-1 within a block).
      - actions: 1D int array of chosen actions (0..2).
      - rewards: 1D float/int array of rewards (0/1).
      - blocks: 1D int array of block indices per trial.
      - set_sizes: 1D int array of set size (3 or 6) per trial.
      - age: array-like with a single number; age >= 45 -> older group.
      - model_parameters: sequence of six floats as described.

    Returns:
      - Negative log-likelihood of the observed choices under the model (float).
    """
    alpha, beta0, k_beta, age_beta_damp, rho_forget, ss_forget_boost = model_parameters
    age_val = age[0]
    is_older = 1.0 if age_val >= 45 else 0.0

    nA = 3
    neg_loglik = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask].astype(int)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_set_sizes = set_sizes[mask].astype(int)

        nS = int(block_set_sizes[0])
        Q = np.zeros((nS, nA), dtype=float)

        # Initialize adaptive components
        beta_t = max(1e-6, beta0 * 10.0)
        r_bar = 0.5  # running reward-rate baseline

        for t in range(len(block_states)):
            s = block_states[t]
            a = block_actions[t]
            r = block_rewards[t]
            ss = float(block_set_sizes[t])

            # Set-size-scaled forgetting
            rho_eff = np.clip(rho_forget * (1.0 + max(0.0, (ss - 3.0) / 3.0) * max(0.0, ss_forget_boost)), 0.0, 0.999)
            Q[s, :] *= (1.0 - rho_eff)

            # Policy with current beta
            logits = beta_t * Q[s, :]
            logits -= np.max(logits)
            p = np.exp(logits)
            p_sum = np.sum(p)
            if p_sum <= 0.0 or not np.isfinite(p_sum):
                p = np.ones(nA) / nA
            else:
                p /= p_sum

            pa = max(1e-12, p[a])
            neg_loglik -= np.log(pa)

            # Q update
            pe = r - Q[s, a]
            Q[s, a] += np.clip(alpha, 1e-6, 1.0) * pe

            # Update reward-rate tracker
            r_bar += np.clip(alpha, 1e-6, 1.0) * (r - r_bar)

            # Adaptive beta update: move beta in direction of (r - r_bar)
            # Age dampens the adaptation magnitude.
            damp = 1.0 / (1.0 + is_older * max(0.0, age_beta_damp))
            beta_t += damp * k_beta * (r - r_bar)
            beta_t = float(np.clip(beta_t, 1e-3, 200.0))

    return float(neg_loglik)