def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with per-state choice-kernel bias and explicit omission policy, modulated by age and set size.

    Summary:
    - Learns Q-values with Q-learning.
    - Adds a per-state "choice kernel" that biases repeating recently chosen actions (recency bias).
    - Includes an explicit omission (-2) policy whose probability depends on age and set size.
    - Older adults and larger set sizes increase the impact of the choice kernel and omission propensity.

    Parameters
    ----------
    states : array-like of int
        State index per trial within block (0..nS-1).
    actions : array-like of int
        Chosen action per trial. Valid actions are {0,1,2}; -2 denotes an omission/timeout.
    rewards : array-like of float/int
        Feedback per trial. 0 or 1; negative values denote invalid trials (e.g., omission).
    blocks : array-like of int
        Block index per trial. Values reset at each block.
    set_sizes : array-like of int
        Set size (3 or 6) for the block/trial.
    age : array-like of float/int
        Participant age; older group (>=45) increases omission tendency and choice-kernel influence.
    model_parameters : list/tuple of length 4
        [alpha, beta, ck_gain, omit_base]
        - alpha: RL learning rate (0-1).
        - beta: inverse temperature for RL softmax (>0).
        - ck_gain: baseline gain for the choice kernel bias (>=0).
        - omit_base: baseline log-odds for omission tendency (real number).

    Returns
    -------
    neg_log_likelihood : float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta, ck_gain, omit_base = model_parameters
    age_val = age[0]
    older = 1.0 if age_val >= 45 else 0.0

    nA = 3
    eps = 1e-12
    neg_log_lik = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask]
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        nS = int(set_sizes[mask][0])

        # Initialize Q and per-state choice kernel K (preference vector over actions)
        Q = np.zeros((nS, nA))
        K = np.zeros((nS, nA))

        # Set-dependent and age-dependent modifiers
        load_factor = 0.0 if nS <= 3 else 1.0  # extra load for set size 6
        # Choice kernel strength multiplier increases with age and load
        ck_mult = 1.0 + 0.5 * older + 0.5 * load_factor
        # Choice kernel learning speed derived from ck_gain (bounded to [0,1])
        kappa = 1.0 - 1.0 / (1.0 + ck_gain * (1.0 + 0.5 * older + 0.5 * load_factor))
        kappa = max(0.0, min(1.0, kappa))
        # Omission log-odds baseline adjusted by age and load
        omit_logit_bias = omit_base + 1.0 * older + 0.8 * load_factor

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # RL softmax over Q
            Qs = Q[s] - np.max(Q[s])
            rl_logits = beta * Qs

            # Choice kernel bias (converted to logits by a gain)
            # Keep K normalized to avoid runaway magnitude
            if np.sum(np.abs(K[s])) > 0:
                K_s_norm = K[s] / (np.max(np.abs(K[s])) + eps)
            else:
                K_s_norm = K[s].copy()
            ck_logits = ck_mult * K_s_norm

            logits = rl_logits + ck_logits
            logits = logits - np.max(logits)
            p_actions = np.exp(logits)
            p_actions = p_actions / (np.sum(p_actions) + eps)

            # Omission probability via logistic transformation
            p_omit = 1.0 / (1.0 + np.exp(-(omit_logit_bias)))
            p_omit = max(0.0, min(1.0, p_omit))

            # Choice probability
            if a == -2:
                p_choice = p_omit
            elif 0 <= a < nA:
                p_choice = (1.0 - p_omit) * p_actions[a]
            else:
                p_choice = eps

            neg_log_lik -= np.log(max(p_choice, eps))

            # Learning updates for valid, non-omission trials with valid reward
            if (a >= 0) and (r >= 0):
                # Q-learning update
                pe = r - Q[s, a]
                Q[s, a] += alpha * pe

                # Choice kernel update: decay towards zero then add to chosen action
                K[s] *= (1.0 - 0.5 * kappa)  # mild decay to keep stable
                K[s, a] += kappa

            # For omissions or invalid reward, allow kernel to mildly decay
            if a == -2 or r < 0:
                K[s] *= (1.0 - 0.25 * kappa)

    return neg_log_lik


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Capacity-limited hypothesis-testing working memory with Bayesian fallback and decay to omissions.

    Summary:
    - WM "slots" store deterministic state→action mappings once evidence crosses a threshold.
    - Capacity (number of concurrently stored mappings) depends on a base parameter, age (older->lower),
      and set size (larger->lower effective capacity).
    - If a state is not stored, choices rely on a Bayesian (count-based) expected-reward estimate with softmax.
    - Stored mappings can be forgotten with a decay rate that is higher for older adults and larger set sizes.
    - Omissions are generated with a probability that increases with decay/forgetfulness, age, and load.

    Parameters
    ----------
    states : array-like of int
        State index per trial within block (0..nS-1).
    actions : array-like of int
        Chosen action per trial. Valid actions are {0,1,2}; -2 denotes an omission/timeout.
    rewards : array-like of float/int
        Feedback per trial. 0 or 1; negative values denote invalid trials (e.g., omission).
    blocks : array-like of int
        Block index per trial. Values reset at each block.
    set_sizes : array-like of int
        Set size (3 or 6) for the block/trial.
    age : array-like of float/int
        Participant age; older group (>=45) reduces effective capacity and increases decay and omissions.
    model_parameters : list/tuple of length 4
        [beta, c_base, thr, forget_rate]
        - beta: inverse temperature for softmax when WM is not stored (>0).
        - c_base: baseline WM capacity in slots (interpreted continuously; clipped to [0, nS]).
        - thr: evidence threshold for committing a state→action mapping to WM (>=0).
        - forget_rate: baseline probability controlling WM decay and omission propensity (>=0).

    Returns
    -------
    neg_log_likelihood : float
        Negative log-likelihood of the observed choices under the model.
    """
    beta, c_base, thr, forget_rate = model_parameters
    age_val = age[0]
    older = 1.0 if age_val >= 45 else 0.0

    nA = 3
    eps = 1e-12
    neg_log_lik = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask]
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        nS = int(set_sizes[mask][0])

        # Effective capacity (slots): reduce with age and set size (load)
        load = 0.0 if nS <= 3 else 1.0
        cap_eff = c_base * (1.0 - 0.35 * older) * (1.0 - 0.25 * load)
        cap_eff = max(0.0, min(float(nS), cap_eff))
        cap_slots = int(np.floor(cap_eff + 1e-9))

        # WM store: -1 means not stored; otherwise store the action index [0..2]
        wm_store = -1 * np.ones(nS, dtype=int)
        wm_evidence = np.zeros((nS, nA))  # evidence per state-action

        # Count-based Bayesian fallback (successes, attempts)
        succ = np.ones((nS, nA)) * 1.0  # symmetric Beta(1,1) prior -> Laplace smoothing
        attm = np.ones((nS, nA)) * 2.0  # implies initial mean 0.5

        # Forgetting and omission tendencies
        # Base decay increases with age and load
        decay_p = 1.0 - 1.0 / (1.0 + forget_rate * (1.0 + 0.75 * older + 0.5 * load))
        decay_p = max(0.0, min(1.0, decay_p))
        # Omission baseline from forgetfulness
        omit_base = -2.0 + 4.0 * decay_p  # maps low decay->low omit; high decay->higher omit

        # Threshold adjusted by age/load: older+load require more evidence to commit
        thr_eff = thr * (1.0 + 0.5 * older + 0.5 * load)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Apply forgetting stochastically: if too many stored, randomly drop; also decay by probability
            # Probabilistic decay
            if wm_store[s] != -1:
                if np.random.rand() < decay_p:
                    wm_store[s] = -1  # forget this mapping

            # Enforce capacity: if number of stored exceeds cap, randomly drop extras
            if cap_slots >= 0:
                stored_indices = np.where(wm_store != -1)[0]
                if len(stored_indices) > cap_slots:
                    # drop random stored items until capacity met
                    drop_count = len(stored_indices) - cap_slots
                    to_drop = np.random.choice(stored_indices, size=drop_count, replace=False)
                    wm_store[to_drop] = -1

            # Compute policy
            if wm_store[s] != -1:
                # Deterministic WM choice with sharp softmax
                logits = np.full(nA, -10.0)  # very low for non-WM actions
                logits[wm_store[s]] = 10.0
                logits -= np.max(logits)
                p_actions = np.exp(logits) / (np.sum(np.exp(logits)) + eps)
                # Omission probability lower when WM is present
                p_omit = 1.0 / (1.0 + np.exp(-(omit_base - 1.0)))
            else:
                # Bayesian expected reward: mean of Beta distribution ~ succ/attm
                Qs = succ[s] / (attm[s] + eps)
                logits = beta * (Qs - np.max(Qs))
                p_actions = np.exp(logits)
                p_actions = p_actions / (np.sum(p_actions) + eps)
                # Omission probability higher when WM absent
                p_omit = 1.0 / (1.0 + np.exp(-(omit_base + 0.5)))

            p_omit = max(0.0, min(1.0, p_omit))

            # Likelihood
            if a == -2:
                p_choice = p_omit
            elif 0 <= a < nA:
                p_choice = (1.0 - p_omit) * p_actions[a]
            else:
                p_choice = eps

            neg_log_lik -= np.log(max(p_choice, eps))

            # Learning updates for valid, non-omission trials with valid reward
            if (a >= 0) and (r >= 0):
                # Update Bayesian counts
                attm[s, a] += 1.0
                succ[s, a] += r

                # Update evidence and possibly commit to WM
                # Evidence accumulates as +1 for reward, -0.5 for non-reward
                wm_evidence[s, a] += (1.0 if r > 0.5 else -0.5)

                # If evidence crosses threshold and capacity allows, commit
                if (wm_evidence[s, a] >= thr_eff):
                    # If already stored something else for s, overwrite
                    if wm_store[s] == -1:
                        # If capacity is full, evict a random stored state
                        stored_indices = np.where(wm_store != -1)[0]
                        if len(stored_indices) >= cap_slots and cap_slots > 0:
                            to_drop_idx = np.random.choice(stored_indices, size=1)[0]
                            wm_store[to_drop_idx] = -1
                    wm_store[s] = a

        # End block
    return neg_log_lik


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Uncertainty-weighted RL with metacognitive exploration and omission control.

    Summary:
    - Standard Q-learning over actions.
    - Temperature adapts to state uncertainty: higher entropy over Q increases exploration.
    - Omission probability increases with uncertainty, age, and set size (reflecting metacognitive disengagement).
    - Reward sensitivity is scaled, allowing different subjective impact of reward.

    Parameters
    ----------
    states : array-like of int
        State index per trial within block (0..nS-1).
    actions : array-like of int
        Chosen action per trial. Valid actions are {0,1,2}; -2 denotes an omission/timeout.
    rewards : array-like of float/int
        Feedback per trial. 0 or 1; negative values denote invalid trials (e.g., omission).
    blocks : array-like of int
        Block index per trial. Values reset at each block.
    set_sizes : array-like of int
        Set size (3 or 6) for the block/trial.
    age : array-like of float/int
        Participant age; older group (>=45) increases uncertainty impact and omission propensity.
    model_parameters : list/tuple of length 5
        [alpha, beta_base, u_gain, omit_base, rew_scale]
        - alpha: RL learning rate (0-1).
        - beta_base: base inverse temperature (>0).
        - u_gain: gain on uncertainty; increases exploration and omissions with uncertainty (>=0).
        - omit_base: baseline log-odds for omission tendency (real number).
        - rew_scale: multiplicative scaling of reward before learning (>=0).

    Returns
    -------
    neg_log_likelihood : float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta_base, u_gain, omit_base, rew_scale = model_parameters
    age_val = age[0]
    older = 1.0 if age_val >= 45 else 0.0

    nA = 3
    eps = 1e-12
    neg_log_lik = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask]
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        nS = int(set_sizes[mask][0])

        Q = np.zeros((nS, nA))

        load = 0.0 if nS <= 3 else 1.0
        # Older and high load amplify the effect of uncertainty
        u_mult = (1.0 + 0.5 * older + 0.5 * load)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r_raw = float(block_rewards[t])

            # Compute uncertainty for state s from current Q via policy entropy
            # Use softmax with base temperature to approximate current policy
            logits_pol = beta_base * (Q[s] - np.max(Q[s]))
            pol = np.exp(logits_pol)
            pol = pol / (np.sum(pol) + eps)
            entropy = -np.sum(pol * (np.log(pol + eps)))
            # Normalize entropy to [0, log(nA)] -> [0,1]
            entropy_norm = entropy / (np.log(nA) + eps)

            # Adaptive temperature: higher uncertainty -> lower beta (more exploration)
            beta_eff = beta_base / (1.0 + u_gain * u_mult * entropy_norm)
            beta_eff = max(1e-6, beta_eff)

            # Action probabilities
            logits = beta_eff * (Q[s] - np.max(Q[s]))
            p_actions = np.exp(logits)
            p_actions = p_actions / (np.sum(p_actions) + eps)

            # Omission probability increases with uncertainty, age, and load
            omit_logit = omit_base + u_gain * u_mult * entropy_norm + 0.5 * older + 0.4 * load
            p_omit = 1.0 / (1.0 + np.exp(-omit_logit))
            p_omit = max(0.0, min(1.0, p_omit))

            # Likelihood
            if a == -2:
                p_choice = p_omit
            elif 0 <= a < nA:
                p_choice = (1.0 - p_omit) * p_actions[a]
            else:
                p_choice = eps

            neg_log_lik -= np.log(max(p_choice, eps))

            # Learning update (valid, non-omission, non-negative reward)
            if (a >= 0) and (r_raw >= 0):
                r = rew_scale * r_raw
                pe = r - Q[s, a]
                Q[s, a] += alpha * pe

    return neg_log_lik