def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Q-learning with load- and age-adaptive exploration via dynamic inverse temperature.

    Summary
    - Standard tabular Q-learning over state-action pairs with a fixed learning rate (alpha).
    - Exploration is controlled by a time-varying inverse temperature:
        beta_t = exp(b0 + eta_set * load + eta_age * I(old))
      where load = (set_size - 3) / 3 is 0 for set size 3 and 1 for set size 6.
      This lets larger set sizes (higher load) and age group shift exploration/exploitation.
    - A small lapse rate (epsilon) captures occasional random choices.
    - Robust to invalid trials: if action outside {0,1,2} or reward not in {0,1}, it assumes
      a uniform choice probability and skips updates.

    Parameters
    ----------
    states : array-like of int
        State index at each trial (0-based within block).
    actions : array-like of int
        Chosen action at each trial. Expected in {0,1,2}.
    rewards : array-like of int
        Observed rewards; expected in {0,1}.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (3 or 6), constant within each block.
    age : array-like (length 1)
        Participant age; <45 -> younger, >=45 -> older.
    model_parameters : tuple/list of floats
        (alpha, b0, eta_set, eta_age, epsilon)
        - alpha: learning rate for Q-learning
        - b0: baseline log-inverse-temperature (exponentiated to enforce positivity)
        - eta_set: load modulation of inverse temperature (applied to (set_size-3)/3)
        - eta_age: age modulation of inverse temperature (added if age>=45)
        - epsilon: lapse rate mixed with uniform choice

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, b0, eta_set, eta_age, epsilon = model_parameters
    age_val = age[0]
    is_old = 1.0 if age_val >= 45 else 0.0

    states = np.asarray(states, dtype=int)
    actions = np.asarray(actions, dtype=int)
    rewards = np.asarray(rewards, dtype=float)
    blocks = np.asarray(blocks, dtype=int)
    set_sizes = np.asarray(set_sizes, dtype=int)

    nll = 0.0
    nA = 3
    eps = np.clip(epsilon, 0.0, 0.2)  # keep lapse in a reasonable range

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx]
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_set_sizes = set_sizes[idx]
        if len(block_states) == 0:
            continue

        # Derive number of states from set size; assume states are 0..nS-1
        nS = int(np.max(block_states) + 1)
        Q = np.zeros((nS, nA))

        # Load factor based on set size; assumed constant within block
        load = (float(block_set_sizes[0]) - 3.0) / 3.0  # 0 for 3, 1 for 6
        # Dynamic inverse temperature for the block (could be extended trial-wise if needed)
        beta = np.exp(b0 + eta_set * load + eta_age * is_old)

        for t in range(len(block_states)):
            s = block_states[t]
            a = block_actions[t]
            r = block_rewards[t]

            # Robustness to invalid entries
            if (a < 0) or (a >= nA) or (r < 0.0) or (r > 1.0) or (s < 0) or (s >= nS):
                nll -= np.log(1.0 / nA)
                continue

            logits = beta * Q[s, :]
            logits -= np.max(logits)  # stability
            exp_logits = np.exp(logits)
            pi = exp_logits / np.sum(exp_logits)

            p_choice = (1.0 - eps) * pi[a] + eps * (1.0 / nA)
            p_choice = np.clip(p_choice, 1e-12, 1.0)
            nll -= np.log(p_choice)

            # Q-learning update
            td = r - Q[s, a]
            Q[s, a] += alpha * td

    return nll


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Q-learning plus a state-wise choice kernel: age- and load-modulated stickiness.

    Summary
    - Standard Q-learning with learning rate alpha and softmax inverse-temperature beta.
    - A state-wise choice kernel K encourages repeating the last chosen action in that state.
    - The strength of stickiness depends on age (k_y vs k_o) and set size (rho_set):
        stickiness = k_age * (1 + rho_set * load), load=(set_size-3)/3.
      This captures stronger reliance on simple repetition under higher load and potential
      age differences in habitual responding.
    - Kernel is updated per state with mild decay to keep it bounded.
    - Robust to invalid trials by assigning uniform likelihood and skipping updates.

    Parameters
    ----------
    states : array-like of int
        State index at each trial (0-based within block).
    actions : array-like of int
        Chosen action at each trial. Expected in {0,1,2}.
    rewards : array-like of int
        Observed rewards; expected in {0,1}.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (3 or 6), constant within each block.
    age : array-like (length 1)
        Participant age; <45 -> younger, >=45 -> older (used for k_age selection).
    model_parameters : tuple/list of floats
        (alpha, beta, k_y, k_o, rho_set)
        - alpha: Q-learning rate
        - beta: inverse temperature of softmax for Q values
        - k_y: stickiness weight for younger participants
        - k_o: stickiness weight for older participants
        - rho_set: load modulation of stickiness (scales with (set_size-3)/3)

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, k_y, k_o, rho_set = model_parameters
    age_val = age[0]
    is_old = age_val >= 45
    k_age = k_o if is_old else k_y

    states = np.asarray(states, dtype=int)
    actions = np.asarray(actions, dtype=int)
    rewards = np.asarray(rewards, dtype=float)
    blocks = np.asarray(blocks, dtype=int)
    set_sizes = np.asarray(set_sizes, dtype=int)

    nll = 0.0
    nA = 3

    # Kernel decay hyperparameter (fixed, not counted among free params)
    kernel_decay = 0.85

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx]
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_set_sizes = set_sizes[idx]
        if len(block_states) == 0:
            continue

        nS = int(np.max(block_states) + 1)
        Q = np.zeros((nS, nA))
        K = np.zeros((nS, nA))  # choice kernel per state

        load = (float(block_set_sizes[0]) - 3.0) / 3.0
        stick = k_age * (1.0 + rho_set * load)

        for t in range(len(block_states)):
            s = block_states[t]
            a = block_actions[t]
            r = block_rewards[t]

            if (a < 0) or (a >= nA) or (r < 0.0) or (r > 1.0) or (s < 0) or (s >= nS):
                nll -= np.log(1.0 / nA)
                continue

            logits = beta * Q[s, :] + stick * K[s, :]
            logits -= np.max(logits)
            exp_logits = np.exp(logits)
            pi = exp_logits / np.sum(exp_logits)

            p = np.clip(pi[a], 1e-12, 1.0)
            nll -= np.log(p)

            # Q-learning update
            td = r - Q[s, a]
            Q[s, a] += alpha * td

            # Choice kernel update with decay
            K[s, :] *= kernel_decay
            K[s, a] += (1.0 - kernel_decay)

    return nll


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Leaky Bayesian (Dirichlet) inference with load-modulated forgetting and fixed lapse.

    Summary
    - For each state, maintains Dirichlet counts over the 3 actions (beliefs about which action is correct).
    - Prior concentration c0 initializes counts equally.
    - After observing (s,a,r), if r=1 increments the count for (s,a); if r=0 increments the other actions,
      implementing simple error-driven reallocation. Counts leaky-integrate over time to model forgetting:
        counts <- (1 - leak) * counts; then add the new increment.
    - Leak increases with set size (load) and is slightly larger for older adults:
        leak = sigmoid(leak0 + k_set * load + 0.5 * I(old)), load=(set_size-3)/3.
    - Action selection uses softmax over the expected reward (mean of Dirichlet) with inverse temperature beta.
    - A small lapse rate (lambda) mixes the softmax with uniform choice.
    - Robust to invalid trials with uniform likelihood and no update.

    Parameters
    ----------
    states : array-like of int
        State index at each trial (0-based within block).
    actions : array-like of int
        Chosen action at each trial. Expected in {0,1,2}.
    rewards : array-like of int
        Observed rewards; expected in {0,1}.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (3 or 6), constant within each block.
    age : array-like (length 1)
        Participant age; <45 -> younger, >=45 -> older (modulates leak).
    model_parameters : tuple/list of floats
        (c0, beta, leak0, k_set, lam)
        - c0: symmetric Dirichlet prior concentration (pseudo-counts per action)
        - beta: inverse temperature for softmax over expected reward
        - leak0: baseline logit of leak; transformed via sigmoid
        - k_set: load modulation of leak (applied to (set_size-3)/3)
        - lam: lapse rate mixed with uniform choice

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    c0, beta, leak0, k_set, lam = model_parameters
    age_val = age[0]
    is_old = 1.0 if age_val >= 45 else 0.0

    states = np.asarray(states, dtype=int)
    actions = np.asarray(actions, dtype=int)
    rewards = np.asarray(rewards, dtype=float)
    blocks = np.asarray(blocks, dtype=int)
    set_sizes = np.asarray(set_sizes, dtype=int)

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    nll = 0.0
    nA = 3
    lam = np.clip(lam, 0.0, 0.2)
    c0_pos = max(c0, 1e-6)  # ensure positive prior

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx]
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_set_sizes = set_sizes[idx]
        if len(block_states) == 0:
            continue

        nS = int(np.max(block_states) + 1)
        counts = np.full((nS, nA), c0_pos, dtype=float)

        load = (float(block_set_sizes[0]) - 3.0) / 3.0
        leak = sigmoid(leak0 + k_set * load + 0.5 * is_old)
        leak = np.clip(leak, 0.0, 0.99)

        for t in range(len(block_states)):
            s = block_states[t]
            a = block_actions[t]
            r = block_rewards[t]

            if (a < 0) or (a >= nA) or (r < 0.0) or (r > 1.0) or (s < 0) or (s >= nS):
                nll -= np.log(1.0 / nA)
                continue

            # Convert counts to expected reward for each action
            alpha_vec = counts[s, :]
            mean_prob = alpha_vec / np.sum(alpha_vec)

            logits = beta * mean_prob
            logits -= np.max(logits)
            exp_logits = np.exp(logits)
            pi = exp_logits / np.sum(exp_logits)

            p = (1.0 - lam) * pi[a] + lam * (1.0 / nA)
            p = np.clip(p, 1e-12, 1.0)
            nll -= np.log(p)

            # Leaky integration of counts (forgetting)
            counts[s, :] *= (1.0 - leak)

            # Update counts based on outcome
            if r >= 0.5:
                counts[s, a] += 1.0
            else:
                # allocate evidence against chosen action by reinforcing the alternatives
                inc = 1.0 / (nA - 1)
                for aa in range(nA):
                    if aa != a:
                        counts[s, aa] += inc

    return nll