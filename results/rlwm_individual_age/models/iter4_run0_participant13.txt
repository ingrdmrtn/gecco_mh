def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + decaying episodic working memory (WM) with age- and load-dependent gating.

    The model blends a standard RL system with a simple episodic WM store that
    retains the last rewarded action per state and decays between visits.
    The mixture weight favoring WM depends on set size (stronger for 3 than 6)
    and age group (younger vs older). A small lapse mixes the policy with uniform.

    Parameters
    ----------
    states : array-like of int
        State indices on each trial (0..nS-1 within block).
    actions : array-like of int
        Chosen actions on each trial (0..nA-1).
    rewards : array-like of int
        Binary rewards (0 or 1) observed each trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size for the block of each trial.
    age : array-like of float
        Participant age; age[0] determines age group (younger < 45).
    model_parameters : tuple/list of floats
        (alpha, beta, wm_base, decay, age_wm_shift, lapse)
        - alpha: RL learning rate (squashed to 0..1)
        - beta: inverse temperature for both RL and WM policies (scaled internally)
        - wm_base: base WM gating strength (squashed to 0..1 before load/age mods)
        - decay: WM decay rate per trial since last visit (squashed to 0..1)
        - age_wm_shift: additive shift to WM gate for younger (negative shifts older)
        - lapse: lapse rate mixing with uniform policy (squashed to 0..0.3)

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, wm_base, decay, age_wm_shift, lapse = model_parameters

    # Squash/scale parameters to sensible ranges
    alpha = 1.0 / (1.0 + np.exp(-alpha))
    beta = (1.0 / (1.0 + np.exp(-beta))) * 12.0
    wm_base = 1.0 / (1.0 + np.exp(-wm_base))
    decay = 1.0 / (1.0 + np.exp(-decay))  # 0..1 per-trial decay
    lapse = (1.0 / (1.0 + np.exp(-lapse))) * 0.3

    age_val = float(age[0])
    younger = 1 if age_val < 45 else 0  # participant is 20 => younger=1

    nll = 0.0
    eps = 1e-12
    nA = 3

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx].astype(int)
        block_actions = actions[idx].astype(int)
        block_rewards = rewards[idx].astype(float)
        nS = int(set_sizes[idx][0])

        # RL values
        Q = np.zeros((nS, nA))

        # WM store: strengths per action, per state; and last-visit time for decay
        M = np.zeros((nS, nA))
        last_visit_time = -np.ones(nS, dtype=int)
        t_global = 0

        for t in range(len(block_states)):
            s = block_states[t]
            a = block_actions[t]
            r = block_rewards[t]

            # Decay WM for this state based on time since last visit
            if last_visit_time[s] >= 0:
                dt = (t_global - last_visit_time[s])
                if dt > 0:
                    M[s, :] *= (1.0 - decay) ** dt

            # Compute RL policy
            q = Q[s, :]
            qc = q - np.max(q)
            pi_rl = np.exp(beta * qc)
            pi_rl = pi_rl / (np.sum(pi_rl) + eps)

            # Compute WM policy: favor stored rewarded action if any strength remains
            m = M[s, :]
            if np.sum(m) > 0:
                mc = m - np.max(m)
                pi_wm = np.exp(beta * mc)
                pi_wm = pi_wm / (np.sum(pi_wm) + eps)
            else:
                pi_wm = np.ones(nA) / nA

            # Mixture weight for WM depends on set size and age
            # Base gate -> scaled by load (3/nS) and shifted by age (for younger)
            gate_raw = wm_base * (3.0 / float(nS)) + age_wm_shift * (younger - 0.5)
            gate = 1.0 / (1.0 + np.exp(-5.0 * (gate_raw - 0.5)))  # keep in 0..1 with nonlinearity

            # Total policy with lapse
            pi = gate * pi_wm + (1.0 - gate) * pi_rl
            pi = (1.0 - lapse) * pi + lapse * (1.0 / nA)

            # Accumulate likelihood
            p = max(pi[a], eps)
            nll -= np.log(p)

            # Learn in RL
            delta = r - Q[s, a]
            Q[s, a] += alpha * delta

            # Update WM on rewarded trials: store action strongly; weakly suppress others
            if r > 0.0:
                M[s, :] *= (1.0 - decay)  # mild consolidation step
                M[s, :] *= 0.0
                M[s, a] = 1.0  # one-shot storage of the correct action

            last_visit_time[s] = t_global
            t_global += 1

    return nll


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Hybrid RL + Win-Stay/Lose-Shift (WSLS) with age-modulated lose-shift and load-modulated mixing.

    The model blends a standard RL softmax policy with a WSLS heuristic policy.
    The WSLS component stays with the last action if it was rewarded; if not,
    it shifts away with a controllable strength. The mixture weight is reduced
    under higher set size. Age modulates the lose-shift strength.

    Parameters
    ----------
    states : array-like of int
        State indices on each trial (0..nS-1 within block).
    actions : array-like of int
        Chosen actions on each trial (0..nA-1).
    rewards : array-like of int
        Binary rewards (0 or 1) observed each trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size for the block of each trial.
    age : array-like of float
        Participant age; age[0] determines age group (older >=45).
    model_parameters : tuple/list of floats
        (alpha, beta, mix_ws, lose_strength, age_lose_bias, lapse)
        - alpha: RL learning rate (squashed 0..1)
        - beta: inverse temperature for the RL softmax (scaled internally)
        - mix_ws: baseline mixture weight for WSLS (squashed 0..1, will be load-adjusted)
        - lose_strength: base strength of shifting away after a loss (squashed 0..1)
        - age_lose_bias: additive bias on lose_strength for older group
        - lapse: lapse rate mixing with uniform policy (squashed to 0..0.3)

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, mix_ws, lose_strength, age_lose_bias, lapse = model_parameters

    alpha = 1.0 / (1.0 + np.exp(-alpha))
    beta = (1.0 / (1.0 + np.exp(-beta))) * 12.0
    mix_ws = 1.0 / (1.0 + np.exp(-mix_ws))
    lose_strength = 1.0 / (1.0 + np.exp(-lose_strength))
    lapse = (1.0 / (1.0 + np.exp(-lapse))) * 0.3

    age_val = float(age[0])
    older = 1 if age_val >= 45 else 0

    nll = 0.0
    eps = 1e-12
    nA = 3

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx].astype(int)
        block_actions = actions[idx].astype(int)
        block_rewards = rewards[idx].astype(float)
        nS = int(set_sizes[idx][0])

        Q = np.zeros((nS, nA))
        last_action = -np.ones(nS, dtype=int)
        last_reward = np.zeros(nS, dtype=float)

        # Effective WSLS mixture weight reduces with load
        mix_ws_eff = mix_ws * (3.0 / float(nS))

        # Effective lose-shift strength incorporates age
        lose_eff = np.clip(lose_strength + age_lose_bias * older, 0.0, 1.0)

        for t in range(len(block_states)):
            s = block_states[t]
            a = block_actions[t]
            r = block_rewards[t]

            # RL policy
            q = Q[s, :]
            q = q - np.max(q)
            pi_rl = np.exp(beta * q)
            pi_rl = pi_rl / (np.sum(pi_rl) + eps)

            # WSLS policy
            pi_ws = np.ones(nA) / nA
            if last_action[s] >= 0:
                la = last_action[s]
                if last_reward[s] > 0.0:
                    # Win-Stay: concentrate prob mass on last action
                    pi_ws = np.ones(nA) * ((1.0 - 0.99) / (nA - 1))
                    pi_ws[la] = 0.99
                else:
                    # Lose-Shift: reduce probability on last action and redistribute
                    shift = lose_eff
                    pi_ws = np.ones(nA) * (shift / (nA - 1))
                    pi_ws[la] = 1.0 - shift

            # Mixture and lapse
            pi = mix_ws_eff * pi_ws + (1.0 - mix_ws_eff) * pi_rl
            pi = (1.0 - lapse) * pi + lapse * (1.0 / nA)

            p = max(pi[a], eps)
            nll -= np.log(p)

            # RL update
            delta = r - Q[s, a]
            Q[s, a] += alpha * delta

            # Update WSLS memory
            last_action[s] = a
            last_reward[s] = r

    return nll


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with eligibility traces and age-/load-modulated directed exploration via visit-count bonus.

    The model uses TD learning with eligibility traces and augments the choice
    preferences with a novelty/uncertainty bonus based on state-action visit counts.
    The exploration bonus is stronger for younger participants and smaller set sizes.

    Parameters
    ----------
    states : array-like of int
        State indices on each trial (0..nS-1 within block).
    actions : array-like of int
        Chosen actions on each trial (0..nA-1).
    rewards : array-like of int
        Binary rewards (0 or 1) observed each trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size for the block of each trial.
    age : array-like of float
        Participant age; age[0] determines age group (younger < 45).
    model_parameters : tuple/list of floats
        (alpha, beta, bonus_scale, lambda_tr, age_bonus_shift, size_bonus_decay)
        - alpha: learning rate for TD updates (squashed 0..1)
        - beta: inverse temperature for softmax (scaled internally)
        - bonus_scale: base scale of directed exploration bonus (>0 via softplus)
        - lambda_tr: eligibility trace decay (squashed 0..1)
        - age_bonus_shift: additive bonus for younger group (can be negative for older)
        - size_bonus_decay: exponent controlling how quickly bonus decays with visits (squashed 0..2)

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, bonus_scale, lambda_tr, age_bonus_shift, size_bonus_decay = model_parameters

    alpha = 1.0 / (1.0 + np.exp(-alpha))
    beta = (1.0 / (1.0 + np.exp(-beta))) * 12.0
    # Softplus to keep positive
    bonus_scale = np.log1p(np.exp(bonus_scale))
    lambda_tr = 1.0 / (1.0 + np.exp(-lambda_tr))
    # Map to 0..2 for breadth of decay control
    size_bonus_decay = 2.0 * (1.0 / (1.0 + np.exp(-size_bonus_decay)))

    age_val = float(age[0])
    younger = 1 if age_val < 45 else 0

    nll = 0.0
    eps = 1e-12
    nA = 3

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx].astype(int)
        block_actions = actions[idx].astype(int)
        block_rewards = rewards[idx].astype(float)
        nS = int(set_sizes[idx][0])

        Q = np.zeros((nS, nA))
        e = np.zeros((nS, nA))  # eligibility traces
        N = np.zeros((nS, nA))  # visit counts

        # Effective exploration bonus scales with age and load
        bonus_age = bonus_scale + age_bonus_shift * (1 if younger else -1)
        bonus_load_scale = 3.0 / float(nS)  # smaller sets => larger directed exploration

        for t in range(len(block_states)):
            s = block_states[t]
            a = block_actions[t]
            r = block_rewards[t]

            # Compute directed exploration bonus from counts
            # Lower counts => larger bonus; modulated by load and age
            bonus_sa = np.zeros(nA)
            for aa in range(nA):
                bonus_sa[aa] = bonus_age * bonus_load_scale / ((N[s, aa] + 1.0) ** max(size_bonus_decay, eps))

            prefs = Q[s, :] + bonus_sa
            prefs = prefs - np.max(prefs)
            pi = np.exp(beta * prefs)
            pi = pi / (np.sum(pi) + eps)

            p = max(pi[a], eps)
            nll -= np.log(p)

            # TD error with immediate reward (no state transition info available)
            delta = r - Q[s, a]

            # Update eligibility traces (accumulating)
            e *= lambda_tr
            e[s, a] += 1.0

            # Update Q values for all state-actions via traces
            Q += alpha * delta * e

            # Update counts after acting
            N[s, a] += 1.0

    return nll