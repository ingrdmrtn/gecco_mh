def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Bayesian RL + decaying WM binding with age- and load-modulated prior and WM reliability.

    Mechanism:
    - For each block, the agent maintains:
      1) A Bayesian action-value estimate per state from a symmetric Dirichlet prior over
         action correctness (counts updated by reward-contingent evidence).
      2) A working-memory (WM) binding that stores the last rewarded action for each state,
         with a trial-wise decay that increases with set size and differs by age group.
    - The prior concentration (kappa_prior) is reduced for larger set sizes and for older adults,
      making the agent more "plastic" under higher load or older age (more weight on data).
    - Policy: a convex combination of WM one-hot policy and Bayesian expected reward (posterior mean),
      with the WM weight decaying with load and age. A small lapse probability mixes in a uniform policy.

    Parameters (model_parameters):
    - kappa_prior: base symmetric Dirichlet concentration (>0). Lower values favor faster learning.
    - beta: inverse temperature for softmax over Bayesian values (>0).
    - wm_decay: base exponential WM decay per intervening trial in same block (0..1).
    - age_capacity_shift: positive values give younger adults higher WM reliability; negative flips this.
    - lapse: base lapse rate mixed uniformly (0..1), scaled by load and age.
    
    Inputs:
    - states: array of int, state index on each trial (0..set_size-1 within block).
    - actions: array of int, chosen action on each trial (0,1,2). Out-of-range treated as lapses.
    - rewards: array of numeric, feedback per trial; <=0 mapped to 0, >0 to 1.
    - blocks: array of int, block index for each trial (resets learning across blocks).
    - set_sizes: array of int, set size for each trial (3 or 6).
    - age: array-like with a single element, participant age in years.
    - model_parameters: list/tuple of 5 floats [kappa_prior, beta, wm_decay, age_capacity_shift, lapse].

    Returns:
    - Negative log-likelihood of observed choices.
    """
    kappa_prior, beta, wm_decay, age_capacity_shift, lapse = model_parameters
    is_older = 1.0 if age[0] >= 45 else 0.0

    nll = 0.0
    eps = 1e-12
    nA = 3

    for b in np.unique(blocks):
        mask = (blocks == b)
        b_states = states[mask]
        b_actions = actions[mask]
        b_rewards = rewards[mask]
        b_set_sizes = set_sizes[mask]

        nS = int(b_set_sizes[0])

        # Symmetric Dirichlet prior counts per state-action
        # Prior is modulated by load and age: larger load and older -> lower concentration.
        load = float(nS)
        age_load_scale = 1.0 + 0.5 * is_older
        kappa_eff = max(1e-6, kappa_prior / (1.0 + 0.25 * (load - 3.0) * age_load_scale))

        counts = np.ones((nS, nA)) * (kappa_eff / nA)

        # WM: store last rewarded action and a reliability weight that decays between visits
        wm_action = -np.ones(nS, dtype=int)
        wm_strength = np.zeros(nS)  # [0,1], decays over time
        last_seen_trial = -np.ones(nS, dtype=int)

        for t in range(len(b_states)):
            s = int(b_states[t])
            a = int(b_actions[t])
            r = 1.0 if b_rewards[t] > 0 else 0.0
            load_t = float(b_set_sizes[t])

            # Posterior mean of "correctness" from Dirichlet-multinomial
            post = counts[s, :]
            mean_val = post / (np.sum(post) + eps)

            # Softmax over Bayesian values
            v = mean_val - np.max(mean_val)
            p_bayes = np.exp(beta * v)
            p_bayes = p_bayes / (np.sum(p_bayes) + eps)

            # WM decay since last visit to this state
            if last_seen_trial[s] >= 0:
                gap = t - last_seen_trial[s]
                # Decay increases with load and for older adults; age_capacity_shift moderates this
                decay_factor = wm_decay + (1.0 - wm_decay) * (load_t - 3.0) / 3.0
                decay_factor = min(0.999, max(0.0, decay_factor))
                age_mod = 1.0 + is_older * max(0.0, 0.5 - 0.25 * age_capacity_shift)
                wm_strength[s] *= (decay_factor ** (gap * age_mod))
            last_seen_trial[s] = t

            # WM policy and weight
            if wm_action[s] >= 0 and wm_strength[s] > 0.0:
                p_wm = np.zeros(nA)
                p_wm[int(wm_action[s])] = 1.0
            else:
                p_wm = np.ones(nA) / nA

            # WM mixing weight: stronger for younger and smaller load
            # Logistic mapping of an age- and load-adjusted "capacity gap"
            capacity_anchor = 4.0 + age_capacity_shift * (1.0 - is_older) + (age_capacity_shift * -1.0) * is_older
            wm_weight = 1.0 / (1.0 + np.exp((load_t - capacity_anchor)))
            wm_weight *= np.clip(wm_strength[s], 0.0, 1.0)

            # Lapse grows with load and age
            lapse_t = np.clip(lapse * ((load_t - 3.0) / 3.0 + 0.5 * is_older), 0.0, 0.49)

            p_mix = wm_weight * p_wm + (1.0 - wm_weight) * p_bayes
            p = (1.0 - lapse_t) * p_mix + lapse_t * (np.ones(nA) / nA)

            if a < 0 or a >= nA:
                # Treat invalid action as pure lapse observation
                p_a = 1.0 / nA
                nll -= np.log(max(p_a, eps))
            else:
                p_a = p[a]
                nll -= np.log(max(p_a, eps))

            # Update Bayesian counts with reward-contingent evidence
            # Reward provides evidence that chosen action is correct for the state.
            if a >= 0 and a < nA:
                # Weighted evidence: stronger update when posterior was uncertain (smaller total)
                uncert = 1.0 / (np.sum(post) + eps)
                if r > 0.0:
                    counts[s, a] += 1.0 + uncert
                    # Refresh WM on reward
                    wm_action[s] = a
                    wm_strength[s] = 1.0
                else:
                    # Penalize chosen action mildly (move mass to others)
                    counts[s, a] = max(kappa_eff / (10.0 * nA), counts[s, a] - (0.25 + 0.5 * uncert))
                    # Small redistribution to other actions keeps total mass reasonable
                    redist = (0.25 + 0.5 * uncert) / (nA - 1)
                    for aa in range(nA):
                        if aa != a:
                            counts[s, aa] += redist

    return nll


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with meta-learned arbitration and age bias, plus load-sensitive lapses.

    Mechanism:
    - Q-learning tracks action values per state.
    - A meta-controller weight w_t(s) trades off between a reflexive WM policy (if recent reward
      binding exists) and RL softmax, learned online via a simple meta-learning rule:
      w <- w + alpha_meta * (r - expected_value) * (signal), bounded to [0,1].
    - The meta weight has an age bias (older adults start with lower w and drift lower)
      and is sensitive to set size (higher load reduces w).
    - WM binding stores last rewarded action per state; if present, WM policy is deterministic.
    - Lapse increases with set size and age.

    Parameters (model_parameters):
    - alpha_q: RL learning rate (0..1)
    - beta: inverse temperature for RL softmax (>0)
    - alpha_meta: learning rate for arbitration weight (0..1)
    - bias_old: prior bias shifting arbitration for older vs. younger (can be negative or positive)
    - lapse0: base lapse rate (0..1), scaled by load and age

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age as described in model1.
    - model_parameters: [alpha_q, beta, alpha_meta, bias_old, lapse0]

    Returns:
    - Negative log-likelihood of observed choices.
    """
    alpha_q, beta, alpha_meta, bias_old, lapse0 = model_parameters
    is_older = 1.0 if age[0] >= 45 else 0.0

    nll = 0.0
    eps = 1e-12
    nA = 3

    for b in np.unique(blocks):
        mask = (blocks == b)
        b_states = states[mask]
        b_actions = actions[mask]
        b_rewards = rewards[mask]
        b_set_sizes = set_sizes[mask]

        nS = int(b_set_sizes[0])

        Q = np.zeros((nS, nA))
        wm_bind = -np.ones(nS, dtype=int)

        # Initialize arbitration weight per state with age bias
        # Older: lower initial weight on WM; Younger: higher unless bias_old flips sign
        w_meta = np.clip(0.5 + (0.25 - 0.25 * is_older) + bias_old * (1.0 if is_older else -1.0), 0.0, 1.0)

        for t in range(len(b_states)):
            s = int(b_states[t])
            a = int(b_actions[t])
            r = 1.0 if b_rewards[t] > 0 else 0.0
            load_t = float(b_set_sizes[t])

            # Softmax over Q
            q_s = Q[s, :]
            q_s = q_s - np.max(q_s)
            p_rl = np.exp(beta * q_s)
            p_rl = p_rl / (np.sum(p_rl) + eps)

            # WM policy if there is a binding
            if wm_bind[s] >= 0:
                p_wm = np.zeros(nA)
                p_wm[int(wm_bind[s])] = 1.0
                wm_present = 1.0
            else:
                p_wm = np.ones(nA) / nA
                wm_present = 0.0

            # Load reduces reliance on WM; older reduce more
            load_penalty = 1.0 / (1.0 + np.exp(-(3.5 - load_t)))  # 0 at low load, ->1 at high load
            w_eff = np.clip(w_meta * (1.0 - 0.6 * load_penalty * (1.0 + 0.5 * is_older)), 0.0, 1.0)
            # If no WM binding, reduce effective weight to zero
            w_eff *= wm_present

            # Lapse increases with load and age
            lapse_t = np.clip(lapse0 * (0.2 + 0.8 * (load_t - 3.0) / 3.0) * (1.0 + 0.5 * is_older), 0.0, 0.49)

            p_mix = w_eff * p_wm + (1.0 - w_eff) * p_rl
            p = (1.0 - lapse_t) * p_mix + lapse_t * (np.ones(nA) / nA)

            if a < 0 or a >= nA:
                p_a = 1.0 / nA
                nll -= np.log(max(p_a, eps))
            else:
                p_a = p[a]
                nll -= np.log(max(p_a, eps))

            # Learning updates
            if a >= 0 and a < nA:
                # RL update
                delta = r - Q[s, a]
                Q[s, a] += alpha_q * delta

                # Update WM binding on reward
                if r > 0.0:
                    wm_bind[s] = a

                # Meta-learning of arbitration weight (global scalar), nudged by prediction error
                # Compute expected value under current policy for the chosen action a
                ev_rl = p_rl[a]
                ev_wm = p_wm[a]
                ev_mix = w_eff * ev_wm + (1.0 - w_eff) * ev_rl
                # Target w increases if WM predicted chosen action and reward was good; decreases otherwise
                target = 1.0 if (ev_wm > ev_rl and r > 0.0) else 0.0
                w_meta = np.clip(w_meta + alpha_meta * (target - ev_mix), 0.0, 1.0)

    return nll


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with load- and age-modulated choice kernel (response momentum) and epsilon-greedy noise.

    Mechanism:
    - Q-learning values per state drive a softmax policy.
    - A state-specific choice kernel K(s,a) captures recent choice momentum (tendency to repeat),
      which decays over time at a rate that increases with set size and with age.
    - The kernel is combined additively with Q-values in the softmax via a gain that scales with
      age and load, allowing the model to capture increased perseveration under higher load/older age.
    - An epsilon-greedy component adds uniform noise, increasing with load and age.

    Parameters (model_parameters):
    - alpha_q: RL learning rate (0..1)
    - inv_temp: inverse temperature for the combined softmax (>0)
    - decay_k: base decay rate of the choice kernel per visit (0..1)
    - slope_age_load: scales kernel gain with load and age (can be negative or positive)
    - eps0: base epsilon for epsilon-greedy, scaled by load and age (0..1)

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age as described above.
    - model_parameters: [alpha_q, inv_temp, decay_k, slope_age_load, eps0]

    Returns:
    - Negative log-likelihood of observed choices.
    """
    alpha_q, inv_temp, decay_k, slope_age_load, eps0 = model_parameters
    is_older = 1.0 if age[0] >= 45 else 0.0

    nll = 0.0
    eps = 1e-12
    nA = 3

    for b in np.unique(blocks):
        mask = (blocks == b)
        b_states = states[mask]
        b_actions = actions[mask]
        b_rewards = rewards[mask]
        b_set_sizes = set_sizes[mask]

        nS = int(b_set_sizes[0])

        Q = np.zeros((nS, nA))
        K = np.zeros((nS, nA))  # choice kernel

        for t in range(len(b_states)):
            s = int(b_states[t])
            a = int(b_actions[t])
            r = 1.0 if b_rewards[t] > 0 else 0.0
            load_t = float(b_set_sizes[t])

            # Decay kernel for this state; faster decay with larger load and for older adults
            decay_eff = np.clip(decay_k + (1.0 - decay_k) * (load_t - 3.0) / 3.0 * (0.5 + 0.5 * is_older), 0.0, 0.999)
            K[s, :] *= (1.0 - decay_eff)

            # Kernel gain increases with load and age via slope_age_load
            gain = 1.0 + slope_age_load * ((load_t - 3.0) / 3.0 + 0.5 * is_older)

            # Combined softmax over Q + gain*K
            logits = Q[s, :] + gain * K[s, :]
            logits = logits - np.max(logits)
            p_soft = np.exp(inv_temp * logits)
            p_soft = p_soft / (np.sum(p_soft) + eps)

            # Epsilon-greedy noise increases with load and age
            eps_t = np.clip(eps0 * ((load_t - 3.0) / 3.0 + 0.5 * is_older), 0.0, 0.49)
            p = (1.0 - eps_t) * p_soft + eps_t * (np.ones(nA) / nA)

            if a < 0 or a >= nA:
                p_a = 1.0 / nA
                nll -= np.log(max(p_a, eps))
            else:
                p_a = p[a]
                nll -= np.log(max(p_a, eps))

            # Learning updates if action is valid
            if a >= 0 and a < nA:
                # RL update
                delta = r - Q[s, a]
                Q[s, a] += alpha_q * delta

                # Choice kernel update: increment chosen action, suppress others slightly
                K[s, a] += 1.0
                for aa in range(nA):
                    if aa != a:
                        K[s, aa] -= 0.2  # small counter-momentum to preserve normalization

    return nll