def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Chunked-state RL with age- and load-dependent state abstraction plus action stickiness.

    Idea:
    - Decisions are based on a convex combination of state-specific Q-values and a shared "chunked" Q across states.
    - The mixing weight p_chunk increases with set size (load) and with being older, reflecting stronger reliance on abstraction.
    - Action stickiness adds a bias to repeat the previous action (capturing motor/perseveration tendencies).
    - Learning updates both the state-specific and the shared Q proportional to their contribution (1 - p_chunk) and p_chunk.

    Parameters (model_parameters):
    - alpha: Learning rate in [0,1]
    - beta: Inverse temperature (>0), internally scaled (x10)
    - chunk_base: Baseline log-odds of chunking; higher => more chunking
    - age_boost: Additional log-odds boost if older (>0)
    - stickiness: Bias added to last chosen action's logit (any real; >0 favors repetition)
    - load_boost: Additional log-odds boost per unit load (set_size factor)
    
    Inputs:
    - states: array of state indices per trial (int)
    - actions: array of chosen actions per trial (int; expected 0..2; invalids handled safely)
    - rewards: array of rewards per trial (real; typically 0/1)
    - blocks: array of block IDs per trial (int)
    - set_sizes: array of set size per trial (int; 3 or 6)
    - age: array-like with a single number (participant age)
    - model_parameters: tuple/list with six parameters described above
    
    Returns:
    - Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta, chunk_base, age_boost, stickiness, load_boost = model_parameters
    # Constrain and scale
    alpha = min(max(alpha, 0.0), 1.0)
    beta = max(1e-6, beta) * 10.0
    stickiness = float(stickiness)
    age_boost = max(0.0, age_boost)
    load_boost = float(load_boost)
    chunk_base = float(chunk_base)

    older = 1 if age[0] > 45 else 0
    nA = 3
    eps = 1e-12
    total_loglik = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx].astype(int)
        block_actions = actions[idx].astype(int)
        block_rewards = rewards[idx].astype(float)
        block_set_sizes = set_sizes[idx].astype(int)

        nS = int(block_set_sizes[0])
        # State-specific Q and shared Q across states
        Q_state = np.zeros((nS, nA))
        Q_shared = np.zeros(nA)

        prev_action = None  # for stickiness within the block

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = block_rewards[t]
            ss = float(block_set_sizes[t])

            # Compute chunking probability via logistic on log-odds
            # Load factor normalized: 0 for 3, 1 for 6 (ss_factor = (ss-3)/3)
            ss_factor = (ss - 3.0) / 3.0
            logit = chunk_base + load_boost * ss_factor + age_boost * older
            p_chunk = 1.0 / (1.0 + np.exp(-logit))

            # Effective Q as mixture
            Q_eff = (1.0 - p_chunk) * Q_state[s, :] + p_chunk * Q_shared

            # Add action stickiness bias to the previous action
            logits = beta * Q_eff
            if prev_action is not None and 0 <= prev_action < nA:
                logits[prev_action] += stickiness

            # Softmax
            m = np.max(logits)
            exps = np.exp(logits - m)
            Z = np.sum(exps)

            # Choice probability
            if 0 <= a < nA and Z > 0:
                p_a = exps[a] / max(Z, eps)
            else:
                # Invalid action: assign tiny probability mass
                p_a = eps
            total_loglik += np.log(max(p_a, eps))

            # Update previous action tracker
            prev_action = a if (0 <= a < nA) else prev_action

            # TD learning updates split between state-specific and shared
            if 0 <= a < nA:
                # Prediction errors for each component relative to its own estimate
                pe_state = r - Q_state[s, a]
                pe_shared = r - Q_shared[a]
                # Weighted by their mixture responsibilities
                Q_state[s, a] += alpha * (1.0 - p_chunk) * pe_state
                Q_shared[a] += alpha * p_chunk * pe_shared

    return -total_loglik


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with meta-learning of decision temperature and load-, age-dependent decay, plus perseverance.

    Idea:
    - Standard Q-learning with within-trial decay (forgetting) that increases with load and if older.
    - The inverse temperature beta_t adapts online via a simple meta-learning rule driven by recent reward rate.
      When recent reward > current expectation, beta increases (more exploitation), otherwise decreases.
    - Persistence (perseveration) bias to repeat the last chosen action across trials within a block.

    Parameters (model_parameters):
    - alpha: Learning rate in [0,1]
    - beta0: Initial inverse temperature (>0), internally scaled (x10)
    - meta_lr: Step size for beta adaptation (>0)
    - age_meta: Multiplicative boost to meta_lr if older (>0)
    - decay_base: Base forgetting rate per decision in [0,1]
    - persev: Perseveration bias added to previous action's logit (any real)

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age: arrays as specified
    - model_parameters: tuple/list with six entries described above

    Returns:
    - Negative log-likelihood of observed actions.
    """
    alpha, beta0, meta_lr, age_meta, decay_base, persev = model_parameters
    alpha = min(max(alpha, 0.0), 1.0)
    beta0 = max(1e-6, beta0) * 10.0
    meta_lr = max(0.0, meta_lr)
    age_meta = max(0.0, age_meta)
    decay_base = min(max(decay_base, 0.0), 1.0)
    persev = float(persev)

    older = 1 if age[0] > 45 else 0
    nA = 3
    eps = 1e-12
    total_loglik = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx].astype(int)
        block_actions = actions[idx].astype(int)
        block_rewards = rewards[idx].astype(float)
        block_set_sizes = set_sizes[idx].astype(int)

        nS = int(block_set_sizes[0])
        Q = np.zeros((nS, nA))

        # Initialize adaptive beta and running reward baseline
        beta_t = beta0
        # Reward baseline timescale depends on load: faster for larger sets
        # tau in (0,1]; we use tau = 1/(1 + ss_factor*2) where ss_factor in {0,1} => tau in {1, 1/3}
        ss_factor = (float(nS) - 3.0) / 3.0
        tau = 1.0 / (1.0 + 2.0 * max(0.0, ss_factor))
        reward_baseline = 0.5  # neutral start

        prev_action = None

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = block_rewards[t]
            ss = float(block_set_sizes[t])

            # Effective decay increases with load and age
            decay_eff = decay_base * (ss / 6.0) * (1.0 + 0.5 * older)
            decay_eff = min(max(decay_eff, 0.0), 1.0)
            Q[s, :] *= (1.0 - decay_eff)

            # Action logits with perseveration
            logits = beta_t * Q[s, :]
            if prev_action is not None and 0 <= prev_action < nA:
                logits[prev_action] += persev

            # Softmax choice probabilities
            m = np.max(logits)
            exps = np.exp(logits - m)
            Z = np.sum(exps)

            if 0 <= a < nA and Z > 0:
                p_a = exps[a] / max(Z, eps)
            else:
                p_a = eps
            total_loglik += np.log(max(p_a, eps))

            # Update prev action
            prev_action = a if (0 <= a < nA) else prev_action

            # Q-learning update
            if 0 <= a < nA:
                pe = r - Q[s, a]
                Q[s, a] += alpha * pe

            # Update running reward baseline (exponential moving average)
            reward_baseline = (1.0 - tau) * reward_baseline + tau * r

            # Meta-learn beta: increase if r > baseline, decrease if r < baseline
            meta_scale = (1.0 + age_meta * older)
            beta_t = beta_t + meta_scale * meta_lr * (r - reward_baseline)
            beta_t = max(1e-6, beta_t)  # keep positive

    return -total_loglik


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Hybrid RL + Working Memory (WM) recall without explicit capacity, with age/load-modulated recall and decay,
    plus a default action bias.

    Idea:
    - RL backbone with standard Q-learning.
    - WM stores the last rewarded action per state with a strength that decays each trial.
    - Probability of using WM recall decreases with set size and is lower in older adults.
    - When WM is used and a memory exists, choice is directed to the remembered action (delta distribution).
      Otherwise, choice follows a softmax over Q-values with a default action bias.
    - WM strength decays with a rate that increases with load and age.

    Parameters (model_parameters):
    - alpha: Learning rate in [0,1]
    - beta: Inverse temperature (>0), internally scaled (x10)
    - wm_base: Baseline recall propensity in [0,1]
    - age_decline: Fractional decrease in recall propensity and WM strength if older (>=0)
    - wm_decay: Base WM decay per trial in [0,1]
    - default_bias: Bias added to action-0 logit under softmax (can be any real; positive favors action 0)

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age: arrays as specified
    - model_parameters: tuple/list with six entries described above

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    alpha, beta, wm_base, age_decline, wm_decay, default_bias = model_parameters
    alpha = min(max(alpha, 0.0), 1.0)
    beta = max(1e-6, beta) * 10.0
    wm_base = min(max(wm_base, 0.0), 1.0)
    age_decline = max(0.0, age_decline)
    wm_decay = min(max(wm_decay, 0.0), 1.0)
    default_bias = float(default_bias)

    older = 1 if age[0] > 45 else 0
    nA = 3
    eps = 1e-12
    total_loglik = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx].astype(int)
        block_actions = actions[idx].astype(int)
        block_rewards = rewards[idx].astype(float)
        block_set_sizes = set_sizes[idx].astype(int)

        nS = int(block_set_sizes[0])
        Q = np.zeros((nS, nA))
        # WM store: remembered action and its strength per state
        wm_action = -np.ones(nS, dtype=int)  # -1 means no memory
        wm_strength = np.zeros(nS)  # in [0,1]

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = block_rewards[t]
            ss = float(block_set_sizes[t])

            # Decay WM strengths globally each trial; stronger with load and age
            decay_eff = wm_decay * (ss / 6.0) * (1.0 + age_decline * older)
            decay_eff = min(max(decay_eff, 0.0), 1.0)
            wm_strength = wm_strength * (1.0 - decay_eff)

            # Compute recall probability: decreases with set size and more if older
            # Use an exponential drop with set size and multiplicative age decline
            load_drop = np.exp(-ss / 3.0)  # 3 -> e^-1, 6 -> e^-2
            recall_base = wm_base * load_drop * (1.0 - age_decline * older)
            p_recall = min(max(recall_base, 0.0), 1.0)

            # Determine policy
            used_wm = (wm_action[s] >= 0) and (wm_strength[s] > 1e-6)
            if used_wm and 0 <= wm_action[s] < nA:
                # Mixture of WM delta and softmax over Q
                # Probability assigned to WM action via p_recall, remaining via softmax
                logits = beta * Q[s, :]
                logits[0] += default_bias  # default bias on action 0
                m = np.max(logits)
                exps = np.exp(logits - m)
                Z = np.sum(exps)
                soft_p = exps / max(Z, eps) if Z > 0 else np.ones(nA) / nA

                wm_vec = np.zeros(nA)
                wm_vec[wm_action[s]] = 1.0
                p_vec = (1.0 - p_recall) * soft_p + p_recall * wm_vec
            else:
                # No usable WM: rely purely on softmax with default bias
                logits = beta * Q[s, :]
                logits[0] += default_bias
                m = np.max(logits)
                exps = np.exp(logits - m)
                Z = np.sum(exps)
                p_vec = exps / max(Z, eps) if Z > 0 else np.ones(nA) / nA

            # Log-likelihood update
            if 0 <= a < nA:
                p_a = p_vec[a]
            else:
                p_a = eps
            total_loglik += np.log(max(p_a, eps))

            # Q-learning update
            if 0 <= a < nA:
                pe = r - Q[s, a]
                Q[s, a] += alpha * pe

            # WM encoding on positive feedback: store the chosen action with full strength
            if (0 <= a < nA) and (r > 0.0):
                wm_action[s] = a
                # Age lowers attainable strength slightly via age_decline
                wm_strength[s] = min(1.0, (1.0 - 0.5 * age_decline * older))

    return -total_loglik