def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + decaying WM cache with age- and load-dependent gating.

    Mechanism:
    - Model-free Q-learning is maintained per state-action.
    - In parallel, a simple working-memory (WM) cache W tracks recently rewarded actions within each state.
      W decays on every trial; when a state-action is rewarded, its WM strength is set to 1 (others decay).
    - The policy on each trial mixes a WM-derived policy with the RL softmax policy:
        p_total = w_load_age * p_WM + (1 - w_load_age) * p_RL
      where the WM weight w_load_age is modulated by age group and set size (load).
      Younger group gets a positive boost (phi_age_boost), and higher load (nS=6) reduces the WM weight.
    - Negative log-likelihood of the observed sequence is returned.

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..set_size-1 within block).
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of {0,1}
        Feedback per trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (constant within block; 3 or 6).
    age : array-like of float
        Participant age; age[0] used. Younger: <45; Older: >=45.
    model_parameters : tuple/list
        (alpha, beta, phi_base, phi_age_boost, decay_wm)
        - alpha: learning rate for Q-learning (0..1).
        - beta: inverse temperature for softmax over RL values.
        - phi_base: baseline WM mixture propensity (pre-sigmoid).
        - phi_age_boost: additive boost to WM propensity for younger group (pre-sigmoid).
        - decay_wm: per-trial WM decay factor (0..1); higher = more persistent WM.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, phi_base, phi_age_boost, decay_wm = model_parameters
    age_value = age[0]
    is_younger = 1 if age_value < 45 else 0

    nA = 3
    total_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask].astype(int)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        nS = int(set_sizes[mask][0])

        # Initialize Q and WM cache
        Q = (1.0 / nA) * np.ones((nS, nA))
        W = np.zeros((nS, nA))  # decaying WM cache of recently rewarded actions

        # WM mixture weight depends on age and load: sigmoid(phi_base + age_boost) scaled by 3/nS
        phi = phi_base + phi_age_boost * is_younger
        wm_propensity = 1.0 / (1.0 + np.exp(-phi))  # in (0,1)
        load_scale = 3.0 / float(max(1, nS))        # 1 at nS=3, 0.5 at nS=6
        w_load_age = wm_propensity * load_scale     # effective WM mixture weight

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # RL policy
            Qs = Q[s, :]
            Q_shift = Qs - np.max(Qs)
            expQ = np.exp(beta * Q_shift)
            p_rl = expQ / np.sum(expQ)

            # WM policy from cache W: proportional to W; if no memory, uniform
            Ws = W[s, :].copy()
            if np.sum(Ws) > 0:
                p_wm = Ws / np.sum(Ws)
            else:
                p_wm = np.ones(nA) / nA

            # Mixture policy
            p_vec = w_load_age * p_wm + (1.0 - w_load_age) * p_rl
            p_choice = max(p_vec[a], 1e-12)
            total_log_p += np.log(p_choice)

            # Q-learning update
            td = r - Q[s, a]
            Q[s, a] += alpha * td

            # WM cache decay and reward-gated strengthening
            W *= decay_wm
            if r > 0.5:
                # Encode rewarded action in WM for the encountered state
                # Set to 1 (max strength) for chosen action; others already decayed
                W[s, a] = 1.0

    return -float(total_log_p)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with eligibility traces and within-state stay bias modulated by age and load.

    Mechanism:
    - Model-free Q-learning with replacing eligibility traces (e) per state-action.
      Traces decay by lambda_trace across updates and accumulate for the visited pair.
      Q updates are distributed via the trace vector.
    - A within-state "stay" bias promotes repeating the last action taken in the same state.
      The bias magnitude is scaled by set size (higher load => stronger reliance on simple heuristics)
      and age group (older group tends to rely more on such biases).
    - Policy: softmax over beta * Q plus an additive bias on the last action for that state.

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..set_size-1 within block).
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of {0,1}
        Feedback per trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (constant within block; 3 or 6).
    age : array-like of float
        Participant age; age[0] used. Younger: <45; Older: >=45.
    model_parameters : tuple/list
        (alpha, beta, gamma_stay, lambda_trace)
        - alpha: learning rate for Q updates (0..1).
        - beta: inverse temperature for softmax over preferences.
        - gamma_stay: base magnitude of stay bias (additive to action preference).
        - lambda_trace: eligibility trace decay (0..1); larger spreads credit more in time.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, gamma_stay, lambda_trace = model_parameters
    age_value = age[0]
    is_older = 1 if age_value >= 45 else 0

    nA = 3
    total_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask].astype(int)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        nS = int(set_sizes[mask][0])

        # Initialize Q and eligibility traces per block
        Q = (1.0 / nA) * np.ones((nS, nA))
        e = np.zeros((nS, nA))  # eligibility traces
        last_action = -1 * np.ones(nS, dtype=int)  # track last action per state

        # Stay bias scales up with load and in older group
        load_scale = float(nS) / 3.0          # 1 at nS=3, 2 at nS=6
        age_scale = 1.0 + 0.5 * is_older      # older gets 1.5x bias, younger 1.0x
        stay_bias_mag = gamma_stay * load_scale * age_scale

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Preference: beta*Q plus stay bias on last action for this state
            prefs = beta * Q[s, :].copy()
            if last_action[s] >= 0:
                prefs[last_action[s]] += stay_bias_mag

            # Softmax
            prefs -= np.max(prefs)
            expP = np.exp(prefs)
            p_vec = expP / np.sum(expP)
            p_choice = max(p_vec[a], 1e-12)
            total_log_p += np.log(p_choice)

            # TD error using current state's chosen action
            td = r - Q[s, a]

            # Update eligibility traces: decay then set the visited (s,a) to 1 (replacing trace)
            e *= lambda_trace
            e[s, a] = 1.0

            # Q update distributed by eligibility traces
            Q += alpha * td * e

            # Update last action for this state
            last_action[s] = a

    return -float(total_log_p)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with count-based uncertainty bonus modulated by age and load.

    Mechanism:
    - Standard Q-learning per state-action.
    - An exploration bonus inversely proportional to sqrt of visit counts encourages sampling
      less-explored actions within each state.
    - The bonus magnitude increases under higher load and depends on age group:
        younger vs older groups are given opposite-signed modulation by age_explore_slope.
      The effective bonus is: b_eff = bonus_base * (1 + age_explore_slope * age_sign) * (nS/3)
      where age_sign = +1 for younger, -1 for older.
    - Policy: softmax over Q + bonus/(sqrt(N+1)).

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..set_size-1 within block).
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of {0,1}
        Feedback per trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (constant within block; 3 or 6).
    age : array-like of float
        Participant age; age[0] used. Younger: <45; Older: >=45.
    model_parameters : tuple/list
        (alpha, beta, bonus_base, age_explore_slope)
        - alpha: learning rate for Q-learning (0..1).
        - beta: inverse temperature for softmax.
        - bonus_base: baseline count-based exploration bonus magnitude (>=0).
        - age_explore_slope: modulates exploration by age group; positive means
          more exploration for younger and less for older (and vice versa if negative).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, bonus_base, age_explore_slope = model_parameters
    age_value = age[0]
    is_younger = 1 if age_value < 45 else 0
    age_sign = 1 if is_younger else -1

    nA = 3
    total_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask].astype(int)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        nS = int(set_sizes[mask][0])

        Q = (1.0 / nA) * np.ones((nS, nA))
        N = np.zeros((nS, nA))  # visit counts per state-action

        # Effective exploration bonus magnitude, modulated by age and load
        b_eff = bonus_base * (1.0 + age_explore_slope * age_sign) * (float(nS) / 3.0)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Compute exploration bonus for current state
            bonus_vec = b_eff / np.sqrt(N[s, :] + 1.0)

            # Softmax over Q + bonus
            V = Q[s, :] + bonus_vec
            V -= np.max(V)
            expV = np.exp(beta * V)
            p_vec = expV / np.sum(expV)
            p_choice = max(p_vec[a], 1e-12)
            total_log_p += np.log(p_choice)

            # Q-learning update
            td = r - Q[s, a]
            Q[s, a] += alpha * td

            # Increment count for the visited pair
            N[s, a] += 1.0

    return -float(total_log_p)