def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Capacity-limited episodic WM with interference blended with RL; decay modulated by set size and age.

    Idea
    - A model-free Q-learner updates state-action values via prediction errors.
    - In parallel, an episodic working-memory (WM) store encodes the last rewarded action per state
      with a strength that decays due to interference. Decay increases with set size and with being older.
    - At choice time, the policy mixes WM retrieval and RL according to the current WM strength.
      If WM is strong for the state, it dominates; otherwise RL drives the choice.

    Parameters
    ----------
    states : 1D array-like of int
        State index per trial within the block (0..nS-1).
    actions : 1D array-like of int
        Chosen action per trial (0..2).
    rewards : 1D array-like of int
        Binary reward feedback per trial (0 or 1).
    blocks : 1D array-like of int
        Block index per trial.
    set_sizes : 1D array-like of int
        Set size for trials in the block (3 or 6).
    age : 1D array-like of float or int
        Participant age; age[0] is used to determine age group.
    model_parameters : tuple/list
        (alpha, beta, p_encode, decay_base, age_weight)
        - alpha: RL learning rate (0..1).
        - beta: inverse temperature for softmax (>0).
        - p_encode: baseline probability to encode a rewarded association into WM (0..1).
        - decay_base: baseline WM interference/decay rate per trial (0..1).
        - age_weight: additive multiplier for decay if older and subtractive if younger.
                      Effective decay = decay_base * (nS/3) * (1 + age_weight * age_flag),
                      where age_flag = +1 for older (>=45), -1 for younger (<45).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta, p_encode, decay_base, age_weight = model_parameters
    age_val = age[0]
    is_older = 1.0 if age_val >= 45 else 0.0
    young_flag = 1.0 - is_older  # 1 for younger, 0 for older

    nA = 3
    total_logp = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask]
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        nS = int(set_sizes[mask][0])

        # RL values
        Q = (1.0 / nA) * np.ones((nS, nA), dtype=float)

        # Episodic WM store: for each state, store an action and a strength in [0,1]
        wm_action = -1 * np.ones(nS, dtype=int)
        wm_strength = np.zeros(nS, dtype=float)

        # Effective decay increases with set size and with older age; decreases for younger
        # age_term = +age_weight if older, -age_weight if younger
        age_term = age_weight * (is_older - young_flag)
        decay_eff = max(0.0, min(1.0, decay_base * (float(nS) / 3.0) * (1.0 + age_term)))

        # Encoding probability can be slightly easier for small sets/younger
        # Modulate encoding by load and age: higher in small sets and for younger
        load_term = 3.0 / float(nS)  # 1 for setsize=3, 0.5 for setsize=6
        p_encode_eff = np.clip(p_encode * (0.75 + 0.25 * load_term) * (1.0 + 0.25 * young_flag), 0.0, 1.0)

        logp = 0.0
        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Compute RL softmax probability for chosen action
            prefs = beta * Q[s, :]
            prefs = prefs - np.max(prefs)
            exp_prefs = np.exp(prefs)
            denom = np.sum(exp_prefs)
            p_rl = max(1e-12, exp_prefs[a] / max(denom, 1e-12))

            # WM retrieval policy: if we have an action stored, proposed WM choice is that action
            # Retrieval probability equals current strength
            p_wm = 0.0
            if wm_action[s] >= 0:
                if wm_action[s] == a:
                    p_wm = wm_strength[s]
                else:
                    # If WM suggests a different action deterministically, its probability mass goes to that action.
                    # Thus probability of observed action from WM is 0, leaving RL to explain it.
                    p_wm = 0.0
            # Mixture: WM mass equals current strength; the rest is RL
            mix = np.clip(wm_strength[s], 0.0, 1.0)
            p_total = mix * p_wm + (1.0 - mix) * p_rl
            p_total = max(1e-12, p_total)
            logp += np.log(p_total)

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # WM decay (interference)
            wm_strength[s] = (1.0 - decay_eff) * wm_strength[s]

            # WM encoding on rewarded trials; overwrite with probability p_encode_eff
            if r > 0.5:
                if np.random.rand() < p_encode_eff:
                    wm_action[s] = a
                    # Refresh strength to near 1; slight dependence on load and age
                    # Stronger refresh for small sets and younger
                    refresh = 0.8 + 0.2 * load_term + 0.1 * young_flag
                    wm_strength[s] = min(1.0, max(wm_strength[s], refresh))

        total_logp += logp

    return -total_logp


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Adaptive learning rate and decision temperature as a function of cognitive load and age.

    Idea
    - Pure RL with model-free Q updates.
    - The effective learning rate and inverse temperature are adapted online to block load (set size)
      and participant age group. Younger adults under low load learn faster and are more deterministic;
      older adults and/or high load reduce both learning rate and inverse temperature.

    Parameters
    ----------
    states : 1D array-like of int
        State index per trial within the block (0..nS-1).
    actions : 1D array-like of int
        Chosen action per trial (0..2).
    rewards : 1D array-like of int
        Binary reward feedback per trial (0 or 1).
    blocks : 1D array-like of int
        Block index per trial.
    set_sizes : 1D array-like of int
        Set size for trials in the block (3 or 6).
    age : 1D array-like of float or int
        Participant age; age[0] used to determine age group.
    model_parameters : tuple/list
        (alpha_base, beta_base, k_load, k_age, k_lr)
        - alpha_base: baseline learning rate before modulation (0..1).
        - beta_base: baseline inverse temperature before modulation (>0).
        - k_load: sensitivity to set size for both alpha and beta (>=0).
        - k_age: age modulation; older (>=45) reduces alpha and beta by factor exp(-k_age),
                 younger increases by exp(+k_age/2).
        - k_lr: additional learning-rate-specific sensitivity to load (can differ from k_load).
                Positive => alpha decreases more with larger set sizes.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha_base, beta_base, k_load, k_age, k_lr = model_parameters
    age_val = age[0]
    is_older = 1.0 if age_val >= 45 else 0.0
    young_flag = 1.0 - is_older

    nA = 3
    total_logp = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask]
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        nS = int(set_sizes[mask][0])

        # Load factor: 0 for set size 3, 1 for set size 6
        load = max(0.0, (float(nS) - 3.0) / 3.0)

        # Age modulation multiplier
        age_mult_alpha = np.exp((-k_age) * is_older + (k_age / 2.0) * young_flag)
        age_mult_beta = np.exp((-k_age) * is_older + (k_age / 2.0) * young_flag)

        # Load modulation
        alpha_eff = np.clip(alpha_base * np.exp(-k_lr * load) * age_mult_alpha, 1e-6, 1.0)
        beta_eff = max(1e-6, beta_base * np.exp(-k_load * load) * age_mult_beta)

        Q = (1.0 / nA) * np.ones((nS, nA), dtype=float)

        logp = 0.0
        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Softmax probability for chosen action
            prefs = beta_eff * Q[s, :]
            prefs = prefs - np.max(prefs)
            exp_prefs = np.exp(prefs)
            denom = np.sum(exp_prefs)
            p = max(1e-12, exp_prefs[a] / max(denom, 1e-12))
            logp += np.log(p)

            # Update Q
            pe = r - Q[s, a]
            Q[s, a] += alpha_eff * pe

        total_logp += logp

    return -total_logp


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Uncertainty-driven exploration (UCB-style) blended into softmax, with age and load effects.

    Idea
    - Model-free Q-learning updates expected values.
    - An uncertainty bonus is added to preferences using a decayed visitation count per state-action.
      Less-visited actions receive higher bonus, encouraging directed exploration.
    - Counts decay faster in larger set sizes, reflecting interference in tracking uncertainty.
    - Age modulates exploration: younger adults weight uncertainty bonuses more than older adults.

    Parameters
    ----------
    states : 1D array-like of int
        State index per trial within the block (0..nS-1).
    actions : 1D array-like of int
        Chosen action per trial (0..2).
    rewards : 1D array-like of int
        Binary reward feedback per trial (0 or 1).
    blocks : 1D array-like of int
        Block index per trial.
    set_sizes : 1D array-like of int
        Set size for trials in the block (3 or 6).
    age : 1D array-like of float or int
        Participant age; age[0] used to determine age group.
    model_parameters : tuple/list
        (alpha, beta, ucb_scale, age_explore, count_decay)
        - alpha: RL learning rate (0..1).
        - beta: inverse temperature for softmax (>0).
        - ucb_scale: base weight on uncertainty (exploration) bonus.
        - age_explore: scales exploration by age; multiplier = (1 + age_explore) for younger,
                       and (1 - age_explore) for older (>=45).
        - count_decay: baseline decay rate of counts per trial (0..1). Effective decay increases with set size.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta, ucb_scale, age_explore, count_decay = model_parameters
    age_val = age[0]
    is_older = 1.0 if age_val >= 45 else 0.0
    young_flag = 1.0 - is_older

    # Age multiplier for exploration bonus
    age_mult = (1.0 + age_explore) * young_flag + (1.0 - age_explore) * is_older
    age_mult = max(0.0, age_mult)  # ensure non-negative scaling

    nA = 3
    total_logp = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask]
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        nS = int(set_sizes[mask][0])

        Q = (1.0 / nA) * np.ones((nS, nA), dtype=float)
        # Decayed visitation counts
        N = np.zeros((nS, nA), dtype=float)

        # Effective decay increases with load
        load = max(0.0, (float(nS) - 3.0) / 3.0)  # 0 for 3, 1 for 6
        decay_eff = np.clip(count_decay * (1.0 + 0.5 * load), 0.0, 1.0)

        logp = 0.0
        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Uncertainty bonus: larger when N is small; decay counts each trial to model interference
            U = 1.0 / np.sqrt(N[s, :] + 1.0)
            bonus = ucb_scale * age_mult * U

            # Softmax over Q + bonus
            prefs = beta * (Q[s, :] + bonus)
            prefs = prefs - np.max(prefs)
            exp_prefs = np.exp(prefs)
            denom = np.sum(exp_prefs)
            p = max(1e-12, exp_prefs[a] / max(denom, 1e-12))
            logp += np.log(p)

            # Q-learning update
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # Update and decay counts (decay first then increment current)
            N[s, :] = (1.0 - decay_eff) * N[s, :]
            N[s, a] += 1.0

        total_logp += logp

    return -total_logp