def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with eligibility traces and WSLS bias modulated by set size and age.

    Idea:
    - A standard TD learner with eligibility traces assigns credit not only to the chosen action
      but also carries over credit within the same state across time via a trace variable.
    - A win-stay/lose-shift (WSLS) bias affects choice policy in a state-dependent manner and is stronger
      under lower load (smaller set sizes). Age modulates the strength of the eligibility trace.

    Parameters (model_parameters): [alpha, beta_base, lambda_base, wsls_gain, age_trace_shift]
    - alpha: (0,1), learning rate for Q updates.
    - beta_base: >0, base inverse temperature (scaled internally).
    - lambda_base: (0,1), base eligibility trace decay.
    - wsls_gain: >=0, strength of WSLS bias in logits.
    - age_trace_shift: real, shifts the effective eligibility trace by age group
                       (lambda_eff = clip(lambda_base + age_trace_shift*age_group, 0, 0.99)).

    Inputs:
    - states, actions, rewards, blocks, set_sizes: arrays (T,)
    - age: array-like with a single numeric age in years; age_group = +1 for younger (<45), -1 for older (>=45).
    - model_parameters: list/tuple of parameters as above.

    Returns:
    - Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta_base, lambda_base, wsls_gain, age_trace_shift = model_parameters
    beta_base = max(1e-6, float(beta_base)) * 6.0
    wsls_gain = max(0.0, float(wsls_gain))
    age_years = float(age[0])
    age_group = 1.0 if age_years < 45.0 else -1.0

    nA = 3
    total_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]
        nS = int(block_set_sizes[0])

        # Initialize Q-values and eligibility traces per state-action
        Q = np.zeros((nS, nA))
        e = np.zeros((nS, nA))  # eligibility traces

        # State-specific memory of last action and reward for WSLS
        last_action = -np.ones(nS, dtype=int)
        last_reward = np.zeros(nS)

        # Effective parameters per block
        beta_eff = beta_base * (3.0 / float(nS))  # lower load => higher effective temperature
        beta_eff = max(1e-6, beta_eff)
        lambda_eff = np.clip(lambda_base + age_trace_shift * age_group, 0.0, 0.99)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # WSLS state-dependent bias vector
            wsls_bias = np.zeros(nA)
            if last_action[s] >= 0:
                prev_a = int(last_action[s])
                if last_reward[s] > 0.5:
                    # win-stay: bias toward previous action
                    wsls_bias[prev_a] += 1.0
                else:
                    # lose-shift: bias away from previous action (spread to others)
                    for aa in range(nA):
                        if aa != prev_a:
                            wsls_bias[aa] += 1.0 / (nA - 1)

            # WSLS gains are stronger under lower load
            wsls_gain_eff = wsls_gain * (3.0 / float(nS))

            logits = beta_eff * Q[s, :] + wsls_gain_eff * wsls_bias
            logits -= np.max(logits)
            pi = np.exp(logits)
            pi /= np.sum(pi)

            p = max(1e-12, float(pi[a]))
            total_log_p += np.log(p)

            # TD error and eligibility update
            delta = r - Q[s, a]

            # Decay all traces in current state and action space, then set trace for visited SA to 1
            e[s, :] *= lambda_eff
            e[s, a] = 1.0

            # Update Q-values using eligibility traces (within this state)
            Q[s, :] += alpha * delta * e[s, :]

            # Update WSLS memory
            last_action[s] = a
            last_reward[s] = r

    return -total_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with set-size and ageâ€“modulated swap (binding) errors and lapse.

    Idea:
    - A standard Q-learning system governs values per state-action.
    - On each trial, with probability p_swap, the agent mistakenly retrieves the value policy
      from another state (binding error), mixing decision evidence from other states.
    - Swap probability increases with set size and can be modulated by age group.
    - A small lapse probability mixes in a uniform random choice.

    Parameters (model_parameters): [alpha, beta, p_swap_base, swap_size_slope, age_swap_shift, lapse]
    - alpha: (0,1), learning rate for Q updates.
    - beta: >0, inverse temperature (scaled internally).
    - p_swap_base: base swap probability at set size = 3 for younger group.
    - swap_size_slope: >=0, additional swap probability per +3 items: p_swap = p_swap_base + swap_size_slope*(nS-3)/3.
    - age_swap_shift: real, additive shift due to age group: + for older, - for younger;
                      implemented as p_swap += age_swap_shift*(1 - age_group), where age_group=1 (younger), -1 (older).
    - lapse: in [0, 0.2], probability of uniform random responding.

    Inputs:
    - states, actions, rewards, blocks, set_sizes: arrays (T,)
    - age: array-like with a single numeric age in years.
    - model_parameters: list/tuple as above.

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    alpha, beta, p_swap_base, swap_size_slope, age_swap_shift, lapse = model_parameters
    beta = max(1e-6, float(beta)) * 6.0
    swap_size_slope = max(0.0, float(swap_size_slope))
    lapse = np.clip(float(lapse), 0.0, 0.2)

    age_years = float(age[0])
    age_group = 1.0 if age_years < 45.0 else -1.0  # younger=+1, older=-1

    nA = 3
    total_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]
        nS = int(block_set_sizes[0])

        Q = np.zeros((nS, nA))

        # Compute effective swap probability for this block
        p_swap = float(p_swap_base) + swap_size_slope * ((nS - 3.0) / 3.0)
        # Age modulation: increase for older, decrease for younger
        p_swap += age_swap_shift * (0.5 * (1.0 - age_group) - 0.5 * (1.0 + age_group))
        # The above equals: -age_swap_shift for younger (age_group=+1), +age_swap_shift for older (age_group=-1)
        p_swap = np.clip(p_swap, 0.0, 0.9)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Softmax policy for the true state
            logits_true = beta * Q[s, :]
            logits_true -= np.max(logits_true)
            pi_true = np.exp(logits_true)
            pi_true /= np.sum(pi_true)

            # Softmax policies for other states (if any), average them to represent a swapped retrieval
            if nS > 1:
                other_states = [ss for ss in range(nS) if ss != s]
                pi_swapped = np.zeros(nA)
                for ss in other_states:
                    logits_other = beta * Q[ss, :]
                    logits_other -= np.max(logits_other)
                    pi_o = np.exp(logits_other)
                    pi_o /= np.sum(pi_o)
                    pi_swapped += pi_o
                pi_swapped /= len(other_states)
            else:
                pi_swapped = np.copy(pi_true)

            # Mix according to swap probability and lapse
            pi_mix = (1.0 - lapse) * ((1.0 - p_swap) * pi_true + p_swap * pi_swapped) + lapse * (np.ones(nA) / nA)

            p = max(1e-12, float(pi_mix[a]))
            total_log_p += np.log(p)

            # Update Q for the actually visited state-action
            Q[s, a] += alpha * (r - Q[s, a])

    return -total_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with action priors: age- and load-modulated global action propensity combined with state values.

    Idea:
    - Maintain standard state-action Q-values.
    - Additionally, maintain a block-global action propensity vector (phi) learned from rewards regardless of state.
      This captures habitual or heuristic action preferences that can assist under higher load.
    - The prior contributes additively to choice logits, with strength modulated by age and set size.
      The learning rate for the prior is modulated by set size.

    Parameters (model_parameters): [alpha, beta, prior_weight_base, age_weight_shift, size_lr_slope]
    - alpha: (0,1), learning rate for Q updates.
    - beta: >0, inverse temperature on Q (scaled internally).
    - prior_weight_base: >=0, base weight of the action prior contribution at set size=3 for younger group.
    - age_weight_shift: real, multiplicative shift on prior weight by age group: weight *= (1 + age_weight_shift*age_group).
    - size_lr_slope: real in [0,1], slope that increases prior learning rate with set size (more reliance under high load).

    Inputs:
    - states, actions, rewards, blocks, set_sizes: arrays (T,)
    - age: array-like with a single numeric age in years.
    - model_parameters: list/tuple as above.

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    alpha, beta, prior_weight_base, age_weight_shift, size_lr_slope = model_parameters
    beta = max(1e-6, float(beta)) * 6.0
    prior_weight_base = max(0.0, float(prior_weight_base))
    size_lr_slope = np.clip(float(size_lr_slope), 0.0, 1.0)

    age_years = float(age[0])
    age_group = 1.0 if age_years < 45.0 else -1.0

    nA = 3
    total_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]
        nS = int(block_set_sizes[0])

        Q = np.zeros((nS, nA))
        phi = np.zeros(nA)  # global action propensity logits

        # Prior weight increases with set size (more reliance when WM load is high)
        prior_weight = prior_weight_base * (1.0 + age_weight_shift * age_group) * (float(nS) / 3.0)
        # Ensure non-negative and bounded to avoid dominating Q entirely
        prior_weight = np.clip(prior_weight, 0.0, 10.0)

        # Prior learning rate increases with set size
        prior_lr = size_lr_slope * ((nS - 3.0) / 3.0 + 1.0)  # equals size_lr_slope at nS=3, doubles at nS=6
        prior_lr = np.clip(prior_lr, 0.0, 1.0)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Combine Q and prior in logits
            logits = beta * Q[s, :] + prior_weight * phi
            logits -= np.max(logits)
            pi = np.exp(logits)
            pi /= np.sum(pi)

            p = max(1e-12, float(pi[a]))
            total_log_p += np.log(p)

            # Update RL values
            Q[s, a] += alpha * (r - Q[s, a])

            # Update global action propensity toward rewarding actions (centered around 0.5 to avoid drift)
            # Encourage chosen action if rewarded, discourage if not; keep phi zero-centered by a small decay
            baseline = 0.5
            phi *= (1.0 - 0.05)  # mild decay to avoid runaway accumulation
            phi[a] += prior_lr * (r - baseline)

    return -total_log_p