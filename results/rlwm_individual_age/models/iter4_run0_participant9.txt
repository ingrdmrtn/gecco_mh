def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Q-learning with meta-controlled inverse temperature (beta) shaped by load and age.

    Mechanism
    - RL values: tabular Q-learning with learning rate alpha.
    - Policy: softmax with a dynamic beta_t that adapts to recent surprise (|PE|).
    - Meta-control: a latent per-state temperature offset z[s] is updated to decrease
      beta when surprise is high and increase it when surprise is low. Load and age
      add tonic shifts to beta.
      - Smaller set sizes increase beta (more exploitation).
      - Older adults have a lower beta (more exploration) via an age shift.

    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..nS-1 within a block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of {0,1}
        Feedback on each trial.
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size for each trial (e.g., 3 or 6).
    age : array-like
        Participant age; uses age[0]. Age group: younger (<45) vs older (>=45).
    model_parameters : sequence of 5 floats
        [alpha, beta0, eta_meta, size_beta_gain, age_beta_shift]
        - alpha: Q-learning rate (sigmoid to [0,1]).
        - beta0: baseline inverse temperature before modulators (softplus >= 0).
        - eta_meta: meta-learning rate for temperature offset z (sigmoid to [0,1]).
        - size_beta_gain: increases beta at smaller set sizes (real-valued).
        - age_beta_shift: reduces beta in older adults (>=0; applied if age>=45).

    Returns
    -------
    nll : float
        Negative log-likelihood of observed actions under the model.
    """
    alpha, beta0, eta_meta, size_beta_gain, age_beta_shift = model_parameters

    # Parameter transforms / constraints
    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))
    def softplus(x):
        return np.log1p(np.exp(-np.abs(x))) + np.maximum(x, 0.0)

    alpha = sigmoid(alpha)
    beta0 = softplus(beta0)
    eta_meta = sigmoid(eta_meta)
    age_beta_shift = np.maximum(0.0, age_beta_shift)

    age_group = 1 if age[0] >= 45 else 0

    nll = 0.0
    nA = 3

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]
        nS = int(block_set_sizes[0])

        # Initialize Q-values and per-state temperature offsets
        Q = (1.0 / nA) * np.ones((nS, nA))
        z = np.zeros(nS)  # meta-controlled additive temperature offsets

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])
            nS_t = int(block_set_sizes[t])

            # Load and age modulators on beta
            # size_term > 0 for small set size (nS=3); ~0 for larger (nS=6)
            size_term = size_beta_gain * ((3.0 / float(nS_t)) - 0.5)
            age_term = -age_group * age_beta_shift

            beta_t = softplus(beta0 + size_term + age_term + z[s])

            # Softmax policy
            logits = beta_t * Q[s, :]
            logits -= np.max(logits)
            exp_logits = np.exp(logits)
            p = exp_logits / np.maximum(1e-12, np.sum(exp_logits))

            p_a = np.clip(p[a], 1e-12, 1.0)
            nll -= np.log(p_a)

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # Meta update for temperature offset: reduce beta when surprise is high
            # z increases beta if pe small; decreases if pe large
            z[s] += -eta_meta * (np.abs(pe) - 0.5)

    return nll


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Bayesian action-value estimation with decaying Beta-Bernoulli memory, with load- and age-dependent decay.

    Mechanism
    - For each state-action pair, maintain a Beta posterior over reward probability:
      Beta(A[s,a], B[s,a]), initialized with symmetric prior kappa (A=B=kappa).
    - On each trial: A[s,a] += r, B[s,a] += (1-r).
    - Between trials, apply leaky forgetting toward the prior: A <- (1-d)*A + d*kappa, same for B.
      Forgetting rate d increases with set size (higher load) and with age (older group).
    - Policy: softmax over posterior means mu[s,a] = A/(A+B) with inverse temperature beta.

    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..nS-1 within a block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of {0,1}
        Feedback on each trial.
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size for each trial (e.g., 3 or 6).
    age : array-like
        Participant age; uses age[0]. Age group: younger (<45) vs older (>=45).
    model_parameters : sequence of 5 floats
        [beta, kappa_prior, decay_base, size_decay_gain, age_decay_gain]
        - beta: inverse temperature for softmax (softplus >= 0).
        - kappa_prior: symmetric Beta prior strength (softplus >= 0).
        - decay_base: baseline forgetting rate d in [0,1] via sigmoid.
        - size_decay_gain: increases d as set size increases (real-valued).
        - age_decay_gain: additional increase in d for older adults (>=0).

    Returns
    -------
    nll : float
        Negative log-likelihood of observed actions under the model.
    """
    beta, kappa_prior, decay_base, size_decay_gain, age_decay_gain = model_parameters

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))
    def softplus(x):
        return np.log1p(np.exp(-np.abs(x))) + np.maximum(x, 0.0)

    beta = softplus(beta)
    kappa_prior = softplus(kappa_prior)
    age_decay_gain = np.maximum(0.0, age_decay_gain)
    age_group = 1 if age[0] >= 45 else 0

    nll = 0.0
    nA = 3

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]
        nS = int(block_set_sizes[0])

        # Initialize symmetric Beta prior for each state-action
        A = kappa_prior * np.ones((nS, nA))
        B = kappa_prior * np.ones((nS, nA))

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])
            nS_t = int(block_set_sizes[t])

            # Compute forgetting rate d_t (toward prior) based on load and age
            # size_term increases with larger set sizes: (nS-3)/3 is 0 (SS=3) or 1 (SS=6)
            size_term = size_decay_gain * ((float(nS_t) - 3.0) / 3.0)
            d_t = sigmoid(decay_base + size_term + age_group * age_decay_gain)

            # Apply decay toward prior before observing current outcome
            A[s, :] = (1.0 - d_t) * A[s, :] + d_t * kappa_prior
            B[s, :] = (1.0 - d_t) * B[s, :] + d_t * kappa_prior

            # Posterior mean as expected value
            mu = A[s, :] / np.maximum(1e-12, (A[s, :] + B[s, :]))

            # Softmax choice
            logits = beta * mu
            logits -= np.max(logits)
            exp_logits = np.exp(logits)
            p = exp_logits / np.maximum(1e-12, np.sum(exp_logits))

            p_a = np.clip(p[a], 1e-12, 1.0)
            nll -= np.log(p_a)

            # Update Beta posterior with observed outcome for chosen action
            A[s, a] += r
            B[s, a] += (1.0 - r)

    return nll


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Q-learning with a decaying choice kernel (perseveration) whose influence depends on load and age.

    Mechanism
    - RL: tabular Q-learning with learning rate alpha.
    - Choice kernel C[s,a]: captures recent choice history within a state.
      On each visit to state s: C[s,:] <- (1 - lambda)*C[s,:]; then C[s,a] += 1 for the chosen action.
    - Policy: softmax over beta*Q + w*C.
      The kernel weight w depends on set size and age via a logistic transform:
        w = sigmoid(pers_bias_base + age_load_gain*(age_group - (nS-3)/3)).
      Thus, smaller set sizes and older age increase reliance on the choice kernel.

    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..nS-1 within a block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of {0,1}
        Feedback on each trial.
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size for each trial (e.g., 3 or 6).
    age : array-like
        Participant age; uses age[0]. Age group: younger (<45) vs older (>=45).
    model_parameters : sequence of 5 floats
        [alpha, beta, lambda_trace, pers_bias_base, age_load_gain]
        - alpha: Q-learning rate (sigmoid to [0,1]).
        - beta: inverse temperature for softmax on Q (softplus >= 0).
        - lambda_trace: decay of choice kernel per state visit (sigmoid to [0,1]).
        - pers_bias_base: baseline logit for kernel weight w (real).
        - age_load_gain: scales combined effect of age (older=+1) and load ((nS-3)/3).

    Returns
    -------
    nll : float
        Negative log-likelihood of observed actions under the model.
    """
    alpha, beta, lambda_trace, pers_bias_base, age_load_gain = model_parameters

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))
    def softplus(x):
        return np.log1p(np.exp(-np.abs(x))) + np.maximum(x, 0.0)

    alpha = sigmoid(alpha)
    beta = softplus(beta)
    lambda_trace = sigmoid(lambda_trace)
    age_group = 1 if age[0] >= 45 else 0

    nll = 0.0
    nA = 3

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]
        nS = int(block_set_sizes[0])

        Q = (1.0 / nA) * np.ones((nS, nA))
        C = np.zeros((nS, nA))  # choice kernel

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])
            nS_t = int(block_set_sizes[t])

            # Kernel weight depends on load and age
            load_metric = (float(nS_t) - 3.0) / 3.0  # 0 for SS=3, 1 for SS=6
            w = sigmoid(pers_bias_base + age_load_gain * (age_group - load_metric))

            # Policy over augmented values
            aug = beta * Q[s, :] + w * C[s, :]
            aug -= np.max(aug)
            exp_aug = np.exp(aug)
            p = exp_aug / np.maximum(1e-12, np.sum(exp_aug))

            p_a = np.clip(p[a], 1e-12, 1.0)
            nll -= np.log(p_a)

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # Update choice kernel for this state
            C[s, :] *= (1.0 - lambda_trace)
            C[s, a] += 1.0

    return nll