def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Q-learning with asymmetric learning rates and age/load-modulated perseveration.

    The model learns state-action values with separate learning rates for positive and negative
    prediction errors. Choice policy is a softmax over Q-values plus a perseveration bias that
    favors repeating the last action taken in the same state. The perseveration strength is
    modulated by set size (higher at larger loads) and age group (stronger in older adults).

    Parameters
    ----------
    states : array-like of int
        State indices per trial (0..set_size-1 within block).
    actions : array-like of int
        Chosen action indices (0..2).
    rewards : array-like of {0,1}
        Feedback per trial.
    blocks : array-like of int
        Block indices; learning and biases reset per block.
    set_sizes : array-like of int
        Set size (3 or 6), per trial.
    age : array-like of int or float
        Participant age; age>=45 considered older group.
    model_parameters : sequence of float
        [alpha_pos, alpha_neg, beta, k_persev, age_load_gain]
          - alpha_pos: learning rate for positive PEs (0..1).
          - alpha_neg: learning rate for negative PEs (0..1).
          - beta: inverse temperature for softmax (>0).
          - k_persev: base strength of perseveration bias (>0).
          - age_load_gain: gain scaling of perseveration with age/load (can be +/-).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha_pos, alpha_neg, beta, k_persev, age_load_gain = model_parameters
    age_val = float(age[0]) if hasattr(age, "__len__") else float(age)
    is_older = age_val >= 45

    nll = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask]
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])
        Q = (1.0 / nA) * np.ones((nS, nA))

        # Track last action per state for perseveration; initialize to -1 (no previous)
        last_action = -1 * np.ones(nS, dtype=int)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            ss = int(block_set_sizes[t])

            # Load and age modulation of perseveration
            # Load term ranges 0 at set size 3, 1 at set size 6
            load_term = max(0.0, (ss - 3) / 3.0)
            age_term = 1.0 if is_older else -1.0
            persev_gain = 1.0 + age_load_gain * (load_term + age_term)
            persev_strength = max(0.0, k_persev * persev_gain)

            # Construct logits: beta*Q plus perseveration bump for repeating last action in this state
            logits = beta * Q[s, :].copy()
            if last_action[s] >= 0:
                logits[last_action[s]] += persev_strength

            # Softmax
            logits -= np.max(logits)
            exp_logits = np.exp(logits)
            p_vec = exp_logits / np.sum(exp_logits)
            p = max(p_vec[a], eps)
            nll -= np.log(p)

            # Update learning
            pe = r - Q[s, a]
            lr = alpha_pos if pe >= 0.0 else alpha_neg
            Q[s, a] += lr * pe

            # Update perseveration memory
            last_action[s] = a

    return nll


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with a recency-weighted associative cache (fast-mapping WM) modulated by load and age.

    The model blends:
      - Model-free Q-learning (single learning rate, softmax policy),
      - A cache that stores the last rewarded action for each state along with its recency.
    On each trial, the cached action (if any) is retrieved with a probability that decays
    with time since last reward for that state, scales down with set size (load), and is
    modulated by age group.

    Parameters
    ----------
    states : array-like of int
        State indices per trial (0..set_size-1 within block).
    actions : array-like of int
        Chosen action indices (0..2).
    rewards : array-like of {0,1}
        Feedback per trial.
    blocks : array-like of int
        Block indices; learning and cache reset per block.
    set_sizes : array-like of int
        Set size (3 or 6), per trial.
    age : array-like of int or float
        Participant age; age>=45 considered older group.
    model_parameters : sequence of float
        [alpha, beta, cache_recall_base, recency_decay, age_recall_bias]
          - alpha: RL learning rate (0..1).
          - beta: inverse temperature for softmax (>0).
          - cache_recall_base: base probability of recalling cached action (0..1).
          - recency_decay: exponential decay rate of cache activation with time since last reward (>0).
          - age_recall_bias: multiplicative factor on recall for younger; reciprocal for older (>0).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, cache_recall_base, recency_decay, age_recall_bias = model_parameters
    age_val = float(age[0]) if hasattr(age, "__len__") else float(age)
    is_older = age_val >= 45

    nll = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask]
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        Q = (1.0 / nA) * np.ones((nS, nA))

        # Cache: for each state, store cached action and last rewarded time index
        cached_action = -1 * np.ones(nS, dtype=int)
        last_reward_time = -1 * np.ones(nS, dtype=int)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            ss = int(block_set_sizes[t])

            # RL policy
            q_s = Q[s, :]
            q_s_shift = q_s - np.max(q_s)
            p_rl_vec = np.exp(beta * q_s_shift)
            p_rl_vec = p_rl_vec / np.sum(p_rl_vec)
            p_rl = p_rl_vec[a]

            # Cache recall probability: decays with time since last reward, reduced by load,
            # and modulated by age group.
            if last_reward_time[s] >= 0:
                gap = max(0, t - last_reward_time[s])
                recency = np.exp(-recency_decay * gap)
                load_factor = 3.0 / max(1.0, ss)  # 1 at set size 3; 0.5 at set size 6
                age_bias = (age_recall_bias if not is_older else 1.0 / max(age_recall_bias, eps))
                p_recall = cache_recall_base * recency * load_factor * age_bias
                p_recall = max(0.0, min(1.0, p_recall))
            else:
                p_recall = 0.0

            # Cache policy: picks the cached action deterministically; uniform otherwise
            if cached_action[s] >= 0:
                p_cache_vec = np.full(nA, eps)
                p_cache_vec[cached_action[s]] = 1.0
                p_cache_vec = p_cache_vec / np.sum(p_cache_vec)
            else:
                p_cache_vec = np.ones(nA) / nA
            p_cache = p_cache_vec[a]

            # Mixture policy
            p = p_recall * p_cache + (1.0 - p_recall) * p_rl
            p = max(p, eps)
            nll -= np.log(p)

            # Update RL
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # Update cache: store only if rewarded
            if r > 0.5:
                cached_action[s] = a
                last_reward_time[s] = t

    return nll


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Actor-Critic with load-dependent critic learning and age-sensitive uncertainty-driven temperature.

    Mechanism:
      - Critic learns state values V(s); actor learns action preferences P(s,a) via policy gradient.
      - The action policy is softmax over P(s,·) with an inverse temperature adapting to uncertainty:
          beta_t(s) = beta_base * (1 + age_temp_bias * U(s) * L(ss) * A(age))
        where U(s) is value uncertainty proxy (max at V≈0.5), L(ss) is a load factor, and A(age)
        flips the modulation sign by age group.
      - Critic learning rate increases with load via load_critic_gain.

    Parameters
    ----------
    states : array-like of int
        State indices per trial (0..set_size-1 within block).
    actions : array-like of int
        Chosen action indices (0..2).
    rewards : array-like of {0,1}
        Feedback per trial.
    blocks : array-like of int
        Block indices; learning resets per block.
    set_sizes : array-like of int
        Set size (3 or 6), per trial.
    age : array-like of int or float
        Participant age; age>=45 considered older group.
    model_parameters : sequence of float
        [alpha_actor, alpha_critic, beta_base, load_critic_gain, age_temp_bias]
          - alpha_actor: actor learning rate (0..1).
          - alpha_critic: base critic learning rate (0..1).
          - beta_base: base inverse temperature (>0).
          - load_critic_gain: multiplicative gain of critic lr with load (can be +/-).
          - age_temp_bias: strength of age-sensitive uncertainty modulation of beta (>0).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha_actor, alpha_critic, beta_base, load_critic_gain, age_temp_bias = model_parameters
    age_val = float(age[0]) if hasattr(age, "__len__") else float(age)
    is_older = age_val >= 45

    nll = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask]
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize actor preferences and critic values
        P = np.zeros((nS, nA))
        V = 0.5 * np.ones(nS)  # initial uncertainty

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            ss = int(block_set_sizes[t])

            # Uncertainty proxy: peaked when V~0.5, low when V near 0 or 1
            U = 1.0 - 2.0 * abs(V[s] - 0.5)  # in [0,1]
            # Load factor: higher modulation at low load
            L = 3.0 / max(1.0, ss)  # 1 at ss=3, 0.5 at ss=6
            # Age modulation sign: younger +, older -
            A = (1.0 if not is_older else -1.0)

            beta_t = beta_base * (1.0 + age_temp_bias * U * L * A)
            beta_t = max(beta_t, eps)

            # Softmax over actor preferences
            logits = beta_t * (P[s, :] - np.max(P[s, :]))
            exp_logits = np.exp(logits)
            p_vec = exp_logits / np.sum(exp_logits)
            p = max(p_vec[a], eps)
            nll -= np.log(p)

            # Compute TD error with critic
            delta = r - V[s]

            # Actor update (policy gradient with baseline)
            # Increase preference for chosen action, decrease others implicitly via softmax normalization
            P[s, a] += alpha_actor * delta * (1.0 - p_vec[a])
            for a_oth in range(nA):
                if a_oth != a:
                    P[s, a_oth] += -alpha_actor * delta * p_vec[a_oth]

            # Critic update with load-dependent learning rate
            load_scale = 1.0 + load_critic_gain * max(0.0, (ss - 3) / 3.0)
            alpha_c_eff = max(0.0, alpha_critic * load_scale)
            V[s] += alpha_c_eff * delta

    return nll