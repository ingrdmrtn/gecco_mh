def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Pearce-Hall associability RL with age- and load-modulated lapse.

    The model implements a model-free RL updated by a dynamic associability (Pearce-Hall),
    and a stochastic lapse that increases with set size and with older age. Associability
    boosts learning following surprising outcomes (large unsigned prediction errors).
    Set-size modulates learning rate gain (smaller sets -> higher effective learning).

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..set_size-1 within a block).
    actions : array-like of int
        Chosen action (0..2).
    rewards : array-like of float {0,1}
        Feedback per trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (3 or 6).
    age : array-like of float
        Participant age; uses age[0]. Younger: age<45; Older: age>=45.
    model_parameters : list or array
        [alpha0, beta0, k_phi, lapse_base, age_lapse_slope, size_lr_slope]
        - alpha0 (0..1): base RL learning rate.
        - beta0 (>0): base inverse temperature; scaled internally by 10.
        - k_phi (0..1): associability update gain towards |PE|.
        - lapse_base (real): baseline lapse log-odds.
        - age_lapse_slope (real): additional lapse log-odds for older group
                                  (younger adds 0; older adds +age_lapse_slope).
        - size_lr_slope (real): multiplicative modulation of learning by set size:
                                lr_eff = alpha0 * [1 + size_lr_slope*(3/set_size - 1)].

    Returns
    -------
    neg_log_likelihood : float
        Negative log-likelihood of observed choices.
    """
    alpha0, beta0, k_phi, lapse_base, age_lapse_slope, size_lr_slope = model_parameters
    beta = max(1e-8, beta0) * 10.0
    k_phi = np.clip(k_phi, 0.0, 1.0)

    age_val = age[0]
    is_older = 1.0 if age_val >= 45 else 0.0
    is_younger = 1.0 - is_older

    nA = 3
    eps = 1e-12
    neg_log_like = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx].astype(int)
        block_actions = actions[idx].astype(int)
        block_rewards = rewards[idx].astype(float)
        block_sets = set_sizes[idx].astype(int)
        nS = int(block_sets[0])

        # Initialize Q-values and associability (phi) per state-action
        Q = (1.0 / nA) * np.ones((nS, nA))
        phi = np.ones((nS, nA)) * 0.5  # start with moderate associability

        log_p = 0.0
        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            set_t = int(block_sets[t])

            # Size-modulated learning rate
            size_gain = 1.0 + size_lr_slope * (3.0 / float(set_t) - 1.0)
            # Effective associability for the chosen pair
            assoc = 0.1 + 0.9 * np.clip(phi[s, a], 0.0, 1.0)
            lr_eff = np.clip(alpha0 * size_gain * assoc, 0.0, 1.0)

            # RL policy (softmax)
            q_s = Q[s, :]
            logits = beta * (q_s - np.max(q_s))
            expv = np.exp(logits)
            pol_rl = expv / (np.sum(expv) + eps)

            # Lapse probability increases with set size and older age
            lapse_logit = lapse_base + is_older * age_lapse_slope + np.log(float(set_t) / 3.0)
            lapse = 1.0 / (1.0 + np.exp(-lapse_logit))
            lapse = np.clip(lapse, 0.0, 1.0)

            pol = (1.0 - lapse) * pol_rl + lapse * (np.ones(nA) / nA)
            p_choice = np.clip(pol[a], eps, 1.0)
            log_p += np.log(p_choice)

            # Update Q with associability-weighted PE
            pe = r - Q[s, a]
            Q[s, a] += lr_eff * pe

            # Update associability toward unsigned PE (Pearce-Hall)
            phi[s, a] = (1.0 - k_phi) * phi[s, a] + k_phi * abs(pe)

        neg_log_like += -log_p

    return float(neg_log_like)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited working memory cache with age-specific slots and retrieval noise.

    The model mixes:
    - Model-free RL (Q-learning).
    - A WM cache that stores rewarded state->action mappings. The probability of using WM
      equals coverage = min(1, slots / set_size), where slots depends on age group.
      WM retrieval is noisy with temperature parameter wm_tau.
    - A set- and age-modulated lapse to uniform choice.

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..set_size-1 within a block).
    actions : array-like of int
        Chosen action (0..2).
    rewards : array-like of float {0,1}
        Feedback per trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (3 or 6).
    age : array-like of float
        Participant age; uses age[0]. Younger: age<45; Older: age>=45.
    model_parameters : list or array
        [alpha, beta, slots_young, slots_old, wm_tau, epsilon]
        - alpha (0..1): RL learning rate.
        - beta (>0): RL inverse temperature; scaled internally by 10.
        - slots_young (>=0): WM slots for younger group.
        - slots_old (>=0): WM slots for older group.
        - wm_tau (>0): WM retrieval temperature; smaller is sharper recall.
        - epsilon (>=0): lapse gain controlling uniform-choice probability
                         that increases with set size and with older age.

    Returns
    -------
    neg_log_likelihood : float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, slots_young, slots_old, wm_tau, epsilon = model_parameters
    beta = max(1e-8, beta) * 10.0
    wm_tau = max(1e-8, wm_tau)
    slots_young = max(0.0, slots_young)
    slots_old = max(0.0, slots_old)
    epsilon = max(0.0, epsilon)

    age_val = age[0]
    is_older = 1.0 if age_val >= 45 else 0.0
    is_younger = 1.0 - is_older

    nA = 3
    eps = 1e-12
    neg_log_like = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx].astype(int)
        block_actions = actions[idx].astype(int)
        block_rewards = rewards[idx].astype(float)
        block_sets = set_sizes[idx].astype(int)
        nS = int(block_sets[0])

        Q = (1.0 / nA) * np.ones((nS, nA))

        # WM cache: stored action and confidence
        wm_act = -np.ones(nS, dtype=int)
        wm_conf = np.zeros(nS)  # 0..1 confidence

        log_p = 0.0

        # Determine available slots by age group for this block
        slots = is_younger * slots_young + is_older * slots_old
        coverage = min(1.0, slots / float(nS + 1e-12))  # probability of using WM

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            set_t = int(block_sets[t])

            # RL policy
            q_s = Q[s, :]
            logits_rl = beta * (q_s - np.max(q_s))
            pol_rl = np.exp(logits_rl)
            pol_rl = pol_rl / (np.sum(pol_rl) + eps)

            # WM policy: if stored, softmax with retrieval noise scaled by confidence
            if wm_act[s] >= 0:
                wm_beta = (1.0 / wm_tau) * max(1e-6, wm_conf[s])
                logits_wm = np.zeros(nA)
                logits_wm[wm_act[s]] = wm_beta
                logits_wm = logits_wm - np.max(logits_wm)
                pol_wm = np.exp(logits_wm)
                pol_wm = pol_wm / (np.sum(pol_wm) + eps)
            else:
                pol_wm = np.ones(nA) / nA

            # Arbitration: use WM with probability equal to coverage
            pol_mix = coverage * pol_wm + (1.0 - coverage) * pol_rl

            # Lapse increases with set size and age
            lapse = 1.0 / (1.0 + np.exp(-(epsilon * (set_t / 3.0) * (0.8 + 0.4 * is_older))))
            pol = (1.0 - lapse) * pol_mix + lapse * (np.ones(nA) / nA)

            p_choice = np.clip(pol[a], eps, 1.0)
            log_p += np.log(p_choice)

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += np.clip(alpha, 0.0, 1.0) * pe

            # WM cache update: store correct mapping on reward; decay otherwise
            if r > 0.5:
                wm_act[s] = a
                wm_conf[s] = 1.0
            else:
                wm_conf[s] *= 0.5  # decay memory confidence on non-reward

        neg_log_like += -log_p

    return float(neg_log_like)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Surprise-calibrated exploration with meta-adaptive temperature and decaying perseveration.

    The model uses:
    - Model-free RL with asymmetric learning rates (scaled for negative PEs).
    - A meta-adaptive inverse temperature that grows with state-specific confidence
      (lower recent surprise), decreases with set size, and is age-shifted.
    - A global perseveration bias toward the previous action that decays as surprise increases.

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..set_size-1 within a block).
    actions : array-like of int
        Chosen action (0..2).
    rewards : array-like of float {0,1}
        Feedback per trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (3 or 6).
    age : array-like of float
        Participant age; uses age[0]. Younger: age<45; Older: age>=45.
    model_parameters : list or array
        [alpha, neg_factor, beta_base, beta_gain, age_beta_shift, stick0]
        - alpha (0..1): baseline learning rate for positive prediction errors.
        - neg_factor (>=0): multiplier for negative PE learning (alpha_neg = alpha*neg_factor).
        - beta_base (>0): base inverse temperature (scaled by 10 internally).
        - beta_gain (>=0): gain translating confidence into added inverse temperature.
        - age_beta_shift (real): multiplicative shift of temperature by age group:
                                 beta_t *= (1 + age_beta_shift) for younger,
                                 beta_t *= (1 - age_beta_shift) for older.
        - stick0 (real): baseline perseveration strength added to previous action's logit;
                         effective stick decays with recent surprise.

    Returns
    -------
    neg_log_likelihood : float
        Negative log-likelihood of observed choices.
    """
    alpha, neg_factor, beta_base, beta_gain, age_beta_shift, stick0 = model_parameters
    alpha = np.clip(alpha, 0.0, 1.0)
    neg_factor = max(0.0, neg_factor)
    beta_base = max(1e-8, beta_base)
    beta_gain = max(0.0, beta_gain)

    age_val = age[0]
    is_younger = 1.0 if age_val < 45 else 0.0
    is_older = 1.0 - is_younger

    nA = 3
    eps = 1e-12
    neg_log_like = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx].astype(int)
        block_actions = actions[idx].astype(int)
        block_rewards = rewards[idx].astype(float)
        block_sets = set_sizes[idx].astype(int)
        nS = int(block_sets[0])

        Q = (1.0 / nA) * np.ones((nS, nA))

        # Track recent surprise per state: running average of |PE| (higher -> more uncertain)
        u = np.ones(nS) * 0.5  # uncertainty proxy (0..1); start moderate
        prev_action = None
        prev_abs_pe = 0.0

        log_p = 0.0

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            set_t = int(block_sets[t])

            # Confidence from recent surprise: conf = 1 - u[s]
            conf_s = np.clip(1.0 - u[s], 0.0, 1.0)

            # Meta-adaptive inverse temperature
            beta_t = beta_base + beta_gain * conf_s * (3.0 / float(set_t))
            age_mult = is_younger * (1.0 + age_beta_shift) + is_older * (1.0 - age_beta_shift)
            age_mult = max(0.0, age_mult)
            beta_t = max(1e-8, beta_t) * age_mult * 10.0

            # RL logits
            q_s = Q[s, :]
            logits = beta_t * (q_s - np.max(q_s))

            # Surprise-sensitive perseveration toward previous action
            if prev_action is not None:
                # Decay stickiness with previous surprise (larger surprise -> less perseveration)
                stick_eff = stick0 * np.exp(-prev_abs_pe)
                stick_vec = np.zeros(nA)
                stick_vec[int(prev_action)] = stick_eff
                logits = logits + stick_vec

            # Softmax policy
            expv = np.exp(logits)
            pol = expv / (np.sum(expv) + eps)
            p_choice = np.clip(pol[a], eps, 1.0)
            log_p += np.log(p_choice)

            # Compute PE and update Q with asymmetric learning rate
            pe = r - Q[s, a]
            lr = alpha if pe >= 0.0 else alpha * neg_factor
            Q[s, a] += np.clip(lr, 0.0, 1.0) * pe

            # Update uncertainty trace u[s] toward |PE|
            u[s] = 0.9 * u[s] + 0.1 * abs(pe)

            # Update prev action and surprise for next perseveration computation
            prev_action = a
            prev_abs_pe = abs(pe)

        neg_log_like += -log_p

    return float(neg_log_like)