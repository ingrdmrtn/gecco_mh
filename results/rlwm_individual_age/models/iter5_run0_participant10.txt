def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + Win-Stay/Lose-Shift arbitration with load- and age-modulated mixture.

    Concept:
    - Two systems produce action probabilities:
      (1) Model-free RL (Q-learning with softmax).
      (2) Win-Stay/Lose-Shift (WSLS) policy that biases toward repeating the last rewarded action
          and away from the last unrewarded action in the same state.
    - The arbitration weight for WSLS increases when set size is small (lower load) and when the
      participant is younger; older adults rely relatively more on RL.
    - WSLS strength also depends on a param controlling the sharpness of win/lose bias.

    Parameters (model_parameters; total 5):
    - lr_q: learning rate for RL (0..1).
    - inv_temp: inverse temperature for RL softmax (>0).
    - ws_strength: strength of WSLS utility bump (>0).
    - load_sensitivity: scales effect of set size on WSLS arbitration (can be +/-).
    - age_shift: age-group shift in WSLS arbitration (positive -> more WSLS for younger, less for older).

    Inputs:
    - states: array of state indices per trial (0..set_size-1 within each block).
    - actions: array of chosen actions per trial (0,1,2).
    - rewards: array of rewards per trial (0/1).
    - blocks: array of block ids; learning and memory reset at block boundaries.
    - set_sizes: array of set sizes per trial (e.g., 3 or 6).
    - age: array-like with single element (years).
    - model_parameters: tuple/list [lr_q, inv_temp, ws_strength, load_sensitivity, age_shift].

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr_q, inv_temp, ws_strength, load_sensitivity, age_shift = model_parameters
    age_val = age[0]
    is_older = 1.0 if age_val >= 45 else 0.0
    is_younger = 1.0 - is_older

    nll = 0.0
    nA = 3
    eps = 1e-12

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nS = int(block_set_sizes[0])
        # RL values
        Q = (1.0 / nA) * np.ones((nS, nA))
        # WSLS memory: last action and last reward per state
        last_act = -1 * np.ones(nS, dtype=int)  # -1 indicates unknown
        last_rew = np.zeros(nS)

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])
            curr_set = int(block_set_sizes[t])

            # RL softmax policy
            q_s = Q[s, :]
            logits = inv_temp * (q_s - np.max(q_s))
            p_rl = np.exp(logits)
            p_rl /= np.sum(p_rl)

            # WSLS policy via utility bump and softmax
            u_ws = np.zeros(nA)
            if last_act[s] != -1:
                la = last_act[s]
                if last_rew[s] > 0.5:
                    # Win-stay: bump last action
                    u_ws[la] += ws_strength
                else:
                    # Lose-shift: penalize last action (push mass to others)
                    u_ws[la] -= ws_strength
            # Convert WS utilities to a probability via softmax (fixed inverse temperature)
            tau_ws = 5.0  # fixed sharpness for WSLS mapping
            logits_ws = tau_ws * (u_ws - np.max(u_ws))
            p_ws = np.exp(logits_ws)
            p_ws /= np.sum(p_ws)

            # Arbitration weight for WSLS: increases with lower load and being younger
            # base input: smaller set -> larger 3/curr_set; younger -> +1, older -> -1
            arb_input = (load_sensitivity * (3.0 / float(curr_set))) + (age_shift * (is_younger - is_older))
            # logistic squash to [0,1]
            w_ws = 1.0 / (1.0 + np.exp(-arb_input))

            # Mixture policy
            p_mix = w_ws * p_ws + (1.0 - w_ws) * p_rl
            p_choice = max(p_mix[a], eps)
            nll -= np.log(p_choice)

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += lr_q * pe

            # Update WSLS memory
            last_act[s] = a
            last_rew[s] = r

    return nll


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with load- and age-dependent decay/interference.

    Concept:
    - Model-free RL values for each state-action pair decay toward uniform at each visit.
    - Decay/interference is stronger with larger set sizes and in older adults, capturing
      working-memory load and age-related maintenance failures that flatten value differences.

    Parameters (model_parameters; total 5):
    - lr_base: RL learning rate (0..1).
    - inv_temp: RL inverse temperature (>0).
    - decay_base: baseline per-visit decay toward uniform (0..1).
    - load_gain: additive modulation of decay by set size (positive -> more decay at larger sets).
    - age_gain: additive modulation of decay for older adults (>=45) relative to younger (positive -> more decay if older).

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age as described above.
    - model_parameters: [lr_base, inv_temp, decay_base, load_gain, age_gain].

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr_base, inv_temp, decay_base, load_gain, age_gain = model_parameters
    age_val = age[0]
    is_older = 1.0 if age_val >= 45 else 0.0

    nll = 0.0
    nA = 3
    eps = 1e-12

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nS = int(block_set_sizes[0])
        Q = (1.0 / nA) * np.ones((nS, nA))

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])
            curr_set = int(block_set_sizes[t])

            # Compute effective decay for this visit to state s
            # Larger sets increase decay; older adults have extra decay
            load_term = load_gain * max(0.0, (curr_set - 3.0) / 3.0)  # 0 for set=3, load_gain for set=6
            d_eff = decay_base + load_term + age_gain * is_older
            d_eff = min(max(d_eff, 0.0), 1.0)

            # Apply decay toward uniform on the visited state row
            Q[s, :] = (1.0 - d_eff) * Q[s, :] + d_eff * (1.0 / nA)

            # Choice policy from RL softmax
            q_s = Q[s, :]
            logits = inv_temp * (q_s - np.max(q_s))
            p = np.exp(logits)
            p /= np.sum(p)
            p_choice = max(p[a], eps)
            nll -= np.log(p_choice)

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += lr_base * pe

    return nll


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Volatility-adaptive RL with load- and age-modulated exploration.

    Concept:
    - Track state-wise outcome volatility as an exponential moving average of absolute prediction errors.
    - Learning rate increases with estimated volatility (more updating when surprising outcomes persist).
    - Exploration/exploitation (inverse temperature) is modulated by set size (more exploration at larger set sizes)
      and by age group (older adults explore more / have lower inverse temperature).

    Parameters (model_parameters; total 5):
    - alpha0: baseline learning rate component (0..1).
    - beta0: baseline inverse temperature (>0).
    - v_sens: sensitivity of learning rate to volatility (>=0).
    - load_beta_gain: modulation of inverse temperature by set size (negative -> lower beta at larger sets).
    - age_beta_shift: multiplicative log-scale shift of inverse temperature by age group
                      (negative -> lower beta for older; positive -> higher beta for younger).

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age as described.
    - model_parameters: [alpha0, beta0, v_sens, load_beta_gain, age_beta_shift].

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    alpha0, beta0, v_sens, load_beta_gain, age_beta_shift = model_parameters
    age_val = age[0]
    is_older = 1.0 if age_val >= 45 else 0.0
    is_younger = 1.0 - is_older

    nll = 0.0
    nA = 3
    eps = 1e-12
    lam_vol = 0.3  # volatility EMA step size

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nS = int(block_set_sizes[0])
        Q = (1.0 / nA) * np.ones((nS, nA))
        Vol = np.zeros((nS,))  # per-state volatility estimate in [0,1] approximately

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])
            curr_set = int(block_set_sizes[t])

            # Effective inverse temperature: reduced with larger set sizes (more exploration)
            # and reduced in older adults (age_beta_shift negative yields lower beta if older).
            load_factor = (3.0 / float(curr_set))  # 1.0 for 3, 0.5 for 6
            beta_eff = beta0 * np.exp(load_beta_gain * (load_factor - 1.0)) * np.exp(age_beta_shift * (is_younger - is_older))
            beta_eff = max(beta_eff, 1e-6)

            # Policy from RL
            q_s = Q[s, :]
            logits = beta_eff * (q_s - np.max(q_s))
            p = np.exp(logits)
            p /= np.sum(p)
            p_choice = max(p[a], eps)
            nll -= np.log(p_choice)

            # RL update with volatility-adaptive learning rate
            pe = r - Q[s, a]
            Vol[s] = (1.0 - lam_vol) * Vol[s] + lam_vol * abs(pe)
            alpha_eff = alpha0 + v_sens * Vol[s]
            alpha_eff = min(max(alpha_eff, 0.0), 1.0)
            Q[s, a] += alpha_eff * pe

    return nll