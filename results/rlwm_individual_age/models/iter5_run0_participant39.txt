def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with attention-modulated learning, age- and set-size-dependent perseveration, and lapse.

    Mechanism:
    - Q-learning, but the effective learning rate is modulated by a state-specific attention signal
      that tracks unsigned prediction error (Pearce-Hall style).
    - A perseveration bias to repeat the last chosen action in a state; stronger in older adults
      and in smaller sets (where habits/generalization may be more impactful).
    - Lapse probability that increases with set size and in older adults.

    Parameters
    ----------
    states : array-like of int
        State indices per trial within each block (0..set_size-1).
    actions : array-like of int
        Chosen actions per trial (0..2).
    rewards : array-like of float/int
        Binary rewards per trial (0 or 1).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (3 or 6).
    age : array-like of float
        Participant age; age[0] is used. Age >= 45 treated as older.
    model_parameters : tuple/list of floats
        (alpha0, beta0, kappa_att, rho_pers, eps0)
        - alpha0: base RL learning rate (0..1)
        - beta0: inverse temperature for softmax
        - kappa_att: attention update speed factor (controls how fast attention tracks |PE|)
        - rho_pers: base perseveration strength (added to the preferred action's logit)
        - eps0: base lapse rate; scaled up with set size and in older adults

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha0, beta0, kappa_att, rho_pers, eps0 = model_parameters
    age_val = float(age[0])
    is_older = 1.0 if age_val >= 45.0 else 0.0

    total_loglik = 0.0
    eps_num = 1e-12
    nA = 3

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nS = int(block_set_sizes[0])

        Q = (1.0 / nA) * np.ones((nS, nA))
        # State-specific attention (tracks unsigned PE)
        att = np.zeros(nS)
        # Last action per state for perseveration, -1 means none yet
        last_action = -1 * np.ones(nS, dtype=int)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            ss = float(block_set_sizes[t])

            # Perseveration strength: stronger in older adults and smaller set sizes
            pers_strength = rho_pers * (1.0 + 0.5 * is_older) * (3.0 / ss)

            # Lapse increases with set size and age
            lapse = eps0 * (ss / 3.0) * (1.0 + 0.5 * is_older)
            lapse = np.clip(lapse, 0.0, 0.49)

            # Softmax logits with perseveration bias
            bias_vec = np.zeros(nA)
            if last_action[s] >= 0:
                bias_vec[last_action[s]] = pers_strength

            logits = beta0 * Q[s, :] + bias_vec
            logits -= np.max(logits)
            pi = np.exp(logits)
            pi = pi / (np.sum(pi) + eps_num)
            # Apply lapse
            pi = (1.0 - lapse) * pi + lapse * (1.0 / nA)

            total_loglik += np.log(pi[a] + eps_num)

            # RL update with attention-modulated learning rate
            pe = r - Q[s, a]
            # Update attention for this state; speed depends on kappa_att, age, and set size
            eta_att = kappa_att * (3.0 / ss) * (1.0 - 0.3 * is_older)
            eta_att = np.clip(eta_att, 0.0, 1.0)
            att[s] = (1.0 - eta_att) * att[s] + eta_att * np.abs(pe)

            # Effective learning rate scales with attention (cap to [0,1])
            alpha_eff = np.clip(alpha0 * (0.5 + att[s]), 0.0, 1.0)
            Q[s, a] = Q[s, a] + alpha_eff * pe

            # Update perseveration memory
            last_action[s] = a

    return -float(total_loglik)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited working memory store with age- and set-size-modulated decay.

    Mechanism:
    - RL system learns Q-values via delta rule.
    - Working Memory (WM) stores the last reliably rewarded action for each state with a strength
      that decays with set size and more strongly in older adults.
    - Arbitration is a probability-of-recall mixture: effective WM weight is proportional to
      capacity coverage (capacity / set size) times the current strength of the WM trace.

    Parameters
    ----------
    states : array-like of int
        State indices per trial within each block (0..set_size-1).
    actions : array-like of int
        Chosen actions per trial (0..2).
    rewards : array-like of float/int
        Binary rewards per trial (0 or 1).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (3 or 6).
    age : array-like of float
        Participant age; age[0] is used. Age >= 45 treated as older.
    model_parameters : tuple/list of floats
        (alpha_rl, beta, slot_frac, decay_wm)
        - alpha_rl: RL learning rate (0..1)
        - beta: inverse temperature used for both RL and WM softmaxes
        - slot_frac: fraction (0..1) determining capacity cap = 1 + 5*slot_frac;
                     effective capacity is reduced in older adults
        - decay_wm: base WM decay rate per encounter of a state; scaled up with set size and age

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha_rl, beta, slot_frac, decay_wm = model_parameters
    age_val = float(age[0])
    is_older = 1.0 if age_val >= 45.0 else 0.0

    total_loglik = 0.0
    eps_num = 1e-12
    nA = 3

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nS = int(block_set_sizes[0])

        Q = (1.0 / nA) * np.ones((nS, nA))
        wm_action = -1 * np.ones(nS, dtype=int)  # stored action per state
        wm_strength = np.zeros(nS)               # confidence/strength (0..1)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            ss = float(block_set_sizes[t])

            # Effective WM capacity and decay
            cap = 1.0 + 5.0 * slot_frac * (1.0 - 0.3 * is_older)  # older -> fewer effective slots
            coverage = np.clip(cap / ss, 0.0, 1.0)

            d_eff = decay_wm * (ss / 3.0) * (1.0 + 0.5 * is_older)
            d_eff = np.clip(d_eff, 0.0, 1.0)

            # Decay the WM strength for the visited state
            wm_strength[s] = wm_strength[s] * (1.0 - d_eff)

            # Construct WM policy (if an action is stored)
            wm_logits = np.zeros(nA)
            if wm_action[s] >= 0:
                # Prefer the stored action proportionally to current strength
                wm_logits[wm_action[s]] = beta * np.clip(wm_strength[s], 0.0, 1.0)
            wm_logits -= np.max(wm_logits)
            wm_pi = np.exp(wm_logits)
            wm_pi = wm_pi / (np.sum(wm_pi) + eps_num)

            # RL policy
            rl_logits = beta * Q[s, :]
            rl_logits -= np.max(rl_logits)
            rl_pi = np.exp(rl_logits)
            rl_pi = rl_pi / (np.sum(rl_pi) + eps_num)

            # Arbitration: probability of relying on WM equals coverage * strength
            wm_weight = np.clip(coverage * wm_strength[s], 0.0, 1.0)
            pi = wm_weight * wm_pi + (1.0 - wm_weight) * rl_pi

            total_loglik += np.log(pi[a] + eps_num)

            # RL update
            pe = r - Q[s, a]
            Q[s, a] = Q[s, a] + alpha_rl * pe

            # WM encoding/update on reward
            if r > 0.0:
                wm_action[s] = a
                wm_strength[s] = 1.0  # refresh to maximal strength upon successful feedback

    return -float(total_loglik)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Arbitration between RL and WM-like habit using confidence vs. RL uncertainty,
    with age- and set-size effects, plus a novelty bonus.

    Mechanism:
    - RL: Q-learning.
    - WM-habit: remembers last rewarded action per state with a confidence that decays with
      set size and more in older adults; confidence increases after rewards and decreases otherwise.
    - Arbitration: soft weight via logistic function of (WM confidence - RL uncertainty),
      shifted by an age and set-size informed bias.
    - Exploration: novelty bonus for less-tried actions; weaker in older adults and larger sets.

    Parameters
    ----------
    states : array-like of int
        State indices per trial within each block (0..set_size-1).
    actions : array-like of int
        Chosen actions per trial (0..2).
    rewards : array-like of float/int
        Binary rewards per trial (0 or 1).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (3 or 6).
    age : array-like of float
        Participant age; age[0] is used. Age >= 45 treated as older.
    model_parameters : tuple/list of floats
        (alpha, beta, arb_slope, arb_bias, novelty)
        - alpha: RL learning rate (0..1)
        - beta: inverse temperature for softmax
        - arb_slope: slope of logistic arbitration (sensitivity to confidence-uncertainty)
        - arb_bias: baseline arbitration bias toward WM (>0) or RL (<0)
        - novelty: base novelty bonus magnitude

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, arb_slope, arb_bias, novelty = model_parameters
    age_val = float(age[0])
    is_older = 1.0 if age_val >= 45.0 else 0.0

    total_loglik = 0.0
    eps_num = 1e-12
    nA = 3

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nS = int(block_set_sizes[0])

        Q = (1.0 / nA) * np.ones((nS, nA))
        last_win_action = -1 * np.ones(nS, dtype=int)
        conf = np.zeros(nS)               # WM confidence per state (0..1)
        N = np.zeros((nS, nA))            # visit counts per state-action for novelty

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            ss = float(block_set_sizes[t])

            # RL policy with novelty bonus
            bonus_scale = novelty * (3.0 / ss) * (1.0 + 0.5 * (1.0 - is_older))
            bonus_vec = bonus_scale / np.sqrt(1.0 + N[s, :])
            rl_logits = beta * (Q[s, :] + bonus_vec)
            rl_logits -= np.max(rl_logits)
            rl_pi = np.exp(rl_logits)
            rl_pi = rl_pi / (np.sum(rl_pi) + eps_num)

            # WM policy: target the last rewarded action if exists
            wm_pi = np.ones(nA) / nA
            if last_win_action[s] >= 0:
                wm_pi = np.zeros(nA)
                wm_pi[last_win_action[s]] = 1.0

            # RL uncertainty proxy: entropy of RL policy
            rl_entropy = -np.sum(rl_pi * np.log(rl_pi + eps_num)) / np.log(nA)  # normalized 0..1

            # Confidence decay influenced by age and set size
            decay_conf = (0.15 * (ss / 3.0) * (1.0 + 0.5 * is_older))
            decay_conf = np.clip(decay_conf, 0.0, 1.0)
            conf[s] = conf[s] * (1.0 - decay_conf)

            # Arbitration weight (toward WM)
            bias_term = arb_bias + (-0.3 * is_older) + (-0.3 * (ss / 6.0))
            x = arb_slope * (conf[s] - rl_entropy) + bias_term
            wm_weight = 1.0 / (1.0 + np.exp(-x))
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            pi = wm_weight * wm_pi + (1.0 - wm_weight) * rl_pi
            total_loglik += np.log(pi[a] + eps_num)

            # RL update
            pe = r - Q[s, a]
            Q[s, a] = Q[s, a] + alpha * pe

            # Update WM memory and confidence
            if r > 0.0:
                last_win_action[s] = a
                conf[s] = np.clip(conf[s] + 0.5, 0.0, 1.0)
            else:
                conf[s] = conf[s] * 0.5  # reduce confidence after a miss

            # Update counts for novelty
            N[s, a] += 1.0

    return -float(total_loglik)