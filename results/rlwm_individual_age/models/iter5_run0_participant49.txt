def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Volatility-adaptive RL: inverse temperature adapts to state-level surprise, with age and load
    increasing apparent volatility (thus increasing exploration).

    Mechanism
    - Standard Q-learning within each block.
    - Maintain a per-state volatility proxy v_s as an exponential moving average of unsigned PEs.
    - The effective inverse temperature beta_eff decreases as volatility increases.
    - Older adults and larger set sizes add to the volatility term, increasing exploration.

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..nS-1 within each block).
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of float/int
        Reward per trial (0 or 1).
    blocks : array-like of int
        Block ID per trial; learning resets per block.
    set_sizes : array-like of int
        Set size per trial (3 or 6); assumed constant within a block.
    age : array-like or scalar
        Participant age; older (>=45) increases the volatility term.
    model_parameters : sequence of 6 floats
        alpha       : Learning rate for Q-values
        beta_init   : Baseline inverse temperature (scaled by 10 internally)
        kappa       : Sensitivity of beta to volatility (higher -> more exploration under volatility)
        vol_lr      : Learning rate for the volatility proxy (0..1)
        age_bias    : Additive volatility offset if older (>=45)
        load_bias   : Volatility increment per unit load ((nS-3)/3)

    Returns
    -------
    nll : float
        Negative log-likelihood of observed actions.
    """
    alpha, beta_init, kappa, vol_lr, age_bias, load_bias = model_parameters
    beta_init = beta_init * 10.0

    try:
        age_val = float(age[0])
    except Exception:
        age_val = float(age)
    is_older = 1.0 if age_val >= 45 else 0.0

    nA = 3
    eps = 1e-12
    total_log_p = 0.0

    unique_blocks = np.unique(blocks)
    for b in unique_blocks:
        idx = (blocks == b)
        block_actions = np.asarray(actions)[idx]
        block_rewards = np.asarray(rewards)[idx]
        block_states = np.asarray(states)[idx]
        block_set_sizes = np.asarray(set_sizes)[idx]
        nS = int(block_set_sizes[0])

        # Initialize Q and per-state volatility proxy
        Q = (1.0 / nA) * np.ones((nS, nA))
        V = np.zeros(nS)  # volatility proxy per state

        # Precompute load contribution
        load_term = load_bias * ((float(nS) - 3.0) / 3.0)
        age_term = age_bias * is_older

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Effective inverse temperature decreases with volatility
            vol = max(V[s] + age_term + load_term, 0.0)
            beta_eff = beta_init / (1.0 + kappa * vol)

            # Softmax over actions
            prefs = Q[s, :] - np.max(Q[s, :])
            p = np.exp(beta_eff * prefs)
            p = p / (np.sum(p) + eps)

            total_log_p += np.log(max(p[a], eps))

            # Update Q
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # Update volatility proxy with unsigned PE
            V[s] = (1.0 - vol_lr) * V[s] + vol_lr * abs(pe)

    return -float(total_log_p)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Capacity-limited Working Memory (WM) with recency decay, mixed with RL.
    Older age and larger set size reduce effective WM contribution via capacity.

    Mechanism
    - RL: standard Q-learning.
    - WM: a table that stores high confidence for recently rewarded action in each state.
      The WM traces decay across trials, controlled by recency_decay.
    - Capacity: Only up to K states receive full WM strength; others get diminished WM due to
      limited capacity. K is reduced by age; WM weight is scaled by min(1, K/nS).
    - Final policy: mixture of WM softmax and RL softmax weighted by effective WM strength.

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..nS-1 within each block).
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of float/int
        Reward per trial (0 or 1).
    blocks : array-like of int
        Block ID per trial; learning resets per block.
    set_sizes : array-like of int
        Set size per trial (3 or 6); assumed constant within a block.
    age : array-like or scalar
        Participant age; older (>=45) reduces WM capacity.
    model_parameters : sequence of 6 floats
        alpha_rl       : RL learning rate
        beta           : Inverse temperature for both policies (scaled by 10 internally)
        wm_strength    : Base WM mixture weight (0..1)
        cap_base       : Baseline WM capacity in number of states (e.g., around 3-4)
        age_cap_drop   : Capacity reduction if older (>=45)
        recency_decay  : Per-trial decay of WM traces (0..1), higher = faster forgetting

    Returns
    -------
    nll : float
        Negative log-likelihood of observed actions.
    """
    alpha_rl, beta, wm_strength, cap_base, age_cap_drop, recency_decay = model_parameters
    beta = beta * 10.0

    try:
        age_val = float(age[0])
    except Exception:
        age_val = float(age)
    is_older = 1.0 if age_val >= 45 else 0.0

    nA = 3
    eps = 1e-12
    total_log_p = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = np.asarray(actions)[idx]
        block_rewards = np.asarray(rewards)[idx]
        block_states = np.asarray(states)[idx]
        block_set_sizes = np.asarray(set_sizes)[idx]

        nS = int(block_set_sizes[0])

        # RL values
        Q = (1.0 / nA) * np.ones((nS, nA))
        # WM traces: confidence for actions per state
        WM = np.zeros((nS, nA))
        # Track last refresh time for states to implement simple recency-based prioritization
        last_seen = -np.ones(nS)

        # Effective capacity adjusted by age
        K = max(0.0, cap_base - age_cap_drop * is_older)
        # WM mixture scaled by how much of the set can be actively maintained
        wm_weight_eff = wm_strength * min(1.0, K / float(nS))

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Decay WM globally
            WM *= (1.0 - recency_decay)

            # RL policy
            prefs_rl = Q[s, :] - np.max(Q[s, :])
            p_rl = np.exp(beta * prefs_rl)
            p_rl = p_rl / (np.sum(p_rl) + eps)

            # WM policy
            prefs_wm = WM[s, :] - np.max(WM[s, :])
            p_wm = np.exp(beta * prefs_wm)
            p_wm = p_wm / (np.sum(p_wm) + eps)

            # Mixture
            p_mix = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            total_log_p += np.log(max(p_mix[a], eps))

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += alpha_rl * pe

            # WM update: if rewarded, strongly store the chosen action; if not, partial store
            # Also embed simple recency priority by boosting recently seen states.
            if r > 0.5:
                WM[s, :] *= 0.0
                WM[s, a] = 1.0
            else:
                # Weak negative evidence: reduce confidence in chosen action
                WM[s, a] = max(0.0, WM[s, a] - 0.5)

            last_seen[s] = t

    return -float(total_log_p)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    WSLS-RL hybrid with age/load-modulated lapse.
    - A win-stay/lose-shift (WSLS) heuristic operates at the state level based on last outcome.
    - RL with asymmetric learning rates tracks action values.
    - An epsilon lapse probability increases with older age and higher load, mixing in uniform choice.

    Policy
    p(a) = (1 - epsilon) * [ ws_weight * p_wsls(a) + (1 - ws_weight) * p_rl(a) ] + epsilon * 1/nA

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..nS-1 within block).
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of float/int
        Reward per trial (0 or 1).
    blocks : array-like of int
        Block ID per trial; learning resets per block.
    set_sizes : array-like of int
        Set size per trial (3 or 6); assumed constant within a block.
    age : array-like or scalar
        Participant age; older (>=45) increases lapse.
    model_parameters : sequence of 6 floats
        alpha_pos         : Learning rate for positive PEs
        alpha_neg         : Learning rate for negative PEs
        beta              : Inverse temperature for softmax (scaled by 10 internally)
        ws_weight         : Mixture weight of WSLS vs RL (0..1)
        lapse_intercept   : Baseline logit of lapse probability
        lapse_age_load    : Slope on (is_older + load) where load = (nS-3)/3

    Returns
    -------
    nll : float
        Negative log-likelihood of observed actions.
    """
    alpha_pos, alpha_neg, beta, ws_weight, lapse_intercept, lapse_age_load = model_parameters
    beta = beta * 10.0

    try:
        age_val = float(age[0])
    except Exception:
        age_val = float(age)
    is_older = 1.0 if age_val >= 45 else 0.0

    nA = 3
    eps = 1e-12
    total_log_p = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = np.asarray(actions)[idx]
        block_rewards = np.asarray(rewards)[idx]
        block_states = np.asarray(states)[idx]
        block_set_sizes = np.asarray(set_sizes)[idx]
        nS = int(block_set_sizes[0])

        # RL values
        Q = (1.0 / nA) * np.ones((nS, nA))
        # For WSLS: store last action and last reward per state
        last_act = -1 * np.ones(nS, dtype=int)
        last_rew = np.zeros(nS)

        # Lapse probability for this block depends on age and set size
        load = (float(nS) - 3.0) / 3.0  # 0 for 3, 1 for 6
        z = lapse_intercept + lapse_age_load * (is_older + load)
        epsilon = 1.0 / (1.0 + np.exp(-z))
        epsilon = min(max(epsilon, 0.0), 1.0)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # RL softmax
            prefs_rl = Q[s, :] - np.max(Q[s, :])
            p_rl = np.exp(beta * prefs_rl)
            p_rl = p_rl / (np.sum(p_rl) + eps)

            # WSLS policy at the state level
            ws_pref = np.zeros(nA)
            if last_act[s] >= 0:
                if last_rew[s] > 0.5:
                    # Win-stay: boost previous action
                    ws_pref[last_act[s]] = 1.0
                else:
                    # Lose-shift: suppress previous action (boost others equally)
                    ws_pref += 1.0 / (nA - 1)
                    ws_pref[last_act[s]] = 0.0
            else:
                # No history: uniform
                ws_pref[:] = 1.0 / nA

            # Convert WSLS preferences to a softmax distribution for better comparability
            ws_prefs = ws_pref - np.max(ws_pref)
            p_ws = np.exp(beta * ws_prefs)
            p_ws = p_ws / (np.sum(p_ws) + eps)

            # Mixture with lapse
            p_mix_core = ws_weight * p_ws + (1.0 - ws_weight) * p_rl
            p_final = (1.0 - epsilon) * p_mix_core + epsilon * (1.0 / nA)

            total_log_p += np.log(max(p_final[a], eps))

            # RL update with asymmetric learning
            pe = r - Q[s, a]
            if pe >= 0:
                Q[s, a] += alpha_pos * pe
            else:
                Q[s, a] += alpha_neg * pe

            # Update WSLS memory
            last_act[s] = a
            last_rew[s] = r

    return -float(total_log_p)