def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with eligibility traces and time-based forgetting, modulated by set size and age, plus lapses.

    The model uses a softmax RL policy over Q-values but incorporates:
    - Eligibility traces within each state to allow recent within-state actions to be credited.
    - Time-based forgetting of Q-values between visits to a state, increasing with set size and age.
    - Learning rate scaled down under higher set sizes.
    - Lapse probability mixing with a uniform policy.

    Parameters
    ----------
    states : array-like of int
        State indices on each trial (0..nS-1 within block).
    actions : array-like of int
        Chosen actions on each trial (0..nA-1).
    rewards : array-like of int
        Binary rewards (0 or 1) observed each trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size for the block of each trial.
    age : array-like of float
        Participant age; age[0] determines age group (older >=45).
    model_parameters : tuple/list of floats
        (alpha_base, beta, trace_lambda, forget_base, age_forget_shift, lapse)
        - alpha_base: base learning rate before load scaling (squashed 0..1)
        - beta: inverse temperature for the RL softmax (scaled internally)
        - trace_lambda: eligibility trace decay factor within state (squashed 0..1)
        - forget_base: base per-trial forgetting rate for Q (squashed 0..0.2)
        - age_forget_shift: additive increase of forgetting for older group (0..0.2 effective)
        - lapse: lapse rate mixing with uniform policy (squashed to 0..0.2)

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha_base, beta, trace_lambda, forget_base, age_forget_shift, lapse = model_parameters

    # Parameter squashing/scaling
    alpha_base = 1.0 / (1.0 + np.exp(-alpha_base))
    beta = (1.0 / (1.0 + np.exp(-beta))) * 12.0
    lam = 1.0 / (1.0 + np.exp(-trace_lambda))
    forget_base = (1.0 / (1.0 + np.exp(-forget_base))) * 0.2
    age_forget_shift = (1.0 / (1.0 + np.exp(-age_forget_shift))) * 0.2
    lapse = (1.0 / (1.0 + np.exp(-lapse))) * 0.2

    age_val = float(age[0])
    older = 1 if age_val >= 45 else 0

    nll = 0.0
    eps = 1e-12
    nA = 3

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx].astype(int)
        block_actions = actions[idx].astype(int)
        block_rewards = rewards[idx].astype(float)
        nS = int(set_sizes[idx][0])

        # Initialize Q and eligibility traces per state
        Q = np.zeros((nS, nA))
        E = np.zeros((nS, nA))

        # Track last time each state was visited for time-based forgetting/trace decay
        last_visit = -np.ones(nS, dtype=int)

        # Effective forgetting increases with set size and age
        # Scale with load factor (nS/3); older add age_forget_shift
        forget_eff = forget_base * (float(nS) / 3.0) + age_forget_shift * older
        forget_eff = np.clip(forget_eff, 0.0, 0.95)

        # Learning rate reduced under higher load
        alpha = alpha_base * (3.0 / float(nS))

        for t in range(len(block_states)):
            s = block_states[t]
            a = block_actions[t]
            r = block_rewards[t]

            # Apply time-based decay to Q(s,.) and E(s,.) if there was a gap since last visit
            if last_visit[s] >= 0:
                gap = (t - last_visit[s])
                # Equivalent multi-step decay: Q <- Q * (1 - forget_eff)^gap
                if gap > 0:
                    decay = (1.0 - forget_eff) ** gap
                    Q[s, :] *= decay
                    E[s, :] *= (lam ** gap)
            last_visit[s] = t

            # Policy: softmax over Q
            q = Q[s, :] - np.max(Q[s, :])
            pi = np.exp(beta * q)
            pi = pi / (np.sum(pi) + eps)

            # Lapse with uniform
            pi = (1.0 - lapse) * pi + lapse * (1.0 / nA)

            p = max(pi[a], eps)
            nll -= np.log(p)

            # TD error and eligibility trace update within this state
            delta = r - Q[s, a]
            # Update traces: decay and set chosen action's trace to 1 (replacing)
            E[s, :] *= lam
            E[s, a] = 1.0
            # Credit assignment to all actions in this state via E
            Q[s, :] += alpha * delta * E[s, :]

    return nll


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Hybrid RL + capacity-limited Working Memory (WM) with load- and age-modulated gating.

    Mechanism:
    - WM stores the most recently rewarded action per state with a strength that reflects
      capacity limits. If set size <= effective capacity, WM strength remains high; otherwise,
      it decays via interference.
    - Policy is a mixture: when WM strength is high, behavior is driven by WM; otherwise by RL.
    - RL updates standard Q-values with softmax action selection.
    - Age modulates WM capacity (younger assumed higher capacity).

    Parameters
    ----------
    states : array-like of int
        State indices on each trial (0..nS-1 within block).
    actions : array-like of int
        Chosen actions on each trial (0..nA-1).
    rewards : array-like of int
        Binary rewards (0 or 1) observed each trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size for the block of each trial.
    age : array-like of float
        Participant age; age[0] determines age group (older >=45).
    model_parameters : tuple/list of floats
        (alpha, beta, K_base, age_K_shift, wm_temp, gate_bias)
        - alpha: RL learning rate (squashed 0..1)
        - beta: RL inverse temperature (scaled internally)
        - K_base: base WM capacity fraction (squashed, mapped to 1..6 slots)
        - age_K_shift: capacity slot shift for older group (can be negative/positive, mapped to -2..+2)
        - wm_temp: inverse temperature for WM policy (scaled internally; higher -> more deterministic)
        - gate_bias: baseline bias to use WM when available (squashed to 0..1)

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, K_base, age_K_shift, wm_temp, gate_bias = model_parameters

    alpha = 1.0 / (1.0 + np.exp(-alpha))
    beta = (1.0 / (1.0 + np.exp(-beta))) * 12.0
    # Map base capacity to [1,6]
    K_base = 1.0 + 5.0 * (1.0 / (1.0 + np.exp(-K_base)))
    # Age shift mapped to [-2, +2]
    age_K_shift = 4.0 * (1.0 / (1.0 + np.exp(-age_K_shift))) - 2.0
    wm_temp = (1.0 / (1.0 + np.exp(-wm_temp))) * 20.0
    gate_bias = 1.0 / (1.0 + np.exp(-gate_bias))

    age_val = float(age[0])
    older = 1 if age_val >= 45 else 0

    nll = 0.0
    eps = 1e-12
    nA = 3

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx].astype(int)
        block_actions = actions[idx].astype(int)
        block_rewards = rewards[idx].astype(float)
        nS = int(set_sizes[idx][0])

        # Effective capacity in slots considering age
        K_eff = np.clip(K_base + age_K_shift * older, 1.0, 6.0)

        # RL values
        Q = np.zeros((nS, nA))

        # WM store: for each state, the stored action and its strength in [0,1]
        wm_action = -np.ones(nS, dtype=int)
        wm_strength = np.zeros(nS, dtype=float)

        # Interference-driven decay per trial: if nS > K_eff, greater decay
        # Convert to per-trial retention factor r in [0,1]
        overload = max(0.0, float(nS) - K_eff)
        # Map overload to retention factor: more overload => lower retention
        # r = 1 / (1 + overload), so 0 (heavy decay) to 1 (no decay)
        retention = 1.0 / (1.0 + overload)

        for t in range(len(block_states)):
            s = block_states[t]
            a = block_actions[t]
            rwd = block_rewards[t]

            # Construct RL policy
            q = Q[s, :] - np.max(Q[s, :])
            pi_rl = np.exp(beta * q)
            pi_rl = pi_rl / (np.sum(pi_rl) + eps)

            # Decay WM strength due to interference every trial
            wm_strength *= retention

            # WM policy for this state
            if wm_action[s] >= 0 and wm_strength[s] > 0:
                logits = np.zeros(nA)
                logits[:] = 0.0
                logits[wm_action[s]] = 1.0
                logits = logits - np.max(logits)
                pi_wm = np.exp(wm_temp * logits)
                pi_wm = pi_wm / (np.sum(pi_wm) + eps)
                # Mixture weight depends on strength and gate bias
                w = np.clip(gate_bias * wm_strength[s], 0.0, 1.0)
            else:
                pi_wm = np.ones(nA) / nA
                w = 0.0

            pi = w * pi_wm + (1.0 - w) * pi_rl

            p = max(pi[a], eps)
            nll -= np.log(p)

            # RL update
            delta = rwd - Q[s, a]
            Q[s, a] += alpha * delta

            # WM update: if rewarded, store action with full strength; if not, slightly weaken
            if rwd > 0.0:
                wm_action[s] = a
                wm_strength[s] = 1.0
            else:
                # Non-reward provides weak evidence against current WM hypothesis
                wm_strength[s] *= 0.8

    return nll


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Uncertainty-sensitive Bayesian action values with age-modulated prior and load-modulated exploration.

    Mechanism:
    - For each state-action, maintain Beta-Bernoulli counts of successes/failures from feedback.
      Action value is the posterior mean.
    - Add an uncertainty bonus proportional to count^{-exponent}; larger when fewer samples.
    - Age increases effective prior strength (making choices more conservative/exploitative).
    - Set size reduces the impact of the uncertainty bonus (higher load -> less exploration).
    - Choices are made via softmax over (mean + exploration bonus). Includes a small lapse.

    Parameters
    ----------
    states : array-like of int
        State indices on each trial (0..nS-1 within block).
    actions : array-like of int
        Chosen actions on each trial (0..nA-1).
    rewards : array-like of int
        Binary rewards (0 or 1) observed each trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size for the block of each trial.
    age : array-like of float
        Participant age; age[0] determines age group (older >=45).
    model_parameters : tuple/list of floats
        (beta, prior_base, age_prior_shift, explore_exponent, lapse)
        - beta: inverse temperature for softmax (scaled internally)
        - prior_base: base prior strength per action (squashed to 0.1..5.0)
        - age_prior_shift: additional prior strength for older group (squashed to 0..5.0)
        - explore_exponent: exponent for uncertainty bonus (squashed to 0.1..1.5)
        - lapse: lapse rate mixing with uniform policy (squashed to 0..0.2)

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    beta, prior_base, age_prior_shift, explore_exponent, lapse = model_parameters

    beta = (1.0 / (1.0 + np.exp(-beta))) * 12.0
    prior_base = 0.1 + 4.9 * (1.0 / (1.0 + np.exp(-prior_base)))
    age_prior_shift = 5.0 * (1.0 / (1.0 + np.exp(-age_prior_shift)))
    explore_exponent = 0.1 + 1.4 * (1.0 / (1.0 + np.exp(-explore_exponent)))
    lapse = (1.0 / (1.0 + np.exp(-lapse))) * 0.2

    age_val = float(age[0])
    older = 1 if age_val >= 45 else 0

    nll = 0.0
    eps = 1e-12
    nA = 3

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx].astype(int)
        block_actions = actions[idx].astype(int)
        block_rewards = rewards[idx].astype(float)
        nS = int(set_sizes[idx][0])

        # Effective prior per action: older -> stronger prior (more conservative)
        prior = prior_base + age_prior_shift * older
        # Initialize Beta prior (alpha0, beta0)
        # Use symmetric prior: successes=prior/2, failures=prior/2
        S = np.ones((nS, nA)) * (prior / 2.0)
        F = np.ones((nS, nA)) * (prior / 2.0)

        # Load-dependent scaling of exploration
        load_scale = 3.0 / float(nS)  # higher set size -> smaller scale

        for t in range(len(block_states)):
            s = block_states[t]
            a = block_actions[t]
            r = block_rewards[t]

            # Posterior mean and counts
            mean = S[s, :] / (S[s, :] + F[s, :] + eps)
            n = (S[s, :] + F[s, :])

            # Uncertainty bonus: n^{-exponent}, scaled by load factor
            bonus = load_scale * (1.0 / (np.maximum(n, 1.0))) ** explore_exponent

            vals = mean + bonus
            vals = vals - np.max(vals)
            pi = np.exp(beta * vals)
            pi = pi / (np.sum(pi) + eps)

            # Lapse mixing
            pi = (1.0 - lapse) * pi + lapse * (1.0 / nA)

            p = max(pi[a], eps)
            nll -= np.log(p)

            # Update counts with observed reward
            S[s, a] += r
            F[s, a] += (1.0 - r)

    return nll