def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Bayesian arbitration with age-dependent WM capacity and uncertainty-adaptive temperature.

    Mechanism:
    - For each state-action, we maintain trial counts and reward counts (Bayesian tracking of Bernoulli outcomes).
      The expected value Q(s,a) is the posterior mean: (R_counts + 0.5) / (T_counts + 1.0).
    - Decision policy is a mixture of:
        (a) a working-memory (WM) policy that outputs a deterministic choice if a recent rewarded binding exists,
        (b) a softmax over Q-values. The softmax inverse temperature decreases with posterior uncertainty
            (higher uncertainty => lower beta).
    - WM retrieval probability depends on set size and age-dependent capacity:
        p_retrieve = min(1, capacity / set_size), where capacity differs for younger vs. older adults.
    - Lapses increase with load; a small base lapse is retained even at low load.

    Parameters (model_parameters):
    - beta0: base inverse temperature for softmax (>0)
    - kappa0: prior pseudo-count added to each state-action (>=0), stabilizes early estimates
    - cap_y: WM capacity for younger participants (>0)
    - cap_o: WM capacity for older participants (>0)
    - lapse0: base lapse rate in [0,1), scaled by load

    Inputs:
    - states: array of state indices per trial (ints in [0, set_size-1])
    - actions: array of chosen actions per trial (ints in [0,2]); invalid values handled as lapses
    - rewards: array of feedback per trial (1 or 0; any >0 treated as 1, <=0 as 0)
    - blocks: array indicating block membership per trial
    - set_sizes: array with the set size for each trial (3 or 6)
    - age: array-like with a single value; age[0] is participant age in years
    - model_parameters: list/tuple [beta0, kappa0, cap_y, cap_o, lapse0]

    Returns:
    - Negative log-likelihood of observed choices under the model.
    """
    beta0, kappa0, cap_y, cap_o, lapse0 = model_parameters
    is_older = 1.0 if age[0] >= 45 else 0.0
    nA = 3
    eps = 1e-12
    nll = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        b_states = states[mask].astype(int)
        b_actions = actions[mask].astype(int)
        b_rewards = rewards[mask]
        b_set_sizes = set_sizes[mask].astype(int)

        nS = int(b_set_sizes[0])
        # Posterior counts
        T_counts = np.zeros((nS, nA)) + kappa0  # total count pseudo prior
        R_counts = np.zeros((nS, nA)) + 0.5     # reward pseudo prior

        # WM binding: last rewarded action per state; -1 means none
        wm_bind = -np.ones(nS, dtype=int)

        # Age-dependent capacity
        capacity = cap_o if is_older > 0.5 else cap_y

        for t in range(len(b_states)):
            s = b_states[t]
            a = b_actions[t]
            r = 1.0 if b_rewards[t] > 0 else 0.0
            load = float(b_set_sizes[t])

            # Compute Q from Bayesian posterior means
            Q_s = (R_counts[s, :] / (T_counts[s, :] + 1e-9))

            # Uncertainty-adaptive temperature: higher concentration -> higher beta
            conc = np.sum(T_counts[s, :])  # includes prior
            # Map concentration to a scaling factor in (0,1], saturating as evidence accumulates
            uncert = 1.0 / (1.0 + conc)  # small when conc large
            beta_eff = beta0 * (1.0 - 0.7 * uncert)  # reduce temperature under high uncertainty

            # RL softmax policy
            q_center = Q_s - np.max(Q_s)
            p_rl_unnorm = np.exp(beta_eff * q_center)
            p_rl = p_rl_unnorm / (np.sum(p_rl_unnorm) + eps)

            # WM policy: deterministic if a rewarded binding exists, else uniform
            if wm_bind[s] >= 0:
                p_wm = np.zeros(nA)
                p_wm[int(wm_bind[s])] = 1.0
                wm_present = 1.0
            else:
                p_wm = np.ones(nA) / nA
                wm_present = 0.0

            # Retrieval probability depends on capacity and load
            p_retrieve = min(1.0, capacity / max(1.0, load))
            w_eff = p_retrieve * wm_present

            # Lapse increases with load (3->6 roughly doubles). Keep it modest.
            load_scale = 0.3 + 0.7 * ((load - 3.0) / 3.0 if load > 3.0 else 0.0)
            lapse_t = np.clip(lapse0 * (1.0 + 0.8 * load_scale), 0.0, 0.49)

            p_mix = w_eff * p_wm + (1.0 - w_eff) * p_rl
            p = (1.0 - lapse_t) * p_mix + lapse_t * (np.ones(nA) / nA)

            # Likelihood and updates
            if a < 0 or a >= nA:
                # Treat invalid actions as pure lapse/uniform
                p_a = 1.0 / nA
                nll -= np.log(max(p_a, eps))
                continue

            p_a = p[a]
            nll -= np.log(max(p_a, eps))

            # Update Bayesian counts
            T_counts[s, a] += 1.0
            R_counts[s, a] += r

            # Update WM binding only on rewarded trials
            if r > 0.5:
                wm_bind[s] = a

    return nll


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Actor-Critic with age- and load-modulated choice perseveration.

    Mechanism:
    - Actor stores state-action preferences P(s,a); Critic stores state values V(s).
    - Choice probabilities are softmax over beta*P(s,a) plus a perseveration bonus
      for the action chosen last time in the same state.
    - Perseveration strength increases with set size (higher load) and with age group (older).
    - Updates use TD error: delta = r - V(s). Actor is updated via REINFORCE with baseline (softmax policy gradient);
      Critic via a simple value update.

    Parameters (model_parameters):
    - alpha_a: learning rate for actor updates (0..1)
    - alpha_c: learning rate for critic updates (0..1)
    - beta: inverse temperature for action selection (>0)
    - persev0: baseline perseveration strength (>=0)
    - age_shift: additive increase in perseveration if older (can be negative or positive)

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age: arrays as described in the task.
    - model_parameters: [alpha_a, alpha_c, beta, persev0, age_shift]

    Returns:
    - Negative log-likelihood of observed choices.
    """
    alpha_a, alpha_c, beta, persev0, age_shift = model_parameters
    is_older = 1.0 if age[0] >= 45 else 0.0
    nA = 3
    eps = 1e-12
    nll = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        b_states = states[mask].astype(int)
        b_actions = actions[mask].astype(int)
        b_rewards = rewards[mask]
        b_set_sizes = set_sizes[mask].astype(int)

        nS = int(b_set_sizes[0])
        P = np.zeros((nS, nA))  # actor preferences
        V = np.zeros(nS)        # critic values
        last_choice = -np.ones(nS, dtype=int)

        for t in range(len(b_states)):
            s = b_states[t]
            a = b_actions[t]
            r = 1.0 if b_rewards[t] > 0 else 0.0
            load = float(b_set_sizes[t])

            # Perseveration bonus, modulated by load and age
            load_factor = 1.0 + 0.8 * ((load - 3.0) / 3.0)  # 1 at low load, up to 1.8 at high
            k_persev = (persev0 + age_shift * is_older) * load_factor
            stick = np.zeros(nA)
            if last_choice[s] >= 0:
                stick[int(last_choice[s])] = k_persev

            # Softmax over preferences + stickiness
            logits = beta * P[s, :] + stick
            logits = logits - np.max(logits)
            pi = np.exp(logits)
            pi = pi / (np.sum(pi) + eps)

            # Likelihood
            if a < 0 or a >= nA:
                p_a = 1.0 / nA
                nll -= np.log(max(p_a, eps))
                # Do not update actor/critic on invalid actions
                continue
            p_a = pi[a]
            nll -= np.log(max(p_a, eps))

            # TD error and updates
            delta = r - V[s]
            V[s] += alpha_c * delta

            # Policy gradient update for actor
            # Increase preference for chosen action, decrease others proportional to pi
            P[s, a] += alpha_a * delta * (1.0 - pi[a])
            for a_other in range(nA):
                if a_other != a:
                    P[s, a_other] -= alpha_a * delta * pi[a_other]

            # Update last choice trace
            last_choice[s] = a

    return nll


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Q-learning with asymmetric prediction errors via a skew parameter and WM decay modulated by age-load.

    Mechanism:
    - RL: standard Q-learning per state-action. Asymmetry in learning from positive vs negative prediction errors
      is controlled by a single skew parameter psi that scales a shared base alpha:
        alpha_pos = alpha * (1 + psi), alpha_neg = alpha * (1 - psi), both clipped to [0,1].
    - WM: stores the last rewarded action per state with a dynamic strength w_s in [0,1].
      On each trial, w_s decays multiplicatively with a rate that increases with set size and age group.
      When a reward is observed, WM binding is refreshed (action is stored) and w_s is set to 1.
    - Policy: mixture between deterministic WM choice (if a binding exists) weighted by current w_s,
      and RL softmax weighted by (1 - w_s).

    Parameters (model_parameters):
    - alpha: base learning rate for Q-learning (0..1)
    - beta: inverse temperature for RL softmax (>0)
    - psi: skew for PE asymmetry (can be negative/positive). Maps to alpha_pos/alpha_neg as above.
    - decay0: base WM decay logit (real), converted via sigmoid to (0,1) decay per trial
    - slope_age_load: increases decay with age and load (real)

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age: arrays as specified.
    - model_parameters: [alpha, beta, psi, decay0, slope_age_load]

    Returns:
    - Negative log-likelihood of observed choices.
    """
    alpha, beta, psi, decay0, slope_age_load = model_parameters
    is_older = 1.0 if age[0] >= 45 else 0.0
    nA = 3
    eps = 1e-12
    nll = 0.0

    # Map psi to asymmetric learning rates, clipped to [0,1]
    alpha_pos = np.clip(alpha * (1.0 + psi), 0.0, 1.0)
    alpha_neg = np.clip(alpha * (1.0 - psi), 0.0, 1.0)

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    for b in np.unique(blocks):
        mask = (blocks == b)
        b_states = states[mask].astype(int)
        b_actions = actions[mask].astype(int)
        b_rewards = rewards[mask]
        b_set_sizes = set_sizes[mask].astype(int)

        nS = int(b_set_sizes[0])

        Q = np.zeros((nS, nA))
        wm_bind = -np.ones(nS, dtype=int)
        wm_strength = np.zeros(nS)  # starts at 0 (no confident binding)

        for t in range(len(b_states)):
            s = b_states[t]
            a = b_actions[t]
            r = 1.0 if b_rewards[t] > 0 else 0.0
            load = float(b_set_sizes[t])

            # Compute load-age adjusted decay per trial
            # decay_rate in (0,1): higher means faster decay
            decay_logit = decay0 + slope_age_load * (is_older + (load - 3.0) / 3.0)
            decay_rate = sigmoid(decay_logit)

            # Decay WM strength
            wm_strength[s] = wm_strength[s] * (1.0 - decay_rate)

            # RL softmax policy
            q_s = Q[s, :] - np.max(Q[s, :])
            p_rl = np.exp(beta * q_s)
            p_rl = p_rl / (np.sum(p_rl) + eps)

            # WM policy
            if wm_bind[s] >= 0:
                p_wm = np.zeros(nA)
                p_wm[int(wm_bind[s])] = 1.0
            else:
                p_wm = np.ones(nA) / nA

            w_eff = np.clip(wm_strength[s], 0.0, 1.0)
            p = w_eff * p_wm + (1.0 - w_eff) * p_rl

            # Likelihood
            if a < 0 or a >= nA:
                p_a = 1.0 / nA
                nll -= np.log(max(p_a, eps))
                continue
            p_a = p[a]
            nll -= np.log(max(p_a, eps))

            # RL update
            pe = r - Q[s, a]
            lr = alpha_pos if pe >= 0 else alpha_neg
            Q[s, a] += lr * pe

            # WM update on reward
            if r > 0.5:
                wm_bind[s] = a
                wm_strength[s] = 1.0

    return nll