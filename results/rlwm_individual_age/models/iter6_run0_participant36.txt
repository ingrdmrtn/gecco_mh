def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Model 1: RL with age- and set-size-modulated action persistence and a small lapse.
    
    Mechanism:
      - Standard Q-learning within blocks.
      - Adds a state-specific action persistence bias that favors repeating the most recent action chosen
        in that state. This bias decays across revisits but is stronger for smaller set sizes and is
        reduced (or enhanced) in older adults via an additive age shift.
      - A small lapse mixes the policy with a uniform distribution to capture occasional random choices.
    
    Parameters (model_parameters)
    --------------------------------
    alpha : float
        Learning rate for Q-learning in [0,1].
    beta : float
        Inverse temperature (>0) controlling exploitation; internally scaled for stability.
    persist_gain : float
        Baseline strength of the action persistence bias (additive to action values in softmax space).
    ss_persist_slope : float
        How much persistence decreases as set size increases (relative to 3). Positive -> weaker persistence for set size 6.
    age_persist_shift : float
        Additive shift to persistence for older adults (>=45). Positive -> stronger persistence in older adults.
    lapse_min : float
        Minimum lapse probability mixed with the policy (in [0,0.2] typically). Ensures non-zero probabilities.
    
    Inputs
    ------
    states : array-like (T,)
        State index per trial (0..nS-1 within a block).
    actions : array-like (T,)
        Chosen action per trial (0..2).
    rewards : array-like (T,)
        Binary rewards (0/1).
    blocks : array-like (T,)
        Block index per trial.
    set_sizes : array-like (T,)
        Set size for each trial's block (3 or 6).
    age : array-like or scalar
        Participant age; older group defined as age >= 45.
    
    Returns
    -------
    nll : float
        Negative log-likelihood of the observed action sequence.
    """
    nA = 3
    eps = 1e-12

    alpha, beta, persist_gain, ss_persist_slope, age_persist_shift, lapse_min = model_parameters
    beta = 5.0 * max(beta, eps)

    # Determine age group
    age_val = age[0] if hasattr(age, "__len__") and not np.isscalar(age) else age
    is_older = 1.0 if age_val >= 45 else 0.0

    total_logp = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx].astype(int)
        block_actions = actions[idx].astype(int)
        block_rewards = rewards[idx].astype(float)
        nS = int(set_sizes[idx][0])

        # RL values and persistence traces
        Q = np.zeros((nS, nA))
        # For each state, keep a "last chosen action" one-hot trace that decays when revisited
        P = np.zeros((nS, nA))  # persistence trace

        # Compute per-block persistence magnitude given set size and age
        ss_factor = (nS - 3) / 3.0  # 0 for 3, 1 for 6
        persist_mag = persist_gain - ss_persist_slope * ss_factor + age_persist_shift * is_older

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Decision values: Q plus persistence bias
            v = Q[s, :].copy() + persist_mag * P[s, :]

            # Softmax with lapse
            v_center = v - np.max(v)
            pi = np.exp(beta * v_center)
            pi = pi / (np.sum(pi) + eps)

            # Mix with uniform via lapse
            lapse = np.clip(lapse_min, 0.0, 0.5)
            p = (1.0 - lapse) * pi + lapse * (1.0 / nA)
            p = np.clip(p, eps, 1.0)
            p /= np.sum(p)

            total_logp += np.log(p[a])

            # Q-learning update
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # Update persistence trace: decay competitors, set chosen to 1
            # Decay rate slightly stronger for larger set sizes and in older adults
            decay_rate = np.clip(0.2 + 0.2 * ss_factor + 0.1 * is_older, 0.0, 1.0)
            P[s, :] *= (1.0 - decay_rate)
            P[s, a] = 1.0

    return -float(total_logp)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Model 2: RL + confidence-gated working memory (WM) with age and set-size modulation.
    
    Mechanism:
      - Model-free Q-learning runs in parallel.
      - A simple WM store tracks, for each state, the currently "believed-best" action A_hat[s]
        and a confidence c[s] in that mapping. Confidence increases with rewarded confirmations
        and decreases with errors and over time (decay).
      - Decision policy mixes WM and RL based on a gating probability that increases with confidence,
        but decreases with set size and is shifted by age group.
    
    Parameters (model_parameters)
    --------------------------------
    alpha : float
        Q-learning rate in [0,1].
    beta : float
        Inverse temperature (>0) for both RL and WM softmax; internally scaled.
    gate_bias : float
        Baseline intercept for WM gating (log-odds space).
    age_gate_shift : float
        Additive shift to WM gating if older (>=45). Positive -> more WM reliance in older adults.
    ss_gate_slope : float
        Decrease in WM gating as set size increases (relative to 3). Positive -> less WM gating for set size 6.
    conf_decay : float
        Per-revisit decay of confidence in [0,1]; higher -> faster decay, stronger at larger set sizes and older adults.
    
    Inputs
    ------
    states : array-like (T,)
        State index per trial (0..nS-1 within a block).
    actions : array-like (T,)
        Chosen action per trial (0..2).
    rewards : array-like (T,)
        Binary rewards (0/1).
    blocks : array-like (T,)
        Block index per trial.
    set_sizes : array-like (T,)
        Set size for each trial's block (3 or 6).
    age : array-like or scalar
        Participant age; older group defined as age >= 45.
    
    Returns
    -------
    nll : float
        Negative log-likelihood of the observed action sequence.
    """
    nA = 3
    eps = 1e-12

    alpha, beta, gate_bias, age_gate_shift, ss_gate_slope, conf_decay = model_parameters
    beta = 5.0 * max(beta, eps)

    age_val = age[0] if hasattr(age, "__len__") and not np.isscalar(age) else age
    is_older = 1.0 if age_val >= 45 else 0.0

    total_logp = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx].astype(int)
        block_actions = actions[idx].astype(int)
        block_rewards = rewards[idx].astype(float)
        nS = int(set_sizes[idx][0])

        # RL values
        Q = np.zeros((nS, nA))
        # WM store: hypothesized best action and confidence per state
        A_hat = -np.ones(nS, dtype=int)
        C = np.zeros(nS, dtype=float)

        ss_factor = (nS - 3) / 3.0  # 0 for 3, 1 for 6

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Confidence decay upon revisit, stronger with larger set size and age
            decay = np.clip(conf_decay * (1.0 + 0.5 * ss_factor) * (1.0 + 0.25 * is_older), 0.0, 1.0)
            C[s] *= (1.0 - decay)

            # RL policy
            q_s = Q[s, :].copy()
            q_s -= np.max(q_s)
            p_rl = np.exp(beta * q_s)
            p_rl = p_rl / (np.sum(p_rl) + eps)

            # WM policy: if we have a hypothesis A_hat[s], put mass on it according to confidence
            if A_hat[s] >= 0:
                wm_pref = np.zeros(nA)
                wm_pref[A_hat[s]] = 1.0
                wm_logits = (2.0 * C[s] - 1.0) * wm_pref  # if C near 1 -> stronger one-hot
                wm_logits -= np.max(wm_logits)
                p_wm = np.exp(beta * wm_logits)
                p_wm = p_wm / (np.sum(p_wm) + eps)
            else:
                p_wm = np.ones(nA) / nA

            # Gate WM vs RL using a logistic of confidence with age and set-size modulation
            logit_gate = gate_bias - ss_gate_slope * ss_factor + age_gate_shift * is_older + 2.0 * (C[s] - 0.5)
            rho = 1.0 / (1.0 + np.exp(-logit_gate))  # in (0,1)

            p = rho * p_wm + (1.0 - rho) * p_rl
            p = np.clip(p, eps, 1.0)
            p /= np.sum(p)

            total_logp += np.log(p[a])

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # WM update: if rewarded, set hypothesis to chosen action and increase confidence;
            # if not rewarded and hypothesis matches chosen action, reduce confidence.
            if r > 0.5:
                A_hat[s] = a
                # Increase confidence, saturating at 1; faster consolidation for smaller sets, slower for older
                inc = np.clip(0.5 * (1.0 - 0.3 * ss_factor) * (1.0 - 0.2 * is_older), 0.0, 1.0)
                C[s] = np.clip(C[s] + inc, 0.0, 1.0)
            else:
                if A_hat[s] == a:
                    dec = np.clip(0.4 * (1.0 + 0.5 * ss_factor) * (1.0 + 0.2 * is_older), 0.0, 1.0)
                    C[s] = np.clip(C[s] - dec, 0.0, 1.0)
                else:
                    # Wrong but different from WM hypothesis -> mild doubt
                    C[s] = np.clip(C[s] - 0.1 * (1.0 + 0.5 * ss_factor), 0.0, 1.0)

    return -float(total_logp)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Model 3: RL with count-based novelty bonus and age/set-size-modulated exploration and learning.
    
    Mechanism:
      - Standard Q-learning augmented with an intrinsic novelty bonus added to action values.
      - Novelty is computed from visit counts N[s,a]; rarer actions get a larger bonus.
      - The strength of the novelty bonus decreases with set size and can be shifted by age group.
      - Learning rate is adaptively scaled by uncertainty (higher when counts are low), with age affecting
        this scaling.
      - A small lapse ensures non-zero probabilities.
    
    Parameters (model_parameters)
    --------------------------------
    alpha_base : float
        Baseline learning rate in [0,1].
    beta : float
        Inverse temperature (>0); internally scaled for stability.
    nov_gain : float
        Baseline gain for the novelty bonus; added to action values before softmax.
    ss_nov_slope : float
        Reduction in novelty gain as set size increases (relative to 3). Positive -> less novelty weight for set size 6.
    age_nov_shift : float
        Additive shift on novelty gain if older (>=45). Positive -> more novelty exploration in older adults.
    lapse : float
        Lapse probability to mix with uniform (in [0,0.3]).
    
    Inputs
    ------
    states : array-like (T,)
        State index per trial (0..nS-1 within a block).
    actions : array-like (T,)
        Chosen action per trial (0..2).
    rewards : array-like (T,)
        Binary rewards (0/1).
    blocks : array-like (T,)
        Block index per trial.
    set_sizes : array-like (T,)
        Set size for each trial's block (3 or 6).
    age : array-like or scalar
        Participant age; older group defined as age >= 45.
    
    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.
    """
    nA = 3
    eps = 1e-12

    alpha_base, beta, nov_gain, ss_nov_slope, age_nov_shift, lapse = model_parameters
    beta = 5.0 * max(beta, eps)

    age_val = age[0] if hasattr(age, "__len__") and not np.isscalar(age) else age
    is_older = 1.0 if age_val >= 45 else 0.0

    total_logp = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx].astype(int)
        block_actions = actions[idx].astype(int)
        block_rewards = rewards[idx].astype(float)
        nS = int(set_sizes[idx][0])

        Q = np.zeros((nS, nA))
        N = np.zeros((nS, nA))  # visit counts

        ss_factor = (nS - 3) / 3.0  # 0 for 3, 1 for 6

        # Effective novelty gain for this block given set size and age
        nov = nov_gain - ss_nov_slope * ss_factor + age_nov_shift * is_older

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Count-based novelty: larger when count is small
            inv_sqrt_counts = 1.0 / np.sqrt(np.maximum(1.0, N[s, :]) )
            bonus = nov * inv_sqrt_counts

            # Decision values
            v = Q[s, :].copy() + bonus
            v -= np.max(v)
            pi = np.exp(beta * v)
            pi = pi / (np.sum(pi) + eps)

            # Lapse mixing
            lapse_p = np.clip(lapse, 0.0, 0.5)
            p = (1.0 - lapse_p) * pi + lapse_p * (1.0 / nA)
            p = np.clip(p, eps, 1.0)
            p /= np.sum(p)

            total_logp += np.log(p[a])

            # Adaptive learning rate: higher when N[s,a] is small, reduced in older adults
            unc = 1.0 / np.sqrt(np.maximum(1.0, N[s, a] + 1.0))
            alpha = np.clip(alpha_base * (1.0 + 0.5 * unc) * (1.0 - 0.2 * is_older + 0.1 * ss_factor), 0.0, 1.0)

            # Q-learning update
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # Update counts
            N[s, a] += 1.0

    return -float(total_logp)