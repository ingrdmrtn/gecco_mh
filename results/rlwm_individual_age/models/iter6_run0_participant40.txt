def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited episodic working memory (WM) retrieval with age-modulated gating.

    The model blends incremental RL with a capacity-limited, one-shot WM that stores the last
    rewarded action for each state within a block. Retrieval probability depends on effective
    capacity relative to set size and is reduced in older adults.

    Parameters
    ----------
    states : np.ndarray of int
        State index at each trial (0..nS-1 within a block).
    actions : np.ndarray of int
        Chosen action at each trial (0..2). If action < 0 it is treated as a lapse (uniform likelihood).
    rewards : np.ndarray of float
        Feedback at each trial. Values in {0,1}; values < 0 indicate invalid/missing feedback (trial ignored for updating).
    blocks : np.ndarray of int
        Block id per trial.
    set_sizes : np.ndarray of int
        Set size per trial (3 or 6), used to scale retrieval probability.
    age : np.ndarray of float
        Array containing participant age (use age[0]); age>=45 considered older group.
    model_parameters : sequence of float
        [alpha, beta, gate_age, cap_ratio, epsilon]
        - alpha: RL learning rate (0..1).
        - beta: inverse temperature for RL softmax.
        - gate_age: proportional reduction of effective WM capacity in older adults (0..1).
        - cap_ratio: base WM capacity in units of 3 items; effective capacity K = 3*cap_ratio (clipped to [1,6]).
        - epsilon: lapse probability (uniform random action with prob epsilon).

    Returns
    -------
    float
        Negative log-likelihood of observed actions under the model.
    """
    alpha, beta, gate_age, cap_ratio, epsilon = model_parameters
    age_val = float(age[0])
    is_older = 1.0 if age_val >= 45.0 else 0.0

    nA = 3
    total_logp = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        b_states = states[idx]
        b_actions = actions[idx]
        b_rewards = rewards[idx]
        b_set_sizes = set_sizes[idx]
        nS = int(b_set_sizes[0])

        # RL values
        Q = (1.0 / nA) * np.ones((nS, nA))

        # Episodic WM store: -1 means empty; otherwise store action index with last reward
        wm_store = -1 * np.ones(nS, dtype=int)

        for t in range(len(b_states)):
            s = int(b_states[t])
            a = int(b_actions[t])
            r = float(b_rewards[t])
            ss = int(b_set_sizes[t])

            # RL policy
            q_s = Q[s, :]
            q_center = q_s - np.max(q_s)
            p_rl = np.exp(beta * q_center)
            p_rl = p_rl / np.sum(p_rl)

            # WM retrieval probability: effective capacity scaled by age and load
            K_base = np.clip(3.0 * cap_ratio, 1.0, 6.0)
            # Older adults have reduced effective capacity by gate_age proportion
            K_eff = K_base * (1.0 - gate_age * is_older)
            # Retrieval probability increases with capacity-to-load ratio
            p_retrieve = np.clip(K_eff / float(ss), 0.0, 1.0)

            # WM policy conditional on successful retrieval
            if wm_store[s] >= 0 and wm_store[s] < nA:
                p_wm = np.zeros(nA)
                p_wm[wm_store[s]] = 1.0
            else:
                # No memory for this state -> acts like a uniform fallback
                p_wm = (1.0 / nA) * np.ones(nA)

            # Mixture: use retrieval probability as arbitration weight
            p_mix = p_retrieve * p_wm + (1.0 - p_retrieve) * p_rl
            p = (1.0 - epsilon) * p_mix + epsilon * (1.0 / nA)

            if a >= 0 and a < nA:
                pa = float(np.clip(p[a], 1e-12, 1.0))
                total_logp += np.log(pa)
            else:
                # Lapse/invalid action: assign uniform probability, skip update
                total_logp += np.log(1.0 / nA)
                continue

            # Learning updates only if reward is valid
            if r >= 0.0:
                # RL update
                delta = r - Q[s, a]
                Q[s, a] += alpha * delta

                # WM update: store action only when it is rewarded (one-shot encoding)
                if r > 0.0:
                    wm_store[s] = a

    return -float(total_logp)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Decaying Dirichlet (Bayesian counts) with age- and load-modulated perseveration.

    The model maintains per-state Dirichlet pseudo-counts over actions. Counts decay toward
    the symmetric prior on each visit, and grow when rewards are obtained. Choice is softmax
    over expected reward (mean of Dirichlet), combined with a flexible perseveration bias
    that is stronger in older adults and attenuated for larger set sizes.

    Parameters
    ----------
    states : np.ndarray of int
        State index at each trial (0..nS-1 within a block).
    actions : np.ndarray of int
        Chosen action at each trial (0..2). If action < 0 it is treated as a lapse (uniform likelihood).
    rewards : np.ndarray of float
        Feedback at each trial. Values in {0,1}; values < 0 indicate invalid/missing feedback (trial ignored for updating).
    blocks : np.ndarray of int
        Block id per trial.
    set_sizes : np.ndarray of int
        Set size per trial (3 or 6), used to scale perseveration.
    age : np.ndarray of float
        Array containing participant age (use age[0]); age>=45 considered older group.
    model_parameters : sequence of float
        [alpha, beta, prior_strength, pers_age, epsilon]
        - alpha: decay/update step size (0..1). Larger values -> faster decay and faster reward incorporation.
        - beta: inverse temperature for softmax over expected rewards.
        - prior_strength: symmetric prior concentration per action (>0).
        - pers_age: baseline perseveration gain that is amplified in older adults.
        - epsilon: lapse probability (uniform random action with prob epsilon).

    Returns
    -------
    float
        Negative log-likelihood of observed actions under the model.
    """
    alpha, beta, prior_strength, pers_age, epsilon = model_parameters
    age_val = float(age[0])
    is_older = 1.0 if age_val >= 45.0 else 0.0

    nA = 3
    total_logp = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        b_states = states[idx]
        b_actions = actions[idx]
        b_rewards = rewards[idx]
        b_set_sizes = set_sizes[idx]
        nS = int(b_set_sizes[0])

        # Dirichlet counts initialized to the symmetric prior
        prior = np.full((nS, nA), prior_strength, dtype=float)
        C = prior.copy()

        # Track last action per state for perseveration
        last_a = -np.ones(nS, dtype=int)

        for t in range(len(b_states)):
            s = int(b_states[t])
            a = int(b_actions[t])
            r = float(b_rewards[t])
            ss = int(b_set_sizes[t])

            # Expected reward under Dirichlet: mean of categorical
            mean_p = C[s, :] / np.sum(C[s, :])

            # Perseveration gain: stronger for older, weaker under higher load
            load_atten = 1.0 / (1.0 + max(0, ss - 3))  # 1 for 3, 1/2 for 6
            kappa = pers_age * (1.0 + 0.5 * is_older) * load_atten

            bias = np.zeros(nA)
            if last_a[s] >= 0 and last_a[s] < nA:
                bias[last_a[s]] = kappa

            # Softmax over expected reward + perseveration bias
            logits = beta * mean_p + bias
            logits = logits - np.max(logits)
            p = np.exp(logits)
            p = p / np.sum(p)
            p = (1.0 - epsilon) * p + epsilon * (1.0 / nA)

            if a >= 0 and a < nA:
                pa = float(np.clip(p[a], 1e-12, 1.0))
                total_logp += np.log(pa)
            else:
                total_logp += np.log(1.0 / nA)
                # do not update on invalid actions
                continue

            # Update counts if reward is valid
            if r >= 0.0:
                # Decay towards prior on each visit to the state
                C[s, :] = (1.0 - alpha) * C[s, :] + alpha * prior[s, :]
                # Reward contingent increment
                if r > 0.0:
                    C[s, a] += alpha

                last_a[s] = a

    return -float(total_logp)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with error-driven adaptive learning rate and age-by-load stabilization.

    The model uses a base learning rate modulated online by unsigned prediction error,
    with additional modulation by age and set size. Older adults exhibit greater stabilization
    (decay toward uniform values) under higher load, capturing reduced flexibility.

    Parameters
    ----------
    states : np.ndarray of int
        State index at each trial (0..nS-1 within a block).
    actions : np.ndarray of int
        Chosen action at each trial (0..2). If action < 0 it is treated as a lapse (uniform likelihood).
    rewards : np.ndarray of float
        Feedback at each trial. Values in {0,1}; values < 0 indicate invalid/missing feedback (trial ignored for updating).
    blocks : np.ndarray of int
        Block id per trial.
    set_sizes : np.ndarray of int
        Set size per trial (3 or 6), used to scale learning-rate modulation and stabilization.
    age : np.ndarray of float
        Array containing participant age (use age[0]); age>=45 considered older group.
    model_parameters : sequence of float
        [alpha_base, beta, att_age, load_slope, epsilon]
        - alpha_base: baseline RL learning rate (0..1).
        - beta: inverse temperature for softmax.
        - att_age: attentional gain that increases (older) or slightly decreases (younger) learning-rate modulation.
        - load_slope: reduces effective learning-rate modulation as set size increases.
        - epsilon: lapse probability (uniform random action with prob epsilon).

    Returns
    -------
    float
        Negative log-likelihood of observed actions under the model.
    """
    alpha_base, beta, att_age, load_slope, epsilon = model_parameters
    age_val = float(age[0])
    is_older = 1.0 if age_val >= 45.0 else 0.0

    nA = 3
    total_logp = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        b_states = states[idx]
        b_actions = actions[idx]
        b_rewards = rewards[idx]
        b_set_sizes = set_sizes[idx]
        nS = int(b_set_sizes[0])

        Q = (1.0 / nA) * np.ones((nS, nA))

        for t in range(len(b_states)):
            s = int(b_states[t])
            a = int(b_actions[t])
            r = float(b_rewards[t])
            ss = int(b_set_sizes[t])

            # Policy from current Q
            q_s = Q[s, :]
            logits = beta * (q_s - np.max(q_s))
            p = np.exp(logits)
            p = p / np.sum(p)
            p = (1.0 - epsilon) * p + epsilon * (1.0 / nA)

            if a >= 0 and a < nA:
                pa = float(np.clip(p[a], 1e-12, 1.0))
                total_logp += np.log(pa)
            else:
                total_logp += np.log(1.0 / nA)
                continue

            # Learning with adaptive alpha (if reward valid)
            if r >= 0.0:
                delta = r - Q[s, a]
                # Age- and load-modulated attention term
                # Older: amplify; Younger: slight attenuation
                age_gain = (1.0 + att_age * is_older) * (1.0 - 0.25 * att_age * (1.0 - is_older))
                load_attn = 1.0 / (1.0 + load_slope * max(0, ss - 3))
                # Error-driven scaling: larger unsigned PE -> larger step
                # Map to (0,1) via a smooth squashing
                mod = age_gain * load_attn
                # combine base rate and PE sensitivity
                alpha_eff = alpha_base * (0.5 + 0.5 * np.clip(abs(delta), 0.0, 1.0)) * np.clip(mod, 0.0, 2.0)
                alpha_eff = float(np.clip(alpha_eff, 0.0, 1.0))

                # Update selected action value
                Q[s, a] += alpha_eff * delta

                # Age-by-load stabilization: decay towards uniform baseline under higher load in older adults
                if ss > 3 and is_older > 0.0:
                    decay = 0.2 * alpha_eff * (ss - 3) / 3.0
                    baseline = (1.0 / nA)
                    Q[s, :] += decay * (baseline - Q[s, :])

    return -float(total_logp)