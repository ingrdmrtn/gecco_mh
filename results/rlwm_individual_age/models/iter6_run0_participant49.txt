def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with uncertainty-driven exploration bonus, modulated by set size and age, plus lapse.

    Mechanism
    - Q-learning drives exploitation.
    - Action preferences include an exploration bonus inversely proportional to visit counts for (state, action).
      This encourages exploration of less-sampled actions; the magnitude of the bonus is modulated by set size (load)
      and by age group (older adults show attenuated use of uncertainty).
    - A small lapse probability mixes in a uniform policy; lapse increases with age and load via an affine transform.

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..nS-1 within a block).
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of float/int
        Reward feedback per trial (0 or 1).
    blocks : array-like of int
        Block IDs. Learning and counts reset at block boundaries.
    set_sizes : array-like of int
        Set size per trial (3 or 6). Constant within a block.
    age : array-like or scalar
        Participant age in years; older group defined as age >= 45.
    model_parameters : sequence of 6 floats
        alpha         : Q-learning rate (0..1)
        beta          : Inverse temperature for softmax (scaled by 10 internally)
        eta_base      : Base coefficient for uncertainty bonus
        age_eta_drop  : Reduction in exploration bonus for older group (>=45)
        load_eta_gain : Sensitivity of exploration bonus to low load (3 vs 6); applied to (3 - set_size)
        lapse_base    : Base lapse rate; transformed by age and load into an effective lapse

    Returns
    -------
    nll : float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta, eta_base, age_eta_drop, load_eta_gain, lapse_base = model_parameters
    beta = beta * 10.0

    # Extract age and determine age group
    try:
        age_val = float(age[0])
    except Exception:
        age_val = float(age)
    is_older = 1.0 if age_val >= 45.0 else 0.0

    nA = 3
    eps = 1e-12
    total_log_p = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = np.asarray(actions)[idx]
        block_rewards = np.asarray(rewards)[idx]
        block_states = np.asarray(states)[idx]
        block_set_sizes = np.asarray(set_sizes)[idx]
        nS = int(block_set_sizes[0])

        # Initialize Q-values and visit counts
        Q = (1.0 / nA) * np.ones((nS, nA))
        N = np.zeros((nS, nA))  # visit counts for uncertainty bonus

        # Precompute block-level modulation terms
        # Uncertainty bonus magnitude:
        #   eta_eff = eta_base + load_eta_gain * (3 - nS) - age_eta_drop * is_older
        # For set size 3: (3 - nS) = 0; for set size 6: (3 - nS) = -3
        eta_eff = eta_base + load_eta_gain * (3.0 - float(nS)) - age_eta_drop * is_older
        eta_eff = max(0.0, eta_eff)  # ensure non-negative bonus

        # Lapse increases with age and with load (higher set size)
        # Convert lapse_base into effective lapse per block:
        #   lapse_eff = sigmoid(logit(lapse_base) + 0.5*is_older + 0.2*(nS-3))
        # Use a stable transform to keep it in (0,1).
        def sigmoid(x):
            return 1.0 / (1.0 + np.exp(-x))

        # avoid edge cases: clip lapse_base
        base = min(max(lapse_base, 1e-6), 1.0 - 1e-6)
        logit_base = np.log(base) - np.log(1.0 - base)
        lapse_eff = sigmoid(logit_base + 0.5 * is_older + 0.2 * (float(nS) - 3.0))

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Uncertainty bonus: higher when action was less visited in this state
            # UCB-like: bonus = eta_eff / sqrt(N+1)
            bonus = eta_eff / np.sqrt(N[s, :] + 1.0)

            prefs = Q[s, :] + bonus
            prefs -= np.max(prefs)
            p_soft = np.exp(beta * prefs)
            p_soft = p_soft / (np.sum(p_soft) + eps)

            # Lapse mixture with uniform
            p = (1.0 - lapse_eff) * p_soft + lapse_eff * (1.0 / nA)
            p_a = p[a]
            total_log_p += np.log(max(p_a, eps))

            # Update Q and counts
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe
            N[s, a] += 1.0

    return -float(total_log_p)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Actor-Critic with a transient WM-like trace that boosts recently rewarded actions.
    WM trace decays with a rate controlled by set size (load) and age.

    Mechanism
    - Policy (actor) preferences A[s,a] updated by TD error with learning rate alpha_actor.
    - State value V[s] (critic) updated with alpha_critic.
    - A WM-like trace M[s,a] stores a transient boost set to reward r for the chosen action and decays over time.
      The decay rate increases with load and is faster for older adults.
    - Choice probability uses softmax over (A[s,a] + wm_boost * M[s,a]) with inverse temperature beta.

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..nS-1 within block).
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of float/int
        Reward per trial (0 or 1).
    blocks : array-like of int
        Block ID per trial; learning resets per block.
    set_sizes : array-like of int
        Set size per trial (3 or 6).
    age : array-like or scalar
        Participant age in years; older group (>=45) increases WM trace decay.
    model_parameters : sequence of 6 floats
        alpha_actor     : Learning rate for actor preferences
        alpha_critic    : Learning rate for critic value
        beta            : Inverse temperature for softmax (scaled by 10 internally)
        wm_boost        : Multiplicative gain for WM trace in the action preferences
        decay_base      : Base decay per trial for the WM trace (0..1)
        age_decay_gain  : Additional decay applied for older group; load further increases decay

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.
    """
    alpha_actor, alpha_critic, beta, wm_boost, decay_base, age_decay_gain = model_parameters
    beta = beta * 10.0

    # Resolve age to group flag
    try:
        age_val = float(age[0])
    except Exception:
        age_val = float(age)
    is_older = 1.0 if age_val >= 45.0 else 0.0

    nA = 3
    eps = 1e-12
    total_log_p = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = np.asarray(actions)[idx]
        block_rewards = np.asarray(rewards)[idx]
        block_states = np.asarray(states)[idx]
        block_set_sizes = np.asarray(set_sizes)[idx]
        nS = int(block_set_sizes[0])

        # Initialize actor preferences, critic values, and WM trace
        A = np.zeros((nS, nA))             # actor preferences
        V = np.zeros(nS)                   # critic baseline
        M = np.zeros((nS, nA))             # transient WM trace

        # Determine effective decay for this block; higher with load and for older adults
        # decay_eff = clip(decay_base + 0.15*(nS-3) + age_decay_gain*is_older, 0, 0.999)
        decay_eff = decay_base + 0.15 * (float(nS) - 3.0) + age_decay_gain * is_older
        decay_eff = min(max(decay_eff, 0.0), 0.999)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Softmax over actor preference plus WM trace
            prefs = A[s, :] + wm_boost * M[s, :]
            prefs -= np.max(prefs)
            p = np.exp(beta * prefs)
            p = p / (np.sum(p) + eps)
            p_a = p[a]
            total_log_p += np.log(max(p_a, eps))

            # TD error and updates
            # episodic/choice tasks are immediate; no next-state dependency
            delta = r - V[s]
            V[s] += alpha_critic * delta

            # Actor update (policy gradient-like with baseline)
            A[s, a] += alpha_actor * delta * (1.0 - p[a])
            for a_other in range(nA):
                if a_other != a:
                    A[s, a_other] -= alpha_actor * delta * p[a_other]

            # WM trace update: decay then set chosen trace to current reward
            M[s, :] = (1.0 - decay_eff) * M[s, :]
            M[s, a] = r

    return -float(total_log_p)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL mixed with Bayesian working-memory (Dirichlet counts) via confidence-based arbitration.
    Arbitration depends on the entropy (uncertainty) of the WM posterior, and is further shifted by age and load.

    Mechanism
    - RL: tabular Q-learning.
    - WM: maintain Dirichlet counts C[s,a]; WM policy is the posterior mean p_wm = C / sum(C).
    - Confidence: 1 - normalized entropy of p_wm for the current state; higher when WM has a peaked distribution.
    - Arbitration weight for WM is a logistic transform of gate_base + conf_slope*confidence + age and load terms.
      Older group reduces the WM weight; lower set size increases the WM weight.
    - Final policy is convex combination of WM and RL softmax.

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..nS-1 within block).
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of float/int
        Reward per trial (0 or 1).
    blocks : array-like of int
        Block IDs. Learning resets per block.
    set_sizes : array-like of int
        Set size per trial (3 or 6).
    age : array-like or scalar
        Participant age in years; older group (>=45) reduces WM arbitration weight.
    model_parameters : sequence of 6 floats
        alpha_rl      : Q-learning rate (0..1)
        beta          : Inverse temperature for softmax (scaled by 10 internally)
        gate_base     : Intercept of arbitration gate
        conf_slope    : Sensitivity of gate to WM confidence (0..)
        age_gate_drop : Reduction of gate for older group (>=45)
        load_gate     : Sensitivity of gate to low load (applied to 3 - set_size)

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.
    """
    alpha_rl, beta, gate_base, conf_slope, age_gate_drop, load_gate = model_parameters
    beta = beta * 10.0

    # Resolve age
    try:
        age_val = float(age[0])
    except Exception:
        age_val = float(age)
    is_older = 1.0 if age_val >= 45.0 else 0.0

    nA = 3
    eps = 1e-12
    total_log_p = 0.0

    # helper
    def softmax_pref(q_row):
        prefs = q_row - np.max(q_row)
        p = np.exp(beta * prefs)
        z = np.sum(p) + eps
        return p / z

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = np.asarray(actions)[idx]
        block_rewards = np.asarray(rewards)[idx]
        block_states = np.asarray(states)[idx]
        block_set_sizes = np.asarray(set_sizes)[idx]
        nS = int(block_set_sizes[0])

        # Initialize RL and WM (Dirichlet counts with uniform prior 1)
        Q = (1.0 / nA) * np.ones((nS, nA))
        C = np.ones((nS, nA))  # prior counts (uninformative)

        # precompute load term for gate
        load_term = load_gate * (3.0 - float(nS))  # 0 for 3, negative for 6 if load_gate>0

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # RL policy
            p_rl = softmax_pref(Q[s, :])

            # WM posterior mean and confidence from entropy
            p_wm = C[s, :] / (np.sum(C[s, :]) + eps)
            # entropy in nats, normalized by max entropy log(nA)
            H = -np.sum(p_wm * (np.log(p_wm + eps)))
            H_max = np.log(float(nA))
            confidence = 1.0 - (H / (H_max + eps))  # in [0,1]

            # Arbitration gate based on confidence, age, and load
            z = gate_base + conf_slope * confidence - age_gate_drop * is_older + load_term
            w = sigmoid(z)  # WM weight

            p_mix = w * p_wm + (1.0 - w) * p_rl
            p_a = p_mix[a]
            total_log_p += np.log(max(p_a, eps))

            # Learning updates
            pe = r - Q[s, a]
            Q[s, a] += alpha_rl * pe

            # WM Dirichlet count update: add 1 to observed action; add reward as well to bias rewarded actions
            # This implements a simple associative memory of successful choices
            C[s, a] += 1.0 + r  # more gain when rewarded

    return -float(total_log_p)