def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Hybrid RL + capacity-limited WM with age- and load-modulated mixture.

    Idea:
    - Two systems contribute to choice:
      (i) Model-free RL with softmax.
      (ii) One-shot Working Memory (WM) that stores the last rewarded action for each state and decays.
    - The mixture weight of WM vs RL is modulated by:
        * WM precision (how sharply WM favors the stored action),
        * Set size (load; larger sets reduce WM contribution via a log-load term),
        * Age group (younger vs. older) via an additive bias,
        * Current WM strength for the state (decays over time).
    - RL and WM policies are combined as a convex mixture.

    Parameters (model_parameters):
    - alpha: (0,1) RL learning rate.
    - beta: >0 base inverse temperature for RL softmax (internally scaled).
    - wm_precision: >=0, magnitude of WM selectivity toward the stored action.
    - wm_decay: (0,1), per-trial decay of WM strength toward 0.
    - age_bias: real, additive bias on WM mixture for age group (+ for younger, - for older).
    - mix_bias: real, global offset for WM mixture (baseline WM reliance).

    Inputs:
    - states, actions, rewards, blocks, set_sizes: arrays of shape (T,)
    - age: array-like with a single numeric age in years.
    - model_parameters: list/tuple as above.

    Returns:
    - Negative log-likelihood of observed choices under the model.
    """
    alpha, beta, wm_precision, wm_decay, age_bias, mix_bias = model_parameters
    beta = max(1e-6, beta) * 6.0
    wm_precision = max(0.0, wm_precision)
    wm_decay = np.clip(wm_decay, 0.0, 1.0)
    age_years = float(age[0])
    age_group = 1.0 if age_years < 45.0 else -1.0

    nA = 3
    total_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask]
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        nS = int(set_sizes[mask][0])

        # RL values
        Q = np.zeros((nS, nA))

        # WM: per-state last rewarded action and its strength
        wm_key = -1 * np.ones(nS, dtype=int)       # -1 = no memory
        wm_strength = np.zeros(nS, dtype=float)    # in [0,1]

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # RL policy
            logits_rl = beta * (Q[s, :] - np.max(Q[s, :]))
            pi_rl = np.exp(logits_rl)
            pi_rl = pi_rl / np.sum(pi_rl)

            # WM policy for this state: one-hot-like controlled by wm_precision
            if wm_key[s] >= 0 and wm_strength[s] > 1e-8:
                wm_vals = np.zeros(nA)
                wm_vals[wm_key[s]] = wm_precision
                wm_vals -= np.max(wm_vals)
                pi_wm = np.exp(wm_vals)
                pi_wm = pi_wm / np.sum(pi_wm)
            else:
                pi_wm = np.ones(nA) / nA  # if no memory, WM is uniform

            # Mixture weight: sigmoid of (precision - log load + biases), scaled by current strength
            load_term = -np.log(float(nS))  # higher nS -> more negative
            mix_lin = mix_bias + wm_precision + load_term + age_bias * age_group
            w_base = 1.0 / (1.0 + np.exp(-mix_lin))
            w = w_base * np.clip(wm_strength[s], 0.0, 1.0)  # state-specific effective WM reliance

            # Combined policy
            pi = w * pi_wm + (1.0 - w) * pi_rl
            p = max(1e-12, float(pi[a]))
            total_log_p += np.log(p)

            # Learning updates
            # RL update
            Q[s, a] += alpha * (r - Q[s, a])

            # WM decay (global per trial)
            wm_strength *= (1.0 - wm_decay)
            wm_strength = np.clip(wm_strength, 0.0, 1.0)

            # WM encoding on reward: store last rewarded action with full strength
            if r > 0.5:
                wm_key[s] = a
                wm_strength[s] = 1.0

    return -total_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with age- and load-modulated temperature and choice stickiness.

    Idea:
    - Single RL system with softmax choice.
    - Add "stickiness" bonus to repeating the previous action in the same state.
    - Both temperature and stickiness are modulated by age group and set size:
        * Temperature: beta_eff = beta * exp(temp_age_gain*age_group + temp_size_gain*(3/nS - 1)).
        * Stickiness: tau_eff = tau_stick * (1 + age_stick_offset*age_group) * (3/nS).

    Parameters (model_parameters):
    - alpha: (0,1) learning rate.
    - beta: >0 base inverse temperature (internally scaled).
    - tau_stick: real, base stickiness (added to the logit of repeating previous action in same state).
    - age_stick_offset: real, age modulation of stickiness (relative multiplier).
    - temp_age_gain: real, exponential modulation of temperature by age group.
    - temp_size_gain: real, exponential modulation of temperature by set size.

    Inputs:
    - states, actions, rewards, blocks, set_sizes: arrays (T,)
    - age: array-like with single numeric age.
    - model_parameters: [alpha, beta, tau_stick, age_stick_offset, temp_age_gain, temp_size_gain]

    Returns:
    - Negative log-likelihood of observed choices.
    """
    alpha, beta, tau_stick, age_stick_offset, temp_age_gain, temp_size_gain = model_parameters
    beta = max(1e-6, beta) * 5.0
    age_years = float(age[0])
    age_group = 1.0 if age_years < 45.0 else -1.0

    nA = 3
    total_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask]
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        nS = int(set_sizes[mask][0])

        Q = np.zeros((nS, nA))
        prev_action = -1 * np.ones(nS, dtype=int)  # previous action per state

        beta_eff = beta * np.exp(temp_age_gain * age_group + temp_size_gain * (3.0 / float(nS) - 1.0))
        beta_eff = max(1e-6, float(beta_eff))

        tau_eff_base = tau_stick * (1.0 + age_stick_offset * age_group) * (3.0 / float(nS))

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            logits = beta_eff * (Q[s, :] - np.max(Q[s, :]))

            # Add stickiness to repeating the previous action in this state
            if prev_action[s] >= 0:
                stick_vec = np.zeros(nA)
                stick_vec[prev_action[s]] = tau_eff_base
                logits += stick_vec

            # Softmax
            m = np.max(logits)
            pi = np.exp(logits - m)
            pi = pi / np.sum(pi)

            p = max(1e-12, float(pi[a]))
            total_log_p += np.log(p)

            # Update
            Q[s, a] += alpha * (r - Q[s, a])
            prev_action[s] = a

    return -total_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with WM gating and age/load-dependent default action bias.

    Idea:
    - Model-free RL (softmax) provides baseline values.
    - A lightweight WM gate adds a positive bonus to the last rewarded action in a state,
      with a confidence that decays over time.
    - Additionally, a default (action-0) bias increases with cognitive load and for older adults,
      reflecting policy compression under load/aging.

    Parameters (model_parameters):
    - alpha: (0,1) RL learning rate.
    - beta: >0 base inverse temperature (internally scaled).
    - wm_hit_bonus: >=0, additive bonus to the logit of the WM-stored action, scaled by confidence.
    - decay: (0,1), per-trial decay of WM confidence.
    - bias0_base: real, baseline bias added to the logit of action 0.
    - bias_age_load_gain: real, scales how bias increases with load and older age.

    Inputs:
    - states, actions, rewards, blocks, set_sizes: arrays (T,)
    - age: array-like with a single numeric age in years.
    - model_parameters: [alpha, beta, wm_hit_bonus, decay, bias0_base, bias_age_load_gain]

    Returns:
    - Negative log-likelihood of observed choices.
    """
    alpha, beta, wm_hit_bonus, decay, bias0_base, bias_age_load_gain = model_parameters
    beta = max(1e-6, beta) * 6.0
    wm_hit_bonus = max(0.0, wm_hit_bonus)
    decay = np.clip(decay, 0.0, 1.0)
    age_years = float(age[0])
    age_group = 1.0 if age_years < 45.0 else -1.0

    nA = 3
    total_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask]
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        nS = int(set_sizes[mask][0])

        Q = np.zeros((nS, nA))

        # WM per state
        wm_a = -1 * np.ones(nS, dtype=int)
        wm_conf = np.zeros(nS, dtype=float)

        # Default action-0 bias increases with load and for older adults
        load_factor = (float(nS) - 3.0) / 3.0  # 0 for nS=3, 1 for nS=6
        bias0 = bias0_base + bias_age_load_gain * load_factor * (-age_group)  # older (age_group=-1) increases bias

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Base RL logits
            logits = beta * (Q[s, :] - np.max(Q[s, :]))

            # Add WM hit bonus if available
            if wm_a[s] >= 0 and wm_conf[s] > 1e-8:
                bonus_vec = np.zeros(nA)
                bonus_vec[wm_a[s]] = wm_hit_bonus * wm_conf[s]
                logits += bonus_vec

            # Add default bias for action 0
            logits[0] += bias0

            # Softmax
            m = np.max(logits)
            pi = np.exp(logits - m)
            pi = pi / np.sum(pi)

            p = max(1e-12, float(pi[a]))
            total_log_p += np.log(p)

            # RL update
            Q[s, a] += alpha * (r - Q[s, a])

            # WM decay
            wm_conf *= (1.0 - decay)
            wm_conf = np.clip(wm_conf, 0.0, 1.0)

            # WM encoding on reward
            if r > 0.5:
                wm_a[s] = a
                wm_conf[s] = 1.0

    return -total_log_p