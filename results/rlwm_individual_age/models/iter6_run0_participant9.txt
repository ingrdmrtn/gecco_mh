def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with block-level generalization: mixture of state-specific and global action values,
    with age- and load-dependent generalization weight.

    Mechanism
    - Maintain both state-specific Q_s(a) and a global within-block action value G(a).
    - Action policy uses a softmax over a convex combination: V(a|s) = λ * Q_s(a) + (1-λ) * G(a).
    - Generalization weight λ depends on set size and age group: larger sets and older age
      push toward more global generalization (smaller λ), reducing reliance on state-specific Q.
    - Both Q_s and G are updated via delta rules on every trial.

    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..nS-1 within a block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of {0,1}
        Feedback on each trial.
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size for each trial (e.g., 3 or 6); constant within a block.
    age : array-like
        Participant age; uses age[0] to define age group (younger <45, older >=45).
    model_parameters : sequence of 5 floats
        [alpha, beta, gen_base, load_gen_gain, age_gen_gain]
        - alpha: learning rate for both Q_s and G (sigmoid to [0,1]).
        - beta: inverse temperature for softmax (>=0).
        - gen_base: baseline log-odds favoring state-specific values (higher -> larger λ).
        - load_gen_gain: effect of set size on log-odds (larger set size reduces λ if negative).
        - age_gen_gain: additional log-odds shift for older group (reduces λ if negative).

    Returns
    -------
    nll : float
        Negative log-likelihood of observed actions.
    """
    alpha, beta, gen_base, load_gen_gain, age_gen_gain = model_parameters

    alpha = 1.0 / (1.0 + np.exp(-alpha))
    beta = np.maximum(beta, 1e-6)
    age_group = 1 if age[0] >= 45 else 0

    nA = 3
    nll = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]
        nS = int(block_set_sizes[0])

        Q = (1.0 / nA) * np.ones((nS, nA))
        G = (1.0 / nA) * np.ones(nA)

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])
            ss = int(block_set_sizes[t])

            load_factor = (ss - 3.0) / 3.0  # 0 for 3, 1 for 6
            logit_lambda = gen_base + load_gen_gain * load_factor + age_gen_gain * age_group
            lam = 1.0 / (1.0 + np.exp(-logit_lambda))
            lam = np.clip(lam, 0.0, 1.0)

            V = lam * Q[s, :] + (1.0 - lam) * G
            logits = beta * (V - np.max(V))
            p = np.exp(logits)
            p /= np.maximum(1e-12, np.sum(p))
            p_a = np.clip(p[a], 1e-12, 1.0)
            nll -= np.log(p_a)

            # Update state-specific and global values independently
            delta_s = r - Q[s, a]
            Q[s, a] += alpha * delta_s

            delta_g = r - G[a]
            G[a] += alpha * delta_g

    return nll


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL–WM arbitration with dynamic load from discovered states (novelty), and age-dependent WM reliance.

    Mechanism
    - RL: Tabular Q-learning with softmax.
    - WM cache: stores the most recent rewarded action for each state when a reward is observed;
      policy is deterministic for cached states and uniform otherwise.
    - Arbitration weight for WM depends on within-block discovered-state load:
      w_wm = sigmoid(wm_bias - novelty_sensitivity * (unique_seen / set_size) - age_wm_penalty * age_group).
      Thus, seeing more unique states (higher effective load) and being older both reduce WM reliance.

    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..nS-1 within a block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of {0,1}
        Feedback on each trial.
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size for each trial (e.g., 3 or 6); constant within a block.
    age : array-like
        Participant age; uses age[0] to define age group (younger <45, older >=45).
    model_parameters : sequence of 5 floats
        [alpha, beta, wm_bias, novelty_sensitivity, age_wm_penalty]
        - alpha: Q-learning rate (sigmoid to [0,1]).
        - beta: inverse temperature for RL softmax (>=0).
        - wm_bias: baseline log-odds of using WM over RL.
        - novelty_sensitivity: penalty per proportion of set discovered (>=0 increases load effect).
        - age_wm_penalty: additional penalty if older (>=0 reduces WM use).

    Returns
    -------
    nll : float
        Negative log-likelihood of observed actions.
    """
    alpha, beta, wm_bias, novelty_sensitivity, age_wm_penalty = model_parameters

    alpha = 1.0 / (1.0 + np.exp(-alpha))
    beta = np.maximum(beta, 1e-6)
    novelty_sensitivity = np.maximum(0.0, novelty_sensitivity)
    age_wm_penalty = np.maximum(0.0, age_wm_penalty)
    age_group = 1 if age[0] >= 45 else 0

    nA = 3
    nll = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]
        nS = int(block_set_sizes[0])

        Q = (1.0 / nA) * np.ones((nS, nA))
        wm_cache = -np.ones(nS, dtype=int)  # -1 means not cached yet
        seen_states = set()

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])
            ss = int(block_set_sizes[t])

            seen_states.add(s)
            load_prop = len(seen_states) / float(max(1, ss))

            # RL policy
            logits = beta * (Q[s, :] - np.max(Q[s, :]))
            p_rl = np.exp(logits)
            p_rl /= np.maximum(1e-12, np.sum(p_rl))

            # WM policy
            pi_wm = np.ones(nA) / nA
            if wm_cache[s] >= 0:
                pi_wm = np.zeros(nA)
                pi_wm[wm_cache[s]] = 1.0

            # Arbitration weight
            logit_w = wm_bias - novelty_sensitivity * load_prop - age_wm_penalty * age_group
            w_wm = 1.0 / (1.0 + np.exp(-logit_w))
            w_wm = np.clip(w_wm, 0.0, 1.0)

            p_mix = w_wm * pi_wm + (1.0 - w_wm) * p_rl
            p_a = np.clip(p_mix[a], 1e-12, 1.0)
            nll -= np.log(p_a)

            # RL update
            delta = r - Q[s, a]
            Q[s, a] += alpha * delta

            # Update WM cache only when rewarded
            if r >= 0.5:
                wm_cache[s] = a

    return nll


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Uncertainty-calibrated softmax RL: state uncertainty reduces effective inverse temperature,
    with age- and load-dependent uncertainty dynamics.

    Mechanism
    - Tabular Q-learning with fixed learning rate alpha.
    - Maintain per (state, action) uncertainty Var[s,a].
    - Effective inverse temperature is reduced under higher uncertainty:
        beta_eff = beta / (1 + uncert_temp_gain * mean_a Var[s,a]).
    - Uncertainty dynamics:
        Var[s,a] <- (1 - alpha) * Var[s,a] + process_noise,
      where process_noise increases with set size and (if older) with an age shift.
      Unchosen actions also gain process noise (uncertainty persists).

    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..nS-1 within a block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of {0,1}
        Feedback on each trial.
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size for each trial (e.g., 3 or 6).
    age : array-like
        Participant age; uses age[0] to define age group (younger <45, older >=45).
    model_parameters : sequence of 5 floats
        [alpha, beta, uncert_temp_gain, load_uncert_gain, age_uncert_shift]
        - alpha: Q-learning rate; also controls how fast uncertainty shrinks (sigmoid to [0,1]).
        - beta: base inverse temperature (>=0).
        - uncert_temp_gain: strength by which uncertainty reduces inverse temperature (>=0).
        - load_uncert_gain: scales process noise as set size increases (>=0).
        - age_uncert_shift: additive increase in process noise for older group (>=0).

    Returns
    -------
    nll : float
        Negative log-likelihood of observed actions.
    """
    alpha, beta, uncert_temp_gain, load_uncert_gain, age_uncert_shift = model_parameters

    alpha = 1.0 / (1.0 + np.exp(-alpha))
    beta = np.maximum(beta, 1e-6)
    uncert_temp_gain = np.maximum(0.0, uncert_temp_gain)
    load_uncert_gain = np.maximum(0.0, load_uncert_gain)
    age_uncert_shift = np.maximum(0.0, age_uncert_shift)
    age_group = 1 if age[0] >= 45 else 0

    nA = 3
    nll = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]
        nS = int(block_set_sizes[0])

        Q = (1.0 / nA) * np.ones((nS, nA))

        # Initialize uncertainty higher when alpha is low; add age effect for older
        Var = ((1.0 - alpha) + age_group * age_uncert_shift) * np.ones((nS, nA))
        Var = np.maximum(Var, 1e-6)

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])
            ss = int(block_set_sizes[t])

            # Process noise increases with load and age
            load_factor = (ss - 3.0) / 3.0  # 0 for 3, 1 for 6
            process_noise = load_uncert_gain * (0.5 + 0.5 * load_factor) + age_group * age_uncert_shift
            process_noise = np.maximum(process_noise, 1e-8)

            # Effective inverse temperature scaled by uncertainty at state s
            mean_uncert = np.mean(Var[s, :])
            beta_eff = beta / (1.0 + uncert_temp_gain * mean_uncert)
            beta_eff = np.maximum(beta_eff, 1e-6)

            logits = beta_eff * (Q[s, :] - np.max(Q[s, :]))
            p = np.exp(logits)
            p /= np.maximum(1e-12, np.sum(p))
            p_a = np.clip(p[a], 1e-12, 1.0)
            nll -= np.log(p_a)

            # RL value update
            delta = r - Q[s, a]
            Q[s, a] += alpha * delta

            # Uncertainty updates: chosen and unchosen
            Var[s, a] = (1.0 - alpha) * Var[s, a] + process_noise
            for a_other in range(nA):
                if a_other != a:
                    Var[s, a_other] = (1.0 - 0.5 * alpha) * Var[s, a_other] + process_noise

    return nll