def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with load- and age-modulated forgetting plus action stickiness.

    Idea:
    - Model-free Q-learning with per-trial value decay ("forgetting") toward the grand mean,
      where the forgetting rate increases with set size and (for older adults) with age.
    - Add a perseveration (stickiness) bias favoring the most recent action taken in the current state.
    - Younger adults (age < 45) are less affected by load-driven forgetting than older adults.

    Parameters (model_parameters):
    - alpha: learning rate for Q-updates in [0,1].
    - beta: inverse temperature for softmax choice (>0).
    - k_forget: base forgetting gain (>0), scales with set size/age to compute decay per trial.
    - stickiness: strength of perseveration bias toward last chosen action (can be >=0).
    - age_forget_bias: adds extra forgetting if older, reduces if younger (can be positive).
      Effective forgetting multiplier = 1 + age_forget_bias * is_older - 0.5*age_forget_bias*(1-is_older)

    Inputs:
    - states: array of state indices per trial (0..set_size-1 within block).
    - actions: array of chosen action indices per trial (0..2).
    - rewards: array of rewards per trial (0 or 1).
    - blocks: array of block IDs per trial (learning resets per block).
    - set_sizes: array with set size for each trial (e.g., 3 or 6).
    - age: array-like with a single value (participant age in years).
    - model_parameters: list/tuple with [alpha, beta, k_forget, stickiness, age_forget_bias].

    Returns:
    - Negative log-likelihood of observed choices.
    """
    alpha, beta, k_forget, stickiness, age_forget_bias = model_parameters
    age_val = age[0]
    is_older = 1.0 if age_val >= 45 else 0.0
    nA = 3
    eps = 1e-12
    nll = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx].astype(int)
        block_actions = actions[idx].astype(int)
        block_rewards = rewards[idx]
        block_set_sizes = set_sizes[idx].astype(int)

        nS = int(block_set_sizes[0])
        # Initialize Q-values around uniform baseline; and track last action per state for stickiness.
        Q = (1.0 / nA) * np.ones((nS, nA))
        last_action = -1 * np.ones(nS, dtype=int)

        for t in range(len(block_states)):
            s = block_states[t]
            a = block_actions[t]
            r = block_rewards[t]
            curr_set = int(block_set_sizes[t])

            # Compute forgetting factor toward uniform baseline
            # Load factor: larger set -> more forgetting; normalize by 3
            load_factor = float(curr_set) / 3.0
            # Age modulation: older -> more forgetting, younger -> slightly less if age_forget_bias > 0
            age_mult = 1.0 + age_forget_bias * is_older - 0.5 * age_forget_bias * (1.0 - is_older)
            f = 1.0 - np.exp(-k_forget * load_factor * age_mult)  # maps to (0,1) with k_forget>0

            # Apply forgetting to all Q-values toward the uniform prior
            Q = (1.0 - f) * Q + f * (1.0 / nA)

            # Compute action values with a stickiness bias for current state
            bias = np.zeros(nA)
            if last_action[s] >= 0:
                bias[last_action[s]] = stickiness

            logits = beta * (Q[s, :] - np.max(Q[s, :])) + bias  # bias is additive in logits
            p = np.exp(logits - np.max(logits))
            p /= np.sum(p)
            nll -= np.log(max(p[a], eps))

            # Q-learning update
            delta = r - Q[s, a]
            Q[s, a] += alpha * delta

            # Update last action in this state
            last_action[s] = a

    return nll


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with reward-gated, decaying working memory (WM) that is harder to write/retain
    under higher load and for older adults.

    Idea:
    - Two systems:
      (1) Model-free Q-learning (baseline choice).
      (2) A WM policy table M[s,:] that, when written, stores a deterministic action (one-hot).
    - WM decays toward uniform each trial; its retention is lower with larger set sizes
      and further reduced for older adults.
    - WM writing is reward-gated with a threshold on the absolute prediction error:
      Only when reward is positive and surprise (|RPE|) exceeds a threshold is memory updated.
      Older adults have a higher effective threshold (harder to commit to WM).
    - Arbitration: mixture policy without an extra free weight: the WM confidence for a state
      conf = max(M[s,:]) - 1/nA, rescaled into [0,1], determines the mixture with RL.

    Parameters (model_parameters):
    - alpha: learning rate for Q-updates in [0,1].
    - beta: inverse temperature for RL softmax (>0).
    - theta_write: base RPE threshold for committing to WM (>=0).
    - rho_decay: base WM retention in [0,1]; higher -> slower decay. Effective retention scales with load and age.
    - age_gate: age modulation (>0): increases write threshold and increases decay for older adults.

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age: arrays as specified.
    - model_parameters: [alpha, beta, theta_write, rho_decay, age_gate]

    Returns:
    - Negative log-likelihood of observed choices.
    """
    alpha, beta, theta_write, rho_decay, age_gate = model_parameters
    age_val = age[0]
    is_older = 1.0 if age_val >= 45 else 0.0

    nA = 3
    eps = 1e-12
    nll = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx].astype(int)
        block_actions = actions[idx].astype(int)
        block_rewards = rewards[idx]
        block_set_sizes = set_sizes[idx].astype(int)

        nS = int(block_set_sizes[0])
        Q = (1.0 / nA) * np.ones((nS, nA))
        M = (1.0 / nA) * np.ones((nS, nA))  # WM policy rows sum to 1

        for t in range(len(block_states)):
            s = block_states[t]
            a = block_actions[t]
            r = block_rewards[t]
            curr_set = int(block_set_sizes[t])

            # RL policy
            q_s = Q[s, :]
            logits_rl = beta * (q_s - np.max(q_s))
            p_rl = np.exp(logits_rl - np.max(logits_rl))
            p_rl /= np.sum(p_rl)

            # WM policy and confidence
            m_s = M[s, :]
            m_s = m_s / np.sum(m_s)
            p_wm = m_s
            conf = np.max(m_s) - (1.0 / nA)  # in [0, 1-1/nA]
            conf /= (1.0 - 1.0 / nA)  # normalize to [0,1]

            # Mixture by WM confidence (no extra free mixing parameter)
            p_mix = conf * p_wm + (1.0 - conf) * p_rl
            nll -= np.log(max(p_mix[a], eps))

            # Learning
            delta = r - Q[s, a]
            Q[s, a] += alpha * delta

            # WM decay: stronger decay with larger set size and for older adults
            load_scale = 3.0 / float(curr_set)  # 1.0 at set=3, 0.5 at set=6
            age_ret_mult = 1.0 - 0.5 * age_gate * (1.0 - is_older)  # reduce retention slightly for young if age_gate>0
            age_ret_mult *= 1.0 - 0.5 * age_gate * is_older         # additional reduction for older
            rho_eff = rho_decay * load_scale * max(0.0, age_ret_mult)
            rho_eff = min(max(rho_eff, 0.0), 1.0)
            M = rho_eff * M + (1.0 - rho_eff) * (1.0 / nA)

            # WM write: reward-gated with age-modulated threshold on |delta|
            theta_eff = theta_write * (1.0 + age_gate * is_older)
            if (r > 0.5) and (abs(delta) >= theta_eff):
                M[s, :] = 0.0
                M[s, a] = 1.0

    return nll


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with novelty bonus and load-/age-modulated attentional temperature.

    Idea:
    - Model-free Q-learning baseline.
    - Add a novelty/exploration bonus to action values based on inverse visitation counts N[s,a].
      This bonus decays as actions are sampled; older adults have reduced novelty weighting.
    - Effective inverse temperature is reduced under higher load and further reduced for older adults
      (attention/precision decreases), making choices noisier in large set sizes and older age.

    Parameters (model_parameters):
    - alpha: learning rate for Q-updates in [0,1].
    - beta_base: base inverse temperature (>0).
    - lambda_nov: weight of novelty bonus (>0).
    - load_attn: load sensitivity (>0) controlling how much set size reduces effective beta.
    - age_explore_shift: reduces novelty bonus for older adults (>=0); effective lambda_nov *= (1 - age_explore_shift*is_older).

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age: arrays as specified.
    - model_parameters: [alpha, beta_base, lambda_nov, load_attn, age_explore_shift]

    Returns:
    - Negative log-likelihood of observed choices.
    """
    alpha, beta_base, lambda_nov, load_attn, age_explore_shift = model_parameters
    age_val = age[0]
    is_older = 1.0 if age_val >= 45 else 0.0

    nA = 3
    eps = 1e-12
    nll = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx].astype(int)
        block_actions = actions[idx].astype(int)
        block_rewards = rewards[idx]
        block_set_sizes = set_sizes[idx].astype(int)

        nS = int(block_set_sizes[0])
        Q = (1.0 / nA) * np.ones((nS, nA))
        N = np.ones((nS, nA))  # visitation counts start at 1 to avoid div-by-zero

        for t in range(len(block_states)):
            s = block_states[t]
            a = block_actions[t]
            r = block_rewards[t]
            curr_set = int(block_set_sizes[t])

            # Effective temperature reduced by load and age
            load_downscale = 1.0 + load_attn * (float(curr_set) / 3.0 - 1.0)  # =1 at set=3, >1 at set=6
            age_downscale = 1.0 + 0.5 * load_attn * is_older  # further reduces precision for older
            beta_eff = beta_base / (load_downscale * age_downscale)
            beta_eff = max(beta_eff, 1e-6)

            # Novelty bonus: higher for less-visited actions; reduced for older adults
            lambda_eff = lambda_nov * (1.0 - age_explore_shift * is_older)
            bonus = lambda_eff / np.sqrt(N[s, :])

            # Softmax over Q + novelty
            vals = Q[s, :] + bonus
            logits = beta_eff * (vals - np.max(vals))
            p = np.exp(logits - np.max(logits))
            p /= np.sum(p)
            nll -= np.log(max(p[a], eps))

            # Update counts and Q-learning
            N[s, a] += 1.0
            delta = r - Q[s, a]
            Q[s, a] += alpha * delta

    return nll