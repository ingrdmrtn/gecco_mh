def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with age-specific choice stickiness and load-modulated temperature.

    The model uses a standard Q-learning value update and a softmax choice rule.
    Two mechanisms modulate choice:
    - Choice stickiness: a bias to repeat the previous action in a given state.
      This bias is age-specific (younger vs older parameter).
    - Load-modulated temperature: the effective inverse temperature is reduced as set size increases.

    A small lapse rate mixes in a uniform random choice.

    Parameters
    ----------
    states : array-like of int
        State indices per trial (0..nS-1 within block).
    actions : array-like of int
        Chosen actions per trial (0..nA-1).
    rewards : array-like of int
        Binary reward outcomes (0 or 1).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size for each trial's block (e.g., 3 or 6).
    age : array-like of float
        Participant age; age[0] used to determine age group (older >=45).
    model_parameters : tuple/list of floats
        (alpha, beta, stick_y, stick_o, size_temp_scale, lapse)
        - alpha: learning rate (0..1 via sigmoid)
        - beta: base inverse temperature (scaled internally)
        - stick_y: stickiness strength for younger group
        - stick_o: stickiness strength for older group
        - size_temp_scale: scales how much temperature is reduced with larger set size
        - lapse: lapse rate mixing with uniform policy (0..0.2 via sigmoid and cap)

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, stick_y, stick_o, size_temp_scale, lapse = model_parameters

    # Constrain parameters
    alpha = 1.0 / (1.0 + np.exp(-alpha))
    beta = (1.0 / (1.0 + np.exp(-beta))) * 12.0
    lapse = (1.0 / (1.0 + np.exp(-lapse))) * 0.2
    size_temp_scale = np.maximum(0.0, size_temp_scale)  # ensure non-negative

    age_val = float(age[0])
    older = 1 if age_val >= 45 else 0
    stick = stick_o if older else stick_y  # use age-specific stickiness

    nll = 0.0
    eps = 1e-12
    nA = 3

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx].astype(int)
        block_actions = actions[idx].astype(int)
        block_rewards = rewards[idx].astype(float)
        nS = int(set_sizes[idx][0])

        # Initialize Q-values and last action per state
        Q = np.zeros((nS, nA))
        last_action = -np.ones(nS, dtype=int)

        # Effective temperature decreases with set size (more load -> noisier policy)
        load_factor = 1.0 + size_temp_scale * max(0, (nS - 3)) / 3.0
        beta_eff = beta / load_factor

        for t in range(len(block_states)):
            s = block_states[t]
            a = block_actions[t]
            r = block_rewards[t]

            # Build logits with stickiness bias toward repeating last action in this state
            logits = beta_eff * Q[s, :].copy()
            if last_action[s] >= 0:
                logits[last_action[s]] += stick

            # Softmax with stabilization
            logits = logits - np.max(logits)
            pi = np.exp(logits)
            pi = pi / (np.sum(pi) + eps)

            # Apply lapse
            pi = (1.0 - lapse) * pi + lapse * (1.0 / nA)
            p = max(pi[a], eps)
            nll -= np.log(p)

            # Q-learning update
            delta = r - Q[s, a]
            Q[s, a] += alpha * delta

            # Update last action
            last_action[s] = a

    return nll


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Hybrid RL + rule-based chunking memory with age and load-modulated gating.

    The model blends:
    - RL softmax policy (Q-learning) and
    - A rule-based "chunking" memory that, once a rewarded action is observed
      for a state, tends to deterministically select that action thereafter.

    The mixture weight for the chunking policy is modulated by set size (lower in larger sets)
    and by age group via a shift in the chunking gate.

    A small lapse rate mixes in a uniform random choice.

    Parameters
    ----------
    states : array-like of int
        State indices per trial (0..nS-1 within block).
    actions : array-like of int
        Chosen actions per trial (0..nA-1).
    rewards : array-like of int
        Binary reward outcomes (0 or 1).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size for each trial's block (e.g., 3 or 6).
    age : array-like of float
        Participant age; age[0] used to determine age group (older >=45).
    model_parameters : tuple/list of floats
        (alpha, beta, chunk_base, age_chunk_shift, size_gate_slope, lapse)
        - alpha: RL learning rate (0..1 via sigmoid)
        - beta: inverse temperature for RL softmax (scaled internally)
        - chunk_base: baseline chunking weight in small sets for younger (0..1 via sigmoid)
        - age_chunk_shift: additive shift on chunking gate for older group (logit units)
        - size_gate_slope: sensitivity of chunking weight to set size (logit per item)
        - lapse: lapse rate mixing with uniform policy (0..0.2 via sigmoid and cap)

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, chunk_base, age_chunk_shift, size_gate_slope, lapse = model_parameters

    # Constrain parameters
    alpha = 1.0 / (1.0 + np.exp(-alpha))
    beta = (1.0 / (1.0 + np.exp(-beta))) * 12.0
    chunk_base = 1.0 / (1.0 + np.exp(-chunk_base))  # convert to probability
    lapse = (1.0 / (1.0 + np.exp(-lapse))) * 0.2

    age_val = float(age[0])
    older = 1 if age_val >= 45 else 0

    def logit(p):
        p = np.clip(p, 1e-6, 1 - 1e-6)
        return np.log(p) - np.log(1 - p)

    def inv_logit(x):
        return 1.0 / (1.0 + np.exp(-x))

    nll = 0.0
    eps = 1e-12
    nA = 3

    base_logit = logit(chunk_base)

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx].astype(int)
        block_actions = actions[idx].astype(int)
        block_rewards = rewards[idx].astype(float)
        nS = int(set_sizes[idx][0])

        # Initialize Q and chunk memory: -1 means unknown; otherwise store the rewarded action
        Q = np.zeros((nS, nA))
        chunk_mem = -np.ones(nS, dtype=int)

        # Compute chunking mixture weight for this block from gate
        # Larger sets reduce chunking weight; older age shifts the gate
        size_effect = size_gate_slope * (3 - nS)  # positive in small sets, negative in large sets
        gate = base_logit + age_chunk_shift * older + size_effect
        w_chunk = inv_logit(gate)

        for t in range(len(block_states)):
            s = block_states[t]
            a = block_actions[t]
            r = block_rewards[t]

            # RL softmax policy
            logits = beta * (Q[s, :] - np.max(Q[s, :]))
            pi_rl = np.exp(logits)
            pi_rl = pi_rl / (np.sum(pi_rl) + eps)

            # Chunking policy: if we have a stored rewarded action, choose it deterministically
            if chunk_mem[s] >= 0:
                pi_chunk = np.zeros(nA)
                pi_chunk[chunk_mem[s]] = 1.0
            else:
                pi_chunk = np.ones(nA) / nA  # before learning any correct action, uniform

            # Mixture and lapse
            pi = w_chunk * pi_chunk + (1.0 - w_chunk) * pi_rl
            pi = (1.0 - lapse) * pi + lapse * (1.0 / nA)

            p = max(pi[a], eps)
            nll -= np.log(p)

            # RL update
            delta = r - Q[s, a]
            Q[s, a] += alpha * delta

            # Update chunk memory only on rewarded trials
            if r > 0.0:
                chunk_mem[s] = a

    return nll


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with Uncertainty-Bonus (UCB-style) exploration, age- and load-modulated.

    The model uses Q-learning and augments action values with an uncertainty bonus:
    actions with fewer selections in a state receive a higher bonus. The bonus scale
    is increased/decreased by age and reduced under higher set sizes. Choices follow
    a softmax over the augmented values. A small lapse rate mixes in uniform choices.

    Parameters
    ----------
    states : array-like of int
        State indices per trial (0..nS-1 within block).
    actions : array-like of int
        Chosen actions per trial (0..nA-1).
    rewards : array-like of int
        Binary reward outcomes (0 or 1).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size for each trial's block (e.g., 3 or 6).
    age : array-like of float
        Participant age; age[0] used to determine age group (older >=45).
    model_parameters : tuple/list of floats
        (alpha, beta, ucb_scale, age_ucb_shift, size_unc_decay, lapse)
        - alpha: Q-learning rate (0..1 via sigmoid)
        - beta: inverse temperature for softmax (scaled internally)
        - ucb_scale: base scale of uncertainty bonus
        - age_ucb_shift: additive shift on ucb_scale for older group
        - size_unc_decay: how strongly the bonus diminishes with larger set sizes
        - lapse: lapse rate mixing with uniform policy (0..0.2 via sigmoid and cap)

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, ucb_scale, age_ucb_shift, size_unc_decay, lapse = model_parameters

    # Constrain parameters
    alpha = 1.0 / (1.0 + np.exp(-alpha))
    beta = (1.0 / (1.0 + np.exp(-beta))) * 12.0
    lapse = (1.0 / (1.0 + np.exp(-lapse))) * 0.2
    # Ensure positive scales
    ucb_scale = np.maximum(0.0, ucb_scale)
    size_unc_decay = np.maximum(0.0, size_unc_decay)

    age_val = float(age[0])
    older = 1 if age_val >= 45 else 0

    nll = 0.0
    eps = 1e-12
    nA = 3

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx].astype(int)
        block_actions = actions[idx].astype(int)
        block_rewards = rewards[idx].astype(float)
        nS = int(set_sizes[idx][0])

        # Initialize Q-values and state-action counts
        Q = np.zeros((nS, nA))
        N = np.zeros((nS, nA))  # counts of selections per (s,a)

        # Effective uncertainty bonus scale: adjusted by age and set size
        base_bonus = ucb_scale + age_ucb_shift * older
        load_factor = 1.0 + size_unc_decay * max(0, (nS - 3)) / 3.0
        bonus_scale_eff = base_bonus / load_factor

        for t in range(len(block_states)):
            s = block_states[t]
            a = block_actions[t]
            r = block_rewards[t]

            # UCB-style bonus: higher for rarely chosen actions; bounded and well-defined at zero count
            bonus = bonus_scale_eff / np.sqrt(N[s, :] + 1.0)

            # Softmax over augmented values
            aug = Q[s, :] + bonus
            logits = beta * (aug - np.max(aug))
            pi = np.exp(logits)
            pi = pi / (np.sum(pi) + eps)

            # Lapse
            pi = (1.0 - lapse) * pi + lapse * (1.0 / nA)
            p = max(pi[a], eps)
            nll -= np.log(p)

            # Learning update and count update
            delta = r - Q[s, a]
            Q[s, a] += alpha * delta
            N[s, a] += 1.0

    return nll