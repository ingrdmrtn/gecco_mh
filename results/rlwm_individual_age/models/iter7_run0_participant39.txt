def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + episodic cache mixture with age- and set-size-modulated recall.

    Mechanism:
    - Model-free Q-learning (state-action values).
    - An episodic "cache" that stores the last rewarded action for each state within a block.
    - On each trial, with a recall probability, the cached action (if any) is retrieved and selected
      deterministically; otherwise, behavior follows the RL softmax.
    - Recall probability decreases with larger set-size and in older adults, and decays with time
      since the cached event.

    Parameters
    ----------
    states : array-like of int
        State indices per trial within block (0..set_size-1).
    actions : array-like of int
        Chosen actions per trial (0..2).
    rewards : array-like of int/float
        Binary feedback (0/1).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (3 or 6).
    age : array-like of float
        Participant age; age[0] used. Age >= 45 treated as older.
    model_parameters : tuple/list of floats
        (alpha, tau, cache_base, decay_rate)
        - alpha: RL learning rate for Q updates.
        - tau: inverse temperature of the RL softmax.
        - cache_base: base recall strength for the episodic cache.
        - decay_rate: exponential decay rate of recall with time since the cached event.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, tau, cache_base, decay_rate = model_parameters
    age_val = float(age[0])
    is_older = 1.0 if age_val >= 45.0 else 0.0

    nA = 3
    eps = 1e-12
    total_loglik = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx]
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_set_sizes = set_sizes[idx]

        nS = int(block_set_sizes[0])

        # Initialize RL values and episodic cache
        Q = (1.0 / nA) * np.ones((nS, nA))
        cached_action = -1 * np.ones(nS, dtype=int)
        cached_time = -1 * np.ones(nS, dtype=int)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            ss = float(block_set_sizes[t])

            # RL policy
            prefs = tau * Q[s, :]
            prefs -= np.max(prefs)
            pi_rl = np.exp(prefs)
            pi_rl = pi_rl / (np.sum(pi_rl) + eps)

            # Episodic recall probability: scaled by set-size and age, decays with time since cache
            has_cache = cached_action[s] >= 0
            if has_cache:
                dt = t - int(cached_time[s])
                recall_strength = cache_base * (3.0 / ss) * (1.0 - 0.35 * is_older)
                recall_strength = np.clip(recall_strength, 0.0, 1.0)
                recall_prob = recall_strength * np.exp(-decay_rate * max(0, dt))
                recall_prob = np.clip(recall_prob, 0.0, 1.0)
                # Deterministic cache policy over actions
                p_cache_choice = 1.0 if a == int(cached_action[s]) else 0.0
            else:
                recall_prob = 0.0
                p_cache_choice = 0.0

            # Mixture likelihood
            p_total = recall_prob * p_cache_choice + (1.0 - recall_prob) * pi_rl[a]
            total_loglik += np.log(p_total + eps)

            # RL update
            pe = r - Q[s, a]
            Q[s, a] = Q[s, a] + alpha * pe

            # Cache update if reward received
            if r > 0.0:
                cached_action[s] = a
                cached_time[s] = t

    return -float(total_loglik)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Beta-Bernoulli (Bayesian) learner with uncertainty-driven exploration,
    modulated by age and set size, plus drift-to-prior forgetting.

    Mechanism:
    - For each state-action, maintains Beta(α, β) posterior over reward probability.
    - Choice uses a softmax over (expected value + exploration bonus * uncertainty).
    - Exploration bonus decreases in older adults and with larger set size.
    - Drift towards the prior (forgetting) grows with set size and in older adults.

    Parameters
    ----------
    states : array-like of int
        State indices per trial within block (0..set_size-1).
    actions : array-like of int
        Chosen actions per trial (0..2).
    rewards : array-like of int/float
        Binary feedback (0/1).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (3 or 6).
    age : array-like of float
        Participant age; age[0] used. Age >= 45 treated as older.
    model_parameters : tuple/list of floats
        (beta_temp, explore, prior_strength, drift)
        - beta_temp: inverse temperature for the softmax over preferences.
        - explore: base weight for the uncertainty bonus.
        - prior_strength: α0 = β0 = prior_strength for the initial prior.
        - drift: base drift-to-prior rate applied on each visit to a state (forgetting).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    beta_temp, explore, prior_strength, drift = model_parameters
    age_val = float(age[0])
    is_older = 1.0 if age_val >= 45.0 else 0.0

    nA = 3
    eps = 1e-12
    total_loglik = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx]
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_set_sizes = set_sizes[idx]

        nS = int(block_set_sizes[0])

        # Initialize Beta priors per state-action
        A = prior_strength * np.ones((nS, nA))
        B = prior_strength * np.ones((nS, nA))

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            ss = float(block_set_sizes[t])

            # Effective drift and exploration weights
            drift_eff = drift * (ss / 3.0) * (1.0 + 0.5 * is_older)
            drift_eff = np.clip(drift_eff, 0.0, 1.0)

            explore_eff = explore * (3.0 / ss) * (1.0 - 0.5 * is_older)
            explore_eff = max(0.0, explore_eff)

            # Apply drift towards the prior on the visited state's parameters
            # Move A[s,:], B[s,:] towards prior_strength
            A[s, :] = (1.0 - drift_eff) * A[s, :] + drift_eff * prior_strength
            B[s, :] = (1.0 - drift_eff) * B[s, :] + drift_eff * prior_strength

            # Compute means and uncertainties for action selection
            alpha_s = A[s, :]
            beta_s = B[s, :]
            mean = alpha_s / (alpha_s + beta_s + eps)
            # Standard deviation of Beta(α,β)
            std = np.sqrt((alpha_s * beta_s) / (((alpha_s + beta_s) ** 2) * (alpha_s + beta_s + 1.0) + eps))

            prefs = beta_temp * mean + explore_eff * std
            prefs -= np.max(prefs)
            pi = np.exp(prefs)
            pi = pi / (np.sum(pi) + eps)

            total_loglik += np.log(pi[a] + eps)

            # Bayesian update for the chosen action
            A[s, a] += r
            B[s, a] += (1.0 - r)

    return -float(total_loglik)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Adaptive arbitration between RL and a rule-based win-stay/lose-shift policy,
    with age- and set-size-biased prior arbitration and online meta-control updates.

    Mechanism:
    - RL: Q-learning with softmax choice.
    - Rule policy: win-stay (repeat last action in that state if it was rewarded),
      lose-shift (avoid last action if it was not rewarded; uniform over others).
    - Arbitration weight w_s per state mixes policies: pi = w_s * pi_rule + (1-w_s) * pi_rl.
    - Initial arbitration bias depends on age and set size (older and larger sets bias away from rule).
    - Online updates of arbitration logits compare the relative success of rule vs RL on the observed
      choice and outcome.

    Parameters
    ----------
    states : array-like of int
        State indices per trial within block (0..set_size-1).
    actions : array-like of int
        Chosen actions per trial (0..2).
    rewards : array-like of int/float
        Binary feedback (0/1).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (3 or 6).
    age : array-like of float
        Participant age; age[0] used. Age >= 45 treated as older.
    model_parameters : tuple/list of floats
        (alpha_q, tau, kappa0, eta)
        - alpha_q: RL learning rate for Q updates.
        - tau: inverse temperature for RL softmax.
        - kappa0: base arbitration logit (w = sigmoid(kappa)); positive favors rule.
        - eta: meta-control update rate for arbitration logits.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha_q, tau, kappa0, eta = model_parameters
    age_val = float(age[0])
    is_older = 1.0 if age_val >= 45.0 else 0.0

    nA = 3
    eps = 1e-12
    total_loglik = 0.0

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx]
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_set_sizes = set_sizes[idx]

        nS = int(block_set_sizes[0])

        Q = (1.0 / nA) * np.ones((nS, nA))

        # Track last action and reward per state for the rule policy
        last_action = -1 * np.ones(nS, dtype=int)
        last_rew = np.zeros(nS)

        # Arbitration logits per state with age and set-size prior bias
        # Older and larger set-size bias away from rule (negative shift).
        size_bias = -0.7 * (block_set_sizes[0] / 6.0)  # - ~0.35 for set=3, -0.7 for set=6
        age_bias = -0.6 * is_older
        kappa_baseline = kappa0 + size_bias + age_bias
        kappa = kappa_baseline * np.ones(nS)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            # ss = float(block_set_sizes[t])  # not needed beyond baseline in this model

            # RL policy
            prefs_rl = tau * Q[s, :]
            prefs_rl -= np.max(prefs_rl)
            pi_rl = np.exp(prefs_rl)
            pi_rl = pi_rl / (np.sum(pi_rl) + eps)

            # Rule policy (win-stay / lose-shift)
            if last_action[s] < 0:
                pi_rule = np.ones(nA) / nA
            else:
                if last_rew[s] > 0.0:
                    # Win-stay: deterministic repeat of last action
                    pi_rule = np.zeros(nA)
                    pi_rule[last_action[s]] = 1.0
                else:
                    # Lose-shift: avoid last action, uniform over others
                    pi_rule = np.ones(nA) / (nA - 1)
                    pi_rule[last_action[s]] = 0.0

            # Arbitration
            w = sigmoid(kappa[s])
            pi = w * pi_rule + (1.0 - w) * pi_rl
            pi = np.clip(pi, eps, 1.0)
            pi = pi / np.sum(pi)

            total_loglik += np.log(pi[a] + eps)

            # RL update
            pe = r - Q[s, a]
            Q[s, a] = Q[s, a] + alpha_q * pe

            # Meta-control update:
            # Compare rule vs RL support for the chosen action, scaled by outcome.
            # If rule assigned more mass than RL to the chosen action and it was rewarded,
            # increase kappa (favor rule); if unrewarded, decrease kappa, and vice versa.
            advantage = pi_rule[a] - pi_rl[a]
            signed_outcome = (2.0 * r - 1.0)  # +1 for reward, -1 for no reward
            kappa[s] += eta * signed_outcome * advantage

            # Small regularization towards baseline bias (prevents runaway)
            kappa[s] += 0.1 * eta * (kappa_baseline - kappa[s])

            # Update rule memory
            last_action[s] = a
            last_rew[s] = r

    return -float(total_loglik)