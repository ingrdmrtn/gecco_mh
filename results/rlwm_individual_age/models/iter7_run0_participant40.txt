def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + episodic one-shot memory with age- and load-modulated arbitration.

    The model blends:
      - Incremental RL (Q-learning) with softmax choice.
      - An episodic/one-shot memory that stores the last rewarded action for a state
        and retrieves it as a sharp policy when available.

    Arbitration between RL and episodic memory depends on:
      - A base episodic weight (omega0),
      - An age modulation (older adults rely more/less on episodic memory via age_bias),
      - A load modulation: larger set size reduces episodic contribution.

    Parameters
    ----------
    states : np.ndarray of int
        State at each trial (0..nS-1 within a block).
    actions : np.ndarray of int
        Chosen action at each trial (0..2). If action outside [0, nA-1], treated as lapse (uniform likelihood).
    rewards : np.ndarray of float
        Feedback (0 or 1). Values < 0 treated as invalid for updating.
    blocks : np.ndarray of int
        Block identifier per trial.
    set_sizes : np.ndarray of int
        Set size per trial (3 or 6). Determines how many distinct states are in the block.
    age : np.ndarray of float
        Array with participant age (use age[0]); age>=45 => older group.
    model_parameters : sequence of float
        [eta, inv_temp, omega0, age_bias, lapse]
        - eta: RL learning rate (0..1).
        - inv_temp: inverse temperature for RL softmax.
        - omega0: base weight of episodic memory in arbitration (0..1).
        - age_bias: additive bias to episodic weight for older adults (can be +/-).
        - lapse: lapse probability; with prob 'lapse' action is uniform (1/nA).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    eta, inv_temp, omega0, age_bias, lapse = model_parameters
    age_val = float(age[0])
    is_older = 1.0 if age_val >= 45.0 else 0.0

    nA = 3
    total_logp = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        b_states = states[idx]
        b_actions = actions[idx]
        b_rewards = rewards[idx]
        b_set_sizes = set_sizes[idx]
        nS = int(b_set_sizes[0])

        # RL values
        Q = (1.0 / nA) * np.ones((nS, nA))

        # Episodic memory: stores last rewarded action and a strength (confidence)
        epi_action = -np.ones(nS, dtype=int)   # -1 means no episodic memory yet
        epi_strength = np.zeros(nS)            # strength in [0,1]

        for t in range(len(b_states)):
            s = int(b_states[t])
            a = int(b_actions[t])
            r = float(b_rewards[t])
            ss = int(b_set_sizes[t])

            # RL softmax policy
            q_s = Q[s, :]
            q_center = q_s - np.max(q_s)
            p_rl = np.exp(inv_temp * q_center)
            p_rl = p_rl / np.sum(p_rl)

            # Episodic policy: if we have a remembered rewarded action, put most mass on it
            p_epi = np.ones(nA) / nA
            if epi_action[s] >= 0 and epi_action[s] < nA and epi_strength[s] > 0.0:
                p_epi = np.zeros(nA)
                p_epi[epi_action[s]] = 1.0
                # Smooth slightly by strength (if <1, mix with uniform)
                if epi_strength[s] < 1.0:
                    p_epi = epi_strength[s] * p_epi + (1.0 - epi_strength[s]) * (np.ones(nA) / nA)

            # Arbitration weight depends on age and load (larger set size reduces episodic reliance)
            load_factor = 1.0 / (1.0 + max(0, ss - 3))  # 1 for ss=3, 1/4 for ss=6
            omega = omega0 + age_bias * is_older
            omega = np.clip(omega, 0.0, 1.0)
            omega_eff = np.clip(omega * load_factor, 0.0, 1.0)

            # Combine policies and apply lapse
            p_mix = omega_eff * p_epi + (1.0 - omega_eff) * p_rl
            p_final = (1.0 - lapse) * p_mix + lapse * (np.ones(nA) / nA)

            # Likelihood contribution
            if 0 <= a < nA:
                pa = float(np.clip(p_final[a], 1e-12, 1.0))
                total_logp += np.log(pa)
            else:
                # Lapse/invalid action gets uniform likelihood; no learning
                total_logp += np.log(1.0 / nA)
                continue

            # Learning and memory update if feedback is valid
            if r >= 0.0:
                # RL update
                Q[s, a] += eta * (r - Q[s, a])

                # Episodic memory: if reward, set memory to chosen action with full strength;
                # if no reward, decay strength
                if r > 0.0:
                    epi_action[s] = a
                    # Strength can depend on how surprising the reward was (optional light dependence on Q)
                    surpr = 1.0 - (1.0 / (1.0 + np.exp(inv_temp * (Q[s, a] - np.mean(Q[s, :])))))
                    epi_strength[s] = np.clip(0.7 + 0.3 * surpr, 0.0, 1.0)
                else:
                    epi_strength[s] *= 0.5  # forget unsuccessful trace

    return -float(total_logp)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with load- and age-adaptive decision temperature and action stickiness.

    The model uses standard Q-learning, but the choice policy includes:
      - A dynamic inverse temperature that decreases with set size (higher load)
        and decreases further for older adults (noisier choices under load).
      - A state-specific action stickiness bias that is stronger for older adults
        and attenuated by load.

    Parameters
    ----------
    states : np.ndarray of int
        State at each trial (0..nS-1 within a block).
    actions : np.ndarray of int
        Chosen action at each trial (0..2); out-of-range treated as lapse.
    rewards : np.ndarray of float
        Feedback (0 or 1); negative values treated as invalid for updating.
    blocks : np.ndarray of int
        Block identifier per trial.
    set_sizes : np.ndarray of int
        Set size per trial (3 or 6).
    age : np.ndarray of float
        Array with participant age (use age[0]); age>=45 => older group.
    model_parameters : sequence of float
        [alpha, inv_temp0, load_slope, stick_age, lapse]
        - alpha: learning rate (0..1).
        - inv_temp0: baseline inverse temperature at low load.
        - load_slope: how much beta is reduced per extra 3 items (>=0).
        - stick_age: base magnitude of action stickiness; amplified for older adults.
        - lapse: lapse probability; with prob 'lapse' action is uniform.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, inv_temp0, load_slope, stick_age, lapse = model_parameters
    age_val = float(age[0])
    is_older = 1.0 if age_val >= 45.0 else 0.0

    nA = 3
    total_logp = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        b_states = states[idx]
        b_actions = actions[idx]
        b_rewards = rewards[idx]
        b_set_sizes = set_sizes[idx]
        nS = int(b_set_sizes[0])

        Q = (1.0 / nA) * np.ones((nS, nA))
        last_action = -np.ones(nS, dtype=int)

        for t in range(len(b_states)):
            s = int(b_states[t])
            a = int(b_actions[t])
            r = float(b_rewards[t])
            ss = int(b_set_sizes[t])

            # Dynamic inverse temperature: reduced by load and more for older adults
            load_units = max(0, ss - 3) / 3.0  # 0 for ss=3, 1 for ss=6
            beta = inv_temp0 / (1.0 + load_slope * (1.0 + 0.5 * is_older) * load_units)
            beta = max(1e-6, beta)

            # Stickiness bias towards last action in this state
            stick_mag = stick_age * (1.0 + 0.5 * is_older) / (1.0 + load_units)
            stick_vec = np.zeros(nA)
            if 0 <= last_action[s] < nA:
                stick_vec[last_action[s]] = stick_mag

            # Softmax over (Q + stickiness logits)
            logits = beta * (Q[s, :] - np.max(Q[s, :])) + stick_vec
            p = np.exp(logits - np.max(logits))
            p = p / np.sum(p)

            # Apply lapse
            p = (1.0 - lapse) * p + lapse * (np.ones(nA) / nA)

            if 0 <= a < nA:
                pa = float(np.clip(p[a], 1e-12, 1.0))
                total_logp += np.log(pa)
            else:
                total_logp += np.log(1.0 / nA)
                # Do not update on invalid action
                continue

            # Q-learning update if feedback valid
            if r >= 0.0:
                Q[s, a] += alpha * (r - Q[s, a])
                last_action[s] = a

    return -float(total_logp)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Capacity-limited WM gating over RL with age-dependent capacity.

    The model assumes:
      - A limited-capacity WM that can store perfect action mappings for up to K states.
        WM is implemented as an LRU cache: states recently rewarded enter/refresh the cache.
      - States inside the WM cache are chosen using a near-deterministic WM policy;
        states outside rely on RL (Q-learning + softmax).
      - Older adults have effectively smaller K via an age penalty.
      - Larger set sizes increase the chance that a state is outside WM and thus rely on RL.

    Parameters
    ----------
    states : np.ndarray of int
        State at each trial (0..nS-1 within a block).
    actions : np.ndarray of int
        Chosen action at each trial (0..2); out-of-range treated as lapse.
    rewards : np.ndarray of float
        Feedback (0 or 1); negative values treated as invalid for updating.
    blocks : np.ndarray of int
        Block identifier per trial.
    set_sizes : np.ndarray of int
        Set size per trial (3 or 6).
    age : np.ndarray of float
        Array with participant age (use age[0]); age>=45 => older group.
    model_parameters : sequence of float
        [alpha, beta, K0, age_penalty, lapse]
        - alpha: Q-learning rate.
        - beta: inverse temperature for RL softmax when WM not available.
        - K0: baseline WM capacity (in number of states).
        - age_penalty: how much capacity is reduced for older adults (can be fractional).
        - lapse: lapse probability; with prob 'lapse' action is uniform.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, K0, age_penalty, lapse = model_parameters
    age_val = float(age[0])
    is_older = 1.0 if age_val >= 45.0 else 0.0

    nA = 3
    total_logp = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        b_states = states[idx]
        b_actions = actions[idx]
        b_rewards = rewards[idx]
        b_set_sizes = set_sizes[idx]
        nS = int(b_set_sizes[0])

        # RL store
        Q = (1.0 / nA) * np.ones((nS, nA))

        # WM cache: for each state in cache, store best action; maintain LRU order
        wm_action = -np.ones(nS, dtype=int)
        # LRU timestamps: larger value means more recent
        timestamp = np.zeros(nS, dtype=float)
        tcount = 0.0

        # Effective capacity for this block
        K_eff = K0 - age_penalty * is_older
        K_eff = max(0.0, min(float(nS), float(K_eff)))

        for t in range(len(b_states)):
            s = int(b_states[t])
            a = int(b_actions[t])
            r = float(b_rewards[t])
            ss = int(b_set_sizes[t])

            # WM available if s has an entry and capacity > 0
            in_wm = (K_eff > 0.0) and (wm_action[s] >= 0)

            # WM policy if available: choose stored action with high prob, small smoothing
            if in_wm:
                p_wm = np.ones(nA) * (0.05 / (nA - 1))
                p_wm[wm_action[s]] = 0.95
                p_policy = p_wm
            else:
                # RL softmax
                q_s = Q[s, :]
                logits = beta * (q_s - np.max(q_s))
                p_rl = np.exp(logits)
                p_rl = p_rl / np.sum(p_rl)
                p_policy = p_rl

            # Apply lapse
            p_final = (1.0 - lapse) * p_policy + lapse * (np.ones(nA) / nA)

            # Likelihood
            if 0 <= a < nA:
                pa = float(np.clip(p_final[a], 1e-12, 1.0))
                total_logp += np.log(pa)
            else:
                total_logp += np.log(1.0 / nA)
                # Skip updates on invalid action
                continue

            # Updates if feedback valid
            if r >= 0.0:
                # RL update always occurs
                Q[s, a] += alpha * (r - Q[s, a])

                # WM cache management: only when reward is positive, candidate to store/refresh
                tcount += 1.0
                if r > 0.0 and K_eff > 0.0:
                    # If already in WM, refresh; else insert and possibly evict LRU
                    if wm_action[s] < 0:
                        # Need free slot? Check current occupancy
                        occupancy = int(np.sum(wm_action >= 0))
                        if occupancy >= int(np.floor(K_eff + 1e-9)):
                            # Evict least recently used
                            # Only consider states currently in WM
                            in_cache_idx = np.where(wm_action >= 0)[0]
                            if in_cache_idx.size > 0:
                                lru_state = int(in_cache_idx[np.argmin(timestamp[in_cache_idx])])
                                wm_action[lru_state] = -1
                                timestamp[lru_state] = 0.0
                        # Insert s
                        wm_action[s] = a
                        timestamp[s] = tcount
                    else:
                        # Refresh and possibly correct stored action
                        wm_action[s] = a
                        timestamp[s] = tcount
                elif r <= 0.0 and in_wm:
                    # On consistent non-reward, slightly erode WM trust by demoting entry
                    # We simulate erosion by probabilistically removing the entry when
                    # RL suggests a different best action strongly.
                    best_a = int(np.argmax(Q[s, :]))
                    if best_a != wm_action[s] and (Q[s, best_a] - Q[s, wm_action[s]]) > 0.2:
                        wm_action[s] = -1
                        timestamp[s] = 0.0

    return -float(total_logp)