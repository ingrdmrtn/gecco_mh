def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Bayesian confidence-weighted learner with age- and load-modulated exploration bonus and lapse.

    Mechanism
    - For each state-action, keep Bayesian success/failure pseudo-counts; posterior mean guides choice.
    - Add an uncertainty/exploration bonus that shrinks as evidence accumulates.
    - Older age and larger set size reduce the exploration bonus (consistent with reduced strategic exploration).
    - Add a small lapse component that can capture unmodeled noise; parameter sets the base lapse rate.

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..nS-1 within block).
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of float/int
        Reward per trial (0 or 1).
    blocks : array-like of int
        Block ID per trial; learning resets per block.
    set_sizes : array-like of int
        Set size per trial (3 or 6). Determines number of states in a block.
    age : array-like or scalar
        Participant age; older (>=45) reduces the exploration bonus.
    model_parameters : sequence of 6 floats
        beta            : Inverse temperature for softmax (scaled by 10 internally)
        prior_strength  : Symmetric beta prior pseudo-count per state-action (>=0)
        bonus_scale     : Base scale of the exploration bonus
        age_bonus_drop  : Reduction in bonus for older adults (>=0)
        load_bonus_drop : Reduction in bonus for larger set size (>=0)
        lapse0          : Base lapse probability (0..1), applies per choice

    Returns
    -------
    nll : float
        Negative log-likelihood of observed actions.
    """
    beta, prior_strength, bonus_scale, age_bonus_drop, load_bonus_drop, lapse0 = model_parameters
    beta = beta * 10.0

    try:
        age_val = float(age[0])
    except Exception:
        age_val = float(age)
    is_older = 1.0 if age_val >= 45 else 0.0

    nA = 3
    eps = 1e-12
    total_log_p = 0.0

    states = np.asarray(states)
    actions = np.asarray(actions)
    rewards = np.asarray(rewards, dtype=float)
    blocks = np.asarray(blocks)
    set_sizes = np.asarray(set_sizes)

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        nS = int(set_sizes[idx][0])

        # Success/failure counts per state-action
        succ = np.zeros((nS, nA))
        fail = np.zeros((nS, nA))

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Posterior mean for reward probability per action
            alpha0 = prior_strength
            beta0 = prior_strength
            denom = (alpha0 + beta0) + succ[s, :] + fail[s, :]
            post_mean = (alpha0 + succ[s, :]) / np.maximum(denom, eps)

            # Uncertainty bonus ~ 1/sqrt(N) scaled, with age and load reductions
            N_sa = (succ[s, :] + fail[s, :]) + (alpha0 + beta0)
            base_bonus = bonus_scale / np.sqrt(np.maximum(N_sa, 1.0))
            load_factor = 1.0 - load_bonus_drop * max(0.0, (nS - 3) / 3.0)
            age_factor = 1.0 - age_bonus_drop * is_older
            bonus = np.maximum(0.0, base_bonus * age_factor * load_factor)

            prefs = post_mean + bonus
            prefs = prefs - np.max(prefs)
            prob = np.exp(beta * prefs)
            prob = prob / (np.sum(prob) + eps)

            # Lapse: mix with uniform
            lapse = min(max(lapse0, 0.0), 1.0)
            p_final = (1.0 - lapse) * prob + lapse * (1.0 / nA)

            p_a = p_final[a]
            total_log_p += np.log(max(p_a, eps))

            # Update counts only for chosen action
            succ[s, a] += r
            fail[s, a] += (1.0 - r)

    return -float(total_log_p)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + episodic one-shot memory with age/load-modulated retrieval and recency-weighted gating.

    Mechanism
    - Model-based episodic memory (WM-like): remembers the most recent rewarded action for a state.
      Its influence decays with recency.
    - RL (Q-learning) provides gradual value learning.
    - Arbitration: the weight on WM equals retrieval_probability * recency_strength.
      Retrieval probability decreases with older age and with larger set size.
    - The mixture policy combines a deterministic WM proposal (if present) with the RL softmax.

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..nS-1 within block).
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of float/int
        Reward per trial (0 or 1).
    blocks : array-like of int
        Block ID per trial; learning resets per block.
    set_sizes : array-like of int
        Set size per trial (3 or 6). Determines number of states in a block.
    age : array-like or scalar
        Participant age; older (>=45) reduces retrieval probability.
    model_parameters : sequence of 6 floats
        alpha           : Q-learning rate (0..1)
        beta            : RL inverse temperature (scaled by 10 internally)
        mem_intercept   : Intercept of logistic retrieval probability
        mem_age_slope   : Effect of older age (>=45) on retrieval (negative reduces retrieval)
        mem_load_slope  : Effect of load (nS-3) on retrieval (negative reduces retrieval)
        recency_decay   : Exponential decay rate of WM strength per intervening occurrence of the state

    Returns
    -------
    nll : float
        Negative log-likelihood of observed actions.
    """
    alpha, beta, mem_intercept, mem_age_slope, mem_load_slope, recency_decay = model_parameters
    beta = beta * 10.0

    try:
        age_val = float(age[0])
    except Exception:
        age_val = float(age)
    is_older = 1.0 if age_val >= 45 else 0.0

    nA = 3
    eps = 1e-12
    total_log_p = 0.0

    states = np.asarray(states)
    actions = np.asarray(actions)
    rewards = np.asarray(rewards, dtype=float)
    blocks = np.asarray(blocks)
    set_sizes = np.asarray(set_sizes)

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        nS = int(set_sizes[idx][0])

        # RL values
        Q = (1.0 / nA) * np.ones((nS, nA))

        # Episodic memory: for each state, store last rewarded action and a recency counter
        last_rewarded_action = -1 * np.ones(nS, dtype=int)
        since_seen_counter = np.zeros(nS)  # counts since last time the state was observed
        since_reward_counter = np.full(nS, np.inf)  # counts since last rewarded action in that state

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Update recency counters
            since_seen_counter += 1.0
            since_seen_counter[s] = 0.0
            since_reward_counter += 1.0

            # RL softmax
            prefs = Q[s, :] - np.max(Q[s, :])
            p_rl = np.exp(beta * prefs)
            p_rl = p_rl / (np.sum(p_rl) + eps)

            # WM retrieval probability depends on age and load
            z = mem_intercept + mem_age_slope * is_older + mem_load_slope * (nS - 3)
            p_retrieve = 1.0 / (1.0 + np.exp(-z))
            p_retrieve = min(max(p_retrieve, 0.0), 1.0)

            # Recency-based WM strength (if a rewarded memory exists)
            s_mem = 0.0
            wm_action = last_rewarded_action[s]
            if wm_action >= 0 and np.isfinite(since_reward_counter[s]):
                s_mem = np.exp(-recency_decay * since_reward_counter[s])
            gate = p_retrieve * s_mem
            gate = min(max(gate, 0.0), 1.0)

            # WM policy: propose remembered rewarded action deterministically if exists; else uniform
            if wm_action >= 0 and s_mem > 0.0:
                p_wm = np.zeros(nA)
                p_wm[wm_action] = 1.0
            else:
                p_wm = np.ones(nA) / nA

            p_mix = gate * p_wm + (1.0 - gate) * p_rl
            p_a = p_mix[a]
            total_log_p += np.log(max(p_a, eps))

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # Update episodic memory if reward obtained
            if r > 0.0:
                last_rewarded_action[s] = a
                since_reward_counter[s] = 0.0

    return -float(total_log_p)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Q-learning with age- and load-dependent cross-state interference (maladaptive generalization).

    Mechanism
    - Standard Q-learning for chosen state-action.
    - After each update, value "bleeds" to the same action in other states (interference/generalization).
      The interference rate increases with older age and larger set size, capturing greater susceptibility
      to interference under cognitive load and aging.
    - Interference strength controls how strongly values are pulled toward the chosen value across states.

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..nS-1 within block).
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of float/int
        Reward per trial (0 or 1).
    blocks : array-like of int
        Block ID per trial; learning resets per block.
    set_sizes : array-like of int
        Set size per trial (3 or 6). Determines number of states in a block.
    age : array-like or scalar
        Participant age; older (>=45) increases interference.
    model_parameters : sequence of 6 floats
        alpha        : Learning rate for Q-learning (0..1)
        beta         : Inverse temperature for softmax (scaled by 10 internally)
        eta          : Interference strength (0..1), scaling size of cross-state update
        rho0         : Interference intercept (logit space)
        rho_age_gain : Additive increase in interference for older adults (logit space)
        rho_load_gain: Additive increase in interference with load nS-3 (logit space)

    Returns
    -------
    nll : float
        Negative log-likelihood of observed actions.
    """
    alpha, beta, eta, rho0, rho_age_gain, rho_load_gain = model_parameters
    beta = beta * 10.0

    try:
        age_val = float(age[0])
    except Exception:
        age_val = float(age)
    is_older = 1.0 if age_val >= 45 else 0.0

    nA = 3
    eps = 1e-12
    total_log_p = 0.0

    states = np.asarray(states)
    actions = np.asarray(actions)
    rewards = np.asarray(rewards, dtype=float)
    blocks = np.asarray(blocks)
    set_sizes = np.asarray(set_sizes)

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        nS = int(set_sizes[idx][0])

        Q = (1.0 / nA) * np.ones((nS, nA))

        # Determine interference probability (expected-rate used deterministically)
        z = rho0 + rho_age_gain * is_older + rho_load_gain * (nS - 3)
        rho = 1.0 / (1.0 + np.exp(-z))
        rho = min(max(rho, 0.0), 1.0)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Choice softmax
            prefs = Q[s, :] - np.max(Q[s, :])
            p = np.exp(beta * prefs)
            p = p / (np.sum(p) + eps)

            p_a = p[a]
            total_log_p += np.log(max(p_a, eps))

            # Standard Q-learning update
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # Cross-state interference: move other states' Q for action a toward Q[s,a]
            if nS > 1 and eta > 0.0 and rho > 0.0:
                target = Q[s, a]
                # Expected interference effect scaled by rho
                delta_scale = eta * rho
                for s2 in range(nS):
                    if s2 == s:
                        continue
                    Q[s2, a] += delta_scale * (target - Q[s2, a])

    return -float(total_log_p)