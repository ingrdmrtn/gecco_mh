def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with action-credit interference (load- and age-modulated).

    Mechanism
    - Tabular Q-learning with softmax policy (3 actions).
    - When a reward prediction error (RPE) is computed, a fraction of that credit
      is "misassigned" to the nonchosen actions in the same state, capturing
      action-credit interference. Interference rises with set size (load) and for
      older adults.
    - Age group: younger (<45) vs older (>=45) modifies the interference level.

    Parameters
    ----------
    states : array-like of int
        State index per trial (within-block indexing: 0..nS-1).
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of {0,1}
        Feedback per trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (3 or 6 for this task).
    age : array-like
        Participant age; uses age[0] to set age group: older (>=45)=1, younger=0.
    model_parameters : sequence of 5 floats
        [alpha, beta, interference_base, load_interference_gain, age_interference_gain]
        - alpha: learning rate for Q updates, passed through sigmoid to [0,1].
        - beta: inverse temperature for softmax; constrained to >=1e-6.
        - interference_base: baseline log-odds for credit interference.
        - load_interference_gain: positive gain for larger set sizes (more interference at nS=6).
        - age_interference_gain: nonnegative increment applied if older (higher interference).

    Returns
    -------
    nll : float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta, interference_base, load_interference_gain, age_interference_gain = model_parameters

    # Parameter transforms and constraints
    alpha = 1.0 / (1.0 + np.exp(-alpha))
    beta = np.maximum(beta, 1e-6)
    load_interference_gain = np.maximum(load_interference_gain, 0.0)
    age_interference_gain = np.maximum(age_interference_gain, 0.0)

    age_group = 1 if age[0] >= 45 else 0
    nA = 3
    nll = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]
        nS = int(block_set_sizes[0])

        # Initialize Q-values uniformly
        Q = (1.0 / nA) * np.ones((nS, nA))

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            nS_t = int(block_set_sizes[t])

            # Softmax policy
            logits = beta * (Q[s, :] - np.max(Q[s, :]))
            p = np.exp(logits)
            p /= np.maximum(1e-12, np.sum(p))
            p_a = np.clip(p[a], 1e-12, 1.0)
            nll -= np.log(p_a)

            # RPE
            delta = r - Q[s, a]

            # Interference rate (probability mass to spread to nonchosen actions)
            # Increases with load (nS=6) and for older adults
            load_term = (float(nS_t) - 3.0) / 3.0  # 0 for 3, 1 for 6
            inter_logit = interference_base + load_interference_gain * load_term + age_group * age_interference_gain
            c = 1.0 / (1.0 + np.exp(-inter_logit))  # in [0,1]

            # Credit assignment: (1-c) to chosen action; c spread equally to the other actions
            assign = np.full(nA, c / (nA - 1))
            assign[a] = 1.0 - c

            # Update Q-values for the state s
            Q[s, :] += alpha * delta * assign

    return nll


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with load- and age-modulated forgetting and temperature.

    Mechanism
    - Tabular Q-learning with softmax choice.
    - Between-trial forgetting toward a neutral prior (uniform over actions), whose
      magnitude increases with set size and (to a lesser extent) with older age.
    - Action selection temperature (inverse temp) is reduced under higher load and
      in older age, capturing noisier choices under cognitive strain.
    - Age group: younger (<45) vs older (>=45) affects both forgetting and temperature.

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..nS-1 within block).
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of {0,1}
        Feedback per trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (3 or 6).
    age : array-like
        Participant age; uses age[0] to define age group (older >=45).
    model_parameters : sequence of 5 floats
        [alpha, beta, forget_base, load_forget_gain, age_temp_shift]
        - alpha: learning rate (sigmoid to [0,1]).
        - beta: base inverse temperature, >=1e-6.
        - forget_base: baseline log-odds for forgetting magnitude.
        - load_forget_gain: positive gain increasing forgetting with larger set size.
        - age_temp_shift: nonnegative factor that reduces inverse temperature for older group;
                          also mildly increases forgetting if older.

    Returns
    -------
    nll : float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta, forget_base, load_forget_gain, age_temp_shift = model_parameters

    alpha = 1.0 / (1.0 + np.exp(-alpha))
    beta = np.maximum(beta, 1e-6)
    load_forget_gain = np.maximum(load_forget_gain, 0.0)
    age_temp_shift = np.maximum(age_temp_shift, 0.0)

    age_group = 1 if age[0] >= 45 else 0
    nA = 3
    nll = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]
        nS = int(block_set_sizes[0])

        # Initialize Q-values to uniform prior
        Q = (1.0 / nA) * np.ones((nS, nA))
        prior = (1.0 / nA) * np.ones(nA)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            nS_t = int(block_set_sizes[t])

            # Compute forgetting magnitude f in [0,1] (toward uniform prior)
            load_term = (float(nS_t) - 3.0) / 3.0  # 0 at 3, 1 at 6
            f_logit = forget_base + load_forget_gain * load_term + 0.5 * age_group * age_temp_shift
            f = 1.0 / (1.0 + np.exp(-f_logit))

            # Apply forgetting to the entire Q table or to the current state's row?
            # Here we apply a mild global forgetting per trial to capture ongoing interference.
            Q = (1.0 - f) * Q + f * prior  # row-wise broadcast toward uniform

            # Effective inverse temperature reduced by load and age
            beta_eff = beta * (1.0 - 0.5 * load_forget_gain * load_term)
            beta_eff *= (1.0 - age_group * age_temp_shift)
            beta_eff = np.maximum(beta_eff, 1e-6)

            # Softmax policy for current state
            logits = beta_eff * (Q[s, :] - np.max(Q[s, :]))
            p = np.exp(logits)
            p /= np.maximum(1e-12, np.sum(p))
            p_a = np.clip(p[a], 1e-12, 1.0)
            nll -= np.log(p_a)

            # Standard Q-learning update for chosen action
            delta = r - Q[s, a]
            Q[s, a] += alpha * delta

    return nll


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + rule-based heuristic mixture with load- and age-dependent arbitration.

    Mechanism
    - RL: Tabular Q-learning with softmax.
    - Rule heuristic: deterministically predicts action = state mod 3 within a block
      (a simple chunking/labeling strategy that is easy to apply at low load).
    - Arbitration: Mixture weight on the rule policy increases at low set size and
      decreases for older adults; the remainder weight goes to RL.
    - Age group: younger (<45)=0, older (>=45)=1 penalizes heuristic reliance.

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..nS-1 within block).
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of {0,1}
        Feedback per trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (3 or 6).
    age : array-like
        Participant age; uses age[0] to define age group.
    model_parameters : sequence of 5 floats
        [alpha, beta, rule_bias, load_rule_gain, age_rule_penalty]
        - alpha: Q-learning rate (sigmoid to [0,1]).
        - beta: inverse temperature for RL softmax (>=1e-6).
        - rule_bias: baseline log-odds of using the heuristic rule.
        - load_rule_gain: increases rule weight at low load (nS=3 vs 6).
        - age_rule_penalty: nonnegative reduction to rule weight for older group.

    Returns
    -------
    nll : float
        Negative log-likelihood of the observed choices under the mixture.
    """
    alpha, beta, rule_bias, load_rule_gain, age_rule_penalty = model_parameters

    alpha = 1.0 / (1.0 + np.exp(-alpha))
    beta = np.maximum(beta, 1e-6)
    load_rule_gain = np.maximum(load_rule_gain, 0.0)
    age_rule_penalty = np.maximum(age_rule_penalty, 0.0)

    age_group = 1 if age[0] >= 45 else 0
    nA = 3
    nll = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]
        nS = int(block_set_sizes[0])

        Q = (1.0 / nA) * np.ones((nS, nA))

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            nS_t = int(block_set_sizes[t])

            # RL policy
            logits = beta * (Q[s, :] - np.max(Q[s, :]))
            p_rl = np.exp(logits)
            p_rl /= np.maximum(1e-12, np.sum(p_rl))

            # Rule policy: action = s mod nA
            rule_action = s % nA
            p_rule = np.zeros(nA)
            p_rule[rule_action] = 1.0

            # Arbitration weight: higher at low load, reduced for older adults
            load_term = (3.0 - float(nS_t)) / 3.0  # 0 for 3? Here 0 when nS=3? Let's make + at low load
            # Recompute to ensure positive at low load: if nS=3 => +1, if nS=6 => 0
            load_low = 1.0 if nS_t == 3 else 0.0
            w_logit = rule_bias + load_rule_gain * load_low - age_group * age_rule_penalty
            w_rule = 1.0 / (1.0 + np.exp(-w_logit))

            p_mix = w_rule * p_rule + (1.0 - w_rule) * p_rl
            p_a = np.clip(p_mix[a], 1e-12, 1.0)
            nll -= np.log(p_a)

            # RL update
            delta = r - Q[s, a]
            Q[s, a] += alpha * delta

    return nll