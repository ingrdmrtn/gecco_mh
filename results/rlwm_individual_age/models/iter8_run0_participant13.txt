def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with Pearce-Hall-like state-specific attention: learning rates adapt to surprise,
    modulated by set size and age.

    The model maintains Q-values per state and action, but the effective learning rate
    per state is dynamically updated via an attention/associability variable that
    tracks the magnitude of recent prediction errors. Attention is reduced under higher
    set size and for older adults (age >= 45). A small lapse rate mixes in a uniform
    policy.

    Parameters
    ----------
    states : array-like of int
        State index per trial within block (0..nS-1).
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of float/int
        Binary reward (0 or 1).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size for the block corresponding to each trial (3 or 6).
    age : array-like of float
        Participant age; age[0] used to define group (older >= 45).
    model_parameters : tuple/list of floats
        (alpha0, beta, phi, age_att_shift, size_att_scale, lapse)
        - alpha0: base learning rate (squashed to 0..1)
        - beta: inverse temperature for softmax policy (scaled internally)
        - phi: learning rate for attention/associability update (squashed 0..1)
        - age_att_shift: additive shift to attention dynamics for older group (>=45);
                         increases or decreases attention update strength
        - size_att_scale: exponent controlling how attention scales with set size
                          via factor (3/nS)^size_att_scale
        - lapse: lapse probability mixed with uniform policy (squashed to 0..0.3)

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha0, beta, phi, age_att_shift, size_att_scale, lapse = model_parameters

    # Parameter transformations
    alpha0 = 1.0 / (1.0 + np.exp(-alpha0))
    beta = (1.0 / (1.0 + np.exp(-beta))) * 12.0
    phi = 1.0 / (1.0 + np.exp(-phi))
    lapse = (1.0 / (1.0 + np.exp(-lapse))) * 0.3
    size_att_scale = size_att_scale  # free (can be negative or positive)

    age_val = float(age[0])
    older = 1 if age_val >= 45 else 0

    nll = 0.0
    eps = 1e-12
    nA = 3

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx].astype(int)
        block_actions = actions[idx].astype(int)
        block_rewards = rewards[idx].astype(float)
        nS = int(set_sizes[idx][0])

        # Initialize Q and attention per state
        Q = np.zeros((nS, nA))
        att = np.ones(nS) * 0.5  # initial moderate associability

        # Block-level scaling for attention due to load and age
        load_scale = (3.0 / float(nS)) ** size_att_scale
        phi_eff = np.clip(phi + age_att_shift * older, 0.0, 1.0)

        for t in range(len(block_states)):
            s = block_states[t]
            a = block_actions[t]
            r = block_rewards[t]

            # Softmax over Q
            q = Q[s, :]
            q = q - np.max(q)
            pi = np.exp(beta * q)
            pi = pi / (np.sum(pi) + eps)

            # Lapse mixture
            pi = (1.0 - lapse) * pi + lapse * (1.0 / nA)

            # Likelihood
            p = max(pi[a], eps)
            nll -= np.log(p)

            # RL update with dynamic learning rate = alpha0 * attention * load_scale
            pe = r - Q[s, a]
            alpha_eff = np.clip(alpha0 * att[s] * load_scale, 0.0, 1.0)
            Q[s, a] += alpha_eff * pe

            # Update associability (Pearce-Hall): tracks absolute PE, with age-modulated dynamics
            att[s] = (1.0 - phi_eff) * att[s] + phi_eff * abs(pe)

    return nll


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Actor-Critic with separate Go/NoGo action channels, age-modulated NoGo learning,
    set-size-scaled learning, and choice inertia bias.

    The policy is based on an action preference A = Go - NoGo + bias, where Go and
    NoGo values are learned from a critic's prediction error. Positive PE strengthens
    Go for chosen action; negative PE strengthens NoGo. Older adults (>=45) have
    amplified NoGo learning. Learning rates are reduced under higher set sizes.
    A perseverative bias favors repeating the last action in the same state.

    Parameters
    ----------
    states : array-like of int
        State index per trial within block (0..nS-1).
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of float/int
        Binary reward (0 or 1).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size for the block corresponding to each trial (3 or 6).
    age : array-like of float
        Participant age; age[0] used to define group (older >= 45).
    model_parameters : tuple/list of floats
        (alpha, beta, nogo_age_gain, size_lr_scale, inertia_bias, lapse)
        - alpha: base learning rate for actor and critic (squashed 0..1)
        - beta: inverse temperature for softmax over preferences (scaled internally)
        - nogo_age_gain: multiplicative gain on NoGo learning in older group (>=45)
        - size_lr_scale: exponent controlling load scaling of learning via (3/nS)^size_lr_scale
        - inertia_bias: strength of bias to repeat the last action in the same state
                        (adds to chosen action preference; can be positive/negative)
        - lapse: lapse probability mixed with uniform policy (squashed to 0..0.3)

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, nogo_age_gain, size_lr_scale, inertia_bias, lapse = model_parameters

    alpha = 1.0 / (1.0 + np.exp(-alpha))
    beta = (1.0 / (1.0 + np.exp(-beta))) * 12.0
    lapse = (1.0 / (1.0 + np.exp(-lapse))) * 0.3
    size_lr_scale = size_lr_scale  # free
    nogo_age_gain = np.exp(nogo_age_gain)  # positive multiplicative factor
    inertia_bias = inertia_bias  # can be positive or negative

    age_val = float(age[0])
    older = 1 if age_val >= 45 else 0

    nll = 0.0
    eps = 1e-12
    nA = 3

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx].astype(int)
        block_actions = actions[idx].astype(int)
        block_rewards = rewards[idx].astype(float)
        nS = int(set_sizes[idx][0])

        # Actor: Go and NoGo channels; Critic: state-action Q for PE computation
        G = np.zeros((nS, nA))
        N = np.zeros((nS, nA))
        Q = np.zeros((nS, nA))

        # State-wise last action for inertia
        last_action = -np.ones(nS, dtype=int)

        # Load-based learning rate scaling
        lr_scale = (3.0 / float(nS)) ** size_lr_scale
        alpha_eff = np.clip(alpha * lr_scale, 0.0, 1.0)
        nogo_gain_eff = 1.0 + (nogo_age_gain - 1.0) * older

        for t in range(len(block_states)):
            s = block_states[t]
            a = block_actions[t]
            r = block_rewards[t]

            # Preferences: Go - NoGo + inertia toward last action
            pref = G[s, :] - N[s, :]
            if last_action[s] >= 0:
                pref[last_action[s]] += inertia_bias

            # Softmax policy over preferences
            pref = pref - np.max(pref)
            pi = np.exp(beta * pref)
            pi = pi / (np.sum(pi) + eps)

            # Lapse mixture
            pi = (1.0 - lapse) * pi + lapse * (1.0 / nA)

            p = max(pi[a], eps)
            nll -= np.log(p)

            # Critic PE
            pe = r - Q[s, a]
            Q[s, a] += alpha_eff * pe

            # Actor updates
            if pe >= 0.0:
                # Positive PE -> strengthen Go for chosen action
                G[s, a] += alpha_eff * pe
            else:
                # Negative PE -> strengthen NoGo for chosen action (age-modulated)
                N[s, a] += alpha_eff * (-pe) * nogo_gain_eff

            # Update inertia memory
            last_action[s] = a

    return nll


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Bayesian Working Memory with leaky Dirichlet counts, load-limited fidelity, and age effect.

    The model maintains Dirichlet-like counts over actions for each state, which are
    leaky (decay toward a symmetric prior each trial). The posterior mean over actions
    in the current state defines a WM policy via softmax over log-probabilities.
    Memory fidelity is reduced under higher set sizes and additionally reduced for
    older adults (>=45). A lapse rate mixes in a uniform policy.

    Feedback update rule:
    - If rewarded: increment chosen action's count.
    - If unrewarded: increment all non-chosen actions equally (evidence that chosen is wrong).

    Parameters
    ----------
    states : array-like of int
        State index per trial within block (0..nS-1).
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of float/int
        Binary reward (0 or 1).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size for the block corresponding to each trial (3 or 6).
    age : array-like of float
        Participant age; age[0] used to define group (older >= 45).
    model_parameters : tuple/list of floats
        (beta_wm, conc0, m_base, age_m_slope, K_cap, lapse)
        - beta_wm: inverse temperature applied to log posterior over actions (scaled internally)
        - conc0: symmetric prior concentration per action (exp-transformed >0)
        - m_base: base memory fidelity before load/age scaling (squashed 0..1)
        - age_m_slope: proportional reduction of fidelity for older group (>=45), clipped to [0,1]
        - K_cap: effective capacity parameter (exp-transformed, in "items"); fidelity scaled by min(1, K_cap/nS)
        - lapse: lapse probability mixed with uniform policy (squashed to 0..0.3)

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    beta_wm, conc0, m_base, age_m_slope, K_cap, lapse = model_parameters

    beta_wm = (1.0 / (1.0 + np.exp(-beta_wm))) * 15.0
    conc0 = np.exp(conc0)  # > 0
    m_base = 1.0 / (1.0 + np.exp(-m_base))  # 0..1
    age_m_slope = np.clip(age_m_slope, 0.0, 1.0)  # reduction factor in older group
    K_cap = np.exp(K_cap)  # > 0
    lapse = (1.0 / (1.0 + np.exp(-lapse))) * 0.3

    age_val = float(age[0])
    older = 1 if age_val >= 45 else 0

    nll = 0.0
    eps = 1e-12
    nA = 3

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx].astype(int)
        block_actions = actions[idx].astype(int)
        block_rewards = rewards[idx].astype(float)
        nS = int(set_sizes[idx][0])

        # Initialize Dirichlet counts per state with symmetric prior
        C = np.ones((nS, nA)) * conc0

        # Compute memory fidelity for this block
        load_factor = min(1.0, K_cap / float(nS))
        age_factor = (1.0 - age_m_slope * older)
        m_eff = np.clip(m_base * load_factor * age_factor, 0.0, 1.0)
        # Leak toward prior: d = 1 - m_eff (larger d => stronger leak)
        decay = 1.0 - m_eff

        for t in range(len(block_states)):
            s = block_states[t]
            a = block_actions[t]
            r = block_rewards[t]

            # Posterior mean for current state
            c_sum = np.sum(C[s, :]) + eps
            p_act = C[s, :] / c_sum

            # Softmax over log probabilities (equivalent to power transform)
            logits = np.log(p_act + eps)
            logits -= np.max(logits)
            pi = np.exp(beta_wm * logits)
            pi = pi / (np.sum(pi) + eps)

            # Lapse mixture
            pi = (1.0 - lapse) * pi + lapse * (1.0 / nA)

            p = max(pi[a], eps)
            nll -= np.log(p)

            # Leaky decay toward symmetric prior
            C[s, :] = (1.0 - decay) * C[s, :] + decay * conc0

            # Feedback-driven increment
            if r > 0.0:
                C[s, a] += 1.0
            else:
                inc_other = 1.0 / (nA - 1)
                for aa in range(nA):
                    if aa != a:
                        C[s, aa] += inc_other

    return nll