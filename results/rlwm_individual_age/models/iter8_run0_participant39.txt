def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Hybrid RL + episodic trace with capacity-based arbitration modulated by age and set size.

    Mechanism:
    - RL: Q-learning over state-action values with softmax choice.
    - Episodic trace: a fast-decaying memory of recent rewarded outcomes for each state-action.
    - Arbitration: mixture weight favoring episodic trace when effective capacity exceeds set size.
      Capacity declines with older age. Arbitration is a smooth logistic function.
    
    Parameters
    ----------
    states : array-like of int
        State indices per trial within each block (0..set_size-1).
    actions : array-like of int
        Chosen actions per trial (0..2).
    rewards : array-like of float/int
        Binary rewards per trial (0 or 1).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (e.g., 3 or 6).
    age : array-like of float
        Participant age; age[0] is used. Age >=45 treated as older.
    model_parameters : tuple/list of floats
        (alpha, beta, cap0, eta, psi)
        - alpha: RL learning rate for Q updates.
        - beta: inverse temperature for softmax (both RL and episodic policies).
        - cap0: baseline working/episodic capacity proxy (arbitration pivot).
        - eta: episodic trace learning/decay rate.
        - psi: arbitration steepness; higher means sharper switch between episodic and RL.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, cap0, eta, psi = model_parameters
    age_val = float(age[0])
    is_older = 1.0 if age_val >= 45.0 else 0.0

    total_loglik = 0.0
    nA = 3
    eps = 1e-12

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nS = int(block_set_sizes[0])

        # Initialize RL values and episodic traces
        Q = (1.0 / nA) * np.ones((nS, nA))
        M = np.zeros((nS, nA))  # episodic trace starts neutral

        # Effective capacity reduced with age (older -> lower capacity)
        # and used in arbitration against current set size.
        cap_eff_base = cap0 * (1.0 - 0.35 * is_older)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            ss = float(block_set_sizes[t])

            # RL policy
            q_s = Q[s, :]
            prefs_rl = beta * (q_s - np.max(q_s))
            pi_rl = np.exp(prefs_rl)
            pi_rl = pi_rl / (np.sum(pi_rl) + eps)

            # Episodic trace policy (emphasizes recently rewarded actions)
            m_s = M[s, :]
            prefs_mem = beta * (m_s - np.max(m_s))
            pi_mem = np.exp(prefs_mem)
            pi_mem = pi_mem / (np.sum(pi_mem) + eps)

            # Arbitration weight: logistic of (capacity - set size)
            # Larger set sizes and older age reduce reliance on episodic trace.
            w_mem = 1.0 / (1.0 + np.exp(-psi * (cap_eff_base - ss)))
            pi = w_mem * pi_mem + (1.0 - w_mem) * pi_rl

            total_loglik += np.log(pi[a] + eps)

            # RL update
            pe = r - Q[s, a]
            Q[s, a] = Q[s, a] + alpha * pe

            # Episodic trace update: decay and add recent outcome at chosen action
            # Faster traces in small sets; slow down with larger set size and older age.
            # Implement by scaling effective eta inversely with set size and age.
            eta_eff = eta * (3.0 / ss) * (1.0 - 0.25 * is_older)
            eta_eff = np.clip(eta_eff, 0.0, 1.0)
            # Decay towards zero (neutral), then add current reward to chosen action
            M[s, :] = (1.0 - eta_eff) * M[s, :]
            M[s, a] += eta_eff * r

    return -float(total_loglik)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Valence-asymmetric RL with age- and load-modulated choice stickiness.

    Mechanism:
    - Separate learning rates for positive vs. negative outcomes.
    - A choice kernel (stickiness) that biases repeating the last chosen action in each state.
      Stickiness grows in older adults and in smaller set sizes (less interference).
    
    Parameters
    ----------
    states : array-like of int
        State indices per trial within each block (0..set_size-1).
    actions : array-like of int
        Chosen actions per trial (0..2).
    rewards : array-like of float/int
        Binary rewards per trial (0 or 1).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (e.g., 3 or 6).
    age : array-like of float
        Participant age; age[0] is used. Age >=45 treated as older.
    model_parameters : tuple/list of floats
        (alr_pos, alr_neg, beta, k_load, stick0)
        - alr_pos: learning rate when r=1.
        - alr_neg: learning rate when r=0.
        - beta: inverse temperature for softmax over Q.
        - k_load: exponent controlling how stickiness scales with set size.
        - stick0: baseline stickiness strength.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alr_pos, alr_neg, beta, k_load, stick0 = model_parameters
    age_val = float(age[0])
    is_older = 1.0 if age_val >= 45.0 else 0.0

    total_loglik = 0.0
    nA = 3
    eps = 1e-12

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nS = int(block_set_sizes[0])

        Q = (1.0 / nA) * np.ones((nS, nA))
        prev_action = -1 * np.ones(nS, dtype=int)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            ss = float(block_set_sizes[t])

            # Build choice kernel centered to avoid baseline shifts
            K = np.zeros(nA)
            if prev_action[s] >= 0:
                K[prev_action[s]] = 1.0
                # Distribute negative mass across other actions to keep sum zero
                K[K == 0.0] = -1.0 / (nA - 1)

            # Effective stickiness: stronger in older adults and smaller sets
            stick_eff = stick0 * (1.0 + 0.5 * is_older) * (3.0 / ss) ** np.clip(k_load, -3.0, 3.0)

            prefs = beta * Q[s, :] + stick_eff * K
            prefs = prefs - np.max(prefs)
            pi = np.exp(prefs)
            pi = pi / (np.sum(pi) + eps)

            total_loglik += np.log(pi[a] + eps)

            # RL update with valence-specific learning rates
            pe = r - Q[s, a]
            alpha = alr_pos if r > 0.0 else alr_neg
            Q[s, a] = Q[s, a] + alpha * pe

            prev_action[s] = a

    return -float(total_loglik)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Rule-gated RL: meta-controller blends hypothesis (rule) use with RL,
    with age- and load-dependent rule confidence and lapse.

    Mechanism:
    - RL: Q-learning with softmax.
    - Rule module: stores the last rewarded action per state as a candidate rule.
    - Confidence in the rule is updated by a meta-value V via delta-learning and transformed
      by a logistic to yield mixture weight c. Older age and larger set sizes reduce
      effective learning of V and initial confidence.
    - Final policy: mixture of rule policy and RL policy, with a small lapse.

    Parameters
    ----------
    states : array-like of int
        State indices per trial within each block (0..set_size-1).
    actions : array-like of int
        Chosen actions per trial (0..2).
    rewards : array-like of float/int
        Binary rewards per trial (0 or 1).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (e.g., 3 or 6).
    age : array-like of float
        Participant age; age[0] is used. Age >=45 treated as older.
    model_parameters : tuple/list of floats
        (alpha, beta, theta, p0, xi)
        - alpha: RL learning rate.
        - beta: inverse temperature for softmax over Q.
        - theta: meta-learning rate controlling updates of rule confidence V.
        - p0: baseline initial rule confidence (in logit space).
        - xi: lapse probability mixed with uniform choice (scaled by age and set size indirectly via mixture).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, theta, p0, xi = model_parameters
    age_val = float(age[0])
    is_older = 1.0 if age_val >= 45.0 else 0.0

    total_loglik = 0.0
    nA = 3
    eps = 1e-12

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nS = int(block_set_sizes[0])

        Q = (1.0 / nA) * np.ones((nS, nA))
        rule_action = -1 * np.ones(nS, dtype=int)
        # Meta-value for rule confidence per state (logit space via V -> sigmoid)
        V = np.zeros(nS)

        # Initialize V based on p0, penalized by load and age
        # Transform p0 (logit-like) into an initial V offset per state
        init_offset = p0 - 0.6 * is_older - 0.5 * max(0, nS - 3)
        V[:] = init_offset

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            ss = float(block_set_sizes[t])

            # Compute rule mixture weight c via logistic of V[s]
            c = 1.0 / (1.0 + np.exp(-V[s]))
            # Additional attenuation with larger set sizes
            c = c * (3.0 / ss)

            # RL policy
            q_s = Q[s, :]
            prefs_rl = beta * (q_s - np.max(q_s))
            pi_rl = np.exp(prefs_rl)
            pi_rl = pi_rl / (np.sum(pi_rl) + eps)

            # Rule policy: if rule exists, deterministic to that action, else uniform
            if rule_action[s] >= 0:
                pi_rule = np.zeros(nA)
                pi_rule[rule_action[s]] = 1.0
            else:
                pi_rule = np.ones(nA) / nA
                c = 0.0  # no rule to exploit yet

            # Age-adjusted lapse: older adults slightly higher effective lapse
            xi_eff = np.clip(xi * (1.0 + 0.25 * is_older), 0.0, 0.5)

            pi = (1.0 - xi_eff) * (c * pi_rule + (1.0 - c) * pi_rl) + xi_eff * (np.ones(nA) / nA)
            total_loglik += np.log(pi[a] + eps)

            # RL update
            pe = r - Q[s, a]
            Q[s, a] = Q[s, a] + alpha * pe

            # Rule learning: set/update rule on reward, keep otherwise
            if r > 0.0:
                rule_action[s] = a

            # Meta-value update for rule confidence:
            # Reward supports current rule if chosen action equals stored rule.
            # If no rule exists, treat consistency term as 0.
            consistency = 1.0 if (rule_action[s] >= 0 and a == rule_action[s]) else 0.0
            target = 0.5 * r + 0.5 * consistency  # combine success and consistency
            # Age- and load-modulated meta learning rate
            theta_eff = theta * (3.0 / ss) * (1.0 - 0.3 * is_older)
            V[s] = V[s] + theta_eff * (target - (1.0 / (1.0 + np.exp(-V[s]))))

    return -float(total_loglik)