def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Entropy-gated learning with age- and load-modulated confidence and lapse.

    Mechanism:
    - Model-free Q-learning within each block.
    - Trial-wise learning rate is gated by decision confidence (1 - normalized entropy) computed
      from the current softmax policy over Q.
    - The confidence impact is modulated by age group and set size: younger participants are assumed
      to translate confidence more strongly into learning at low load (set size 3), whereas this
      mapping is dampened at higher load; modulation is controlled by k_age_load.
    - A lapse component mixes in a uniform random policy.

    Parameters
    ----------
    states : array-like of int
        State indices for each trial (0..set_size-1, within block).
    actions : array-like of int
        Chosen actions (0..2) for each trial.
    rewards : array-like of {0,1}
        Binary outcomes for each trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (constant within block; either 3 or 6).
    age : array-like of float
        Participant age; age[0] is used to determine group (<45 younger, >=45 older).
    model_parameters : tuple/list
        (alpha_min, alpha_max, beta, k_age_load, lapse)
        - alpha_min: minimum learning rate (lower bound).
        - alpha_max: maximum learning rate (upper bound).
        - beta: inverse temperature for softmax over Q.
        - k_age_load: modulation strength of confidence-to-learning mapping by age and load.
        - lapse: lapse probability for random responding (0..1).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha_min, alpha_max, beta, k_age_load, lapse = model_parameters
    age_value = age[0]
    is_older = 1 if age_value >= 45 else 0

    nA = 3
    total_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask]
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        nS = int(set_sizes[mask][0])

        Q = (1.0 / nA) * np.ones((nS, nA))

        # Age-load modulation of confidence mapping:
        # - Younger: stronger mapping at low load; Older: weaker, especially at high load.
        # Compute a multiplier m in (0,2) approximately via sigmoid.
        load_factor = (3.0 / float(max(1, nS)))  # 1.0 (nS=3) vs 0.5 (nS=6)
        # Signed factor: younger positive at low load, older negative at high load
        sign = 1.0 if is_older == 0 else -1.0
        m_conf = 1.0 + sign * (2.0 * (1.0 / (1.0 + np.exp(-k_age_load * (load_factor - 0.75)))) - 1.0)
        # m_conf >1 for young at low load; <1 for older, especially when load_factor small.

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Softmax action probabilities from Q
            q_s = Q[s, :]
            q_center = q_s - np.max(q_s)
            p_vec = np.exp(beta * q_center)
            p_vec = p_vec / np.sum(p_vec)

            # Lapse-mixed choice probability
            p_choice = (1.0 - lapse) * p_vec[a] + lapse * (1.0 / nA)
            p_choice = max(p_choice, 1e-12)
            total_log_p += np.log(p_choice)

            # Confidence = 1 - normalized entropy
            entropy = -np.sum(p_vec * np.log(np.clip(p_vec, 1e-12, 1.0)))
            max_entropy = np.log(nA)
            confidence = 1.0 - (entropy / max_entropy)

            # Age- and load-modulated confidence shaping
            conf_shaped = np.clip(confidence ** np.clip(m_conf, 0.1, 3.0), 0.0, 1.0)

            # Dynamic learning rate
            alpha_t = alpha_min + (alpha_max - alpha_min) * conf_shaped

            # Q-learning update
            td = r - Q[s, a]
            Q[s, a] += alpha_t * td

    return -float(total_log_p)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL plus retrieval-noisy associative cache with age- and load-dependent gating and within-state stickiness.

    Mechanism:
    - Standard Q-learning operates throughout.
    - In parallel, a one-shot associative cache stores the most recently rewarded action for each state.
      The cache is consulted probabilistically; consultation probability is age- and load-dependent.
    - Retrieval from cache is noisy; noise increases with set size and for older participants.
    - The policy is a mixture between RL softmax and cache-guided choice, adjusted by retrieval noise.
    - A within-state stickiness bias adds log-odds to repeat the last chosen action in that state.

    Parameters
    ----------
    states : array-like of int
        State indices per trial.
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of {0,1}
        Binary feedback.
    blocks : array-like of int
        Block indices.
    set_sizes : array-like of int
        Set size per trial (constant within block).
    age : array-like of float
        Participant age; age[0] used to set group.
    model_parameters : tuple/list
        (alpha, beta, gate_base, noise_base, stickiness)
        - alpha: learning rate for Q updates.
        - beta: inverse temperature for RL softmax (also applied to stickiness bias).
        - gate_base: base logit for engaging the cache (larger => more likely).
        - noise_base: base logit for retrieval noise (larger => more noise).
        - stickiness: strength of within-state perseveration bias (added to chosen action preference).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, gate_base, noise_base, stickiness = model_parameters
    age_value = age[0]
    is_older = 1 if age_value >= 45 else 0

    nA = 3
    total_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask]
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        nS = int(set_sizes[mask][0])

        Q = (1.0 / nA) * np.ones((nS, nA))
        cache = -np.ones(nS, dtype=int)  # -1 means empty
        last_action = -np.ones(nS, dtype=int)

        # Age- and load-dependent cache engagement (logit -> prob)
        load_penalty = (nS - 3) / 3.0  # 0 for 3, 1 for 6
        age_term = -0.5 if is_older else +0.5
        gate_logit = gate_base + (0.8 * age_term) - (1.0 * load_penalty)
        p_gate = 1.0 / (1.0 + np.exp(-gate_logit))
        p_gate = np.clip(p_gate, 0.0, 1.0)

        # Retrieval noise increases with load and for older
        noise_logit = noise_base + (0.8 * (1 if is_older else 0)) + (0.8 * load_penalty)
        p_noise = 1.0 / (1.0 + np.exp(-noise_logit))
        p_noise = np.clip(p_noise, 0.0, 1.0)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # RL softmax probabilities with stickiness
            prefs = Q[s, :].copy()
            # Add stickiness bias to last action in this state
            if last_action[s] >= 0:
                prefs[last_action[s]] += stickiness

            prefs_center = prefs - np.max(prefs)
            p_rl = np.exp(beta * prefs_center)
            p_rl = p_rl / np.sum(p_rl)

            # Cache-guided policy (deterministic choice of cached action, but with retrieval noise)
            if cache[s] >= 0:
                cached = int(cache[s])
                p_cache = np.full(nA, (p_noise) / (nA - 1.0))
                p_cache[cached] = 1.0 - p_noise
            else:
                # If cache empty, behave uniformly (equivalent to not helpful)
                p_cache = np.full(nA, 1.0 / nA)

            # Mixture of cache and RL
            p_mix = p_gate * p_cache + (1.0 - p_gate) * p_rl
            p_choice = np.clip(p_mix[a], 1e-12, 1.0)
            total_log_p += np.log(p_choice)

            # Q-learning update
            td = r - Q[s, a]
            Q[s, a] += alpha * td

            # Update cache only when reward is delivered
            if r > 0.5:
                cache[s] = a

            # Update last action
            last_action[s] = a

    return -float(total_log_p)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Actor-Critic with eligibility traces and age- and load-dependent exploration scaling.

    Mechanism:
    - Policy is represented by state-action preferences h(s,a); softmax over h with inverse temperature beta_eff.
    - Critic maintains a state value V(s).
    - Eligibility traces for the actor focus credit on recently chosen actions within a state.
    - beta_eff is modulated by age group and set size: younger participants exhibit higher effective
      inverse temperature at low load (more exploitation), older participants are more exploratory at high load.
    - Two separate learning rates for actor and critic.

    Parameters
    ----------
    states : array-like of int
        State indices per trial.
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of {0,1}
        Binary feedback.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (constant within block; 3 or 6).
    age : array-like of float
        Participant age; age[0] is used to set group.
    model_parameters : tuple/list
        (alpha_actor, alpha_critic, beta, lambda_trace, age_load_bias)
        - alpha_actor: step size for updating policy preferences.
        - alpha_critic: step size for updating state values.
        - beta: baseline inverse temperature.
        - lambda_trace: eligibility decay for the actor (0..1).
        - age_load_bias: scales the age- and load-dependent adjustment to beta.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha_actor, alpha_critic, beta, lambda_trace, age_load_bias = model_parameters
    age_value = age[0]
    is_older = 1 if age_value >= 45 else 0

    nA = 3
    total_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask]
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        nS = int(set_sizes[mask][0])

        # Initialize actor and critic
        h = np.zeros((nS, nA))             # policy preferences
        V = np.zeros(nS)                   # state values
        e = np.zeros((nS, nA))             # actor eligibility traces

        # Compute effective beta per block from age and load:
        # Younger at low load -> higher beta (more exploitation); Older at high load -> lower beta.
        load_factor = (3.0 / float(max(1, nS)))  # 1.0 (nS=3), 0.5 (nS=6)
        direction = (+1.0 if is_older == 0 else -1.0)
        beta_eff = beta * np.exp(age_load_bias * direction * (load_factor - 0.5))

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Softmax policy
            h_s = h[s, :]
            h_center = h_s - np.max(h_s)
            pi = np.exp(beta_eff * h_center)
            pi = pi / np.sum(pi)

            p_choice = np.clip(pi[a], 1e-12, 1.0)
            total_log_p += np.log(p_choice)

            # TD error with critic (no discount across trials within state; episodic per trial)
            # We treat next state's value as zero since trials are independent across states in this task.
            delta = r - V[s]

            # Update critic
            V[s] += alpha_critic * delta

            # Update actor eligibility traces
            e *= lambda_trace
            # Increment eligibility for chosen action in current state
            e[s, :] *= 0.0
            e[s, a] = 1.0

            # Policy gradient update using compatible features (advantage = delta)
            # Update preferences for all actions in proportion to (1[a==i] - pi[i])
            grad = -pi
            grad[a] += 1.0
            h[s, :] += alpha_actor * delta * grad

    return -float(total_log_p)