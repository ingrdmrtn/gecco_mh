def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with age- and load-modulated temperature and learning rate, plus perseveration and omission bias.

    Idea:
    - Single model-free Q-learner.
    - Learning rate and softmax temperature adapt to set size (3 vs 6) and age group.
      Older adults show reduced learning rate and more noise under high load.
    - Choice kernel (perseveration) that is stronger in older adults.
    - Omission bias that increases with load and with age; omissions have explicit likelihood.

    Parameters
    ----------
    states : array-like of int
        State index per trial within block (0..nS-1 for that block).
    actions : array-like of int
        Chosen action per trial. Valid: {0,1,2}; -2 denotes omission.
    rewards : array-like of float/int
        Feedback per trial. 0 or 1; negative values denote invalid/omission (no learning).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size (3 or 6) per trial, constant within a block.
    age : array-like of float/int
        Participant age; older group is >=45.
    model_parameters : list/tuple of length 5
        [alpha_base, beta_base, prior_bias, stickiness, omit_bias]
        - alpha_base: base learning rate (0-1).
        - beta_base: base inverse temperature (>0).
        - prior_bias: prior toward one-to-one mapping (boost current best action) (>=0).
        - stickiness: choice perseveration strength (>=0).
        - omit_bias: baseline omission logit; affected by load and age.

    Returns
    -------
    neg_log_lik : float
        Negative log-likelihood of observed choices.
    """
    alpha_base, beta_base, prior_bias, stickiness, omit_bias = model_parameters
    age_val = float(age[0])
    older = 1.0 if age_val >= 45 else 0.0

    nA = 3
    eps = 1e-12
    neg_ll = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask].astype(int)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        nS = int(set_sizes[mask][0])

        # Initialize Q and choice kernel
        Q = (1.0 / nA) * np.ones((nS, nA))
        prev_a = -1  # for stickiness

        # Age- and load-modulated parameters
        # Older: lower alpha; high load: lower alpha
        alpha = alpha_base * (0.85 if nS > 3 else 1.0) * (0.8 if older else 1.0)
        alpha = max(0.0, min(1.0, alpha))

        # Beta decreases under load and for older adults
        beta = beta_base * (1.0 if nS <= 3 else 0.7) * (0.8 if older else 1.0)
        beta = max(eps, beta)

        # Omission probability via logistic; increases with load and age
        # logit(p_omit) = omit_bias + c_load + c_age
        omit_logit = omit_bias + (0.6 if nS > 3 else 0.0) + (0.5 if older else 0.0)
        p_omit = 1.0 / (1.0 + np.exp(-omit_logit))
        p_omit = max(0.0, min(0.95, p_omit))  # cap to avoid degenerate

        for t in range(len(block_states)):
            s = block_states[t]
            a = block_actions[t]
            r = block_rewards[t]

            # Compute RL policy with prior one-to-one bias
            Qs = Q[s].copy()
            # prior_bias: accentuate current best action (one-to-one mapping prior)
            best_a = int(np.argmax(Qs))
            prior_vec = np.zeros(nA)
            prior_vec[best_a] = prior_bias

            # Stickiness: add bias to previous chosen action
            stick_vec = np.zeros(nA)
            if prev_a >= 0:
                stick_vec[prev_a] = stickiness * (1.2 if older else 1.0)

            logits = beta * Qs + prior_vec + stick_vec
            logits = logits - np.max(logits)
            pol = np.exp(logits)
            pol /= np.sum(pol)

            # Mixture with omission
            if a == -2:
                p_choice = p_omit
            elif 0 <= a < nA:
                p_choice = (1.0 - p_omit) * pol[a]
            else:
                p_choice = eps

            neg_ll -= np.log(max(p_choice, eps))

            # Learning (only for valid actions and non-negative rewards)
            if (a >= 0) and (r >= 0):
                pe = r - Q[s, a]
                Q[s, a] += alpha * pe

            # Update prev action (only if a valid action)
            if a >= 0:
                prev_a = a

    return neg_ll


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with load- and age-modulated "chunking"/blending across states and lapse-driven omissions.

    Idea:
    - Model-free Q over state-action pairs.
    - After each learning step, values partially blend toward the across-state average (chunking),
      which increases with set size (more load) and with age (older adults rely more on coarse chunks).
    - Action policy is softmax over current state Q; small global lapse generates random choice or omission.

    Parameters
    ----------
    states : array-like of int
        State index per trial within each block.
    actions : array-like of int
        Chosen action per trial. {0,1,2} or -2 for omission.
    rewards : array-like of float/int
        Feedback (0/1). Negative indicates omission/invalid (no update).
    blocks : array-like of int
        Block index.
    set_sizes : array-like of int
        Set size per trial (3 or 6) constant within a block.
    age : array-like of float/int
        Participant age; older group (>=45) has stronger chunking.
    model_parameters : list/tuple of length 5
        [alpha, beta, chunk, phi_age, lapse]
        - alpha: learning rate (0-1).
        - beta: inverse temperature (>0).
        - chunk: base chunking/blending strength (0-1).
        - phi_age: multiplicative increase in chunking for older adults (>=0).
        - lapse: lapse probability (0-1), split equally between random choice and omission.

    Returns
    -------
    neg_log_lik : float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, chunk, phi_age, lapse = model_parameters
    age_val = float(age[0])
    older = 1.0 if age_val >= 45 else 0.0

    nA = 3
    eps = 1e-12
    neg_ll = 0.0

    # Effective lapse increases slightly with load and age
    def eff_lapse(nS):
        base = lapse
        base *= (1.15 if nS > 3 else 1.0)
        base *= (1.2 if older else 1.0)
        return max(0.0, min(0.5, base))

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask].astype(int)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        nS = int(set_sizes[mask][0])

        Q = (1.0 / nA) * np.ones((nS, nA))

        # Chunking factor grows with set size and age
        chunk_eff = chunk * (nS / 3.0) * (1.0 + phi_age * older)
        chunk_eff = max(0.0, min(0.9, chunk_eff))  # cap to keep stable

        lapse_eff = eff_lapse(nS)

        for t in range(len(block_states)):
            s = block_states[t]
            a = block_actions[t]
            r = block_rewards[t]

            # Softmax policy from current state
            logits = beta * (Q[s] - np.max(Q[s]))
            pol = np.exp(logits - np.max(logits))
            pol /= np.sum(pol)

            p_rand = lapse_eff * 0.5
            p_omit = lapse_eff * 0.5

            if a == -2:
                p_choice = p_omit
            elif 0 <= a < nA:
                p_choice = (1.0 - lapse_eff) * pol[a] + p_rand * (1.0 / nA)
            else:
                p_choice = eps

            neg_ll -= np.log(max(p_choice, eps))

            # Learning and chunking (valid action and reward available)
            if (a >= 0) and (r >= 0):
                pe = r - Q[s, a]
                Q[s, a] += alpha * pe

                # Chunking/blending: move state s toward across-state action means
                mean_Q = np.mean(Q, axis=0)
                Q[s] = (1.0 - chunk_eff) * Q[s] + chunk_eff * mean_Q

                # Optional small global smoothing to keep numerically stable
                Q[s] = np.clip(Q[s], -10.0, 10.0)

    return neg_ll


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Hybrid RL + binding map with entropy-driven omissions and age-modulated precision.

    Idea:
    - RL: standard Q-learning.
    - Binding map B: when a state is rewarded, a precise binding is formed to that action (like episodic/WM).
      Precision is controlled by tau_bind and decays otherwise.
    - Policy: mix RL softmax with binding-derived policy where weight equals current binding confidence.
    - Omissions: probability increases with decision uncertainty (entropy of mixed policy) and is amplified by age.

    Parameters
    ----------
    states : array-like of int
        State index per trial within block.
    actions : array-like of int
        Chosen action per trial. {0,1,2} or -2 for omission.
    rewards : array-like of float/int
        Feedback (0/1). Negative if omission/invalid.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size (3 or 6) per trial.
    age : array-like of float/int
        Participant age; older group (>=45) shows reduced binding precision and higher omission sensitivity.
    model_parameters : list/tuple of length 5
        [alpha, beta, tau_bind, k_unc2om, beta_age_slope]
        - alpha: RL learning rate (0-1).
        - beta: inverse temperature for RL softmax (>0).
        - tau_bind: binding precision (higher -> sharper binding); also sets consolidation gain (>=0).
        - k_unc2om: sensitivity of omission probability to policy entropy (>=0).
        - beta_age_slope: additional temperature dampening for older and high load (>=0).

    Returns
    -------
    neg_log_lik : float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, tau_bind, k_unc2om, beta_age_slope = model_parameters
    age_val = float(age[0])
    older = 1.0 if age_val >= 45 else 0.0

    nA = 3
    eps = 1e-12
    neg_ll = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask].astype(int)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        nS = int(set_sizes[mask][0])

        # RL values
        Q = (1.0 / nA) * np.ones((nS, nA))

        # Binding map (probability distribution per state)
        B = (1.0 / nA) * np.ones((nS, nA))
        conf = np.zeros(nS)  # confidence per state (0..1)

        # Age and load adjust beta (older and large set -> lower effective beta)
        beta_eff = beta / (1.0 + beta_age_slope * older * (1.0 if nS > 3 else 0.0))
        beta_eff = max(eps, beta_eff)

        # Binding decay increases with load and age
        decay = 0.05 * (1.0 + 0.5 * older) * (1.0 + (nS - 3) / 3.0)
        decay = min(0.5, max(0.0, decay))

        for t in range(len(block_states)):
            s = block_states[t]
            a = block_actions[t]
            r = block_rewards[t]

            # RL policy
            q_s = Q[s].copy()
            q_s = q_s - np.max(q_s)
            rl_logits = beta_eff * q_s
            rl = np.exp(rl_logits - np.max(rl_logits))
            rl /= np.sum(rl)

            # Binding policy from B with precision scaled by tau_bind and confidence
            # Older adults have lower effective precision through confidence dynamics
            beta_bind = tau_bind * (0.8 if older else 1.0) * (0.8 if nS > 3 else 1.0)
            b_s = B[s].copy()
            b_s = np.maximum(b_s, eps)
            b_s /= np.sum(b_s)
            bind_logits = beta_bind * (b_s - np.max(b_s))
            bind = np.exp(bind_logits - np.max(bind_logits))
            bind /= np.sum(bind)

            # Mixture weight equals confidence
            w = conf[s]
            mix = w * bind + (1.0 - w) * rl
            mix = np.maximum(mix, eps)
            mix /= np.sum(mix)

            # Entropy-driven omissions (higher entropy -> more omissions), amplified by age
            entropy = -np.sum(mix * np.log(np.maximum(mix, eps))) / np.log(nA)  # normalized 0..1
            p_omit = 1.0 / (1.0 + np.exp(-k_unc2om * (entropy * (1.2 if older else 1.0) + (0.1 if nS > 3 else 0.0) - 0.5)))
            p_omit = max(0.0, min(0.7, p_omit))

            if a == -2:
                p_choice = p_omit
            elif 0 <= a < nA:
                p_choice = (1.0 - p_omit) * mix[a]
            else:
                p_choice = eps

            neg_ll -= np.log(max(p_choice, eps))

            # Learning (valid action and reward available)
            if (a >= 0) and (r >= 0):
                # RL update
                pe = r - Q[s, a]
                Q[s, a] += alpha * pe

                # Binding update: rewarded action increases binding and confidence, else decay
                if r > 0.5:
                    gain = 1.0 - np.exp(-tau_bind)  # maps tau_bind to (0..1)
                    # sharpen toward chosen action
                    B[s, a] = B[s, a] + gain * (1.0 - B[s, a])
                    for aa in range(nA):
                        if aa != a:
                            B[s, aa] = (1.0 - gain) * B[s, aa]
                    conf[s] = min(1.0, (1.0 - decay) * conf[s] + gain * (1.0 - conf[s]))
                else:
                    # decay confidence and flatten binding
                    conf[s] = max(0.0, (1.0 - decay) * conf[s])
                    flat = decay
                    B[s] = (1.0 - flat) * B[s] + flat * (np.ones(nA) / nA)

                # Ongoing decay toward uniform to model forgetting/interference
                B[s] = (1.0 - decay) * B[s] + decay * (np.ones(nA) / nA)
                B[s] = np.maximum(B[s], eps)
                B[s] /= np.sum(B[s])

    return neg_ll