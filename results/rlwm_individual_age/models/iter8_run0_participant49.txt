def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Interference-prone RL: Q-values suffer load- and age-dependent interference toward uniformity.
    This models how larger set sizes and older age degrade value representations even without direct experience.

    Mechanism
    - Standard model-free RL update within each block.
    - Before action selection on each trial, all Q-values in the current block drift toward a uniform prior.
      The drift/interference rate increases with set size and with being older.
    - Softmax action selection from current state's Q-values.

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..nS-1 within block).
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of float/int
        Reward per trial (0 or 1).
    blocks : array-like of int
        Block ID per trial; learning and interference state reset per block.
    set_sizes : array-like of int
        Set size per trial (3 or 6). Assumed constant within a block.
    age : array-like or scalar
        Participant age; ages >=45 are treated as older.
    model_parameters : sequence of 5 floats
        alpha        : RL learning rate for Q updates
        beta         : Inverse temperature (scaled by 10 internally)
        interf_base  : Baseline interference rate per trial (0..1)
        age_gain     : Multiplicative gain on interference if older (>=45)
        load_gain    : Multiplicative gain on interference when set size increases from 3 to 6

    Returns
    -------
    nll : float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, interf_base, age_gain, load_gain = model_parameters
    beta = beta * 10.0

    try:
        age_val = float(age[0])
    except Exception:
        age_val = float(age)
    is_older = 1.0 if age_val >= 45 else 0.0

    nA = 3
    eps = 1e-12
    total_log_p = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = np.asarray(actions)[idx]
        block_rewards = np.asarray(rewards)[idx]
        block_states = np.asarray(states)[idx]
        block_set_sizes = np.asarray(set_sizes)[idx]
        nS = int(block_set_sizes[0])

        # Initialize Q to uniform prior
        Q = (1.0 / nA) * np.ones((nS, nA))

        # Precompute interference multiplier for this block
        load_factor = 1.0 + load_gain * max(0.0, (nS - 3.0) / 3.0)  # 1 for 3, 1+load_gain for 6
        age_factor = 1.0 + age_gain * is_older
        interf_rate = np.clip(interf_base * age_factor * load_factor, 0.0, 1.0)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Interference: drift all Q towards uniform prior
            Q = (1.0 - interf_rate) * Q + interf_rate * (1.0 / nA)

            # Policy: softmax over current state's Q
            prefs = Q[s, :] - np.max(Q[s, :])
            p = np.exp(beta * prefs)
            p = p / (np.sum(p) + eps)

            total_log_p += np.log(max(p[a], eps))

            # RL update on chosen action
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

    return -float(total_log_p)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with age- and load-modulated Win/Loss repetition biases.
    After a win, the previous action in that state is favored; after a loss, it can be penalized or favored.
    Bias strength increases with higher set size and older age, reflecting reliance on simple heuristics.

    Mechanism
    - Standard RL Q-learning within block.
    - For each state, track the last action and its outcome.
    - At choice time, add a bias to the last action in that state:
        +win_bias if last outcome was reward, +loss_bias if it was no reward.
      This bias is scaled by age (older increases bias) and by load (larger set size increases bias).
    - Softmax over biased preferences.

    Parameters
    ----------
    states : array-like of int
        State index per trial.
    actions : array-like of int
        Chosen actions (0..2).
    rewards : array-like of float/int
        Rewards (0/1).
    blocks : array-like of int
        Block indices; resets per block.
    set_sizes : array-like of int
        Set size per trial (3 or 6).
    age : array-like or scalar
        Participant age; >=45 treated as older.
    model_parameters : sequence of 6 floats
        alpha       : RL learning rate
        beta        : Inverse temperature (scaled by 10)
        win_bias    : Additive preference bias for repeating the last action after reward
        loss_bias   : Additive preference bias for repeating the last action after no reward
        age_gain    : Multiplicative gain on bias if older
        load_gain   : Multiplicative gain on bias when set size increases from 3 to 6

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, win_bias, loss_bias, age_gain, load_gain = model_parameters
    beta = beta * 10.0

    try:
        age_val = float(age[0])
    except Exception:
        age_val = float(age)
    is_older = 1.0 if age_val >= 45 else 0.0

    nA = 3
    eps = 1e-12
    total_log_p = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = np.asarray(actions)[idx]
        block_rewards = np.asarray(rewards)[idx]
        block_states = np.asarray(states)[idx]
        block_set_sizes = np.asarray(set_sizes)[idx]
        nS = int(block_set_sizes[0])

        Q = (1.0 / nA) * np.ones((nS, nA))
        last_action = -1 * np.ones(nS, dtype=int)
        last_reward = np.zeros(nS)

        # Bias scale determined by age and load within the block
        bias_scale = (1.0 + age_gain * is_older) * (1.0 + load_gain * max(0.0, (nS - 3.0) / 3.0))

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Base preferences from Q
            prefs = Q[s, :].copy()

            # Apply repetition bias based on last outcome for this state
            if last_action[s] >= 0:
                bias = win_bias if last_reward[s] > 0.5 else loss_bias
                prefs[last_action[s]] += bias_scale * bias

            # Softmax choice probabilities
            prefs -= np.max(prefs)
            p = np.exp(beta * prefs)
            p = p / (np.sum(p) + eps)

            total_log_p += np.log(max(p[a], eps))

            # Q-learning update
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # Update memory of last action/outcome for this state
            last_action[s] = a
            last_reward[s] = r

    return -float(total_log_p)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL plus global habit system with age- and load-modulated arbitration weight.
    Under higher load and in older adults, a global, state-agnostic habit bias has greater influence on choices.

    Mechanism
    - Model-free Q-learning per state.
    - A global habit vector H over actions (length 3) is updated toward the chosen action,
      strengthened by obtained reward (reward-modulated habit learning).
    - Choice preferences combine state-specific Q and global habit H via a convex combination.
      The habit weight increases with set size and with being older.
    - Softmax over combined preferences.

    Parameters
    ----------
    states : array-like of int
        State index per trial.
    actions : array-like of int
        Chosen actions (0..2).
    rewards : array-like of float/int
        Rewards (0/1).
    blocks : array-like of int
        Block ID per trial; learning resets per block.
    set_sizes : array-like of int
        Set size per trial (3 or 6).
    age : array-like or scalar
        Participant age; ages >=45 treated as older.
    model_parameters : sequence of 6 floats
        alpha_q           : Learning rate for Q-values
        beta              : Inverse temperature (scaled by 10)
        alpha_habit       : Learning rate for habit vector H
        habit_w_base      : Baseline weight (0..1) on habit in the mixture
        habit_age_gain    : Additive increase to habit weight if older
        habit_load_gain   : Additive increase to habit weight when set size increases from 3 to 6

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.
    """
    alpha_q, beta, alpha_habit, habit_w_base, habit_age_gain, habit_load_gain = model_parameters
    beta = beta * 10.0

    try:
        age_val = float(age[0])
    except Exception:
        age_val = float(age)
    is_older = 1.0 if age_val >= 45 else 0.0

    nA = 3
    eps = 1e-12
    total_log_p = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = np.asarray(actions)[idx]
        block_rewards = np.asarray(rewards)[idx]
        block_states = np.asarray(states)[idx]
        block_set_sizes = np.asarray(set_sizes)[idx]
        nS = int(block_set_sizes[0])

        # Initialize Q and habit vector H
        Q = (1.0 / nA) * np.ones((nS, nA))
        H = np.zeros(nA)  # centered habit; will grow toward chosen action

        # Compute habit weight for this block
        load_term = habit_load_gain * max(0.0, (nS - 3.0) / 3.0)  # 0 for 3, gain for 6
        age_term = habit_age_gain * is_older
        habit_w = np.clip(habit_w_base + age_term + load_term, 0.0, 1.0)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Preferences: convex combination of Q[s] and H
            pref_q = Q[s, :]
            pref_h = H

            combined_pref = (1.0 - habit_w) * pref_q + habit_w * pref_h
            combined_pref = combined_pref - np.max(combined_pref)

            p = np.exp(beta * combined_pref)
            p = p / (np.sum(p) + eps)

            total_log_p += np.log(max(p[a], eps))

            # Q-learning update
            pe = r - Q[s, a]
            Q[s, a] += alpha_q * pe

            # Habit update: move H toward the chosen action, scaled by reward
            target = np.zeros(nA)
            target[a] = 1.0
            H += alpha_habit * r * (target - H)

    return -float(total_log_p)