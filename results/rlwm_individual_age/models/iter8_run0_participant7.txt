def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + uncertainty-gated, capacity-limited working memory (WM) with age-by-load modulation.

    Idea:
    - Two systems: (i) model-free RL (softmax); (ii) working memory that stores rewarded
      state-action pairs with decay.
    - Arbitration weight for WM increases when RL is uncertain (high entropy). Younger adults
      and smaller set sizes increase WM reliance.
    - WM encodes after reward with a probability that also depends on age-by-load; WM traces decay.

    Parameters (model_parameters): [alpha, beta, enc_base, wm_decay, gate_slope, age_load_gain]
    - alpha: (0,1) RL learning rate.
    - beta: >0 base inverse temperature for RL softmax (internally scaled).
    - enc_base: real, baseline log-odds for WM encoding on reward (converted via sigmoid).
    - wm_decay: (0,1) per-trial decay of WM strength.
    - gate_slope: real, sensitivity of WM arbitration to RL entropy and age-by-load input.
    - age_load_gain: real, scales the influence of age group and load (set size) on both encoding
                     and arbitration. Younger group (=+1) increases WM usage if positive.

    Inputs:
    - states: array of state indices (T,)
    - actions: array of chosen actions (T,) in {0,1,2}
    - rewards: array of rewards (T,) in {0,1}
    - blocks: array of block indices (T,)
    - set_sizes: array of set sizes per trial (T,), in {3,6}
    - age: array-like with a single numeric age in years.
    - model_parameters: list/tuple with the 6 parameters above.

    Returns:
    - Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta, enc_base, wm_decay, gate_slope, age_load_gain = model_parameters
    beta = max(1e-6, beta) * 6.0
    age_years = float(age[0])
    age_group = 1.0 if age_years < 45.0 else -1.0

    nA = 3
    total_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask].astype(int)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        nS = int(set_sizes[mask][0])

        # RL values
        Q = np.zeros((nS, nA))
        # WM strengths (unnormalized; will be turned into a distribution per state)
        M = np.zeros((nS, nA))

        for t in range(len(block_states)):
            s = block_states[t]
            a = block_actions[t]
            r = block_rewards[t]

            # RL policy
            logits_rl = beta * (Q[s, :] - np.max(Q[s, :]))
            pi_rl = np.exp(logits_rl)
            pi_rl /= np.sum(pi_rl)

            # WM policy: normalized strengths; if all zeros, uniform
            Ms = M[s, :].copy()
            if Ms.sum() <= 0:
                pi_wm = np.ones(nA) / nA
            else:
                pi_wm = Ms / np.sum(Ms)

            # Uncertainty of RL: normalized entropy in [0,1]
            H = -np.sum(pi_rl * np.log(np.clip(pi_rl, eps, 1.0)))
            H_norm = H / np.log(nA)

            # Arbitration weight for WM: sigmoid of (entropy + age-by-load)
            # Load factor higher for smaller sets (easier WM): 3/nS in [0.5,1]
            load_fac = 3.0 / float(nS)
            gate_input = (H_norm - 0.5) + age_load_gain * age_group * load_fac
            w_wm = 1.0 / (1.0 + np.exp(-gate_slope * gate_input))

            # Mixture policy
            pi = w_wm * pi_wm + (1.0 - w_wm) * pi_rl
            p = max(eps, float(pi[a]))
            total_log_p += np.log(p)

            # Learning updates
            # RL
            Q[s, a] += alpha * (r - Q[s, a])

            # WM decay
            M *= (1.0 - np.clip(wm_decay, 0.0, 1.0))
            # WM encoding only on rewarded trials, with age-by-load modulated probability
            if r > 0.0:
                enc_logit = enc_base + age_load_gain * age_group * load_fac
                p_enc = 1.0 / (1.0 + np.exp(-enc_logit))
                # Expected-strength update: add p_enc to the chosen action's WM strength
                M[s, a] += p_enc

    return -total_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Adaptive learning rate and temperature driven by prediction-error magnitude, with age-by-load
    modulation of the learning rate.

    Idea:
    - The learning rate is a logistic function of baseline + age group + load (3/nS).
      Younger group and smaller set sizes increase alpha if parameters are positive.
    - The inverse temperature is dynamically adapted: it increases when recent prediction errors
      are small (more certainty) and decreases when errors are large (more exploration).
      Implemented as multiplicative update based on |RPE|.

    Parameters (model_parameters): [lr_base, lr_age_mod, lr_load_mod, beta0, temp_adapt_rate]
    - lr_base: real, baseline for learning rate logit.
    - lr_age_mod: real, gain for age group effect on learning rate (younger=+1, older=-1).
    - lr_load_mod: real, gain for load factor (3/nS) effect on learning rate.
    - beta0: >0, initial inverse temperature at the start of each block (internally scaled).
    - temp_adapt_rate: real, how strongly beta adapts to |RPE| around 0.5 per trial.

    Inputs:
    - states: array of state indices (T,)
    - actions: array of chosen actions (T,) in {0,1,2}
    - rewards: array of rewards (T,) in {0,1}
    - blocks: array of block indices (T,)
    - set_sizes: array of set sizes per trial (T,), in {3,6}
    - age: array-like with a single numeric age in years.
    - model_parameters: list/tuple with the 5 parameters above.

    Returns:
    - Negative log-likelihood of the observed choices under the model.
    """
    lr_base, lr_age_mod, lr_load_mod, beta0, temp_adapt_rate = model_parameters
    beta0 = max(1e-6, beta0) * 4.0
    age_years = float(age[0])
    age_group = 1.0 if age_years < 45.0 else -1.0

    nA = 3
    total_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask].astype(int)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        nS = int(set_sizes[mask][0])

        Q = np.zeros((nS, nA))
        beta_t = float(beta0)

        # Precompute alpha for this block based on its set size and age
        load_fac = 3.0 / float(nS)
        alpha_logit = lr_base + lr_age_mod * age_group + lr_load_mod * load_fac
        alpha = 1.0 / (1.0 + np.exp(-alpha_logit))

        for t in range(len(block_states)):
            s = block_states[t]
            a = block_actions[t]
            r = block_rewards[t]

            # Choice policy with current temperature
            logits = beta_t * (Q[s, :] - np.max(Q[s, :]))
            pi = np.exp(logits)
            pi /= np.sum(pi)
            p = max(eps, float(pi[a]))
            total_log_p += np.log(p)

            # RL update
            rpe = r - Q[s, a]
            Q[s, a] += alpha * rpe

            # Adaptive temperature update: more certainty (small |RPE|) -> higher beta
            # Center around 0.5 so that large errors (>0.5) reduce beta.
            beta_t = max(1e-6, beta_t * np.exp(temp_adapt_rate * (0.5 - abs(rpe))))

    return -total_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with success-memory boost and age-modulated exploration bonus (UCB-like).

    Idea:
    - Model-free RL provides baseline Q-values.
    - If a state-action has been observed to produce reward in the past, a WM-like
      "success boost" is added to that action's preference for that state (captures
      rapid learning in small sets).
    - An exploration bonus is added to under-sampled actions using a 1/sqrt(N) form.
      Younger adults receive a larger exploration bonus (if age_explore_shift > 0).
      Larger set sizes increase the effective bonus.

    Parameters (model_parameters): [alpha, beta, wm_success_boost, explore_bonus_base, age_explore_shift]
    - alpha: (0,1) RL learning rate.
    - beta: >0 inverse temperature for softmax (internally scaled).
    - wm_success_boost: >=0 additive boost to the preference of previously-successful action in a state.
    - explore_bonus_base: >=0 baseline exploration bonus magnitude.
    - age_explore_shift: real, additive shift to exploration bonus by age group (younger=+1, older=-1).

    Inputs:
    - states: array of state indices (T,)
    - actions: array of chosen actions (T,) in {0,1,2}
    - rewards: array of rewards (T,) in {0,1}
    - blocks: array of block indices (T,)
    - set_sizes: array of set sizes per trial (T,), in {3,6}
    - age: array-like with a single numeric age in years.
    - model_parameters: list/tuple with the 5 parameters above.

    Returns:
    - Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta, wm_success_boost, explore_bonus_base, age_explore_shift = model_parameters
    beta = max(1e-6, beta) * 5.0
    age_years = float(age[0])
    age_group = 1.0 if age_years < 45.0 else -1.0

    nA = 3
    total_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask].astype(int)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        nS = int(set_sizes[mask][0])

        Q = np.zeros((nS, nA))
        # Success memory: track the best-known successful action per state
        has_success = np.zeros((nS, nA), dtype=bool)
        # Visit counts for UCB-like exploration
        N = np.zeros((nS, nA)) + 1e-6  # small offset to avoid div by zero

        # Exploration bonus scaled by age and load: larger sets -> larger bonus
        load_scale = float(nS) / 3.0  # 1 for nS=3, 2 for nS=6
        explore_bonus = max(0.0, explore_bonus_base + age_explore_shift * age_group) * load_scale

        for t in range(len(block_states)):
            s = block_states[t]
            a = block_actions[t]
            r = block_rewards[t]

            # Base preferences from Q
            pref = Q[s, :].copy()

            # Add success-memory boost to any previously successful action(s) in this state
            if has_success[s, :].any():
                boost_vec = np.zeros(nA)
                boost_vec[has_success[s, :]] = wm_success_boost
                pref = pref + boost_vec

            # Add exploration bonus to under-sampled actions (UCB-like)
            bonus = explore_bonus / np.sqrt(N[s, :] + 0.0)
            pref = pref + bonus

            # Softmax choice probability
            logits = beta * (pref - np.max(pref))
            pi = np.exp(logits)
            pi /= np.sum(pi)
            p = max(eps, float(pi[a]))
            total_log_p += np.log(p)

            # Update counts and learning
            N[s, a] += 1.0
            Q[s, a] += alpha * (r - Q[s, a])

            # Update success memory if reward received
            if r > 0.0:
                has_success[s, a] = True

    return -total_log_p