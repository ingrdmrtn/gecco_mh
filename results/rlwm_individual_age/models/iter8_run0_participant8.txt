def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Actor–Critic with eligibility traces and age/set-size–modulated temporal credit assignment.

    Mechanism
    - Policy (actor): preferences over actions are updated by TD-error via eligibility traces.
    - Value (critic): baseline state value updated by TD-error.
    - Eligibility trace strength (lambda) is reduced by larger set sizes and by age (older group),
      modeling reduced temporal credit assignment/maintenance under higher load and with aging.
    - Action selection: softmax over actor preferences with inverse temperature beta.

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..nS-1 within block).
    actions : array-like of int
        Observed action (0..2). Out-of-range actions are treated as lapses (uniform likelihood, no learning).
    rewards : array-like of float
        Reward observed on each trial (typically 0/1). Negative rewards indicate invalid trials (uniform likelihood).
    blocks : array-like of int
        Block index per trial; states reset per block.
    set_sizes : array-like of int
        Set size per trial (3 or 6).
    age : array-like of float
        Participant age as a single-element array. Older group if age > 45.
    model_parameters : iterable of 5 floats
        alpha_v       : Critic learning rate (0..1).
        alpha_p       : Actor learning rate (0..1).
        beta          : Inverse temperature for softmax (>0).
        lambda0       : Baseline eligibility trace parameter (0..1).
        age_set_gain  : Strength of age- and set-size modulation on lambda (>=0).

    Returns
    -------
    neg_log_likelihood : float
        Negative log-likelihood of the observed action sequence under the model.
    """
    alpha_v, alpha_p, beta, lambda0, age_set_gain = model_parameters
    age_val = float(age[0])
    is_older = 1.0 if age_val > 45.0 else 0.0

    total_log_p = 0.0
    nA = 3

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = np.asarray(actions)[idx]
        block_rewards = np.asarray(rewards)[idx]
        block_states = np.asarray(states)[idx]
        block_set_sizes = np.asarray(set_sizes)[idx]

        nS = int(block_set_sizes[0])
        # Actor preferences and critic values
        P = np.zeros((nS, nA))
        V = np.zeros(nS)
        # Eligibility traces for actor
        E = np.zeros((nS, nA))

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            ss = float(block_set_sizes[t])

            # Invalid trials: uniform choice probability, no learning.
            if not (0 <= s < nS) or not (0 <= a < nA) or r < 0:
                total_log_p += np.log(1.0 / nA)
                continue

            # Softmax over preferences for policy
            pref = beta * P[s, :]
            pref -= np.max(pref)
            pi = np.exp(pref)
            pi /= np.sum(pi)

            pa = np.clip(pi[a], 1e-12, 1.0)
            total_log_p += np.log(pa)

            # Critic TD error (bandit-style AC): delta = r - V(s)
            delta = r - V[s]
            V[s] += alpha_v * delta

            # Eligibility trace decay; gamma as a mild persistence factor decreasing with set-size
            # Higher set size -> smaller gamma (shallower temporal credit).
            gamma = 1.0 - (0.15 * (ss - 3.0))  # 1.0 for 3, 0.85 for 6
            gamma = np.clip(gamma, 0.5, 1.0)

            # Effective lambda reduced by set size and by older age
            lambda_eff = lambda0 * (1.0 - age_set_gain * (0.5 * (ss - 3.0) + 1.0 * is_older))
            lambda_eff = np.clip(lambda_eff, 0.0, 1.0)

            # Decay traces, then increment for chosen action at current state
            E *= (gamma * lambda_eff)
            E[s, a] += 1.0

            # Update actor preferences by TD error via eligibility
            P += alpha_p * delta * E

    return -float(total_log_p)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Uncertainty-weighted RL–WM arbitration with age- and set-size–dependent WM availability.

    Mechanism
    - RL: tabular Q-learning with softmax action selection.
    - WM: stores the most recently rewarded action per state.
    - Arbitration: WM weight is a saturating (sigmoidal) function of an effective WM capacity term
      that decreases with larger set sizes and with older age; higher interference further reduces WM weight.
      When WM has no item for a state, it defaults to uniform.
    - The model blends WM and RL policies according to WM weight.

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..nS-1 within block).
    actions : array-like of int
        Observed action (0..2). Out-of-range actions are treated as lapses (uniform likelihood, no learning).
    rewards : array-like of float
        Reward on each trial; negative indicates invalid trial (uniform likelihood).
    blocks : array-like of int
        Block index per trial; states reset per block.
    set_sizes : array-like of int
        Set size per trial (3 or 6).
    age : array-like of float
        Participant age as a single-element array. Older group if age > 45.
    model_parameters : iterable of 5 floats
        lr           : Q-learning rate (0..1).
        beta         : Inverse temperature for RL softmax (>0).
        cap_base     : Baseline WM capacity drive (can be any real; higher -> stronger WM weight).
        interference : Set-size interference factor reducing WM (>=0).
        age_shift    : Additional WM reduction for older group (>=0).

    Returns
    -------
    neg_log_likelihood : float
        Negative log-likelihood of the observed action sequence.
    """
    lr, beta, cap_base, interference, age_shift = model_parameters
    age_val = float(age[0])
    is_older = 1.0 if age_val > 45.0 else 0.0

    total_log_p = 0.0
    nA = 3

    # Helper: sigmoid without import
    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = np.asarray(actions)[idx]
        block_rewards = np.asarray(rewards)[idx]
        block_states = np.asarray(states)[idx]
        block_set_sizes = np.asarray(set_sizes)[idx]

        nS = int(block_set_sizes[0])

        Q = np.zeros((nS, nA))
        # WM store: -1 means empty; otherwise stores an action index
        WM = -1 * np.ones(nS, dtype=int)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            ss = float(block_set_sizes[t])

            # Invalid trials -> uniform likelihood, no learning
            if not (0 <= s < nS) or not (0 <= a < nA) or r < 0:
                total_log_p += np.log(1.0 / nA)
                continue

            # RL policy
            q = beta * Q[s, :]
            q -= np.max(q)
            p_rl = np.exp(q)
            p_rl /= np.sum(p_rl)

            # WM policy
            if WM[s] >= 0:
                p_wm = np.zeros(nA)
                p_wm[WM[s]] = 1.0
            else:
                p_wm = np.ones(nA) / nA

            # WM availability weight
            # Effective capacity decreases with set size and with older age
            cap_eff = cap_base - interference * (ss - 3.0) - age_shift * is_older
            w = np.clip(sigmoid(cap_eff), 0.0, 1.0)

            # Blend policies
            p_mix = w * p_wm + (1.0 - w) * p_rl
            pa = np.clip(p_mix[a], 1e-12, 1.0)
            total_log_p += np.log(pa)

            # Learning
            pe = r - Q[s, a]
            Q[s, a] += lr * pe

            # WM update: only store on rewarded trials (1)
            if r > 0.0:
                WM[s] = a

    return -float(total_log_p)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Noisy Win–Stay / Lose–Shift (WSLS) with age- and set-size–dependent lapses.

    Mechanism
    - If the same state was seen before:
        * After a win (reward=1), prefer repeating the previous action (stay) with strength stay_bias.
        * After a loss (reward=0), prefer switching to other actions with strength shift_rate.
      Preferences are converted to probabilities via a softmax with inverse temperature beta_exp.
    - If the state is new (no prior), use uniform choice.
    - Lapses (uniform choice) increase with set size and with older age.

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..nS-1 within block).
    actions : array-like of int
        Observed action (0..2). Out-of-range actions are treated as lapses (uniform likelihood).
    rewards : array-like of float
        Reward on each trial; negative indicates invalid trial (uniform likelihood).
    blocks : array-like of int
        Block index per trial; states reset per block.
    set_sizes : array-like of int
        Set size per trial (3 or 6).
    age : array-like of float
        Participant age as a single-element array. Older group if age > 45.
    model_parameters : iterable of 5 floats
        stay_bias     : Utility bonus to repeat previous action after a win (real; larger -> stronger stay).
        shift_rate    : Utility bonus assigned to non-previous actions after a loss (real; larger -> stronger shift).
        beta_exp      : Inverse temperature for WSLS softmax (>0).
        lapse0        : Baseline lapse log-odds (can be any real).
        age_size_gain : Gain scaling how age and set size increase lapses (>=0).

    Returns
    -------
    neg_log_likelihood : float
        Negative log-likelihood of the observed action sequence.
    """
    stay_bias, shift_rate, beta_exp, lapse0, age_size_gain = model_parameters
    age_val = float(age[0])
    is_older = 1.0 if age_val > 45.0 else 0.0

    total_log_p = 0.0
    nA = 3

    # Helper: sigmoid
    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = np.asarray(actions)[idx]
        block_rewards = np.asarray(rewards)[idx]
        block_states = np.asarray(states)[idx]
        block_set_sizes = np.asarray(set_sizes)[idx]

        nS = int(block_set_sizes[0])

        # Memory of last action and reward per state
        last_action = -1 * np.ones(nS, dtype=int)
        last_reward = -1.0 * np.ones(nS)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            ss = float(block_set_sizes[t])

            # Invalid trials -> uniform likelihood, no updates
            if not (0 <= s < nS) or not (0 <= a < nA) or r < 0:
                total_log_p += np.log(1.0 / nA)
                continue

            # Lapse probability increases with set size and older age
            lapse_logit = lapse0 + age_size_gain * (is_older + 0.2 * (ss - 3.0))
            eps = np.clip(sigmoid(lapse_logit), 0.0, 1.0)

            # WSLS policy
            if last_action[s] >= 0 and last_reward[s] >= 0.0:
                util = np.zeros(nA)
                if last_reward[s] > 0.0:
                    # Win: prefer staying with last action
                    util[last_action[s]] = stay_bias
                else:
                    # Loss: prefer switching away from last action
                    for aa in range(nA):
                        if aa != last_action[s]:
                            util[aa] = shift_rate
                u = beta_exp * util
                u -= np.max(u)
                p_wsls = np.exp(u)
                p_wsls /= np.sum(p_wsls)
            else:
                p_wsls = np.ones(nA) / nA  # new state: no prior, uniform policy

            # Mix lapse with WSLS policy
            p_mix = eps * (np.ones(nA) / nA) + (1.0 - eps) * p_wsls
            pa = np.clip(p_mix[a], 1e-12, 1.0)
            total_log_p += np.log(pa)

            # Update memory
            last_action[s] = a
            last_reward[s] = r

    return -float(total_log_p)