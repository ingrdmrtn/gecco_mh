def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with uncertainty-bonus exploration modulated by load and age.

    Mechanism
    - Tabular Q-learning for state-action values.
    - Bayesian action uncertainty for each state-action (Beta-Bernoulli posterior variance).
    - Directed exploration bonus: add phi * uncertainty(s,a) to Q before softmax.
    - The exploration gain phi is a logistic transform of a baseline plus a load term
      (attenuated under higher set size) and an age penalty (reduced in older group).

    Parameters
    ----------
    states : array-like of int
        State index on each trial within block (0..nS-1).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of {0,1}
        Feedback on each trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (3 or 6).
    age : array-like
        Participant age; uses age[0] to define age group: younger (<45) vs older (>=45).
    model_parameters : sequence of 5 floats
        [alpha, beta, phi0, load_effect, age_effect]
        - alpha: learning rate in [0,1] after sigmoid transform.
        - beta: inverse temperature for softmax (>=0).
        - phi0: baseline log-odds for exploration gain (controls average exploration bonus).
        - load_effect: how much low load (small set size) increases exploration gain.
        - age_effect: nonnegative penalty on exploration gain for older group.

    Returns
    -------
    nll : float
        Negative log-likelihood of observed actions under the model.
    """
    alpha, beta, phi0, load_effect, age_effect = model_parameters
    alpha = 1.0 / (1.0 + np.exp(-alpha))
    beta = np.maximum(beta, 1e-6)
    age_effect = np.maximum(0.0, age_effect)

    age_group = 1 if age[0] >= 45 else 0
    nA = 3
    nll = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)
        nS = int(block_set_sizes[0])

        # Initialize Q and Beta-Bernoulli counts for uncertainty
        Q = (1.0 / nA) * np.ones((nS, nA))
        N = np.zeros((nS, nA))         # total observations per state-action
        Rsum = np.zeros((nS, nA))      # total rewards per state-action

        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]
            nS_t = int(block_set_sizes[t])

            # Beta posterior parameters for uncertainty (alpha0=1, beta0=1)
            alpha_post = 1.0 + Rsum[s, :]
            beta_post = 1.0 + (N[s, :] - Rsum[s, :])
            denom = (alpha_post + beta_post)
            # Posterior variance of Bernoulli p under Beta(alpha_post, beta_post)
            unc = (alpha_post * beta_post) / (np.maximum(1e-12, denom**2 * (denom + 1.0)))

            # Load term: higher under low set size (easier blocks), reduced under high load
            load_term = (6.0 - float(nS_t)) / 3.0  # equals 1 for nS=3, 0 for nS=6

            # Exploration weight via logistic link
            phi_logit = phi0 + load_effect * load_term - age_group * age_effect
            phi = 1.0 / (1.0 + np.exp(-phi_logit))

            # Softmax over Q + directed exploration bonus
            logits = beta * (Q[s, :] + phi * unc)
            logits -= np.max(logits)
            p = np.exp(logits)
            p /= np.maximum(1e-12, np.sum(p))

            pa = np.clip(p[a], 1e-12, 1.0)
            nll -= np.log(pa)

            # Update Q with TD error
            delta = r - Q[s, a]
            Q[s, a] += alpha * delta

            # Update counts for uncertainty
            N[s, a] += 1.0
            Rsum[s, a] += r

    return nll


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with Dyna-style replay rate modulated by load and age.

    Mechanism
    - Tabular Q-learning for value learning.
    - Effective learning rate is boosted by a replay factor that mimics additional
      memory reactivation updates per trial.
    - Replay factor is a logistic function of a baseline plus a load term
      (reduced under higher set size) and an age penalty (reduced in older group).

    Parameters
    ----------
    states : array-like of int
        State index on each trial within block (0..nS-1).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of {0,1}
        Feedback on each trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (3 or 6).
    age : array-like
        Participant age; uses age[0] to define age group: younger (<45) vs older (>=45).
    model_parameters : sequence of 5 floats
        [alpha, beta, replay_base, load_replay_gain, age_replay_penalty]
        - alpha: base learning rate (sigmoid to [0,1]).
        - beta: inverse temperature for softmax (>=0).
        - replay_base: baseline log-odds for replay strength.
        - load_replay_gain: increases replay under low load (set size 3).
        - age_replay_penalty: nonnegative penalty to replay for older group.

    Returns
    -------
    nll : float
        Negative log-likelihood of observed actions under the model.
    """
    alpha, beta, replay_base, load_replay_gain, age_replay_penalty = model_parameters
    alpha = 1.0 / (1.0 + np.exp(-alpha))
    beta = np.maximum(beta, 1e-6)
    age_replay_penalty = np.maximum(0.0, age_replay_penalty)

    age_group = 1 if age[0] >= 45 else 0
    nA = 3
    nll = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)
        nS = int(block_set_sizes[0])

        Q = (1.0 / nA) * np.ones((nS, nA))

        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]
            nS_t = int(block_set_sizes[t])

            # Load term higher in low set size
            load_term = (6.0 - float(nS_t)) / 3.0  # 1 for 3, 0 for 6

            # Replay factor via logistic; map to [0,1]
            replay_logit = replay_base + load_replay_gain * load_term - age_group * age_replay_penalty
            replay_factor = 1.0 / (1.0 + np.exp(-replay_logit))

            # Convert replay factor into an effective multiplicative boost to alpha.
            # Scale: up to 1 + 5*replay_factor (i.e., up to 6x the base rate).
            alpha_eff = alpha * (1.0 + 5.0 * replay_factor)
            alpha_eff = np.clip(alpha_eff, 0.0, 1.0)

            # Softmax policy from current Q
            logits = beta * Q[s, :]
            logits -= np.max(logits)
            p = np.exp(logits)
            p /= np.maximum(1e-12, np.sum(p))

            pa = np.clip(p[a], 1e-12, 1.0)
            nll -= np.log(pa)

            # TD update with effective learning rate
            delta = r - Q[s, a]
            Q[s, a] += alpha_eff * delta

    return nll


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + surprise-gated working memory mixture with load- and age-driven interference.

    Mechanism
    - RL: Tabular Q-learning with softmax choice.
    - WM store: The last rewarded action for a state can be stored in a working-memory slot.
    - Surprise gating: The probability to update WM on a trial increases with absolute
      prediction error |delta| via a logistic gate; baseline gate bias is also included.
    - Interference/decay: Stored WM strength decays over trials; decay increases with load
      (larger set size) and with older age (single joint penalty parameter).
    - Mixture: Final policy is w_wm * WM_policy + (1 - w_wm) * RL_policy, where w_wm is the
      current WM strength for that state.

    Parameters
    ----------
    states : array-like of int
        State index on each trial within block (0..nS-1).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of {0,1}
        Feedback on each trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (3 or 6).
    age : array-like
        Participant age; uses age[0] to define age group: younger (<45) vs older (>=45).
    model_parameters : sequence of 5 floats
        [alpha, beta, gate_bias, pe_gate_gain, age_load_penalty]
        - alpha: Q-learning rate (sigmoid to [0,1]).
        - beta: inverse temperature for RL softmax (>=0).
        - gate_bias: baseline log-odds for storing in WM (higher favors storing).
        - pe_gate_gain: sensitivity of WM gate to |prediction error|.
        - age_load_penalty: nonnegative coefficient increasing WM decay for older age
          and larger set size (combined penalty).

    Returns
    -------
    nll : float
        Negative log-likelihood of observed actions under the model.
    """
    alpha, beta, gate_bias, pe_gate_gain, age_load_penalty = model_parameters
    alpha = 1.0 / (1.0 + np.exp(-alpha))
    beta = np.maximum(beta, 1e-6)
    pe_gate_gain = np.maximum(0.0, pe_gate_gain)
    age_load_penalty = np.maximum(0.0, age_load_penalty)

    age_group = 1 if age[0] >= 45 else 0
    nA = 3
    nll = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)
        nS = int(block_set_sizes[0])

        Q = (1.0 / nA) * np.ones((nS, nA))

        # WM store: action index per state; -1 if none
        wm_action = -np.ones(nS, dtype=int)
        # WM strength per state in [0,1]
        wm_strength = np.zeros(nS)

        # Track last visited state to apply between-state interference conveniently
        last_state = -1

        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]
            nS_t = int(block_set_sizes[t])

            # Interference/decay before decision on current trial:
            # decay increases with load (nS=6) and with older age, via single penalty param.
            load_component = (float(nS_t) - 3.0) / 3.0  # 0 for 3, 1 for 6
            # Base decay magnitude from the penalty, capped to [0, 0.5] per step for stability
            base_decay = np.minimum(0.5, age_load_penalty * (0.5 * age_group + load_component))
            if last_state >= 0:
                # Apply decay to all states' WM strength (global interference)
                wm_strength = wm_strength * (1.0 - base_decay)
                # If decay drives strength very small, clear wm_action for numerical cleanliness
                mask_clear = wm_strength < 1e-6
                wm_action[mask_clear] = -1
                wm_strength[mask_clear] = 0.0

            # RL policy
            logits_rl = beta * Q[s, :]
            logits_rl -= np.max(logits_rl)
            p_rl = np.exp(logits_rl)
            p_rl /= np.maximum(1e-12, np.sum(p_rl))

            # WM policy for current state
            if wm_action[s] >= 0 and wm_strength[s] > 0.0:
                p_wm = np.zeros(nA)
                p_wm[wm_action[s]] = 1.0
            else:
                p_wm = np.ones(nA) / nA

            # Mixture with current state's WM strength as weight
            w = np.clip(wm_strength[s], 0.0, 1.0)
            p_total = w * p_wm + (1.0 - w) * p_rl
            pa = np.clip(p_total[a], 1e-12, 1.0)
            nll -= np.log(pa)

            # Learning
            delta = r - Q[s, a]
            Q[s, a] += alpha * delta

            # Surprise-gated WM update: probability to store or refresh increases with |delta|
            pe = abs(delta)
            gate_logit = gate_bias + pe_gate_gain * pe
            p_update = 1.0 / (1.0 + np.exp(-gate_logit))

            # Deterministic update using expected update (since we compute likelihood, not simulate):
            # We move WM strength toward 1 if we update, else keep as is (after decay).
            # Additionally, only store rewarded actions; if not rewarded, we only refresh existing mapping.
            if r >= 0.5:
                # Update strength toward 1 by fraction p_update and set the stored action
                wm_action[s] = a
                wm_strength[s] = wm_strength[s] + p_update * (1.0 - wm_strength[s])
            else:
                # Non-rewarded outcomes reduce confidence: move strength toward 0 by fraction p_update
                wm_strength[s] = (1.0 - p_update) * wm_strength[s]
                # If strength becomes tiny, clear mapping
                if wm_strength[s] < 1e-6:
                    wm_action[s] = -1
                    wm_strength[s] = 0.0

            last_state = s

    return nll