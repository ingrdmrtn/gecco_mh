def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited, decaying working memory (WM) with age-modulated capacity.

    Mechanism:
    - A standard Q-learner updates action values per state with learning rate alpha and softmax with beta.
    - A WM store holds high-confidence associations (state->action) that are refreshed by rewarded outcomes
      and decay otherwise.
    - Arbitration is a probabilistic mixture: the weight on WM depends on (i) effective capacity relative to
      set size and (ii) the current WM strength for the queried state.
    - Age modulates the effective WM capacity: younger get a positive shift; older get a negative shift.

    Parameters (model_parameters):
    - alpha: RL learning rate in (0,1]
    - beta: inverse temperature (>0) for softmax over Q-values
    - theta_cap: baseline WM capacity (in "number of items")
    - age_mod: age-dependent capacity shift (added for younger, subtracted for older)
    - phi_decay: per-trial decay for WM strengths in [0,1]; larger = faster forgetting

    Inputs:
    - states: array of state indices (0..nS-1 within each block)
    - actions: array of chosen actions (0..2); out-of-range treated as uniform-lapse choices
    - rewards: array of feedback; mapped to r_bin = 1 if >0 else 0
    - blocks: array of block indices
    - set_sizes: array of set sizes (3 or 6)
    - age: array-like with single value (years); younger <45, older >=45
    - model_parameters: [alpha, beta, theta_cap, age_mod, phi_decay]

    Returns:
    - Negative log-likelihood of observed choices.
    """
    alpha, beta, theta_cap, age_mod, phi_decay = model_parameters
    is_older = 1.0 if age[0] >= 45 else 0.0
    is_younger = 1.0 - is_older

    nll = 0.0
    eps = 1e-12
    nA = 3

    for b in np.unique(blocks):
        mask = (blocks == b)
        b_states = states[mask]
        b_actions = actions[mask]
        b_rewards = rewards[mask]
        b_set_sizes = set_sizes[mask]
        nS = int(b_set_sizes[0])

        # RL values
        Q = (1.0 / nA) * np.ones((nS, nA))
        # WM strengths (row-normalized when used as probabilities)
        M = np.zeros((nS, nA))

        # Effective capacity and maximal WM weight per trial (before state-specific strength)
        K_eff = max(0.0, theta_cap + age_mod * (is_younger - is_older))
        K_eff = min(max(K_eff, 0.0), 6.0)  # clamp to plausible range
        for t in range(len(b_states)):
            s = int(b_states[t])
            a = int(b_actions[t])
            r = 1.0 if b_rewards[t] > 0 else 0.0
            set_size = float(b_set_sizes[t])

            # RL policy
            q_s = Q[s, :].copy()
            logits_rl = beta * (q_s - np.max(q_s))
            p_rl = np.exp(logits_rl)
            p_rl = p_rl / (np.sum(p_rl) + eps)

            # WM policy from strengths
            m_s = M[s, :].copy()
            if np.sum(m_s) <= eps:
                p_wm_pol = np.ones(nA) / nA
                wm_strength = 0.0
            else:
                p_wm_pol = m_s / (np.sum(m_s) + eps)
                wm_strength = np.max(p_wm_pol)  # confidence in best WM action

            # Arbitration weight: capacity/load scaled, further gated by current WM strength
            base_wm = min(1.0, K_eff / max(1.0, set_size))
            w_wm = base_wm * wm_strength
            w_wm = max(0.0, min(1.0, w_wm))

            # Mixture policy
            p_final = w_wm * p_wm_pol + (1.0 - w_wm) * p_rl

            # Log-likelihood accounting for invalid actions as uniform lapse
            if a < 0 or a >= nA:
                nll -= np.log(1.0 / nA + eps)
            else:
                nll -= np.log(max(p_final[a], eps))

            # Learning updates
            if 0 <= a < nA:
                # RL update
                delta = r - Q[s, a]
                Q[s, a] += alpha * delta

                # WM decay and update for the current state
                # Decay strengths at the current state
                M[s, :] = (1.0 - phi_decay) * M[s, :]
                if r > 0.0:
                    # Refresh: store a "pointer-like" memory for the rewarded action
                    M[s, :] = 0.0
                    M[s, a] = 1.0
                else:
                    # After no-reward, slightly suppress the chosen action relative to others via decay already applied
                    pass

    return nll


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Eligibility-trace RL with load- and age-modulated exploration.

    Mechanism:
    - Q-learning within each state with replacing eligibility traces on the chosen action.
    - The inverse temperature (exploitation) decreases with set size (cognitive load) and with age (if age_explore > 0).
    - Traces allow credit assignment to persist for recently chosen actions in a state, amplifying learning on repeated choices.

    Parameters (model_parameters):
    - eta: learning rate (>0)
    - invtemp0: base inverse temperature (>0) at minimal load and neutral age
    - trace: eligibility trace decay in [0,1]; higher keeps traces longer
    - load_slope: nonnegative slope scaling how much load (set size) reduces invtemp
    - age_explore: age term shifting exploration; positive => older explore more (lower invtemp), negative => older exploit more

    Inputs:
    - states: array of state indices
    - actions: array of chosen actions (0..2); out-of-range treated as uniform lapses
    - rewards: array of feedback; mapped to r_bin = 1 if >0 else 0
    - blocks: array of block indices
    - set_sizes: array of set sizes (3 or 6)
    - age: array-like with single value in years; older if >=45
    - model_parameters: [eta, invtemp0, trace, load_slope, age_explore]

    Returns:
    - Negative log-likelihood of observed choices.
    """
    eta, invtemp0, trace, load_slope, age_explore = model_parameters
    is_older = 1.0 if age[0] >= 45 else 0.0
    age_sign = (2.0 * is_older - 1.0)  # +1 older, -1 younger

    nll = 0.0
    eps = 1e-12
    nA = 3

    for b in np.unique(blocks):
        mask = (blocks == b)
        b_states = states[mask]
        b_actions = actions[mask]
        b_rewards = rewards[mask]
        b_set_sizes = set_sizes[mask]
        nS = int(b_set_sizes[0])

        Q = (1.0 / nA) * np.ones((nS, nA))
        # Eligibility traces per state-action
        E = np.zeros((nS, nA))

        for t in range(len(b_states)):
            s = int(b_states[t])
            a = int(b_actions[t])
            r = 1.0 if b_rewards[t] > 0 else 0.0
            load = float(b_set_sizes[t])

            # Inverse temperature decreases with load and age (if age_explore>0)
            denom = 1.0 + load_slope * max(0.0, load - 3.0) + age_explore * max(0.0, is_older)
            invtemp = invtemp0 / max(denom, 1e-6)

            # Softmax over Q
            q_s = Q[s, :].copy()
            logits = invtemp * (q_s - np.max(q_s))
            p = np.exp(logits)
            p = p / (np.sum(p) + eps)

            if a < 0 or a >= nA:
                nll -= np.log(1.0 / nA + eps)
                # Decay traces even on lapse trials
                E *= trace
                continue
            else:
                nll -= np.log(max(p[a], eps))

            # Replace traces: decay then set chosen trace to 1
            E *= trace
            E[s, a] = 1.0

            # TD error and Q update with eligibility weighting (within the same state-action dimension)
            delta = r - Q[s, a]
            # Update all actions in the current state proportionally to their traces in that state
            # (traces in other states are near-zero except recent visits)
            Q += eta * delta * E

            # Optional small age-dependent stabilization toward 0 for older (absorbed by traces through decay)
            if is_older > 0 and age_explore < 0:
                # If older are set to exploit more (negative age_explore), slightly dampen non-chosen traces
                E[s, :] *= (1.0 - 0.0 * age_sign)  # keeps parameter usage coherent without extra params

    return nll


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Bayesian rule learner with age-shifted priors, load-dependent lapses, and older-action bias.

    Mechanism:
    - For each state, maintain a Dirichlet belief over the 3 actions indicating which action is correct.
    - Choice uses a softmax over log posterior means (conjugate to categorical).
    - Lapse rate increases with set size; older additionally receive an action-0 bias.
    - Updates: increment the chosen action's count on rewarded trials (no forgetting here).

    Parameters (model_parameters):
    - temp: inverse temperature (>0) applied to log posterior means
    - prior0: baseline prior concentration per action (>0)
    - age_prior_shift: additive shift to prior for younger (+) and opposite for older (-); prior = prior0 + age_sign*age_prior_shift
    - k_load_lapse: scales how much lapse increases with set size
    - bias_old: logit bias added to action 0 for older adults (can be negative to penalize)

    Inputs:
    - states: array of state indices
    - actions: array of chosen actions (0..2); out-of-range treated as uniform lapses
    - rewards: array of feedback; mapped to r_bin = 1 if >0 else 0
    - blocks: array of block indices
    - set_sizes: array of set sizes (3 or 6)
    - age: array-like with single value, participant age in years
    - model_parameters: [temp, prior0, age_prior_shift, k_load_lapse, bias_old]

    Returns:
    - Negative log-likelihood of observed choices under the model.
    """
    temp, prior0, age_prior_shift, k_load_lapse, bias_old = model_parameters
    is_older = 1.0 if age[0] >= 45 else 0.0
    age_sign = 1.0 if is_older < 0.5 else -1.0  # +1 younger, -1 older

    nll = 0.0
    eps = 1e-12
    nA = 3

    # Age-shifted prior concentration
    prior_conc = max(eps, prior0 + age_sign * age_prior_shift)

    for b in np.unique(blocks):
        mask = (blocks == b)
        b_states = states[mask]
        b_actions = actions[mask]
        b_rewards = rewards[mask]
        b_set_sizes = set_sizes[mask]
        nS = int(b_set_sizes[0])

        Alpha = np.ones((nS, nA)) * prior_conc

        for t in range(len(b_states)):
            s = int(b_states[t])
            a = int(b_actions[t])
            r = 1.0 if b_rewards[t] > 0 else 0.0
            load = float(b_set_sizes[t])

            mean_p = Alpha[s, :] / (np.sum(Alpha[s, :]) + eps)

            # Decision logits from posterior means
            logits = temp * np.log(mean_p + eps)
            # Older bias toward action 0 (if bias_old > 0)
            if is_older > 0.5:
                logits = logits.copy()
                logits[0] += bias_old

            # Stable softmax
            logits = logits - np.max(logits)
            p = np.exp(logits)
            p = p / (np.sum(p) + eps)

            # Load-dependent lapse, more lapse as set size increases; cap to avoid degeneracy
            lapse = k_load_lapse * max(0.0, (load - 3.0) / 3.0)
            lapse = min(0.49, max(0.0, lapse))
            p_final = (1.0 - lapse) * p + lapse * (np.ones(nA) / nA)

            if a < 0 or a >= nA:
                nll -= np.log(1.0 / nA + eps)
                continue
            else:
                nll -= np.log(max(p_final[a], eps))

            # Update Dirichlet counts on reward
            if r > 0.0:
                Alpha[s, a] += 1.0
            else:
                # No decrement to keep counts valid; implicit learning via non-updates
                pass

    return nll