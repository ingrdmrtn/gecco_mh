def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Model 1: RL with set-size-dependent forgetting and age-modulated choice perseveration (choice kernel).
    
    Mechanism:
      - Model-free Q-learning with separate learning rates for positive and negative prediction errors.
      - Value forgetting within a block increases with set size (greater interference with 6 vs 3).
      - A choice kernel captures short-term action perseveration within each state; its influence is
        stronger in older adults.
      - Action selection uses a softmax over the sum of Q-values (scaled by inverse temperature) and
        the choice kernel bias.
    
    Parameters (model_parameters):
      lr_pos : float
          Learning rate for positive prediction errors (0..1).
      lr_neg : float
          Learning rate for negative prediction errors (0..1).
      invtemp : float
          Base inverse temperature (>0); internally scaled for stability.
      forget_base : float
          Base per-trial forgetting rate of Q-values (0..1), scaled up by set size.
      pers_base : float
          Baseline perseveration strength (weight on the choice kernel).
      age_pers_gain : float
          Multiplicative gain on perseveration if older (>=45). Effective pers = pers_base*(1 + age_pers_gain*is_older).
    
    Inputs:
      states : array-like (T,)
          State index per trial (0..nS-1 within a block).
      actions : array-like (T,)
          Chosen action per trial (0..2).
      rewards : array-like (T,)
          Binary rewards (0/1).
      blocks : array-like (T,)
          Block index per trial. Values indicate trials belonging to the same block.
      set_sizes : array-like (T,)
          Set size for each trial in its block (typically 3 or 6).
      age : array-like or scalar
          Participant age; older group defined as age >= 45.
      model_parameters : tuple
          Tuple of the six parameters described above.
    
    Returns:
      nll : float
          Negative log-likelihood of observed choices under the model.
    """
    eps = 1e-12
    nA = 3

    lr_pos, lr_neg, invtemp, forget_base, pers_base, age_pers_gain = model_parameters
    beta = 5.0 * max(invtemp, eps)

    age_val = age[0] if hasattr(age, "__len__") and not np.isscalar(age) else age
    is_older = 1.0 if age_val >= 45 else 0.0
    pers_weight_global = pers_base * (1.0 + age_pers_gain * is_older)

    total_logp = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx].astype(int)
        block_actions = actions[idx].astype(int)
        block_rewards = rewards[idx].astype(float)
        nS = int(set_sizes[idx][0])

        # Set-size dependent forgetting rate
        ss_factor = (nS - 3) / 3.0  # 0 for 3, 1 for 6
        forget = np.clip(forget_base * (1.0 + ss_factor), 0.0, 1.0)

        Q = np.zeros((nS, nA))
        # Choice kernel: tracks recent choice tendency per state
        CK = np.zeros((nS, nA))
        # Decay for choice kernel to keep it bounded
        ck_decay = 0.1 + 0.4 * ss_factor  # faster decay when set size is larger

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Apply global Q forgetting and choice-kernel decay
            if forget > 0:
                Q *= (1.0 - forget)
            CK *= (1.0 - ck_decay)

            # Build logits = beta*Q + pers*CK
            logits = beta * Q[s, :] + pers_weight_global * CK[s, :]

            # Softmax probability
            logits -= np.max(logits)
            p = np.exp(logits)
            p_sum = np.sum(p) + eps
            p /= p_sum

            total_logp += np.log(p[a] + eps)

            # Update Q with signed learning rate
            pe = r - Q[s, a]
            lr = lr_pos if pe >= 0 else lr_neg
            Q[s, a] += lr * pe

            # Update choice kernel to favor the chosen action in that state
            CK[s, a] += 1.0

    return -float(total_logp)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Model 2: RL + gated rule memory with age- and load-dependent gating and confidence-driven mixing.
    
    Mechanism:
      - Baseline Q-learning runs on all trials.
      - A "rule memory" stores a single action per state when learning seems reliable (reward with positive PE).
      - The probability to gate a rule into memory is controlled by a logistic function modulated by set size
        (less gating under higher load) and age (older adults have altered gating).
      - Each stored rule has a confidence C[s] that decays with load; action selection mixes a WM policy based
        on the stored rule and the RL policy, weighted by both gating propensity and current confidence.
    
    Parameters (model_parameters):
      lr : float
          Q-learning rate (0..1).
      invtemp : float
          Base inverse temperature for both policies; internally scaled for stability.
      gate_base : float
          Baseline gating logit; higher increases the probability to store a rule on rewarded trials.
      ss_gate_boost : float
          Decrease in gating logit per unit set-size factor (0 for 3, 1 for 6). Positive -> less gating at set size 6.
      age_gate_boost : float
          Additive increase to gating logit if older (>=45).
      wm_temp : float
          Inverse temperature used to construct the WM policy from the stored rule (sharpness of WM choice).
    
    Inputs:
      states : array-like (T,)
          State index per trial (0..nS-1 within a block).
      actions : array-like (T,)
          Chosen action per trial (0..2).
      rewards : array-like (T,)
          Binary rewards (0/1).
      blocks : array-like (T,)
          Block index per trial. Values indicate trials belonging to the same block.
      set_sizes : array-like (T,)
          Set size for each trial in its block (typically 3 or 6).
      age : array-like or scalar
          Participant age; older group defined as age >= 45.
      model_parameters : tuple
          Tuple of the six parameters described above.
    
    Returns:
      nll : float
          Negative log-likelihood of observed choices under the model.
    """
    eps = 1e-12
    nA = 3

    lr, invtemp, gate_base, ss_gate_boost, age_gate_boost, wm_temp = model_parameters
    beta = 5.0 * max(invtemp, eps)
    beta_wm = 5.0 * max(wm_temp, eps)

    age_val = age[0] if hasattr(age, "__len__") and not np.isscalar(age) else age
    is_older = 1.0 if age_val >= 45 else 0.0

    total_logp = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx].astype(int)
        block_actions = actions[idx].astype(int)
        block_rewards = rewards[idx].astype(float)
        nS = int(set_sizes[idx][0])

        ss_factor = (nS - 3) / 3.0  # 0 for 3, 1 for 6

        # Gating logit and probability
        gate_logit = gate_base - ss_gate_boost * ss_factor + age_gate_boost * is_older
        gate_prob = 1.0 / (1.0 + np.exp(-gate_logit))

        # Forgetting of WM confidence increases with load; tie to gate_base for parsimony
        forget_wm = (1.0 / (1.0 + np.exp(gate_base))) * (0.5 + 0.5 * ss_factor)  # in (0,1)

        Q = np.zeros((nS, nA))
        # Rule memory: stored action per state; -1 means none
        A_star = -1 * np.ones(nS, dtype=int)
        # Confidence per state in stored rule
        C = np.zeros(nS, dtype=float)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Decay WM confidence with load-dependent forgetting
            if forget_wm > 0:
                C *= (1.0 - forget_wm)
                # If confidence becomes tiny, drop the stored rule
                drop_idx = C < 1e-4
                if np.any(drop_idx):
                    A_star[drop_idx] = -1
                    C[drop_idx] = 0.0

            # RL policy
            q_s = Q[s, :].copy()
            q_s -= np.max(q_s)
            p_rl = np.exp(beta * q_s)
            p_rl = p_rl / (np.sum(p_rl) + eps)

            # WM policy: if a rule is stored, center policy on that action with sharpness beta_wm and weight C[s]
            if A_star[s] >= 0:
                logits_wm = np.zeros(nA) - 1.0  # base penalty
                logits_wm[A_star[s]] = 0.0      # favored action
                logits_wm *= beta_wm
                logits_wm -= np.max(logits_wm)
                p_wm = np.exp(logits_wm)
                p_wm = p_wm / (np.sum(p_wm) + eps)
                # Mixing weight depends on both gating propensity and current confidence
                rho = gate_prob * np.clip(C[s], 0.0, 1.0)
            else:
                p_wm = np.ones(nA) / nA
                rho = 0.0

            p = rho * p_wm + (1.0 - rho) * p_rl
            p = np.clip(p, eps, 1.0)
            p /= np.sum(p)

            total_logp += np.log(p[a])

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += lr * pe

            # Gating: if rewarded and PE positive, store rule with probability gate_prob
            if (r > 0.5) and (pe > 0):
                # Deterministic expectation under gating probability: update confidence toward gate_prob
                # This uses gate_prob meaningfully without sampling.
                A_star[s] = a
                C[s] = np.clip(C[s] + (1.0 - C[s]) * gate_prob, 0.0, 1.0)

    return -float(total_logp)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Model 3: RL with working-memory replay boost modulated by age and set size.
    
    Mechanism:
      - Baseline model-free Q-learning governs action values.
      - When the same state reoccurs within a limited "replay span" (in trials), learning is boosted,
        mimicking short-term working-memory aided consolidation of recent associations.
      - Older adults have a different effective replay span (age shift), and higher set size reduces
        the effective span (interference).
      - Action selection uses a standard softmax over Q-values.
    
    Parameters (model_parameters):
      lr : float
          Baseline Q-learning rate (0..1).
      invtemp : float
          Base inverse temperature (>0); internally scaled for stability.
      replay_gain : float
          Multiplicative learning-rate boost applied when within the replay span (>=0).
      replay_span : float
          Baseline replay span in trials (>=0), before age and set-size adjustments.
      age_replay_shift : float
          Additive shift to replay span if older (>=45). Positive extends span in older adults.
      ss_replay_cost : float
          Reduction in replay span per unit set-size factor (0 for 3, 1 for 6). Positive shrinks span at set size 6.
    
    Inputs:
      states : array-like (T,)
          State index per trial (0..nS-1 within a block).
      actions : array-like (T,)
          Chosen action per trial (0..2).
      rewards : array-like (T,)
          Binary rewards (0/1).
      blocks : array-like (T,)
          Block index per trial. Values indicate trials belonging to the same block.
      set_sizes : array-like (T,)
          Set size for each trial in its block (typically 3 or 6).
      age : array-like or scalar
          Participant age; older group defined as age >= 45.
      model_parameters : tuple
          Tuple of the six parameters described above.
    
    Returns:
      nll : float
          Negative log-likelihood of observed choices under the model.
    """
    eps = 1e-12
    nA = 3

    lr, invtemp, replay_gain, replay_span, age_replay_shift, ss_replay_cost = model_parameters
    beta = 5.0 * max(invtemp, eps)

    age_val = age[0] if hasattr(age, "__len__") and not np.isscalar(age) else age
    is_older = 1.0 if age_val >= 45 else 0.0

    total_logp = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx].astype(int)
        block_actions = actions[idx].astype(int)
        block_rewards = rewards[idx].astype(float)
        nS = int(set_sizes[idx][0])

        ss_factor = (nS - 3) / 3.0  # 0 for 3, 1 for 6

        # Effective replay span after age and load adjustments
        eff_span = replay_span + age_replay_shift * is_older - ss_replay_cost * ss_factor
        eff_span = max(0.0, eff_span)  # cannot be negative
        # We'll use integer lag threshold
        span_L = int(np.floor(eff_span + 1e-9))

        Q = np.zeros((nS, nA))

        # Track last occurrence index (within block) for each state
        last_occ = -1 * np.ones(nS, dtype=int)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Compute RL policy
            q_s = Q[s, :].copy()
            q_s -= np.max(q_s)
            p = np.exp(beta * q_s)
            p = p / (np.sum(p) + eps)

            total_logp += np.log(p[a] + eps)

            # Determine if current trial is within replay span since last occurrence
            lag_ok = False
            if last_occ[s] >= 0:
                lag = t - last_occ[s]
                if lag <= span_L and span_L > 0:
                    lag_ok = True

            # Learning-rate boost if within span
            boost = (1.0 + replay_gain) if lag_ok else 1.0
            Q[s, a] += lr * boost * (r - Q[s, a])

            # Update last occurrence
            last_occ[s] = t

    return -float(total_logp)