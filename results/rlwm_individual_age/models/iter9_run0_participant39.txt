def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Hybrid RL + episodic working-memory with age- and load-dependent encoding and decay.

    Mechanism:
    - RL: Q-learning over state-action values.
    - WM: When a state is encountered, the chosen action can be encoded into a slot-like memory
      with a probability that decreases with set size and in older adults. The memory trace decays
      as a function of the number of intervening trials since last visit to that state.
    - Policy: Mixture of WM and RL. If a reliable WM trace exists for a state, it dominates choice;
      otherwise choices are guided by RL softmax.

    Parameters
    ----------
    states : array-like of int
        State indices per trial within each block (0..set_size-1).
    actions : array-like of int
        Chosen actions per trial (0..2).
    rewards : array-like of float/int
        Binary rewards per trial (0 or 1).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (e.g., 3 or 6).
    age : array-like of float
        Participant age; age[0] is used. Age >= 45 treated as older.
    model_parameters : tuple/list of floats
        (alpha, beta, p_mem_base, decay_tau)
        - alpha: RL learning rate for Q-values.
        - beta: inverse temperature for the RL softmax policy.
        - p_mem_base: base probability of encoding an action into WM upon visit.
        - decay_tau: decay time constant (in trials) for WM strength between visits.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, p_mem_base, decay_tau = model_parameters
    age_val = float(age[0])
    is_older = 1.0 if age_val >= 45.0 else 0.0

    nA = 3
    eps = 1e-12
    total_loglik = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nS = int(block_set_sizes[0])

        # RL values
        Q = (1.0 / nA) * np.ones((nS, nA))

        # WM store: for each state, a stored action and a strength in [0,1], and last-visit time
        wm_action = -1 * np.ones(nS, dtype=int)
        wm_strength = np.zeros(nS)
        last_visit_t = -1 * np.ones(nS, dtype=int)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            ss = float(block_set_sizes[t])

            # Update WM decay based on time since last visit to state s
            if last_visit_t[s] >= 0:
                dt = (t - last_visit_t[s])
                # exponential decay in absence of visits
                if decay_tau > 0:
                    wm_strength[s] *= np.exp(-dt / max(decay_tau, 1e-6))
            last_visit_t[s] = t  # mark visit

            # Compute RL softmax policy
            q_s = Q[s, :]
            prefs = beta * (q_s - np.max(q_s))
            pi_rl = np.exp(prefs)
            pi_rl = pi_rl / (np.sum(pi_rl) + eps)

            # WM policy: if we have a stored action, propose a delta distribution, else uniform
            pi_wm = np.ones(nA) / nA
            if wm_action[s] >= 0 and wm_strength[s] > 0:
                pi_wm = np.zeros(nA)
                pi_wm[wm_action[s]] = 1.0

            # Arbitration weight equals current WM strength, clipped to [0,1]
            w_wm = float(np.clip(wm_strength[s], 0.0, 1.0))
            pi = w_wm * pi_wm + (1.0 - w_wm) * pi_rl

            total_loglik += np.log(pi[a] + eps)

            # RL update
            pe = r - Q[s, a]
            Q[s, a] = Q[s, a] + alpha * pe

            # WM encoding/update after observing outcome:
            # Base encoding probability decreases with set size and in older adults
            # so memory is less likely to be stored when load is high or participant is older.
            p_enc = p_mem_base * (3.0 / ss) * (1.0 - 0.3 * is_older)
            p_enc = float(np.clip(p_enc, 0.0, 1.0))

            # If reward is 1, we strengthen/overwrite memory toward chosen action.
            # If reward is 0, we can still encode but with lower effective gain via the same p_enc.
            if np.random.rand() < p_enc:
                wm_action[s] = a
                # Strength increment is larger following reward; capped at 1.0
                inc = 0.6 if r > 0.0 else 0.2
                wm_strength[s] = float(np.clip(wm_strength[s] + inc * (1.0 - 0.2 * is_older), 0.0, 1.0))

    return -float(total_loglik)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Actor-Critic with age- and load-modulated inverse temperature and policy eligibility trace.

    Mechanism:
    - Actor maintains state-action preferences H(s,a) updated by TD error with an eligibility trace.
    - Critic maintains a state value V(s) to compute TD error.
    - Choice probabilities are softmax over H(s,Â·) with an effective beta that decreases with load
      and for older adults.
    - Eligibility trace persists within a state to capture short-term working-memory benefits,
      but decays faster with larger set size.

    Parameters
    ----------
    states : array-like of int
        State indices per trial within each block (0..set_size-1).
    actions : array-like of int
        Chosen actions per trial (0..2).
    rewards : array-like of float/int
        Binary rewards per trial (0 or 1).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (e.g., 3 or 6).
    age : array-like of float
        Participant age; age[0] is used. Age >= 45 treated as older.
    model_parameters : tuple/list of floats
        (lr_actor, lr_critic, beta_base, lambda_tr, age_gain)
        - lr_actor: learning rate for updating actor preferences H.
        - lr_critic: learning rate for updating critic values V.
        - beta_base: base inverse temperature for the softmax over H.
        - lambda_tr: base eligibility trace decay (0..1).
        - age_gain: scales TD impact (older adults have reduced effective gain).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    lrA, lrC, beta_base, lambda_tr, age_gain = model_parameters
    age_val = float(age[0])
    is_older = 1.0 if age_val >= 45.0 else 0.0

    nA = 3
    eps = 1e-12
    total_loglik = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nS = int(block_set_sizes[0])

        # Actor preferences and critic values
        H = np.zeros((nS, nA))
        V = np.zeros(nS)

        # Eligibility traces per state-action
        E = np.zeros((nS, nA))

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            ss = float(block_set_sizes[t])

            # Effective inverse temperature decreases with load and in older adults
            beta_eff = beta_base * (3.0 / ss) * (1.0 - 0.3 * is_older)
            beta_eff = max(beta_eff, 1e-6)

            # Softmax policy over preferences H
            prefs = beta_eff * (H[s, :] - np.max(H[s, :]))
            pi = np.exp(prefs)
            pi = pi / (np.sum(pi) + eps)

            total_loglik += np.log(pi[a] + eps)

            # TD error using critic; no discounting across single-step trials
            delta = r - V[s]

            # Reduce effective TD impact in older adults
            delta_eff = delta * (1.0 - 0.4 * is_older) * age_gain

            # Update eligibility trace (state-conditional trace)
            # Decay is stronger under higher load
            lam_eff = np.clip(lambda_tr * (3.0 / ss), 0.0, 1.0)
            E[s, :] *= lam_eff
            E[s, a] += 1.0  # accumulate credit for chosen action in this state

            # Actor update with trace
            H += lrA * delta_eff * E

            # Optional soft normalization to keep preferences centered within state
            H[s, :] -= lrA * delta_eff * (np.mean(E[s, :]))

            # Critic update
            V[s] += lrC * delta

    return -float(total_loglik)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Bayesian success-rate learning with age- and load-modulated uncertainty bonus (UCB-like).

    Mechanism:
    - For each state-action pair, maintain a Beta(a,b) posterior over reward probability.
    - Use the posterior mean plus an uncertainty bonus proportional to posterior std as utility.
    - The exploration coefficient increases with set size and in older adults (more reliance on uncertainty).
    - Choices follow a softmax over utilities.

    Parameters
    ----------
    states : array-like of int
        State indices per trial within each block (0..set_size-1).
    actions : array-like of int
        Chosen actions per trial (0..2).
    rewards : array-like of float/int
        Binary rewards per trial (0 or 1).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (e.g., 3 or 6).
    age : array-like of float
        Participant age; age[0] is used. Age >= 45 treated as older.
    model_parameters : tuple/list of floats
        (a0, b0, beta, kappa)
        - a0: prior successes for Beta posterior (must be >0).
        - b0: prior failures for Beta posterior (must be >0).
        - beta: inverse temperature for softmax over utilities.
        - kappa: base coefficient for uncertainty bonus.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    a0, b0, beta, kappa = model_parameters
    a0 = max(a0, 1e-6)
    b0 = max(b0, 1e-6)
    age_val = float(age[0])
    is_older = 1.0 if age_val >= 45.0 else 0.0

    nA = 3
    eps = 1e-12
    total_loglik = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nS = int(block_set_sizes[0])

        # Beta posterior parameters per state-action
        A = a0 * np.ones((nS, nA))  # successes
        B = b0 * np.ones((nS, nA))  # failures

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            ss = float(block_set_sizes[t])

            # Posterior mean and std for each action in state s
            a_s = A[s, :]
            b_s = B[s, :]
            mean = a_s / (a_s + b_s)
            var = (a_s * b_s) / (((a_s + b_s) ** 2) * (a_s + b_s + 1.0) + eps)
            std = np.sqrt(var + eps)

            # Age- and load-modulated exploration coefficient
            k_eff = kappa * (ss / 3.0) * (1.0 + 0.5 * is_older)

            utility = mean + k_eff * std

            prefs = beta * (utility - np.max(utility))
            pi = np.exp(prefs)
            pi = pi / (np.sum(pi) + eps)

            total_loglik += np.log(pi[a] + eps)

            # Update Beta posterior only for chosen action
            A[s, a] += r
            B[s, a] += (1.0 - r)

    return -float(total_loglik)