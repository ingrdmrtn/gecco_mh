def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with execution confusion (action implementation noise) that increases with age group and set size.

    Mechanism
    ----------
    - The agent forms an intended choice via a softmax over Q-values (RL).
    - With probability 'confusion', the intended action is mis-executed uniformly to another action.
      This yields a closed-form mixture over the observed action.
    - Confusion probability is a logistic function of an intercept, age group (older >=45), and load (set size).

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..nS-1 within block).
    actions : array-like of int
        Executed action per trial (0..2).
    rewards : array-like of float/int
        Reward per trial (0 or 1).
    blocks : array-like of int
        Block ID per trial; learning resets per block.
    set_sizes : array-like of int
        Set size per trial (3 or 6). Constant within a block.
    age : array-like or scalar
        Participant age; age>=45 considered older (higher confusion).
    model_parameters : sequence of 5 floats
        alpha     : RL learning rate
        beta      : Inverse temperature for intended choice policy (scaled by 10 internally)
        c0        : Confusion intercept (log-odds)
        c_age     : Additional confusion for older group (additive on log-odds)
        c_load    : Confusion sensitivity to load (multiplies (nS-3) on log-odds)

    Returns
    -------
    nll : float
        Negative log-likelihood of observed actions.
    """
    alpha, beta, c0, c_age, c_load = model_parameters
    beta = beta * 10.0

    try:
        age_val = float(age[0])
    except Exception:
        age_val = float(age)
    is_older = 1.0 if age_val >= 45.0 else 0.0

    nA = 3
    eps = 1e-12
    total_log_p = 0.0

    states = np.asarray(states)
    actions = np.asarray(actions)
    rewards = np.asarray(rewards)
    blocks = np.asarray(blocks)
    set_sizes = np.asarray(set_sizes)

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]
        nS = int(block_set_sizes[0])

        # Initialize Q-values
        Q = (1.0 / nA) * np.ones((nS, nA))

        # Compute block-specific confusion probability (constant within block)
        z = c0 + c_age * is_older + c_load * (float(nS) - 3.0)
        confusion = 1.0 / (1.0 + np.exp(-z))
        confusion = min(max(confusion, 0.0), 1.0)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Intended policy via softmax over Q
            prefs = Q[s, :].copy()
            prefs -= np.max(prefs)
            pi = np.exp(beta * prefs)
            pi = pi / (np.sum(pi) + eps)

            # Execution confusion: P(exec=a) = (1-c)*pi[a] + c/(nA-1)*(1 - pi[a])
            p_a = (1.0 - confusion) * pi[a] + (confusion / (nA - 1.0)) * (1.0 - pi[a])
            total_log_p += np.log(max(p_a, eps))

            # RL update with the actually executed action and obtained reward
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

    return -float(total_log_p)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Hybrid RL + episodic (one-shot) memory with age- and load-dependent retrieval gate.

    Mechanism
    ----------
    - RL system: Q-learning with softmax policy (beta_rl).
    - Episodic memory: upon first rewarded action in a state, store that action as a memory trace.
      The episodic policy favors the memorized action via a separate precision beta_epi.
    - Arbitration: A logistic gate controls the probability of relying on episodic vs RL.
      Gate is modulated by age group (older reduces retrieval) and set size (lower load increases retrieval).

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..nS-1 within block).
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of float/int
        Reward per trial (0 or 1).
    blocks : array-like of int
        Block ID per trial; learning resets per block.
    set_sizes : array-like of int
        Set size per trial (3 or 6). Constant within a block.
    age : array-like or scalar
        Participant age; older (>=45) assumed to have reduced episodic retrieval.
    model_parameters : sequence of 6 floats
        alpha     : RL learning rate
        beta_rl   : Inverse temperature for RL (scaled by 10 internally)
        beta_epi  : Inverse temperature for episodic policy (scaled by 10 internally)
        gate0     : Retrieval gate intercept (log-odds)
        gate_age  : Retrieval gate age effect (multiplied by is_older, typically negative)
        gate_load : Retrieval gate load effect (multiplies (3 - nS); positive favors small set sizes)

    Returns
    -------
    nll : float
        Negative log-likelihood of observed actions.
    """
    alpha, beta_rl, beta_epi, gate0, gate_age, gate_load = model_parameters
    beta_rl = beta_rl * 10.0
    beta_epi = beta_epi * 10.0

    try:
        age_val = float(age[0])
    except Exception:
        age_val = float(age)
    is_older = 1.0 if age_val >= 45.0 else 0.0

    nA = 3
    eps = 1e-12
    total_log_p = 0.0

    states = np.asarray(states)
    actions = np.asarray(actions)
    rewards = np.asarray(rewards)
    blocks = np.asarray(blocks)
    set_sizes = np.asarray(set_sizes)

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]
        nS = int(block_set_sizes[0])

        # RL values
        Q = (1.0 / nA) * np.ones((nS, nA))
        # Episodic memory: -1 means no memory for that state yet
        M = -1 * np.ones(nS, dtype=int)

        # Gate is constant within block (depends on load and age)
        z_gate = gate0 + gate_age * is_older + gate_load * (3.0 - float(nS))
        gate = 1.0 / (1.0 + np.exp(-z_gate))
        gate = min(max(gate, 0.0), 1.0)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # RL policy
            prefs_rl = Q[s, :].copy()
            prefs_rl -= np.max(prefs_rl)
            p_rl = np.exp(beta_rl * prefs_rl)
            p_rl = p_rl / (np.sum(p_rl) + eps)

            # Episodic policy: if memory exists, bias to that action
            if M[s] >= 0:
                onehot = np.zeros(nA)
                onehot[M[s]] = 1.0
                prefs_epi = onehot  # preference vector with 1 at memorized action
                prefs_epi -= np.max(prefs_epi)
                p_epi = np.exp(beta_epi * prefs_epi)
                p_epi = p_epi / (np.sum(p_epi) + eps)
            else:
                # If no memory, episodic policy is uniform
                p_epi = np.ones(nA) / nA

            # Arbitration mixture
            p_mix = gate * p_epi + (1.0 - gate) * p_rl
            p_a = p_mix[a]
            total_log_p += np.log(max(p_a, eps))

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # Episodic memory update: one-shot store on rewarded action
            if (M[s] < 0) and (r > 0.0):
                M[s] = a

    return -float(total_log_p)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with uncertainty bonus (UCB-like) modulated by age group and set size.

    Mechanism
    ----------
    - Standard Q-learning for value updates.
    - Choice preferences include an exploration bonus inversely proportional to sqrt(visit count),
      encouraging exploration early on.
    - The strength of the uncertainty bonus is a linear function of an intercept plus
      age group (older >=45) and load (set size) terms. Older adults and larger set sizes
      can show altered bonus magnitude (e.g., reduced directed exploration).

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..nS-1 within block).
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of float/int
        Reward per trial (0 or 1).
    blocks : array-like of int
        Block ID per trial; learning resets per block.
    set_sizes : array-like of int
        Set size per trial (3 or 6). Constant within a block.
    age : array-like or scalar
        Participant age; older (>=45) modifies uncertainty bonus.
    model_parameters : sequence of 5 floats
        alpha      : RL learning rate
        beta       : Inverse temperature for softmax (scaled by 10 internally)
        bonus0     : Base uncertainty bonus scale
        bonus_age  : Additional bonus for older group (additive)
        bonus_load : Bonus sensitivity to load (multiplies (3 - nS); positive favors smaller sets)

    Returns
    -------
    nll : float
        Negative log-likelihood of observed actions.
    """
    alpha, beta, bonus0, bonus_age, bonus_load = model_parameters
    beta = beta * 10.0

    try:
        age_val = float(age[0])
    except Exception:
        age_val = float(age)
    is_older = 1.0 if age_val >= 45.0 else 0.0

    nA = 3
    eps = 1e-12
    total_log_p = 0.0

    states = np.asarray(states)
    actions = np.asarray(actions)
    rewards = np.asarray(rewards)
    blocks = np.asarray(blocks)
    set_sizes = np.asarray(set_sizes)

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]
        nS = int(block_set_sizes[0])

        # Initialize Q-values and visit counts
        Q = (1.0 / nA) * np.ones((nS, nA))
        N = np.zeros((nS, nA))  # visit counts to compute uncertainty bonus

        # Block-specific bonus scale
        bonus_scale = bonus0 + bonus_age * is_older + bonus_load * (3.0 - float(nS))

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Uncertainty bonus: higher for low-visited actions
            bonus = bonus_scale / np.sqrt(N[s, :] + 1.0)

            prefs = Q[s, :] + bonus
            prefs -= np.max(prefs)
            p = np.exp(beta * prefs)
            p = p / (np.sum(p) + eps)

            p_a = p[a]
            total_log_p += np.log(max(p_a, eps))

            # Update counts and Q
            N[s, a] += 1.0
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

    return -float(total_log_p)