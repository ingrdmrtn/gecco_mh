def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + Bayesian inference mixture with age-specific capacity and lapse.

    Mechanism
    - Model-free RL: Tabular Q-learning with softmax choice.
    - Model-based inference: For each state, keep Dirichlet counts over actions and use the
      posterior-predictive P(a|s) from counts as a "deterministic rule" learner.
    - Capacity-based arbitration: The weight on inference increases when the participant's
      capacity exceeds the current set size. Younger and older groups have separate capacity
      parameters.
    - Lapse noise: A small probability of random choice independent of policy.

    Parameters
    ----------
    states : array-like of int
        State index on each trial within block (0..nS-1).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of {0,1}
        Feedback on each trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (3 or 6).
    age : array-like
        Participant age (use age[0]); age group: younger (<45) vs older (>=45).
    model_parameters : sequence of 5 floats
        [alpha, beta, cap_young, cap_old, lapse]
        - alpha: Q-learning rate (sigmoid to [0,1]).
        - beta: inverse temperature for RL softmax (>=0).
        - cap_young: capacity parameter for younger group (real, higher => more inference).
        - cap_old: capacity parameter for older group.
        - lapse: lapse probability (sigmoid to [0,1]) mixed with uniform.

    Returns
    -------
    nll : float
        Negative log-likelihood of observed actions under the model.
    """
    alpha, beta, cap_y, cap_o, lapse = model_parameters
    alpha = 1.0 / (1.0 + np.exp(-alpha))
    beta = np.maximum(beta, 1e-6)
    lapse = 1.0 / (1.0 + np.exp(-lapse))
    age_group = 1 if age[0] >= 45 else 0
    capacity = cap_o if age_group == 1 else cap_y

    nA = 3
    nll = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)
        nS = int(block_set_sizes[0])

        # Initialize RL and Dirichlet counts
        Q = (1.0 / nA) * np.ones((nS, nA))
        counts = np.ones((nS, nA))  # symmetric Dirichlet(1) prior

        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]
            nS_t = int(block_set_sizes[t])

            # RL policy
            logits_rl = beta * (Q[s, :] - np.max(Q[s, :]))
            p_rl = np.exp(logits_rl)
            p_rl_sum = np.maximum(1e-12, np.sum(p_rl))
            p_rl /= p_rl_sum

            # Inference policy from Dirichlet posterior-predictive
            p_inf = counts[s, :].astype(float)
            p_inf /= np.maximum(1e-12, np.sum(p_inf))

            # Arbitration weight: more weight to inference when capacity exceeds load
            arb_logit = capacity - float(nS_t)
            w_inf = 1.0 / (1.0 + np.exp(-arb_logit))
            p_mix = w_inf * p_inf + (1.0 - w_inf) * p_rl

            # Lapse mixture
            p_total = (1.0 - lapse) * p_mix + lapse * (np.ones(nA) / nA)

            pa = np.clip(p_total[a], 1e-12, 1.0)
            nll -= np.log(pa)

            # Updates
            delta = r - Q[s, a]
            Q[s, a] += alpha * delta

            # Update counts only with evidence consistent with a deterministic mapping:
            # add reward to chosen action; small leak to others on nonreward.
            if r >= 0.5:
                counts[s, a] += 1.0
            else:
                # weak evidence against chosen action: softly increment others to keep total scale
                inc = 1.0 / (nA - 1)
                for aa in range(nA):
                    if aa != a:
                        counts[s, aa] += inc

    return nll


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with load- and age-modulated asymmetric learning and age-sensitive perseveration.

    Mechanism
    - Asymmetric learning rates: Separate positive vs negative learning rates derived from
      a base rate and a negative/positive ratio. Both are reduced under high load.
    - Perseveration bias: Additive bias to repeat the last action within a state; the bias
      magnitude depends on age group (scaled down in older adults).
    - Softmax action selection with inverse temperature beta.

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..nS-1).
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of {0,1}
        Feedback per trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (3 or 6).
    age : array-like
        Participant age; age[0] determines group (<45 younger, >=45 older).
    model_parameters : sequence of 5 floats
        [alpha_base, beta, neg_ratio, load_alpha_drop, age_stick_weight]
        - alpha_base: base (positive) learning rate (sigmoid to [0,1]).
        - beta: inverse temperature for softmax (>=0).
        - neg_ratio: maps via sigmoid to (0,1) to scale negative LR relative to positive LR.
        - load_alpha_drop: fraction in [0,1] (sigmoid) that reduces learning rates at set size 6.
        - age_stick_weight: base perseveration strength; scaled by age group.

    Returns
    -------
    nll : float
        Negative log-likelihood of observed actions.
    """
    alpha_base, beta, neg_ratio, load_alpha_drop, age_stick_weight = model_parameters
    alpha_pos = 1.0 / (1.0 + np.exp(-alpha_base))
    neg_scale = 1.0 / (1.0 + np.exp(-neg_ratio))
    alpha_neg = np.clip(alpha_pos * neg_scale, 0.0, 1.0)
    load_drop = 1.0 / (1.0 + np.exp(-load_alpha_drop))  # in [0,1]
    beta = np.maximum(beta, 1e-6)
    age_group = 1 if age[0] >= 45 else 0
    # Age-sensitive perseveration: reduced to 50% in older group
    stick_strength = age_stick_weight * (1.0 - 0.5 * age_group)

    nA = 3
    nll = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)
        nS = int(block_set_sizes[0])

        Q = (1.0 / nA) * np.ones((nS, nA))
        last_action = -np.ones(nS, dtype=int)

        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]
            nS_t = int(block_set_sizes[t])

            # Load-dependent adjustment to learning rates
            load_component = (float(nS_t) - 3.0) / 3.0  # 0 for 3, 1 for 6
            adj = 1.0 - load_drop * load_component
            alpha_pos_t = np.clip(alpha_pos * adj, 0.0, 1.0)
            alpha_neg_t = np.clip(alpha_neg * adj, 0.0, 1.0)

            # Softmax with perseveration bias on last action in this state
            logits = beta * (Q[s, :] - np.max(Q[s, :]))
            if last_action[s] >= 0:
                logits[last_action[s]] += stick_strength
            p = np.exp(logits)
            p /= np.maximum(1e-12, np.sum(p))

            pa = np.clip(p[a], 1e-12, 1.0)
            nll -= np.log(pa)

            # Update Q
            delta = r - Q[s, a]
            if delta >= 0.0:
                Q[s, a] += alpha_pos_t * delta
            else:
                Q[s, a] += alpha_neg_t * delta

            # Update perseveration memory
            last_action[s] = a

    return nll


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with global action prior arbitration modulated by load and age.

    Mechanism
    - Model-free RL: State-action Q-learning with softmax choice.
    - Global prior: A block-level action prior G over the 3 actions, updated from outcomes
      irrespective of state (captures episodic/global heuristics such as "action 0 tends to pay off").
    - Arbitration: Mix the state-specific RL policy with the global prior; the reliance on the
      global prior increases with set size and with older age.
    - All parameters are used: alpha (LR), beta (temperature), prior_gain (LR for global prior),
      load_prior_penalty and age_prior_penalty (determine reliance on global prior).

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..nS-1).
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of {0,1}
        Feedback per trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (3 or 6).
    age : array-like
        Participant age; age[0] defines group (<45 younger, >=45 older).
    model_parameters : sequence of 5 floats
        [alpha, beta, prior_gain, load_prior_penalty, age_prior_penalty]
        - alpha: Q-learning rate (sigmoid to [0,1]).
        - beta: inverse temperature for RL softmax (>=0).
        - prior_gain: learning rate for updating the global prior (sigmoid to [0,1]).
        - load_prior_penalty: nonnegative coefficient scaling reliance on the global prior with load.
        - age_prior_penalty: nonnegative coefficient scaling reliance on the global prior with age.

    Returns
    -------
    nll : float
        Negative log-likelihood of observed actions under the model.
    """
    alpha, beta, prior_gain, load_prior_penalty, age_prior_penalty = model_parameters
    alpha = 1.0 / (1.0 + np.exp(-alpha))
    beta = np.maximum(beta, 1e-6)
    prior_gain = 1.0 / (1.0 + np.exp(-prior_gain))
    load_prior_penalty = np.maximum(0.0, load_prior_penalty)
    age_prior_penalty = np.maximum(0.0, age_prior_penalty)
    age_group = 1 if age[0] >= 45 else 0

    nA = 3
    nll = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)
        nS = int(block_set_sizes[0])

        Q = (1.0 / nA) * np.ones((nS, nA))

        # Global prior represented by logits H and probabilities G
        H = np.zeros(nA)
        # Initialize G uniform
        G = np.ones(nA) / nA

        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]
            nS_t = int(block_set_sizes[t])

            # RL policy for current state
            logits_rl = beta * (Q[s, :] - np.max(Q[s, :]))
            p_rl = np.exp(logits_rl)
            p_rl /= np.maximum(1e-12, np.sum(p_rl))

            # Global prior probabilities from logits H
            H_centered = H - np.max(H)
            G = np.exp(H_centered)
            G /= np.maximum(1e-12, np.sum(G))

            # Arbitration weight toward global prior increases with load and age
            load_component = (float(nS_t) - 3.0) / 3.0  # 0 for 3, 1 for 6
            w_logit = load_prior_penalty * load_component + age_prior_penalty * age_group
            w = 1.0 / (1.0 + np.exp(-w_logit))
            p_total = (1.0 - w) * p_rl + w * G

            pa = np.clip(p_total[a], 1e-12, 1.0)
            nll -= np.log(pa)

            # Update state-action Q
            delta = r - Q[s, a]
            Q[s, a] += alpha * delta

            # Update global prior logits with bandit-like delta rule
            # Push chosen action up if r=1, down if r=0, relative to current G[a]
            g_delta = r - G[a]
            H[a] += prior_gain * g_delta

    return nll