def simulate_model(stimulus, blocks, set_sizes, correct_answer, age, parameters):
    """
    RL+WM mixture with decay and load-/age-dependent arbitration (simulation).

    Parameters:
    - stimulus: array of state identifiers per trial (assumed 0..nS-1 within each block)
    - blocks: array of block indices per trial
    - set_sizes: array of set size per trial (constant within a block; typically 3 or 6)
    - correct_answer: array of correct actions per trial (0..2)
    - age: participant's age (float)
    - parameters: list with 3-6 elements
        If len==3:      [lr, wm_weight, softmax_beta]
        If len==4:      [lr, wm_weight, softmax_beta, wm_decay]
        If len==5:      [lr, wm_weight, softmax_beta, wm_decay, wm_eta]
        If len==6:      [lr, wm_weight, softmax_beta, wm_decay, wm_eta, lapse]

    Returns:
    - simulated_actions: array of sampled actions per trial
    - simulated_rewards: array of binary rewards per trial
    """
    import numpy as np

    # Unpack parameters with defaults as in fitting code
    if len(parameters) == 3:
        lr, wm_weight, softmax_beta = parameters
        wm_decay = 0.1
        wm_eta = 0.9
        lapse = 1e-6
    elif len(parameters) == 4:
        lr, wm_weight, softmax_beta, wm_decay = parameters
        wm_eta = 0.9
        lapse = 1e-6
    elif len(parameters) == 5:
        lr, wm_weight, softmax_beta, wm_decay, wm_eta = parameters
        lapse = 1e-6
    else:
        lr, wm_weight, softmax_beta, wm_decay, wm_eta, lapse = parameters

    # Scales
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # near-deterministic WM

    n_trials = len(stimulus)
    simulated_actions = np.zeros(n_trials, dtype=int)
    simulated_rewards = np.zeros(n_trials, dtype=int)

    # Age factor
    age_factor = 1.0 if age < 45 else 0.7

    # Iterate by block to reset latent states
    trial_order = np.arange(n_trials)
    for b in np.unique(blocks):
        idx = trial_order[blocks == b]
        block_states = stimulus[idx]
        block_correct = correct_answer[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])  # set size constant within block

        # Initialize RL and WM
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Load factor based on set size (1.0 for 3; 0.5 for 6)
        load_factor = min(1.0, 3.0 / float(nS))
        wm_w_eff = wm_weight * age_factor * load_factor

        for t_local, tr in enumerate(idx):
            s = int(block_states[t_local])
            # Compute RL policy (stable softmax)
            Q_s = q[s, :]
            z_rl = softmax_beta * (Q_s - np.max(Q_s))
            p_rl = np.exp(z_rl)
            p_rl /= np.sum(p_rl)

            # Compute WM policy (stable softmax)
            W_s = w[s, :]
            z_wm = softmax_beta_wm * (W_s - np.max(W_s))
            p_wm = np.exp(z_wm)
            p_wm /= np.sum(p_wm)

            # Mixture and lapse
            p_mix = wm_w_eff * p_wm + (1.0 - wm_w_eff) * p_rl
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)

            # Sample action
            a = np.random.choice(nA, p=p_total)
            simulated_actions[tr] = a

            # Outcome determined by correct action on this trial
            r = 1 if a == int(block_correct[t_local]) else 0
            simulated_rewards[tr] = r

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # Global WM decay toward uniform prior
            w = (1.0 - wm_decay) * w + wm_decay * w_0

            # WM update depending on outcome
            if r == 1:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = (1.0 - wm_eta) * w[s, :] + wm_eta * onehot
            else:
                w[s, :] = (1.0 - 0.5 * wm_eta) * w[s, :] + 0.5 * wm_eta * w_0[s, :]

    return simulated_actions, simulated_rewards