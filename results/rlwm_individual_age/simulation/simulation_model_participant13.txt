def simulate_model(stimulus, blocks, set_sizes, correct_answer, age, parameters):
    """
    Simulate choices and rewards using the RL+WM model with set-size and age-based WM gating,
    separate RL learning rates for gains/losses, and a lapse process.

    Inputs:
    - stimulus: array of state identities per trial (int, assumed 0..nS-1 within block)
    - blocks: array of block indices per trial
    - set_sizes: array of block set sizes per trial (constant within a block)
    - correct_answer: array of correct action per trial (int in 0..2)
    - age: participant age (float)
    - parameters: [lr_pos, lr_neg, wm_weight, softmax_beta, lapse, gate_slope]

    Returns:
    - simulated_actions: array of sampled actions per trial (int)
    - simulated_rewards: array of binary rewards per trial (int)
    """
    import numpy as np

    lr_pos, lr_neg, wm_weight, softmax_beta, lapse, gate_slope = parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    n_trials = len(stimulus)
    simulated_actions = np.zeros(n_trials, dtype=int)
    simulated_rewards = np.zeros(n_trials, dtype=int)

    # Age-dependent gating settings
    is_older = 1 if float(age) >= 45 else 0
    K0 = 3.0 if is_older else 4.0
    slope_factor = 1.5 if is_older else 1.0

    # Iterate by blocks
    unique_blocks = np.unique(blocks)
    tr_global = 0
    for b in unique_blocks:
        block_idx = np.where(blocks == b)[0]
        block_states = stimulus[block_idx].astype(int)
        block_correct = correct_answer[block_idx].astype(int)

        # Determine action/state spaces
        nA = 3
        # Use provided block set size (assumed constant within block)
        nS = int(set_sizes[block_idx][0])

        # Map state -> correct action for this block (assumes stable mapping within block)
        # If multiple observations per state, take the first occurrence.
        correct_actions_per_state = np.zeros(nS, dtype=int)
        for s in range(nS):
            # Find first trial with this state; if none (should not happen), default to 0
            mask_s = np.where(block_states == s)[0]
            correct_actions_per_state[s] = block_correct[mask_s[0]] if len(mask_s) > 0 else 0

        # Initialize RL and WM
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size and age-dependent WM gating
        slope = slope_factor * gate_slope
        gate = 1.0 / (1.0 + np.exp(slope * (nS - K0)))
        wm_mix = wm_weight * gate

        # Trial loop within block
        for t in range(len(block_states)):
            s = int(block_states[t])

            # Softmax policies
            Q_s = q[s, :]
            W_s = w[s, :]

            # RL softmax
            z_rl = softmax_beta * (Q_s - np.max(Q_s))
            p_rl = np.exp(z_rl)
            p_rl /= np.sum(p_rl)

            # WM softmax
            z_wm = softmax_beta_wm * (W_s - np.max(W_s))
            p_wm = np.exp(z_wm)
            p_wm /= np.sum(p_wm)

            # Mixture and lapse
            p_mixture = wm_mix * p_wm + (1.0 - wm_mix) * p_rl
            p_total = (1.0 - lapse) * p_mixture + lapse * (1.0 / nA)
            p_total = np.clip(p_total, 1e-12, 1.0)
            p_total /= np.sum(p_total)

            # Sample action
            a = int(np.random.choice(nA, p=p_total))
            simulated_actions[block_idx[t]] = a

            # Outcome
            correct_a = int(correct_actions_per_state[s])
            r = 1 if a == correct_a else 0
            simulated_rewards[block_idx[t]] = r

            # WM decay before update (as in fitting code)
            decay = 0.05
            w = (1.0 - decay) * w + decay * w_0

            # RL update with asymmetric learning rates
            pe = r - Q_s[a]
            lr = lr_pos if pe > 0 else lr_neg
            q[s, a] += lr * pe

            # WM update: store rewarded action or softly avoid failed action
            if r >= 1.0:
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                shift = 0.2
                loss_mass = min(shift, w[s, a])
                w[s, a] -= loss_mass
                distribute = loss_mass / (nA - 1)
                for aa in range(nA):
                    if aa != a:
                        w[s, aa] += distribute

            tr_global += 1

    return simulated_actions, simulated_rewards