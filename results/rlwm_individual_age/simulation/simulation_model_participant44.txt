def simulate_model(stimulus, blocks, set_sizes, correct_answer, age, parameters):
    """
    Simulate choices and rewards from the RL+WM model with perseveration, lapse, and win-stay WM.

    Parameters
    ----------
    - stimulus: array of state identifiers per trial (can be arbitrary labels within a block)
    - blocks: array of block index per trial
    - set_sizes: array of set size per trial (constant within a block; used to set nS)
    - correct_answer: array of correct actions per trial (0..2)
    - age: float participant age
    - parameters: list/tuple
        (lr_pos, lr_neg, wm_weight, softmax_beta, perseveration, lapse)

    Returns
    -------
    - simulated_actions: array of simulated action choices (0..2)
    - simulated_rewards: array of simulated binary rewards (0/1)
    """
    import numpy as np

    lr_pos, lr_neg, wm_weight, softmax_beta, perseveration, lapse = parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    age_val = float(age)
    is_older = 1.0 if age_val >= 45 else 0.0

    n_trials = len(stimulus)
    simulated_actions = np.zeros(n_trials, dtype=int)
    simulated_rewards = np.zeros(n_trials, dtype=int)

    tr = 0
    # iterate blocks in order of appearance
    for b in np.unique(blocks):
        block_idx = np.where(blocks == b)[0]
        block_states_raw = stimulus[block_idx]
        block_correct = correct_answer[block_idx]
        # Map block-specific states to 0..nS-1
        unique_states = np.unique(block_states_raw)
        state_to_local = {s: i for i, s in enumerate(unique_states)}
        block_states = np.array([state_to_local[s] for s in block_states_raw], dtype=int)

        nA = 3
        # Use provided set size for the block (as in fitting code)
        nS = int(set_sizes[block_idx[0]])

        # Build per-state correct action (assume consistent mapping within state)
        correct_actions = np.zeros(nS, dtype=int)
        # For safety, if some states in set_sizes aren't present in unique_states,
        # we only fill those we see.
        for s_local in range(len(unique_states)):
            mask_s = (block_states == s_local)
            # take first occurrence
            correct_actions[s_local] = int(block_correct[mask_s][0])

        # Initialize RL
        q = (1.0 / nA) * np.ones((nS, nA))

        # WM: implicit representation via (mem_action, mem_strength)
        mem_action = -1 * np.ones(nS, dtype=int)
        mem_strength = np.zeros(nS)

        # Perseveration: last chosen action per state
        last_action = -1 * np.ones(nS, dtype=int)
        pers_bias = perseveration * (1.0 + 0.5 * is_older)

        # Lapse and WM weight depend on age and load
        lapse_rate = 1.0 / (1.0 + np.exp(-(lapse + 0.5 * is_older)))
        lapse_rate = float(np.clip(lapse_rate, 0.0, 1.0))

        wm_block_weight = wm_weight * (3.0 / float(nS)) * (1.0 / (1.0 + 0.5 * is_older))
        wm_block_weight = float(np.clip(wm_block_weight, 0.0, 1.0))

        # WM decay toward zero-strength memory
        gamma = 1.0 - np.exp(-0.6 * (3.0 / float(nS)) / (1.0 + 0.5 * is_older))
        gamma = float(np.clip(gamma, 0.0, 1.0))

        for t in range(len(block_states)):
            global_t = block_idx[t]
            s = int(block_states[t])

            # RL policy with perseveration bias on last action in this state
            Q_s = q[s, :].copy()
            if last_action[s] >= 0:
                Q_s[last_action[s]] += pers_bias

            exp_Q = np.exp(softmax_beta * Q_s)
            p_rl = exp_Q / np.sum(exp_Q)

            # WM policy: only remembers last rewarded action for state with some strength
            W_s = np.zeros(nA)
            if mem_action[s] >= 0:
                W_s[mem_action[s]] = mem_strength[s]

            exp_W = np.exp(softmax_beta_wm * W_s)
            p_wm = exp_W / np.sum(exp_W)




            # Mixture and lapse
            p_mix = wm_block_weight * p_wm + (1.0 - wm_block_weight) * p_rl
            p_total = (1.0 - lapse_rate) * p_mix + lapse_rate * (1.0 / nA)

            # Sample action
            a = int(np.random.choice(nA, p=p_total))
            simulated_actions[global_t] = a

            # Outcome given correct action for this state
            correct_a = int(correct_actions[s])
            r = 1 if a == correct_a else 0
            simulated_rewards[global_t] = r

            # RL update
            alpha = lr_pos if r > 0.5 else lr_neg
            q[s, a] += alpha * (r - q[s, a])

            # WM decay and win-stay update
            mem_strength[s] = (1.0 - gamma) * mem_strength[s]
            if r > 0.5:
                mem_action[s] = a
                mem_strength[s] = 1.0

            # Update perseveration memory
            last_action[s] = a

            tr += 1

    return simulated_actions, simulated_rewards