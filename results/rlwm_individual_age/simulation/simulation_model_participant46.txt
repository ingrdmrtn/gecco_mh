def simulate_model(stimulus, blocks, set_sizes, correct_answer, age, parameters):
    """
    Simulate choices and rewards using the RL+WM mixture with capacity-limited working memory
    and age-related capacity penalty.

    Inputs:
    - stimulus: array of state identities per trial
    - blocks: array of block indices per trial
    - set_sizes: array of set sizes per trial (block-constant)
    - correct_answer: array of correct action per trial (assumed consistent within state)
    - age: participant's age (float)
    - parameters: [lr, wm_weight, softmax_beta, K, decay, age_penalty]

    Returns:
    - simulated_actions: array of sampled actions per trial (ints in [0,2])
    - simulated_rewards: array of rewards (0/1) per trial
    """
    import numpy as np

    lr, wm_weight, softmax_beta, K, decay, age_penalty = parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    eps = 1e-12
    age_val = float(age)
    older = 1.0 if age_val >= 45 else 0.0
    K_eff = max(0.0, K - age_penalty * older)

    n_trials = len(stimulus)
    simulated_actions = np.zeros(n_trials, dtype=int)
    simulated_rewards = np.zeros(n_trials, dtype=int)

    tr = 0
    for b in np.unique(blocks):
        block_idx = np.where(blocks == b)[0]
        block_states_raw = np.array(stimulus[block_idx], dtype=int)
        block_correct_raw = np.array(correct_answer[block_idx], dtype=int)

        # Map state labels within block to 0..nS-1 to ensure valid indexing
        unique_states = np.unique(block_states_raw)
        state_map = {s: i for i, s in enumerate(unique_states)}
        block_states = np.array([state_map[s] for s in block_states_raw], dtype=int)

        # Determine set size (model uses block-constant set size)
        if len(block_idx) > 0:
            nS = int(set_sizes[block_idx[0]])
        else:
            nS = len(unique_states)
        nS = max(nS, len(unique_states))  # be robust if metadata is off
        nA = 3

        # Derive correct action per state (assumed consistent within a block)
        correct_per_state = np.zeros(nS, dtype=int)
        for s_new in range(nS):
            mask = np.where(block_states == s_new)[0]
            if len(mask) > 0:
                correct_per_state[s_new] = int(block_correct_raw[mask[0]])
            else:
                # Fallback: if a state index is missing (shouldn't happen), set a default
                correct_per_state[s_new] = 0

        # Initialize RL and WM
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Precompute mixture scaling due to load/capacity
        load_factor = 0.0 if nS <= 0 else min(1.0, K_eff / float(nS))
        eta_base = np.clip(wm_weight * load_factor, 0.0, 1.0)

        for t in range(len(block_states)):
            s = int(block_states[t])

            # WM decay toward uniform before choice
            w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]

            Q_s = q[s, :]
            W_s = w[s, :]


            exp_Q = np.exp(softmax_beta * Q_s)
            p_rl = exp_Q / np.sum(exp_Q)

            exp_W = np.exp(softmax_beta_wm * W_s)
            p_wm = exp_W / np.sum(exp_W)


            eta = eta_base  # constant within block given nS and age/capacity
            p_total = eta * p_wm + (1.0 - eta) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            p_total /= np.sum(p_total)

            # Sample action
            a = int(np.random.choice(nA, p=p_total))
            simulated_actions[tr + t] = a

            # Reward based on correct mapping
            correct_a = int(correct_per_state[s])
            r = 1 if a == correct_a else 0
            simulated_rewards[tr + t] = r

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM update
            if r > 0.5:
                # Move toward one-hot at chosen action
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - lr) * w[s, :] + lr * target
            else:
                # Suppress chosen action slightly, renormalize
                w[s, a] = max(eps, w[s, a] * (1.0 - lr))
                row_sum = np.sum(w[s, :])
                if row_sum > 0:
                    w[s, :] /= row_sum
                else:
                    w[s, :] = w_0[s, :]

        tr += len(block_states)

    return simulated_actions, simulated_rewards