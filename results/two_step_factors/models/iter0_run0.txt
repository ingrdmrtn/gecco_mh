In this task, the participant makes a sequence of choices to earn money. 
On each trial, they first select between two images, which probabilistically 
transitions them to one of two possible second-stage screens. 
This transition relies on fixed rules; for example, one image might have a 
70% chance of leading to screen A (a "common" transition) and a 30% chance 
of leading to screen B (a "rare" transition). 
Once in the second stage, the participant chooses between two new images, 
each carrying its own probability of yielding a 25-cent reward. 
Crucially, these reward probabilities drift slowly over time between 25% and 75%, 
meaning the participant must continuously adapt to figure out which final image 
is currently paying out the most.

Here is the data from several participants: 
Participant 1:
The participant chose spaceship 0, traveled to second stage state 0,
made choice 0, and received 1 coins.

The participant chose spaceship 0, traveled to second stage state 0,
made choice 0, and received 1 coins.

The participant chose spaceship 0, traveled to second stage state 0,
made choice 0, and received 1 coins.

The participant chose spaceship 0, traveled to second stage state 0,
made choice 1, and received 0 coins.

The participant chose spaceship 0, traveled to second stage state 0,
made choice 1, and received 0 coins.

The participant chose spaceship 0, traveled to second stage state 1,
made choice 1, and received 0 coins.

The participant chose spaceship 0, traveled to second stage state 0,
made choice 1, and received 0 coins.

The participant chose spaceship 1, traveled to second stage state 1,
made choice 0, and received 1 coins.

The participant chose spaceship 1, traveled to second stage state 0,
made choice 0, and received 0 coins.

The participant chose spaceship 1, traveled to second stage state 0,
made choice 0, and received 1 coins.


Participant 2:
The participant chose spaceship 1, traveled to second stage state 1,
made choice 0, and received 0 coins.

The participant chose spaceship 1, traveled to second stage state 0,
made choice 1, and received 0 coins.

The participant chose spaceship 1, traveled to second stage state 1,
made choice 1, and received 1 coins.

The participant chose spaceship 1, traveled to second stage state 1,
made choice 1, and received 0 coins.

The participant chose spaceship 0, traveled to second stage state 0,
made choice 0, and received 1 coins.

The participant chose spaceship 0, traveled to second stage state 0,
made choice 0, and received 0 coins.

The participant chose spaceship 0, traveled to second stage state 0,
made choice 0, and received 1 coins.

The participant chose spaceship 0, traveled to second stage state 0,
made choice 0, and received 1 coins.

The participant chose spaceship 0, traveled to second stage state 0,
made choice 0, and received 0 coins.

The participant chose spaceship 0, traveled to second stage state 1,
made choice 0, and received 1 coins.

Your task: Propose **3 new cognitive models** as Python functions:
`cognitive_model1`, `cognitive_model2`, `cognitive_model3`

### Implementation Guidelines
Each model must be a standalone Python function.
Function names must be `cognitive_model1`, `cognitive_model2`, etc.
Take as input: `action_1, state, action_2, reward, model_parameters`.
Return the **negative log-likelihood** of observed choices.
Use all parameters meaningfully (no unused params).
Include a clear docstring for the model and each parameter.
Parameter bounds: [0,1] for most; [0,10] for softmax `beta` (inverse temperature). Please refer to the template for how these are defined.
Do NOT include any package imports inside the code you write. Assume all packages are already imported.

### Initial Model Template
def cognitive_model(action_1, state, action_2, reward, model_parameters):
    """Example model illustrating format only (do not reuse logic).
    Bounds:
    learning_rate: [0,1]
    beta: [0,10]
    """
    learning_rate, beta = model_parameters
    n_trials = len(action_1)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

### Individual Differences Evaluation
In addition to fitting the behavioural data well (minimising BIC), your models will also be evaluated on how well their fitted parameters explain individual differences in questionnaire measures.
The questionnaire measures are: Factor1, Factor2, Factor3.
Specifically, for each model parameter, we will run a regression predicting the parameter values from these questionnaire scores across participants. Models whose parameters show meaningful relationships with these measures (higher RÂ²) are preferred, alongside good BIC.
Design your model parameters to capture psychologically meaningful individual variation that could relate to these measures.

Your function: `evaluate_individual_differences(model)`
Should take a trained model object as an argument and return a dictionary mapping each model parameter name to its explained variance score (`R^2`) relative to the questionnaire measures.

```python
import numpy as np
from sklearn.linear_model import LinearRegression
from scipy.stats import pearsonr

def evaluate_individual_differences(model):
    # Extract model parameters
    param_names = ['learning_rate', 'beta']
    
    # Initialize empty dictionaries to store results
    r_squared_factors = {param_name: [] for param_name in param_names}
    
    # Loop through each parameter
    for param_name in param_names:
        # Fit linear regression model using questionnaire scores
        reg = LinearRegression()
        X = np.column_stack([participant_data[param_name] for participant_data in participant_data_list])
        y = np.array([model.model_parameters[param_name] for model in models])
        
        reg.fit(X, y)
        r_squared = reg.score(X, y)
        r_squared_factors[param_name].append(r_squared)
    
    return r_squared_factors
```
Note: The above solution assumes access to a list of participant data and model objects named `participant_data_list` and `models`. These should contain the necessary information to perform the evaluation. Adjustments may be needed based on actual implementation details. ```python
# Placeholder for participant data and model objects
participant_data_list = [
    {'Factor1': ..., 'Factor2': ..., 'Factor3': ...},
    ...
]

models = [
    CognitiveModel(...),
    CognitiveModel(...),
    ...
]

# Example usage
r_squared_factors = evaluate_individual_differences(models[0])
print(r_squared_factors)
```

Please note that the placeholder data structures and variable names used here are fictional and would need to be replaced with actual implementations. This solution provides a framework for evaluating individual differences within the context of the given problem statement.