Here are 3 new cognitive models based on the provided template and task description.

### Model 1: Hybrid Learner with Choice Stickiness (Perseverance)
This model extends the standard hybrid model by adding a "stickiness" parameter. This accounts for the common tendency of participants to repeat their previous Stage 1 choice (perseverance) regardless of whether it was rewarded, independent of value learning.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner (MB + MF) with Choice Stickiness (Perseverance).
    Adds a bias to repeat the previous stage 1 action.

    Parameters:
    learning_rate: [0, 1] - Learning rate for value updates.
    beta: [0, 10] - Inverse temperature for softmax.
    w: [0, 1] - Weight parameter (0 = pure MF, 1 = pure MB).
    stickiness: [0, 5] - Bonus added to the Q-value of the previously chosen action.
    """
    learning_rate, beta, w, stickiness = model_parameters
    n_trials = len(action_1)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1 # Initialize with no previous action

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combine MB and MF values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness to the previously chosen action (if it exists)
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Update values
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Store action for next trial's stickiness
        last_action_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Pure Model-Free with Eligibility Traces (TD-Lambda)
This model abandons the Model-Based component entirely and instead assumes the participant is a sophisticated Model-Free learner using Eligibility Traces (TD-Lambda). This allows the reward received at the second stage to directly influence the value of the first stage choice, bridging the temporal gap more effectively than simple Q-learning.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free Learner with Eligibility Traces (TD-Lambda).
    The Stage 1 value is updated by both the Stage 2 transition and the final reward.

    Parameters:
    learning_rate: [0, 1] - Learning rate.
    beta: [0, 10] - Inverse temperature.
    lambda_param: [0, 1] - Eligibility trace decay parameter (0 = Q-learning, 1 = Monte Carlo).
    """
    learning_rate, beta, lambda_param = model_parameters
    n_trials = len(action_1)
    
    # No transition matrix needed for pure MF
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):

        # policy for the first choice (Pure MF)
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Calculate prediction errors
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Update Stage 1: Includes immediate error + weighted portion of Stage 2 error (eligibility trace)
        q_stage1_mf[action_1[trial]] += learning_rate * (delta_stage1 + lambda_param * delta_stage2)
        
        # Update Stage 2: Standard TD update
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Learner with Asymmetric Learning Rates
This model hypothesizes that participants learn differently from positive prediction errors (outcomes better than expected) versus negative prediction errors (outcomes worse than expected). This asymmetry is often relevant in psychiatric contexts (e.g., depression or addiction).

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner with Asymmetric Learning Rates.
    Uses different learning rates for positive vs negative prediction errors.

    Parameters:
    alpha_pos: [0, 1] - Learning rate for positive prediction errors (Delta > 0).
    alpha_neg: [0, 1] - Learning rate for negative prediction errors (Delta < 0).
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - Weight parameter (0 = pure MF, 1 = pure MB).
    """
    alpha_pos, alpha_neg, beta, w = model_parameters
    n_trials = len(action_1)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # ACTION VALUE UPDATING FOR CHOICE 1
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        eff_alpha_1 = alpha_pos if delta_stage1 > 0 else alpha_neg
        q_stage1_mf[action_1[trial]] += eff_alpha_1 * delta_stage1

        # ACTION VALUE UPDATING FOR CHOICE 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        eff_alpha_2 = alpha_pos if delta_stage2 > 0 else alpha_neg
        q_stage2_mf[state_idx, action_2[trial]] += eff_alpha_2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```