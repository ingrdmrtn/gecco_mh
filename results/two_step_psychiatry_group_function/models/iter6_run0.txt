Here are three new cognitive models for the two-step task. These models introduce mechanisms for asymmetric learning, dynamic structure learning, and memory decay, using parameter combinations distinct from those previously attempted.

### Model 1: Asymmetric Learning Rates (Confirmation Bias)
This model hypothesizes that participants update their expectations differently depending on whether the outcome was better than expected (positive prediction error) or worse than expected (negative prediction error). This is often linked to optimism bias or confirmation bias.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner with Asymmetric Learning Rates.
    
    This model distinguishes between learning from positive prediction errors 
    (better than expected) and negative prediction errors (worse than expected).
    It applies different learning rates (alpha_pos, alpha_neg) based on the sign of delta.

    Parameters:
    alpha_pos: [0, 1] - Learning rate for positive prediction errors (delta > 0).
    alpha_neg: [0, 1] - Learning rate for negative prediction errors (delta < 0).
    beta: [0, 10] - Inverse temperature for softmax choice policy.
    w: [0, 1] - Weight parameter (0 = pure MF, 1 = pure MB).
    """
    alpha_pos, alpha_neg, beta, w = model_parameters
    n_trials = len(action_1)
    
    # Standard fixed transition matrix for the MB component
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # Shape: (State/Planet, Action/Alien)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial] # Planet arrived at

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates with Asymmetry ---
        
        # Stage 1 Update (SARSA-style)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        # Select learning rate based on sign of error
        lr_s1 = alpha_pos if delta_stage1 > 0 else alpha_neg
        q_stage1_mf[action_1[trial]] += lr_s1 * delta_stage1
        
        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        # Select learning rate based on sign of error
        lr_s2 = alpha_pos if delta_stage2 > 0 else alpha_neg
        q_stage2_mf[state_idx, action_2[trial]] += lr_s2 * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Dynamic Transition Learning
Standard MB models assume the agent knows the fixed 70/30 transition probabilities. This model assumes the agent *learns* the transition matrix online based on observed transitions. This captures variability in how quickly participants grasp the task structure or if they believe the structure is changing.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner with Dynamic Transition Learning.
    
    Instead of using a fixed 0.7/0.3 transition matrix, this agent learns the 
    transition probabilities (Spaceship -> Planet) trial-by-trial.

    Parameters:
    lr_reward: [0, 1] - Learning rate for value (reward) updates.
    lr_transition: [0, 1] - Learning rate for updating the transition matrix.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - Weight parameter (0 = pure MF, 1 = pure MB).
    """
    lr_reward, lr_transition, beta, w = model_parameters
    n_trials = len(action_1)
    
    # Initialize transition matrix with a flat prior (0.5/0.5)
    # Rows: Action 1 (Spaceship 0 or 1), Cols: State (Planet 0 or 1)
    current_transition_matrix = np.array([[0.5, 0.5], [0.5, 0.5]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # MB calculation uses the *current learned* transition matrix
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = current_transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Value Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr_reward * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr_reward * delta_stage2
        
        # --- Transition Matrix Update ---
        chosen_spaceship = action_1[trial]
        arrived_planet = state[trial]
        
        # Update the probability of the observed transition towards 1.0
        # The transition error is (1 - current_probability)
        trans_error = 1.0 - current_transition_matrix[chosen_spaceship, arrived_planet]
        current_transition_matrix[chosen_spaceship, arrived_planet] += lr_transition * trans_error
        
        # Ensure probabilities sum to 1 by adjusting the other planet's probability
        other_planet = 1 - arrived_planet
        current_transition_matrix[chosen_spaceship, other_planet] = 1.0 - current_transition_matrix[chosen_spaceship, arrived_planet]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid with Passive Decay (Forgetting)
This model adds a memory constraint. While "stickiness" (tried previously) increases the value of chosen actions, "decay" decreases the value of *unchosen* actions, simulating forgetting. If an option isn't visited, its value decays back toward 0.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner with Passive Decay (Forgetting).
    
    Q-values for unchosen actions decay towards 0 on every trial. This simulates
    working memory constraints where information degrades if not refreshed.

    Parameters:
    learning_rate: [0, 1] - Learning rate for chosen actions.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - Weight parameter (0 = pure MF, 1 = pure MB).
    decay_rate: [0, 1] - Rate at which unchosen Q-values decay (0 = no decay, 1 = instant forgetting).
    """
    learning_rate, beta, w, decay_rate = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates with Decay ---
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Decay unchosen Stage 1 option
        unchosen_s1 = 1 - action_1[trial]
        q_stage1_mf[unchosen_s1] *= (1.0 - decay_rate)
        
        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Decay unchosen Stage 2 option (only for the current planet/state)
        unchosen_s2 = 1 - action_2[trial]
        q_stage2_mf[state_idx, unchosen_s2] *= (1.0 - decay_rate)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```