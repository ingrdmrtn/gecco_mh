Here are three new cognitive models that explore different mechanisms for learning and decision-making in the two-step task. These models introduce variations in how prediction errors are handled, how uncertainty is managed, and how rewards are processed.

### Model 1: Hybrid Learner with Separate Learning Rates for Stages
This model hypothesizes that participants update their expectations at different rates for the first stage (choosing a spaceship) versus the second stage (choosing an alien). This reflects the idea that the volatility of the environment might be perceived differently at the two levels (stable transitions vs. drifting reward probabilities).

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner (MB + MF) with Dual Learning Rates.
    Uses separate learning rates for Stage 1 (transition/spaceship) and Stage 2 (reward/alien) updates.

    Parameters:
    alpha1: [0, 1] - Learning rate for Stage 1 Q-value updates.
    alpha2: [0, 1] - Learning rate for Stage 2 Q-value updates.
    beta: [0, 10] - Inverse temperature for softmax.
    w: [0, 1] - Weight parameter (0 = pure MF, 1 = pure MB).
    """
    alpha1, alpha2, beta, w = model_parameters
    n_trials = len(action_1)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2

        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning ---
        # Update Stage 1 MF values using alpha1
        # Note: In standard TD(0), the error is based on the value of the next state.
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += alpha1 * delta_stage1
        
        # Update Stage 2 MF values using alpha2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += alpha2 * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Pure Model-Based with Reward Sensitivity
This model assumes the participant is purely Model-Based (no Model-Free caching at stage 1) but introduces a `reward_sensitivity` parameter. This parameter scales the incoming reward before it updates the second-stage Q-values. This captures individual differences in subjective utility or motivation, decoupling the magnitude of the reward signal from the rate of learning.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Based Learner with Reward Sensitivity.
    Assumes w=1 (Pure MB) but allows the subjective impact of the reward to scale.

    Parameters:
    learning_rate: [0, 1] - Learning rate for Stage 2 value updates.
    beta: [0, 10] - Inverse temperature for softmax.
    reward_sensitivity: [0, 5] - Scalar multiplier for the reward signal (subjective utility).
    stickiness: [0, 5] - Perseverance bonus for the previous stage 1 action.
    """
    learning_rate, beta, reward_sensitivity, stickiness = model_parameters
    n_trials = len(action_1)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    # No q_stage1_mf needed as we assume pure MB logic for stage 1 calculation
    q_stage2_mf = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Decision (Pure MB) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Apply stickiness to MB values directly
        q_net = q_stage1_mb.copy()
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning ---
        # Scale reward by sensitivity parameter
        subjective_reward = reward[trial] * reward_sensitivity
        
        # Update Stage 2 values
        delta_stage2 = subjective_reward - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        last_action_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Learner with Forgetting (Decay)
This model introduces a memory decay or "forgetting" mechanism. In dynamic environments, older information becomes less relevant. On every trial, unchosen actions in the second stage decay toward a neutral value (0). This helps the model adapt to the drifting probabilities of the aliens by slowly erasing outdated value estimates.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner (MB + MF) with Value Decay (Forgetting).
    Unchosen actions in Stage 2 decay towards 0 on every trial.

    Parameters:
    learning_rate: [0, 1] - Learning rate for chosen action updates.
    beta: [0, 10] - Inverse temperature for softmax.
    w: [0, 1] - Weight parameter (0 = pure MF, 1 = pure MB).
    decay_rate: [0, 1] - Rate at which unchosen Q-values decay to 0 (0 = no decay, 1 = instant forgetting).
    """
    learning_rate, beta, w, decay_rate = model_parameters
    n_trials = len(action_1)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2

        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning ---
        # Update Stage 1 MF
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Update Stage 2 MF (Chosen Action)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Decay Unchosen Actions in Stage 2
        # Iterate over all states and actions to apply decay to unvisited options
        for s in range(2):
            for a in range(2):
                if s == state_idx and a == action_2[trial]:
                    continue # Skip the chosen action (already updated)
                else:
                    # Decay towards 0
                    q_stage2_mf[s, a] = (1 - decay_rate) * q_stage2_mf[s, a]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```