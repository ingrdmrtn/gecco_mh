Here are three new cognitive models that explore different mechanisms for learning and decision-making in the two-step task. These models introduce variations in eligibility traces, separate learning rates for different stages, and forgetting mechanisms.

### Model 1: Hybrid Learner with Eligibility Traces (TD(λ))
This model implements a classic Temporal Difference learning algorithm with eligibility traces (TD(λ)). Instead of just updating the immediately preceding state, an eligibility trace allows the reward at the second stage to directly update the value of the first-stage choice, bridging the gap between the final outcome and the initial decision. This is often used to model how credit is assigned to earlier actions.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner with Eligibility Traces (TD(lambda)).
    Uses an eligibility trace to allow Stage 2 rewards to directly update Stage 1 values,
    modulated by a decay parameter lambda.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate for value updates.
    beta: [0, 10] - Inverse temperature for softmax.
    w: [0, 1] - Weight parameter (0 = pure MF, 1 = pure MB).
    lambda_trace: [0, 1] - Eligibility trace decay parameter (0 = TD(0), 1 = Monte Carlo).
    """
    learning_rate, beta, w, lambda_trace = model_parameters
    n_trials = len(action_1)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):

        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial] # Planet arrived at

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning ---
        
        # Stage 1 Prediction Error (TD error)
        # The value of the state arrived at (max Q or Q of chosen action) minus current Q
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        # Update Stage 1 MF value based on immediate transition
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Stage 2 Prediction Error
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Update Stage 2 MF value
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Eligibility Trace Update:
        # The Stage 2 error also updates Stage 1, scaled by lambda
        q_stage1_mf[action_1[trial]] += learning_rate * lambda_trace * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Learner with Separate Stage Learning Rates
This model hypothesizes that learning might occur at different speeds for the high-level planning (Stage 1 spaceship choice) versus the low-level concrete reward association (Stage 2 alien choice). It separates the learning rate into `alpha_stage1` and `alpha_stage2`.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner with Separate Stage Learning Rates.
    Allows for different plasticity in the first-stage (abstract) vs second-stage (concrete) transitions.

    Parameters:
    alpha_stage1: [0, 1] - Learning rate for Stage 1 MF values.
    alpha_stage2: [0, 1] - Learning rate for Stage 2 MF values.
    beta: [0, 10] - Inverse temperature for softmax.
    w: [0, 1] - Weight parameter (0 = pure MF, 1 = pure MB).
    """
    alpha_stage1, alpha_stage2, beta, w = model_parameters
    n_trials = len(action_1)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):

        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning ---
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += alpha_stage1 * delta_stage1
        
        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += alpha_stage2 * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Learner with Passive Forgetting
This model incorporates a decay or "forgetting" mechanism for unchosen options. In dynamic environments, old information becomes less reliable. This model decays the Q-values of unchosen actions at Stage 2 towards 0 (or a neutral value) on every trial, controlled by a `decay_rate`. This represents memory degradation or uncertainty growth over time.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner with Passive Forgetting (Decay).
    Unchosen actions at the second stage slowly decay toward zero, simulating memory loss.

    Parameters:
    learning_rate: [0, 1] - Learning rate for chosen actions.
    beta: [0, 10] - Inverse temperature for softmax.
    w: [0, 1] - Weight parameter (0 = pure MF, 1 = pure MB).
    decay_rate: [0, 1] - Rate at which unchosen Q-values decay (0 = no decay, 1 = instant forgetting).
    """
    learning_rate, beta, w, decay_rate = model_parameters
    n_trials = len(action_1)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):

        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning ---
        
        # Standard MF updates
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Decay for unchosen Stage 2 actions
        # We iterate over both states (planets) and both actions (aliens)
        for s in range(2):
            for a in range(2):
                if s == state_idx and a == action_2[trial]:
                    continue # Don't decay the action we just updated
                else:
                    q_stage2_mf[s, a] *= (1 - decay_rate)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```