Here are three new cognitive models exploring different parameter combinations and mechanisms based on the two-step task structure.

### Model 1: Asymmetric Learning Rates (Positive vs Negative Prediction Errors)
This model hypothesizes that participants update their expectations differently depending on whether the outcome was better or worse than expected. This is often observed in clinical populations (e.g., depression/anxiety) where sensitivity to reward versus punishment differs.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Model with asymmetric learning rates for positive and negative prediction errors.
    
    This model assumes Model-Free (MF) learning only but splits the learning rate
    into alpha_pos (for positive RPEs) and alpha_neg (for negative RPEs).
    
    Bounds:
    alpha_pos: [0,1]
    alpha_neg: [0,1]
    beta: [0,10]
    lambd: [0,1]
    """
    alpha_pos, alpha_neg, beta, lambd = model_parameters
    n_trials = len(action_1)
  
    # transition_matrix is unused in pure MF but kept for structure consistency if needed later
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Initialize Q-values
    q_stage1_mf = np.ones(2) * 0.5
    q_stage2_mf = np.ones((2, 2)) * 0.5

    for trial in range(n_trials):

        # Stage 1 Choice (Model-Free only here)
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # Stage 2 Choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Stage 1 Update (TD-error using Stage 2 Q-value)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        # Apply asymmetric learning rate for Stage 1
        lr_s1 = alpha_pos if delta_stage1 > 0 else alpha_neg
        q_stage1_mf[action_1[trial]] += lr_s1 * delta_stage1
        
        # Stage 2 Update (TD-error using Reward)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Apply asymmetric learning rate for Stage 2
        lr_s2 = alpha_pos if delta_stage2 > 0 else alpha_neg
        q_stage2_mf[state_idx, action_2[trial]] += lr_s2 * delta_stage2
        
        # Eligibility Trace Update (Updating Stage 1 based on Stage 2 outcome)
        # We reuse the stage 2 learning rate logic for the trace update
        q_stage1_mf[action_1[trial]] += lr_s2 * lambd * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model with Separate Inverse Temperatures (Beta) for Stages
This model proposes that the exploration-exploitation balance differs between the first stage (spaceship choice) and the second stage (alien choice). The first stage is more abstract and strategic, while the second stage is a direct bandit task.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model (MB/MF) with separate Beta parameters for Stage 1 and Stage 2.
    
    This allows the model to capture different levels of decision noise or 
    stochasticity at the two different hierarchical levels of the task.
    
    Bounds:
    learning_rate: [0,1]
    beta_1: [0,10] (Inverse temperature for Stage 1)
    beta_2: [0,10] (Inverse temperature for Stage 2)
    w: [0,1] (Mixing weight: 1=Pure MB, 0=Pure MF)
    """
    learning_rate, beta_1, beta_2, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.ones(2) * 0.5
    q_stage2_mf = np.ones((2, 2)) * 0.5

    for trial in range(n_trials):

        # Stage 1 Choice: Hybrid MB/MF
        # Calculate Model-Based value
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrate
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Use Beta 1
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # Stage 2 Choice: Pure MF
        # Use Beta 2
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Note: No eligibility trace (lambda) in this specific variant to focus on Beta split

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model with Forgetting (Decay)
This model introduces a decay parameter. In many reinforcement learning tasks, unchosen options are assumed to decay back to a baseline or participants simply forget values over time. This helps the model react to the slowly changing reward probabilities described in the task prompt.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model (MB/MF) with value decay (forgetting) for unchosen Stage 2 options.
    
    This mechanism helps the agent track non-stationary rewards by pulling 
    estimates of unvisited states back towards a neutral prior (0.5), 
    preventing the agent from getting stuck on old values.
    
    Bounds:
    learning_rate: [0,1]
    beta: [0,10]
    w: [0,1]
    lambd: [0,1]
    decay_rate: [0,1] (Rate at which unchosen Q-values decay to 0.5)
    """
    learning_rate, beta, w, lambd, decay_rate = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.ones(2) * 0.5
    q_stage2_mf = np.ones((2, 2)) * 0.5

    for trial in range(n_trials):

        # Stage 1 Choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # Stage 2 Choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Update Stage 1 MF
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Update Stage 2 MF
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Eligibility Trace
        q_stage1_mf[action_1[trial]] += learning_rate * lambd * delta_stage2

        # Decay/Forgetting for unchosen Stage 2 options
        # We iterate over all 4 alien options (2 planets * 2 aliens)
        for s in range(2):
            for a in range(2):
                if not (s == state_idx and a == action_2[trial]):
                    # Decay towards 0.5
                    q_stage2_mf[s, a] = (1 - decay_rate) * q_stage2_mf[s, a] + decay_rate * 0.5

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```