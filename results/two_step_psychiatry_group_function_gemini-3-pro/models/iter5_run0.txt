Here are three new cognitive models for the two-step decision task. These models explore different parameter configurations and mechanisms not fully covered in the previous attempts, specifically focusing on separate learning rates for stages, separate inverse temperatures, and asymmetric learning from positive/negative prediction errors.

### Model 1: Hybrid Model with Separate Stage Learning Rates
This model posits that participants might update their values at different rates for the first stage (spaceship choice) versus the second stage (alien choice). This captures the idea that the abstract choice between spaceships might be learned differently than the concrete reward-based choice of aliens. It combines Model-Based (MB) and Model-Free (MF) control via a mixing weight `w`.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Separate Learning Rates for Stage 1 and Stage 2.
    
    This model allows for different plasticity in the two stages of the task.
    It assumes the participant might learn the value of spaceships (Stage 1)
    at a different speed than the value of aliens (Stage 2).
    
    Bounds:
    lr_1: [0,1] (Learning rate for stage 1 updates)
    lr_2: [0,1] (Learning rate for stage 2 updates)
    beta: [0,10] (Inverse temperature)
    w: [0,1] (Weighting parameter: 1=Pure MB, 0=Pure MF)
    """
    lr_1, lr_2, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Initialize Q-values (0.5 assumes neutral prior for binary 0/1 rewards)
    q_stage1_mf = np.ones(2) * 0.5
    q_stage2_mf = np.ones((2, 2)) * 0.5

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        # Model-Based value calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid value calculation
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial] # The planet reached (0 or 1)

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Value Updating ---
        
        # Stage 1 MF update (TD(0)) using lr_1
        # Prediction error driven by the value of the state reached (Q_stage2)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr_1 * delta_stage1
        
        # Stage 2 MF update using lr_2
        # Prediction error driven by the reward received
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr_2 * delta_stage2
        
        # Note: This model uses a simple TD(0) for stage 1 without an eligibility trace 
        # (lambda=0) to isolate the effect of split learning rates.

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Pure Model-Free with Asymmetric Learning Rates (Pos/Neg)
This model drops the Model-Based component entirely to focus on how participants process gains versus losses (or non-gains). It implements asymmetric learning rates (`alpha_pos` and `alpha_neg`) to see if the participant is more sensitive to rewards or the absence of rewards. It includes an eligibility trace (`lambd`) to allow the second-stage outcome to reinforce the first-stage choice.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free with Asymmetric Learning Rates (+/-) and Eligibility Trace.
    
    This model assumes no model-based planning (w=0) but focuses on 
    differential sensitivity to positive vs negative prediction errors.
    
    Bounds:
    alpha_pos: [0,1] (Learning rate for positive prediction errors)
    alpha_neg: [0,1] (Learning rate for negative prediction errors)
    beta: [0,10] (Inverse temperature)
    lambd: [0,1] (Eligibility trace decay)
    """
    alpha_pos, alpha_neg, beta, lambd = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]]) # Not used in MF, but kept for structure
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.ones(2) * 0.5
    q_stage2_mf = np.ones((2, 2)) * 0.5

    for trial in range(n_trials):

        # --- Stage 1 Policy (Pure MF) ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Value Updating ---
        
        # Stage 1 Update (TD error)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        lr_s1 = alpha_pos if delta_stage1 > 0 else alpha_neg
        q_stage1_mf[action_1[trial]] += lr_s1 * delta_stage1

        # Stage 2 Update (Reward error)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        lr_s2 = alpha_pos if delta_stage2 > 0 else alpha_neg
        q_stage2_mf[state_idx, action_2[trial]] += lr_s2 * delta_stage2
        
        # Eligibility Trace Update for Stage 1 based on Stage 2 outcome
        # Using the learning rate appropriate for the Stage 2 error
        q_stage1_mf[action_1[trial]] += lr_s2 * lambd * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model with Separate Exploration (Beta) per Stage
This model reintroduces the hybrid MB/MF architecture but hypothesizes that the level of exploration/exploitation differs between the two stages. For example, a participant might be very deterministic (high beta) when choosing a spaceship but very exploratory (low beta) when choosing an alien, or vice versa.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Separate Inverse Temperatures (Beta) for Stages 1 and 2.
    
    This model allows for different levels of choice stochasticity (exploration)
    at the two different decision stages. It uses a single learning rate.
    
    Bounds:
    learning_rate: [0,1]
    beta_1: [0,10] (Inverse temperature for Stage 1)
    beta_2: [0,10] (Inverse temperature for Stage 2)
    w: [0,1] (Weighting parameter: 1=Pure MB, 0=Pure MF)
    """
    learning_rate, beta_1, beta_2, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.ones(2) * 0.5
    q_stage2_mf = np.ones((2, 2)) * 0.5

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Use beta_1 for the first decision
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        # Use beta_2 for the second decision
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Value Updating ---
        
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1

        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # No eligibility trace (lambda=0 implicitly) to focus on beta differences

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```