Here are three new cognitive models for the two-step task. These models explore variations in learning rates, perseveration, and how model-based vs. model-free values are integrated, aiming to capture different nuances of human decision-making in this paradigm.

### Model 1: Asymmetric Learning Rates (Positive vs. Negative Prediction Errors)
This model hypothesizes that participants update their beliefs differently depending on whether the outcome was better or worse than expected. It splits the learning rate into `alpha_pos` and `alpha_neg`. It uses a pure Model-Free strategy with eligibility traces to link the second-stage outcome to the first-stage choice.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free with Asymmetric Learning Rates and Eligibility Trace.
    
    Splits learning into positive and negative prediction error updates.
    
    Bounds:
    alpha_pos: [0,1]
    alpha_neg: [0,1]
    beta: [0,10]
    lambd: [0,1]
    """
    alpha_pos, alpha_neg, beta, lambd = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition matrix (not used in pure MF but kept for structure consistency)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values initialized to 0.5 (assuming simplistic prior)
    q_stage1_mf = np.ones(2) * 0.5
    q_stage2_mf = np.ones((2, 2)) * 0.5

    for trial in range(n_trials):

        # policy for the first choice
        # Pure MF logic for stage 1 here
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        lr_s1 = alpha_pos if delta_stage1 > 0 else alpha_neg
        q_stage1_mf[action_1[trial]] += lr_s1 * delta_stage1
        
        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        lr_s2 = alpha_pos if delta_stage2 > 0 else alpha_neg
        q_stage2_mf[state_idx, action_2[trial]] += lr_s2 * delta_stage2
        
        # Eligibility Trace Update (connecting S2 reward to S1 choice)
        # We apply the S2 learning rate logic to the trace update as well
        q_stage1_mf[action_1[trial]] += lr_s2 * lambd * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model with Separate Stage Betas and Stickiness
This model acknowledges that the exploration/exploitation trade-off might differ between the first stage (spaceship choice) and the second stage (alien choice). It separates the inverse temperature into `beta_stage1` and `beta_stage2`. It also includes a `w` parameter for Model-Based/Model-Free weighting and a `stickiness` parameter to capture repetition bias on the first stage.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Separate Stage Betas and Stickiness.
    
    Allows different noise levels for Stage 1 vs Stage 2 choices.
    
    Bounds:
    learning_rate: [0,1]
    beta_s1: [0,10]
    beta_s2: [0,10]
    w: [0,1]
    stickiness: [-5, 5]
    """
    learning_rate, beta_s1, beta_s2, w, stickiness = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.ones(2) * 0.5
    q_stage2_mf = np.ones((2, 2)) * 0.5
    
    last_action_1 = -1

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta_s1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # policy for the second choice
        # Only MF values exist for stage 2
        exp_q2 = np.exp(beta_s2 * q_stage2_mf[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Updates
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Note: No lambda eligibility trace here, focusing on MB/MF trade-off
        
        last_action_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model with Forgetting (Decay)
This model introduces a `decay_rate` parameter. In dynamic environments, older information becomes less relevant. This model assumes that unchosen Q-values decay back toward a neutral value (0.5) over time, rather than staying static until chosen again. This is combined with the standard hybrid MB/MF architecture.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Q-value Decay (Forgetting).
    
    Unchosen options decay toward 0.5 at each step.
    
    Bounds:
    learning_rate: [0,1]
    beta: [0,10]
    w: [0,1]
    decay_rate: [0,1]
    """
    learning_rate, beta, w, decay_rate = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.ones(2) * 0.5
    q_stage2_mf = np.ones((2, 2)) * 0.5

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Value Updating
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Decay (Forgetting) Step for unchosen/unvisited states
        # Decay Stage 1 unchosen
        unchosen_1 = 1 - action_1[trial]
        q_stage1_mf[unchosen_1] = (1 - decay_rate) * q_stage1_mf[unchosen_1] + decay_rate * 0.5
        
        # Decay Stage 2 unchosen (in the visited state)
        unchosen_2 = 1 - action_2[trial]
        q_stage2_mf[state_idx, unchosen_2] = (1 - decay_rate) * q_stage2_mf[state_idx, unchosen_2] + decay_rate * 0.5
        
        # Decay Stage 2 unvisited state (both aliens)
        unvisited_state = 1 - state_idx
        q_stage2_mf[unvisited_state, :] = (1 - decay_rate) * q_stage2_mf[unvisited_state, :] + decay_rate * 0.5
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```