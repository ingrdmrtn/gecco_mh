Here are three new cognitive models based on the two-step task paradigm, exploring different mechanisms and parameter combinations than those previously listed.

### Model 1: Hybrid Model with Separate Learning Rates for Stages
This model hypothesizes that participants update their expectations at different speeds for the first stage (spaceship choice) versus the second stage (alien choice). This captures the idea that the abstract "value" of a spaceship might be learned differently than the concrete reward probability of an alien. It combines Model-Based (MB) and Model-Free (MF) control via a weighting parameter `w`.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Separate Learning Rates for Stage 1 and Stage 2.
    
    This model allows for different update speeds for the first-stage values (spaceships)
    and second-stage values (aliens). It combines MB and MF systems.

    Bounds:
    lr_1: [0,1] (Learning rate for stage 1 MF values)
    lr_2: [0,1] (Learning rate for stage 2 MF values)
    beta: [0,10] (Inverse temperature)
    w: [0,1] (Mixing weight: 1=Pure MB, 0=Pure MF)
    """
    lr_1, lr_2, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Initialize Q-values at 0.5 (neutral expectation)
    q_stage1_mf = np.ones(2) * 0.5
    q_stage2_mf = np.ones((2, 2)) * 0.5

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        # Model-Based Value: Transition * Max(Stage 2 Values)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updating ---
        # Stage 1 MF Update using Stage 2 Q-value (TD-learning)
        # Note: We use lr_1 here
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr_1 * delta_stage1
        
        # Stage 2 MF Update using Reward
        # Note: We use lr_2 here
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr_2 * delta_stage2
        
        # Eligibility Trace: Updating Stage 1 based on Stage 2 RPE
        # Often lambda is used here, but in this specific 4-param variant, 
        # we assume full eligibility (lambda=1) scaled by the stage 1 learning rate
        # to minimize parameter count while testing the split-LR hypothesis.
        q_stage1_mf[action_1[trial]] += lr_1 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Pure Model-Free with Asymmetric Learning (Pos/Neg) and Eligibility
This model abandons the Model-Based component entirely to focus on how rewards are processed. It tests the hypothesis that participants learn more from positive prediction errors (wins) than negative ones (losses), or vice versa, specifically within a Model-Free framework that uses eligibility traces to propagate credit back to the first stage.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free Model with Asymmetric Learning Rates (+/-) and Eligibility Trace.
    
    This model assumes no knowledge of the transition structure (w=0).
    It differentiates between learning from better-than-expected outcomes (alpha_pos)
    and worse-than-expected outcomes (alpha_neg).

    Bounds:
    alpha_pos: [0,1] (Learning rate for positive RPE)
    alpha_neg: [0,1] (Learning rate for negative RPE)
    beta: [0,10] (Inverse temperature)
    lambd: [0,1] (Eligibility trace decay)
    """
    alpha_pos, alpha_neg, beta, lambd = model_parameters
    n_trials = len(action_1)
  
    # No transition matrix needed for Pure MF
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.ones(2) * 0.5
    q_stage2_mf = np.ones((2, 2)) * 0.5

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        # Pure MF choice
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updating ---
        
        # 1. Update Stage 1 based on transition to Stage 2 (TD error)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        if delta_stage1 > 0:
            q_stage1_mf[action_1[trial]] += alpha_pos * delta_stage1
        else:
            q_stage1_mf[action_1[trial]] += alpha_neg * delta_stage1

        # 2. Update Stage 2 based on Reward
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        if delta_stage2 > 0:
            q_stage2_mf[state_idx, action_2[trial]] += alpha_pos * delta_stage2
            current_lr = alpha_pos
        else:
            q_stage2_mf[state_idx, action_2[trial]] += alpha_neg * delta_stage2
            current_lr = alpha_neg
            
        # 3. Eligibility Trace: Update Stage 1 based on Stage 2 Reward RPE
        # We use the learning rate associated with the Stage 2 outcome
        q_stage1_mf[action_1[trial]] += current_lr * lambd * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model with Forgetting (Decay) and Sticky Choice
This model introduces a "forgetting" parameter (`decay`) alongside the standard hybrid architecture. In volatile environments, older values should sometimes decay back to a neutral prior (0.5) rather than persisting indefinitely. It also includes choice stickiness (perseverance) for the first stage.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model (MB/MF) with Value Decay and Choice Stickiness.
    
    Includes a decay parameter where unchosen option values slowly revert to 0.5.
    This helps the model handle changing reward probabilities.
    Also includes stickiness to account for repetition bias.

    Bounds:
    learning_rate: [0,1]
    beta: [0,10]
    w: [0,1]
    decay: [0,1] (Rate at which values decay to 0.5)
    stickiness: [-5,5] (Bonus for repeating previous Stage 1 choice)
    """
    learning_rate, beta, w, decay, stickiness = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.ones(2) * 0.5
    q_stage2_mf = np.ones((2, 2)) * 0.5
    
    last_action_1 = -1

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Apply stickiness to net values
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updating ---
        
        # Standard TD Updates
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1

        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Note: No eligibility trace (lambda=0 implicitly) in this model to focus on decay
        
        # --- Decay ---
        # Decay all Stage 2 Q-values towards 0.5
        # This represents forgetting or uncertainty increasing over time
        q_stage2_mf = q_stage2_mf * (1 - decay) + 0.5 * decay
        
        last_action_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```