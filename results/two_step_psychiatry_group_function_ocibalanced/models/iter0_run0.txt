Here are three cognitive models representing different strategies for the two-step task.

### Model 1: Pure Model-Based (MB) Reinforcement Learning
This model assumes the participant builds a mental map of the task structure. They learn the value of the second-stage aliens (Q-values) based on rewards. To make a first-stage choice, they calculate the expected value of each spaceship by combining the known transition probabilities (70/30) with the maximum value available at the resulting planets. This model ignores Model-Free learning at the first stage.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Based Reinforcement Learning.
    
    The agent calculates Stage 1 values solely by planning: multiplying the 
    transition matrix by the max Q-values of Stage 2.
    
    Bounds:
    learning_rate: [0,1]
    beta_1: [0,10] (inverse temperature for stage 1)
    beta_2: [0,10] (inverse temperature for stage 2)
    """
    learning_rate, beta_1, beta_2 = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition matrix as per task description
    # Row 0: Spaceship 0 -> [Planet 0 (0.7), Planet 1 (0.3)]
    # Row 1: Spaceship 1 -> [Planet 0 (0.3), Planet 1 (0.7)]
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values for the second stage (2 planets x 2 aliens)
    q_stage2_mf = np.zeros((2, 2)) # Initialize at 0 (or 0.5)

    for trial in range(n_trials):
        # --- STAGE 1 POLICY (Model-Based) ---
        # Calculate expected value of each spaceship based on planning
        max_q_stage2 = np.max(q_stage2_mf, axis=1) # Best value at Planet 0, Best value at Planet 1
        q_stage1_mb = transition_matrix @ max_q_stage2 # Expected value
        
        exp_q1 = np.exp(beta_1 * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Transition to state (planet)
        state_idx = state[trial]

        # --- STAGE 2 POLICY ---
        # Standard softmax over the aliens available at the current planet
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta_2 * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- UPDATING ---
        # Update Stage 2 Q-values based on reward (Model-Free update)
        prediction_error = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * prediction_error
        
        # Note: In a pure MB model, we do not update a Stage 1 MF Q-value.
        # The Stage 1 values change automatically because q_stage2_mf changes.

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Pure Model-Free (MF) Reinforcement Learning (TD-Learning)
This model assumes the participant does not use the transition structure of the task. Instead, they learn the value of the spaceships directly from experience using Temporal Difference (TD) learning. The value of a spaceship is updated based on the value of the state it leads to (SARSA-style update), and the value of the alien is updated based on the reward received.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free Reinforcement Learning (TD-Learning).
    
    The agent learns Stage 1 values via direct experience (temporal difference),
    ignoring the transition structure.
    
    Bounds:
    learning_rate: [0,1]
    beta: [0,10]
    eligibility_trace: [0,1] (lambda parameter for TD(lambda))
    """
    learning_rate, beta, eligibility_trace = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)      # Values for Spaceships
    q_stage2_mf = np.zeros((2, 2)) # Values for Aliens (Planet x Alien)

    for trial in range(n_trials):

        # --- STAGE 1 POLICY ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- STAGE 2 POLICY ---
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- UPDATING ---
        
        # Prediction error at Stage 2 (Reward - Value of Alien)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Prediction error at Stage 1 (Value of Alien - Value of Spaceship)
        # This is effectively a TD(0) or SARSA update logic for the first step
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        # Update Stage 2 value
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Update Stage 1 value using both the immediate transition error and 
        # the eligibility trace of the second stage error
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1 + (learning_rate * eligibility_trace * delta_stage2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model-Based / Model-Free Learning
This is the classic "Daw two-step" model. It assumes the participant uses a weighted combination of Model-Based (planning) and Model-Free (habitual) strategies to make the first-stage choice. A mixing parameter `w` determines the balance: `w=1` is pure Model-Based, `w=0` is pure Model-Free.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Reinforcement Learning.
    
    Combines MB planning and MF caching for the first stage choice.
    
    Bounds:
    learning_rate: [0,1]
    beta: [0,10]
    w: [0,1] (mixing weight: 0=Pure MF, 1=Pure MB)
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)      # MF values for Stage 1
    q_stage2_mf = np.zeros((2, 2)) # MF values for Stage 2 (shared by MB system)

    for trial in range(n_trials):

        # --- STAGE 1 POLICY ---
        # 1. Calculate Model-Based Value
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Combine MB and MF values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- STAGE 2 POLICY ---
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- UPDATING ---
        # Stage 2 PE: Reward vs Expected Alien Value
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Stage 1 PE: Value of chosen Alien vs Value of Spaceship
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        # Update Stage 2 values
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Update Stage 1 MF values (TD(1) logic often used here, or simple TD(0))
        # Here we use TD(1) logic: accumulating both errors
        q_stage1_mf[action_1[trial]] += learning_rate * (delta_stage1 + delta_stage2)
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```