Here are three new cognitive models based on the provided data and task structure.

### Cognitive Model 1: Asymmetric Learning Hybrid Model
This model hypothesizes that participants update their expectations differently depending on whether the outcome was better or worse than expected (Positive vs. Negative prediction errors). This is often observed in clinical populations or specific reinforcement learning phenotypes where "good news" and "bad news" are processed at different rates.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Asymmetric Learning Hybrid Model.
    
    Distinguishes between learning from positive prediction errors (better than expected)
    and negative prediction errors (worse than expected) for the Model-Free components.
    
    Parameters:
    alpha_pos: [0, 1] - Learning rate for positive prediction errors (delta > 0).
    alpha_neg: [0, 1] - Learning rate for negative prediction errors (delta < 0).
    beta: [0, 10] - Inverse temperature (randomness) for both stages.
    w: [0, 1] - Weighting parameter (0 = Pure MF, 1 = Pure MB).
    """
    alpha_pos, alpha_neg, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based Value: Transition matrix * Max Stage 2 values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Weighted integrated value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # --- Updates ---
        # Stage 1 Update (TD-0 from Stage 2 value)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        lr_1 = alpha_pos if delta_stage1 > 0 else alpha_neg
        q_stage1_mf[a1] += lr_1 * delta_stage1
        
        # Stage 2 Update (from Reward)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        lr_2 = alpha_pos if delta_stage2 > 0 else alpha_neg
        q_stage2_mf[state_idx, a2] += lr_2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2: Dual-Beta Hybrid Model
This model separates the exploration/exploitation trade-off for Stage 1 (Spaceship choice) and Stage 2 (Alien choice). The rationale is that Stage 1 involves a complex trade-off between Model-Based and Model-Free systems, potentially requiring a different level of stochasticity (`beta1`) compared to the simpler bandit task in Stage 2 (`beta2`).

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Dual-Beta Hybrid Model.
    
    Uses distinct inverse temperature parameters for the first stage (planning/transport)
    and the second stage (harvesting/alien choice), capturing different levels of 
    decision noise or exploration in the two contexts.
    
    Parameters:
    learning_rate: [0, 1] - Update rate for Q-values.
    beta_1: [0, 10] - Inverse temperature for Stage 1 (Spaceship choice).
    beta_2: [0, 10] - Inverse temperature for Stage 2 (Alien choice).
    w: [0, 1] - Weighting parameter (0 = Pure MF, 1 = Pure MB).
    """
    learning_rate, beta_1, beta_2, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy (Uses beta_1) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])

        # --- Stage 2 Policy (Uses beta_2) ---
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3: Hybrid Model with Passive Decay (Forgetting)
This model accounts for the fact that reward probabilities "changed slowly over trials." In dynamic environments, information about unvisited options becomes outdated. This model implements a passive decay mechanism where the Q-values of *unchosen* actions decay toward 0 (or neutral) on every trial, representing forgetting or uncertainty growth.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Passive Decay (Forgetting).
    
    Includes a decay parameter that reduces the value of unchosen actions 
    on every trial. This allows the agent to "forget" old values in the 
    drifting environment defined in the task description.
    
    Parameters:
    learning_rate: [0, 1] - Rate of learning for chosen actions.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - Model-Based weight.
    decay: [0, 1] - Rate at which unchosen action values decay toward 0.
    """
    learning_rate, beta, w, decay = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # --- Updates ---
        # Update chosen Stage 1
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        # Decay unchosen Stage 1
        q_stage1_mf[1 - a1] *= (1 - decay)

        # Update chosen Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        # Decay unchosen Stage 2 (in the current state)
        q_stage2_mf[state_idx, 1 - a2] *= (1 - decay)
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```