Here are the three proposed cognitive models based on the two-step task structure and the participant data provided.

### Model 1: Hybrid Model with Passive Decay (Forgetting)
This model hypothesizes that participants actively learn about chosen options but passively "forget" the value of unchosen options over time. This captures the behavior where a participant might cease to trust a previously good option simply because they haven't tried it in a while, pushing Q-values back toward neutrality.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Passive Decay (Forgetting).
    
    In addition to Model-Based (MB) and Model-Free (MF) learning, this model 
    assumes that the value of unchosen actions decays towards zero on every trial.
    This explains behaviors where participants switch strategies not because of 
    negative feedback, but because value estimates of unvisited states erode.

    Parameters:
    learning_rate: [0, 1] - Rate of updating value estimates for chosen actions.
    beta: [0, 10] - Inverse temperature (exploration/exploitation balance).
    w: [0, 1] - Weighting parameter (0 = pure MF, 1 = pure MB).
    decay_rate: [0, 1] - Rate at which unchosen action values decay toward 0.
    """
    learning_rate, beta, w, decay_rate = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])
        
        # Handle missing/invalid data
        if a1 < 0 or s_idx < 0 or a2 < 0:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        
        # Stage 1 MF Update (SARSA-like TD(0))
        # Note: In standard two-step, Stage 1 MF is updated by the value of the *state* reached (max_q or chosen_q)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Decay unchosen Stage 1 action
        unchosen_a1 = 1 - a1
        q_stage1_mf[unchosen_a1] *= (1 - decay_rate)

        # Stage 2 MF Update
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Decay unchosen Stage 2 action (in the visited state)
        unchosen_a2 = 1 - a2
        q_stage2_mf[s_idx, unchosen_a2] *= (1 - decay_rate)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model with Asymmetric Learning and Stickiness
This model combines choice perseverance (stickiness) with asymmetric learning rates for positive and negative prediction errors. This addresses the possibility that participants may be "risk-averse" (learning more from losses) or "optimistic" (learning more from wins), while also accounting for the strong motor perseverance seen in the data (e.g., Participant 1).

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Asymmetric Learning and Stickiness.
    
    Differentiates between learning from positive prediction errors (better than expected)
    and negative prediction errors (worse than expected). Also includes a stickiness
    bonus for repeating the previous Stage 1 choice.

    Parameters:
    lr_pos: [0, 1] - Learning rate for positive prediction errors.
    lr_neg: [0, 1] - Learning rate for negative prediction errors.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - Model-Based weight.
    stickiness: [0, 5] - Bonus added to the logit of the previously chosen action.
    """
    lr_pos, lr_neg, beta, w, stickiness = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])
        
        if a1 < 0 or s_idx < 0 or a2 < 0:
            p_choice_1[trial] = 0.5; p_choice_2[trial] = 0.5; continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf

        logits = beta * q_net
        if last_action_1 != -1:
            logits[last_action_1] += stickiness
            
        exp_q1 = np.exp(logits)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        lr_s1 = lr_pos if delta_stage1 > 0 else lr_neg
        q_stage1_mf[a1] += lr_s1 * delta_stage1
        
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        lr_s2 = lr_pos if delta_stage2 > 0 else lr_neg
        q_stage2_mf[s_idx, a2] += lr_s2 * delta_stage2
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model with Stage-Specific Noise and Stickiness
This model acknowledges that the decision noise (exploration) might differ between the abstract choice of spaceships (Stage 1) and the concrete choice of aliens (Stage 2). It separates the inverse temperature `beta` for the two stages.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Stage-Specific Noise and Stickiness.
    
    Separates the inverse temperature (beta) for Stage 1 and Stage 2. 
    Participants often show different levels of randomness when choosing the 
    spaceship (planning step) versus choosing the alien (bandit step).
    
    Parameters:
    learning_rate: [0, 1] - Update rate.
    beta_1: [0, 10] - Inverse temperature for Stage 1 (Spaceship choice).
    beta_2: [0, 10] - Inverse temperature for Stage 2 (Alien choice).
    w: [0, 1] - Model-Based weight.
    stickiness: [0, 5] - Choice perseverance bonus for Stage 1.
    """
    learning_rate, beta_1, beta_2, w, stickiness = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])
        
        if a1 < 0 or s_idx < 0 or a2 < 0:
            p_choice_1[trial] = 0.5; p_choice_2[trial] = 0.5; continue

        # --- Stage 1 Policy (uses beta_1) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf

        logits = beta_1 * q_net
        if last_action_1 != -1:
            logits[last_action_1] += stickiness
            
        exp_q1 = np.exp(logits)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy (uses beta_2) ---
        exp_q2 = np.exp(beta_2 * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```