Here are three new cognitive models for the two-step decision task, implemented as standalone Python functions.

### Model 1: Adaptive Transition Hybrid Model
This model extends the standard hybrid approach by allowing the agent to **learn the transition structure** of the task (Space $\to$ Planet) dynamically, rather than assuming fixed probabilities (0.7/0.3). This captures the behavior of participants who might become skeptical of the "common" transitions after experiencing rare ones.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Adaptive Transition Hybrid Model.
    
    The agent learns the transition probabilities T(s'|s,a) dynamically based on 
    experienced transitions, rather than using a fixed matrix.
    
    Parameters:
    lr_val: [0, 1] - Learning rate for Q-values (Stage 2).
    lr_trans: [0, 1] - Learning rate for transition probabilities.
    beta: [0, 10] - Inverse temperature (exploration/exploitation).
    w: [0, 1] - Weighting between Model-Based (1) and Model-Free (0).
    stickiness: [0, 5] - Repetition bonus for Stage 1 choice.
    """
    lr_val, lr_trans, beta, w, stickiness = model_parameters
    n_trials = len(action_1)
    
    # Initialize Transition Matrix: Row=Action(A/U), Col=State(X/Y)
    # Start with standard belief: A->X (0->0) is 0.7, U->Y (1->1) is 0.7
    trans_probs = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = float(reward[trial])

        # --- Stage 1 Policy ---
        # MB Value calculation using dynamic transition matrix
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = trans_probs @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits = beta * q_net
        if last_action_1 != -1:
            logits[last_action_1] += stickiness
            
        exp_q1 = np.exp(logits)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        if a2 != -1:
            p_choice_2[trial] = probs_2[a2]
            
            # --- Updates ---
            # 1. MF Stage 1 Update
            delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
            q_stage1_mf[a1] += lr_val * delta_stage1
            
            # 2. Stage 2 Update
            delta_stage2 = r - q_stage2_mf[s_idx, a2]
            q_stage2_mf[s_idx, a2] += lr_val * delta_stage2
            
            # 3. Transition Probability Update
            # Update the row corresponding to the chosen spaceship (a1)
            # Increase prob of the observed state (s_idx), decrease others
            err = 1.0 - trans_probs[a1, s_idx]
            update = lr_trans * err
            trans_probs[a1, s_idx] += update
            trans_probs[a1, 1-s_idx] -= update
        else:
            p_choice_2[trial] = 1.0 # Neutral likelihood for missing data

        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Choice Kernel Hybrid Model
This model replaces the simple "1-back" stickiness with a **Choice Kernel**. The kernel integrates a history of past choices with a decay rate, allowing the model to capture stronger perseverance (or fatigue) effects that build up over multiple repetitions, distinct from value-based learning.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Choice Kernel Hybrid Model.
    
    Uses a decaying 'Choice Kernel' to track choice history/frequency. 
    Stickiness is applied based on this integrated history rather than just the previous trial.
    
    Parameters:
    learning_rate: [0, 1] - Q-value learning rate.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - MB/MF weight.
    lr_kernel: [0, 1] - Decay rate for the choice kernel (higher = faster decay).
    w_kernel: [0, 5] - Weight of the choice kernel (strength of perseverance).
    """
    learning_rate, beta, w, lr_kernel, w_kernel = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Initialize Choice Kernel (State variable for stickiness)
    choice_kernel = np.zeros(2)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = float(reward[trial])

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add Choice Kernel influence to logits
        logits = beta * q_net + w_kernel * choice_kernel
        
        exp_q1 = np.exp(logits)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Update Choice Kernel: K(a) <- K(a) + alpha_k * (I(a) - K(a))
        choice_vec = np.zeros(2)
        choice_vec[a1] = 1.0
        choice_kernel += lr_kernel * (choice_vec - choice_kernel)

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        if a2 != -1:
            p_choice_2[trial] = probs_2[a2]
            
            # --- Updates ---
            delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
            q_stage1_mf[a1] += learning_rate * delta_stage1

            delta_stage2 = r - q_stage2_mf[s_idx, a2]
            q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        else:
            p_choice_2[trial] = 1.0

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Epsilon-Greedy Lapse Hybrid Model
This model incorporates a **Lapse Rate (epsilon)**. It assumes that on a certain proportion of trials, the participant chooses randomly (due to attention lapses or noise) rather than following the value-based policy. This is robust to outliers and "mistakes" in the data.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Epsilon-Greedy Lapse.
    
    Models the decision policy as a mixture of the softmax value-based policy
    and a uniform random guess (lapse), controlled by parameter epsilon.
    
    Parameters:
    learning_rate: [0, 1] - Q-value learning rate.
    beta: [0, 10] - Inverse temperature for the softmax component.
    w: [0, 1] - MB/MF weight.
    stickiness: [0, 5] - Repetition bonus.
    epsilon: [0, 1] - Probability of random choice (lapse rate).
    """
    learning_rate, beta, w, stickiness, epsilon = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = float(reward[trial])

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits = beta * q_net
        if last_action_1 != -1:
            logits[last_action_1] += stickiness
            
        exp_q1 = np.exp(logits)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        # Apply Epsilon Mixture (Lapse)
        # P(a) = (1 - epsilon) * P_softmax(a) + epsilon * (1 / n_options)
        probs_1 = (1 - epsilon) * probs_1 + epsilon * 0.5
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        # Apply Epsilon Mixture to Stage 2 as well
        probs_2 = (1 - epsilon) * probs_2 + epsilon * 0.5
        
        if a2 != -1:
            p_choice_2[trial] = probs_2[a2]
            
            # --- Updates ---
            delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
            q_stage1_mf[a1] += learning_rate * delta_stage1

            delta_stage2 = r - q_stage2_mf[s_idx, a2]
            q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        else:
            p_choice_2[trial] = 1.0

        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```