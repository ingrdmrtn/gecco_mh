Here are the three proposed cognitive models.

### Model 1: Forgetting with Stickiness
This model introduces a **decay** parameter (`forgetting_rate`) alongside the standard stickiness. In the participant data, we see long streaks of behavior (stickiness), but the environment includes drifting probabilities. A "forgetting" mechanism allows the value of unchosen actions to decay toward zero (or a baseline), which helps the agent break out of old habits when the environment changes, effectively modeling memory constraints or active uncertainty management.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Q-Learning with Forgetting and Stickiness.
    
    Incorporates a decay term for unchosen options. This allows the model to 
    'forget' the value of actions not recently taken, which is useful in 
    environments with drifting probabilities.
    
    Parameters:
    learning_rate: [0, 1] - Update rate for chosen Q-values.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - Weighting between Model-Based (1) and Model-Free (0).
    stickiness: [0, 5] - Perseverance bonus for the previously chosen spaceship.
    forgetting_rate: [0, 1] - Rate at which unchosen Q-values decay toward 0.
    """
    learning_rate, beta, w, stickiness, forgetting_rate = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # 2 states (planets), 2 actions (aliens)
    
    last_action_1 = -1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = float(reward[trial])

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        logits = beta * q_net
        
        if last_action_1 != -1:
            logits[last_action_1] += stickiness
            
        exp_q1 = np.exp(logits)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates with Forgetting ---
        
        # Stage 1 MF Update
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        # Decay unchosen stage 1
        q_stage1_mf[1 - a1] *= (1 - forgetting_rate)

        # Stage 2 MF Update
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        # Decay unchosen stage 2 (on the current planet)
        q_stage2_mf[s_idx, 1 - a2] *= (1 - forgetting_rate)
        # Note: We could also decay the unvisited planet's values, 
        # but typically forgetting is modeled on the available alternatives.
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Dual Asymmetry Model
This model combines asymmetric learning rates (learning differently from better-than-expected vs. worse-than-expected outcomes) with asymmetric stickiness (persevering differently after wins vs. losses). This is a highly flexible model (6 parameters) designed to capture the nuanced "Win-Stay" vs "Lose-Stay" behaviors seen in the data, while also acknowledging that the *valuation* update itself might be biased (e.g., ignoring negative prediction errors).

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Dual Asymmetry Model (Split Learning Rate + Split Stickiness).
    
    Differentiates between positive and negative prediction errors (learning rates)
    AND differentiates perseverance based on the previous outcome (stickiness).
    
    Parameters:
    lr_pos: [0, 1] - Learning rate for positive prediction errors.
    lr_neg: [0, 1] - Learning rate for negative prediction errors.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - Model-Based weight.
    stick_win: [0, 5] - Stickiness bonus if previous trial was rewarded.
    stick_lose: [0, 5] - Stickiness bonus if previous trial was unrewarded.
    """
    lr_pos, lr_neg, beta, w, stick_win, stick_lose = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    last_reward = 0

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = float(reward[trial])

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        logits = beta * q_net
        
        if last_action_1 != -1:
            bonus = stick_win if last_reward > 0 else stick_lose
            logits[last_action_1] += bonus
            
        exp_q1 = np.exp(logits)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates with Asymmetric Learning Rates ---
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        lr_s1 = lr_pos if delta_stage1 > 0 else lr_neg
        q_stage1_mf[a1] += lr_s1 * delta_stage1

        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        lr_s2 = lr_pos if delta_stage2 > 0 else lr_neg
        q_stage2_mf[s_idx, a2] += lr_s2 * delta_stage2
        
        last_action_1 = a1
        last_reward = r

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Action vs. Planet Stickiness
This model distinguishes between "Motor Stickiness" (repeating the same spaceship choice) and "Goal Stickiness" (trying to return to the same planet). The data shows participants sometimes stick to a spaceship, but arguably they might be trying to stick to the *planet* where they last received a reward (or simply visited). This model calculates a "Planet Stickiness" bonus by using the transition matrix to boost the spaceship most likely to lead back to the previous planet.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Action vs. Planet Stickiness Model.
    
    Separates perseverance into two components:
    1. Action Stickiness: Tendency to repeat the same motor choice (spaceship).
    2. Planet Stickiness: Tendency to choose the action that leads to the 
       previously visited planet (Goal perseverance).
    
    Parameters:
    learning_rate: [0, 1] - Update rate.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - Model-Based weight.
    stick_action: [0, 5] - Bonus for repeating the previous spaceship.
    stick_planet: [0, 5] - Bonus for the spaceship leading to the previous planet.
    """
    learning_rate, beta, w, stick_action, stick_planet = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    last_state = -1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = float(reward[trial])

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        logits = beta * q_net
        
        # Apply Stickiness
        if last_action_1 != -1:
            # 1. Action Stickiness (Motor)
            logits[last_action_1] += stick_action
            
            # 2. Planet Stickiness (Goal)
            # Add bonus proportional to the probability of reaching the last planet
            # If last planet was 0, bonus to A0 is 0.7*S, to A1 is 0.3*S
            if last_state != -1:
                planet_stickiness_vec = transition_matrix[:, last_state] * stick_planet
                logits += planet_stickiness_vec
            
        exp_q1 = np.exp(logits)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        last_action_1 = a1
        last_state = s_idx

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```