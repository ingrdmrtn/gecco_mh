Here are three new cognitive models for the two-step decision task, designed to capture different potential mechanisms underlying the participants' behavior.

### Model 1: Dual-Learning Rate Hybrid Model
This model hypothesizes that participants track reward probabilities using two parallel learning processes with different time scales (fast and slow) to handle the "slowly changing" reward probabilities described in the task.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Dual-Learning Rate Hybrid Model.
    
    Stage 2 values are tracked by two separate learners (fast and slow) to capture 
    slowly changing reward probabilities while reacting to recent changes.
    The Model-Based system uses a weighted combination of these two value estimates.
    
    Parameters:
    lr_fast: [0, 1] - Learning rate for the fast process (rapid updating).
    lr_slow: [0, 1] - Learning rate for the slow process (stable integration).
    w_fast: [0, 1] - Weight given to the fast process in the combined Q-value (1 = all fast).
    beta: [0, 10] - Inverse temperature (exploration/exploitation).
    w: [0, 1] - Weighting between Model-Based (1) and Model-Free (0) control.
    stickiness: [0, 5] - Choice perseverance bonus for the Stage 1 choice.
    """
    lr_fast, lr_slow, w_fast, beta, w, stickiness = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Standard MF Q-values for Stage 1
    q_stage1_mf = np.zeros(2)
    
    # Dual Q-values for Stage 2
    q_stage2_fast = np.zeros((2, 2))
    q_stage2_slow = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = float(reward[trial])

        # Combine fast and slow Stage 2 values
        q_stage2_combined = w_fast * q_stage2_fast + (1 - w_fast) * q_stage2_slow

        # --- Policy for Stage 1 ---
        max_q_stage2 = np.max(q_stage2_combined, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_s1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        logits_1 = beta * q_net_s1
        
        if last_action_1 != -1:
            logits_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Policy for Stage 2 ---
        # Use the combined value for choice
        logits_2 = beta * q_stage2_combined[s_idx]
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Update Stage 1 MF (TD(0))
        delta_stage1 = q_stage2_combined[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr_fast * delta_stage1 # Use fast rate for MF stage 1 for parsimony

        # Update Stage 2 Dual Learners
        delta_fast = r - q_stage2_fast[s_idx, a2]
        q_stage2_fast[s_idx, a2] += lr_fast * delta_fast
        
        delta_slow = r - q_stage2_slow[s_idx, a2]
        q_stage2_slow[s_idx, a2] += lr_slow * delta_slow
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Independent Beta Hybrid Model
This model decouples the Model-Based and Model-Free systems by assigning them independent inverse temperatures ($\beta$) instead of a mixing weight ($w$). This allows the model to find that one system is more "confident" or precise than the other without enforcing a linear trade-off.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Independent Beta Hybrid Model.
    
    Uses separate inverse temperature parameters for Model-Based and Model-Free 
    systems in Stage 1, allowing for independent scaling of their influence.
    Also uses a separate beta for Stage 2 choices.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate for value updates.
    beta_mb: [0, 10] - Inverse temperature for Model-Based values in Stage 1.
    beta_mf: [0, 10] - Inverse temperature for Model-Free values in Stage 1.
    beta_s2: [0, 10] - Inverse temperature for Stage 2 choices.
    stickiness: [0, 5] - Choice perseverance bonus for Stage 1.
    """
    learning_rate, beta_mb, beta_mf, beta_s2, stickiness = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = float(reward[trial])

        # --- Policy for Stage 1 ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Logits are sum of weighted components
        logits_1 = (beta_mb * q_stage1_mb) + (beta_mf * q_stage1_mf)
        
        if last_action_1 != -1:
            logits_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Policy for Stage 2 ---
        logits_2 = beta_s2 * q_stage2_mf[s_idx]
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Transition-Split Learning Hybrid Model
This model proposes that participants update their Model-Free values differently depending on whether the transition they experienced was "Common" or "Rare". This allows the agent to gate Model-Free learning based on the likelihood of the transition, potentially ignoring "lucky" or "unlucky" rare transitions when updating the spaceship values.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Transition-Split Learning Hybrid Model.
    
    Applies different learning rates to Model-Free updates of the first-stage 
    action values depending on whether the transition experienced was common 
    or rare.
    
    Parameters:
    lr_base: [0, 1] - Learning rate for Stage 2 updates and Common-transition Stage 1 updates.
    lr_rare: [0, 1] - Learning rate for Rare-transition Stage 1 updates.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - Weighting between Model-Based and Model-Free.
    stickiness: [0, 5] - Choice perseverance bonus.
    """
    lr_base, lr_rare, beta, w, stickiness = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = float(reward[trial])

        # --- Policy for Stage 1 ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        logits_1 = beta * q_net
        
        if last_action_1 != -1:
            logits_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Policy for Stage 2 ---
        logits_2 = beta * q_stage2_mf[s_idx]
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Determine if transition was common or rare
        # Common: (A=0 -> S=0) or (A=1 -> S=1)
        is_common = (a1 == s_idx)
        
        current_lr_s1 = lr_base if is_common else lr_rare
        
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += current_lr_s1 * delta_stage1

        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr_base * delta_stage2
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```