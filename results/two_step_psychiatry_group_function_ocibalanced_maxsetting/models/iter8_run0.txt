Here are the three proposed cognitive models.

### Model 1: Hybrid Model with Loss Aversion
This model introduces a `loss_aversion` parameter. The participant data includes instances of negative rewards (e.g., -1 coins). Standard reinforcement learning models treat rewards linearly, but humans often weigh losses more heavily than equivalent gains. This model scales negative rewards by `loss_aversion` before updating Q-values, allowing the model to capture stronger avoidance learning from negative outcomes.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Loss Aversion.
    
    Incorporates a loss aversion parameter that scales the subjective value 
    of negative rewards. This addresses the negative rewards (e.g., -1) observed
    in the participant data.
    
    Parameters:
    learning_rate: [0, 1] - Update rate for Q-values.
    beta: [0, 10] - Inverse temperature (exploration/exploitation).
    w: [0, 1] - Weighting between Model-Based (1) and Model-Free (0).
    stickiness: [0, 5] - Choice persistence bonus.
    loss_aversion: [0, 10] - Scaling factor for negative rewards.
    """
    learning_rate, beta, w, stickiness, loss_aversion = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r_raw = float(reward[trial])
        
        # Apply loss aversion to the reward
        if r_raw < 0:
            r = r_raw * loss_aversion
        else:
            r = r_raw

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        logits = beta * q_net
        
        if last_action_1 != -1:
            logits[last_action_1] += stickiness
            
        exp_q1 = np.exp(logits)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Learning
        # Stage 1 Update (TD)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Stage 2 Update (Reward)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model with Transition-Dependent Stickiness
This model differentiates stickiness based on whether the previous trial's transition was Common or Rare. While standard stickiness captures a general tendency to repeat choices, this model tests if participants are differentially prone to perseverate depending on whether the environment behaved predictably (Common) or unpredictably (Rare) in the last trial, offering a heuristic mechanism distinct from model-based value calculation.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Transition-Dependent Stickiness.
    
    Stickiness is modulated by the type of transition (Common vs Rare)
    experienced in the previous trial.
    
    Parameters:
    learning_rate: [0, 1] - Update rate for Q-values.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - Weighting between Model-Based (1) and Model-Free (0).
    stick_common: [0, 5] - Stickiness bonus after a Common transition.
    stick_rare: [0, 5] - Stickiness bonus after a Rare transition.
    """
    learning_rate, beta, w, stick_common, stick_rare = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    last_state = -1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = float(reward[trial])

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        logits = beta * q_net
        
        if last_action_1 != -1:
            # Determine if previous transition was Common or Rare
            # Common: (A=0 -> S=0) or (A=1 -> S=1) given 0.7 diagonal
            is_common = (last_action_1 == 0 and last_state == 0) or \
                        (last_action_1 == 1 and last_state == 1)
            
            bonus = stick_common if is_common else stick_rare
            logits[last_action_1] += bonus
            
        exp_q1 = np.exp(logits)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Learning
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        last_action_1 = a1
        last_state = s_idx

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model with Outcome-Dependent Exploration (Split Beta)
This model allows the inverse temperature (`beta`) to vary based on the previous trial's outcome. It tests the hypothesis that participants may shift between exploitation (higher beta, more deterministic) after receiving a reward and exploration (lower beta, more random) after a lack of reward or loss. This is distinct from stickiness, as it affects the sensitivity to value differences rather than adding a fixed bias.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Reward-Dependent Exploration (Split Beta).
    
    Uses different inverse temperature (beta) parameters depending on 
    the previous trial's outcome (Win vs Lose/No-Reward).
    
    Parameters:
    learning_rate: [0, 1] - Update rate for Q-values.
    beta_win: [0, 10] - Inverse temperature after a rewarded trial.
    beta_lose: [0, 10] - Inverse temperature after an unrewarded/loss trial.
    w: [0, 1] - Weighting between Model-Based (1) and Model-Free (0).
    stickiness: [0, 5] - Choice persistence bonus.
    """
    learning_rate, beta_win, beta_lose, w, stickiness = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    last_reward = 0.0 # Default to 'lose' state or neutral for first trial

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = float(reward[trial])

        # Determine current beta based on previous outcome
        current_beta = beta_win if last_reward > 0 else beta_lose

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        logits = current_beta * q_net
        
        if last_action_1 != -1:
            logits[last_action_1] += stickiness
            
        exp_q1 = np.exp(logits)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2 Policy (using same beta for consistency)
        exp_q2 = np.exp(current_beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Learning
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        last_action_1 = a1
        last_reward = r

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```