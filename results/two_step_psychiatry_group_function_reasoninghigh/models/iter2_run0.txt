Here are three new cognitive models for the two-step task. These models introduce stage-specific mechanisms (learning rates, inverse temperatures) and memory decay to capture different aspects of decision-making dynamics not covered by the standard hybrid or stickiness models.

### Model 1: Stage-Specific Learning Rates
This model differentiates between the learning speed of the high-level transition structure (Stage 1) and the immediate reward values of the aliens (Stage 2). It assumes participants might update their values for spaceships and aliens at different rates.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Dual Learning Rate Model.
    
    Distinguishes between learning rates for the first stage (transitions/planning)
    and the second stage (reward estimation). This accounts for potential differences
    in plasticity between learning the value of spaceships versus aliens.

    Parameters:
    - lr_stage1: [0, 1] Learning rate for Stage 1 (TD update from Stage 2 value).
    - lr_stage2: [0, 1] Learning rate for Stage 2 (Reward update).
    - beta: [0, 10] Inverse temperature (choice consistency).
    - w: [0, 1] Weighting. 1 = Model-Based, 0 = Model-Free.
    """
    lr_stage1, lr_stage2, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # state x action

    for trial in range(n_trials):

        # --- Policy for the first choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Weighted mixture of Model-Based and Model-Free values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Policy for the second choice ---
        # Standard Softmax on Stage 2 MF values
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # Stage 1 Update (using lr_stage1)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr_stage1 * delta_stage1
        
        # Stage 2 Update (using lr_stage2)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr_stage2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Stage-Specific Inverse Temperatures
This model posits that the level of decision noise (exploration vs. exploitation) differs between the two stages. For example, the choice between spaceships (Stage 1) might be more calculated (or more noisy) than the simple bandit choice between aliens (Stage 2).

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Dual Beta Model.
    
    Uses separate inverse temperature parameters for Stage 1 and Stage 2.
    This allows the model to capture different levels of exploration or 
    decision noise in the planning phase (Stage 1) versus the bandit phase (Stage 2).

    Parameters:
    - learning_rate: [0, 1] Universal learning rate.
    - beta_1: [0, 10] Inverse temperature for Stage 1 choice.
    - beta_2: [0, 10] Inverse temperature for Stage 2 choice.
    - w: [0, 1] Weighting. 1 = Model-Based, 0 = Model-Free.
    """
    learning_rate, beta_1, beta_2, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Policy for the first choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Use beta_1 for Stage 1
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Policy for the second choice ---
        # Use beta_2 for Stage 2
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model with Forgetting (Decay)
This model adds a decay parameter to the second-stage Q-values. Since the reward probabilities of the aliens drift slowly over time, participants may "forget" or devalue the estimates of aliens they haven't visited recently, effectively modeling a belief in environmental volatility.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Q-value Decay (Forgetting).
    
    Incorporates a decay parameter for Stage 2 Q-values. Unchosen options 
    in Stage 2 decay towards zero, representing forgetting or a belief that 
    information becomes outdated (volatility) over time.

    Parameters:
    - learning_rate: [0, 1] Update rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weighting. 1 = Model-Based, 0 = Model-Free.
    - decay: [0, 1] Decay rate for unchosen Stage 2 options (0 = no decay, 1 = instant forget).
    """
    learning_rate, beta, w, decay = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Policy for the first choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Policy for the second choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Apply decay to unchosen options in Stage 2 (both states)
        # This models forgetting of specific alien values over time
        for s in range(2):
            for a in range(2):
                if not (s == state_idx and a == action_2[trial]):
                    q_stage2_mf[s, a] *= (1.0 - decay)
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```