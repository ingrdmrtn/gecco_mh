Here are 3 new cognitive models for the two-step task.

### Model 1: Asymmetric Learning with Stickiness
This model combines **Asymmetric Learning Rates** (confirmation bias) with **Choice Stickiness**. It assumes participants update their value estimates differently depending on whether the outcome was better (positive prediction error) or worse (negative prediction error) than expected, while also accounting for the tendency to repeat the previous Stage 1 choice.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Asymmetric Learning Model with Choice Stickiness.
    
    Splits the learning rate into positive and negative components to capture 
    confirmation bias or risk aversion, combined with a stickiness parameter 
    to account for motor/choice perseverance.

    Parameters:
    - lr_pos: [0, 1] Learning rate for positive prediction errors (better than expected).
    - lr_neg: [0, 1] Learning rate for negative prediction errors (worse than expected).
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Model-based weight.
    - stickiness: [0, 5] Perseverance bonus for the previously chosen Stage 1 action.
    """
    lr_pos, lr_neg, beta, w, stickiness = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Apply stickiness to the Q-value of the previously chosen action
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_action_1 = action_1[trial]
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Update Stage 1 (TD(0))
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        current_lr_s1 = lr_pos if delta_stage1 > 0 else lr_neg
        q_stage1_mf[action_1[trial]] += current_lr_s1 * delta_stage1
        
        # Update Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        current_lr_s2 = lr_pos if delta_stage2 > 0 else lr_neg
        q_stage2_mf[state_idx, action_2[trial]] += current_lr_s2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid with Passive Decay and Stickiness
This model incorporates **Passive Decay** (forgetting) alongside the standard hybrid mechanisms and stickiness. It assumes that the values of actions *not* chosen on a given trial slowly decay toward zero (or a neutral value), reflecting memory degradation or fictive updating where unchosen options are devalued.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Passive Decay and Stickiness.
    
    Incorporates a decay parameter applied to unchosen actions, simulating 
    forgetting or counterfactual devaluation, alongside choice stickiness.

    Parameters:
    - learning_rate: [0, 1] Update rate for chosen actions.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Model-based weight.
    - stickiness: [0, 5] Perseverance bonus.
    - decay_rate: [0, 1] Retention rate for unchosen actions (1 = no decay, 0 = full reset).
    """
    learning_rate, beta, w, stickiness, decay_rate = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_action_1 = action_1[trial]
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Update Stage 1
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        # Decay unchosen Stage 1
        q_stage1_mf[1 - action_1[trial]] *= decay_rate

        # Update Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        # Decay unchosen Stage 2 (for the current state)
        q_stage2_mf[state_idx, 1 - action_2[trial]] *= decay_rate
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Independent System Temperatures (MB/MF)
Instead of a single inverse temperature (`beta`) and a weighting parameter (`w`), this model uses **separate inverse temperatures** for the Model-Based and Model-Free systems. This allows the confidence (or influence) of the two systems to vary independently in magnitude, rather than just in ratio, providing a different geometric constraint on the decision surface.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Independent Model-Based and Model-Free Temperatures.
    
    Replaces the standard beta/w parameterization with distinct beta parameters
    for the MB and MF systems. This allows the 'sharpness' of the two systems
    to vary independently. Includes stickiness.

    Parameters:
    - learning_rate: [0, 1] Update rate.
    - beta_mb: [0, 10] Inverse temperature for the Model-Based component.
    - beta_mf: [0, 10] Inverse temperature for the Model-Free component.
    - stickiness: [0, 5] Perseverance bonus.
    """
    learning_rate, beta_mb, beta_mf, stickiness = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combine using independent betas rather than w weighting
        logits_1 = (beta_mb * q_stage1_mb) + (beta_mf * q_stage1_mf)
        
        if last_action_1 != -1:
            logits_1[last_action_1] += stickiness

        exp_q1 = np.exp(logits_1) # Beta is implicitly inside logits
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_action_1 = action_1[trial]
        state_idx = state[trial]

        # policy for the second choice (Pure Model-Free)
        # We use beta_mf here for consistency within the MF system
        exp_q2 = np.exp(beta_mf * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Update Stage 1
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1

        # Update Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```