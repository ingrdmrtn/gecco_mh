Here are the 3 proposed cognitive models.

### Model 1: Subjective Transition Belief
This model hypothesizes that participants may not internalize the exact ground-truth transition probabilities (0.7/0.3). Instead, they treat the transition structure as having a subjective probability $P$, which they use for Model-Based planning. This allows the model to capture behavior ranging from treating transitions as random ($P=0.5$) to deterministic ($P=1.0$).

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Subjective Transition Belief Model.
    
    Standard hybrid models assume the participant knows the transition matrix 
    is exactly [[0.7, 0.3], [0.3, 0.7]]. This model fits a parameter 
    'transition_prob' representing the participant's subjective belief 
    about the probability of the common transition.
    
    Parameters:
    - learning_rate: [0, 1] Update rate for MF values.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weighting (1 = Model-Based, 0 = Model-Free).
    - transition_prob: [0, 1] Subjective belief of common transition probability. 
                       (e.g., 1.0 = deterministic belief, 0.5 = no structure).
    """
    learning_rate, beta, w, transition_prob = model_parameters
    n_trials = len(action_1)
  
    # Construct subjective transition matrix
    # Row 0: Probabilities from Spaceship A (Common->X, Rare->Y)
    # Row 1: Probabilities from Spaceship U (Rare->X, Common->Y)
    # assuming symmetry in belief
    tp = transition_prob
    transition_matrix = np.array([[tp, 1-tp], [1-tp, tp]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # Policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        # Use subjective matrix for planning
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # Policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Updating
        # Stage 1 MF Update (SARSA-like)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Stage 2 MF Update (Reward prediction error)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Fictive (Counterfactual) Learning
This model implements "fictive" or counterfactual updating at the second stage. If a participant chooses Alien A and receives Reward 0, they may infer that Alien B would have yielded Reward 1 (and vice versa). This heuristic is common in binary choice tasks, even if the environment does not strictly enforce anti-correlated rewards.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Fictive (Counterfactual) Learning Model.
    
    Adds a mechanism to update the unchosen option in Stage 2.
    If the participant assumes rewards are anti-correlated (if A is bad, B is good),
    they update the unchosen alien based on the inverse of the received reward.
    
    Parameters:
    - learning_rate: [0, 1] Standard update rate for chosen options.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weighting (1 = Model-Based, 0 = Model-Free).
    - alpha_fictive: [0, 1] Learning rate for the unchosen option (counterfactual).
    """
    learning_rate, beta, w, alpha_fictive = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # Policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # Policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Updating
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Stage 2 MF Update (Chosen)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Stage 2 Fictive Update (Unchosen)
        # Assume unchosen option yielded (1 - reward)
        unchosen_action = 1 - action_2[trial]
        fictive_reward = 1 - reward[trial]
        delta_fictive = fictive_reward - q_stage2_mf[state_idx, unchosen_action]
        q_stage2_mf[state_idx, unchosen_action] += alpha_fictive * delta_fictive

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Accumulating Choice Trace (Decaying Stickiness)
Previous models tested simple "last-choice" stickiness. This model implements an "Accumulating Choice Trace." Stickiness is not just a binary flag of the last trial, but a continuous variable that builds up if a choice is repeated and decays over time. This captures "momentum" or deep habituation where a long run of choosing 'A' makes it much harder to switch than a single choice of 'A'.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Accumulating Choice Trace (Decaying Stickiness) Model.
    
    Instead of adding a fixed bonus for the immediately preceding choice,
    this model maintains a 'choice trace' that accumulates when an action is 
    chosen and decays on every trial. This captures choice momentum.
    
    Parameters:
    - learning_rate: [0, 1] Update rate for MF values.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weighting (1 = Model-Based, 0 = Model-Free).
    - stick_weight: [0, 5] Weight of the choice trace added to Q-values.
    - stick_decay: [0, 1] Decay rate of the trace (0 = instant forgetting, 1 = no decay).
    """
    learning_rate, beta, w, stick_weight, stick_decay = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Trace for Stage 1 actions (Space A vs Space U)
    choice_trace = np.zeros(2)

    for trial in range(n_trials):

        # Policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add accumulating stickiness trace
        q_net_sticky = q_net + (stick_weight * choice_trace)
        
        exp_q1 = np.exp(beta * q_net_sticky)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Update Choice Trace
        # Decay both traces
        choice_trace *= stick_decay
        # Increment trace for chosen action
        choice_trace[action_1[trial]] += 1.0
        
        state_idx = state[trial]

        # Policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Updating
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```