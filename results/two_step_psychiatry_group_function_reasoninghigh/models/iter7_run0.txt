Here are three new cognitive models for the two-step task. These models explore specific mechanisms like stage-specific learning rates, eligibility traces (pure model-free learning), and attentional lapses, which are distinct from the parameter combinations previously attempted.

### Model 1: Hybrid Model with Stage-Specific Learning Rates and Stickiness
This model hypothesizes that participants learn the values of the "planning" stage (choosing a spaceship) at a different rate than the "harvesting" stage (choosing an alien), perhaps due to the different cognitive demands (abstract transition vs. concrete reward). It combines this with the essential stickiness parameter.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Stage-Specific Learning Rates and Stickiness.

    This model separates the learning rate for the first stage (transitions/planning)
    from the second stage (terminal reward). This allows the model to capture 
    differences in plasticity between the structural decision and the reward-based decision.

    Parameters:
    - lr_stage1: [0, 1] Learning rate for Stage 1 (Spaceship values).
    - lr_stage2: [0, 1] Learning rate for Stage 2 (Alien values).
    - beta: [0, 10] Inverse temperature (choice consistency).
    - w: [0, 1] Weighting parameter (1 = Model-Based, 0 = Model-Free).
    - stickiness: [0, 5] Choice perseverance bonus for Stage 1.
    """
    lr_stage1, lr_stage2, beta, w, stickiness = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action

    last_action_1 = -1

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        # Model-Based Value
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Apply Stickiness
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_action_1 = action_1[trial]
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updating ---
        # Stage 1 Update (using lr_stage1)
        # Note: Standard MF update uses the value of the state actually reached (SARSA-like logic here)
        # or the max of the next state. Standard Daw task uses max Q of arrived state.
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr_stage1 * delta_stage1
        
        # Stage 2 Update (using lr_stage2)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr_stage2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Pure Model-Free with Eligibility Traces and Stickiness
This model removes the Model-Based component entirely (setting $w=0$) to test if behavior can be explained solely by sophisticated habit learning. It uses an eligibility trace ($\lambda$) which allows the Stage 2 reward prediction error to directly update Stage 1 choices, mimicking planning-like behavior without an explicit transition model.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free with Eligibility Traces and Stickiness.

    This model assumes no Model-Based planning (w=0). Instead, it uses an 
    eligibility trace (lambda) to allow outcomes at Stage 2 to update Stage 1 
    values directly. This tests if "planning" is actually just efficient 
    temporal difference learning.

    Parameters:
    - learning_rate: [0, 1] Update rate for values.
    - beta: [0, 10] Inverse temperature.
    - lambda_eligibility: [0, 1] Decay of eligibility trace (0 = TD(0), 1 = Monte Carlo).
    - stickiness: [0, 5] Choice perseverance bonus for Stage 1.
    """
    learning_rate, beta, lambda_eligibility, stickiness = model_parameters
    n_trials = len(action_1)
  
    # No transition matrix needed for pure MF
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    last_action_1 = -1

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        q_net = q_stage1_mf.copy()
        
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_action_1 = action_1[trial]
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updating ---
        # 1. Prediction Error at Stage 1 (State 1 -> State 2)
        # We use the value of the chosen action in stage 2 as the target
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # 2. Prediction Error at Stage 2 (State 2 -> Reward)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # 3. Eligibility Trace Update
        # The Stage 2 error propagates back to Stage 1 scaled by lambda
        q_stage1_mf[action_1[trial]] += learning_rate * lambda_eligibility * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model with Stickiness and Random Lapses (Epsilon)
This model introduces an `epsilon` parameter to account for "lapses" or random noise in decision-making. Unlike `beta` (which models value-dependent exploration), `epsilon` models value-independent random errors (e.g., pressing the wrong button or momentary inattention), which is often crucial in fitting real-world participant data.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Stickiness and Random Lapses (Epsilon-Mixture).

    This model adds a 'lapse rate' (epsilon) to the standard hybrid learner.
    The final choice probability is a mixture of the softmax distribution 
    and a uniform random distribution. This makes the model robust to 
    outliers or attentional lapses.

    Parameters:
    - learning_rate: [0, 1] Update rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weighting (1 = Model-Based, 0 = Model-Free).
    - stickiness: [0, 5] Perseverance bonus.
    - epsilon: [0, 1] Probability of choosing randomly (Lapse rate).
    """
    learning_rate, beta, w, stickiness, epsilon = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    last_action_1 = -1

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness

        exp_q1 = np.exp(beta * q_net)
        probs_1_softmax = exp_q1 / np.sum(exp_q1)
        
        # Apply Epsilon Mixture (Lapse)
        # P(choice) = (1-eps) * Softmax + eps * (1/n_actions)
        probs_1 = (1 - epsilon) * probs_1_softmax + epsilon * 0.5
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_action_1 = action_1[trial]
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2_softmax = exp_q2 / np.sum(exp_q2)
        
        # Apply Epsilon Mixture to Stage 2 as well
        probs_2 = (1 - epsilon) * probs_2_softmax + epsilon * 0.5
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updating ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```