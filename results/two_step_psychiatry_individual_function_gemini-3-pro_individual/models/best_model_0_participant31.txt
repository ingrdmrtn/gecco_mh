def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free Learner (TD(0)).
    
    This model learns Q-values for both stages using direct prediction error updates.
    It does not use the transition matrix to plan. It updates the first stage 
    value based on the value of the state reached in the second stage.

    Parameters:
    learning_rate: [0,1] Rate at which Q-values are updated.
    beta: [0,10] Inverse temperature for softmax.
    """
    learning_rate, beta = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)      # Values for Spaceship A (0) and U (1)
    q_stage2_mf = np.zeros((2, 2)) # Values for Planet X (aliens 0,1) and Planet Y (aliens 0,1)

    for trial in range(n_trials):

        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial] # The planet we actually arrived at

        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]



        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1

        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss