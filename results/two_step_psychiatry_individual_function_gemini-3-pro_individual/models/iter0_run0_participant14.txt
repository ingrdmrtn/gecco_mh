Here are three cognitive models based on the two-step task paradigm, ranging from purely model-free to purely model-based, and finally a hybrid model.

### Model 1: Pure Model-Free TD Learning
This model assumes the participant learns values solely through trial-and-error (temporal difference learning). It does not use knowledge of the transition structure (probabilities of moving from spaceship to planet). It updates values for the first stage based on the second stage's value (SARSA-style or simple TD), and updates second-stage values based on the reward.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free Reinforcement Learning Model.
    
    This model assumes the agent learns action values (Q-values) purely through 
    reward prediction errors (TD learning) without building a model of the 
    environment's transition structure.
    
    Parameters:
    learning_rate: [0, 1] Rate at which Q-values are updated.
    beta: [0, 10] Inverse temperature for softmax choice rule (exploration/exploitation).
    """
    learning_rate, beta = model_parameters
    n_trials = len(action_1)
  
    # Initialize Q-values
    q_stage1_mf = np.zeros(2)      # Values for Spaceship A (0) and U (1)
    q_stage2_mf = np.zeros((2, 2)) # Values for Aliens on Planet X (0) and Y (1)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Calculate probabilities using Softmax on Model-Free Q-values
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Determine which planet (state) was reached
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        # Calculate probabilities for the aliens at the current planet
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Value Updating ---
        # 1. Update Stage 1 Q-value based on the value of the state reached (TD(0))
        # Here we use the value of the chosen second-stage action as the proxy for the state value
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # 2. Update Stage 2 Q-value based on the received reward
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Pure Model-Based Learning
This model assumes the participant plans their first decision by calculating expected values using the known transition matrix (70/30 split). It does not cache model-free values for the first stage. Instead, it computes the value of a spaceship by combining the values of the planets it leads to, weighted by the transition probabilities.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Based Reinforcement Learning Model.
    
    This model computes Stage 1 values by projecting the known transition matrix 
    onto the learned values of Stage 2 (aliens). It does not maintain separate 
    Model-Free Q-values for Stage 1.
    
    Parameters:
    learning_rate: [0, 1] Rate at which Stage 2 Q-values are updated.
    beta: [0, 10] Inverse temperature for softmax choice rule.
    """
    learning_rate, beta = model_parameters
    n_trials = len(action_1)
  
    # Known transition structure: Row 0 -> [Planet X, Planet Y], Row 1 -> [Planet X, Planet Y]
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Only need to track Stage 2 values; Stage 1 values are computed on the fly
    q_stage2_mf = np.zeros((2, 2)) 

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):

        # --- Stage 1 Choice ---
        # Calculate Model-Based values: V(Spaceship) = P(Planet|Spaceship) * max(Q(Alien|Planet))
        max_q_stage2 = np.max(q_stage2_mf, axis=1) # Best value available on each planet
        q_stage1_mb = transition_matrix @ max_q_stage2 # Expected value of each spaceship
        
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        # Standard softmax on the alien values
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Value Updating ---
        # Only update Stage 2 values based on reward. 
        # Stage 1 values change automatically because they are derived from Stage 2.
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model-Based / Model-Free
This is the classic "Two-Step" model (Daw et al., 2011). It assumes the brain maintains both a Model-Free (habitual) value and a Model-Based (goal-directed) value for the first stage. The final decision is a weighted combination of both, controlled by a mixing parameter `w`.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Reinforcement Learning Model.
    
    This model combines Model-Free (TD) and Model-Based (Planning) valuations 
    for the first-stage choice using a weighting parameter 'w'.
    
    Parameters:
    learning_rate: [0, 1] Rate at which Q-values are updated.
    beta: [0, 10] Inverse temperature for softmax choice rule.
    w: [0, 1] Weighting parameter. 0 = Pure Model-Free, 1 = Pure Model-Based.
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)      # Model-Free values for Stage 1
    q_stage2_mf = np.zeros((2, 2)) # Values for Stage 2 (shared by MB and MF systems)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):

        # --- Stage 1 Choice ---
        # 1. Calculate Model-Based Value
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Calculate Net Value (Hybrid)
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # 3. Choice Probability
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        # Standard softmax on Stage 2 values
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Value Updating ---
        # 1. Update MF Stage 1 value (TD update)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # 2. Update Stage 2 value (Reward update)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```