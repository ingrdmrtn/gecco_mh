Here are three cognitive models based on the provided template and task description.

### Model 1: Pure Model-Based Reinforcement Learning
This model assumes the participant builds a mental map of the task structure (transition probabilities) and calculates the value of stage 1 actions by looking ahead to the expected value of stage 2. It ignores Model-Free learning at stage 1, relying entirely on the transition matrix and stage 2 values.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Based Reinforcement Learning.
    
    This model assumes the agent makes stage 1 decisions by calculating the expected 
    value of the second stage states using the known transition matrix. It does not 
    maintain a separate model-free Q-value for stage 1 actions.
    
    Parameters:
    learning_rate: [0, 1] Rate at which Q-values for the second stage are updated.
    beta: [0, 10] Inverse temperature for softmax choice (exploration/exploitation).
    """
    learning_rate, beta = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition matrix as per task description (A->X, U->Y commonly)
    # Rows: Action 1 (0 or 1), Cols: State (0 or 1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values for stage 2: 2 states (planets) x 2 actions (aliens)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy (Model-Based) ---
        # Calculate max value available in each state (planet)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        
        # Calculate MB value: sum(P(state|action) * max_Q(state))
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updating ---
        # No Stage 1 update needed because q_stage1_mb is computed fresh every trial 
        # based on the evolving q_stage2_mf.
        
        # Update Stage 2 Q-values based on reward
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model-Based / Model-Free (Dyna-style)
This is the classic "Daw et al. (2011)" style hybrid model. It maintains both Model-Free Q-values (habitual, based on direct reward history) and Model-Based Q-values (planning, based on transition structure) for the first stage. A mixing parameter `w` determines the balance between these two systems.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Reinforcement Learning.
    
    This model combines habitual (Model-Free) and planning (Model-Based) strategies.
    A mixing parameter 'w' determines the weight given to the Model-Based system.
    
    Parameters:
    learning_rate: [0, 1] Update rate for Q-values.
    beta: [0, 10] Inverse temperature for softmax.
    w: [0, 1] Weight for Model-Based control (0=Pure MF, 1=Pure MB).
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # MF values for Stage 1 (2 actions)
    q_stage1_mf = np.zeros(2)
    # MF values for Stage 2 (2 states x 2 actions)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based Value Calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Value: weighted sum of MB and MF values
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updating ---
        # Stage 1 MF Update (TD(0)): Driven by the value of the state actually reached (state_idx)
        # Note: We use the max value of the next state as the proxy for V(state)
        v_next_state = np.max(q_stage2_mf[state_idx])
        delta_stage1 = v_next_state - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Stage 2 MF Update: Driven by actual reward
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Note: In a full TD(1) or lambda model, the stage 1 MF value might also get updated 
        # by the stage 2 prediction error (eligibility traces), but this is a simplified TD(0) implementation.

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Choice Kernel (Perseveration) + Model-Free
This model ignores the model-based transition structure entirely. Instead, it relies on Model-Free learning but adds a "Choice Kernel." This kernel tracks the tendency to repeat the previously chosen action regardless of reward (perseveration) or switch (alternation), capturing basic motor or decision inertia often seen in behavioral data.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Reinforcement Learning with Choice Perseveration.
    
    This model is purely model-free but includes a 'choice stickiness' parameter.
    It tracks how often an action was recently chosen and adds a bias to repeat it.
    
    Parameters:
    learning_rate: [0, 1] Update rate for Q-values.
    beta: [0, 10] Inverse temperature for softmax.
    perseveration: [0, 5] Weight added to the utility of the previously chosen action.
    """
    learning_rate, beta, perseveration = model_parameters
    n_trials = len(action_1)
  
    # No transition matrix needed for pure MF
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Choice trace: 1 if action was taken last trial, 0 otherwise
    # Initialize to 0
    last_action_1 = -1 

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Calculate effective values: Q-value + Perseveration Bonus
        q_net = q_stage1_mf.copy()
        if last_action_1 != -1:
            q_net[last_action_1] += perseveration
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        # Perseveration is usually only modeled at the first stage in this task
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updating ---
        # Standard TD(0) updates
        v_next_state = np.max(q_stage2_mf[state_idx])
        delta_stage1 = v_next_state - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Update choice history for next trial
        last_action_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```