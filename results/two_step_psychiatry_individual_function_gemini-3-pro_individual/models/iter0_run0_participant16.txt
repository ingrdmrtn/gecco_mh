Here are three cognitive models based on the two-step task framework. They progress from a pure Model-Based approach to a hybrid Model-Based/Model-Free reinforcement learning model, and finally to a model that incorporates habit (perseveration).

### Model 1: Pure Model-Based Reinforcement Learning
This model assumes the participant builds a mental map of the task structure. They learn the values of the aliens (Stage 2) using a simple delta rule. However, for the spaceship choice (Stage 1), they do *not* learn values directly from experience. Instead, they calculate the expected value of a spaceship by combining the known transition probabilities (70/30) with the learned values of the aliens on the destination planets.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Based Reinforcement Learning Model.
    
    This model assumes the agent calculates Stage 1 values solely by planning:
    combining the fixed transition matrix with the learned values of the Stage 2 states.
    It does not maintain a separate Model-Free value for Stage 1.
    
    Parameters:
    learning_rate: [0, 1] - Rate at which Stage 2 (alien) values are updated.
    beta: [0, 10] - Inverse temperature for softmax choice stochasticity.
    """
    learning_rate, beta = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition matrix as per task description
    # Row 0: Spaceship 0 -> [70% Planet 0, 30% Planet 1]
    # Row 1: Spaceship 1 -> [30% Planet 0, 70% Planet 1]
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values for Stage 2 (Aliens): 2 planets x 2 aliens
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- STAGE 1 CHOICE (Model-Based) ---
        # Calculate max value available on each planet
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        
        # Calculate MB values for Stage 1: Transition Prob * Max Stage 2 Value
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Softmax policy for Stage 1
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial] # Actual planet arrived at

        # --- STAGE 2 CHOICE ---
        # Softmax policy for Stage 2 (choosing an alien)
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- UPDATING ---
        # Note: In pure MB, we do not update Stage 1 values directly.
        # We only update the Stage 2 values based on reward.
        
        # Prediction error for Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Update Stage 2 Q-value
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model-Based / Model-Free RL
This is the classic "Daw two-step" model. It assumes the brain uses a mixture of two strategies. The Model-Free (MF) system learns Stage 1 values based on simple reward history (TD-learning). The Model-Based (MB) system calculates Stage 1 values based on the transition structure. A mixing parameter `w` determines which system controls the decision.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Reinforcement Learning Model.
    
    This model computes Stage 1 values as a weighted sum of Model-Based (planning)
    and Model-Free (habitual/TD) values.
    
    Parameters:
    learning_rate: [0, 1] - Rate for updating Q-values.
    beta: [0, 10] - Inverse temperature for softmax.
    w: [0, 1] - Weighting parameter (0 = Pure MF, 1 = Pure MB).
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # MF Q-values for Stage 1 (Spaceships)
    q_stage1_mf = np.zeros(2)
    # MF Q-values for Stage 2 (Aliens)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- STAGE 1 CHOICE (Hybrid) ---
        # 1. Model-Based Calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Hybrid Combination
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax policy for Stage 1
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- STAGE 2 CHOICE ---
        # Softmax policy for Stage 2
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- UPDATING ---
        # 1. Stage 1 MF Update (TD(0))
        # The value of the chosen spaceship is updated toward the value of the state reached
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # 2. Stage 2 MF Update
        # The value of the chosen alien is updated toward the reward received
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Note: In full TD(1) models, there is often an eligibility trace updating Stage 1
        # based on the Stage 2 reward outcome. For simplicity here, we use simple TD chaining.

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid RL with Choice Perseveration
This model extends the Hybrid model by adding a "perseveration" parameter. Humans often have a tendency to repeat their previous choice regardless of reward (stickiness). This model adds a bonus to the Q-value of the spaceship chosen on the immediately preceding trial.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid RL Model with Choice Perseveration (Stickiness).
    
    Adds a 'stickiness' parameter that biases the agent to repeat the 
    previous Stage 1 choice, independent of reward history.
    
    Parameters:
    learning_rate: [0, 1] - Rate for updating Q-values.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - MB/MF mixing weight.
    perseveration: [0, 5] - Bonus added to the previously chosen action (Stickiness).
    """
    learning_rate, beta, w, perseveration = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track the previous action (initialize to -1 or handle first trial separately)
    last_action_1 = -1

    for trial in range(n_trials):

        # --- STAGE 1 CHOICE ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add perseveration bonus
        # We create a temporary Q-vector for decision making so we don't
        # permanently alter the learned Q-values with the stickiness bonus.
        decision_q = q_net.copy()
        if last_action_1 != -1:
            decision_q[last_action_1] += perseveration
        
        exp_q1 = np.exp(beta * decision_q)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Update last action
        last_action_1 = action_1[trial]
        state_idx = state[trial]

        # --- STAGE 2 CHOICE ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- UPDATING ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```