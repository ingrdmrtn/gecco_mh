Here are three cognitive models representing different hypotheses about how the participant learns and makes decisions.

### Model 1: Pure Model-Free Learning (TD-Learning)
This model hypothesizes that the participant does not use a mental map of the task structure (the transition probabilities). Instead, they rely solely on temporal-difference (TD) learning. They update the value of the spaceship chosen based on the value of the subsequent state (the planet reached) and update the value of the alien chosen based on the reward received. This is a standard SARSA-like learning mechanism.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free Learner (TD-Learning).
    
    This model assumes the participant learns purely through trial and error without 
    using the transition structure of the task. It updates Stage 1 values based on 
    the value of the state reached in Stage 2 (TD-error), and Stage 2 values based 
    on the received reward.

    Parameters:
    - learning_rate: [0, 1] Rate at which Q-values are updated.
    - beta: [0, 10] Inverse temperature for softmax choice (exploration vs exploitation).
    """
    learning_rate, beta = model_parameters
    n_trials = len(action_1)
  
    # Q-values initialization
    # Stage 1: 2 spaceships
    q_stage1_mf = np.zeros(2) 
    # Stage 2: 2 planets (states) x 2 aliens (actions)
    q_stage2_mf = np.zeros((2, 2)) 

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):

        # --- Stage 1 Choice Policy ---
        # Softmax based on Model-Free Q-values only
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial] # The planet actually reached

        # --- Stage 2 Choice Policy ---
        # Softmax based on Stage 2 Q-values for the current state
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Value Updating ---
        # TD(0) update for Stage 1: Driven by the value of the chosen Stage 2 action
        # Note: Some variants use max(Q_stage2), this uses Q(s, a) (SARSA-style)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Prediction Error update for Stage 2: Driven by actual reward
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Pure Model-Based Learning
This model assumes the participant is a sophisticated planner. They ignore the direct history of rewards associated with spaceships. Instead, they learn the values of the aliens (Stage 2) and calculate the value of the spaceships (Stage 1) by combining the alien values with the known transition probabilities (70/30). This is "forward planning" or "goal-directed" behavior.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Based Learner.
    
    This model assumes the participant calculates the value of Stage 1 actions 
    by multiplying the learned max values of Stage 2 (aliens) by the fixed 
    transition probability matrix. They do not maintain separate cached 
    Model-Free values for Stage 1.

    Parameters:
    - learning_rate: [0, 1] Rate at which Stage 2 Q-values are updated.
    - beta: [0, 10] Inverse temperature for softmax choice.
    """
    learning_rate, beta = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition matrix: Row 0 -> [P(State0|Act0), P(State1|Act0)]
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Only Stage 2 values are learned directly from reward
    q_stage2_mf = np.zeros((2, 2)) 

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):

        # --- Stage 1 Choice Policy ---
        # Calculate Model-Based values on the fly
        # Value of a state is the max value available in that state
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        # Bellman equation: Q_MB(a1) = sum(P(s|a1) * V(s))
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Choice Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Value Updating ---
        # No Stage 1 update needed because it is derived purely from Stage 2
        
        # Standard Q-learning update for Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model (Model-Based + Model-Free + Choice Persistence)
This is a more realistic model (often called the "Daw 2011" style model). It assumes the participant uses a mix of both strategies: they maintain a habit-based value (Model-Free) and a planning-based value (Model-Based) for the first stage. Additionally, it includes a "choice persistence" (or stickiness) parameter, which captures the tendency to simply repeat the previous motor action regardless of reward history.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner with Choice Persistence.
    
    Combines Model-Based (planning) and Model-Free (habit) values for the first 
    stage choice using a weighting parameter (omega). Also includes a 'stickiness' 
    parameter to account for motor perseveration (repeating the last choice).

    Parameters:
    - learning_rate: [0, 1] Update rate for Q-values.
    - beta: [0, 10] Inverse temperature.
    - omega: [0, 1] Weighting: 1 = Pure Model-Based, 0 = Pure Model-Free.
    - stickiness: [0, 5] Bonus added to the previously chosen action.
    """
    learning_rate, beta, omega, stickiness = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Track previous choice for stickiness
    prev_choice = -1 

    for trial in range(n_trials):

        # --- Stage 1 Choice Policy ---
        # 1. Calculate Model-Based Values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Integrate MB and MF values
        q_net = (omega * q_stage1_mb) + ((1 - omega) * q_stage1_mf)
        
        # 3. Add stickiness bonus
        stick_vector = np.zeros(2)
        if prev_choice != -1:
            stick_vector[prev_choice] = stickiness
            
        # 4. Softmax
        exp_q1 = np.exp(beta * q_net + stick_vector)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Update prev_choice for next trial
        prev_choice = action_1[trial]
        
        state_idx = state[trial]

        # --- Stage 2 Choice Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Value Updating ---
        # TD(0) update for Stage 1 (Model-Free component)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Prediction Error update for Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```