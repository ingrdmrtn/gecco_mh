Here are three cognitive models based on the provided template, ranging from a purely Model-Based approach to a hybrid Model-Based/Model-Free reinforcement learning model.

### Model 1: Pure Model-Based Learning
This model assumes the participant builds an internal model of the task structure (transition probabilities) and values the first-stage choices based *only* on the expected value of the second-stage states. It ignores direct reinforcement of the first-stage action (no Model-Free component at stage 1), relying entirely on planning.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Based Reinforcement Learning Model.
    
    This model assumes the agent calculates the value of stage 1 actions solely by 
    planning through the transition matrix to the learned values of stage 2 states.
    It does not maintain a separate cached value (Q-MF) for stage 1 actions.
    
    Parameters:
    - learning_rate: [0, 1] Rate at which stage 2 values are updated.
    - beta: [0, 10] Inverse temperature for softmax choice (exploration/exploitation).
    """
    learning_rate, beta = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition matrix as described in task (A->X/Y, U->X/Y)
    # Rows: Actions (0, 1), Cols: States (0, 1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values for the second stage (2 states x 2 aliens)
    q_stage2_mf = np.zeros((2, 2)) # Initialized at 0.5 (neutral) or 0

    for trial in range(n_trials):

        # --- STAGE 1 POLICY (Model-Based Planning) ---
        # Calculate max value available in each state (greedy over aliens)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        
        # Plan: Value of action = Transition Prob * Value of Next State
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- STAGE 2 POLICY (Simple Q-Learning) ---
        # Standard softmax over the two aliens in the current state
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- UPDATING ---
        # Note: In a pure MB model, we do not update a Q-MF for stage 1.
        # We only update the values of the aliens (Stage 2).
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Pure Model-Free TD(1) Learning
This model ignores the transition structure of the task entirely. It treats the first-stage choice as a simple bandit problem where the outcome is the eventual reward obtained at the end of the trial. This is often called "direct reinforcement" or TD(1), as the reward signal propagates directly back to the first choice without considering the intermediate state.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free TD(1) Learning.
    
    This model ignores the transition structure (common/rare). It updates the 
    value of the first-stage action based directly on the final reward received,
    bypassing the value of the second-stage state.
    
    Parameters:
    - learning_rate: [0, 1] Rate at which Q-values are updated.
    - beta: [0, 10] Inverse temperature for softmax choice.
    - eligibility: [0, 1] (Lambda) How much credit the first stage gets for the reward. 
                   Here used as a weight for the direct update.
    """
    learning_rate, beta, eligibility = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Model-Free Q-values
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- STAGE 1 POLICY (Model-Free) ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- STAGE 2 POLICY (Model-Free) ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- UPDATING ---
        
        # Stage 2 update (Standard Q-learning)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Stage 1 update: TD(1) / Direct Reinforcement
        # The first stage action is updated based on the final reward, 
        # scaled by 'eligibility' trace parameter.
        delta_stage1_direct = reward[trial] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * eligibility * delta_stage1_direct
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model-Based / Model-Free (Daw et al. 2011 style)
This is the classic "Two-Step" model. It assumes the agent computes both a Model-Based value (planning via transitions) and a Model-Free value (learning from direct experience) for the first stage. The final decision is a weighted combination of these two systems, controlled by a mixing parameter `w`.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Reinforcement Learning.
    
    The agent calculates a weighted average of Model-Based (planning) and 
    Model-Free (TD-learning) values to make the first-stage decision.
    
    Parameters:
    - learning_rate: [0, 1] Update rate for Q-values.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight. 1 = Pure Model-Based, 0 = Pure Model-Free.
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- STAGE 1 POLICY (Hybrid) ---
        # 1. Model-Based Value calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Combine MB and MF values
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- STAGE 2 POLICY ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- UPDATING ---
        
        # Update Stage 1 MF value using TD(0) - prediction error from Stage 2 value
        # This represents the immediate value of the state transition
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Update Stage 2 value based on reward
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Note: In full implementations, there is often a TD(1) trace for stage 1 
        # based on the reward too, but here we stick to the core TD(0) logic for simplicity
        # and to fit parameter constraints.

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```