Here are three cognitive models based on the provided template, ranging from pure model-free learning to a hybrid model-based/model-free approach, and finally a model with distinct learning rates for positive and negative outcomes.

### Model 1: Pure Model-Free TD Learning (SARSA)
This model ignores the transition structure of the task. It learns the value of the first-stage actions (spaceships) based solely on the value of the second-stage state (planets) it actually arrived at, and learns second-stage values based on rewards. It assumes the participant treats the task as a simple chain of associations without a map of the environment.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free Temporal Difference (TD) Learning Model.
    
    This model assumes the participant learns purely through trial and error, 
    chaining values backward from reward to the second stage, and from the 
    second stage to the first stage, without using the transition matrix.
    
    Parameters:
    learning_rate: [0, 1] - Rate at which Q-values are updated (alpha).
    beta: [0, 10] - Inverse temperature for softmax choice rule.
    """
    learning_rate, beta = model_parameters
    n_trials = len(action_1)
  
    # Initialize Q-values
    # q_stage1_mf: Values for Spaceship 0 and 1
    q_stage1_mf = np.zeros(2) 
    # q_stage2_mf: Values for Planet 0 (Aliens 0,1) and Planet 1 (Aliens 0,1)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):

        # --- Policy for the first choice (Stage 1) ---
        # Softmax over model-free Q-values for stage 1
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Transition to state (Planet)
        state_idx = state[trial]

        # --- Policy for the second choice (Stage 2) ---
        # Softmax over model-free Q-values for the specific planet (state_idx)
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Value Updating ---
        
        # TD Error 1: Difference between value of state reached and value of chosen spaceship
        # Note: We use the value of the *chosen* action in stage 2 (SARSA-like update)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # TD Error 2: Difference between reward received and value of chosen alien
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model-Based / Model-Free Learning
This is the classic "Two-Step" model. It posits that the participant combines a Model-Based (MB) strategy (planning using the transition matrix) and a Model-Free (MF) strategy (habitual caching of values). A weighting parameter `w` determines the balance between these two systems for the first-stage choice.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Reinforcement Learning Model.
    
    Combines a planning-based valuation (MB) using the known transition structure
    with a habit-based valuation (MF).
    
    Parameters:
    learning_rate: [0, 1] - Update rate for Q-values.
    beta: [0, 10] - Inverse temperature for softmax.
    w: [0, 1] - Weighting parameter (0 = Pure MF, 1 = Pure MB).
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition matrix as described in task (70% common, 30% rare)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Policy for the first choice ---
        # Model-Based Value: Expected value of next stage's best option weighted by transition probs
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Value: Weighted sum of MB and MF values
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Policy for the second choice ---
        # Standard softmax on stage 2 values
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Value Updating ---
        # MF update for Stage 1 (TD(0))
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # MF update for Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Asymmetric Learning Rate (Pos/Neg) Model
This model is a variation of the pure model-free learner, but it acknowledges that participants often learn differently from positive outcomes (finding gold) versus negative outcomes (finding nothing). It splits the learning rate into `alpha_pos` and `alpha_neg`.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Asymmetric Learning Rate Model (Model-Free).
    
    Assumes the participant updates values differently depending on whether 
    the prediction error is positive (better than expected) or negative 
    (worse than expected).
    
    Parameters:
    alpha_pos: [0, 1] - Learning rate for positive prediction errors.
    alpha_neg: [0, 1] - Learning rate for negative prediction errors.
    beta: [0, 10] - Inverse temperature for softmax.
    """
    alpha_pos, alpha_neg, beta = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Policy for the first choice ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Policy for the second choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Value Updating ---
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        if delta_stage1 > 0:
            q_stage1_mf[action_1[trial]] += alpha_pos * delta_stage1
        else:
            q_stage1_mf[action_1[trial]] += alpha_neg * delta_stage1
            
        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        if delta_stage2 > 0:
            q_stage2_mf[state_idx, action_2[trial]] += alpha_pos * delta_stage2
        else:
            q_stage2_mf[state_idx, action_2[trial]] += alpha_neg * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```