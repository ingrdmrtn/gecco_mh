Here are three cognitive models explaining the participant data, ranging from a pure model-based approach to a hybrid model-based/model-free reinforcement learning agent.

### Model 1: Pure Model-Based Agent
This model assumes the participant builds a mental map of the task structure (transition probabilities) and values the first-stage choices based entirely on the expected value of the second-stage states. It does not use "habitual" (model-free) caching for the first stage.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Based Reinforcement Learning Model.
    
    This agent learns the values of the second-stage aliens (Q-values) via prediction error.
    However, for the first-stage choice, it calculates values purely by planning:
    multiplying the learned second-stage values by the known transition matrix.
    It does not maintain separate Model-Free Q-values for the first stage.

    Parameters:
    learning_rate: [0, 1] - Rate at which second-stage Q-values are updated.
    beta: [0, 10] - Inverse temperature for softmax choice (exploration/exploitation balance).
    """
    learning_rate, beta = model_parameters
    n_trials = len(action_1)
  
    # Known transition matrix: 
    # Row 0: Spaceship 0 (A) -> 70% Planet 0 (X), 30% Planet 1 (Y)
    # Row 1: Spaceship 1 (U) -> 30% Planet 0 (X), 70% Planet 1 (Y)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values for Stage 2: 2 planets x 2 aliens
    q_stage2_mf = np.zeros((2, 2)) + 0.5  # Initialize with neutral expectation

    for trial in range(n_trials):

        # --- STEP 1: PLANNING ---
        # Calculate Model-Based values for Stage 1
        # The agent looks ahead: "If I go to Planet X, what's the best I can get? 
        # If I go to Planet Y, what's the best I can get?"
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        
        # Weighted sum based on transition probabilities
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Softmax policy for Stage 1
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- STEP 2: SECOND STAGE CHOICE ---
        state_idx = state[trial] # Which planet we actually arrived at
        
        # Standard Q-learning choice for the alien
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- LEARNING ---
        # Update Stage 2 Q-values based on reward received
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Note: No Stage 1 update here because this is a pure Model-Based agent 
        # that derives Stage 1 values dynamically from Stage 2 values every trial.

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model-Based / Model-Free Learner
This is the classic "Daw two-step" model. The agent maintains two separate valuation systems for the first stage: a Model-Based system (planning using the transition matrix) and a Model-Free system (learning from direct experience/TD-error). A mixing parameter `w` determines which system controls the decision.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Reinforcement Learning Model.
    
    The agent computes Stage 1 values as a weighted combination of:
    1. Model-Based (MB): Planning via transition matrix.
    2. Model-Free (MF): Caching values via TD-learning.
    
    Parameters:
    learning_rate: [0, 1] - Rate for updating Q-values.
    beta: [0, 10] - Inverse temperature for softmax.
    w: [0, 1] - Mixing weight. 1 = Pure Model-Based, 0 = Pure Model-Free.
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Stage 1 MF values (habitual)
    q_stage1_mf = np.zeros(2) + 0.5
    # Stage 2 MF values (aliens)
    q_stage2_mf = np.zeros((2, 2)) + 0.5

    for trial in range(n_trials):

        # --- STEP 1: HYBRID POLICY ---
        # 1. Calculate Model-Based values (Planning)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Combine MB and MF values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax policy
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- STEP 2: SECOND STAGE CHOICE ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- LEARNING ---
        # Calculate prediction errors
        # TD(0) error for Stage 1: Difference between Stage 2 value and Stage 1 expectation
        # We use the value of the chosen option in stage 2 as the target
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        # TD error for Stage 2: Difference between reward and Stage 2 expectation
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Update values
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        # In a full TD(lambda) model, eligibility traces would connect the reward back to stage 1.
        # Here we use a simplified TD(1) approximation by adding delta_stage2 to stage 1 update
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage2 
        
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Choice Perseveration Model (Model-Free)
This model assumes the participant is purely Model-Free (habitual) but has a specific bias: "Choice Perseveration." They tend to repeat their previous Stage 1 choice regardless of reward, represented by a "stickiness" parameter `perseveration`. This captures motor repetition or cognitive inertia common in these tasks.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Agent with Choice Perseveration (Stickiness).
    
    This agent does not plan (no transition matrix usage). It relies on TD-learning.
    It includes a perseveration parameter that biases the agent to repeat the 
    previous Stage 1 action, independent of reward history.
    
    Parameters:
    learning_rate: [0, 1] - Rate for updating Q-values.
    beta: [0, 10] - Inverse temperature.
    perseveration: [0, 5] - Bonus added to the Q-value of the previously chosen action.
    """
    learning_rate, beta, perseveration = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2) + 0.5
    q_stage2_mf = np.zeros((2, 2)) + 0.5
    
    last_action_1 = -1 # Initialize with no previous action

    for trial in range(n_trials):

        # --- STEP 1: POLICY WITH PERSEVERATION ---
        # Copy Q-values to add temporary bias without corrupting memory
        current_q_stage1 = q_stage1_mf.copy()
        
        # If there was a previous trial, add stickiness bonus to that action
        if last_action_1 != -1:
            current_q_stage1[last_action_1] += perseveration
            
        exp_q1 = np.exp(beta * current_q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Record this action for the next trial's perseveration bonus
        last_action_1 = action_1[trial]
        state_idx = state[trial]

        # --- STEP 2: POLICY ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- LEARNING ---
        # Standard SARSA/Q-learning updates
        
        # Stage 1 Update (TD-error using Stage 2 Q-value)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Stage 2 Update (Reward prediction error)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Direct reinforcement of Stage 1 choice by final reward (TD(1)-like logic)
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```