Here are three cognitive models representing different strategies for the two-step decision task.

### Model 1: Model-Free Reinforcement Learning (TD-0)
This model assumes the participant relies purely on trial-and-error learning (Model-Free). It does not use knowledge of the transition structure (which spaceship goes to which planet). Instead, it updates the value of the first-stage choice based on the value of the second-stage state it arrived at (Temporal Difference learning).

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free Reinforcement Learning (TD-0).
    
    This model assumes the agent learns values for both stages purely through 
    experience (Temporal Difference errors), ignoring the transition structure 
    between stage 1 and stage 2.
    
    Parameters:
    - learning_rate: [0, 1] Rate at which Q-values are updated.
    - beta: [0, 10] Inverse temperature for softmax choice (exploration/exploitation).
    """
    learning_rate, beta = model_parameters
    n_trials = len(action_1)
  
    # Q-values initialization
    # Stage 1: 2 choices (Spaceships)
    q_stage1_mf = np.zeros(2) 
    # Stage 2: 2 states (Planets) x 2 choices (Aliens)
    q_stage2_mf = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Determine state arrived at (Planet)
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Value Updating ---
        # Note: In standard TD(0) for this task, the Stage 1 value is updated 
        # towards the value of the chosen option in Stage 2 (SARSA-style) or 
        # the max value of Stage 2 (Q-learning). Here we use the value of the chosen action.
        
        # Prediction error for Stage 1: (Value of Stage 2 choice) - (Current Value of Stage 1 choice)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Prediction error for Stage 2: (Reward) - (Current Value of Stage 2 choice)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Pure Model-Based Reinforcement Learning
This model assumes the participant is a "planner." They learn the values of the aliens (Stage 2) directly from rewards. However, for the first stage (Spaceships), they calculate values by combining the known transition probabilities (70/30) with the maximum value available at the resulting planets. They do not maintain a separate cached value for Stage 1 actions.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Based Reinforcement Learning.
    
    This model computes Stage 1 values on the fly by combining the fixed 
    transition matrix with the learned values of the Stage 2 options. 
    It assumes the agent perfectly understands the 70/30 transition structure.
    
    Parameters:
    - learning_rate: [0, 1] Rate at which Stage 2 Q-values are updated.
    - beta: [0, 10] Inverse temperature for softmax choice.
    """
    learning_rate, beta = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition matrix: Row 0 -> Spaceship A, Row 1 -> Spaceship U
    # Col 0 -> Planet X, Col 1 -> Planet Y
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Q-values initialization
    # Only Stage 2 values are learned directly from reward
    q_stage2_mf = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice (Model-Based Calculation) ---
        # Value of a planet is the max value of the aliens there
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        
        # Value of spaceship = (Prob -> Planet X * MaxVal X) + (Prob -> Planet Y * MaxVal Y)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Value Updating ---
        # Only Stage 2 is updated via prediction error. Stage 1 is derived.
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model (Model-Based + Model-Free) with Mixing Parameter
This is the classic "Two-Step" model. It assumes the participant uses a weighted combination of both strategies. They maintain Model-Free values (from habits/TD learning) and calculate Model-Based values (from planning). A mixing parameter `w` determines which system dominates the Stage 1 choice.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Reinforcement Learning.
    
    The agent computes both a Model-Based value (using the transition matrix)
    and a Model-Free value (using TD errors) for Stage 1. The final choice 
    is a weighted combination of both.
    
    Parameters:
    - learning_rate: [0, 1] Update rate for Q-values.
    - beta: [0, 10] Inverse temperature for choice.
    - w: [0, 1] Mixing weight. 1 = Pure Model-Based, 0 = Pure Model-Free.
    - lambda_eligibility: [0, 1] Eligibility trace parameter. Determines how much the 
      Stage 2 reward directly updates Stage 1 values (skipping the Stage 2 value step).
    """
    learning_rate, beta, w, lambda_eligibility = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # 1. Calculate Model-Based Value
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Combine MB and MF values
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Value Updating ---
        # 1. Update Stage 1 MF value based on Stage 2 value (TD error)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # 2. Update Stage 2 MF value based on Reward
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # 3. Eligibility Trace Update (Direct reinforcement of Stage 1 choice by Reward)
        # This allows the reward to "leak" back to the first choice scaled by lambda
        q_stage1_mf[action_1[trial]] += learning_rate * lambda_eligibility * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```