Here are three cognitive models for the two-step decision task, ranging from simple model-free reinforcement learning to a complex hybrid model combining model-based and model-free strategies with perseveration.

### Model 1: Pure Model-Free TD Learning (SARSA)
This model assumes the participant does not use knowledge of the transition structure (which spaceship leads to which planet). Instead, it learns the value of spaceships and aliens purely through trial and error using Temporal Difference (TD) learning. It updates the value of the chosen spaceship based on the value of the subsequent state (the chosen alien).

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free TD Learning (SARSA) model.
    This model updates values based on direct experience without using the transition matrix.
    
    Parameters:
    - learning_rate (alpha): [0, 1] Rate at which prediction errors update values.
    - beta: [0, 10] Inverse temperature for softmax choice rule (exploration/exploitation).
    """
    learning_rate, beta = model_parameters
    n_trials = len(action_1)
  
    # Initialize Q-values
    # Stage 1: 2 spaceships
    q_stage1 = np.zeros(2) 
    # Stage 2: 2 planets x 2 aliens
    q_stage2 = np.zeros((2, 2)) 

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        # Store probability of the observed action
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Determine the state (planet) arrived at
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        # Select between the two aliens on the current planet
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        # Store probability of the observed action
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning (TD Updates) ---
        
        # Prediction Error 1: Difference between value of chosen spaceship and value of chosen alien
        # Note: In SARSA, we use the Q-value of the action actually taken in stage 2
        delta_stage1 = q_stage2[state_idx, action_2[trial]] - q_stage1[action_1[trial]]
        q_stage1[action_1[trial]] += learning_rate * delta_stage1
        
        # Prediction Error 2: Difference between reward received and value of chosen alien
        delta_stage2 = reward[trial] - q_stage2[state_idx, action_2[trial]]
        q_stage2[state_idx, action_2[trial]] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model-Based / Model-Free Learning
This model represents the classic "Two-Step" analysis. It assumes the participant uses a mixture of two strategies:
1.  **Model-Based (MB):** Calculates the value of a spaceship by combining the transition probabilities (70/30) with the maximum value available on the destination planets.
2.  **Model-Free (MF):** Learns values simply by what happened previously (TD learning).
A mixing parameter `w` determines the balance between these two systems.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free reinforcement learning model.
    Combines a habitual (MF) system with a goal-directed (MB) planning system.
    
    Parameters:
    - learning_rate: [0, 1] Update rate for Q-values.
    - beta: [0, 10] Inverse temperature for softmax.
    - w: [0, 1] Mixing weight. 1 = Pure Model-Based, 0 = Pure Model-Free.
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition matrix (Space A->Planet X is 0.7, etc.)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Model-Free Q-values
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # 1. Calculate Model-Based values
        # Max value obtainable at each planet
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        # Expected value of spaceship based on transition probs
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Integrate MB and MF values
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        # Standard Q-learning choice for the second stage
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning ---
        # Update Model-Free Stage 1 values (TD(0))
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Update Model-Free Stage 2 values
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model with Choice Perseveration
This model extends the Hybrid MB/MF model by adding a "perseveration" parameter. Humans often have a "stickiness" bias where they tend to repeat the same Stage 1 choice regardless of reward (motor perseveration). This model captures that tendency explicitly.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free model with Choice Perseveration (stickiness).
    Adds a bias to repeat the previous Stage 1 action.
    
    Parameters:
    - learning_rate: [0, 1] Update rate for Q-values.
    - beta: [0, 10] Inverse temperature for softmax.
    - w: [0, 1] Mixing weight (MB vs MF).
    - perseveration: [0, 5] Bonus added to the Q-value of the previously chosen action.
    """
    learning_rate, beta, w, perseveration = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track the previous action (initialize with -1 or handle first trial logic)
    last_action_1 = -1

    for trial in range(n_trials):

        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combine MB and MF values
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Add perseveration bonus if applicable
        if last_action_1 != -1:
            q_net[last_action_1] += perseveration
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Save current action for next trial's perseveration
        last_action_1 = action_1[trial]
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```