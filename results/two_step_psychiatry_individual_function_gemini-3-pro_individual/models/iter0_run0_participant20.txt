Here are three cognitive models designed to explain the participant's behavior in the two-step task.

### Model 1: Pure Model-Based Reinforcement Learning
This model assumes the participant builds a mental map of the task structure (transition probabilities) and plans their first-stage choices by calculating the expected value of the second stage. It ignores direct "model-free" learning for the first stage, relying entirely on the computed values derived from the second-stage estimates.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Based Reinforcement Learning.
    
    This model assumes the agent calculates the value of stage 1 actions
    by combining the known transition matrix with the learned values of stage 2.
    It does not maintain a separate model-free Q-value for stage 1.

    Parameters:
    learning_rate: [0, 1] Rate at which stage 2 Q-values are updated.
    beta: [0, 10] Inverse temperature for softmax choice (exploration/exploitation).
    """
    learning_rate, beta = model_parameters
    n_trials = len(action_1)

    # Fixed transition matrix as per task structure (70% common, 30% rare)
    # Row 0: Spaceship 0 -> [Planet 0, Planet 1]
    # Row 1: Spaceship 1 -> [Planet 0, Planet 1]
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values for stage 2: 2 states (planets) x 2 actions (aliens)
    q_stage2_mf = np.zeros((2, 2)) # Initialize at 0 or 0.5

    for trial in range(n_trials):
        # --- Stage 1 Decision (Model-Based) ---
        # Calculate the max value available at each planet
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        
        # Bellman equation: Q_MB(s1, a1) = sum(P(s2|s1,a1) * max(Q(s2, a2)))
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Softmax policy for stage 1
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        # --- Stage 2 Decision ---
        state_idx = state[trial] # Current planet
        
        # Softmax policy for stage 2 based on current Q-values for this planet
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning ---
        # Only update Stage 2 values based on reward (Model-Free update at the terminal state)
        prediction_error = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * prediction_error

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model-Based / Model-Free RL
This is the classic "Daw model" implementation. It posits that the brain runs two parallel systems: a "habitual" (model-free) system that learns stage 1 values based on direct reward history, and a "goal-directed" (model-based) system that calculates values based on the transition structure. A mixing parameter `w` determines the balance between these two systems during the first-stage choice.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Reinforcement Learning.
    
    This model computes stage 1 Q-values as a weighted sum of Model-Based (MB)
    and Model-Free (MF) estimates.
    
    Parameters:
    learning_rate: [0, 1] Learning rate for value updates.
    beta: [0, 10] Inverse temperature for softmax.
    w: [0, 1] Weighting parameter (0 = Pure MF, 1 = Pure MB).
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)      # Model-free values for spaceships
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for aliens

    for trial in range(n_trials):

        # --- Stage 1 Decision ---
        # 1. Calculate Model-Based values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Integrate MB and MF values
        q_integrated = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_integrated)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        # Update Stage 1 MF value (TD-learning using Stage 2 value as proxy)
        # Note: In full SARSA(lambda), this is often more complex, but this is the standard TD(0) step
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Update Stage 2 MF value (Reward prediction error)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Second step update for Stage 1 (eligibility trace / TD(1) logic often applied here)
        # We add the stage 2 RPE to stage 1 value as well to link reward to the first choice
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Choice Kernel + Model-Free RL
This model assumes the participant is purely model-free (habitual) but is also influenced by simple perseveration (a "choice kernel"). They tend to repeat their previous Stage 1 choice regardless of the outcome, captured by a `stickiness` parameter. This captures motor repetition or cognitive inertia often seen in behavioral data.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Reinforcement Learning with Choice Stickiness (Perseveration).
    
    This model assumes the agent learns purely from reward history but has a bias
    to repeat the previously chosen stage 1 action.
    
    Parameters:
    learning_rate: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    stickiness: [0, 5] Bonus added to the Q-value of the previously chosen action.
    """
    learning_rate, beta, stickiness = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track the previous action (initialize with -1 or handle first trial separately)
    last_action_1 = -1

    for trial in range(n_trials):

        # --- Stage 1 Decision ---
        # Add stickiness bonus to the Q-values before softmax
        q_stage1_with_stickiness = q_stage1_mf.copy()
        if last_action_1 != -1:
            q_stage1_with_stickiness[last_action_1] += stickiness

        exp_q1 = np.exp(beta * q_stage1_with_stickiness)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Store action for next trial's stickiness
        last_action_1 = action_1[trial]
        state_idx = state[trial]

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning (Pure Model-Free) ---
        # Update Stage 1 based on Stage 2 value
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Update Stage 2 based on reward
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Eligibility trace update: pass reward error back to stage 1
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```