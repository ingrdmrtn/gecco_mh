Here are three cognitive models representing different strategies for the two-step task.

### Model 1: Pure Model-Free Reinforcement Learning (TD-Learning)
This model assumes the participant ignores the transition structure (which spaceship leads to which planet) and learns purely from experience. It uses Temporal Difference (TD) learning to update values. The first-stage values are updated based on the second-stage values (TD(0)), treating the transition as a simple chain of events.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free Learner (TD-Learning).
    
    This model assumes the participant learns the value of actions solely through 
    prediction errors (Temporal Difference learning), ignoring the transition matrix structure.
    It updates stage 1 values based on the value of the state reached in stage 2.

    Parameters:
    learning_rate: [0,1] - Rate at which value estimates are updated (alpha).
    beta_1: [0,10] - Inverse temperature for stage 1 softmax (exploration/exploitation).
    beta_2: [0,10] - Inverse temperature for stage 2 softmax.
    """
    learning_rate, beta_1, beta_2 = model_parameters
    n_trials = len(action_1)
  
    # Initialize values
    # Stage 1: 2 actions (Spaceships A, U)
    q_stage1_mf = np.zeros(2)  
    # Stage 2: 2 states (Planets X, Y) x 2 actions (Aliens)
    q_stage2_mf = np.zeros((2, 2)) 

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Standard Softmax on Model-Free Q-values
        exp_q1 = np.exp(beta_1 * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Determine which planet (state) was reached
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        # Softmax on Stage 2 Q-values for the specific planet reached
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning Updates ---
        
        # Prediction Error 1: Difference between the value of the state reached (max Q at stage 2) 
        # and the value of the action taken at stage 1.
        # Note: Using max(Q_stage2) is Q-learning; using Q_stage2[chosen] is SARSA. 
        # We use the value of the chosen action here (SARSA-like) to chain values.
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Prediction Error 2: Difference between reward received and value of action taken at stage 2.
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Pure Model-Based Reinforcement Learning
This model assumes the participant calculates values by planning forward. They track the values of the aliens (Stage 2) but choose spaceships (Stage 1) by combining those values with the known transition probabilities (70/30). They do not maintain a separate cached value for the spaceships; they re-calculate it every time.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Based Learner.
    
    This model computes the value of stage 1 actions by projecting the expected 
    value of stage 2 states through the transition matrix. It does not cache 
    stage 1 values (no model-free Q-values for stage 1).

    Parameters:
    learning_rate: [0,1] - Rate for updating stage 2 values.
    beta: [0,10] - Inverse temperature for both stages (assumed shared for simplicity).
    """
    learning_rate, beta = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition matrix: [P(Planet X|Space A), P(Planet Y|Space A)]
    #                          [P(Planet X|Space U), P(Planet Y|Space U)]
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Only Stage 2 Q-values are learned directly
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):

        # --- Stage 1 Choice (Model-Based Planning) ---
        # Find the best value available on each planet
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        
        # Calculate Model-Based value: Transition Probability * Value of Best Option on Planet
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning Updates ---
        # No Stage 1 update because values are computed on the fly.
        
        # Stage 2 update based on reward
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model (Model-Based + Model-Free + Perseveration)
This is the classic "Two-Step Task" model (often called the Daw model). It assumes the participant uses a weighted combination of Model-Free (habitual) and Model-Based (planning) values to make the first choice. It also includes a "perseveration" parameter, capturing the tendency to simply repeat the last action regardless of value.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner with Perseveration.
    
    Combines Model-Free (MF) and Model-Based (MB) value estimation for the first stage.
    Also includes a perseveration bonus to capture "stickiness" of choices.
    
    Parameters:
    learning_rate: [0,1] - Update rate for Q-values.
    beta: [0,10] - Inverse temperature.
    w: [0,1] - Weighting parameter (0 = Pure MF, 1 = Pure MB).
    perseveration: [0,1] - Bonus added to the previously chosen action (choice stickiness).
    """
    learning_rate, beta, w, perseveration = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track previous choice for perseveration
    last_action_1 = -1 

    for trial in range(n_trials):

        # --- Stage 1 Choice ---
        # 1. Model-Based Value Calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Hybrid Value Calculation
        q_hybrid = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # 3. Add Perseveration Bonus
        # Create a vector of zeros, add bonus to the index of the last action
        pers_bonus = np.zeros(2)
        if last_action_1 != -1:
            pers_bonus[last_action_1] = perseveration
            
        # Combine for Softmax
        exp_q1 = np.exp(beta * (q_hybrid + pers_bonus))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        last_action_1 = action_1[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning Updates ---
        
        # MF Update for Stage 1 (TD-Learning)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # MF Update for Stage 2 (Reward Prediction Error)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```