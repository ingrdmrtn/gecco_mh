Here are three cognitive models representing different strategies for the two-step task.

### Model 1: Hybrid Model-Based/Model-Free Reinforcement Learning
This is the classic "Daw et al. (2011)" style model. It assumes the participant uses a mixture of two strategies:
1.  **Model-Free (MF):** Learning from direct experience (TD learning). If a choice led to a reward, do it again.
2.  **Model-Based (MB):** Using knowledge of the transition structure (70/30 probabilities) to plan. If a rare transition leads to a reward, the MB agent realizes the *other* spaceship is actually the better bet to reach that good state again.

This model includes a mixing weight `w` to arbitrate between these two systems.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Reinforcement Learning Model.
    
    This model assumes the agent uses a weighted combination of model-based (planning using 
    transition probabilities) and model-free (temporal difference learning) values to make 
    decisions at the first stage. The second stage is purely model-free.

    Parameters:
    - learning_rate: Rate at which Q-values are updated (alpha).
    - beta: Inverse temperature for softmax choice (exploration/exploitation trade-off).
    - w: Mixing weight (0 = pure Model-Free, 1 = pure Model-Based).
    
    Bounds:
    - learning_rate: [0, 1]
    - beta: [0, 10]
    - w: [0, 1]
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition matrix as described in task (70% common, 30% rare)
    # Row 0: Spaceship 0 -> [Planet 0 (0.7), Planet 1 (0.3)]
    # Row 1: Spaceship 1 -> [Planet 0 (0.3), Planet 1 (0.7)]
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)      # Model-free values for spaceships
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for aliens (2 planets x 2 aliens)

    for trial in range(n_trials):
        # --- STAGE 1 CHOICE ---
        # 1. Calculate Model-Based values
        # The value of a spaceship is the expected value of the best action on the resulting planets
        # weighted by the transition probability.
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Combine MB and MF values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # 3. Softmax policy for stage 1
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Determine actual state (planet) reached
        state_idx = state[trial]

        # --- STAGE 2 CHOICE ---
        # Standard Softmax on the Q-values for the specific planet reached
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- LEARNING / UPDATING ---
        # 1. Update Stage 1 MF values (TD(1) logic roughly, using stage 2 Q-value as proxy)
        # Note: In full TD(lambda), we might use eligibility traces. Here we use a simpler
        # SARSA-like update or direct update from the second stage value.
        # A common simplification is updating Q1 based on Q2 of the chosen action.
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # 2. Update Stage 2 MF values based on reward
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Pure Model-Free with Eligibility Traces (TD(Î»))
This model assumes the participant does *not* use a map of the task (no transition matrix). Instead, they rely entirely on trial-and-error. However, it adds an `eligibility_lambda` parameter. This parameter allows the reward received at the *end* of the trial (Step 2) to directly reinforce the choice made at the *beginning* (Step 1), bridging the temporal gap.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free TD(lambda) Learning Model.
    
    This model assumes the agent relies solely on temporal difference learning.
    It includes an eligibility trace parameter (lambda) which determines how much 
    the reward at the second stage reinforces the choice at the first stage directly.
    
    Parameters:
    - learning_rate: Rate at which Q-values are updated.
    - beta: Inverse temperature for softmax choice.
    - eligibility_lambda: Decay rate of eligibility traces (0 = TD(0), 1 = Monte Carlo).
    
    Bounds:
    - learning_rate: [0, 1]
    - beta: [0, 10]
    - eligibility_lambda: [0, 1]
    """
    learning_rate, beta, eligibility_lambda = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- STAGE 1 CHOICE ---
        # Pure Model-Free choice
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        chosen_a1 = action_1[trial]

        # --- STAGE 2 CHOICE ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        chosen_a2 = action_2[trial]
        r = reward[trial]

        # --- LEARNING ---
        
        # Prediction Error 1: Difference between Stage 2 value and Stage 1 value
        # This is the "immediate" reward of reaching the state (which is 0) + discounted next state value
        pe_1 = q_stage2_mf[state_idx, chosen_a2] - q_stage1_mf[chosen_a1]
        
        # Prediction Error 2: Difference between Received Reward and Stage 2 value
        pe_2 = r - q_stage2_mf[state_idx, chosen_a2]
        
        # Update Stage 2 Q-values (standard TD)
        q_stage2_mf[state_idx, chosen_a2] += learning_rate * pe_2
        
        # Update Stage 1 Q-values
        # The update includes the immediate prediction error (pe_1)
        # PLUS the eligibility trace of the second stage prediction error (pe_2 * lambda)
        # This allows the final reward to propagate back to the first choice.
        q_stage1_mf[chosen_a1] += learning_rate * (pe_1 + eligibility_lambda * pe_2)
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Perseveration Model (Stickiness)
This model is a Model-Free learner that includes a "perseveration" or "stickiness" parameter. This captures the tendency of participants to simply repeat their last choice (or switch from it) regardless of the reward outcome. This is a common heuristic in human and animal decision-making that often explains variance not captured by value learning alone.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Reinforcement Learning with Choice Perseveration (Stickiness).
    
    This model is a standard Q-learning model but adds a 'stickiness' parameter.
    This parameter biases the agent to repeat (or avoid) the action taken on the 
    immediately preceding trial, independent of reward history.
    
    Parameters:
    - learning_rate: Rate at which Q-values are updated.
    - beta: Inverse temperature for softmax choice.
    - stickiness: Bias added to the previously chosen action (positive = repeat, negative = switch).
    
    Bounds:
    - learning_rate: [0, 1]
    - beta: [0, 10]
    - stickiness: [-5, 5] (Though usually small positive)
    """
    learning_rate, beta, stickiness = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1 # Initialize with a value that doesn't match 0 or 1

    for trial in range(n_trials):

        # --- STAGE 1 CHOICE ---
        # Calculate "net" values: Q-value + Stickiness Bonus
        net_values_1 = q_stage1_mf.copy()
        
        if last_action_1 != -1:
            # Add stickiness bonus to the action taken on the previous trial
            net_values_1[last_action_1] += stickiness

        exp_q1 = np.exp(beta * net_values_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Record action for next trial's stickiness
        last_action_1 = action_1[trial] 
        state_idx = state[trial]

        # --- STAGE 2 CHOICE ---
        # (Stickiness is usually only modeled on the first stage choice in this task, 
        # as stage 2 states change, but standard Q-learning here)
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- LEARNING ---
        # Standard SARSA/Q-learning updates
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```