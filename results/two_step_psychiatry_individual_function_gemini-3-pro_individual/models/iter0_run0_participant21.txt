Here are three cognitive models based on the provided template and task description.

### Model 1: Pure Model-Based Reinforcement Learning
This model assumes the participant builds a mental model of the task structure (transition probabilities) and plans their first-stage choices by calculating the expected value of the second stage. It ignores direct "Model-Free" learning for the first stage, relying entirely on the computed values from the second stage.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Based RL Model.
    
    This model assumes the agent learns the values of the second-stage aliens (Model-Free)
    but makes first-stage decisions solely by planning through the transition matrix (Model-Based).
    It does not maintain a separate Model-Free Q-value for the first stage.

    Parameters:
    learning_rate: [0, 1] - Rate at which second-stage Q-values are updated.
    beta: [0, 10] - Inverse temperature for softmax choice stochasticity.
    """
    learning_rate, beta = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition matrix as per task description (A->X common, U->Y common)
    # Rows: Spaceships (0, 1), Cols: Planets (0, 1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values for Stage 2: 2 planets x 2 aliens
    q_stage2_mf = np.zeros((2, 2)) # initialized at 0 or 0.5 (neutral)

    for trial in range(n_trials):
        # --- Stage 1 Policy (Model-Based) ---
        # Calculate max value available in each state (planet)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        
        # Bellman equation: Q_MB(s1, a1) = sum(P(s2|s1,a1) * max(Q(s2, a2)))
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial] # Actual planet arrived at

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        # No Stage 1 MF update because this is a Pure MB model.
        
        # Stage 2 Update (Standard Q-learning)
        # Prediction Error: R - Q(s2, a2)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model-Based / Model-Free (Dyna-style)
This is the classic "Daw et al. (2011)" style model. It assumes the agent uses a weighted combination of Model-Based planning (calculating expected future value) and Model-Free learning (reinforcing actions that led to reward previously) to make the first-stage decision.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free RL Model.
    
    The agent computes Stage 1 values as a weighted sum of Model-Based values 
    (planning via transition matrix) and Model-Free values (TD learning).
    
    Parameters:
    lr: [0, 1] - Learning rate for Q-value updates.
    beta: [0, 10] - Inverse temperature for softmax.
    w: [0, 1] - Mixing weight. 1 = Pure Model-Based, 0 = Pure Model-Free.
    """
    lr, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2) # Model-free values for spaceships
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for aliens

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        # 1. Calculate Model-Based values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Integrate MB and MF values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        # Stage 1 MF Update (TD(0))
        # Note: In some versions, this is SARSA(lambda), but simple TD(0) is used here for simplicity within template
        # The target for stage 1 is the value of the state reached in stage 2 (max Q)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr * delta_stage1
        
        # Stage 2 MF Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Pure Model-Free with Eligibility Traces (TD(1))
This model assumes the participant does not use the transition probabilities (no map of the task). Instead, they learn entirely through trial and error. Crucially, it uses an eligibility parameter (`lambda` implicitly set to 1 here via direct update) to allow the reward at the second stage to directly reinforce the first-stage choice.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free RL with Direct Reward Reinforcement (TD(1)-like logic).
    
    The agent ignores transition probabilities. The reward obtained at the end
    is used to update both the second-stage choice AND the first-stage choice directly.
    This captures the "habitual" behavior where a reward reinforces the entire chain of actions.

    Parameters:
    lr: [0, 1] - Learning rate.
    beta: [0, 10] - Inverse temperature.
    decay: [0, 1] - How much the Stage 2 reward updates Stage 1 (Eligibility trace decay).
    """
    lr, beta, decay = model_parameters
    n_trials = len(action_1)
  
    # No transition matrix needed for Pure MF
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        
        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr * delta_stage2
        
        # Stage 1 Update
        # In simple TD(0), Stage 1 updates based on Stage 2 Value.
        # In TD(1) / Eligibility Trace, Stage 1 updates based on the final Reward.
        # Here we implement a direct update from Reward to Stage 1 Q-value scaled by 'decay'.
        
        # First, standard TD update (transition to state 2)
        # We use the value of the CHOSEN option in stage 2 as the proxy for state value
        delta_stage1_td = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr * delta_stage1_td
        
        # Second, Eligibility Trace update: The reward prediction error from stage 2 propagates back
        q_stage1_mf[action_1[trial]] += lr * decay * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```