Here are three cognitive models designed to explain the participant's behavior in the two-step decision task. They range from a purely model-free learner to a hybrid learner, and finally a model that incorporates perseveration.

### Model 1: Pure Model-Free Learner (TD-Learning)
This model assumes the participant does not use knowledge of the transition structure (the map of spaceships to planets). Instead, they learn the value of spaceships and aliens purely through trial and error using Temporal Difference (TD) learning. They update the value of the chosen spaceship based on the value of the planet they arrived at, regardless of whether the transition was common or rare.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free Learner (TD-Learning).
    This model learns action values solely through reward prediction errors.
    It does not use the transition matrix to calculate Model-Based values.
    
    Parameters:
    learning_rate: [0,1] Rate at which Q-values are updated (alpha).
    beta: [0,10] Inverse temperature for softmax choice rule.
    """
    learning_rate, beta = model_parameters
    n_trials = len(action_1)
  
    # Initialize Q-values
    # q_stage1_mf: Value of choosing spaceship 0 or 1
    # q_stage2_mf: Value of choosing alien 0 or 1 given planet 0 or 1
    q_stage1_mf = np.zeros(2) # Initialized at 0 (or 0.5)
    q_stage2_mf = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Calculate probabilities for the first choice (Softmax on MF values)
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Observe state (planet)
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        # Calculate probabilities for the second choice (Softmax on MF values for current state)
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning ---
        # 1. Update Stage 1 Q-values (TD(0))
        # The 'reward' for stage 1 is the value of the state we arrived at (q_stage2_mf)
        # We use the value of the action actually taken in stage 2 (SARSA-style update)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # 2. Update Stage 2 Q-values
        # The reward is the actual coin received
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model-Based / Model-Free Learner
This is the classic "Daw et al. (2011)" style model. It assumes the participant uses a mixture of two strategies. The **Model-Based (MB)** system calculates the value of a spaceship by looking at the best possible rewards on the planets and weighting them by the known transition probabilities (0.7/0.3). The **Model-Free (MF)** system learns simply from experience. A mixing parameter `w` determines the balance between these two systems.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Learner.
    Combines a model-based evaluation (using the transition matrix) and a 
    model-free evaluation (TD learning) for the first stage choice.
    
    Parameters:
    learning_rate: [0,1] Update rate for Q-values.
    beta: [0,10] Inverse temperature for softmax.
    w: [0,1] Mixing weight. 1 = Pure Model-Based, 0 = Pure Model-Free.
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition matrix as per task description
    # Row 0: Spaceship A -> 70% Planet X (0), 30% Planet Y (1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Choice ---
        # Model-Based Value Calculation:
        # Value of spaceship = P(Planet X)*Max(Val X) + P(Planet Y)*Max(Val Y)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Q-value (Weighted sum of MB and MF)
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax policy
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        # Standard Softmax on Stage 2 values
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        # Update Stage 1 MF value
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Update Stage 2 MF value (used by both MB and MF systems)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Learner with Choice Perseveration
This model extends the Hybrid learner by adding a "stickiness" or perseveration parameter. Participants often have a tendency to repeat their previous Stage 1 choice regardless of reward (motor perseveration). This model adds a bonus to the Q-value of the previously chosen spaceship.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner with Choice Perseveration.
    Includes Model-Based and Model-Free learning, plus a parameter capturing 
    the tendency to repeat the previous stage-1 action ('stickiness').
    
    Parameters:
    learning_rate: [0,1] Update rate.
    beta: [0,10] Inverse temperature.
    w: [0,1] Mixing weight (MB vs MF).
    perseveration: [0,5] Bonus added to the previously chosen action (Stickiness).
    """
    learning_rate, beta, w, perseveration = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track previous choice (initialized to -1 or None, handled in loop)
    last_action_1 = -1

    for trial in range(n_trials):

        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add perseveration bonus
        if last_action_1 != -1:
            q_net[last_action_1] += perseveration

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Update tracker
        last_action_1 = action_1[trial]
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```