Here are three cognitive models based on the two-step reinforcement learning framework, ranging from a simple Model-Free learner to a hybrid Model-Based/Model-Free learner, and finally a model incorporating a perseverance (stickiness) bias.

### Model 1: Pure Model-Free Learner (TD Learning)
This model assumes the participant does not use knowledge of the transition structure (which spaceship goes to which planet). Instead, it learns the value of the first-stage spaceships solely based on the eventual reward, chaining values backward using Temporal Difference (TD) learning. It ignores the "Model-Based" component entirely.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free Reinforcement Learning Model.
    
    This model assumes the agent learns value functions for both stages purely through 
    reward prediction errors (Temporal Difference learning), without using a 
    transition matrix to plan.
    
    Parameters:
    learning_rate: [0,1] - The rate at which Q-values are updated (alpha).
    beta: [0,10] - Inverse temperature for softmax choice rule.
    """
    learning_rate, beta = model_parameters
    n_trials = len(action_1)
  
    # Initialize Q-values
    # Stage 1: 2 actions (Spaceship A, Spaceship U)
    q_stage1_mf = np.zeros(2)  # Initialized to 0 or 0.5 (neutral)
    
    # Stage 2: 2 states (Planet X, Planet Y) x 2 actions (Alien 1, Alien 2)
    q_stage2_mf = np.zeros((2, 2)) # Initialized to 0 or 0.5

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Calculate probabilities using Softmax on Model-Free Q-values
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        # Store probability of the observed action
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Determine which state (planet) was reached
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        # Calculate probabilities for the specific state reached
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        # Store probability of the observed action
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning (TD Updates) ---
        
        # 1. Prediction error for Stage 1:
        # The value of the chosen stage 1 action is updated towards the value 
        # of the state reached in stage 2 (SARSA-style or Q-learning style).
        # Here we use the value of the chosen stage 2 action as the target.
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # 2. Prediction error for Stage 2:
        # The value of the chosen stage 2 action is updated towards the actual reward received.
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model-Based / Model-Free Learner
This is the classic "Two-Step" model. It assumes the participant mixes two strategies: a Model-Free strategy (like Model 1) and a Model-Based strategy that uses the known transition matrix (0.7/0.3) to calculate the expected value of a spaceship based on the current values of the planets. A mixing parameter `w` determines the balance between these two systems.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Reinforcement Learning Model.
    
    This model combines a Model-Free learner (TD) with a Model-Based learner 
    (planning using the transition matrix). A weighting parameter controls the mix.
    
    Parameters:
    learning_rate: [0,1] - Rate of Q-value updating.
    beta: [0,10] - Inverse temperature for softmax.
    w: [0,1] - Weighting parameter (0 = pure MF, 1 = pure MB).
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition matrix as described in task (A->X 0.7, U->Y 0.7)
    # Rows: Actions (A, U), Cols: States (X, Y)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Choice ---
        # Model-Based Value: Expected value of next state (max of Q2) weighted by transitions
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Value: Weighted sum of MB and MF values
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        # Standard Softmax on Stage 2 values
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        # Update Stage 1 MF value (TD-learning)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Update Stage 2 MF value (Reward prediction error)
        # Note: These Q-values are used by BOTH the MF and MB systems (MB uses them for planning)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Learner with Choice Stickiness (Perseverance)
Humans often repeat their previous choice regardless of reward (perseverance) or switch more often than expected. This model extends the Hybrid model by adding a `stickiness` parameter. This parameter adds a "bonus" to the Q-value of the action chosen in the immediately preceding trial.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Model with Choice Stickiness.
    
    Adds a perseverance bonus to the previously chosen action in Stage 1,
    capturing the tendency to repeat (or switch) choices regardless of value.
    
    Parameters:
    learning_rate: [0,1] - Rate of Q-value updating.
    beta: [0,10] - Inverse temperature for softmax.
    w: [0,1] - Weighting parameter (0 = pure MF, 1 = pure MB).
    stickiness: [0,5] - Bonus added to the previously chosen action (positive = repeat, negative = switch).
    """
    learning_rate, beta, w, stickiness = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track the previous action for stickiness (initialize to -1 or None)
    prev_action_1 = -1

    for trial in range(n_trials):

        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Calculate net Q-value
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Add Stickiness Bonus
        # We create a temporary Q-vector for decision making so we don't corrupt the learned values
        q_decision = q_net.copy()
        if prev_action_1 != -1:
            q_decision[prev_action_1] += stickiness

        exp_q1 = np.exp(beta * q_decision)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Update previous action tracker
        prev_action_1 = action_1[trial]
        
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```