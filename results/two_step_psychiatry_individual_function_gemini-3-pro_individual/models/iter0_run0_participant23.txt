Here are three cognitive models based on the provided template, ranging from pure model-free learning to a hybrid model-based/model-free approach, and finally a model incorporating perseveration.

### Model 1: Pure Model-Free TD Learning (SARSA-like)
This model assumes the participant ignores the transition structure of the task (which spaceships go to which planets) and simply learns the value of actions based on direct experience using Temporal Difference (TD) learning. It treats the two stages as a continuous chain of actions and rewards.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free TD Learning Model.
    
    This model assumes the agent learns purely from reward prediction errors
    without using a model of the transition probabilities. It updates values
    using a standard TD(0) approach (SARSA-like).

    Parameters:
    learning_rate: [0, 1] rate at which Q-values are updated.
    beta: [0, 10] inverse temperature for softmax choice rule (exploration/exploitation).
    """
    learning_rate, beta = model_parameters
    n_trials = len(action_1)
  
    # Initialize probabilities and Q-values
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values for Stage 1: 2 actions (Spaceships)
    q_stage1_mf = np.zeros(2) 
    # Q-values for Stage 2: 2 states (Planets) x 2 actions (Aliens)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        
        # --- Stage 1 Choice ---
        # Calculate probabilities for Stage 1 using Softmax on Model-Free Q-values
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Observe state (planet)
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        # Calculate probabilities for Stage 2 using Softmax
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning (TD Updates) ---
        # 1. Prediction error for Stage 1: Value of chosen Stage 2 action - Value of Stage 1 action
        # Note: This effectively uses the Q-value of the action actually taken in stage 2 (SARSA style)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        # Update Stage 1 Q-value
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # 2. Prediction error for Stage 2: Reward received - Value of Stage 2 action
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Update Stage 2 Q-value
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model-Based / Model-Free Learning
This model represents the classic "Daw et al. (2011)" style analysis. It assumes the agent computes values in two ways: a Model-Based (MB) way that uses the transition matrix to plan, and a Model-Free (MF) way that learns from experience. A mixing parameter `w` determines the balance between these two systems during the first-stage choice.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Reinforcement Learning.

    This model combines a Model-Based (MB) planner, which uses the transition matrix,
    with a Model-Free (MF) learner. The stage 1 choice is a weighted combination
    of both valuations.

    Parameters:
    learning_rate: [0, 1] rate at which MF Q-values are updated.
    beta: [0, 10] inverse temperature for softmax.
    w: [0, 1] mixing weight. 1 = Pure Model-Based, 0 = Pure Model-Free.
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition matrix as per task description (70% common, 30% rare)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Choice ---
        # 1. Calculate Model-Based values (Bellman equation)
        # Max value available in each state (planet)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        # Expected value of Stage 1 actions based on transition probs
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Integrate MB and MF values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # 3. Softmax on net values
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        # Standard Softmax on Stage 2 Q-values
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        # TD(1) / SARSA style update for Model-Free system
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model with Choice Perseveration
This model extends the Hybrid model by adding a "perseveration" parameter. Participants often have a tendency to repeat their previous Stage 1 choice regardless of reward (stickiness). This model captures that behavioral tic, which is distinct from learning.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free RL with Choice Perseveration.

    Adds a 'stickiness' parameter to the hybrid model. This accounts for the 
    tendency to repeat the previous stage 1 action regardless of the outcome.

    Parameters:
    learning_rate: [0, 1] rate at which MF Q-values are updated.
    beta: [0, 10] inverse temperature for softmax.
    w: [0, 1] mixing weight. 1 = Pure MB, 0 = Pure MF.
    perseveration: [0, 5] bonus added to the Q-value of the previously chosen action.
    """
    learning_rate, beta, w, perseveration = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track the previous action taken in Stage 1
    last_action_1 = -1 

    for trial in range(n_trials):

        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Calculate Net Q-value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Apply Perseveration Bonus
        # If this isn't the first trial, add bonus to the previously chosen action
        if last_action_1 != -1:
            q_net[last_action_1] += perseveration

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Update last action
        last_action_1 = action_1[trial]
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```