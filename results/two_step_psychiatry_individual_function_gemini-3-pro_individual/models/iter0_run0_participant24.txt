Here are three cognitive models analyzing the participant's data, ranging from pure model-based learning to hybrid reinforcement learning strategies.

### Model 1: Pure Model-Based Learning
This model assumes the participant builds an internal map of the task structure. They learn the value of the aliens (Stage 2) and use the known transition probabilities (Stage 1) to calculate the expected value of each spaceship. They do not use model-free caching for the first stage.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Based Reinforcement Learning Model.
    
    This model assumes the agent calculates the value of stage 1 actions solely 
    by planning forward using the transition matrix and the learned values of 
    stage 2 states. It does not maintain a separate model-free value for stage 1.

    Parameters:
    - learning_rate: Rate at which stage 2 values are updated based on rewards.
    - beta: Inverse temperature for softmax choice (exploration/exploitation balance).
    
    Bounds:
    - learning_rate: [0, 1]
    - beta: [0, 10]
    """
    learning_rate, beta = model_parameters
    n_trials = len(action_1)
  
    # Transition matrix: T[action_1, state]
    # Row 0 (Spaceship A) -> 0.7 to Planet X (0), 0.3 to Planet Y (1)
    # Row 1 (Spaceship U) -> 0.3 to Planet X (0), 0.7 to Planet Y (1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values for Stage 2: 2 planets (states) x 2 aliens (actions)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- STAGE 1 CHOICE (Model-Based) ---
        # Calculate the max value available at each planet
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        
        # Bellman equation: Q_MB(s1, a1) = sum(P(s2|s1,a1) * max(Q(s2, a2)))
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Softmax policy for stage 1
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- STAGE 2 CHOICE ---
        state_idx = state[trial] # Current planet
        
        # Softmax policy for stage 2 based on learned Q-values
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- LEARNING ---
        # Update Stage 2 values based on reward received
        # No Stage 1 update because this is pure MB (values are derived, not stored)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Learner (Model-Based + Model-Free)
This is the classic "Daw two-step" model. It assumes the participant uses a mixture of habits (Model-Free) and planning (Model-Based) to make decisions at the first stage. A mixing parameter `w` determines the balance between these two systems.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Reinforcement Learning Model.
    
    This model assumes the agent's stage 1 choice is a weighted combination 
    of model-free values (TD learning) and model-based values (planning).
    
    Parameters:
    - learning_rate: Rate at which Q-values are updated.
    - beta: Inverse temperature for softmax choice.
    - w: Weighting parameter. 1 = Pure Model-Based, 0 = Pure Model-Free.
    - lambda_eligibility: Eligibility trace parameter (0 to 1) connecting stage 2 reward to stage 1 choice.
    
    Bounds:
    - learning_rate: [0, 1]
    - beta: [0, 10]
    - w: [0, 1]
    - lambda_eligibility: [0, 1]
    """
    learning_rate, beta, w, lambda_eligibility = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)      # Model-free values for spaceships
    q_stage2_mf = np.zeros((2, 2)) # Values for aliens

    for trial in range(n_trials):

        # --- STAGE 1 CHOICE ---
        # 1. Calculate Model-Based values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Integrate with Model-Free values using weight 'w'
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- STAGE 2 CHOICE ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- LEARNING ---
        # Prediction Error 1: Difference between Stage 2 value and Stage 1 expectation
        # Note: We use the value of the chosen stage 2 action (SARSA-like logic for the trace)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Prediction Error 2: Difference between Reward and Stage 2 expectation
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Eligibility Trace: Update Stage 1 again based on the final reward
        q_stage1_mf[action_1[trial]] += learning_rate * lambda_eligibility * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Perseveration Model (Habitual + Stickiness)
This model is primarily Model-Free (habit-based) but adds a "perseveration" parameter. This accounts for the common human tendency to simply repeat the last pressed button (motor stickiness) regardless of the reward outcome or transition structure.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Reinforcement Learning with Choice Perseveration.
    
    This model assumes the agent learns via TD-learning (Model-Free) but has an 
    additional bias to repeat the previously chosen stage 1 action (perseveration),
    regardless of reward history.
    
    Parameters:
    - learning_rate: Rate at which Q-values are updated.
    - beta: Inverse temperature for softmax choice.
    - perseveration: Bonus added to the Q-value of the previously chosen action. 
                     Positive values = stickiness, Negative = alternation.
    
    Bounds:
    - learning_rate: [0, 1]
    - beta: [0, 10]
    - perseveration: [-5, 5] (Arbitrary bounds to allow switching or sticking)
    """
    learning_rate, beta, perseveration = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track the previous action (initialize with -1 or handle first trial separately)
    last_action_1 = -1

    for trial in range(n_trials):

        # --- STAGE 1 CHOICE ---
        # Calculate net values: MF value + Perseveration Bonus
        q_net = q_stage1_mf.copy()
        if last_action_1 != -1:
            q_net[last_action_1] += perseveration
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Update last action for next trial
        last_action_1 = action_1[trial]
        state_idx = state[trial]

        # --- STAGE 2 CHOICE ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- LEARNING ---
        # Standard TD updates (Model-Free)
        
        # TD(0) update for Stage 1
        # Using max of stage 2 for Q-learning style update here
        delta_stage1 = np.max(q_stage2_mf[state_idx]) - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # TD(0) update for Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```