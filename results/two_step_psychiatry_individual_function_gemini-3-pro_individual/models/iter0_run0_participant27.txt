Here are three cognitive models explaining the participant's behavior in the two-step task. These models progress from a pure Model-Free learner to a pure Model-Based learner, and finally a Hybrid learner that combines both strategies.

### Model 1: Pure Model-Free Learner (TD-Learning)
This model assumes the participant does not use knowledge of the transition structure (which spaceship goes to which planet). Instead, they learn the value of the first-stage spaceships based purely on the eventual reward, chaining values backward using Temporal Difference (TD) learning.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free learner using Temporal Difference (TD) learning (SARSA).
    
    This model updates the value of the first-stage action based on the value of the 
    state reached in the second stage, without considering transition probabilities.
    
    Parameters:
    - learning_rate: Rate at which Q-values are updated [0,1].
    - beta: Inverse temperature for softmax choice [0,10].
    """
    learning_rate, beta = model_parameters
    n_trials = len(action_1)
    
    # Q-values for Stage 1 (2 spaceships) and Stage 2 (2 planets x 2 aliens)
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2)) # state (planet) x action (alien)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        curr_state = state[trial]
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[curr_state])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning (TD Updates) ---
        
        # Prediction error for Stage 2: Reward - Q_stage2(state, alien)
        delta_stage2 = reward[trial] - q_stage2[curr_state, action_2[trial]]
        q_stage2[curr_state, action_2[trial]] += learning_rate * delta_stage2
        
        # Prediction error for Stage 1: Q_stage2(state, alien) - Q_stage1(spaceship)
        # Note: This is a SARSA-style update where we use the Q-value of the chosen 2nd stage action
        delta_stage1 = q_stage2[curr_state, action_2[trial]] - q_stage1[action_1[trial]]
        q_stage1[action_1[trial]] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Pure Model-Based Learner
This model assumes the participant explicitly uses the transition structure of the task. They learn the values of the aliens (Stage 2) directly from rewards. However, for Stage 1, they do not learn values directly; instead, they calculate the expected value of a spaceship by combining the known transition probabilities with the maximum value available on the destination planets.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Based learner.
    
    This model calculates Stage 1 values by planning: multiplying the transition matrix
    by the max Q-values of the second stage. It does not cache Stage 1 values.
    
    Parameters:
    - learning_rate: Rate at which Stage 2 Q-values are updated [0,1].
    - beta: Inverse temperature for softmax choice [0,10].
    """
    learning_rate, beta = model_parameters
    n_trials = len(action_1)
    
    # Fixed transition matrix: Row=Spaceship, Col=Planet
    # Spaceship 0 (A) -> Planet 0 (X) w/ 0.7
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Q-values for Stage 2 only. Stage 1 values are computed on the fly.
    q_stage2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice (Model-Based Planning) ---
        # Value of a planet is the max value of aliens on it
        max_q_stage2 = np.max(q_stage2, axis=1)
        
        # Value of spaceship is weighted sum of planet values
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        curr_state = state[trial]
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[curr_state])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning ---
        # Only Stage 2 values are updated from reward
        delta_stage2 = reward[trial] - q_stage2[curr_state, action_2[trial]]
        q_stage2[curr_state, action_2[trial]] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Learner (MB/MF Mixture)
This is the classic "Daw et al. (2011)" style model. The participant maintains both Model-Free (TD) values and Model-Based (Planning) values. Their final decision at Stage 1 is a weighted combination of both systems, controlled by a mixing parameter `w`.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free learner.
    
    The Stage 1 Q-value is a weighted sum of the Model-Based value (planning) and 
    the Model-Free value (TD learning).
    
    Parameters:
    - learning_rate: Rate for updating MF and MB values [0,1].
    - beta: Inverse temperature [0,10].
    - w: Weighting parameter [0,1]. 0 = Pure MF, 1 = Pure MB.
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # MF Q-values
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # Used for both MF and MB components (as terminal values)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice (Hybrid) ---
        
        # 1. Calculate Model-Based values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Combine MB and MF values
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        
        # Stage 1 MF Update (TD(1) style - using reward directly or TD(0) using next state Q)
        # Here we use the standard TD chain: Q1 updates toward Q2
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Stage 2 Update (Used by both systems)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```