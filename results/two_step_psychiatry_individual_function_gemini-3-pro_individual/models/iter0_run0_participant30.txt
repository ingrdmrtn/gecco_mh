Here are three cognitive models based on the two-step decision-making task. Each model hypothesizes a different balance of model-based (planning) and model-free (habitual) learning strategies.

### Model 1: Pure Model-Free TD Learning (SARSA)
This model assumes the participant does not use knowledge of the transition structure (the spaceship-planet probabilities). Instead, they learn the value of the first-stage spaceships solely based on the eventual reward received, chaining values backward using Temporal Difference (TD) learning. It ignores the "model" of the world.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free TD Learning (SARSA)
    
    This model assumes the participant learns values purely through experience 
    without using the transition structure. It updates Stage 1 values based on 
    the value of the state reached in Stage 2 (TD(0)).

    Parameters:
    learning_rate: [0, 1] Rate at which Q-values are updated.
    beta: [0, 10] Inverse temperature for softmax choice (exploration/exploitation).
    """
    learning_rate, beta = model_parameters
    n_trials = len(action_1)
  
    # Initialize Q-values
    # q_stage1: values for Spaceship A (0) and U (1)
    q_stage1 = np.zeros(2) 
    # q_stage2: values for Planet X (0) [Aliens 0,1] and Planet Y (1) [Aliens 0,1]
    q_stage2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Softmax policy based on Stage 1 Q-values
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Determine which state (planet) was reached
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        # Softmax policy based on Stage 2 Q-values for the specific planet
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning (Update Q-values) ---
        
        # Calculate Prediction Error for Stage 1 (TD Error)
        # The 'reward' for Stage 1 is the value of the chosen option in Stage 2
        delta_stage1 = q_stage2[state_idx, action_2[trial]] - q_stage1[action_1[trial]]
        q_stage1[action_1[trial]] += learning_rate * delta_stage1

        # Calculate Prediction Error for Stage 2
        # The reward is the actual coin received
        delta_stage2 = reward[trial] - q_stage2[state_idx, action_2[trial]]
        q_stage2[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Pure Model-Based Learning
This model assumes the participant is a perfect planner. They learn the values of the aliens (Stage 2) directly from rewards. However, when choosing a spaceship (Stage 1), they calculate the expected value by combining the known transition probabilities with the maximum value available on each planet. They do *not* cache Stage 1 values; they re-compute them every time.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Based Learning
    
    This model computes Stage 1 values on the fly by combining the fixed 
    transition matrix with the learned values of the Stage 2 states. It assumes
    the participant understands the structure of the task perfectly.

    Parameters:
    learning_rate: [0, 1] Rate at which Stage 2 Q-values are updated.
    beta: [0, 10] Inverse temperature for softmax choice.
    """
    learning_rate, beta = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition matrix: Row 0=Spaceship A, Row 1=Spaceship U
    # Col 0=Planet X, Col 1=Planet Y
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # We do not need stored Q-values for Stage 1, as they are computed on demand.
    q_stage2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Calculate Model-Based Values: V(planet) = max(Q(planet, alien))
        max_q_stage2 = np.max(q_stage2, axis=1)
        
        # Q_MB(spaceship) = P(planetX|spaceship)*V(planetX) + P(planetY|spaceship)*V(planetY)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning ---
        # We only update Stage 2 values based on reward. Stage 1 values are derived.
        # There is no delta_stage1 update in a pure MB model of this type.
        delta_stage2 = reward[trial] - q_stage2[state_idx, action_2[trial]]
        q_stage2[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model (Model-Based + Model-Free weighting)
This is the classic "Two-Step" analysis model (Daw et al., 2011). It posits that the brain computes *both* a Model-Free value (based on cached history) and a Model-Based value (based on planning) for the first stage. The final choice is a weighted combination of these two valuations, controlled by a mixing parameter `w`.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Learner
    
    Combines a Model-Free TD learner with a Model-Based planner. The Stage 1 
    choice is driven by a weighted sum of Q_MF and Q_MB.

    Parameters:
    learning_rate: [0, 1] Update rate for Q-values.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Mixing weight. 0 = Pure Model-Free, 1 = Pure Model-Based.
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Model-Free values for Stage 1
    q_stage1_mf = np.zeros(2)
    # Shared Stage 2 values (used by both systems)
    q_stage2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # 1. Calculate Model-Based value
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Combine MB and MF values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        # Standard Q-learning choice at the second stage
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning ---
        
        # Update Model-Free Stage 1 value using TD(0)
        # Note: In full implementations, this is often TD(lambda), but TD(0) is used here for simplicity within constraints.
        delta_stage1 = q_stage2[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Update Stage 2 value
        delta_stage2 = reward[trial] - q_stage2[state_idx, action_2[trial]]
        q_stage2[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```