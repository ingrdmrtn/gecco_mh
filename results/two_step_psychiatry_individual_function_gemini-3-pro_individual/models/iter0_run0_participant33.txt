Here are three cognitive models representing different strategies for the two-step decision task. They range from a pure Model-Free learner to a pure Model-Based learner, and finally a hybrid learner that combines both strategies.

### Model 1: Pure Model-Free Learner (TD Learning)
This model assumes the participant does not use knowledge of the transition structure (which spaceship goes to which planet). Instead, it learns the value of the first-stage spaceships solely based on the reward received at the very end of the trial, using Temporal Difference (TD) learning. It "caches" values for actions without planning.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free Learner using SARSA(lambda) style updates (lambda=1 here for direct reinforcement).
    This model ignores the transition structure and learns Q-values for stage 1 based directly
    on the Q-values of the chosen stage 2 option.

    Parameters:
    learning_rate: [0, 1] - How much new information overrides old Q-values.
    beta: [0, 10] - Inverse temperature for softmax choice (exploration vs exploitation).
    """
    learning_rate, beta = model_parameters
    n_trials = len(action_1)
  
    # Q-values for Stage 1: [Spaceship A, Spaceship U]
    q_stage1_mf = np.zeros(2)
    # Q-values for Stage 2: [Planet X (Aliens W, S), Planet Y (Aliens P, H)]
    q_stage2_mf = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # Softmax policy based on Model-Free Q-values
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Observe state transition
        state_idx = state[trial] # 0 for Planet X, 1 for Planet Y

        # --- Stage 2 Decision ---
        # Softmax policy based on Stage 2 Q-values for the specific planet
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning ---
        # Prediction Error 2: Reward vs Expected Value of Alien
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Update Stage 2 Q-value
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Prediction Error 1: Value of chosen Alien vs Value of chosen Spaceship
        # In a pure MF model (TD(1) logic often applied here), the reward propogates back.
        # Here we use the updated Q-value of the second stage as the target for the first stage.
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        # Update Stage 1 Q-value
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Pure Model-Based Learner
This model assumes the participant is fully aware of the transition matrix (70/30 split). It does not maintain a separate "cached" value for the spaceships. Instead, on every trial, it calculates the value of a spaceship by looking ahead: combining the known transition probabilities with the learned values of the aliens on the planets.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Based Learner.
    This model computes Stage 1 values dynamically by multiplying the fixed transition matrix
    with the maximum Q-values available at Stage 2. It does not learn Stage 1 values directly.

    Parameters:
    learning_rate: [0, 1] - Rate of updating Stage 2 (Alien) values.
    beta: [0, 10] - Inverse temperature for softmax choice.
    """
    learning_rate, beta = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition structure: 
    # Row 0 (Spaceship A) -> [0.7 to Planet X, 0.3 to Planet Y]
    # Row 1 (Spaceship U) -> [0.3 to Planet X, 0.7 to Planet Y]
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Q-values for Stage 2 only: [Planet X (Aliens W, S), Planet Y (Aliens P, H)]
    q_stage2_mf = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Decision (Model-Based Planning) ---
        # 1. Find the best value currently available on each planet
        max_q_stage2 = np.max(q_stage2_mf, axis=1) # Shape (2,) for Planet X and Y
        
        # 2. Compute expected value of spaceships using the transition matrix
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 3. Softmax choice
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning ---
        # Only Stage 2 values are updated via reward prediction error
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Note: No Stage 1 update occurs here because MB relies on planning, not cached values.

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Learner (Daw et al. 2011 style)
This is the classic "Two-Step" model. It assumes the participant uses a weighted combination of Model-Free (habitual) and Model-Based (planning) strategies to make the first-stage decision. A parameter `w` controls the balance between the two systems.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Learner.
    Combines cached Q-values (MF) and planned Q-values (MB) for the first stage choice
    using a mixing weight 'w'.

    Parameters:
    learning_rate: [0, 1] - Update rate for Q-values.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - Mixing weight. 0 = Pure MF, 1 = Pure MB.
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # MF values for Stage 1
    q_stage1_mf = np.zeros(2)
    # MF values for Stage 2 (used by both systems)
    q_stage2_mf = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):

        # --- Stage 1 Decision ---
        # 1. Calculate Model-Based values (Planning)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Integrate MB and MF values
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # 3. Softmax choice
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        # 1. Update Stage 1 MF value (SARSA-style TD(0) update)
        # Note: In standard hybrid models, Stage 1 MF is often updated using the Stage 2 Q-value 
        # of the chosen option as the target.
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # 2. Update Stage 2 value
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```