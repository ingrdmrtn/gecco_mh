Here are three cognitive models for the two-step decision task, implementing different reinforcement learning strategies (Model-Free, Model-Based, and a Hybrid).

### Model 1: Pure Model-Free Learner (TD-Learning)
This model assumes the participant ignores the transition structure of the task (the probabilities of spaceships leading to planets). It relies entirely on Temporal Difference (TD) learning, updating the value of the spaceship based on the value of the planet reached, and the value of the planet based on the reward received. It does not calculate `q_stage1_mb`.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free (TD-Learning) Model.
    
    This model updates action values based purely on experienced rewards and 
    temporal difference errors, ignoring the transition matrix structure.
    
    Parameters:
    learning_rate (alpha): [0, 1] Rate at which Q-values are updated.
    beta: [0, 10] Inverse temperature for softmax choice (exploration/exploitation).
    eligibility_trace (lambda): [0, 1] How much credit the first stage action gets for the second stage reward.
    """
    learning_rate, beta, eligibility_trace = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values for Stage 1 (Spaceships 0 and 1)
    q_stage1 = np.zeros(2) 
    # Q-values for Stage 2 (Planets 0 and 1, Aliens 0 and 1)
    q_stage2 = np.zeros((2, 2)) 

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial] # The planet reached (0 or 1)

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        # Prediction error at stage 1: Difference between Stage 2 value and Stage 1 expectation
        # We use the Q-value of the chosen alien as the value of the state
        delta_stage1 = q_stage2[state_idx, action_2[trial]] - q_stage1[action_1[trial]]
        
        # Prediction error at stage 2: Difference between Reward and Stage 2 expectation
        delta_stage2 = reward[trial] - q_stage2[state_idx, action_2[trial]]
        
        # Update Stage 2 Q-values (Standard Rescorla-Wagner)
        q_stage2[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Update Stage 1 Q-values
        # The update includes the stage 1 error, plus a portion of the stage 2 error (eligibility trace)
        q_stage1[action_1[trial]] += learning_rate * (delta_stage1 + eligibility_trace * delta_stage2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Pure Model-Based Learner
This model assumes the participant is fully aware of the transition structure. It does not cache values for the first stage (`q_stage1_mf`). Instead, it dynamically calculates the value of the spaceships at every trial by multiplying the known transition matrix by the current maximum values of the planets (aliens).

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Based Learner.
    
    This model calculates Stage 1 values strictly by projecting the best Stage 2 
    values through the transition matrix. It does not maintain separate MF values for Stage 1.
    
    Parameters:
    learning_rate (alpha): [0, 1] Rate at which Stage 2 Q-values update.
    beta: [0, 10] Inverse temperature for softmax choice.
    """
    learning_rate, beta = model_parameters
    n_trials = len(action_1)
  
    # Assuming standard transition probabilities for the task
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Only tracking Stage 2 values (Planets/Aliens)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy (Model-Based) ---
        # Value of a spaceship is the weighted average of the best values of the planets it leads to
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # Only Stage 2 is updated via prediction error. Stage 1 is derived.
        delta_stage2 = reward[trial] - q_stage2[state_idx, action_2[trial]]
        q_stage2[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model-Based / Model-Free Learner
This is the standard model for this task (often called the Daw et al. 2011 model). It assumes the participant uses a weighted combination of Model-Free (habitual) and Model-Based (goal-directed) values to make the first-stage decision.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Learner.
    
    Combines Model-Based (planning) and Model-Free (caching) values for the 
    first stage choice using a weighting parameter (w).
    
    Parameters:
    learning_rate (alpha): [0, 1] Learning rate for Q-values.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Weight of Model-Based control (0 = Pure MF, 1 = Pure MB).
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)     # Model-free values for spaceships
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for aliens

    for trial in range(n_trials):

        # --- Stage 1 Policy (Hybrid) ---
        # 1. Calculate Model-Based Value
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Combine MB and MF values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # SARSA-style update for Stage 1 MF
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Standard update for Stage 2 MF
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Note: In a full SARSA(lambda) implementation, the stage 1 value would also get an 
        # eligibility trace update from delta_stage2, but simpler hybrid models often 
        # omit the trace to isolate the MB vs MF distinction clearly.

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```