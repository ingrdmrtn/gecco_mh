Here are three cognitive models representing different strategies participants might use in the two-step task.

### Model 1: Model-Based (MB) Learner
This model assumes the participant builds a map of the task structure. Instead of just learning which spaceship is "good" based on past rewards (Model-Free), they learn the values of the final aliens and use the known transition probabilities (70/30) to calculate the expected value of each spaceship. They plan their first step based on where they want to end up.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Model 1: Pure Model-Based (MB) Learner.
    This model assumes the agent calculates the value of stage 1 actions
    by combining the learned values of stage 2 states with the known 
    transition matrix (planning), rather than learning stage 1 values directly from experience.

    Parameters:
    learning_rate: [0, 1] Rate at which stage 2 (alien) values are updated.
    beta: [0, 10] Inverse temperature for softmax choice (exploration/exploitation).
    """
    learning_rate, beta = model_parameters
    n_trials = len(action_1)
  
    # Known transition structure: Row 0 is Spaceship A (0.7 to Planet X/0, 0.3 to Planet Y/1)
    # Row 1 is Spaceship U (0.3 to Planet X/0, 0.7 to Planet Y/1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values for the second stage (aliens): 2 planets x 2 aliens
    q_stage2_mf = np.zeros((2, 2)) # initialized at 0 or 0.5 (neutral)

    for trial in range(n_trials):
        # --- Stage 1 Decision (Model-Based) ---
        # Calculate the max value available at each planet (state)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        
        # Calculate Model-Based Q-values for Stage 1:
        # Q_MB(spaceship) = P(planet|spaceship) * Max_Q(planet)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial] # Actual planet arrived at

        # --- Stage 2 Decision (Model-Free) ---
        # Standard softmax selection among aliens on the current planet
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        # Note: Pure MB does not update a Stage 1 Q-value directly.
        # It only updates the Stage 2 values, which changes Stage 1 choices via the transition matrix.
        
        # Update Stage 2 Q-values (Alien values) based on reward
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Model-Free (MF) Learner with Eligibility Traces
This model represents a habitual, "Model-Free" learner. It does not use the transition probabilities to plan. Instead, it relies on Temporal Difference (TD) learning. Crucially, it uses an eligibility trace parameter (`lambda`), which allows the reward received at the very end to "flow back" and directly update the value of the spaceship chosen at the beginning, reinforcing the full chain of actions.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model 2: Model-Free TD(lambda) Learner.
    This model learns values based purely on reward history without using the transition matrix.
    It uses an eligibility trace (lambda) to connect the final reward back to the first stage choice.

    Parameters:
    learning_rate: [0, 1] Learning rate for value updates.
    beta: [0, 10] Inverse temperature for softmax.
    lam: [0, 1] Eligibility trace decay parameter (0 = TD(0), 1 = Monte Carlo).
    """
    learning_rate, beta, lam = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)      # Values for spaceships
    q_stage2_mf = np.zeros((2, 2)) # Values for aliens

    for trial in range(n_trials):

        # --- Stage 1 Decision ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        # Prediction Error 1: Difference between Stage 2 value and Stage 1 expectation
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        # Update Stage 1 value based on the transition (TD(0) update)
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Prediction Error 2: Difference between Reward and Stage 2 expectation
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Update Stage 2 value
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Eligibility Trace Update:
        # The Stage 1 value is ALSO updated by the Stage 2 prediction error, 
        # scaled by lambda. This connects the reward to the spaceship choice.
        q_stage1_mf[action_1[trial]] += learning_rate * lam * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Learner (Mixture of MB and MF)
This is the classic "Two-Step" model often used in psychiatry. It assumes the participant uses a weighted combination of both strategies. They have a "habit" (MF) system that just repeats what was rewarded, and a "goal-directed" (MB) system that plans based on transitions. The parameter `w` controls the balance: `w=1` is pure Model-Based, `w=0` is pure Model-Free.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model 3: Hybrid MB/MF Learner.
    Combines Model-Based planning and Model-Free reinforcement learning.
    
    Parameters:
    learning_rate: [0, 1] Learning rate for all value updates.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Weighting parameter (0 = Pure MF, 1 = Pure MB).
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)      # Model-Free values for Stage 1
    q_stage2_mf = np.zeros((2, 2)) # Values for Stage 2 (shared by MB and MF)

    for trial in range(n_trials):

        # --- Stage 1 Decision ---
        # 1. Calculate Model-Based values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Integrate MB and MF values using weight w
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        # TD(1) / SARSA-like update for the MF component
        
        # PE1: Transition from Stage 1 to Stage 2
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # PE2: Reward outcome
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Allow the Stage 2 PE to update Stage 1 MF values (assuming lambda=1 for simplicity in this hybrid)
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```