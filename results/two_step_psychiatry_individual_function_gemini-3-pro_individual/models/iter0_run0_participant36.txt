Here are three cognitive models based on the two-step task paradigm. They progress from a pure Model-Free learner to a pure Model-Based learner, and finally a Hybrid learner combining both strategies.

### Model 1: Pure Model-Free (TD Learning)
This model assumes the participant does not use knowledge of the transition structure (which spaceship goes to which planet). Instead, it learns the value of spaceships solely based on the eventual rewards obtained, using Temporal Difference (TD) learning. It updates the first-stage values based on the second-stage values (TD(0)).

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free Learner (TD-0).
    This model learns values for both stages using simple temporal difference updates.
    It does not use the transition matrix to calculate expected values.
    
    Parameters:
    learning_rate (alpha): [0,1] Rate at which Q-values are updated.
    beta: [0,10] Inverse temperature for softmax choice (exploration/exploitation).
    """
    learning_rate, beta = model_parameters
    n_trials = len(action_1)
  
    # Initialize Q-values
    q_stage1_mf = np.zeros(2)      # Values for Spaceships A(0) and U(1)
    q_stage2_mf = np.zeros((2, 2)) # Values for Aliens (Planet 0: 0,1; Planet 1: 0,1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice (Spaceship) ---
        # Softmax policy based on Model-Free Q-values
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial] # Planet arrived at

        # --- Stage 2 Choice (Alien) ---
        # Softmax policy based on Stage 2 Q-values for the specific state
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        # 1. Update Stage 1 Q-value using Stage 2 Q-value (TD error)
        # The value of the chosen spaceship is updated toward the value of the chosen alien
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # 2. Update Stage 2 Q-value using Reward (Prediction error)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Pure Model-Based
This model relies entirely on the structural knowledge of the task. It learns the values of the aliens (Stage 2) directly from rewards. However, for Stage 1 (spaceships), it computes the value by multiplying the transition matrix (probability of reaching a planet) by the best available value on that planet. It ignores direct reinforcement of Stage 1 actions.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Based Learner.
    This model calculates Stage 1 values by projecting Stage 2 values 
    through the known transition matrix.
    
    Parameters:
    learning_rate (alpha): [0,1] Rate at which Stage 2 Q-values are updated.
    beta: [0,10] Inverse temperature for softmax choice.
    """
    learning_rate, beta = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition matrix: Row=Spaceship, Col=Planet
    # A(0) -> X(0) is 0.7, U(1) -> Y(1) is 0.7
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage2_mf = np.zeros((2, 2)) # Model-Free values only needed for Stage 2 (Aliens)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):

        # --- Stage 1 Choice (Spaceship) ---
        # Calculate Model-Based values: Max value of each planet weighted by transition prob
        max_q_stage2 = np.max(q_stage2_mf, axis=1) # Max value available on Planet 0 and Planet 1
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Choice (Alien) ---
        # Standard softmax on learned alien values
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        # Only Stage 2 is updated via prediction error. 
        # Stage 1 is derived dynamically at the start of the next trial.
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model-Based / Model-Free
This is the classic "Daw et al. (2011)" style model. It assumes the brain computes *both* a Model-Free value (from habit/TD learning) and a Model-Based value (from planning) for the first stage. The final decision is a weighted combination of these two valuations, controlled by a mixing parameter `w`.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner (MB/MF Mixture).
    Combines Model-Free (TD) and Model-Based (Planning) values for the first stage choice.
    
    Parameters:
    learning_rate: [0,1] Learning rate for Q-values.
    beta: [0,10] Inverse temperature.
    w: [0,1] Weighting parameter. 0 = Pure MF, 1 = Pure MB.
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)      # Model-Free values for Stage 1
    q_stage2_mf = np.zeros((2, 2)) # Values for Stage 2
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):

        # --- Stage 1 Choice ---
        # 1. Calculate Model-Based Value
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Combine MB and MF values
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        # Update Stage 1 MF value (TD-0 style)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Update Stage 2 value (Standard Rescorla-Wagner)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```