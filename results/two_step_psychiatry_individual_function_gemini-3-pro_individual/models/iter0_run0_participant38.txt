Here are three cognitive models based on the provided template and task description.

### Model 1: Pure Model-Free Reinforcement Learning (TD-Learning)
This model assumes the participant ignores the transition structure (which spaceship goes to which planet) and learns purely from experienced rewards using Temporal Difference (TD) errors. It uses a single learning rate for both stages and an eligibility trace parameter `lambda` to modulate how much the Stage 2 outcome updates Stage 1 values directly.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free TD Learner (Sarsa-lambda).
    
    This model learns action values solely based on reward prediction errors.
    It does not use a transition matrix (model-based planning). It links
    stage 1 and stage 2 via an eligibility trace parameter (lambda).

    Parameters:
    learning_rate: [0, 1] - Rate at which Q-values are updated.
    beta: [0, 10] - Inverse temperature for softmax choice (exploration/exploitation).
    lambd: [0, 1] - Eligibility trace decay (0 = TD(0), 1 = Monte Carlo).
    """
    learning_rate, beta, lambd = model_parameters
    n_trials = len(action_1)
  
    # Q-values initialization
    # Stage 1: 2 actions (Spaceships)
    q_stage1_mf = np.zeros(2) 
    # Stage 2: 2 states (Planets) x 2 actions (Aliens)
    q_stage2_mf = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = action_1[trial]
        p_choice_1[trial] = probs_1[a1]
        
        # Transition to state (planet)
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = action_2[trial]
        p_choice_2[trial] = probs_2[a2]

        # --- Learning Updates ---
        r = reward[trial]
        
        # Prediction error at stage 2 (Reward - Q_stage2)
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        
        # Prediction error at stage 1 (Q_stage2 - Q_stage1)
        # Standard SARSA update uses the Q-value of the chosen second action
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]

        # Update Stage 2 values
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Update Stage 1 values
        # The update includes the immediate stage 1 error plus a portion of the stage 2 error (lambda)
        q_stage1_mf[a1] += learning_rate * (delta_stage1 + lambd * delta_stage2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Pure Model-Based Reinforcement Learning
This model assumes the participant relies entirely on the known transition structure of the task. They learn the values of the aliens (Stage 2) directly from rewards, but they calculate the value of the spaceships (Stage 1) by "planning" forward: multiplying the transition matrix by the best available value at the second stage.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Based Learner.
    
    This model computes Stage 1 values by planning through the transition matrix.
    It learns Stage 2 values from reward, but Stage 1 values are derived analytically
    (Value Iteration / Bellman equation) rather than learned via TD error.

    Parameters:
    learning_rate: [0, 1] - Rate at which Stage 2 Q-values are updated.
    beta: [0, 10] - Inverse temperature for softmax choice.
    """
    learning_rate, beta = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition matrix as described in task: 
    # Row 0 (Spaceship A) -> 0.7 Planet X, 0.3 Planet Y
    # Row 1 (Spaceship U) -> 0.3 Planet X, 0.7 Planet Y
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Q-values
    # Note: We don't store persistent q_stage1_mb, we recompute it every trial.
    q_stage2_mf = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):

        # --- Stage 1 Choice (Model-Based) ---
        # Calculate max value available in each state (planet)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        
        # Bellman equation: V(action1) = sum(P(state|action1) * max(Q(state, action2)))
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = action_1[trial]
        p_choice_1[trial] = probs_1[a1]
        
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = action_2[trial]
        p_choice_2[trial] = probs_2[a2]
  
        # --- Learning Updates ---
        r = reward[trial]
        
        # Only Stage 2 is updated via Learning Rate. Stage 1 is derived.
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model-Based / Model-Free Learner (The "Daw" Model)
This is the classic "Two-Step" model. It assumes the participant uses a weighted combination of Model-Free (habitual) and Model-Based (goal-directed) strategies. A mixing parameter `w` determines the balance. If `w=1`, it is purely Model-Based; if `w=0`, it is purely Model-Free.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Learner.
    
    Combines Q-values from a TD learner (MF) and a transition-matrix planner (MB).
    The final Stage 1 value is a weighted sum: w * Q_MB + (1-w) * Q_MF.

    Parameters:
    learning_rate: [0, 1] - Update rate for Q-values.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - Weighting parameter (0 = Pure MF, 1 = Pure MB).
    lambd: [0, 1] - Eligibility trace for the Model-Free component.
    """
    learning_rate, beta, w, lambd = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # MF Q-values
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # Used by both MF and MB logic for terminal values

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):

        # --- Stage 1 Choice (Hybrid) ---
        # 1. Calculate MB values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Combine MB and MF values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = action_1[trial]
        p_choice_1[trial] = probs_1[a1]
        
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        # At stage 2, MB and MF are identical (just choosing best alien)
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = action_2[trial]
        p_choice_2[trial] = probs_2[a2]
  
        # --- Learning Updates ---
        r = reward[trial]
        
        # Stage 2 RPE
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        
        # Stage 1 RPE (Model-Free only)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        
        # Update Stage 2 values
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Update Stage 1 MF values (using eligibility trace)
        q_stage1_mf[a1] += learning_rate * (delta_stage1 + lambd * delta_stage2)
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```