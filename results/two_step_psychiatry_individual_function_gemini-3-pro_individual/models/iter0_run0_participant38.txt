Here are three cognitive models based on the provided template and task description.

### Model 1: Pure Model-Based Reinforcement Learning
This model assumes the participant builds a mental map of the task structure (transition probabilities) and plans their first-stage choices by calculating the expected value of the second stage. It ignores model-free caching at the first stage, relying entirely on the transition matrix and second-stage values.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Based Reinforcement Learning.
    
    This model assumes the agent uses the known transition matrix to compute 
    Q-values for the first stage based on the max values of the second stage.
    It updates second-stage values using a simple Rescorla-Wagner rule.

    Parameters:
    - learning_rate: Rate at which second-stage Q-values are updated [0, 1].
    - beta: Inverse temperature for softmax choice [0, 10].
    """
    learning_rate, beta = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition matrix as per task description (A->X common, U->Y common)
    # Rows: Actions (0, 1), Cols: States (0, 1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values for Stage 2: 2 states (planets) x 2 aliens
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy (Model-Based) ---
        # Calculate max Q-value available in each state (planet)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        
        # Calculate expected value of each stage 1 action using transition matrix
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial] # Actual planet arrived at

        # --- Stage 2 Policy ---
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updating ---
        # Only update Stage 2 values based on reward.
        # Stage 1 values are derived dynamically in the next loop.
        prediction_error = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * prediction_error

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model-Based / Model-Free (Daw et al. style)
This is the classic "Two-Step" model. It assumes the participant's Stage 1 choice is a weighted combination of Model-Based (planning) and Model-Free (habitual) values. A mixing parameter `w` determines the balance between these two systems.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Reinforcement Learning.
    
    Combines a Model-Free learner (TD-learning) with a Model-Based learner 
    (planning using transition matrix).
    
    Parameters:
    - learning_rate: Update rate for Q-values [0, 1].
    - beta: Inverse temperature for softmax [0, 10].
    - w: Weighting parameter [0, 1]. 
         0 = Pure Model-Free, 1 = Pure Model-Based.
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Model-Free Q-values for Stage 1 (2 actions)
    q_stage1_mf = np.zeros(2)
    # Model-Free Q-values for Stage 2 (2 states x 2 actions)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy (Hybrid) ---
        # 1. Calculate Model-Based values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Integrate with Model-Free values using weight 'w'
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updating (TD Learning) ---
        # Stage 1 prediction error (using Stage 2 value as proxy for reward)
        # Note: We use the value of the CHOSEN option in stage 2 (SARSA-like update for stage 1)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Stage 2 prediction error (using actual reward)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Model-Free with Eligibility Traces (TD(1))
This model ignores the transition structure (Model-Free) but acknowledges the temporal link between the first choice and the final reward. It uses an eligibility trace parameter `lambda` to allow the final reward to directly reinforce the Stage 1 choice, rather than just reinforcing Stage 2.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Reinforcement Learning with Eligibility Traces (TD(lambda)).
    
    This model is purely model-free but allows the reward at the end of the 
    trial to 'drift back' and update the first-stage choice directly via 
    an eligibility trace parameter (lambda_param).
    
    Parameters:
    - learning_rate: Update rate for Q-values [0, 1].
    - beta: Inverse temperature for softmax [0, 10].
    - lambda_param: Eligibility trace decay [0, 1]. 
                    0 = TD(0) (only immediate updates), 1 = Monte Carlo.
    """
    learning_rate, beta, lambda_param = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy (Pure Model-Free) ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updating with Eligibility Trace ---
        
        # 1. Update Stage 1 based on transition to Stage 2 (TD(0) part)
        # Value of the state we landed in (max Q or chosen Q)
        # Here we use the value of the chosen action (SARSA style)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # 2. Update Stage 2 based on Reward
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # 3. Eligibility Trace Update: Apply Stage 2 prediction error to Stage 1 choice
        # This allows the final reward to influence the first choice 
        # scaled by lambda.
        q_stage1_mf[action_1[trial]] += learning_rate * lambda_param * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```