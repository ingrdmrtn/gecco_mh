Here are three cognitive models representing different strategies for the two-step task.

### Model 1: Model-Free Learner (TD Learning)
This model relies purely on temporal difference (TD) learning (SARSA). It does not use a model of the transition structure (the spaceship-planet probabilities). Instead, it learns the value of the first-stage actions based on the value of the state reached in the second stage. This is a classic "habitual" learner.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Model 1: Pure Model-Free Learner (TD-0 / SARSA).
    This model updates values based on direct experience without using the transition structure.
    
    Bounds:
    learning_rate: [0, 1] - Controls how quickly Q-values update.
    beta: [0, 10] - Inverse temperature for softmax choice (exploration/exploitation).
    """
    learning_rate, beta = model_parameters
    n_trials = len(action_1)
  
    # Q-values initialization
    # Stage 1: 2 actions (Spaceships)
    q_stage1_mf = np.zeros(2) 
    # Stage 2: 2 states (Planets) x 2 actions (Aliens)
    q_stage2_mf = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Observe state transition
        s1_choice = action_1[trial]
        state_idx = state[trial] # The planet arrived at

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        s2_choice = action_2[trial]
        r = reward[trial]

        # --- Learning (TD Updates) ---
        
        # Stage 1 Update: Based on the value of the chosen action in Stage 2 (SARSA-like)
        # Prediction Error: (Value of next state/action) - (Value of current action)
        delta_stage1 = q_stage2_mf[state_idx, s2_choice] - q_stage1_mf[s1_choice]
        q_stage1_mf[s1_choice] += learning_rate * delta_stage1

        # Stage 2 Update: Based on the reward received
        delta_stage2 = r - q_stage2_mf[state_idx, s2_choice]
        q_stage2_mf[state_idx, s2_choice] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Model-Based Learner
This model uses the known transition structure (70/30 probabilities) to plan. It learns the values of the second-stage aliens (model-free), but for the first stage, it calculates the expected value of a spaceship by combining the transition probabilities with the maximum values available on the destination planets. It is "goal-directed."

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model 2: Pure Model-Based Learner.
    This model computes Stage 1 values by planning through the transition matrix.
    
    Bounds:
    learning_rate: [0, 1] - Controls update speed for Stage 2 Q-values.
    beta: [0, 10] - Inverse temperature for softmax.
    """
    learning_rate, beta = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition matrix: Row=Action, Col=State
    # Action 0 (A) -> 0.7 to State 0 (X), 0.3 to State 1 (Y)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for the aliens
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):

        # --- Stage 1 Choice (Model-Based) ---
        # Calculate expected value of each spaceship based on max value available on planets
        max_q_stage2 = np.max(q_stage2_mf, axis=1) # Best value on Planet 0, Best value on Planet 1
        q_stage1_mb = transition_matrix @ max_q_stage2 # Planning step
        
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        s2_choice = action_2[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[s2_choice]
  
        # --- Learning ---
        # Note: In a pure MB model, we only update the terminal values (Stage 2) based on reward.
        # Stage 1 values are re-computed every trial via the transition matrix.
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, s2_choice]
        q_stage2_mf[state_idx, s2_choice] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Learner (Model-Based + Model-Free)
This model combines both strategies. It maintains separate Q-values for Model-Free (habitual) and Model-Based (planning) estimates. The final decision at Stage 1 is a weighted combination of both, controlled by a mixing parameter `omega`. This is the standard model often used to analyze behavior in this specific task.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model 3: Hybrid Model-Based / Model-Free Learner.
    Combines habitual (MF) and planning (MB) values using a mixing parameter omega.
    
    Bounds:
    learning_rate: [0, 1] - Update rate for Q-values.
    beta: [0, 10] - Inverse temperature.
    omega: [0, 1] - Weight of Model-Based control (0=Pure MF, 1=Pure MB).
    """
    learning_rate, beta, omega = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)      # Habitual values for spaceships
    q_stage2_mf = np.zeros((2, 2)) # Values for aliens
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):

        # --- Stage 1 Choice (Hybrid) ---
        # 1. Calculate Model-Based values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Combine MF and MB values
        q_net = (omega * q_stage1_mb) + ((1 - omega) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        s1_choice = action_1[trial]
        s2_choice = action_2[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[s2_choice]
  
        # --- Learning ---
        
        # Update Stage 1 MF values (SARSA logic)
        delta_stage1 = q_stage2_mf[state_idx, s2_choice] - q_stage1_mf[s1_choice]
        q_stage1_mf[s1_choice] += learning_rate * delta_stage1
        
        # Update Stage 2 MF values (Reward logic)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, s2_choice]
        q_stage2_mf[state_idx, s2_choice] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```