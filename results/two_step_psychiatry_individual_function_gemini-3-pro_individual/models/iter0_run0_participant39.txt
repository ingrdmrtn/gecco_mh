Here are three cognitive models based on the two-step task structure, ranging from simple model-free reinforcement learning to a hybrid model-based/model-free approach.

### Model 1: Simple Model-Free TD Learning (TD-0)
This model assumes the participant learns purely through temporal difference (TD) errors without building a model of the transition structure. It updates values for the first stage based on the value of the state reached in the second stage, and updates second-stage values based on the reward received. It uses a single learning rate for simplicity.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Standard Model-Free TD-Learning (TD-0) model.
    This model updates action values based on prediction errors at each stage.
    It does not use the transition matrix structure (purely model-free).

    Parameters:
    learning_rate: [0, 1] - Rate at which Q-values are updated.
    beta: [0, 10] - Inverse temperature for softmax choice rule (exploration/exploitation).
    """
    learning_rate, beta = model_parameters
    n_trials = len(action_1)
  
    # Initialize Q-values
    # Stage 1: 2 choices (Spaceships)
    q_stage1_mf = np.zeros(2) 
    # Stage 2: 2 states (Planets) x 2 choices (Aliens)
    q_stage2_mf = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Determine state reached (Planet X=0 or Y=1)
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning / Updates ---
        
        # Stage 1 Prediction Error: The value of the state reached (max Q at stage 2) minus current Q1
        # Note: Standard TD(0) often uses V(s') or Q(s', a'). Here we use Q(s', a') of the chosen action.
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Stage 2 Prediction Error: Reward received minus current Q2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model-Based / Model-Free Learning
This model implements the classic "Daw et al. (2011)" style hybrid architecture. It maintains both a model-free value (learned via TD) and a model-based value (calculated using the transition matrix). A mixing parameter `w` determines the balance between these two systems during the first-stage decision.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Reinforcement Learning.
    Combines a Model-Based (planning) controller with a Model-Free (habitual) controller.
    
    Parameters:
    learning_rate: [0, 1] - Update rate for MF Q-values.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - Weighting parameter (0 = Pure MF, 1 = Pure MB).
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition matrix as per task description
    # Row 0: Spaceship A -> [70% Planet X, 30% Planet Y]
    # Row 1: Spaceship U -> [30% Planet X, 70% Planet Y]
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # Used by both MB and MF for terminal values

    for trial in range(n_trials):

        # --- Stage 1 Choice ---
        # 1. Calculate Model-Based values
        # MB value is the expected value of the next state given the transition matrix
        max_q_stage2 = np.max(q_stage2_mf, axis=1) # Value of the best action at each state
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Integrate MB and MF values
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        # Standard softmax on stage 2 values
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning / Updates ---
        # Note: In this simplified hybrid model, only the MF values are updated via error.
        # The MB system uses the updated stage 2 values for the next trial's planning.
        
        # Stage 1 MF update (SARSA-style update using stage 2 Q-value)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Stage 2 MF update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Model-Free with Choice Perseveration
This model is a variation of the Model-Free learner that accounts for "stickiness" or choice perseveration. It adds a bonus to the Q-value of the previously chosen action at stage 1, regardless of the reward outcome. This is common in this task as participants often repeat choices irrespective of value.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free RL with Choice Perseveration (Stickiness).
    Adds a 'stickiness' parameter that biases the agent to repeat the 
    previous Stage 1 choice, independent of reward history.

    Parameters:
    learning_rate: [0, 1] - Update rate.
    beta: [0, 10] - Inverse temperature.
    perseveration: [0, 5] - Bonus added to the previously chosen action (positive = sticky).
    """
    learning_rate, beta, perseveration = model_parameters
    n_trials = len(action_1)
  
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Track previous choice (initialize as -1 or None)
    last_choice_1 = -1

    for trial in range(n_trials):

        # --- Stage 1 Choice ---
        # Calculate effective Q-values including perseveration bonus
        q_stage1_effective = q_stage1_mf.copy()
        if last_choice_1 != -1:
            q_stage1_effective[last_choice_1] += perseveration
            
        exp_q1 = np.exp(beta * q_stage1_effective)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Record choice for next trial's bonus
        last_choice_1 = action_1[trial]
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning / Updates ---
        
        # Stage 1 Update (TD-0)
        # We use the raw Q-value (without perseveration bonus) for the learning signal
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```