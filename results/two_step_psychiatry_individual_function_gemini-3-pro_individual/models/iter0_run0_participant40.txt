Here are three cognitive models representing different hypotheses about how the participant learns and makes decisions. They range from pure model-free learning to a hybrid model-based/model-free approach, and finally a model that considers perseveration (stickiness).

### Model 1: Pure Model-Free TD Learning (SARSA-like)
This model assumes the participant does not build a map of the task structure (transitions). Instead, they learn the value of spaceships and aliens directly through trial and error using Temporal Difference (TD) learning. They update the value of the chosen spaceship based on the value of the planet they arrived at, and the value of the alien based on the reward received.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free Learner (TD Learning).
    
    This model assumes the participant learns the value of stage 1 actions (spaceships)
    and stage 2 actions (aliens) purely through reward prediction errors, without 
    using a transition model of the task structure.

    Parameters:
    - learning_rate: [0, 1] Rate at which Q-values are updated.
    - beta: [0, 10] Inverse temperature for softmax choice (exploration/exploitation).
    """
    learning_rate, beta = model_parameters
    n_trials = len(action_1)
    
    # Q-values initialization
    # Stage 1: 2 spaceships
    q_stage1_mf = np.zeros(2) 
    # Stage 2: 2 planets x 2 aliens
    q_stage2_mf = np.zeros((2, 2)) 

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # Softmax policy based on Model-Free Q-values only
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Observe state (planet)
        state_idx = state[trial]

        # --- Stage 2 Decision ---
        # Softmax policy for the specific planet
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning Updates ---
        # 1. Update Stage 1 Q-values based on the value of the chosen Stage 2 action (TD(0))
        # Prediction error: (Value of chosen Stage 2 option) - (Value of chosen Stage 1 option)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1

        # 2. Update Stage 2 Q-values based on received reward
        # Prediction error: Reward - (Value of chosen Stage 2 option)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model-Based / Model-Free Learner
This is the classic "Daw et al. (2011)" style model. It assumes the participant uses a mixture of two strategies:
1.  **Model-Free (MF):** Just like Model 1, learning from direct experience.
2.  **Model-Based (MB):** Calculating the value of a spaceship by using the known transition probabilities (70%/30%) combined with the current values of the aliens on the destination planets.
A mixing parameter `w` determines the balance between these two systems.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Learner.
    
    This model combines a habit-based system (MF) with a planning system (MB).
    The MB system calculates Stage 1 values by projecting future rewards through 
    the transition matrix.

    Parameters:
    - learning_rate: [0, 1] Update rate for MF Q-values.
    - beta: [0, 10] Inverse temperature for choice stochasticity.
    - w: [0, 1] Weight parameter. 0 = Pure MF, 1 = Pure MB.
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition matrix as described in task (70% common, 30% rare)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # Used by both systems for terminal values

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        
        # 1. Calculate Model-Based values
        # Maximize over aliens at the second stage to get state values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        # Multiply transition matrix by best values of next states
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Integrate MB and MF values using weight w
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Softmax choice
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Decision ---
        # Standard softmax on stage 2 values
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning Updates ---
        # Update Stage 1 MF values (TD learning)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Update Stage 2 values (Reward prediction error)
        # Note: These updated values are used immediately by the MB system in the next trial
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Learner with Choice Perseveration
This model extends Model 2 by adding a "stickiness" or perseveration parameter. Participants often tend to repeat their previous choice regardless of reward (or switch away from it). This model captures that motor-level repetition bias, which is distinct from value-based learning.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner with Choice Perseveration (Stickiness).
    
    Adds a parameter to account for the tendency to repeat the previous Stage 1 
    choice regardless of the outcome (perseveration).

    Parameters:
    - learning_rate: [0, 1] Update rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] MB/MF weight.
    - stickiness: [0, 5] Bonus added to the Q-value of the previously chosen spaceship.
    """
    learning_rate, beta, w, stickiness = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track the previous action (initialize with -1 or handle first trial logic)
    last_action_1 = -1

    for trial in range(n_trials):

        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combined Value
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Add Stickiness Bonus
        # If it's not the first trial, add bonus to the previously chosen option
        q_net_stick = q_net.copy()
        if last_action_1 != -1:
            q_net_stick[last_action_1] += stickiness

        exp_q1 = np.exp(beta * q_net_stick)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Store action for next trial's stickiness
        last_action_1 = action_1[trial]
        state_idx = state[trial]

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```