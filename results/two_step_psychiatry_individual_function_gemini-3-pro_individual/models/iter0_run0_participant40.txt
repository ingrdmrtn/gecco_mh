Here are three cognitive models representing different hypotheses about how the participant learns and makes decisions. They range from a pure model-free learner to a hybrid learner, and finally a model that incorporates perseverance (habit).

### Model 1: Pure Model-Free Learner (TD Learning)
This model hypothesizes that the participant does not build an internal map of the task (i.e., they ignore the transition probabilities between spaceships and planets). Instead, they rely entirely on Temporal Difference (TD) learning (SARSA/Q-learning) to update values based on direct experience. They update Stage 1 values based on the value of the state reached in Stage 2.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free Learner (TD Learning).
    
    This model assumes the participant learns values purely through trial and error 
    using temporal difference errors, without using a model of the transition structure.
    
    Parameters:
    learning_rate: [0, 1] Rate at which Q-values are updated.
    beta: [0, 10] Inverse temperature for softmax choice (exploration/exploitation).
    """
    learning_rate, beta = model_parameters
    n_trials = len(action_1)
  
    # Initialize Q-values
    # q_stage1_mf: Q-values for the two spaceships (0 and 1)
    q_stage1_mf = np.zeros(2) 
    # q_stage2_mf: Q-values for the two aliens (0 and 1) on each of the two planets (0 and 1)
    q_stage2_mf = np.zeros((2, 2)) 

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # Softmax policy based on Model-Free Q-values
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Observe state (planet) transition
        state_idx = state[trial]

        # --- Stage 2 Decision ---
        # Softmax policy based on Stage 2 Q-values for the current planet
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning / Updating ---
        
        # 1. Update Stage 1 Q-values (TD(0)):
        # The target for Stage 1 is the value of the chosen option in Stage 2
        prediction_error_1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * prediction_error_1
        
        # 2. Update Stage 2 Q-values:
        # The target for Stage 2 is the actual reward received
        prediction_error_2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * prediction_error_2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model-Based / Model-Free Learner
This is the classic "Daw et al. (2011)" style model. It assumes the participant uses a mixture of two strategies:
1.  **Model-Based (MB):** Calculates Stage 1 values by multiplying the known transition matrix by the maximum values available at Stage 2.
2.  **Model-Free (MF):** Learns Stage 1 values from direct experience (TD errors).
A mixing parameter `w` determines the balance between these two systems.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Learner.
    
    Combines a model-based controller (using the transition matrix) and a 
    model-free controller (using TD learning).
    
    Parameters:
    learning_rate: [0, 1] Rate at which Q-values are updated.
    beta: [0, 10] Inverse temperature for softmax choice.
    w: [0, 1] Weighting parameter. 0 = Pure Model-Free, 1 = Pure Model-Based.
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition matrix as per task description
    # Row 0: Spaceship 0 -> [Planet 0 (0.7), Planet 1 (0.3)]
    # Row 1: Spaceship 1 -> [Planet 0 (0.3), Planet 1 (0.7)]
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Decision ---
        # Model-Based Value Calculation: 
        # Expected value = P(State|Action) * Max_Value(State)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Q-value: weighted sum of MB and MF values
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Decision ---
        # Standard Q-learning choice (MB and MF coincide at the second stage)
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning / Updating ---
        
        # Update Stage 1 MF values (TD learning)
        # Note: In the full Daw 2011 model, eligibility traces might be used (lambda),
        # but here we implement simple TD(0) for the MF component as per the prompt structure.
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Update Stage 2 MF values
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Learner with Choice Stickiness (Perseverance)
This model extends the hybrid learner by adding a "stickiness" or perseverance parameter (`p`). This parameter captures the tendency of participants to repeat their previous Stage 1 choice regardless of the reward outcome, a common behavioral phenomenon in this task that is distinct from reinforcement learning.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner with Choice Stickiness (Perseverance).
    
    Adds a 'stickiness' parameter to the hybrid model. This accounts for a 
    tendency to repeat the previous Stage 1 action regardless of reward.
    
    Parameters:
    learning_rate: [0, 1] Rate at which Q-values are updated.
    beta: [0, 10] Inverse temperature for softmax choice.
    w: [0, 1] Weighting parameter. 0 = Pure Model-Free, 1 = Pure Model-Based.
    p: [0, 5] Perseverance parameter (stickiness). Positive values encourage repetition.
    """
    learning_rate, beta, w, p = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track the previous choice for the perseverance bonus
    # Initialize with -1 or handle first trial separately
    last_action_1 = -1 

    for trial in range(n_trials):

        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Calculate net Q-value
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Add perseverance bonus to the logits
        logits = beta * q_net
        if last_action_1 != -1:
            logits[last_action_1] += p
            
        exp_q1 = np.exp(logits)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning / Updating ---
        
        # Update Stage 1 MF
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Update Stage 2 MF
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Update history
        last_action_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```