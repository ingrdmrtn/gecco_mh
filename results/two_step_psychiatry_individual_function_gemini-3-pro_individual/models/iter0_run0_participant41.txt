Here are three cognitive models based on the two-step task paradigm, implementing different strategies for learning and decision-making.

### Model 1: Pure Model-Based Reinforcement Learning
This model assumes the participant builds an internal model of the task structure (transition probabilities) and uses it to plan their first-stage choices. They calculate the expected value of each spaceship by combining the known transition probabilities with the learned values of the aliens on each planet.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Based Reinforcement Learning Model.
    
    This model assumes the agent uses the transition structure of the task to compute 
    values for the first-stage actions (Model-Based). It learns second-stage values 
    via a simple Rescorla-Wagner update.
    
    Parameters:
    - learning_rate: [0, 1] Rate at which second-stage Q-values are updated.
    - beta: [0, 10] Inverse temperature for softmax choice (exploration/exploitation).
    """
    learning_rate, beta = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition matrix as per task description (A->X common, U->Y common)
    # Rows: Action 1 (0 or 1), Cols: State (0 or 1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values for the second stage (2 states x 2 actions)
    q_stage2_mf = np.zeros((2, 2)) # initialized at 0 or 0.5 (neutral)

    for trial in range(n_trials):
        # --- Stage 1 Policy (Model-Based) ---
        # Calculate the max value available at each state (planet)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        
        # Compute Model-Based Q-values for Stage 1: Transition * Value of next states
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Softmax for Stage 1
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        # Standard Softmax based on learned Q-values for the current state
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning Updates ---
        # Update Stage 2 Q-values based on reward received
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Note: In a pure Model-Based agent, Stage 1 values are re-computed every trial 
        # based on the transition matrix and current Stage 2 values, so no TD-update 
        # for a separate q_stage1_mf is strictly necessary/used for choice.

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Pure Model-Free Reinforcement Learning (TD-Learning)
This model assumes the participant relies solely on direct trial-and-error experience. They do not use the knowledge of transition probabilities. Instead, they update the value of the chosen spaceship based on the value of the state they arrived at (TD(0)) or the final reward, ignoring the "common/rare" transition structure.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free Reinforcement Learning Model (TD-Learning).
    
    This model ignores the transition matrix structure. It updates first-stage values 
    based on the value of the state reached (Temporal Difference learning).
    
    Parameters:
    - learning_rate: [0, 1] Rate at which Q-values are updated.
    - beta: [0, 10] Inverse temperature for softmax choice.
    - eligibility_trace: [0, 1] (Lambda) How much the Stage 1 choice is updated by the Stage 2 reward directly.
    """
    learning_rate, beta, eligibility_trace = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy (Model-Free) ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning Updates ---
        
        # Prediction Error 1: Difference between Stage 2 value and Stage 1 expectation
        # (Standard SARSA-style or Q-learning update for stage 1)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Prediction Error 2: Difference between Reward and Stage 2 expectation
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Eligibility Trace update: The Stage 1 action is also reinforced by the Stage 2 prediction error
        # This connects the final reward back to the first choice
        q_stage1_mf[action_1[trial]] += learning_rate * eligibility_trace * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model-Based / Model-Free Learner
This is the classic "Daw two-step" model. It posits that the brain computes both a Model-Based value (using the transition map) and a Model-Free value (using direct experience) for the first-stage choice. The actual decision is a weighted combination of these two systems, controlled by a weighting parameter `w`.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Reinforcement Learning Model.
    
    This model computes a weighted average of Model-Based and Model-Free Q-values 
    to make the first-stage decision.
    
    Parameters:
    - learning_rate: [0, 1] Update rate for Q-values.
    - beta: [0, 10] Inverse temperature for softmax.
    - w: [0, 1] Weighting parameter. 1 = Pure Model-Based, 0 = Pure Model-Free.
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy (Hybrid) ---
        # 1. Calculate Model-Based Values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Combine MB and MF values
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning Updates ---
        
        # Update Model-Free Stage 1 Value (TD0)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Update Stage 2 Value (used by both systems)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Note: A lambda/eligibility trace is often included in the full Daw model 
        # for the MF update, but omitted here to stay within parameter limits and focus on the 'w' mix.
        # The MF system here updates stage 1 via the stage 2 value (TD0).

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```