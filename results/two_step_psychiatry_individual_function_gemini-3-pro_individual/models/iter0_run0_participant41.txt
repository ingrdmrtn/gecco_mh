Here are three cognitive models representing different strategies for the two-step task.

### Model 1: Pure Model-Based Reinforcement Learning
This model assumes the participant builds an internal model of the task structure (the transition probabilities) and uses it to plan their first-stage choices. They calculate the expected value of each spaceship by combining the transition probabilities with the values of the aliens on the planets. It ignores the direct "habitual" reward history of the spaceships themselves.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Based Reinforcement Learning Model.
    
    This model assumes the agent calculates Stage 1 values solely by planning:
    combining the known transition matrix with the learned values of the Stage 2 states.
    It does not maintain a separate model-free value for Stage 1 actions.
    
    Parameters:
    - learning_rate: Rate at which Stage 2 (alien) values are updated [0, 1].
    - beta_1: Inverse temperature for Stage 1 choice (exploration/exploitation) [0, 10].
    - beta_2: Inverse temperature for Stage 2 choice [0, 10].
    
    Bounds:
    learning_rate: [0, 1]
    beta_1: [0, 10]
    beta_2: [0, 10]
    """
    learning_rate, beta_1, beta_2 = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition matrix as described in the task (A->X common, U->Y common)
    # Rows: Action 0 (A), Action 1 (U)
    # Cols: State 0 (Planet X), State 1 (Planet Y)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values for Stage 2 (2 planets, 2 aliens each)
    q_stage2_mf = np.zeros((2, 2)) # initialized at 0.0 (or 0.5)

    for trial in range(n_trials):
        # --- Stage 1 Policy (Model-Based) ---
        # Calculate max value available in each state (planet)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        
        # Bellman equation for Model-Based values: Transition * Value of Next State
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Softmax for Stage 1
        exp_q1 = np.exp(beta_1 * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Policy ---
        state_idx = state[trial]
        
        # Softmax for Stage 2
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Value Updating ---
        # Note: In Pure MB, we do not update Stage 1 values directly via prediction error.
        # We only update Stage 2 values based on reward.
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Pure Model-Free Reinforcement Learning (TD-Learning)
This model assumes the participant relies entirely on trial-and-error (habitual) learning. They update the value of the spaceship chosen based on the value of the planet reached (Temporal Difference learning), and subsequently update the planet/alien value based on the reward. They do not use the transition probabilities to plan.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free Reinforcement Learning (TD-Learning).
    
    This model ignores the transition structure (MB) and learns Stage 1 values
    purely through temporal difference errors from the transitions actually experienced.
    
    Parameters:
    - learning_rate: Rate for updating values [0, 1].
    - beta: Inverse temperature for both Stage 1 and Stage 2 choices [0, 10].
    - eligibility_trace: How much the Stage 1 choice is reinforced by the final reward (lambda) [0, 1].
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    eligibility_trace: [0, 1]
    """
    learning_rate, beta, eligibility_trace = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)      # Values for spaceships
    q_stage2_mf = np.zeros((2, 2)) # Values for aliens

    for trial in range(n_trials):

        # --- Stage 1 Policy (Model-Free) ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial] # Planet arrived at

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Value Updating ---
        
        # Prediction Error 1: Difference between value of state reached (planet) and state left (spaceship)
        # We use the Q-value of the chosen second action as the value of the state reached (SARSA-like)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        # Update Stage 1 value
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Prediction Error 2: Difference between reward and value of alien
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Update Stage 2 value
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Eligibility Trace: The Stage 1 action is also updated by the Stage 2 prediction error
        # This connects the final reward back to the initial choice.
        q_stage1_mf[action_1[trial]] += learning_rate * eligibility_trace * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model-Based / Model-Free (Dyna-style)
This is the classic "Two-Step" model (Daw et al., 2011). It assumes the participant uses a weighted combination of Model-Based (planning) and Model-Free (habit) systems to make the first-stage decision. A mixing parameter `w` determines the balance between the two systems.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Reinforcement Learning.
    
    Combines MB and MF values for the first stage choice using a weighting parameter 'w'.
    Stage 2 is purely Model-Free.
    
    Parameters:
    - learning_rate: Rate for updating MF values [0, 1].
    - beta: Inverse temperature for choices [0, 10].
    - w: Weighting parameter (0 = Pure MF, 1 = Pure MB) [0, 1].
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    w: [0, 1]
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy (Hybrid) ---
        # 1. Calculate Model-Based Values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Combine MB and MF values
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Value Updating ---
        
        # TD Error 1: Value of state reached vs value of spaceship
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # TD Error 2: Reward vs value of alien
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Direct reinforcement of Stage 1 choice by final reward (TD(1) logic for MF path)
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```