Here are three cognitive models designed to explain the participant's behavior in the two-step decision task.

### Model 1: Pure Model-Free Reinforcement Learning (TD-Learning)
This model assumes the participant does not use knowledge of the transition structure (which spaceship goes to which planet). Instead, they learn the value of the spaceships solely based on the reward prediction errors experienced at each stage. This is a standard Temporal Difference (TD) learning agent.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free Reinforcement Learning (TD-Learning).
    The agent updates values based on direct experience without using the transition matrix.
    
    Bounds:
    learning_rate: [0, 1] - How much new information overrides old values.
    beta: [0, 10] - Inverse temperature for softmax choice (exploration/exploitation).
    """
    learning_rate, beta = model_parameters
    n_trials = len(action_1)
    
    # Initialize probabilities and Q-values
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values for Stage 1 (2 spaceships)
    q_stage1_mf = np.zeros(2)  
    # Q-values for Stage 2 (2 planets, 2 aliens each)
    q_stage2_mf = np.zeros((2, 2)) 

    for trial in range(n_trials):
        
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Observe state (planet) transition
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Value Updating (TD Learning) ---
        
        # Prediction error for Stage 1: (Value of Stage 2 choice) - (Value of Stage 1 choice)
        # Note: In pure TD(0), we often use the value of the state we landed in (max or actual choice). 
        # Here we use SARSA-style update using the Q-value of the action actually taken in stage 2.
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Prediction error for Stage 2: (Reward) - (Value of Stage 2 choice)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Pure Model-Based Reinforcement Learning
This model assumes the participant relies entirely on the structure of the task. They learn the values of the aliens (Stage 2) directly from rewards. However, to decide which spaceship to pick (Stage 1), they calculate the expected value by multiplying the alien values by the known transition probabilities (70/30). They do *not* cache Stage 1 values based on prediction errors.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Based Reinforcement Learning.
    The agent calculates Stage 1 values by planning through the transition matrix.
    
    Bounds:
    learning_rate: [0, 1] - Learning rate for Stage 2 (alien) values.
    beta: [0, 10] - Inverse temperature for softmax choice.
    """
    learning_rate, beta = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition matrix: Row 0 -> Planet 0 (0.7), Planet 1 (0.3)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Only Stage 2 Q-values are learned directly from reward
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Choice (Model-Based Planning) ---
        # Calculate the value of the best option available at each planet
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        
        # Compute Stage 1 values by weighting Stage 2 max values by transition probabilities
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Value Updating ---
        
        # No Stage 1 update needed (values are computed on the fly next trial)
        
        # Update Stage 2 values based on reward
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model-Based / Model-Free (Daw et al. 2011 style)
This is the classic "Two-Step" model. It assumes the participant uses a weighted combination of Model-Free (habitual) and Model-Based (goal-directed) systems. A mixing parameter `w` determines the balance: `w=1` is pure Model-Based, `w=0` is pure Model-Free.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Reinforcement Learning.
    Combines cached values (MF) and planned values (MB) using a mixing weight 'w'.
    
    Bounds:
    learning_rate: [0, 1] - Learning rate for value updates.
    beta: [0, 10] - Inverse temperature for softmax choice.
    w: [0, 1] - Weighting parameter (0 = Pure MF, 1 = Pure MB).
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Choice (Hybrid) ---
        
        # 1. Calculate Model-Based values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Integrate MB and MF values
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        # Stage 2 is purely model-free (there is no subsequent step to plan for)
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Value Updating ---
        
        # Update Stage 1 MF values (TD prediction error)
        # Note: Using the value of the chosen Stage 2 action as the target
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Update Stage 2 MF values (Reward prediction error)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```