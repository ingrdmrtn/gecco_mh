Here are three cognitive models based on the two-step task framework, ranging from simple model-free learning to hybrid model-based/model-free reinforcement learning.

### Model 1: Pure Model-Free Temporal Difference Learning (TD-0)
This model assumes the participant does not use knowledge of the transition structure (which spaceship goes to which planet). Instead, it learns the value of spaceships and aliens directly from reward prediction errors. It uses a simple TD(0) algorithm where the value of the first-stage choice is updated by the value of the second-stage state.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free TD Learner (TD-0).
    
    This model learns values solely through reward prediction errors. It updates
    Stage 1 values based on the value of the state reached in Stage 2, and 
    Stage 2 values based on the reward received. It ignores transition probabilities.

    Parameters:
    - learning_rate: [0, 1] Rate at which Q-values are updated.
    - beta: [0, 10] Inverse temperature for softmax choice (exploration/exploitation).
    """
    learning_rate, beta = model_parameters
    n_trials = len(action_1)
  
    # Q-values
    # Stage 1: 2 spaceships (A, U)
    q_stage1 = np.zeros(2) 
    # Stage 2: 2 planets (X, Y) x 2 aliens
    q_stage2 = np.zeros((2, 2)) 

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Softmax policy for spaceship choice
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Determine which planet was reached
        state_idx = state[trial] # 0 or 1

        # --- Stage 2 Choice ---
        # Softmax policy for alien choice within the specific planet
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning ---
        
        # 1. Update Stage 1 (Spaceship) value
        # The 'reward' for stage 1 is the value of the state reached (Q-value of chosen alien)
        # Standard TD(0): V(s1) <- V(s1) + alpha * (V(s2) - V(s1))
        # Here we use the Q-value of the chosen action in stage 2 as the proxy for V(s2) (SARSA-like)
        prediction_error_1 = q_stage2[state_idx, action_2[trial]] - q_stage1[action_1[trial]]
        q_stage1[action_1[trial]] += learning_rate * prediction_error_1
        
        # 2. Update Stage 2 (Alien) value
        # The reward is the actual coin received
        prediction_error_2 = reward[trial] - q_stage2[state_idx, action_2[trial]]
        q_stage2[state_idx, action_2[trial]] += learning_rate * prediction_error_2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model-Based / Model-Free Learner
This is the classic "Daw et al. (2011)" style model. It assumes the participant uses a mixture of two strategies: a Model-Free (MF) strategy that learns from habit/experience, and a Model-Based (MB) strategy that plans forward using the known transition matrix (70/30). A weighting parameter `w` determines the balance between these two systems during the first-stage choice.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Learner.
    
    Combines a habitual (Model-Free) system with a planning (Model-Based) system.
    The MB system calculates Stage 1 values by multiplying Stage 2 values by the 
    transition matrix. The MF system learns Stage 1 values via TD errors.
    
    Parameters:
    - learning_rate: [0, 1] Update rate for Q-values.
    - beta: [0, 10] Inverse temperature for softmax.
    - w: [0, 1] Weighting parameter (0 = Pure MF, 1 = Pure MB).
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition matrix: Row 0 -> Planet X (70%), Planet Y (30%); Row 1 -> Flip
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Q-values
    q_stage1_mf = np.zeros(2)       # Model-free values for stage 1
    q_stage2 = np.zeros((2, 2))     # Values for stage 2 (aliens), shared by MB and MF
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):

        # --- Stage 1 Choice ---
        # 1. Model-Based Value Calculation: Bellman equation using max Q from stage 2
        # V(planet) = max(Q(alien 1), Q(alien 2))
        max_q_stage2 = np.max(q_stage2, axis=1) 
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Integrated Value: Weighted sum of MB and MF values
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Softmax on integrated value
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        # Standard softmax on Stage 2 values
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        
        # Update Stage 1 MF values (TD learning)
        # Note: In the standard hybrid model, the MF system often uses TD(1) or TD(lambda).
        # Here we use a simple TD update looking at the Stage 2 value.
        delta_stage1 = q_stage2[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Update Stage 2 values (Reward prediction error)
        delta_stage2 = reward[trial] - q_stage2[state_idx, action_2[trial]]
        q_stage2[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Model-Free with Eligibility Traces (TD-Lambda)
This model represents a more sophisticated habitual learner. Instead of just updating the immediately preceding state, it uses an eligibility trace (parameter `lambda`) to update the first-stage choice based on the final reward outcome. If `lambda` is 1, the reward received at the very end fully updates the spaceship choice (Monte Carlo-like). If `lambda` is 0, it behaves like Model 1.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Learner with Eligibility Traces (TD-Lambda).
    
    This model allows the reward received at the end of the trial to directly 
    influence the value of the first-stage choice, mediated by an eligibility 
    trace parameter lambda. This bridges the temporal gap between choice 1 and reward.

    Parameters:
    - learning_rate: [0, 1] Update rate for Q-values.
    - beta: [0, 10] Inverse temperature.
    - lambd: [0, 1] Eligibility trace decay (0 = TD(0), 1 = Monte Carlo).
    """
    learning_rate, beta, lambd = model_parameters
    n_trials = len(action_1)
  
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):

        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning with Eligibility Trace ---
        
        # 1. Calculate Prediction Error at Stage 1 (State 2 value - State 1 value)
        pe_1 = q_stage2[state_idx, action_2[trial]] - q_stage1[action_1[trial]]
        
        # 2. Update Stage 1 value based on immediate transition (TD-0 part)
        q_stage1[action_1[trial]] += learning_rate * pe_1
        
        # 3. Calculate Prediction Error at Stage 2 (Reward - State 2 value)
        pe_2 = reward[trial] - q_stage2[state_idx, action_2[trial]]
        
        # 4. Update Stage 2 value
        q_stage2[state_idx, action_2[trial]] += learning_rate * pe_2
        
        # 5. ELIGIBILITY TRACE UPDATE:
        # Pass the Stage 2 prediction error back to Stage 1, scaled by lambda.
        # This allows the final reward to 'correct' the Stage 1 value directly.
        q_stage1[action_1[trial]] += learning_rate * lambd * pe_2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```