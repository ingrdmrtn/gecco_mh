Here are three cognitive models based on the two-step decision-making task.

### Model 1: Pure Model-Based Reinforcement Learning
This model assumes the participant builds a mental map of the task structure. They learn the values of the aliens (Stage 2) using a simple Rescorla-Wagner rule. However, when making the Stage 1 decision (Spaceship), they do not rely on past rewards directly associated with the spaceship. Instead, they calculate the expected value of each spaceship by combining the known transition probabilities (70/30) with the current values of the planets they lead to. This is a classic "planning" or "goal-directed" agent.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Based Reinforcement Learning agent.
    
    This agent calculates Stage 1 values by planning forward using the fixed 
    transition matrix and the learned Stage 2 values. It does not cache 
    Stage 1 values directly (no Model-Free component at Stage 1).

    Parameters:
    learning_rate: [0,1] - Rate at which Stage 2 (alien) values are updated.
    beta: [0,10] - Inverse temperature for softmax choice stochasticity.
    """
    learning_rate, beta = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition matrix as described in task: A->X (0->0) is common (0.7)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values for Stage 2 (aliens): 2 planets x 2 aliens
    q_stage2_mf = np.zeros((2, 2)) 
    # Note: No q_stage1_mf is used in a pure model-based agent for decision making

    for trial in range(n_trials):
        # --- Stage 1 Decision (Model-Based) ---
        # Value of a planet is the max value of the aliens on it
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        
        # Value of a spaceship is the weighted sum of planet values
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Softmax policy for Stage 1
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Decision ---
        state_idx = state[trial] # Current planet
        
        # Softmax policy for Stage 2
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        # Update Stage 2 values based on reward received
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # No Stage 1 update in this pure MB model; values are re-computed next trial

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Pure Model-Free Reinforcement Learning (TD-Learning)
This model assumes the participant relies entirely on habit / cached values. They do not use the transition probabilities to plan. Instead, they learn the value of spaceships (Stage 1) and aliens (Stage 2) via Temporal Difference (TD) errors. The value of a spaceship is updated based on the value of the planet arrived at (TD(0)), ignoring whether the transition was rare or common. This agent is sensitive to "reward prediction errors" but blind to "state prediction errors."

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free Reinforcement Learning agent (TD-Learning).
    
    This agent learns Stage 1 values by updating them based on the value of the 
    state reached in Stage 2. It ignores transition probabilities (rare vs common).

    Parameters:
    learning_rate: [0,1] - Rate at which values are updated.
    beta: [0,10] - Inverse temperature for softmax choice stochasticity.
    eligibility_trace: [0,1] - How much the Stage 2 reward directly affects Stage 1 choice (TD(1) vs TD(0)).
    """
    learning_rate, beta, eligibility_trace = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)       # Values for spaceships
    q_stage2_mf = np.zeros((2, 2))  # Values for aliens

    for trial in range(n_trials):

        # --- Stage 1 Decision (Model-Free) ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        # 1. Prediction error at Stage 1 (Difference between value of State 2 and Choice 1)
        # We use the value of the chosen action at stage 2 as the target
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # 2. Prediction error at Stage 2 (Difference between Reward and Choice 2)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # 3. Eligibility Trace: Allow the final reward to further update the Stage 1 choice
        q_stage1_mf[action_1[trial]] += learning_rate * eligibility_trace * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model-Based / Model-Free (Dyna-Style)
This is the standard model often used for this specific task (Daw et al., 2011). It posits that the brain uses a mixture of both strategies. The agent calculates a Model-Based value (using the matrix) and a Model-Free value (using TD errors) for the first stage. The final decision variable is a weighted combination of both. A mixing parameter `w` determines if the participant is more "planner" (w=1) or "habitual" (w=0).

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free agent.
    
    Combines a planning strategy (MB) with a cached-value strategy (MF).
    
    Parameters:
    learning_rate: [0,1] - Learning rate for value updates.
    beta: [0,10] - Inverse temperature for softmax.
    w: [0,1] - Weighting parameter (0 = Pure MF, 1 = Pure MB).
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Decision (Hybrid) ---
        # 1. Calculate Model-Based values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Combine MB and MF values
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # 3. Choice probability
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Decision ---
        # Standard value-based choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        # Update Stage 1 MF value (TD-Learning)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Update Stage 2 MF value (Reward Prediction Error)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Note: We also apply the stage 2 error to stage 1 (TD(1) logic common in this task)
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```