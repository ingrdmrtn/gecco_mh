Here are three cognitive models ranging from simple model-free learning to a hybrid model-based/model-free approach, and finally a model with perseveration.

### Model 1: Pure Model-Free TD Learning (SARSA)
This model ignores the transition structure of the task (the planets). It treats the first-stage choice (spaceship) and second-stage choice (alien) as a simple chain of actions learned via Temporal Difference (TD) errors. It assumes the participant learns the value of spaceships based on the value of the aliens they lead to, without building a map of the task.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    A pure Model-Free (TD-learning/SARSA) model.
    It updates values based on prediction errors at both stages.
    
    Parameters:
    learning_rate: [0,1] Rate at which Q-values are updated.
    beta: [0,10] Inverse temperature for softmax choice.
    """
    learning_rate, beta = model_parameters
    n_trials = len(action_1)
  
    # Initialize Q-values
    # Stage 1: 2 spaceships
    q_stage1 = np.zeros(2) 
    # Stage 2: 2 planets x 2 aliens
    q_stage2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Determine state (planet) arrived at
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Value Updating ---
        # Prediction error for stage 1: Difference between value of state 2 (chosen alien) and state 1
        # Note: In standard TD(0), we use the value of the *chosen* next action.
        delta_stage1 = q_stage2[state_idx, action_2[trial]] - q_stage1[action_1[trial]]
        q_stage1[action_1[trial]] += learning_rate * delta_stage1
        
        # Prediction error for stage 2: Difference between reward and state 2
        delta_stage2 = reward[trial] - q_stage2[state_idx, action_2[trial]]
        q_stage2[state_idx, action_2[trial]] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model-Based / Model-Free
This is the classic "Daw et al. (2011)" style model. It assumes the participant makes decisions by combining two systems:
1.  **Model-Free (MF):** Learns from direct experience (like Model 1).
2.  **Model-Based (MB):** Uses the known transition matrix (0.7/0.3) to calculate the expected value of a spaceship based on the current values of the aliens on the connected planets.
A parameter `w` controls the balance between these two systems.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free learner.
    Combines Q-values from a TD learner and a planning (transition-matrix) learner.
    
    Parameters:
    learning_rate: [0,1] Update rate for MF values.
    beta: [0,10] Inverse temperature.
    w: [0,1] Weighting parameter (0 = Pure MF, 1 = Pure MB).
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition matrix as described in prompt (A->X, U->Y commonly)
    # Rows: Spaceships (0, 1), Cols: Planets (0, 1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # Used by both MB and MF for terminal values

    for trial in range(n_trials):

        # --- Stage 1 Choice ---
        # 1. Model-Based Value: V(Spaceship) = P(Planet|Spaceship) * max(Q(Alien|Planet))
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Hybrid Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        # Stage 2 is purely model-free (no further transitions to plan)
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Value Updating ---
        # TD(1) / SARSA-style update for MF values
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Model-Free with Choice Perseveration
This model extends the simple Model-Free learner (Model 1) by adding a "perseveration" parameter. This captures the tendency to repeat the previous action regardless of reward (habitual repetition). This is often observed in behavioral data where participants stick to the same spaceship or alien simply because they pressed it last time.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free learner with Choice Perseveration (stickiness).
    Adds a bonus to the Q-value of the previously chosen action.
    
    Parameters:
    learning_rate: [0,1] Update rate.
    beta: [0,10] Inverse temperature.
    perseveration: [0,5] Bonus added to the previously chosen action (both stages).
    """
    learning_rate, beta, perseveration = model_parameters
    n_trials = len(action_1)
  
    # Q-values
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Track previous choices (initialize with -1 for none)
    last_action_1 = -1
    last_action_2 = -1 # We track last action 2 generally, or could be per state. 
                       # Here we implement simple motor perseveration (last button press).

    for trial in range(n_trials):

        # --- Stage 1 Choice ---
        # Calculate effective Q (Value + Perseveration Bonus)
        q_eff_1 = q_stage1.copy()
        if last_action_1 != -1:
            q_eff_1[last_action_1] += perseveration

        exp_q1 = np.exp(beta * q_eff_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        # Calculate effective Q
        q_eff_2 = q_stage2[state_idx].copy()
        
        # Note: Perseveration is often modeled as 'stickiness' to the specific motor response 
        # or the specific option. Here we apply it to the specific option (Alien) if available.
        if last_action_2 != -1:
             q_eff_2[last_action_2] += perseveration

        exp_q2 = np.exp(beta * q_eff_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Value Updating ---
        delta_stage1 = q_stage2[state_idx, action_2[trial]] - q_stage1[action_1[trial]]
        q_stage1[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2[state_idx, action_2[trial]]
        q_stage2[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Update history
        last_action_1 = action_1[trial]
        last_action_2 = action_2[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```