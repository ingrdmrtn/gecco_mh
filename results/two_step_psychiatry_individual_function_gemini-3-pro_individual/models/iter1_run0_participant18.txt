Here are three new cognitive models based on the two-step task structure, exploring different mechanisms like perseverance, hybrid learning, and separate learning rates for positive/negative outcomes.

### Model 1: Hybrid Model-Based/Model-Free with Perseverance
This model combines model-based planning (using the transition matrix) and model-free reinforcement learning (learning from direct experience) for the first-stage choice. It also includes a "perseverance" parameter, which captures the tendency to repeat the previous choice regardless of the outcome.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Reinforcement Learning Model with Choice Perseverance.
    
    This agent computes first-stage values as a weighted sum of Model-Based (MB) and Model-Free (MF) values.
    It also includes a perseverance bonus that biases the agent to repeat the last chosen action.
    
    Parameters:
    learning_rate: [0, 1] - Rate at which Q-values are updated.
    beta: [0, 10] - Inverse temperature for softmax choice.
    w: [0, 1] - Weighting parameter (0 = Pure MF, 1 = Pure MB).
    perseverance: [0, 5] - Bonus added to the Q-value of the previously chosen action (repetition bias).
    """
    learning_rate, beta, w, perseverance = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2) + 0.5
    q_stage2_mf = np.zeros((2, 2)) + 0.5
    
    last_action_1 = -1 # Initialize with an impossible action index

    for trial in range(n_trials):

        # --- Policy for the first choice ---
        # 1. Calculate Model-Based Value: Transition Matrix * Max Stage 2 Value
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Combine MB and MF values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # 3. Add perseverance bonus if applicable
        if last_action_1 != -1:
            q_net[last_action_1] += perseverance

        # 4. Softmax selection
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial] # Planet arrived at

        # --- Policy for the second choice ---
        # Standard Softmax on Stage 2 Q-values
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # Update Stage 1 MF value using TD-error (SARSA-like update using stage 2 value)
        # Note: We use the value of the state we actually landed in (q_stage2_mf[state_idx, ...])
        # often simplified to just the max value of that state or the value of the choice made.
        # Here we use the value of the choice made in stage 2.
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Update Stage 2 MF value using Reward Prediction Error
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        last_action_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Pure Model-Free TD(1) (Eligibility Traces)
This model ignores the transition structure (Model-Free) but connects the final reward directly back to the first-stage choice using an eligibility trace parameter (lambda). This allows the first-stage choice to be reinforced by the final outcome without needing the intermediate step value to be fully learned first.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free TD(lambda) Agent.
    
    This agent does not use the transition matrix (it is 'model-free').
    Instead of just updating stage 1 based on stage 2's value (TD(0)), it allows the 
    final reward to directly influence the stage 1 update via an eligibility trace parameter (lambda_param).
    
    Parameters:
    learning_rate: [0, 1] - Rate at which Q-values are updated.
    beta: [0, 10] - Inverse temperature for softmax choice.
    lambda_param: [0, 1] - Eligibility trace decay. 
                           0 = Standard TD learning (Stage 1 updates only from Stage 2 value).
                           1 = Monte Carlo-like (Stage 1 updates directly from Reward).
    """
    learning_rate, beta, lambda_param = model_parameters
    n_trials = len(action_1)
  
    # No transition matrix usage here (Pure MF)
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2) + 0.5
    q_stage2_mf = np.zeros((2, 2)) + 0.5

    for trial in range(n_trials):

        # --- Policy for the first choice ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Policy for the second choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # Calculate Stage 2 Prediction Error (RPE)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Calculate Stage 1 Prediction Error (based on Stage 2 Q-value)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        # Update Stage 2
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Update Stage 1
        # The update combines the immediate step error (delta_stage1) 
        # plus a portion of the second stage error (lambda * delta_stage2)
        combined_error = delta_stage1 + lambda_param * delta_stage2
        q_stage1_mf[action_1[trial]] += learning_rate * combined_error

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Asymmetric Learning Rate Model-Based
This is a Model-Based agent (planning using the transition matrix) that updates its internal estimates of the aliens' values (Stage 2) differently depending on whether the outcome was better or worse than expected. This captures "optimism" or "pessimism" biases often seen in clinical populations.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Based Agent with Asymmetric Learning Rates (Positive/Negative).
    
    This agent is purely model-based for stage 1 decision making.
    However, when updating the values of the aliens (Stage 2), it uses different learning rates
    for positive prediction errors (better than expected) and negative prediction errors (worse than expected).
    
    Parameters:
    alpha_pos: [0, 1] - Learning rate for positive prediction errors.
    alpha_neg: [0, 1] - Learning rate for negative prediction errors.
    beta: [0, 10] - Inverse temperature for softmax choice.
    """
    alpha_pos, alpha_neg, beta = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # We only track Stage 2 values because Stage 1 is derived via MB planning
    q_stage2_mf = np.zeros((2, 2)) + 0.5

    for trial in range(n_trials):

        # --- Policy for the first choice ---
        # Pure Model-Based calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Policy for the second choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # Calculate Prediction Error
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Apply asymmetric learning rates
        if delta_stage2 >= 0:
            q_stage2_mf[state_idx, action_2[trial]] += alpha_pos * delta_stage2
        else:
            q_stage2_mf[state_idx, action_2[trial]] += alpha_neg * delta_stage2
            
        # No explicit Stage 1 update needed for pure MB agents as q_stage1 is re-computed every trial

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```