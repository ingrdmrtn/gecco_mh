Here are 3 new cognitive models based on the two-step task structure.

### Model 1: Hybrid Model with Choice Perseveration (Stickiness)
This model extends the standard Hybrid approach by adding a "stickiness" parameter. This accounts for the tendency of participants to repeat their previous Stage 1 choice regardless of reward history (motor perseveration), which is distinct from learning.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free RL with Choice Perseveration (Stickiness).
    
    Adds a 'stickiness' bonus to the action chosen in the previous trial, 
    capturing motor repetition or habit independent of value learning.
    
    Parameters:
    - learning_rate: [0, 1] Update rate for Q-values.
    - beta: [0, 10] Inverse temperature for choice stochasticity.
    - w: [0, 1] Weighting between Model-Based (1) and Model-Free (0).
    - stickiness: [0, 5] Bonus added to the Q-value of the previously chosen action.
    """
    learning_rate, beta, w, stickiness = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track previous choice for stickiness (initialize as None or -1)
    last_action_1 = -1

    for trial in range(n_trials):

        # --- Policy for the first choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combine MB and MF values
        q_integrated = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Add stickiness bonus to the previously selected action
        if last_action_1 != -1:
            q_integrated[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_integrated)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Update last action
        last_action_1 = action_1[trial]
        
        state_idx = state[trial]

        # --- Policy for the second choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # Stage 1 TD error
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Stage 2 TD error
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Note: No eligibility trace (lambda) in this model, purely MB/MF mixture + stickiness.

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Asymmetric Learning Rate (Dual Alpha) Model
This model hypothesizes that the participant learns differently from positive outcomes (finding gold) versus negative outcomes (no gold/loss). It uses separate learning rates for positive and negative prediction errors. For simplicity, this is implemented as a pure Model-Free learner (ignoring the Model-Based component) to isolate the learning rate mechanism.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Asymmetric Learning Rate Model (Dual Alpha).
    
    This model assumes the agent updates value estimates differently depending on 
    whether the outcome was better (positive prediction error) or worse 
    (negative prediction error) than expected. It is a pure Model-Free implementation.
    
    Parameters:
    - alpha_pos: [0, 1] Learning rate for positive prediction errors (delta > 0).
    - alpha_neg: [0, 1] Learning rate for negative prediction errors (delta < 0).
    - beta: [0, 10] Inverse temperature.
    """
    alpha_pos, alpha_neg, beta = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]]) # Unused in pure MF logic but kept for structure
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Policy for the first choice ---
        # Pure Model-Free choice at Stage 1
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Policy for the second choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        current_lr_1 = alpha_pos if delta_stage1 > 0 else alpha_neg
        q_stage1_mf[action_1[trial]] += current_lr_1 * delta_stage1
        
        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        current_lr_2 = alpha_pos if delta_stage2 > 0 else alpha_neg
        q_stage2_mf[state_idx, action_2[trial]] += current_lr_2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Q-Learning with Passive Decay
This model introduces a memory decay mechanism. While chosen actions are updated via reinforcement learning, unchosen actions slowly decay in value (forgetting). This helps explain behavior where participants might re-explore options simply because they have "forgotten" how bad they were, or lose confidence in options they haven't tried in a while.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Q-Learning with Passive Decay.
    
    Standard Q-learning where unchosen actions decay toward zero.
    This simulates forgetting or a return to baseline for unobserved options.
    
    Parameters:
    - learning_rate: [0, 1] Update rate for chosen actions.
    - beta: [0, 10] Inverse temperature.
    - decay_rate: [0, 1] Rate at which unchosen Q-values decay per trial.
    """
    learning_rate, beta, decay_rate = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Policy for the first choice ---
        # Pure Model-Free with decay
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Policy for the second choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        
        # Identify chosen and unchosen indices for Stage 1
        chosen_s1 = action_1[trial]
        unchosen_s1 = 1 - chosen_s1
        
        # Update chosen Stage 1
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[chosen_s1]
        q_stage1_mf[chosen_s1] += learning_rate * delta_stage1
        # Decay unchosen Stage 1
        q_stage1_mf[unchosen_s1] *= (1 - decay_rate)
        
        # Identify chosen and unchosen indices for Stage 2
        chosen_s2 = action_2[trial]
        unchosen_s2 = 1 - chosen_s2
        
        # Update chosen Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, chosen_s2]
        q_stage2_mf[state_idx, chosen_s2] += learning_rate * delta_stage2
        # Decay unchosen Stage 2 (in the current state only)
        q_stage2_mf[state_idx, unchosen_s2] *= (1 - decay_rate)
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```