Here are three new cognitive models based on the two-step task structure.

The first model introduces a hybrid approach (Model-Based + Model-Free), which is the standard in this literature. The second model adds "perseveration" to the choice rule, capturing the tendency to repeat recent actions regardless of reward. The third model explores separate learning rates for positive and negative prediction errors, capturing asymmetry in learning.

### Model 1: Hybrid Model-Based / Model-Free Reinforcement Learning
This model posits that the participant combines two distinct strategies: a model-based strategy (planning using the transition structure) and a model-free strategy (learning from direct experience, TD learning). A mixing parameter `w` controls the balance between them.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Reinforcement Learning.
    
    Combines a Model-Based (planning) evaluation of stage 1 actions with a Model-Free 
    (TD-learning) evaluation. A weighting parameter `w` determines the balance.

    Parameters:
    learning_rate: [0, 1] Update rate for Q-values.
    beta: [0, 10] Inverse temperature for softmax.
    w: [0, 1] Weight for Model-Based control (1 = Pure MB, 0 = Pure MF).
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)       # Model-free values for stage 1 (Spaceships)
    q_stage2_mf = np.zeros((2, 2))  # Model-free values for stage 2 (Aliens)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # 1. Calculate Model-Based value: Transition * Max(Stage 2 values)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Combine MB and MF values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # 3. Softmax
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial] # Planet arrived at

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning Updates ---
        # TD Error 1: Difference between value of state 2 (chosen alien) and state 1
        # Note: We use the value of the chosen stage 2 action as the target for stage 1
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # TD Error 2: Difference between reward and value of state 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Model-Free RL with Choice Perseveration
This model is purely model-free (it ignores the transition structure) but includes a "perseveration" parameter `p`. This parameter captures the tendency to repeat the previous Stage 1 choice, regardless of whether it was rewarded. This is a common heuristic observed in human data.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Reinforcement Learning with Choice Perseveration.
    
    A pure TD-learning model (SARSA-like) that includes a perseveration bonus,
    making the agent more likely to repeat the previous stage 1 action.

    Parameters:
    learning_rate: [0, 1] Update rate for Q-values.
    beta: [0, 10] Inverse temperature.
    perseveration: [-2, 2] Bonus added to the previously chosen stage 1 action. 
                   (Positive = stickiness, Negative = alternation).
    """
    learning_rate, beta, perseveration = model_parameters
    n_trials = len(action_1)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1 # Indicator for previous action

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Add perseveration bonus to Q-values before softmax
        q_net = q_stage1_mf.copy()
        if prev_action_1 != -1:
            q_net[prev_action_1] += perseveration

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning Updates ---
        # Standard TD learning
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Update history
        prev_action_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Asymmetric Learning Rate Model
This model investigates if the participant learns differently from success versus failure. It splits the learning rate into `alpha_pos` (for positive prediction errors) and `alpha_neg` (for negative prediction errors). It uses a simple model-free architecture to isolate this effect.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Asymmetric Learning Rate Model (Model-Free).
    
    Differentiates learning from positive outcomes (better than expected) 
    and negative outcomes (worse than expected).

    Parameters:
    alpha_pos: [0, 1] Learning rate for positive prediction errors (delta > 0).
    alpha_neg: [0, 1] Learning rate for negative prediction errors (delta < 0).
    beta: [0, 10] Inverse temperature.
    """
    alpha_pos, alpha_neg, beta = model_parameters
    n_trials = len(action_1)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning Updates ---
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        if delta_stage1 > 0:
            q_stage1_mf[action_1[trial]] += alpha_pos * delta_stage1
        else:
            q_stage1_mf[action_1[trial]] += alpha_neg * delta_stage1
            
        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        if delta_stage2 > 0:
            q_stage2_mf[state_idx, action_2[trial]] += alpha_pos * delta_stage2
        else:
            q_stage2_mf[state_idx, action_2[trial]] += alpha_neg * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```