Here are three new cognitive models exploring different parameter configurations and mechanisms for the two-step task.

### Model 1: Asymmetric Learning Rates (Positive/Negative)
This model hypothesizes that the participant updates their beliefs differently based on whether the outcome was better than expected (positive prediction error) or worse than expected (negative prediction error). This is a pure Model-Free learner but with nuanced updating.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Learner with Asymmetric Learning Rates.
    
    Distinguishes between learning from positive prediction errors (better than expected)
    and negative prediction errors (worse than expected).
    
    Parameters:
    alpha_pos: [0,1] - Learning rate for positive prediction errors (RPE > 0).
    alpha_neg: [0,1] - Learning rate for negative prediction errors (RPE < 0).
    beta: [0,10] - Inverse temperature for softmax choice.
    """
    alpha_pos, alpha_neg, beta = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice (Pure MF here)
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Second stage update (Reward Prediction Error)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Asymmetric update for stage 2
        if delta_stage2 > 0:
            q_stage2_mf[state_idx, action_2[trial]] += alpha_pos * delta_stage2
        else:
            q_stage2_mf[state_idx, action_2[trial]] += alpha_neg * delta_stage2

        # First stage update (TD Error using updated stage 2 value as proxy for V(s'))
        # Note: Standard TD(0) uses the value of the state arrived at.
        # Here we use the Q-value of the action taken in stage 2 as the value of that state.
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        # Asymmetric update for stage 1
        if delta_stage1 > 0:
            q_stage1_mf[action_1[trial]] += alpha_pos * delta_stage1
        else:
            q_stage1_mf[action_1[trial]] += alpha_neg * delta_stage1
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Learner with Eligibility Traces (TD-Lambda)
This model introduces an eligibility trace parameter (`lambda`). Instead of just updating the first stage based on the second stage's value, the reward received at the very end is allowed to directly influence the first-stage choice, weighted by lambda. This bridges the gap between Model-Free TD(0) and Monte Carlo learning.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based/Model-Free with Eligibility Trace (Lambda).
    
    Uses an eligibility trace to allow the final reward to directly update the 
    first-stage Q-values, modulated by lambda.
    
    Parameters:
    learning_rate: [0,1] - Update rate.
    beta: [0,10] - Inverse temperature.
    w: [0,1] - Weighting parameter (0 = Pure MF, 1 = Pure MB).
    lam: [0,1] - Eligibility trace decay (lambda). 0 = TD(0), 1 = Monte Carlo.
    """
    learning_rate, beta, w, lam = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice (Hybrid MB/MF)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_hybrid = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_hybrid)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Update Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Update Stage 1
        # TD(0) error part: difference between Stage 2 value and Stage 1 expectation
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        # Combined update: 
        # The immediate TD error (delta_stage1) plus the propagated error from stage 2 (delta_stage2) weighted by lambda
        q_stage1_mf[action_1[trial]] += learning_rate * (delta_stage1 + lam * delta_stage2)
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Pure Model-Based with Forgetting
This model assumes the participant is primarily Model-Based (planning using the transition matrix) but has imperfect memory. The Q-values for the second stage (the aliens) decay towards a neutral value (0.5) on every trial. This captures the "slowly changing probabilities" aspect of the task by assuming old information becomes less reliable.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Based Learner with Memory Decay (Forgetting).
    
    This model relies entirely on the transition structure for Stage 1 choices,
    but the values learned in Stage 2 decay over time toward a neutral prior.
    
    Parameters:
    learning_rate: [0,1] - Update rate for Stage 2 values.
    beta: [0,10] - Inverse temperature.
    decay_rate: [0,1] - Rate at which unchosen values decay to 0.5 (1 = no decay, 0 = instant forgetting).
    """
    learning_rate, beta, decay_rate = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Initialize Stage 2 values at 0.5 (neutral probability of reward)
    q_stage2_mf = np.ones((2, 2)) * 0.5

    for trial in range(n_trials):

        # policy for the first choice (Pure MB)
        # We calculate Stage 1 values strictly by projecting Stage 2 values through the transition matrix
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Update chosen action in Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Apply decay to ALL Stage 2 values (forgetting)
        # Values drift towards 0.5
        q_stage2_mf = (q_stage2_mf * decay_rate) + (0.5 * (1 - decay_rate))
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```