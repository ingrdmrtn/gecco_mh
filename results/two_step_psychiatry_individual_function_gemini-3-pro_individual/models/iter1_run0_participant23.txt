Here are three new cognitive models based on the two-step task structure.

### Model 1: Hybrid Model-Based / Model-Free Learning
This model combines both strategies. It assumes the agent maintains a Model-Free (MF) value based on direct experience and a Model-Based (MB) value calculated using the known transition structure. The final choice at Stage 1 is a weighted combination of both.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free learner.
    
    The agent computes Q-values for the first stage using a weighted average of:
    1. Model-Based (MB) values: Derived from the transition matrix and max stage 2 values.
    2. Model-Free (MF) values: Derived from temporal difference errors.
    
    Parameters:
    learning_rate: [0, 1] Update rate for Q-values.
    beta: [0, 10] Inverse temperature for softmax.
    w_mb: [0, 1] Weight of Model-Based control (0 = pure MF, 1 = pure MB).
    """
    learning_rate, beta, w_mb = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition matrix as per task description (A->X mostly, U->Y mostly)
    # 0 -> 0 (0.7), 0 -> 1 (0.3); 1 -> 0 (0.3), 1 -> 1 (0.7)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2) # TD-learned values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # TD-learned values for stage 2

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        # 1. Calculate Model-Based values
        max_q_stage2 = np.max(q_stage2_mf, axis=1) # Max value of each state (planet)
        q_stage1_mb = transition_matrix @ max_q_stage2 # Expected value based on transitions
        
        # 2. Combine MB and MF values
        q_net = w_mb * q_stage1_mb + (1 - w_mb) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        a1 = action_1[trial]
        a2 = action_2[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # --- Updates ---
        # TD(0) updates for Model-Free values
        
        # Stage 1 MF update: driven by the value of the state actually reached
        # (Using Q(s2, a2) as the target, akin to SARSA)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Stage 2 MF update: driven by reward
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Perseveration Model (Model-Free + Stickiness)
This model is a pure Model-Free learner but adds a "perseveration" or "stickiness" parameter. This accounts for the tendency of participants to repeat their previous choice regardless of reward (motor repetition) or switch frequently, independent of value learning.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free learner with Choice Perseveration (Stickiness).
    
    Adds a bias term to the softmax of the first stage choice based on the 
    immediately preceding choice.
    
    Parameters:
    learning_rate: [0, 1] Update rate for Q-values.
    beta: [0, 10] Inverse temperature for softmax.
    perseveration: [0, 5] Bonus added to the Q-value of the previously chosen action.
    """
    learning_rate, beta, perseveration = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1 # Initialize with an invalid action

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        # Add stickiness bonus to the Q-values before softmax
        q_stage1_with_pers = q_stage1_mf.copy()
        if last_action_1 != -1:
            q_stage1_with_pers[last_action_1] += perseveration

        exp_q1 = np.exp(beta * q_stage1_with_pers)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        a1 = action_1[trial]
        a2 = action_2[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Store action for next trial's perseveration
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Separate Learning Rates for Stage 1 and Stage 2
This model assumes the participant might learn about the high-level spaceship choices at a different speed than they learn about the specific alien probabilities. This is a pure Model-Free approach but allows for different plasticity at different levels of the hierarchy.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free learner with Separate Learning Rates.
    
    Uses alpha_1 for updating spaceship values and alpha_2 for updating alien values.
    
    Parameters:
    alpha_1: [0, 1] Learning rate for Stage 1 (Spaceships).
    alpha_2: [0, 1] Learning rate for Stage 2 (Aliens).
    beta: [0, 10] Inverse temperature for softmax.
    """
    alpha_1, alpha_2, beta = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        a1 = action_1[trial]
        a2 = action_2[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # --- Updates ---
        # Update Stage 1 using alpha_1
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha_1 * delta_stage1
        
        # Update Stage 2 using alpha_2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += alpha_2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```