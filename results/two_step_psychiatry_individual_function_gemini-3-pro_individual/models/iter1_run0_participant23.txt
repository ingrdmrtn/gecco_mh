Here are three new cognitive models exploring different mechanisms and parameter combinations than those previously tested.

### Model 1: Model-Free Learner with Eligibility Traces (`lambda`)
This model focuses purely on model-free learning (TD-learning) but introduces an eligibility trace parameter `lam` (lambda). Instead of just updating the first stage based on the second stage value (TD(0)), the reward at the second stage "leaks back" to update the first stage action directly. This connects the final outcome to the initial choice more strongly than simple TD-learning.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Learner with Eligibility Traces (TD(lambda)).
    This model allows the reward at the second stage to directly influence
    the value update of the first stage action, bridging the gap between stages.

    Parameters:
    learning_rate: [0,1] - Update rate for Q-values.
    beta: [0,10] - Inverse temperature for softmax choice.
    lam: [0,1] - Eligibility trace parameter (lambda). 
                 0 = standard TD learning (only immediate next state matters).
                 1 = Monte Carlo-like update (final reward fully propagates back).
    """
    learning_rate, beta, lam = model_parameters
    n_trials = len(action_1)
  
    # Standard transition matrix assumption (not used in pure MF but kept for consistency)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)      # Values for Spaceships A, U
    q_stage2_mf = np.zeros((2, 2)) # Values for Aliens (Planet X: W, S; Planet Y: P, H)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        state_idx = state[trial] # 0 or 1 (Planet X or Y)

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]

        # --- Learning Updates ---
        # Prediction error at stage 2 (Reward - Q_stage2)
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        
        # Prediction error at stage 1 (Q_stage2 - Q_stage1)
        # Note: In standard SARSA/Q-learning, this is based on the value of the state arrived at.
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]

        # Update Stage 2 value
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Update Stage 1 value: 
        # It gets its own error (delta_stage1) PLUS a fraction (lambda) of the stage 2 error.
        # This allows the final reward 'r' to boost q_stage1_mf directly.
        q_stage1_mf[a1] += learning_rate * (delta_stage1 + lam * delta_stage2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Learner with Separate Learning Rates (`lr_1`, `lr_2`)
Previous hybrid models assumed a single learning rate for both the first-stage (spaceship) and second-stage (alien) updates. It is biologically plausible that learning about stable transitions or high-level choices happens at a different rate than learning about volatile reward probabilities. This model separates these rates.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner with Separate Learning Rates.
    Distinguishes between the learning rate for the first stage (action values)
    and the second stage (reward probabilities).

    Parameters:
    lr_1: [0,1] - Learning rate for Stage 1 (Spaceship values).
    lr_2: [0,1] - Learning rate for Stage 2 (Alien reward values).
    beta: [0,10] - Inverse temperature.
    w: [0,1] - Mixing weight (1 = pure Model-Based, 0 = pure Model-Free).
    """
    lr_1, lr_2, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy (Hybrid) ---
        # Model-Based Value: Transition * Max(Stage 2 Values)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Weighted mixture
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        a1 = action_1[trial]
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]

        # --- Updating ---
        # Stage 1 update uses lr_1
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr_1 * delta_stage1
        
        # Stage 2 update uses lr_2
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += lr_2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Asymmetric Learning Rates (Positive vs Negative Prediction Errors)
This model investigates if the participant learns differently from good news (positive prediction errors) versus bad news (negative prediction errors). This is a common finding in reinforcement learning studies (e.g., pessimism/optimism biases). It modifies the standard Q-learning update to use `alpha_pos` when `delta > 0` and `alpha_neg` when `delta < 0`.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Learner with Asymmetric Learning Rates.
    Uses different learning rates for positive and negative prediction errors.
    
    Parameters:
    alpha_pos: [0,1] - Learning rate for positive prediction errors (better than expected).
    alpha_neg: [0,1] - Learning rate for negative prediction errors (worse than expected).
    beta: [0,10] - Inverse temperature.
    """
    alpha_pos, alpha_neg, beta = model_parameters
    n_trials = len(action_1)
  
    # Standard MF setup
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]

        # --- Asymmetric Updating ---
        
        # Stage 1 PE
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        if delta_stage1 > 0:
            q_stage1_mf[a1] += alpha_pos * delta_stage1
        else:
            q_stage1_mf[a1] += alpha_neg * delta_stage1
        
        # Stage 2 PE
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        if delta_stage2 > 0:
            q_stage2_mf[state_idx, a2] += alpha_pos * delta_stage2
        else:
            q_stage2_mf[state_idx, a2] += alpha_neg * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```