Here are three new cognitive models exploring different mechanisms for learning and decision-making in the two-step task.

### Model 1: Asymmetric Learning Rates (Positive vs. Negative Prediction Errors)
This model hypothesizes that the participant learns differently from positive outcomes (finding gold) versus negative outcomes (no gold). This is a common finding in reinforcement learning literature, often linked to dopamine signaling. It extends the standard Q-learning framework by splitting the learning rate.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Asymmetric Learning Rate Model (Model-Free).
    
    This model assumes the agent updates values differently depending on whether 
    the prediction error is positive (better than expected) or negative (worse than expected).
    
    Parameters:
    - alpha_pos: Learning rate for positive prediction errors.
    - alpha_neg: Learning rate for negative prediction errors.
    - beta: Inverse temperature for softmax choice.
    
    Bounds:
    - alpha_pos: [0, 1]
    - alpha_neg: [0, 1]
    - beta: [0, 10]
    """
    alpha_pos, alpha_neg, beta = model_parameters
    n_trials = len(action_1)
  
    # Simple Model-Free setup
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial] # 0 or 1 (Planet X or Y)

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updating ---
        # Stage 1 Update (TD(0))
        # Note: In pure MF, stage 1 is updated by the value of the state reached (Planet X/Y)
        # We approximate the value of the state as the max Q of that state (Q-learning) or value of chosen option (SARSA)
        # Here we use the value of the chosen option at stage 2 to drive the stage 1 update.
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        if delta_stage1 > 0:
            q_stage1_mf[action_1[trial]] += alpha_pos * delta_stage1
        else:
            q_stage1_mf[action_1[trial]] += alpha_neg * delta_stage1

        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        if delta_stage2 > 0:
            q_stage2_mf[state_idx, action_2[trial]] += alpha_pos * delta_stage2
        else:
            q_stage2_mf[state_idx, action_2[trial]] += alpha_neg * delta_stage2
        
        # Eligibility trace update (propagate reward back to stage 1)
        # We apply the same asymmetry to the direct reward signal hitting stage 1
        if delta_stage2 > 0:
            q_stage1_mf[action_1[trial]] += alpha_pos * delta_stage2
        else:
            q_stage1_mf[action_1[trial]] += alpha_neg * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Decay-Based Reinforcement Learning
This model introduces a memory decay parameter. In standard RL, Q-values persist indefinitely until updated. In this model, unchosen options or general value representations "decay" back toward a neutral point (0 or 0.5) on every trial. This accounts for forgetting or the uncertainty that grows when an option hasn't been sampled in a while.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Reinforcement Learning with Memory Decay.
    
    This model assumes that Q-values for all options decay toward zero on every trial,
    representing forgetting or a return to a prior.
    
    Parameters:
    - learning_rate: Rate at which Q-values are updated from experience.
    - decay_rate: Rate at which Q-values decay toward 0 each trial (0=no decay, 1=instant forgetting).
    - beta: Inverse temperature for softmax choice.
    
    Bounds:
    - learning_rate: [0, 1]
    - decay_rate: [0, 1]
    - beta: [0, 10]
    """
    learning_rate, decay_rate, beta = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Initialize values
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        
        # 1. Decay all values before update (passive forgetting)
        q_stage1 *= (1 - decay_rate)
        q_stage2 *= (1 - decay_rate)

        # 2. Standard TD Updates
        # Stage 1 update based on Stage 2 value
        delta_stage1 = q_stage2[state_idx, action_2[trial]] - q_stage1[action_1[trial]]
        q_stage1[action_1[trial]] += learning_rate * delta_stage1
        
        # Stage 2 update based on Reward
        delta_stage2 = reward[trial] - q_stage2[state_idx, action_2[trial]]
        q_stage2[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Direct reinforcement of stage 1 from reward (simple eligibility)
        q_stage1[action_1[trial]] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid MB/MF with Separate Stage 2 Learning Rate
Previous models often use a single learning rate for both stages or complex eligibility traces. This model posits that the learning dynamics for the second stage (aliens, direct reward) might be distinct from the first stage (spaceships, transitions). It combines a weighted Model-Based/Model-Free first stage with distinct learning rates for the two stages.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Separate Stage Learning Rates.
    
    Allows the learning rate for the immediate reward (Stage 2) to differ from 
    the learning rate for the transition structure/Stage 1 values.
    
    Parameters:
    - lr_stage1: Learning rate for updating Stage 1 values.
    - lr_stage2: Learning rate for updating Stage 2 values (from reward).
    - beta: Inverse temperature.
    - w: Model-based weight (0=MF, 1=MB).
    
    Bounds:
    - lr_stage1: [0, 1]
    - lr_stage2: [0, 1]
    - beta: [0, 10]
    - w: [0, 1]
    """
    lr_stage1, lr_stage2, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy (Hybrid) ---
        # Model-based value calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net value
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy (Model-Free) ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        
        # Stage 1 MF update using lr_stage1
        # Driven by the value of the state we landed in
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr_stage1 * delta_stage1
        
        # Stage 2 MF update using lr_stage2
        # Driven by the actual reward
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr_stage2 * delta_stage2
        
        # No eligibility trace parameter here, but we can assume the stage 1 update 
        # effectively captures the transition value via the delta_stage1 term.

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```