Here are three new cognitive models exploring different mechanisms for learning and decision-making in the two-step task.

### Model 1: Asymmetric Learning Rates (Positive vs. Negative Prediction Errors)
This model hypothesizes that the participant learns differently from positive surprises (rewards better than expected) versus negative surprises (rewards worse than expected). This asymmetry is often observed in psychiatric populations or individuals with specific risk sensitivities.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Asymmetric Learning Rates (Positive vs. Negative Prediction Errors).
    
    This model separates the learning rate into two components: one for positive 
    prediction errors (better than expected) and one for negative prediction errors 
    (worse than expected). This captures potential biases in processing gains vs. losses.

    Parameters:
    - alpha_pos: Learning rate for positive prediction errors (delta > 0).
    - alpha_neg: Learning rate for negative prediction errors (delta < 0).
    - beta: Inverse temperature for softmax choice.
    
    Bounds:
    alpha_pos: [0,1]
    alpha_neg: [0,1]
    beta: [0,10]
    """
    alpha_pos, alpha_neg, beta = model_parameters
    n_trials = len(action_1)
  
    # Values initialized to 0.5 (neutral) rather than 0 to avoid extreme initial updates
    q_stage1_mf = np.zeros(2) + 0.5
    q_stage2_mf = np.zeros((2, 2)) + 0.5

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        chosen_a1 = action_1[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        chosen_a2 = action_2[trial]

        # --- Learning ---
        
        # Stage 2 update (Reward prediction error)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, chosen_a2]
        
        # Apply asymmetric learning rates for Stage 2
        lr_2 = alpha_pos if delta_stage2 > 0 else alpha_neg
        q_stage2_mf[state_idx, chosen_a2] += lr_2 * delta_stage2
        
        # Stage 1 update (TD prediction error using updated Q2)
        # We use the value of the state we actually arrived at as the target
        # This is a standard SARSA-like update for the first stage in this task context
        delta_stage1 = q_stage2_mf[state_idx, chosen_a2] - q_stage1_mf[chosen_a1]
        
        # Apply asymmetric learning rates for Stage 1
        lr_1 = alpha_pos if delta_stage1 > 0 else alpha_neg
        q_stage1_mf[chosen_a1] += lr_1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Eligibility Trace (Lambda) Model
This model introduces an eligibility trace parameter (`lambda`). In standard TD learning, the Stage 1 value is updated based on the Stage 2 value. However, the reward itself can also "directly" influence the Stage 1 choice. A high lambda means the reward obtained at the end of the trial reinforces the Stage 1 choice more strongly, bridging the gap between the initial decision and the final outcome.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Reinforcement Learning with Eligibility Traces (TD-Lambda).
    
    This model allows the reward received at the second stage to directly influence 
    the update of the first stage action values, scaled by an eligibility trace parameter (lambda).
    This bridges the temporal gap between the first action and the final reward.

    Parameters:
    - learning_rate: Update rate for Q-values.
    - beta: Inverse temperature.
    - lam: Eligibility trace parameter (lambda). Controls how much credit Stage 1 gets for Stage 2 reward.
    
    Bounds:
    learning_rate: [0,1]
    beta: [0,10]
    lam: [0,1]
    """
    learning_rate, beta, lam = model_parameters
    n_trials = len(action_1)
  
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        chosen_a1 = action_1[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        chosen_a2 = action_2[trial]

        # --- Learning ---
        
        # 1. Calculate Prediction Errors
        # TD error for stage 1: Difference between Value(Stage 2) and Value(Stage 1)
        # Note: We use the value of the chosen option in Stage 2 as the proxy for state value
        delta_stage1 = q_stage2_mf[state_idx, chosen_a2] - q_stage1_mf[chosen_a1]
        
        # TD error for stage 2: Difference between Reward and Value(Stage 2)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, chosen_a2]
        
        # 2. Update Values
        # Standard Q-learning update for Stage 1 based on transition to Stage 2
        q_stage1_mf[chosen_a1] += learning_rate * delta_stage1
        
        # Update Stage 2 value based on reward
        q_stage2_mf[state_idx, chosen_a2] += learning_rate * delta_stage2
        
        # 3. Eligibility Trace Update for Stage 1
        # The Stage 1 value is effectively updated *again* by the Stage 2 error, scaled by lambda.
        # This allows the final reward to propagate back to the start immediately.
        q_stage1_mf[chosen_a1] += learning_rate * lam * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Separate Learning Rates for Stage 1 and Stage 2
Standard models often assume a single learning rate for the whole task. However, Stage 1 involves learning about transitions and abstract values, while Stage 2 involves learning direct reward probabilities from specific aliens. This model proposes that the participant might have different plasticity (learning rates) for these two distinct hierarchical levels.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Dual Learning Rates (Stage-Specific Plasticity).
    
    This model posits that learning occurs at different rates for the first-stage 
    (choosing a spaceship) and the second-stage (choosing an alien). 
    This separates the learning of abstract state values from direct reward associations.

    Parameters:
    - alpha_stage1: Learning rate for the first decision stage.
    - alpha_stage2: Learning rate for the second decision stage.
    - beta: Inverse temperature.
    
    Bounds:
    alpha_stage1: [0,1]
    alpha_stage2: [0,1]
    beta: [0,10]
    """
    alpha_stage1, alpha_stage2, beta = model_parameters
    n_trials = len(action_1)
  
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        chosen_a1 = action_1[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        chosen_a2 = action_2[trial]

        # --- Learning ---
        
        # Calculate errors
        delta_stage1 = q_stage2_mf[state_idx, chosen_a2] - q_stage1_mf[chosen_a1]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, chosen_a2]
        
        # Update Stage 1 using alpha_stage1
        q_stage1_mf[chosen_a1] += alpha_stage1 * delta_stage1
        
        # Update Stage 2 using alpha_stage2
        q_stage2_mf[state_idx, chosen_a2] += alpha_stage2 * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```