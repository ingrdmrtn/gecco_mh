Here are three new cognitive models that explore different mechanisms for learning and decision-making in the two-step task, distinct from the previously tried combinations.

### Model 1: Asymmetric Learning Rates (Positive/Negative)
This model hypothesizes that the participant learns differently from positive outcomes (rewards) versus negative outcomes (omission of reward). It splits the learning rate into `alpha_pos` and `alpha_neg`. This is a classic modification in reinforcement learning to capture optimism or pessimism biases.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Learner with Asymmetric Learning Rates.
    
    This model assumes the participant is purely model-free (no model-based planning)
    but updates their value estimates differently depending on whether the outcome
    was better or worse than expected (positive vs negative prediction errors).

    Parameters:
    alpha_pos: [0, 1] Learning rate for positive prediction errors.
    alpha_neg: [0, 1] Learning rate for negative prediction errors.
    beta: [0, 10] Inverse temperature (choice consistency).
    """
    alpha_pos, alpha_neg, beta = model_parameters
    n_trials = len(action_1)
  
    # Transition matrix is not used for MB calculation here as this is purely MF, 
    # but we keep the structure consistent with the task.
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # 2 states (planets), 2 actions (aliens)

    for trial in range(n_trials):

        # --- Stage 1 Choice ---
        # Pure Model-Free choice at stage 1
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial] # The planet reached (0 or 1)

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        
        # Stage 2 Update (Reward Prediction Error)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        if delta_stage2 >= 0:
            q_stage2_mf[state_idx, action_2[trial]] += alpha_pos * delta_stage2
        else:
            q_stage2_mf[state_idx, action_2[trial]] += alpha_neg * delta_stage2

        # Stage 1 Update (TD Prediction Error using updated Stage 2 value)
        # Note: Standard TD uses the value of the state reached.
        # We use the value of the chosen action in the second stage as the proxy for state value V(s').
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        if delta_stage1 >= 0:
            q_stage1_mf[action_1[trial]] += alpha_pos * delta_stage1
        else:
            q_stage1_mf[action_1[trial]] += alpha_neg * delta_stage1
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Eligibility Trace (TD-Lambda)
This model introduces an eligibility trace parameter (`lambda`). Instead of just updating stage 1 values based on the stage 2 state value, the reward received at the very end directly updates the stage 1 choice, scaled by `lambda`. This bridges the gap between the first action and the final outcome more directly than standard 1-step Q-learning.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Learner with Eligibility Traces (TD-Lambda).
    
    This model allows the reward at the second stage to directly influence 
    the value of the first stage choice, mediated by a decay parameter lambda.
    This captures how much credit is assigned to the first action for the final result.

    Parameters:
    learning_rate: [0, 1] Standard learning rate.
    beta: [0, 10] Inverse temperature.
    lam: [0, 1] Eligibility trace decay parameter (lambda). 0 = TD(0), 1 = Monte Carlo.
    """
    learning_rate, beta, lam = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        
        # 1. Calculate prediction error at stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # 2. Update Stage 2 value
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # 3. Calculate prediction error at stage 1 based on the state reached
        # We use the value of the action taken at stage 2 as the value of that state.
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        # 4. Update Stage 1 value
        # The update includes the immediate TD error (delta_stage1)
        # PLUS the trace of the stage 2 error scaled by lambda
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1 + learning_rate * lam * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model with Separate Stage-Specific Betas
Previous models assumed the same exploration/exploitation trade-off (`beta`) for both the spaceship choice (Stage 1) and the alien choice (Stage 2). This model proposes that decision noise might differ between the high-level planning stage and the low-level bandit stage. It combines this with the hybrid (MB/MF) weighting `w`.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner with Separate Inverse Temperatures.
    
    This model assumes the participant uses a hybrid of Model-Based and Model-Free
    strategies, but applies different levels of decision noise (beta) 
    to the first stage (spaceship choice) vs the second stage (alien choice).

    Parameters:
    learning_rate: [0, 1] Update rate.
    w: [0, 1] Weight for Model-Based control (0=Pure MF, 1=Pure MB).
    beta_1: [0, 10] Inverse temperature for Stage 1 choice.
    beta_2: [0, 10] Inverse temperature for Stage 2 choice.
    """
    learning_rate, w, beta_1, beta_2 = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Choice ---
        # Calculate Model-Based values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combine MF and MB values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Use beta_1 for stage 1
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        # Use beta_2 for stage 2 (Pure MF at this stage naturally)
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```