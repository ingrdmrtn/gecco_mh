Here are 3 new cognitive models based on the two-step decision-making task.

### Model 1: Pure Model-Free TD(1)
This model ignores the transition structure of the task entirely. It treats the first-stage choice as directly leading to the reward, "skipping" the second stage state in terms of value calculation for the first stage. This is often called "direct reinforcement" or TD(1), where the reward at the end of the trial reinforces the choice made at the very beginning.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free TD(1) / Direct Reinforcement Model.
    
    This model updates the first-stage action values directly based on the final reward 
    received, effectively ignoring the intermediate state transition structure. It represents 
    a simple "habitual" learner that repeats actions that led to reward previously.

    Parameters:
    - learning_rate: Rate at which Q-values are updated. [0, 1]
    - beta: Inverse temperature for softmax choice. [0, 10]
    """
    learning_rate, beta = model_parameters
    n_trials = len(action_1)
  
    # transition_matrix is NOT used here because it's purely model-free
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values for Stage 1 (2 spaceships)
    q_stage1_mf = np.zeros(2) 
    # Q-values for Stage 2 (2 planets x 2 aliens)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        
        # In TD(1), the Stage 1 value is updated by the final reward directly.
        # This skips the "bootstrapping" from Stage 2 values.
        delta_stage1 = reward[trial] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Stage 2 update is standard Q-learning
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model-Based / Model-Free (Daw et al. 2011)
This is the classic "hybrid" model that posits decision-making is a weighted combination of goal-directed (Model-Based) and habitual (Model-Free) systems. The parameter `w` controls the balance: `w=1` is purely Model-Based, `w=0` is purely Model-Free.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Model.
    
    This model assumes the agent computes Q-values using both a Model-Based (MB) strategy
    (using the transition matrix) and a Model-Free (MF) strategy (using TD errors).
    The final choice at Stage 1 is a weighted combination of both.

    Parameters:
    - learning_rate: Rate at which Stage 1 and Stage 2 values are updated. [0, 1]
    - beta: Inverse temperature for softmax choice. [0, 10]
    - w: Weighting parameter. 1 = Pure MB, 0 = Pure MF. [0, 1]
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Model-Based Calculation ---
        # Value of Stage 1 based on planning through the transition matrix
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # --- Hybrid Value Combination ---
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf

        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # Stage 1 MF update (TD(0)): Driven by the value of the state arrived at
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Stage 2 MF update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Note: In the simplified version, we often apply a second update to stage 1 
        # driven by the reward (eligibility trace), but here we stick to simple TD(0) for MF part.

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Choice Kernel (Perseveration) + Model-Free
This model adds a "Choice Kernel" (or perseverance bonus) to a standard Model-Free learner. This captures the tendency of participants to simply repeat their previous motor action regardless of reward (stickiness), or potentially switch away from it. This is distinct from value-based learning.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Q-Learning with Choice Stickiness (Perseveration).
    
    This model is a standard Model-Free learner (TD(0)) but adds a 'stickiness' 
    parameter that biases the agent to repeat the last chosen action at Stage 1,
    regardless of the outcome.

    Parameters:
    - learning_rate: Rate at which Q-values are updated. [0, 1]
    - beta: Inverse temperature for softmax choice. [0, 10]
    - stickiness: Bonus added to the logits of the previously chosen action. 
                  Positive = repetition bias, Negative = alternation bias. [-5, 5]
    """
    learning_rate, beta, stickiness = model_parameters
    n_trials = len(action_1)
  
    # Transition matrix not used for value estimation in pure MF
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track previous action (initialize with -1 or handle first trial separately)
    prev_action_1 = -1

    for trial in range(n_trials):

        # --- Stage 1 Choice ---
        # Calculate logits: Value * beta + stickiness bonus
        logits_1 = beta * q_stage1_mf
        
        # Add stickiness to the previously chosen action (if not the first trial)
        if prev_action_1 != -1:
            logits_1[prev_action_1] += stickiness
            
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Update previous action for next trial
        prev_action_1 = action_1[trial]
        
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # Standard TD(0) updates
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```