Here are three new cognitive models exploring different mechanisms of learning and decision-making in the two-step task.

### Model 1: Hybrid Model-Based / Model-Free (MB/MF)
This model represents the classic hypothesis for this task: behavior is a mixture of a habitual (Model-Free) system and a planning (Model-Based) system. The Model-Free system reinforces actions based on direct reward history, while the Model-Based system calculates values using the transition structure. A mixing parameter `w` determines the balance between these two systems.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free RL.
    
    The agent computes Stage 1 values as a weighted sum of Model-Based (MB) 
    and Model-Free (MF) values. MB values are derived from the transition matrix 
    and Stage 2 values. MF values are learned via TD-learning (SARSA).

    Parameters:
    learning_rate: [0,1] - Update rate for Q-values.
    beta: [0,10] - Inverse temperature for softmax.
    w: [0,1] - Weighting parameter (0 = Pure MF, 1 = Pure MB).
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)

    # Fixed transition matrix: Row 0 -> [Planet 0, Planet 1], Row 1 -> [Planet 0, Planet 1]
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)      # Model-free values for spaceships
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for aliens (planets x aliens)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # 1. Calculate Model-Based values: Transition * Max(Stage 2 Values)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Combine MB and MF values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # 3. Softmax selection
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial] # Actual planet reached

        # --- Stage 2 Choice ---
        q_values_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * q_values_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning ---
        # Update Stage 1 MF value using the value of the chosen Stage 2 action (SARSA-style)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Update Stage 2 MF value based on reward
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Pure Model-Free with Eligibility Traces (TD(lambda))
This model assumes the participant does not use the transition structure at all (no planning). Instead, they rely purely on temporal difference learning. Crucially, it includes an eligibility trace parameter `lambda`. When a reward is received at the second stage, it reinforces the second-stage choice, but the "credit" also propagates back to the first-stage choice proportional to `lambda`.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free RL with Eligibility Traces (TD-lambda).
    
    Does not use the transition matrix. Instead, reward information propagates 
    directly from Stage 2 outcome to Stage 1 choice via an eligibility trace.
    If lambda=1, it mimics Monte Carlo updating; if lambda=0, it is pure TD(0).

    Parameters:
    learning_rate: [0,1] - Update rate.
    beta: [0,10] - Inverse temperature.
    lam: [0,1] - Eligibility trace decay (lambda).
    """
    learning_rate, beta, lam = model_parameters
    n_trials = len(action_1)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice (Pure MF) ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        q_values_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * q_values_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning ---
        # 1. Prediction error at Stage 1 (driven by Stage 2 value)
        # Note: In standard TD(lambda), the trace is E. Here we implement the 
        # direct update logic for the specific 2-step structure.
        delta_1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        # 2. Prediction error at Stage 2 (driven by reward)
        delta_2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Update Stage 2
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_2
        
        # Update Stage 1:
        # It gets the direct TD update (learning_rate * delta_1)
        # PLUS the eligibility trace update from Stage 2 error (learning_rate * lambda * delta_2)
        q_stage1_mf[action_1[trial]] += learning_rate * delta_1 + (learning_rate * lam * delta_2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Choice Kernel (Perseveration) + Model-Based
This model adds a primitive "habit" mechanism called a Choice Kernel to the Model-Based planner. The Choice Kernel simply tracks the frequency of choosing specific actions, independent of reward. This captures the tendency of participants to repeat (perseverate) or switch choices regardless of the outcome, blended with a sophisticated planning model.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Based RL with Choice Kernel (Perseveration).
    
    The agent is primarily model-based but is biased by a 'choice kernel' 
    which tracks recent choice history. This captures perseveration (stickiness) 
    or alternation biases regardless of reward.

    Parameters:
    learning_rate: [0,1] - Update rate for Q-values.
    beta: [0,10] - Inverse temperature for value-based choice.
    choice_stickiness: [0,10] - Weight of the choice kernel (positive = perseveration).
    kernel_decay: [0,1] - Decay rate of the choice trace (how fast history is forgotten).
    """
    learning_rate, beta, choice_stickiness, kernel_decay = model_parameters
    n_trials = len(action_1)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage2_mf = np.zeros((2, 2))
    
    # Choice kernel for Stage 1 actions (initialized to 0)
    choice_kernel = np.zeros(2)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Decision variable combines Value (MB) and Habit (Kernel)
        logits_1 = (beta * q_stage1_mb) + (choice_stickiness * choice_kernel)
        
        exp_q1 = np.exp(logits_1 - np.max(logits_1)) # Subtract max for numerical stability
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Choice (Standard Softmax on Values) ---
        q_values_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * q_values_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning ---
        # Update Stage 2 values based on reward
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Update Choice Kernel
        # Decay existing traces
        choice_kernel *= (1 - kernel_decay)
        # Increment trace for the chosen action (indicator function)
        choice_kernel[action_1[trial]] += 1.0

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```