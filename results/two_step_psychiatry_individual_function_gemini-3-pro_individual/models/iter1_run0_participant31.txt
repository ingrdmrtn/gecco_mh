Here are three cognitive models for the two-step task, exploring different reinforcement learning strategies (Model-Free, Hybrid, and Perseverance).

### Model 1: Pure Model-Free Learner (TD-Learning)
This model ignores the transition structure of the task entirely. It learns the value of the first-stage spaceships based purely on the reward eventually received, mediated by the second-stage value (Temporal Difference learning). This is the standard "habitual" controller.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free Learner (TD(0)).
    
    This model learns Q-values for both stages using direct prediction error updates.
    It does not use the transition matrix to plan. It updates the first stage 
    value based on the value of the state reached in the second stage.

    Parameters:
    learning_rate: [0,1] Rate at which Q-values are updated.
    beta: [0,10] Inverse temperature for softmax.
    """
    learning_rate, beta = model_parameters
    n_trials = len(action_1)
  
    # No transition matrix used here
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)      # Values for Spaceship A (0) and U (1)
    q_stage2_mf = np.zeros((2, 2)) # Values for Planet X (aliens 0,1) and Planet Y (aliens 0,1)

    for trial in range(n_trials):

        # --- Policy for the first choice ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial] # The planet we actually arrived at

        # --- Policy for the second choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # TD(0) update for Stage 1: Driven by the value of the state reached (Planet X or Y)
        # Note: We use the Q-value of the chosen action in stage 2 as the target for stage 1
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # TD(0) update for Stage 2: Driven by the reward received
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model-Based / Model-Free Learner
This is the classic "Daw et al. (2011)" style model. It calculates the first-stage values as a weighted mixture of a Model-Based (planning) component and a Model-Free (habit) component. The mixing weight `w` determines which system dominates.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Learner.
    
    Combines a model-based evaluation (using the transition matrix) and a 
    model-free evaluation (using TD errors) for the first-stage choice.

    Parameters:
    learning_rate: [0,1] Update rate for Q-values.
    beta: [0,10] Inverse temperature.
    w: [0,1] Weight parameter. 1 = Pure Model-Based, 0 = Pure Model-Free.
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Policy for the first choice ---
        # 1. Calculate Model-Based values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Integrate MB and MF values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Policy for the second choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # Model-Free update for Stage 1
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Update for Stage 2 (shared by both systems)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Choice Perseverance (Stickiness) Model
This model extends the Model-Free learner by adding a "stickiness" parameter. This captures the tendency of participants to repeat their previous first-stage choice regardless of the outcome (reward) or transition type. This is a common heuristic in human behavior that is distinct from reinforcement learning.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Learner with Choice Perseverance (Stickiness).
    
    Adds a 'stickiness' bonus to the action chosen on the previous trial,
    biasing the agent to repeat choices regardless of reward history.

    Parameters:
    learning_rate: [0,1] Update rate.
    beta: [0,10] Inverse temperature.
    stickiness: [0,5] Bonus added to the log-odds of repeating the previous choice.
    """
    learning_rate, beta, stickiness = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1 # Initialize with an invalid action

    for trial in range(n_trials):

        # --- Policy for the first choice ---
        # Calculate logits (pre-softmax values)
        logits = beta * q_stage1_mf
        
        # Add stickiness bonus if we have a previous action
        if last_action_1 != -1:
            logits[last_action_1] += stickiness
            
        exp_q1 = np.exp(logits)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Record this action for the next trial's stickiness
        last_action_1 = action_1[trial] 
        state_idx = state[trial]

        # --- Policy for the second choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```