Here are three new cognitive models that explore different mechanisms for learning and decision-making in the two-step task, distinct from the simple hybrid MB/MF model provided in the feedback.

### Model 1: Perseveration Model
This model hypothesizes that the participant has a tendency to repeat their previous choice regardless of reward (perseveration), or switch away from it. This is a common feature in reinforcement learning models of human behavior. It adds a `perseveration_strength` parameter to the standard Q-learning framework.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Q-Learning with Choice Perseveration.
    
    This model assumes the participant is primarily a model-free learner but is biased 
    towards repeating their previous Stage 1 choice (perseveration).
    
    Parameters:
    - learning_rate: [0, 1] Update rate for Q-values based on prediction error.
    - beta: [0, 10] Inverse temperature for softmax choices.
    - perseveration_strength: [-5, 5] Bonus added to the Q-value of the previously chosen action. 
                                     Positive values encourage repeating choices; negative values encourage switching.
                                     (Note: Bounds are conceptual; in optimization, map appropriately).
    """
    learning_rate, beta, perseveration_strength = model_parameters
    n_trials = len(action_1)
  
    # Initialize Q-values
    q_stage1_mf = np.zeros(2)      # Values for Spaceship A vs U
    q_stage2_mf = np.zeros((2, 2)) # Values for Aliens (Planet X: W, S; Planet Y: P, H)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_action_1 = -1 # No previous action on trial 0

    for trial in range(n_trials):
        
        # --- Stage 1 Choice (Spaceship) ---
        # Add perseveration bonus to the Q-values before softmax
        q_stage1_biased = q_stage1_mf.copy()
        if prev_action_1 != -1:
            q_stage1_biased[prev_action_1] += perseveration_strength
            
        exp_q1 = np.exp(beta * q_stage1_biased)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial] # 0 for Planet X, 1 for Planet Y

        # --- Stage 2 Choice (Alien) ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning Updates ---
        # Update Stage 1 Q-value using the value of the state actually reached (SARSA-like logic for stage 1)
        # We use the value of the chosen stage 2 action as the proxy for the state value
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Update Stage 2 Q-value based on reward
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Store action for next trial's perseveration
        prev_action_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Separate Learning Rates for Positive/Negative Errors
This model investigates if the participant learns differently from positive surprises (getting gold when not expected) versus negative surprises (getting nothing when expecting gold). This asymmetry is often observed in clinical populations.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Dual Learning Rate Model (Asymmetric Learning).
    
    This model separates the learning rate into two components: one for positive prediction errors 
    (learning_rate_pos) and one for negative prediction errors (learning_rate_neg). This tests 
    if the participant updates beliefs differently after reward vs. omission.
    
    Parameters:
    - learning_rate_pos: [0, 1] Learning rate when prediction error is positive (better than expected).
    - learning_rate_neg: [0, 1] Learning rate when prediction error is negative (worse than expected).
    - beta: [0, 10] Inverse temperature for softmax.
    """
    learning_rate_pos, learning_rate_neg, beta = model_parameters
    n_trials = len(action_1)
  
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):

        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning Updates ---
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        alpha_1 = learning_rate_pos if delta_stage1 > 0 else learning_rate_neg
        q_stage1_mf[action_1[trial]] += alpha_1 * delta_stage1
        
        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        alpha_2 = learning_rate_pos if delta_stage2 > 0 else learning_rate_neg
        q_stage2_mf[state_idx, action_2[trial]] += alpha_2 * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Decay Model
This model introduces a forgetting mechanism. Unlike standard Q-learning where unchosen values stay static, here all Q-values slowly decay back to a neutral baseline (0.5) on every trial. This captures the idea that memory of value fades over time.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Q-Learning with Memory Decay.
    
    In this model, Q-values for all options decay toward a neutral value (0.5) on every trial, 
    representing forgetting. Only the chosen option receives a learning update.
    
    Parameters:
    - learning_rate: [0, 1] Update rate for the chosen option.
    - beta: [0, 10] Inverse temperature.
    - decay_rate: [0, 1] Rate at which Q-values decay to 0.5 (0 = no decay, 1 = instant reset).
    """
    learning_rate, beta, decay_rate = model_parameters
    n_trials = len(action_1)
  
    # Initialize Q-values at neutral 0.5 (since rewards are 0 or 1)
    q_stage1_mf = np.ones(2) * 0.5
    q_stage2_mf = np.ones((2, 2)) * 0.5
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):

        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning Updates ---
        
        # 1. Decay all values toward 0.5
        q_stage1_mf = q_stage1_mf * (1 - decay_rate) + 0.5 * decay_rate
        q_stage2_mf = q_stage2_mf * (1 - decay_rate) + 0.5 * decay_rate

        # 2. Standard TD Update for chosen actions (applied on top of decay)
        # Note: We calculate PE based on values *before* this specific update but *after* decay 
        # (or simultaneously). Here we apply update to the decayed values.
        
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```