Here are three new cognitive models for the two-step task. These models introduce different mechanisms for how value is computed and updated, specifically exploring the balance between model-based and model-free reinforcement learning, as well as perseveration biases.

### Model 1: Hybrid Learner (Model-Based + Model-Free)
This model combines model-free (habitual) and model-based (goal-directed) learning. The model-free system learns values based on direct experience (TD learning), while the model-based system computes values by planning through a transition matrix. A weighting parameter `w` controls the balance between these two systems during the first-stage choice.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner combining Model-Based (MB) and Model-Free (MF) value estimation.
    
    The stage 1 choice is a weighted combination of MF values (learnt via TD) and 
    MB values (computed via the transition matrix). Stage 2 is purely MF.

    Parameters:
    learning_rate: [0, 1] - Learning rate for value updates.
    beta: [0, 10] - Inverse temperature for softmax choice.
    w: [0, 1] - Weighting parameter (0 = pure MF, 1 = pure MB).
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)
    
    # Fixed transition matrix as per task description
    # Row 0: Spaceship A -> [Planet X, Planet Y]
    # Row 1: Spaceship U -> [Planet X, Planet Y]
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    q_stage1_mf = np.zeros(2)      # Model-free Q-values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Model-free Q-values for stage 2 (aliens)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # 1. Compute Model-Based Values: V_MB(s1) = T * max(Q(s2))
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Combine MF and MB values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # 3. Softmax
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        # --- Stage 2 Choice ---
        state_idx = state[trial] # 0 or 1
        
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning ---
        # Update Stage 2 Q-values (Standard Rescorla-Wagner / Q-learning)
        # Prediction error at outcome
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Update Stage 1 MF Q-values (TD(0))
        # Note: In standard hybrid models, MF updates often use the Q-value of the chosen 
        # second stage option as the target, or the value of the state. 
        # Here we use the standard TD(1) style update often used in simplified models 
        # where the reward drives the update directly back to stage 1, or TD(0) using stage 2 value.
        # Let's use the value of the state reached as the proxy for reward at stage 1.
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Model-Free Learner with Choice Perseveration
This model is a pure Model-Free learner (like the "best so far") but adds a **Choice Perseveration** parameter (`perseveration`). This parameter captures the tendency of participants to repeat their previous first-stage choice regardless of reward (stickiness), or to switch (if negative). This is a common nuisance parameter in two-step task modeling.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Learner with Choice Perseveration ("Stickiness").
    
    Adds a bias to repeat the previous stage 1 action, independent of reward history.

    Parameters:
    learning_rate: [0, 1] - Learning rate.
    beta: [0, 10] - Inverse temperature.
    perseveration: [-5, 5] - Bias to repeat previous choice (positive) or switch (negative).
                             Note: Bounds in optimization should handle this range.
                             If strictly [0,1] is required by system, we can map it, 
                             but usually stickiness can be negative. Assuming [0,1] 
                             maps to a small positive stickiness for this exercise.
    """
    learning_rate, beta, perseveration = model_parameters
    n_trials = len(action_1)

    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1 # Indicator for previous choice

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Add perseveration bonus to the Q-value of the previously chosen action
        q_net = q_stage1.copy()
        if last_action_1 != -1:
            q_net[last_action_1] += perseveration

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Save this action for next trial's perseveration
        last_action_1 = action_1[trial]

        # --- Stage 2 Choice ---
        state_idx = state[trial]
        exp_q2 = np.exp(beta * q_stage2[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning ---
        # Update Stage 2
        delta_stage2 = reward[trial] - q_stage2[state_idx, action_2[trial]]
        q_stage2[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Update Stage 1 (SARSA-style / TD)
        # Using the value of the chosen second stage option to update first stage
        delta_stage1 = q_stage2[state_idx, action_2[trial]] - q_stage1[action_1[trial]]
        q_stage1[action_1[trial]] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Separate Learning Rates for Stage 1 and Stage 2
This model acknowledges that learning dynamics might differ between the abstract choice of a spaceship (Stage 1) and the concrete choice of an alien (Stage 2). It implements a pure Model-Free learner but utilizes two distinct learning rates: `alpha1` for the first stage and `alpha2` for the second stage.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Dual-Learning Rate Model-Free Learner.
    
    Uses separate learning rates for the first stage (spaceship choice) and 
    second stage (alien choice), allowing for different timescales of integration 
    for the transition structure vs. the reward structure.

    Parameters:
    alpha1: [0, 1] - Learning rate for stage 1 (Spaceships).
    alpha2: [0, 1] - Learning rate for stage 2 (Aliens).
    beta: [0, 10] - Inverse temperature.
    """
    alpha1, alpha2, beta = model_parameters
    n_trials = len(action_1)

    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        # --- Stage 2 Choice ---
        state_idx = state[trial]
        exp_q2 = np.exp(beta * q_stage2[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning ---
        # Update Stage 2 using alpha2
        delta_stage2 = reward[trial] - q_stage2[state_idx, action_2[trial]]
        q_stage2[state_idx, action_2[trial]] += alpha2 * delta_stage2
        
        # Update Stage 1 using alpha1
        # The teaching signal is the updated value of the second stage choice
        delta_stage1 = q_stage2[state_idx, action_2[trial]] - q_stage1[action_1[trial]]
        q_stage1[action_1[trial]] += alpha1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```