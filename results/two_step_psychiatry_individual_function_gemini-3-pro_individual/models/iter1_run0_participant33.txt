Here are three cognitive models representing different hypotheses about how the participant makes decisions.

### Model 1: Pure Model-Free (TD Learning)
This model assumes the participant does **not** use the transition structure (the 70/30 probabilities) to plan. Instead, they learn the value of spaceships (Stage 1) and aliens (Stage 2) solely through trial-and-error using Temporal Difference (TD) learning. They update the value of the chosen spaceship based on the value of the subsequent state (SARSA-style).

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free (TD) Learning Model.
    
    This model ignores the transition matrix structure. It learns Q-values for 
    Stage 1 (spaceships) and Stage 2 (aliens) purely from experience using 
    Temporal Difference errors.
    
    Parameters:
    - learning_rate: Rate at which Q-values are updated [0, 1].
    - beta: Inverse temperature for softmax choice [0, 10].
    """
    learning_rate, beta = model_parameters
    n_trials = len(action_1)
    
    # Initialize Q-values
    # q_stage1_mf: Values for Spaceship 0 and 1
    # q_stage2_mf: Values for Planet 0 (Aliens 0,1) and Planet 1 (Aliens 0,1)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        
        # --- Stage 1 Policy (Model-Free) ---
        # Choice depends only on learned MF values, ignoring transition matrix
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial] # The planet arrived at
        
        # --- Stage 2 Policy (Model-Free) ---
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Value Updating ---
        
        # Update Stage 1 Q-values using SARSA update (TD error from Stage 2 Q-value)
        # We use the value of the action *actually taken* in stage 2 as the target
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Update Stage 2 Q-values using Reward Prediction Error
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model-Based / Model-Free
This model assumes the participant uses a mixture of both strategies. They calculate a goal-directed value (Model-Based) using the transition matrix and a habitual value (Model-Free) from experience. The final decision at Stage 1 is a weighted combination of these two values.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model (Model-Based + Model-Free).
    
    The agent computes a net Q-value for Stage 1 which is a weighted sum of 
    the Model-Based value (planning via transition matrix) and the Model-Free 
    value (learned via TD).
    
    Parameters:
    - learning_rate: Rate at which Q-values are updated [0, 1].
    - beta: Inverse temperature for softmax choice [0, 10].
    - w: Weighting parameter [0, 1]. 1 = Pure MB, 0 = Pure MF.
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):

        # --- Stage 1 Policy (Hybrid) ---
        
        # 1. Calculate Model-Based values (Planning)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Combine with Model-Free values
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Value Updating ---
        
        # Update Stage 1 MF values
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Update Stage 2 MF values (used by both MB and MF systems)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Model-Based with Choice Perseverance
This model extends the "Best Model So Far" (Pure MB) by adding a "stickiness" parameter. This accounts for the common human tendency to repeat the previously chosen action regardless of reward (motor perseverance). The agent plans using the transition matrix but is biased toward repeating their last spaceship choice.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Based Reinforcement Learning with Choice Perseverance.
    
    The agent uses the transition matrix to compute values (Model-Based), 
    but adds a 'stickiness' bonus to the action chosen on the previous trial,
    capturing the tendency to repeat choices.
    
    Parameters:
    - learning_rate: Rate at which stage 2 Q-values are updated [0, 1].
    - beta: Inverse temperature for softmax choice [0, 10].
    - stickiness: Bonus added to the previous choice's value [0, 5].
    """
    learning_rate, beta, stickiness = model_parameters
    n_trials = len(action_1)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1 # Initialize with no previous action

    for trial in range(n_trials):

        # --- Stage 1 Policy (MB + Stickiness) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Add stickiness bonus to the previously chosen action (if it exists)
        q_net = q_stage1_mb.copy()
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Update tracker for next trial
        last_action_1 = action_1[trial]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Value Updating ---
        # Only Stage 2 is learned; Stage 1 is calculated via MB planning
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```