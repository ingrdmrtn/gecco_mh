Here are three cognitive models based on the two-step task structure, exploring different mechanisms for learning and decision-making.

### Model 1: Hybrid Model-Based / Model-Free Learner
This model combines model-based (planning using the transition matrix) and model-free (direct reinforcement) strategies. A weighting parameter `w` controls the balance between these two systems for the first-stage choice. This is the standard "Daw et al. (2011)" style model for this task.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Learner.
    
    Combines a model-based controller (planning using the transition matrix) and a 
    model-free controller (TD-learning). A mixing weight determines the influence of each.
    
    Parameters:
    learning_rate: [0, 1] Update rate for Q-values.
    beta: [0, 10] Inverse temperature for softmax.
    w: [0, 1] Mixing weight (0 = pure Model-Free, 1 = pure Model-Based).
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition matrix as per task description (A->X common, U->Y common)
    # Rows: Actions (0, 1), Cols: States (0, 1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)       # Model-free values for stage 1
    q_stage2_mf = np.zeros((2, 2))  # Model-free values for stage 2 (aliens)

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # Model-Based Value: Expected max value of next stage
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        # Note: In standard hybrid models, stage 2 is purely model-free
        
        # SARSA / TD(1) style updates
        # Update Stage 1 MF value based on Stage 2 value (TD error)
        # Note: We use the value of the CHOSEN action in stage 2, not max (SARSA-like for stage 1)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Update Stage 2 MF value based on reward
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Second stage update also trains first stage (eligibility trace = 1 implicit here for simplicity in this variant)
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Perseveration Model (Habitual Repeater)
This model assumes the participant relies on a model-free strategy but is also influenced by a simple motor "perseveration" bias. Regardless of reward, they have a tendency to repeat the last chosen action at Stage 1. This captures behavioral inertia often seen in these tasks.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Learner with Choice Perseveration.
    
    Adds a 'stickiness' or perseveration parameter that biases the agent 
    to repeat the previous Stage 1 choice, regardless of outcome.
    
    Parameters:
    learning_rate: [0, 1] Update rate for Q-values.
    beta: [0, 10] Inverse temperature.
    perseveration: [0, 5] Bonus added to the Q-value of the previously chosen action.
    """
    learning_rate, beta, perseveration = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    last_action_1 = -1 # Indicator for no previous action

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # Add perseveration bonus to the previously chosen action
        q_stage1_biased = q_stage1.copy()
        if last_action_1 != -1:
            q_stage1_biased[last_action_1] += perseveration

        exp_q1 = np.exp(beta * q_stage1_biased)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        # Standard TD(1) / Eligibility trace = 1 logic
        
        # 1. Update Stage 2 based on reward
        delta_stage2 = reward[trial] - q_stage2[state_idx, action_2[trial]]
        q_stage2[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # 2. Update Stage 1 based on Stage 2 value and Stage 2 RPE
        delta_stage1 = q_stage2[state_idx, action_2[trial]] - q_stage1[action_1[trial]]
        
        # Full transfer of stage 2 reward error to stage 1 (lambda=1)
        q_stage1[action_1[trial]] += learning_rate * (delta_stage1 + delta_stage2)
        
        last_action_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Asymmetric Learning Rates (Positive/Negative)
This model hypothesizes that the participant learns differently from positive outcomes (finding gold) versus negative outcomes (no gold). This "positivity bias" or "negativity bias" is common in psychiatric populations. It is purely model-free but splits the learning rate.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Asymmetric Learning Rate Model (Model-Free).
    
    Uses different learning rates for positive prediction errors (better than expected)
    and negative prediction errors (worse than expected).
    
    Parameters:
    alpha_pos: [0, 1] Learning rate for positive prediction errors.
    alpha_neg: [0, 1] Learning rate for negative prediction errors.
    beta: [0, 10] Inverse temperature.
    """
    alpha_pos, alpha_neg, beta = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Decision ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        
        # Calculate Prediction Errors
        delta_stage1 = q_stage2[state_idx, action_2[trial]] - q_stage1[action_1[trial]]
        delta_stage2 = reward[trial] - q_stage2[state_idx, action_2[trial]]
        
        # Select learning rate for Stage 2 update
        if delta_stage2 >= 0:
            lr_s2 = alpha_pos
        else:
            lr_s2 = alpha_neg
            
        q_stage2[state_idx, action_2[trial]] += lr_s2 * delta_stage2
        
        # Select learning rate for Stage 1 update (based on total error signal)
        # We treat the accumulated error as the signal for learning rate selection
        total_error = delta_stage1 + delta_stage2
        
        if total_error >= 0:
            lr_s1 = alpha_pos
        else:
            lr_s1 = alpha_neg
            
        # Update Stage 1 (assuming eligibility trace = 1 for direct path)
        q_stage1[action_1[trial]] += lr_s1 * total_error
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```