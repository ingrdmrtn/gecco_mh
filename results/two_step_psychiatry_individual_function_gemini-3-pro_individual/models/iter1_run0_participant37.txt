Here are three new cognitive models that explore different mechanisms for solving the two-step task.

### Model 1: Hybrid Model (Model-Based + Model-Free)
This is the classic "Daw et al. (2011)" style model. It posits that participants use a weighted combination of two systems:
1.  **Model-Free (MF):** Learns values from direct experience (TD learning), sensitive to reward history but insensitive to transition structure.
2.  **Model-Based (MB):** Computes values by planning through the known transition matrix (70/30).
A mixing parameter `w` controls the balance between these two systems for the first-stage choice.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Learner.
    
    The agent computes Stage 1 values as a weighted sum of Model-Based (MB) and Model-Free (MF) values.
    MB values are derived from the transition matrix and Stage 2 max values.
    MF values are learned via TD-learning (SARSA).
    
    Parameters:
    learning_rate: [0,1] Rate for updating Q-values (MF system).
    beta: [0,10] Inverse temperature for softmax.
    w: [0,1] Weighting parameter (0 = pure MF, 1 = pure MB).
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)
    
    # Fixed transition matrix as per task description
    # Row 0: Spaceship A -> [Planet X (70%), Planet Y (30%)]
    # Row 1: Spaceship U -> [Planet X (30%), Planet Y (70%)]
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2) + 0.5
    q_stage2_mf = np.zeros((2, 2)) + 0.5
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based Value: V_MB(s1) = T * max(Q_s2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value: w * MB + (1-w) * MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        state_idx = state[trial]
        
        # --- Stage 2 Choice ---
        # Standard softmax on Stage 2 Q-values
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]
        
        # --- Learning ---
        # 1. Update Stage 1 MF value using Stage 2 Q-value (SARSA-style TD(0))
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # 2. Update Stage 2 MF value using reward
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Choice Kernel (Perseveration) Model
This model is a pure Model-Free learner (like the previous best model) but adds a **perseveration bonus** (or choice kernel). Humans often repeat their previous choice regardless of reward ("stickiness"). This model captures that tendency.
- `k` tracks the "stickiness" of the previous Stage 1 choice.
- If `perseveration_weight` is positive, the agent tends to repeat the last spaceship choice.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Learner with Choice Perseveration.
    
    Adds a 'stickiness' parameter to the Stage 1 choice. The agent is biased 
    towards repeating the previously chosen spaceship, independent of reward history.
    
    Parameters:
    learning_rate: [0,1] Learning rate for Q-values.
    beta: [0,10] Inverse temperature.
    perseveration_weight: [0,5] Strength of the tendency to repeat the previous Stage 1 action.
    """
    learning_rate, beta, perseveration_weight = model_parameters
    n_trials = len(action_1)

    q_stage1_mf = np.zeros(2) + 0.5
    q_stage2_mf = np.zeros((2, 2)) + 0.5
    
    # Choice kernel: tracks the previous choice (0 or 1)
    # Initialize to 0 bias
    last_action_1 = -1 

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Calculate net value: Q-value + Perseveration Bonus
        q_net = q_stage1_mf.copy()
        
        if last_action_1 != -1:
            q_net[last_action_1] += perseveration_weight

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        state_idx = state[trial]
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]
        
        # --- Learning ---
        # TD(0) updates
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Update history
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Eligibility Trace (TD(lambda)) Model
This model introduces an **eligibility trace** parameter, `lambda`. In simple TD(0) (pure MF), the Stage 1 value is updated by the value of the *state* reached in Stage 2. However, the final reward received at the end of Stage 2 can also directly reinforce the Stage 1 choice.
- `lambda` controls how much the final reward `r` "drifts back" to update the first stage choice directly, bridging the gap between the first action and the final outcome.
- If `lambda` = 1, it resembles Monte Carlo learning (direct reinforcement). If `lambda` = 0, it is pure TD(0).

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Learner with Eligibility Traces (TD-Lambda).
    
    Allows the reward at the end of the trial to directly update the Stage 1 choice,
    bridging the temporal gap. This creates a blend between updating Stage 1 based on 
    Stage 2 expectations vs. the actual outcome.
    
    Parameters:
    learning_rate: [0,1] Learning rate.
    beta: [0,10] Inverse temperature.
    lambda_param: [0,1] Eligibility trace decay parameter. 
                  (0 = Update Stage 1 only via Stage 2 prediction; 1 = Update Stage 1 via full reward).
    """
    learning_rate, beta, lambda_param = model_parameters
    n_trials = len(action_1)

    q_stage1_mf = np.zeros(2) + 0.5
    q_stage2_mf = np.zeros((2, 2)) + 0.5

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]

        # --- Learning ---
        # 1. Prediction error at Stage 2 transition
        # This is the standard TD error for the first step
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        
        # 2. Prediction error at Reward
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        
        # Update Stage 2 value
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Update Stage 1 value
        # The update includes the immediate error (delta_stage1)
        # PLUS the discounted error from the second stage (lambda * delta_stage2)
        # This allows the final reward 'r' to influence Q(s1, a1) directly.
        q_stage1_mf[a1] += learning_rate * (delta_stage1 + lambda_param * delta_stage2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```