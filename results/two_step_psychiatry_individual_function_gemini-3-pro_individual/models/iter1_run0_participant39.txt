Here are three new cognitive models based on the two-step task structure.

### Model 1: Hybrid Model-Based / Model-Free Learner
This model combines model-based (planning using the transition matrix) and model-free (learning from direct experience) valuation systems. A mixing parameter `w` controls the balance between these two systems for the first-stage choice.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Model 1: Hybrid Model-Based / Model-Free Learner.
    Combines MB planning (using transition matrix) and MF learning (TD).
    
    Bounds:
    learning_rate: [0, 1] - Learning rate for Q-value updates.
    beta: [0, 10] - Inverse temperature for softmax.
    w: [0, 1] - Weighting parameter (0 = Pure MF, 1 = Pure MB).
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Initialize Q-values
    q_stage1_mf = np.zeros(2) # Model-free Q-values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Model-free Q-values for stage 2 (Aliens)

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        # Model-Based Value: Expected max value of stage 2 weighted by transitions
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value: Weighted sum of MB and MF values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        a1 = action_1[trial]

        # --- Stage 2 Policy ---
        # Standard softmax on Stage 2 Q-values
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]

        # --- Updates ---
        # Stage 1 MF Update (TD-0 using Stage 2 Q-value as proxy for V(s'))
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Stage 2 MF Update (Standard Rescorla-Wagner)
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Choice Perseveration (Habit) Model
This model extends the pure Model-Free learner by adding a "stickiness" or perseveration parameter. This captures the tendency to repeat the previous action regardless of reward (motor habit), which is often observed in this task.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model 2: Model-Free Learner with Choice Perseveration.
    Adds a 'stickiness' bonus to the previously chosen action.
    
    Bounds:
    learning_rate: [0, 1] - Learning rate.
    beta: [0, 10] - Inverse temperature.
    perseveration: [0, 5] - Bonus added to the previously chosen action (logits).
    """
    learning_rate, beta, perseveration = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track previous choice for stage 1 (initialize to -1 or None)
    last_choice_1 = -1

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        # Calculate logits (Q-values * beta)
        logits_1 = beta * q_stage1_mf
        
        # Add perseveration bonus if a previous choice exists
        if last_choice_1 != -1:
            logits_1[last_choice_1] += perseveration

        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        a1 = action_1[trial]
        last_choice_1 = a1 # Update history

        # --- Stage 2 Policy ---
        # No perseveration on stage 2 assumed here for simplicity, just standard MF
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]
  
        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Separate Learning Rates for Reward/No-Reward
This model implements an asymmetric learning mechanism where the participant updates their values differently depending on whether the outcome was positive (reward) or negative (no reward). This captures potential optimism or pessimism biases.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model 3: Asymmetric Learning Rates (Pos/Neg).
    Updates Q-values differently for rewarded vs unrewarded trials.
    
    Bounds:
    alpha_pos: [0, 1] - Learning rate for positive prediction errors (rewards).
    alpha_neg: [0, 1] - Learning rate for negative prediction errors (omissions).
    beta: [0, 10] - Inverse temperature.
    """
    alpha_pos, alpha_neg, beta = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        a1 = action_1[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]
  
        # --- Updates ---
        
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        if delta_stage2 >= 0:
            eff_alpha_2 = alpha_pos
        else:
            eff_alpha_2 = alpha_neg
        q_stage2_mf[state_idx, a2] += eff_alpha_2 * delta_stage2
        
        # Stage 1 Update
        # Note: We use the updated stage 2 value to drive stage 1 (SARSA-like)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        
        # We apply the asymmetry to the stage 1 update as well based on the sign of its RPE
        if delta_stage1 >= 0:
            eff_alpha_1 = alpha_pos
        else:
            eff_alpha_1 = alpha_neg
            
        q_stage1_mf[a1] += eff_alpha_1 * delta_stage1
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```