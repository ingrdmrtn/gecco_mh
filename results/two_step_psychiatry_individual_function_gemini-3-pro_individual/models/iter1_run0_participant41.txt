Here are three new cognitive models for the two-step decision task, exploring different mechanisms for learning and decision-making.

### Model 1: Hybrid Learner with Separate Learning Rates
This model implements the classic "Daw et al. (2011)" style hybrid reinforcement learning, which mixes Model-Based (MB) and Model-Free (MF) values. Crucially, it uses **separate learning rates** for the first stage (spaceship choice) and the second stage (alien choice). This allows the model to capture participants who might update their beliefs about which spaceship is best at a different speed than their beliefs about which alien is best.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free learner with separate learning rates for each stage.
    
    This model mixes model-based planning (using the transition matrix) and model-free 
    TD learning. It allows for distinct update speeds for stage 1 (spaceships) and 
    stage 2 (aliens).

    Parameters:
    - lr_stage1: Learning rate for stage 1 MF values [0, 1].
    - lr_stage2: Learning rate for stage 2 MF values [0, 1].
    - beta: Inverse temperature for softmax choices [0, 10].
    - w: Mixing weight (0 = pure MF, 1 = pure MB) [0, 1].
    
    Bounds:
    lr_stage1: [0, 1]
    lr_stage2: [0, 1]
    beta: [0, 10]
    w: [0, 1]
    """
    lr_stage1, lr_stage2, beta, w = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition matrix: Row 0 -> [0.7 to Planet 0, 0.3 to Planet 1]
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)      # MF values for spaceships
    q_stage2_mf = np.zeros((2, 2)) # MF values for aliens (Planet x Alien)

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # Calculate Model-Based values: V_MB(s1) = T * max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Mix MB and MF values
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial] # Planet arrived at

        # --- Stage 2 Decision ---
        # Stage 2 is purely model-free (MB doesn't apply at the terminal step)
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning ---
        # 1. Update Stage 1 MF value using TD(0) error from the transition
        # The value of the state we landed in is the value of the action we took there
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr_stage1 * delta_stage1
        
        # 2. Update Stage 2 MF value using reward prediction error
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr_stage2 * delta_stage2
        
        # 3. Eligibility trace: The stage 2 RPE also updates stage 1 (TD(1))
        # We use lr_stage1 for this update as it pertains to the stage 1 value
        q_stage1_mf[action_1[trial]] += lr_stage1 * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Perseveration Model
This model is a pure Model-Free learner (similar to the one in your feedback) but adds a **perseveration parameter**. Participants often have a tendency to repeat their previous choice regardless of reward (stickiness), or sometimes switch away from it. This model captures that "choice autocorrelation" which is distinct from reward-driven learning.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Reinforcement Learning with Choice Perseveration (Stickiness).
    
    This model includes a 'stickiness' parameter that biases the agent to repeat 
    (or avoid) the immediately preceding choice at Stage 1, independent of the 
    reward history.
    
    Parameters:
    - learning_rate: Rate for updating values [0, 1].
    - beta: Inverse temperature [0, 10].
    - eligibility_trace: Strength of TD(1) update for stage 1 [0, 1].
    - stickiness: Bias to repeat the previous stage 1 choice. Positive values mean repetition, 
                  negative mean switching. Range is technically unbounded but typically [-5, 5]. 
                  For these bounds we map [0, 1] to a reasonable range or treat as magnitude.
                  Here we treat it as strictly positive repetition bias [0, 5].
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    eligibility_trace: [0, 1]
    stickiness: [0, 5]
    """
    learning_rate, beta, eligibility_trace, stickiness = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1 # Initialize with no previous action

    for trial in range(n_trials):

        # --- Stage 1 Decision ---
        # Add stickiness bonus to the Q-value of the previously chosen action
        q_net_stage1 = q_stage1_mf.copy()
        if last_action_1 != -1:
            q_net_stage1[last_action_1] += stickiness

        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Update tracker for next trial
        last_action_1 = action_1[trial]
        state_idx = state[trial]

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Eligibility trace update
        q_stage1_mf[action_1[trial]] += learning_rate * eligibility_trace * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Asymmetric Learning Rates (Positive vs Negative RPE)
This model investigates whether the participant learns differently from positive surprises (getting a coin when not expecting one) versus negative surprises (getting nothing when expecting a coin). It splits the learning rate into `alpha_pos` and `alpha_neg`. This asymmetry is a common finding in psychiatric populations (e.g., depression or anxiety).

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Learner with Asymmetric Learning Rates for Positive and Negative RPEs.
    
    This model allows the agent to update their values more strongly in response to 
    positive prediction errors (better than expected) or negative prediction errors 
    (worse than expected).
    
    Parameters:
    - alpha_pos: Learning rate when prediction error is >= 0 [0, 1].
    - alpha_neg: Learning rate when prediction error is < 0 [0, 1].
    - beta: Inverse temperature [0, 10].
    
    Bounds:
    alpha_pos: [0, 1]
    alpha_neg: [0, 1]
    beta: [0, 10]
    """
    alpha_pos, alpha_neg, beta = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Decision ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        if delta_stage1 >= 0:
            q_stage1_mf[action_1[trial]] += alpha_pos * delta_stage1
        else:
            q_stage1_mf[action_1[trial]] += alpha_neg * delta_stage1
            
        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        if delta_stage2 >= 0:
            q_stage2_mf[state_idx, action_2[trial]] += alpha_pos * delta_stage2
        else:
            q_stage2_mf[state_idx, action_2[trial]] += alpha_neg * delta_stage2
            
        # Note: No eligibility trace/lambda parameter here to keep parameter count low (3 params)
        # We assume simple TD(0) for stage 1 based on stage 2 value, but no direct reward propagation.

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```