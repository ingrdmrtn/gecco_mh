Here are three new cognitive models based on the two-step task paradigm, exploring different mechanisms like model-based planning, eligibility traces, and perseverance.

### Model 1: Hybrid Model-Based / Model-Free RL
This model implements the classic "hybrid" theory of the two-step task. It assumes the participant uses a weighted combination of a Model-Based (planning) strategy and a Model-Free (habitual) strategy. The Model-Based system uses the known transition matrix to plan, while the Model-Free system relies on TD-error updates.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free RL.
    The agent chooses stage 1 actions based on a weighted mix of model-based (planning) 
    and model-free (habitual) values.
    
    Bounds:
    learning_rate: [0, 1] - Learning rate for value updates.
    beta: [0, 10] - Inverse temperature for softmax.
    w: [0, 1] - Weighting parameter (0 = Pure MF, 1 = Pure MB).
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition matrix as per task description (70% common, 30% rare)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)       # Model-free values for stage 1 (A/U)
    q_stage2_mf = np.zeros((2, 2))  # Model-free values for stage 2 (Aliens on X/Y)

    for trial in range(n_trials):
        
        # --- Stage 1 Choice ---
        # 1. Calculate Model-Based values
        # Max Q-value at stage 2 for each state (planet)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        # Plan backwards using the transition matrix
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Combine MB and MF values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # 3. Softmax policy
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Choice ---
        state_idx = state[trial] # 0 for Planet X, 1 for Planet Y
        
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        # TD(0) update for Stage 1 MF values
        # Note: In pure Hybrid models, stage 1 MF is often updated by the stage 2 Q-value (SARSA-style)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # TD(0) update for Stage 2 MF values
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Model-Free RL with Eligibility Traces (TD(1))
This model assumes the participant is purely Model-Free but uses eligibility traces to connect the final reward directly back to the first-stage choice. Unlike simple TD(0) which only updates the stage 1 value based on the stage 2 value, TD(λ) (here simplified to λ=1) allows the reward outcome to directly reinforce the spaceship choice.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free RL with Eligibility Traces (TD(1)).
    The reward at the end of the trial directly updates the Stage 1 choice, 
    bypassing the need for the Stage 2 value to bridge the gap perfectly.
    
    Bounds:
    learning_rate: [0, 1] - Learning rate for value updates.
    beta: [0, 10] - Inverse temperature for softmax.
    lambda_eligibility: [0, 1] - Eligibility trace decay (0=TD(0), 1=Monte Carlo).
    """
    learning_rate, beta, lambda_eligibility = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        # Prediction error at stage 2 (Reward - Q_stage2)
        rpe_2 = reward[trial] - q_stage2[state_idx, action_2[trial]]
        
        # Update Stage 2 value
        q_stage2[state_idx, action_2[trial]] += learning_rate * rpe_2
        
        # Prediction error at stage 1 (Q_stage2 - Q_stage1)
        rpe_1 = q_stage2[state_idx, action_2[trial]] - q_stage1[action_1[trial]]
        
        # Update Stage 1 value using both the immediate step error (rpe_1)
        # AND the eligibility trace of the second step error (rpe_2)
        # This allows the final reward to 'leak' back to the first choice immediately.
        q_stage1[action_1[trial]] += learning_rate * rpe_1 + (learning_rate * lambda_eligibility * rpe_2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Model-Free RL with Choice Perseverance
This model adds a "perseverance" (or "stickiness") bonus to the basic Model-Free framework. Participants often repeat their previous choice regardless of reward (motor perseverance) or switch frequently. This model captures that tendency by adding a bonus to the Q-value of the previously chosen action at Stage 1.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free RL with Choice Perseverance.
    Includes a 'stickiness' parameter that biases the agent to repeat (or switch)
    the previous Stage 1 action, independent of value learning.
    
    Bounds:
    learning_rate: [0, 1] - Learning rate.
    beta: [0, 10] - Inverse temperature.
    perseverance: [0, 5] - Bonus added to the previously chosen action (positive=stick, negative=switch).
    """
    learning_rate, beta, perseverance = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    last_action_1 = -1 # Initialize with no previous action

    for trial in range(n_trials):

        # --- Stage 1 Choice ---
        # Calculate effective Q-values including perseverance bonus
        q_stage1_effective = q_stage1.copy()
        if last_action_1 != -1:
            q_stage1_effective[last_action_1] += perseverance

        exp_q1 = np.exp(beta * q_stage1_effective)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Store action for next trial's perseverance bonus
        last_action_1 = action_1[trial]
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # Standard TD(0) updates
        delta_stage1 = q_stage2[state_idx, action_2[trial]] - q_stage1[action_1[trial]]
        q_stage1[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2[state_idx, action_2[trial]]
        q_stage2[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```