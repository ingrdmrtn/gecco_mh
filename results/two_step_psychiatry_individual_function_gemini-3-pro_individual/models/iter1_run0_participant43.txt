Here are three cognitive models for the two-step decision task, ranging from pure model-based learning to a hybrid model mixing model-based and model-free strategies.

### Model 1: Pure Model-Based Learner
This model assumes the participant builds a mental map of the task structure. It learns the values of the aliens (Stage 2) directly from rewards. For Stage 1 choices, it calculates the expected value of each spaceship by combining the known transition probabilities with the learned values of the best alien on each planet. It does not use temporal difference learning for Stage 1.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Based Learner.
    
    This model calculates Stage 1 values by explicitly multiplying the 
    fixed transition matrix with the learned maximum values of the Stage 2 states.
    It does not cache Stage 1 values via TD learning.

    Parameters:
    - learning_rate: [0, 1] Rate at which Stage 2 Q-values are updated from rewards.
    - beta: [0, 10] Inverse temperature for softmax choice.
    """
    learning_rate, beta = model_parameters
    n_trials = len(action_1)
    
    # Transition matrix: A->X (0.7), A->Y (0.3); U->Y (0.7), U->X (0.3)
    # Rows are actions (0, 1), Cols are states (0, 1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Q-values for Stage 2 (aliens): 2 planets x 2 aliens
    q_stage2 = np.zeros((2, 2)) 
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        
        # --- Stage 1 Decision (Model-Based) ---
        # Calculate max value available in each state (planet)
        max_q_stage2 = np.max(q_stage2, axis=1)
        
        # Compute Q-values for stage 1 by planning: T * V(state)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        state_idx = state[trial] # Planet X (0) or Y (1)

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning ---
        # Only Stage 2 values are learned from experience here.
        # Stage 1 values are derived on the fly.
        prediction_error_2 = reward[trial] - q_stage2[state_idx, action_2[trial]]
        q_stage2[state_idx, action_2[trial]] += learning_rate * prediction_error_2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model-Based / Model-Free Learner
This is the classic "Daw et al. (2011)" style model. It maintains two sets of valuations for Stage 1: a Model-Free (MF) value learned via TD errors, and a Model-Based (MB) value calculated via planning. The final choice is a weighted sum of these two valuations controlled by a mixing parameter `w`.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Learner.
    
    Combines a Model-Free TD learner (sensitive to recent rewards) with a 
    Model-Based planner (sensitive to task structure).

    Parameters:
    - learning_rate: [0, 1] Update rate for Q-values.
    - beta: [0, 10] Inverse temperature for softmax.
    - w: [0, 1] Weighting parameter. 0 = Pure MF, 1 = Pure MB.
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # MF values
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # Used for both MF and MB calculations
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        
        # --- Stage 1 Decision (Hybrid) ---
        # 1. Model-Based Value
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Integrated Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning (TD-0) ---
        # Note: In a full hybrid model, Stage 1 MF is updated by the value of the 
        # chosen Stage 2 option (SARSA-style) or the best option (Q-learning).
        # Here we use the value of the chosen option (SARSA-like update for stage 1).
        
        # Stage 1 MF update
        # The "reward" for stage 1 is the value of the state reached in stage 2
        prediction_error_1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * prediction_error_1
        
        # Stage 2 MF update
        prediction_error_2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * prediction_error_2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Model-Free Learner with Eligibility Traces (TD-Lambda)
This model is purely model-free but adds "eligibility traces" (controlled by `lambda_param`). This allows the reward received at the very end of the trial (Stage 2) to directly update the value of the Stage 1 choice, bridging the gap between the first action and the final outcome without relying solely on the intermediate state value.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Learner with Eligibility Traces (TD-Lambda).
    
    Allows the reward at the second stage to directly influence the value 
    of the first stage choice, effectively 'smoothing' the credit assignment.

    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - lambda_param: [0, 1] Eligibility trace decay. 0 = TD(0), 1 = Monte Carlo.
    """
    learning_rate, beta, lambda_param = model_parameters
    n_trials = len(action_1)

    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):

        # --- Stage 1 Decision ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        state_idx = state[trial]

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning with Eligibility Trace ---
        
        # 1. Prediction Error at Stage 1 (TD error based on Stage 2 value)
        pe_1 = q_stage2[state_idx, action_2[trial]] - q_stage1[action_1[trial]]
        
        # 2. Prediction Error at Stage 2 (TD error based on Reward)
        pe_2 = reward[trial] - q_stage2[state_idx, action_2[trial]]
        
        # Update Stage 2 value (Standard TD)
        q_stage2[state_idx, action_2[trial]] += learning_rate * pe_2
        
        # Update Stage 1 value
        # It gets updated by its own immediate error (pe_1)
        # PLUS the error from the next step (pe_2) scaled by lambda
        q_stage1[action_1[trial]] += learning_rate * (pe_1 + lambda_param * pe_2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```