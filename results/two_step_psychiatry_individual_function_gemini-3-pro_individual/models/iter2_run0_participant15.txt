Here are 3 new cognitive models based on the two-step task structure, introducing mechanisms like eligibility traces, choice perseverance (stickiness), and asymmetric learning rates.

### Model 1: Model-Free with Eligibility Trace (TD($\lambda$))
This model assumes the participant is purely Model-Free (ignoring the transition matrix structure) but links the outcome at the second stage back to the first stage choice. The parameter $\lambda$ controls how much the second-stage prediction error updates the first-stage value.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Q-Learning with Eligibility Traces (TD-lambda).
    
    This model relies purely on temporal difference learning. However, it includes 
    an eligibility trace parameter (lambda). This allows the reward received at 
    the second stage to directly influence the value of the first stage choice, 
    bridging the gap between the initial decision and the final outcome.

    Parameters:
    learning_rate: [0, 1] - The rate at which Q-values are updated.
    beta: [0, 10] - Inverse temperature for softmax choice (exploration/exploitation).
    lambda_eligibility: [0, 1] - Decay factor for the eligibility trace. 
                                 0 = TD(0) (only immediate next state matters), 
                                 1 = Monte Carlo (outcome fully propagates back).
    """
    learning_rate, beta, lambda_eligibility = model_parameters
    n_trials = len(action_1)
  
    # Transition matrix is ignored in Pure MF, but defined for consistency
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)      # Values for choosing Spaceship A or U
    q_stage2_mf = np.zeros((2, 2)) # Values for aliens at Planet X and Y

    for trial in range(n_trials):

        # --- Policy for the first choice ---
        # Pure Model-Free: Choice depends only on learned MF values
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Policy for the second choice ---
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Value Updating ---
        
        # 1. Prediction Error Stage 1: Difference between Stage 2 value and Stage 1 expectation
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        # Update Stage 1 value (Standard TD(0) part)
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # 2. Prediction Error Stage 2: Difference between Reward and Stage 2 expectation
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Update Stage 2 value
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # 3. Eligibility Trace Update: Propagate Stage 2 error back to Stage 1
        # If lambda is high, the final reward strongly affects the spaceship choice
        q_stage1_mf[action_1[trial]] += learning_rate * lambda_eligibility * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model with Choice Stickiness
This model combines Model-Based and Model-Free learning (using a weighting parameter `w`), but adds a **stickiness** parameter. This accounts for the tendency of participants to repeat their previous choice regardless of value or reward, a common behavioral artifact in this task.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model (MB + MF) with Choice Stickiness (Perseveration).
    
    This model assumes the agent uses a weighted combination of Model-Based (planning)
    and Model-Free (habit) values. Additionally, it includes a 'stickiness' parameter
    which biases the agent to repeat the action taken on the immediately preceding trial,
    independent of reward history.

    Parameters:
    learning_rate: [0, 1] - Rate for updating MF Q-values.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - Weighting parameter. 1 = Pure MB, 0 = Pure MF.
    stickiness: [0, 5] - Additional bonus added to the logits of the previously chosen action.
    """
    learning_rate, beta, w, stickiness = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1 # Initialize as -1 (no previous choice)

    for trial in range(n_trials):

        # --- Policy for the first choice ---
        
        # 1. Model-Based Value: Transition prob * max value of next stage
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Hybrid Value: Weighted sum
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # 3. Add Stickiness (Perseveration)
        # We work with 'logits' before exponentiating
        logits = beta * q_net
        if prev_action_1 != -1:
            logits[prev_action_1] += stickiness

        exp_q1 = np.exp(logits)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Update tracker for next trial
        prev_action_1 = action_1[trial]
        
        state_idx = state[trial]

        # --- Policy for the second choice ---
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Value Updating ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Asymmetric Learning Rate Model-Free
This model posits that the participant learns differently from positive outcomes (finding gold) versus negative outcomes (no gold). This is a pure Model-Free learner that splits the learning rate into `alpha_pos` and `alpha_neg`.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Asymmetric Learning Rate Model (Pure Model-Free).
    
    This model assumes the agent updates their values with different intensities 
    depending on whether the prediction error was positive (better than expected) 
    or negative (worse than expected). This captures biases like optimism or 
    pessimism/loss aversion.

    Parameters:
    alpha_pos: [0, 1] - Learning rate when prediction error is positive (delta > 0).
    alpha_neg: [0, 1] - Learning rate when prediction error is negative (delta < 0).
    beta: [0, 10] - Inverse temperature.
    """
    alpha_pos, alpha_neg, beta = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Policy for the first choice ---
        # Pure Model-Free choice
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Policy for the second choice ---
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Value Updating with Asymmetry ---
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        if delta_stage1 > 0:
            q_stage1_mf[action_1[trial]] += alpha_pos * delta_stage1
        else:
            q_stage1_mf[action_1[trial]] += alpha_neg * delta_stage1
            
        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        if delta_stage2 > 0:
             q_stage2_mf[state_idx, action_2[trial]] += alpha_pos * delta_stage2
        else:
             q_stage2_mf[state_idx, action_2[trial]] += alpha_neg * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```