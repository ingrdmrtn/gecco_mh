Here are three new cognitive models exploring different mechanisms of learning and decision-making in the two-step task.

### Model 1: Hybrid Model-Based / Model-Free (Daw et al. 2011)
This is the canonical model for this task. It assumes the agent uses a mixture of two systems: a Model-Based (MB) system that uses the transition matrix to plan, and a Model-Free (MF) system that learns from direct experience. A mixing parameter `w` determines the balance.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Reinforcement Learning.
    
    This model combines a Model-Based (MB) controller, which calculates values based on 
    the transition structure and second-stage values, and a Model-Free (MF) controller 
    (TD(1) style). A mixing weight 'w' determines the influence of each system.

    Parameters:
    learning_rate: [0, 1] - Update rate for Q-values.
    beta: [0, 10] - Inverse temperature for softmax.
    w: [0, 1] - Mixing weight (0 = Pure MF, 1 = Pure MB).
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)

    # Fixed transition matrix as per task description (70% common, 30% rare)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)       # Model-free values for stage 1
    q_stage2_mf = np.zeros((2, 2))  # Values for stage 2 (aliens), shared by MB and MF
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # Model-Based Value Calculation: V_MB = Transition_Matrix * Max_Q_Stage2
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value: Q_net = w * Q_MB + (1-w) * Q_MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Decision ---
        # Standard softmax on stage 2 values
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning ---
        # Update Stage 2 Q-values (Standard Rescorla-Wagner)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Update Stage 1 MF Q-values (TD(1) / Direct Reinforcement)
        # Using reward directly to update stage 1 (skipping stage 2 prediction error for simplicity in this variant)
        delta_stage1 = reward[trial] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Perseveration Model (Habitual Repetition)
This model extends the basic learning framework by adding a "perseveration" parameter. This captures the tendency of participants to simply repeat their previous first-stage choice regardless of the reward outcome, a common behavioral tick often distinct from reinforcement learning.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free RL with Choice Perseveration.
    
    This model is a pure Model-Free learner (TD(1)) but adds a 'stickiness' or 
    perseveration parameter 'p'. This parameter biases the agent to repeat the 
    Action 1 choice made on the immediately preceding trial, independent of value.

    Parameters:
    learning_rate: [0, 1] - Update rate for Q-values.
    beta: [0, 10] - Inverse temperature for softmax.
    perseveration: [0, 5] - Bonus added to the Q-value of the previously chosen action.
    """
    learning_rate, beta, perseveration = model_parameters
    n_trials = len(action_1)

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1 # Initialize with no previous action

    for trial in range(n_trials):
        
        # --- Stage 1 Decision ---
        # Add perseveration bonus to the previously chosen action
        current_q_stage1 = q_stage1_mf.copy()
        if last_action_1 != -1:
            current_q_stage1[last_action_1] += perseveration

        exp_q1 = np.exp(beta * current_q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        
        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning ---
        # Update Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

        # Update Stage 1 (TD(1))
        delta_stage1 = reward[trial] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Store action for next trial's perseveration
        last_action_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Asymmetric Learning Rates (Positive vs. Negative)
This model hypothesizes that the participant learns differently from success (getting a coin) versus failure (getting zero coins). It splits the learning rate into `alpha_pos` and `alpha_neg`. This is often observed in clinical populations where sensitivity to reward or punishment differs.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free RL with Asymmetric Learning Rates.
    
    This model assumes the agent updates their beliefs differently depending on whether
    the outcome was positive (reward=1) or negative (reward=0). It uses two separate
    learning rates, alpha_pos and alpha_neg.

    Parameters:
    alpha_pos: [0, 1] - Learning rate for rewarded trials.
    alpha_neg: [0, 1] - Learning rate for unrewarded trials.
    beta: [0, 10] - Inverse temperature for softmax.
    """
    alpha_pos, alpha_neg, beta = model_parameters
    n_trials = len(action_1)

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # Determine which learning rate to use based on the outcome
        current_alpha = alpha_pos if reward[trial] == 1 else alpha_neg

        # --- Stage 1 Decision ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning ---
        # Update Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += current_alpha * delta_stage2

        # Update Stage 1 (TD(1))
        delta_stage1 = reward[trial] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += current_alpha * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```