Here are three new cognitive models based on the two-step task structure. I have focused on introducing mechanisms like **perseveration**, **eligibility traces**, and **separate learning rates** to diversify beyond the standard hybrid model.

### Model 1: Hybrid Model with Choice Perseveration
This model extends the standard hybrid learner by adding a "stickiness" or perseveration parameter. Participants often have a tendency to repeat their previous choice regardless of reward (habitual responding), or switch frequently. This model captures that variance.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free RL with Choice Perseveration.
    Adds a 'stickiness' parameter to the first-stage choice to capture repetition bias.

    Parameters:
    - learning_rate: [0, 1] Update rate for Q-values.
    - beta: [0, 10] Inverse temperature for softmax.
    - w: [0, 1] Weighting parameter (1 = MB, 0 = MF).
    - p_pers: [0, 5] Perseveration bonus added to the previously chosen action.
    """
    learning_rate, beta, w, p_pers = model_parameters
    n_trials = len(action_1)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    # Track previous choice for perseveration (initialize with -1 so no bonus on trial 0)
    prev_choice = -1

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add perseveration bonus to the net Q-values
        q_net_pers = q_net.copy()
        if prev_choice != -1:
            q_net_pers[prev_choice] += p_pers

        exp_q1 = np.exp(beta * q_net_pers)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Update previous choice
        prev_choice = action_1[trial]
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Value Updates ---
        # Stage 2 update (TD error)
        delta_stage2 = reward[trial] - q_stage2[state_idx, action_2[trial]]
        q_stage2[state_idx, action_2[trial]] += learning_rate * delta_stage2

        # Stage 1 update (TD error using Stage 2 value as proxy)
        delta_stage1 = q_stage2[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Pure Model-Free with Eligibility Traces (TD(lambda))
Instead of a hybrid model, this model assumes the participant is purely Model-Free but uses **eligibility traces**. This allows the reward at the second stage to directly influence the value of the first-stage choice, bridging the temporal gap more effectively than simple one-step TD learning. This is often called TD($\lambda$).

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free TD(lambda) Learning.
    Uses eligibility traces to update first-stage values based on second-stage outcomes directly.

    Parameters:
    - learning_rate: [0, 1] Update rate for Q-values.
    - beta: [0, 10] Inverse temperature for softmax.
    - lambd: [0, 1] Eligibility trace decay parameter. 
               (0 = standard TD, 1 = Monte Carlo-like updates).
    """
    learning_rate, beta, lambd = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Value Updates ---
        
        # 1. Prediction error at stage 1 (based on stage 2 value)
        # Note: In standard TD(lambda), this is often just 0 if Q(S2) is initialized to 0, 
        # but here we use the current estimate of Q(S2).
        delta1 = q_stage2[state_idx, action_2[trial]] - q_stage1[action_1[trial]]
        
        # 2. Prediction error at stage 2 (based on reward)
        delta2 = reward[trial] - q_stage2[state_idx, action_2[trial]]
        
        # Update Stage 2 Q-value
        q_stage2[state_idx, action_2[trial]] += learning_rate * delta2
        
        # Update Stage 1 Q-value
        # It gets updated by its own immediate error (delta1) AND a portion of the future error (delta2)
        # scaled by lambda. This connects the reward to the first choice.
        q_stage1[action_1[trial]] += learning_rate * (delta1 + lambd * delta2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model with Separate Learning Rates
This model hypothesizes that the participant might learn about the first stage (spaceships) and the second stage (aliens) at different speeds. For example, the aliens' reward probabilities drift slowly, requiring a higher learning rate, while the spaceship values might be more stable or treated differently.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free with Separate Learning Rates.
    Distinguishes between learning at Stage 1 (spaceships) and Stage 2 (aliens).

    Parameters:
    - lr_stage1: [0, 1] Learning rate for Stage 1 (MF values).
    - lr_stage2: [0, 1] Learning rate for Stage 2 (Alien values).
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weighting parameter (1 = MB, 0 = MF).
    """
    lr_stage1, lr_stage2, beta, w = model_parameters
    n_trials = len(action_1)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Value Updates ---
        
        # Stage 2 update uses lr_stage2
        delta_stage2 = reward[trial] - q_stage2[state_idx, action_2[trial]]
        q_stage2[state_idx, action_2[trial]] += lr_stage2 * delta_stage2

        # Stage 1 update uses lr_stage1
        delta_stage1 = q_stage2[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr_stage1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```