Here are three new cognitive models based on the two-step task structure, introducing mechanisms like choice stickiness (perseveration), eligibility traces, and stage-specific learning rates.

### Model 1: Hybrid Model with Choice Perseveration
This model extends the "best so far" Hybrid model by adding a "stickiness" or perseveration parameter. Participants often have a tendency to repeat their previous Stage 1 choice regardless of the outcome (motor perseveration) or due to cognitive inertia. This model captures that bias.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free RL with Choice Perseveration.
    
    Adds a 'stickiness' parameter to the standard hybrid model. This parameter 
    biases the agent to repeat the action taken in the previous trial, 
    independent of reward history.

    Parameters:
    learning_rate: [0, 1] Update rate for Q-values.
    beta: [0, 10] Inverse temperature for softmax (choice consistency).
    w: [0, 1] Weight for Model-Based control (1 = Pure MB, 0 = Pure MF).
    perseveration: [0, 5] Bonus added to the logits of the previously chosen action.
    """
    learning_rate, beta, w, perseveration = model_parameters
    n_trials = len(action_1)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)       
    q_stage2_mf = np.zeros((2, 2))  
    
    last_action_1 = -1 # Initialize with an impossible action

    for trial in range(n_trials):

        # --- POLICY STAGE 1 ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2

        # Combine MB and MF values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Apply perseveration bonus to the previously chosen action (if not first trial)
        logits_1 = beta * q_net
        if last_action_1 != -1:
            logits_1[last_action_1] += perseveration

        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial] 

        # --- POLICY STAGE 2 ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- UPDATES ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1

        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Store action for next trial's perseveration
        last_action_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Model-Free TD($\lambda$)
This model abandons the Model-Based component to focus on a more sophisticated Model-Free mechanism: Eligibility Traces (TD-Lambda). In a standard TD(0) model, the Stage 1 choice is only updated by the value of the Stage 2 state. In TD($\lambda$), the Stage 1 choice is *also* updated by the final reward received at the end of the trial, "bridging the gap" between the first choice and the final outcome.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free RL with Eligibility Traces (TD-Lambda).
    
    Instead of using a model-based planner, this model allows the reward prediction 
    error at the second stage to directly influence the value of the first stage 
    action, scaled by an eligibility trace parameter (lambda).

    Parameters:
    learning_rate: [0, 1] Update rate for Q-values.
    beta: [0, 10] Inverse temperature for softmax.
    lam: [0, 1] Eligibility trace decay (0 = TD(0), 1 = Monte Carlo).
    """
    learning_rate, beta, lam = model_parameters
    n_trials = len(action_1)
    
    # No transition matrix needed for pure MF
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)       
    q_stage2_mf = np.zeros((2, 2))  

    for trial in range(n_trials):

        # --- POLICY STAGE 1 ---
        # Pure Model-Free decision making
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial] 

        # --- POLICY STAGE 2 ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- UPDATES ---
        # 1. Prediction error at transition (Stage 1 -> Stage 2)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        # 2. Prediction error at reward (Stage 2 -> Reward)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Update Stage 1: Standard TD update + Eligibility Trace from Stage 2 error
        q_stage1_mf[action_1[trial]] += learning_rate * (delta_stage1 + lam * delta_stage2)

        # Update Stage 2: Standard TD update
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Dual Learning Rate Model
This model hypothesizes that learning happens at different speeds for different levels of the task hierarchy. The participant might update their beliefs about the spaceships (abstract cues, Stage 1) at a different rate than they update their beliefs about the aliens (proximal reward cues, Stage 2).

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Dual Learning Rate Model (Hierarchical Differentiation).
    
    This model assumes the participant utilizes different learning rates for 
    the two stages of the task. 'alpha_1' governs how quickly spaceship values 
    change, while 'alpha_2' governs how quickly alien values change. 
    It assumes a basic Model-Free architecture.

    Parameters:
    alpha_1: [0, 1] Learning rate for Stage 1 (Spaceships).
    alpha_2: [0, 1] Learning rate for Stage 2 (Aliens).
    beta: [0, 10] Inverse temperature for softmax.
    """
    alpha_1, alpha_2, beta = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- POLICY STAGE 1 ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- POLICY STAGE 2 ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- UPDATES ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        # Update Stage 1 using alpha_1
        q_stage1_mf[action_1[trial]] += alpha_1 * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        # Update Stage 2 using alpha_2
        q_stage2_mf[state_idx, action_2[trial]] += alpha_2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```