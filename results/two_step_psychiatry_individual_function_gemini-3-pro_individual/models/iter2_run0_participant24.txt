Here are three new cognitive models exploring different mechanisms for learning and decision-making in the two-step task.

### Model 1: Hybrid Learner with Separate Learning Rates
This model implements the classic "Hybrid" architecture, combining Model-Based (MB) and Model-Free (MF) reinforcement learning. The key innovation here relative to previous attempts is the use of **separate learning rates** for the two stages. This accounts for the possibility that the participant learns about the spaceship values (Stage 1) at a different speed than the alien values (Stage 2), perhaps due to the different cognitive demands or salience of the transitions versus the direct rewards.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free learner with separate learning rates for stages.
    
    This model assumes the agent mixes a model-based evaluation (using the transition matrix)
    and a model-free evaluation (TD learning) for the first stage choice. Crucially, it allows
    for different plasticity in stage 1 vs stage 2 updates.

    Parameters:
    lr_stage1: [0, 1] - Learning rate for stage 1 (spaceship) updates.
    lr_stage2: [0, 1] - Learning rate for stage 2 (alien) updates.
    beta: [0, 10] - Inverse temperature for softmax.
    w: [0, 1] - Mixing weight (0 = Pure MF, 1 = Pure MB).
    """
    lr_stage1, lr_stage2, beta, w = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition matrix as per task description:
    # A (0) -> X (0) with 0.7, Y (1) with 0.3
    # U (1) -> X (0) with 0.3, Y (1) with 0.7
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- STAGE 1 CHOICE ---
        # Model-Based Value: Expected value of next stage based on transitions
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value: Weighted sum of MB and MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- STAGE 2 CHOICE ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- LEARNING ---
        # Update Stage 1 MF value using prediction error from Stage 2 value
        # Note: Using max Q for SARSA-max (Q-learning) style update at stage 1
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr_stage1 * delta_stage1
        
        # Update Stage 2 MF value using reward prediction error
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr_stage2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Choice Stickiness (Perseveration)
This model is a pure Model-Free learner, but it adds a **stickiness** parameter. In behavioral tasks, participants often repeat their previous action regardless of the reward outcome (perseveration) or switch more often than expected (alternation). This model captures that tendency, which is distinct from reward-driven learning. It helps explain data where the participant (like the one in the example) repeatedly chooses the same spaceship (0) or alien.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free TD Learning with Choice Stickiness (Perseveration).
    
    Adds a 'stickiness' bonus to the action taken on the immediately preceding trial.
    This captures motor or cognitive inertia independent of value learning.

    Parameters:
    learning_rate: [0, 1] - Learning rate for Q-values.
    beta: [0, 10] - Inverse temperature.
    stickiness: [-5, 5] - Bonus added to the previous choice (positive = repeat, negative = switch).
    """
    learning_rate, beta, stickiness = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track previous choices (initialize with -1 for none)
    last_choice_1 = -1

    for trial in range(n_trials):

        # --- STAGE 1 CHOICE ---
        # Calculate effective values including stickiness
        q_eff_1 = q_stage1_mf.copy()
        if last_choice_1 != -1:
            q_eff_1[last_choice_1] += stickiness

        exp_q1 = np.exp(beta * q_eff_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Store choice for next trial stickiness
        last_choice_1 = action_1[trial]
        
        state_idx = state[trial]

        # --- STAGE 2 CHOICE ---
        # We apply stickiness only to stage 1 (common in literature), 
        # as stage 2 states change freqently.
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- LEARNING ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Decay Reinforcement Learning
This model posits that Q-values are not just updated by prediction errors but also naturally **decay** toward zero (or a neutral point) over time if an option is not chosen. This "forgetting" mechanism is biologically plausible and helps explain behavior where participants re-explore options they haven't tried in a while, or where values become less certain over time.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Learning with Value Decay.
    
    Unchosen options slowly decay toward 0. This simulates forgetting or 
    uncertainty growth over time.
    
    Parameters:
    learning_rate: [0, 1] - Rate for updating chosen options.
    beta: [0, 10] - Inverse temperature.
    decay_rate: [0, 1] - Rate at which ALL Q-values decay per trial (1 = no decay, 0 = instant forgetting).
    """
    learning_rate, beta, decay_rate = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- STAGE 1 CHOICE ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- STAGE 2 CHOICE ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- LEARNING ---
        # Standard TD Update
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

        # --- DECAY ---
        # Apply decay to all Q-values at the end of the trial
        # This pulls values back towards 0
        q_stage1_mf *= decay_rate
        q_stage2_mf *= decay_rate
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```