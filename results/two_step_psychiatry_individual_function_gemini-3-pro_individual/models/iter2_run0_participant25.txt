Here are 3 new cognitive models that explore different mechanisms than the standard hybrid model.

### Model 1: Perseveration + Model-Based Learning
This model hypothesizes that the participant relies on a Model-Based strategy (planning) but is also influenced by a "stickiness" or perseveration bias. They tend to repeat their previous Stage 1 choice regardless of the outcome, a common heuristic in behavioral data.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Model-Based Reinforcement Learning with Perseveration (Stickiness).

    This model assumes the agent is purely model-based (planning using the transition matrix)
    but has a bias to repeat the last chosen action at stage 1 (perseveration).

    Parameters:
    - learning_rate: [0, 1] Rate at which Q-values are updated.
    - beta: [0, 10] Inverse temperature for softmax choices.
    - perseveration: [0, 5] Bonus added to the Q-value of the previously chosen action.
    """
    learning_rate, beta, perseveration = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values for the second stage (aliens)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track previous choice for perseveration
    last_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Policy (Model-Based + Perseveration) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Add perseveration bonus if a previous action exists
        q_net = q_stage1_mb.copy()
        if last_action_1 != -1:
            q_net[last_action_1] += perseveration

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Update tracker
        last_action_1 = action_1[trial]
        state_idx = state[trial]

        # --- Stage 2 Policy (Standard Softmax) ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning (Only updating Stage 2 values) ---
        # Note: In a pure MB model, Stage 1 values are derived, not learned directly via TD.
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Separate Learning Rates for Positive/Negative Outcomes (Asymmetric Learning)
This model investigates valence-dependent learning. The participant might update their beliefs differently when they get a reward (gold coin) versus when they get nothing. This is a purely Model-Free approach but with split learning rates.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Q-Learning with Asymmetric Learning Rates.

    This model assumes the agent is purely model-free (TD learning) but learns 
    differently from positive prediction errors (rewards) vs negative prediction errors (omissions).

    Parameters:
    - alpha_pos: [0, 1] Learning rate for positive prediction errors (delta > 0).
    - alpha_neg: [0, 1] Learning rate for negative prediction errors (delta < 0).
    - beta: [0, 10] Inverse temperature for softmax choices.
    """
    alpha_pos, alpha_neg, beta = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning Stage 1 ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        if delta_stage1 > 0:
            q_stage1_mf[action_1[trial]] += alpha_pos * delta_stage1
        else:
            q_stage1_mf[action_1[trial]] += alpha_neg * delta_stage1

        # --- Learning Stage 2 ---
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        if delta_stage2 > 0:
            q_stage2_mf[state_idx, action_2[trial]] += alpha_pos * delta_stage2
        else:
            q_stage2_mf[state_idx, action_2[trial]] += alpha_neg * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Eligibility Traces (TD(lambda))
This model posits that the credit for the reward is "passed back" to the first stage choice more directly than standard Q-learning. Instead of just updating Stage 1 based on Stage 2's value, the reward at Stage 2 directly influences Stage 1 values via an eligibility trace parameter `lambda`.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free TD(lambda) Learning.

    This model uses eligibility traces to allow the reward at the second stage 
    to directly update the value of the first stage choice, bridging the temporal gap.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - lambda_trace: [0, 1] Decay rate of the eligibility trace. 
                    0 = standard TD (SARSA), 1 = Monte Carlo-like updates.
    """
    learning_rate, beta, lambda_trace = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Update Stage 1 (TD Error) ---
        # The immediate reward for stage 1 is 0. The "value" of the next state is Q(s', a')
        delta_stage1 = (0 + q_stage2_mf[state_idx, action_2[trial]]) - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1

        # --- Update Stage 2 ---
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

        # --- Eligibility Trace Update for Stage 1 ---
        # The Stage 1 choice also receives a portion of the Stage 2 prediction error
        # driven by the lambda parameter.
        q_stage1_mf[action_1[trial]] += learning_rate * lambda_trace * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```