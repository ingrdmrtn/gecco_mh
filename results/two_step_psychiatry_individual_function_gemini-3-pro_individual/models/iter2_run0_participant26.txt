Here are 3 new cognitive models based on the two-step decision task, designed to capture different psychological mechanisms beyond standard reinforcement learning.

### Model 1: Hybrid Model-Based/Model-Free RL with Eligibility Traces
This model introduces an eligibility trace parameter (`lambda`), which bridges the gap between the first-stage choice and the second-stage outcome. Instead of only updating the first stage based on the second stage's value (TD(0)), this model allows the final reward to directly influence the first-stage values, a key feature of temporal difference learning algorithms like SARSA($\lambda$).

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based/Model-Free RL with Eligibility Traces.
    
    This model allows the reward at the second stage to directly update the 
    first-stage Q-values via an eligibility trace parameter (lam). It mixes 
    model-based planning (using the transition matrix) with model-free learning.

    Parameters:
    - learning_rate: Update rate for Q-values.
    - beta: Inverse temperature for softmax choice.
    - w: Weighting parameter (0 = pure Model-Free, 1 = pure Model-Based).
    - lam: Eligibility trace decay (0 = TD(0), 1 = Monte Carlo).
    
    Bounds:
    learning_rate: [0,1]
    beta: [0,10]
    w: [0,1]
    lam: [0,1]
    """
    learning_rate, beta, w, lam = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based Value Calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial] # 0 or 1 (Planet X or Y)

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        # Stage 1 MF Update (TD error using Stage 2 Q-value)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Stage 2 MF Update (Reward prediction error)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Eligibility Trace Update: Propagate Stage 2 reward error back to Stage 1
        # This connects the final reward to the initial spaceship choice directly.
        q_stage1_mf[action_1[trial]] += learning_rate * lam * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Asymmetric Learning Rates (Reward vs. Punishment)
This model hypothesizes that the participant learns differently from positive outcomes (finding gold) versus negative outcomes (no gold). This "positivity bias" or "negativity bias" is common in clinical populations. It uses two separate learning rates: `alpha_pos` for when `delta > 0` and `alpha_neg` for when `delta < 0`.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Asymmetric Learning Rates (Reward vs. Punishment).
    
    This model differentiates between learning from positive prediction errors 
    (better than expected) and negative prediction errors (worse than expected).
    
    Parameters:
    - alpha_pos: Learning rate for positive prediction errors.
    - alpha_neg: Learning rate for negative prediction errors.
    - beta: Inverse temperature.
    - perseverance: Sticky choice bonus for the first stage.
    
    Bounds:
    alpha_pos: [0,1]
    alpha_neg: [0,1]
    beta: [0,10]
    perseverance: [0,5]
    """
    alpha_pos, alpha_neg, beta, perseverance = model_parameters
    n_trials = len(action_1)
  
    # No transition matrix needed as this is a pure MF model with asymmetry
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        q_effective = q_stage1_mf.copy()
        if last_action_1 != -1:
            q_effective[last_action_1] += perseverance
            
        exp_q1 = np.exp(beta * q_effective)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_action_1 = action_1[trial]
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        lr_2 = alpha_pos if delta_stage2 > 0 else alpha_neg
        q_stage2_mf[state_idx, action_2[trial]] += lr_2 * delta_stage2
        
        # Stage 1 Update (using updated stage 2 value)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        lr_1 = alpha_pos if delta_stage1 > 0 else alpha_neg
        q_stage1_mf[action_1[trial]] += lr_1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Decay-Based Reinforcement Learning
Instead of standard Q-learning where unchosen options remain static, this model implements a "forgetting" or decay mechanism. On every trial, the values of unchosen actions decay toward a baseline (0.5), simulating memory loss or a return to priors. This captures the phenomenon where participants might re-explore an option simply because they haven't tried it in a while and have forgotten its value.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Decay-Based Reinforcement Learning.
    
    This model assumes that Q-values for unchosen actions decay over time 
    towards a neutral value (0.5), simulating forgetting.
    
    Parameters:
    - learning_rate: Update rate for chosen actions.
    - decay_rate: Rate at which unchosen Q-values decay to 0.5.
    - beta: Inverse temperature.
    - w: Weighting parameter (0 = pure Model-Free, 1 = pure Model-Based).
    
    Bounds:
    learning_rate: [0,1]
    decay_rate: [0,1]
    beta: [0,10]
    w: [0,1]
    """
    learning_rate, decay_rate, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Initialize Q-values at 0.5 (neutral expectation)
    q_stage1_mf = np.full(2, 0.5)
    q_stage2_mf = np.full((2, 2), 0.5)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        # Stage 1: Update chosen, decay unchosen
        chosen_1 = action_1[trial]
        unchosen_1 = 1 - chosen_1
        
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[chosen_1]
        q_stage1_mf[chosen_1] += learning_rate * delta_stage1
        q_stage1_mf[unchosen_1] += decay_rate * (0.5 - q_stage1_mf[unchosen_1]) # Decay to 0.5

        # Stage 2: Update chosen, decay unchosen (only for the visited state)
        chosen_2 = action_2[trial]
        unchosen_2 = 1 - chosen_2
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, chosen_2]
        q_stage2_mf[state_idx, chosen_2] += learning_rate * delta_stage2
        q_stage2_mf[state_idx, unchosen_2] += decay_rate * (0.5 - q_stage2_mf[state_idx, unchosen_2])

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```