Here are three new cognitive models for the two-step decision task.

### Model 1: Pure Model-Free TD Learning (SARSA-like)
This model assumes the participant ignores the transition structure of the task entirely. Instead of planning based on the transition probabilities (Model-Based), they learn the value of the first-stage spaceships solely based on the reward prediction errors experienced. This is a standard Model-Free Temporal Difference (TD) learning approach.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free Reinforcement Learning (TD-0).
    
    The agent learns values for both the first stage (spaceships) and second stage (aliens)
    purely through temporal difference errors. It does not use the transition matrix
    to plan. The value of a spaceship is updated based on the value of the state reached.

    Parameters:
    - learning_rate: [0, 1] Rate at which Q-values are updated.
    - beta: [0, 10] Inverse temperature for softmax choice.
    """
    learning_rate, beta = model_parameters
    n_trials = len(action_1)
  
    # No transition matrix needed for pure MF
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values for stage 1 (2 spaceships) and stage 2 (2 planets x 2 aliens)
    q_stage1_mf = np.zeros(2) 
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Policy for the first choice (Stage 1) ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial] # Actual planet arrived at

        # --- Policy for the second choice (Stage 2) ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Value Updating ---
        
        # TD error for stage 1: Difference between value of state reached and chosen spaceship value
        # Note: We use the value of the chosen option in stage 2 as the target (SARSA-style logic)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # TD error for stage 2: Difference between reward received and chosen alien value
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model (Model-Based + Model-Free) with Mixing Parameter
This is the classic "Daw et al. (2011)" hybrid model. It assumes the participant computes both a Model-Based value (using the transition matrix) and a Model-Free value (using TD errors) for the first stage. The final decision value is a weighted combination of both, controlled by a mixing parameter `w`.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Reinforcement Learning.
    
    The agent computes Q-values for the first stage using both a Model-Based strategy 
    (planning via transition matrix) and a Model-Free strategy (TD learning). 
    The final Q-value is a weighted average determined by parameter 'w'.

    Parameters:
    - learning_rate: [0, 1] Update rate for Q-values.
    - beta: [0, 10] Inverse temperature for softmax.
    - w: [0, 1] Mixing weight. 1 = Pure Model-Based, 0 = Pure Model-Free.
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # Used by both MB and MF systems for terminal values

    for trial in range(n_trials):

        # --- Policy for the first choice ---
        # 1. Calculate Model-Based values (planning)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Combine MB and MF values
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Policy for the second choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Value Updating ---
        # Update Stage 1 MF value using TD error
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Update Stage 2 values (common to both systems)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Model-Free with Choice Perseveration
This model builds on the simple Model-Free learner (Model 1) but adds a "perseveration" parameter. This captures the tendency of participants to simply repeat their previous Stage 1 choice regardless of reward history (motor repetition or "stickiness").

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Reinforcement Learning with Choice Perseveration.
    
    Standard TD learning, but the Stage 1 choice probability is biased by 
    which action was taken on the immediately preceding trial. 
    A positive 'perseveration' parameter increases the likelihood of repeating the action.

    Parameters:
    - learning_rate: [0, 1] Update rate for Q-values.
    - beta: [0, 10] Inverse temperature.
    - perseveration: [0, 5] Bonus added to the Q-value of the previously chosen action.
    """
    learning_rate, beta, perseveration = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1 # Initialize with an impossible action

    for trial in range(n_trials):

        # --- Policy for the first choice ---
        # Add perseveration bonus to the Q-values locally for choice calculation
        current_q_stage1 = q_stage1_mf.copy()
        if last_action_1 != -1:
            current_q_stage1[last_action_1] += perseveration
            
        exp_q1 = np.exp(beta * current_q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Track last action
        last_action_1 = action_1[trial]
        
        state_idx = state[trial]

        # --- Policy for the second choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Value Updating ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```