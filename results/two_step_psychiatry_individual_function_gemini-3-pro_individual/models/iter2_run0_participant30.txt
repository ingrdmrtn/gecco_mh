Here are three new cognitive models for the two-step task. These models move beyond the pure model-based approach and the standard hybrid model by exploring different mechanisms for learning and choice persistence.

### Model 1: Hybrid Model with Eligibility Traces (TD(lambda))
This model introduces an eligibility trace parameter (`lambda_eligibility`). Instead of strictly separating model-based and model-free updates, it uses a TD(lambda) approach. This allows the reward at the second stage to update the value of the first-stage choice directly, scaled by `lambda`. This bridges the gap between simple TD(0) (pure Model-Free) and Monte Carlo methods, offering a different way to credit assignment.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Eligibility Traces (TD-lambda).
    
    This model implements a temporal difference learning algorithm with eligibility traces.
    The parameter lambda controls how much the second-stage reward directly affects
    the first-stage value.
    
    Parameters:
    learning_rate: [0,1] - Update rate for Q-values.
    beta: [0,10] - Inverse temperature for softmax.
    w: [0,1] - Weight mixing Model-Based (1) and Model-Free (0) values for Stage 1 choice.
    lambda_eligibility: [0,1] - Eligibility trace decay parameter.
    """
    learning_rate, beta, w, lambda_eligibility = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Choice ---
        # Model-Based Value Calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax Policy
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        # Stage 1 TD error (SARSA style prediction error)
        # Note: We use the value of the state actually reached (state_idx)
        # and the max value of that state as a proxy for V(s')
        v_stage2 = np.max(q_stage2_mf[state_idx]) 
        delta_stage1 = v_stage2 - q_stage1_mf[action_1[trial]]
        
        # Stage 2 TD error
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]

        # Update Stage 1 MF value using both immediate error and eligibility trace of stage 2 error
        q_stage1_mf[action_1[trial]] += learning_rate * (delta_stage1 + lambda_eligibility * delta_stage2)
        
        # Update Stage 2 MF value
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Model-Free with Choice Perseveration
This model abandons the model-based component entirely to test if a simpler heuristic explains the data better. It combines standard Model-Free Q-learning with a "stickiness" or perseveration parameter (`perseveration`). This parameter captures the tendency to repeat the previous action regardless of reward (motor repetition), which is often a strong signal in behavioral data that can be conflated with learning.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free Q-Learning with Choice Perseveration.
    
    This model assumes no knowledge of the transition structure. It relies solely
    on caching values from rewards. It adds a perseveration bonus to the
    previously chosen action at Stage 1, capturing 'stickiness'.

    Parameters:
    learning_rate: [0,1] - Update rate for Q-values.
    beta: [0,10] - Inverse temperature for softmax.
    perseveration: [0,5] - Bonus added to the logits of the previously chosen action (Stage 1).
    """
    learning_rate, beta, perseveration = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):

        # --- Stage 1 Choice ---
        # Add perseveration bonus to Q-values temporarily for decision making
        q_stage1_decision = q_stage1_mf.copy()
        if last_action_1 != -1:
            q_stage1_decision[last_action_1] += perseveration

        exp_q1 = np.exp(beta * q_stage1_decision)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        last_action_1 = action_1[trial] # Update tracker

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        # Standard Q-learning (SARSA(0)) logic
        # Stage 1 update: driven by the value of the state reached (max Q of stage 2)
        v_stage2_state = np.max(q_stage2_mf[state_idx])
        delta_stage1 = v_stage2_state - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Stage 2 update: driven by reward
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model with Separate Learning Rates
This model hypothesizes that the participant learns about the two stages of the task at different speeds. It implements a Hybrid (Model-Based/Model-Free) architecture but splits the learning rate into `lr_stage1` and `lr_stage2`. This allows the model to capture behavior where, for example, the participant quickly updates their preferences for aliens (Stage 2) but is slow to update the cached value of the spaceships (Stage 1), or vice versa.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Separate Learning Rates per Stage.
    
    This model distinguishes between the speed of learning for the first stage
    (spaceship choice) and the second stage (alien choice). It mixes MB and MF
    values for the first stage.

    Parameters:
    lr_stage1: [0,1] - Learning rate for Stage 1 MF values.
    lr_stage2: [0,1] - Learning rate for Stage 2 MF values.
    beta: [0,10] - Inverse temperature.
    w: [0,1] - Weight mixing Model-Based (1) and Model-Free (0).
    """
    lr_stage1, lr_stage2, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        # Update Stage 1 MF using lr_stage1
        # We use the value of the state reached (max Q of stage 2) as the target
        v_next = np.max(q_stage2_mf[state_idx])
        delta_stage1 = v_next - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr_stage1 * delta_stage1
        
        # Update Stage 2 MF using lr_stage2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr_stage2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```