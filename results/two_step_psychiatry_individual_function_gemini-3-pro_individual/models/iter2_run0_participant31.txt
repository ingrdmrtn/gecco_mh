Here are three new cognitive models based on the two-step task structure, exploring different mechanisms like model-based planning, eligibility traces, and perseverance.

### Model 1: Pure Model-Based Learner
This model assumes the participant builds an internal map of the task structure (the transition matrix) and uses it to calculate the expected value of the first-stage choices based on the learned values of the second-stage options. It ignores the direct model-free value of the first stage.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Based Learner.
    
    This model calculates the value of first-stage actions (spaceships) strictly by 
    combining the known transition probabilities with the learned values of the 
    second-stage states (planets). It does not maintain a separate cache of 
    first-stage values (Q_MF1).

    Parameters:
    learning_rate: [0,1] Rate at which second-stage Q-values are updated.
    beta: [0,10] Inverse temperature for softmax choice.
    """
    learning_rate, beta = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition structure: Spaceship 0 -> Planet 0 (0.7), Spaceship 1 -> Planet 1 (0.7)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # We only need Q-values for the second stage (aliens on planets)
    q_stage2_mf = np.zeros((2, 2)) # (Planet, Alien)

    for trial in range(n_trials):

        # --- Stage 1 Policy (Model-Based) ---
        # 1. Determine the best value available on each planet
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        
        # 2. Calculate Model-Based values for Stage 1 actions
        # V(Action) = P(Planet0|Action)*V(Planet0) + P(Planet1|Action)*V(Planet1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 3. Softmax
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial] # The planet actually reached

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Value Updating ---
        # Only update the second stage values based on reward
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Learner (Model-Based + Model-Free)
This is the classic "Daw et al. (2011)" style model. It assumes the brain computes *both* a model-free value (based on past experience) and a model-based value (based on planning) for the first stage, and combines them using a weighting parameter `w`.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner (Model-Based + Model-Free).
    
    This model computes a weighted average of Model-Based and Model-Free values 
    to make the first-stage decision.
    
    Parameters:
    learning_rate: [0,1] Update rate for Q-values.
    beta: [0,10] Softmax inverse temperature.
    w: [0,1] Weighting parameter (0 = Pure MF, 1 = Pure MB).
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy (Hybrid) ---
        # Model-Based Component
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combine MB and MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Value Updating ---
        # TD(0) update for Stage 1 (Model-Free path)
        # Note: We use the value of the CHOSEN second stage action, not the max
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Update for Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Model-Free Learner with Eligibility Traces (TD(lambda))
This model is purely model-free but adds an "eligibility trace" (lambda). Instead of just updating the first stage based on the second stage value, the reward received at the *end* of the trial "bleeds through" directly to update the first stage choice. This connects the final outcome to the initial choice more strongly than simple TD(0).

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Learner with Eligibility Traces (TD(lambda)).
    
    The first stage choice is updated not just by the transition to the second stage,
    but also directly by the final reward, scaled by lambda.
    
    Parameters:
    learning_rate: [0,1] Update rate.
    beta: [0,10] Softmax inverse temperature.
    lambd: [0,1] Eligibility trace decay (0=TD(0), 1=Monte Carlo).
    """
    learning_rate, beta, lambd = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Value Updating ---
        
        # 1. Prediction error at stage 1 (based on value of state 2)
        delta_1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_1
        
        # 2. Prediction error at stage 2 (based on reward)
        delta_2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_2
        
        # 3. Eligibility Trace Update:
        # Apply the stage 2 prediction error (delta_2) to the stage 1 choice, 
        # scaled by lambda. This allows the reward to directly influence the first choice.
        q_stage1_mf[action_1[trial]] += learning_rate * lambd * delta_2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```