Here are three new cognitive models exploring different mechanisms for learning and decision-making in the two-step task.

### Model 1: Separate Learning Rates for Positive and Negative Outcomes
This model is a hybrid reinforcement learning model (mixing model-based and model-free values) but introduces an asymmetry in learning. It uses two separate learning rates: `alpha_pos` for trials where a reward was received, and `alpha_neg` for trials where no reward was received. This captures the possibility that the participant learns differently from success versus failure.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free RL with Asymmetric Learning Rates.
    
    This model distinguishes between learning from positive outcomes (rewards)
    and negative outcomes (omissions). This is often observed in clinical
    populations or specific individuals who may be hypersensitive to reward
    or punishment.

    Parameters:
    alpha_pos: [0, 1] - Learning rate for rewarded trials.
    alpha_neg: [0, 1] - Learning rate for unrewarded trials.
    beta: [0, 10] - Inverse temperature for softmax choice.
    w: [0, 1] - Weighting parameter (0 = pure Model-Free, 1 = pure Model-Based).
    """
    alpha_pos, alpha_neg, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2

        # Weighted integration of MB and MF values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updating ---
        # Determine which learning rate to use based on the reward
        current_alpha = alpha_pos if reward[trial] == 1 else alpha_neg

        # TD Error Stage 1 (SARSA-style update using the Q-value of the state actually reached)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += current_alpha * delta_stage1
        
        # TD Error Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += current_alpha * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Pure Model-Free with Eligibility Traces (TD(lambda))
This model abandons the hybrid MB/MF architecture and instead uses a pure Model-Free approach with an eligibility trace parameter (`lambda`). This parameter controls how much the Stage 2 reward outcome directly updates the Stage 1 choice. If `lambda` is 1, the full reward prediction error from the second stage propagates back to the first stage immediately. If `lambda` is 0, Stage 1 is only updated by the value of the Stage 2 state (standard TD learning).

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free RL with Eligibility Traces (TD-lambda).
    
    Instead of explicitly calculating transition matrices (Model-Based), 
    this model allows the reward at the end of the trial to directly influence 
    the value of the first-stage choice via an eligibility trace.
    
    Parameters:
    learning_rate: [0, 1] - Rate at which Q-values are updated.
    beta: [0, 10] - Inverse temperature for softmax choice.
    lam: [0, 1] - Eligibility trace decay parameter (lambda). 
                  Controls how much credit the first action gets for the final reward.
    perseveration: [0, 5] - Sticky choice bonus for Stage 1.
    """
    learning_rate, beta, lam, perseveration = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Simple Q-values for Stage 1 (2 actions) and Stage 2 (2 states x 2 actions)
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        q_net_1 = q_stage1.copy()
        if last_action_1 != -1:
            q_net_1[last_action_1] += perseveration

        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updating ---
        
        # 1. Prediction error at transition (Stage 1 -> Stage 2)
        # The value of the chosen Stage 2 action is the target
        delta_1 = q_stage2[state_idx, action_2[trial]] - q_stage1[action_1[trial]]
        
        # 2. Prediction error at outcome (Stage 2 -> Reward)
        delta_2 = reward[trial] - q_stage2[state_idx, action_2[trial]]
        
        # Update Stage 2 Q-values (standard TD)
        q_stage2[state_idx, action_2[trial]] += learning_rate * delta_2
        
        # Update Stage 1 Q-values
        # It learns from the transition error (delta_1) AND a portion of the outcome error (delta_2)
        # determined by lambda.
        q_stage1[action_1[trial]] += learning_rate * (delta_1 + lam * delta_2)
        
        last_action_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model with Decay
This model assumes that Q-values are not perfectly maintained but decay over time toward zero (forgetting). This is relevant in environments where reward probabilities drift, or if the participant has limited working memory capacity. It adds a `decay` parameter.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free RL with Memory Decay.
    
    This model introduces a forgetting mechanism. At each trial, unchosen 
    Q-values decay towards 0 (or a neutral value), simulating memory loss 
    or the assumption of environmental volatility.

    Parameters:
    learning_rate: [0, 1] - Rate at which Q-values are updated.
    beta: [0, 10] - Inverse temperature for softmax choice.
    w: [0, 1] - Weighting parameter (0 = pure Model-Free, 1 = pure Model-Based).
    decay: [0, 1] - Rate at which Q-values decay per trial (1 = no decay, 0 = full reset).
    """
    learning_rate, beta, w, decay = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updating ---
        
        # Standard Hybrid updates
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # --- Decay ---
        # Apply decay to all Q-values to simulate forgetting/volatility
        # Note: We apply decay *after* the update for the current trial
        q_stage1_mf *= decay
        q_stage2_mf *= decay

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```