Here are three cognitive models representing different strategies for the two-step decision task.

### Model 1: Pure Model-Based Reinforcement Learning
This model assumes the participant fully utilizes the structure of the task. Instead of learning the value of the spaceships (Stage 1) directly from reward history (like model-free algorithms), it calculates the value of spaceships by combining the known transition probabilities with the learned values of the aliens (Stage 2).

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Based Reinforcement Learning Model.
    
    This model computes Stage 1 values (spaceships) by explicitly planning forward.
    It combines the fixed transition matrix with the learned Q-values of the aliens 
    (Stage 2) to calculate the expected value of each spaceship. Stage 2 values are
    learned via simple Rescorla-Wagner updating.
    
    Parameters:
    - learning_rate: Rate at which Stage 2 Q-values update [0, 1].
    - beta: Inverse temperature for softmax choice [0, 10].
    """
    learning_rate, beta = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition structure: 
    # Row 0: Spaceship A -> 70% Planet X (0), 30% Planet Y (1)
    # Row 1: Spaceship U -> 30% Planet X (0), 70% Planet Y (1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # We only track Q-values for the second stage (aliens)
    q_stage2_mf = np.zeros((2, 2)) # (Planet, Alien)

    for trial in range(n_trials):
        # --- Stage 1 Policy (Model-Based) ---
        # Calculate expected value of best action at Stage 2
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        
        # Bellman equation: Q_MB(s1, a) = sum(P(s2|s1,a) * max(Q(s2, a')))
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial] # Planet arrived at

        # --- Stage 2 Policy ---
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updating ---
        # Only update Stage 2 values based on reward
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model-Based / Model-Free (Dyna-style)
This is the classic "Two-Step" model often used in psychiatry research. It assumes the brain maintains two separate valuation systems: a Model-Free system (TD learning) and a Model-Based system (planning). The final decision at Stage 1 is a weighted combination of both systems.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Model.
    
    This model calculates Stage 1 Q-values as a weighted sum of a Model-Free (TD)
    component and a Model-Based (Planning) component. A mixing parameter 'w' 
    determines the balance between habits (MF) and goal-directed planning (MB).
    
    Parameters:
    - learning_rate: Rate for Q-value updating [0, 1].
    - beta: Inverse temperature [0, 10].
    - w: Mixing weight [0, 1]. 0 = Pure MF, 1 = Pure MB.
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy (Hybrid) ---
        # 1. Calculate Model-Based values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Combine MB and MF values
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updating ---
        # SARSA / TD(0) update for Stage 1 MF values
        # Note: We use the value of the chosen stage 2 action, not the max (SARSA-like)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Prediction error update for Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Model-Free with Eligibility Traces (TD(lambda))
This model is purely model-free but adds complexity to how credit is assigned. Instead of just updating the immediately preceding state, it uses an eligibility trace (controlled by lambda) to allow the reward at the very end of the trial to directly update the value of the spaceship chosen at the very beginning, bypassing the intermediate state. This can mimic some model-based behaviors without an explicit transition model.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free TD(lambda) Learning.
    
    This model uses eligibility traces to update Stage 1 values based on the final
    reward outcome. This allows 'long-distance' credit assignment.
    
    Parameters:
    - learning_rate: Rate for Q-value updating [0, 1].
    - beta: Inverse temperature [0, 10].
    - lam: Lambda (eligibility trace decay) [0, 1]. 
           0 = TD(0) (only immediate next state updates prev state), 
           1 = Monte Carlo (outcome fully updates all previous states).
    """
    learning_rate, beta, lam = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updating with Eligibility Trace ---
        
        # 1. Calculate Prediction Error at Stage 1 (based on Stage 2 value)
        # This is the immediate surprise of landing in a specific state
        delta_1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        # 2. Calculate Prediction Error at Stage 2 (based on Reward)
        delta_2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # 3. Update Stage 2 Q-values (standard TD)
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_2
        
        # 4. Update Stage 1 Q-values (TD(lambda))
        # The update includes the immediate error (delta_1) PLUS a portion of the 
        # second stage error (delta_2) carried back by lambda.
        q_stage1_mf[action_1[trial]] += learning_rate * (delta_1 + (lam * delta_2))
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```