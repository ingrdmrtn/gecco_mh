Here are three new cognitive models exploring different mechanisms for learning and decision-making in the two-step task.

### Model 1: Perseveration + Model-Based/Model-Free Hybrid
This model extends the standard hybrid learner by adding a "perseveration" parameter. This accounts for the tendency to repeat the previous choice regardless of reward, a common behavioral quirk in this task.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner with Choice Perseveration.
    
    Combines Model-Based and Model-Free learning but adds a "stickiness" or 
    perseveration bonus to the previously chosen action at stage 1.
    
    Parameters:
    learning_rate: [0, 1] Update rate for Q-values.
    beta: [0, 10] Inverse temperature for softmax.
    w: [0, 1] Mixing weight (0 = pure MF, 1 = pure MB).
    perseveration: [0, 5] Bonus added to the Q-value of the previously chosen action.
    """
    learning_rate, beta, w, perseveration = model_parameters
    n_trials = len(action_1)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)       
    q_stage2_mf = np.zeros((2, 2))  

    last_action_1 = -1 # Initialize with an impossible action

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add perseveration bonus
        if last_action_1 != -1:
            q_net[last_action_1] += perseveration

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        # SARSA(0) / TD(0) update for Stage 1
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1

        # TD update for Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Eligibility trace (lambda=1) update for Stage 1 based on Stage 2 RPE
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage2

        last_action_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Separate Learning Rates for Positive and Negative Prediction Errors
This model investigates asymmetry in learning. Participants often update their beliefs differently when surprised by a reward (positive RPE) versus an omission (negative RPE). This is a pure Model-Free learner (to isolate the learning rate effect) with dual learning rates.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Asymmetric Learning Rate Model (Pure Model-Free).
    
    Uses different learning rates for positive prediction errors (better than expected)
    and negative prediction errors (worse than expected).
    
    Parameters:
    alpha_pos: [0, 1] Learning rate for positive RPEs.
    alpha_neg: [0, 1] Learning rate for negative RPEs.
    beta: [0, 10] Inverse temperature.
    eligibility_trace: [0, 1] Decay factor for the eligibility trace.
    """
    alpha_pos, alpha_neg, beta, eligibility_trace = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)       
    q_stage2_mf = np.zeros((2, 2))  

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        # Stage 1 prediction error
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        lr_1 = alpha_pos if delta_stage1 > 0 else alpha_neg
        q_stage1_mf[action_1[trial]] += lr_1 * delta_stage1

        # Stage 2 prediction error
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        lr_2 = alpha_pos if delta_stage2 > 0 else alpha_neg
        q_stage2_mf[state_idx, action_2[trial]] += lr_2 * delta_stage2
        
        # Eligibility trace update for Stage 1
        q_stage1_mf[action_1[trial]] += lr_2 * eligibility_trace * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: "Forgetful" Q-Learning
This model introduces a decay parameter. In standard Q-learning, unchosen options retain their values indefinitely. In this model, unchosen action values decay toward a neutral point (0.5) or zero over time, simulating memory loss or passive forgetting.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Forgetful Q-Learner.
    
    Standard Model-Free learning, but Q-values for unchosen actions decay 
    towards 0 on every trial.
    
    Parameters:
    learning_rate: [0, 1] Update rate for chosen actions.
    beta: [0, 10] Inverse temperature.
    decay_rate: [0, 1] Rate at which unchosen Q-values decay (1 = no decay, 0 = instant forgetting).
    """
    learning_rate, beta, decay_rate = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)       
    q_stage2_mf = np.zeros((2, 2))  

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        # 1. Decay all Q-values slightly before update (simulating time passing)
        q_stage1_mf *= decay_rate
        q_stage2_mf *= decay_rate

        # 2. Standard TD Updates
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1

        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Direct update to stage 1 based on outcome (TD(1) logic simplified)
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```