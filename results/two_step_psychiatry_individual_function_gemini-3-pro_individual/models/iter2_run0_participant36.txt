Here are three cognitive models representing different strategies in the two-step task.

### Model 1: Pure Model-Free Learner (TD-Learning)
This model ignores the transition structure of the task entirely. It treats the first stage as just another state to learn values for based on direct experience (Temporal Difference learning), rather than planning ahead. It relies on a simple chain of reinforcement: Reward updates Stage 2 values, and the value of the chosen Stage 2 option updates Stage 1 values.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free Learner (TD(0)).
    This model relies entirely on temporal difference learning. It updates Stage 1
    values based on the value of the state reached in Stage 2, without using the 
    transition probability matrix.

    Parameters:
    learning_rate: [0,1] Rate for updating both Stage 1 and Stage 2 Q-values.
    beta: [0,10] Inverse temperature for softmax choice.
    """
    learning_rate, beta = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values for Stage 1 (Spaceships) and Stage 2 (Aliens)
    q_stage1_mf = np.zeros(2)      # [Val_Spaceship0, Val_Spaceship1]
    q_stage2_mf = np.zeros((2, 2)) # [[Val_Planet0_Alien0, Val_Planet0_Alien1], ...]

    for trial in range(n_trials):

        # --- Policy for Stage 1 ---
        # Model-Free: Use learned Q-values directly
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Policy for Stage 2 ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # Update Stage 1 based on the value of the Stage 2 state/action chosen (TD-0)
        # Note: In SARSA/TD(0), we update Q1 towards Q2 of the action actually taken.
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Update Stage 2 based on reward
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Learner (Mixture of MB and MF)
This is the classic "Daw et al. (2011)" style model. It assumes the participant maintains *both* a Model-Based (planning) valuation and a Model-Free (habitual) valuation for the first stage. The final decision weight is a linear combination of these two systems controlled by a mixing parameter `w`.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Learner.
    The Stage 1 Q-values are a weighted mix of Model-Based (planning via transition matrix)
    and Model-Free (TD learning) values.

    Parameters:
    learning_rate: [0,1] Learning rate for Q-values.
    beta: [0,10] Inverse temperature.
    w: [0,1] Weighting parameter (0 = Pure MF, 1 = Pure MB).
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Policy for Stage 1 ---
        # 1. Calculate Model-Based values (Planning)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Combine MB and MF values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Policy for Stage 2 ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # Update Model-Free Stage 1
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Update Stage 2 (used by both MB and MF systems)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Choice Kernel (Perseveration) Learner
This model adds a "Choice Kernel" to the basic Model-Free framework. It hypothesizes that participants have a tendency to repeat (or avoid) their previous Stage 1 choice regardless of reward, simply due to motor perserveration or novelty seeking. This is captured by the `stickiness` parameter.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Learner with Choice Stickiness (Perseveration).
    Adds a 'choice kernel' that biases the Stage 1 decision towards the previously
    chosen action, independent of reward history.

    Parameters:
    learning_rate: [0,1] Learning rate.
    beta: [0,10] Inverse temperature.
    stickiness: [0,5] Bonus added to the Q-value of the previously chosen action.
    """
    learning_rate, beta, stickiness = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track the previous action (initialize to -1 or handle first trial separately)
    last_action = -1

    for trial in range(n_trials):

        # --- Policy for Stage 1 ---
        # Add stickiness bonus to Q-values before softmax
        q_modified = q_stage1_mf.copy()
        if last_action != -1:
            q_modified[last_action] += stickiness
            
        exp_q1 = np.exp(beta * q_modified)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Update last action
        last_action = action_1[trial]
        state_idx = state[trial]

        # --- Policy for Stage 2 ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```