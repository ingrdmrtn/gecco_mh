Here are three new cognitive models that explore different mechanisms for decision-making in the two-step task. These models introduce variations in how rewards are processed (perseveration), how learning rates might differ for positive vs. negative outcomes, and how eligibility traces can bridge the gap between stages.

### Model 1: Choice Perseveration Model
This model extends the standard hybrid learner by adding a "perseveration" parameter. This parameter captures the tendency to repeat (or switch from) the previous Stage 1 choice, regardless of the reward outcome. This is a common heuristic in human decision-making.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Learner with Choice Perseveration.
    
    Adds a 'stickiness' or perseveration bonus to the previously chosen action 
    at Stage 1. This captures habitual repetition independent of reward value.
    
    Parameters:
    learning_rate: [0,1] Rate for updating Q-values.
    beta: [0,10] Inverse temperature for softmax.
    w: [0,1] Weighting parameter (0 = pure MF, 1 = pure MB).
    perseveration: [0,5] Bonus added to the logits of the previously chosen action.
    """
    learning_rate, beta, w, perseveration = model_parameters
    n_trials = len(action_1)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Initialize Q-values to 0.5 (midpoint of reward range [0,1])
    q_stage1_mf = np.zeros(2) + 0.5
    q_stage2_mf = np.zeros((2, 2)) + 0.5
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Track the previous action (initially -1 to indicate no previous action)
    last_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2

        # Combine MB and MF values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Calculate logits (values before exponentiation)
        logits = beta * q_net
        
        # Add perseveration bonus if it's not the first trial
        if last_action_1 != -1:
            logits[last_action_1] += perseveration
            
        exp_q1 = np.exp(logits)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        state_idx = state[trial]
        
        # Update last action tracker
        last_action_1 = a1

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]

        # --- Learning (Q-value Updates) ---
        # Update Stage 1 MF value (SARSA-style TD(0))
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

        # Update Stage 2 MF value
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Asymmetric Learning Rate Model (Pure Model-Free)
This model drops the Model-Based component to focus solely on Model-Free learning, but with a twist: it uses separate learning rates for positive prediction errors (better than expected) and negative prediction errors (worse than expected). This accounts for optimism or pessimism biases in learning.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free Learner with Asymmetric Learning Rates.
    
    Uses two different learning rates: one for positive prediction errors (alpha_pos)
    and one for negative prediction errors (alpha_neg). It assumes no Model-Based
    planning (w=0).
    
    Parameters:
    alpha_pos: [0,1] Learning rate for positive prediction errors (delta > 0).
    alpha_neg: [0,1] Learning rate for negative prediction errors (delta < 0).
    beta: [0,10] Inverse temperature for softmax.
    """
    alpha_pos, alpha_neg, beta = model_parameters
    n_trials = len(action_1)
    
    q_stage1_mf = np.zeros(2) + 0.5
    q_stage2_mf = np.zeros((2, 2)) + 0.5
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Policy (Pure MF) ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]

        # --- Learning with Asymmetric Rates ---
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        if delta_stage1 > 0:
            q_stage1_mf[a1] += alpha_pos * delta_stage1
        else:
            q_stage1_mf[a1] += alpha_neg * delta_stage1

        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        if delta_stage2 > 0:
            q_stage2_mf[state_idx, a2] += alpha_pos * delta_stage2
        else:
            q_stage2_mf[state_idx, a2] += alpha_neg * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Eligibility Trace Model (TD(lambda))
This model introduces an eligibility trace parameter (`lambda`). Instead of just updating Stage 1 based on the Stage 2 state value (TD(0)), the reward received at the *end* of the trial is allowed to directly influence the Stage 1 choice value. This bridges the gap between the final outcome and the initial decision more aggressively than standard chain-learning.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Learner with Eligibility Traces (TD(lambda)).
    
    The Stage 1 value is updated not just by the transition to Stage 2, but also 
    directly by the final reward, scaled by the eligibility trace parameter lambda.
    This effectively allows the reward to propagate back to the first stage immediately.
    
    Parameters:
    learning_rate: [0,1] Rate for updating Q-values.
    beta: [0,10] Inverse temperature for softmax.
    lambda_param: [0,1] Eligibility trace decay (0 = TD(0), 1 = Monte Carlo).
    """
    learning_rate, beta, lambda_param = model_parameters
    n_trials = len(action_1)
    
    q_stage1_mf = np.zeros(2) + 0.5
    q_stage2_mf = np.zeros((2, 2)) + 0.5
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]

        # --- Learning with Eligibility Traces ---
        
        # 1. Calculate Prediction Error at Stage 2
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        
        # 2. Update Stage 2 Value
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # 3. Calculate Prediction Error at Stage 1
        # Note: In standard TD(0), we update Stage 1 based on V(Stage2).
        # Here we calculate the standard TD error for Stage 1:
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1] # Using updated Q2 is closer to SARSA
        
        # 4. Update Stage 1 Value
        # The update includes the immediate TD error (delta_stage1) PLUS 
        # a portion of the Stage 2 error (delta_stage2) carried back by lambda.
        # This is a simplified implementation of traces for a fixed 2-step task.
        q_stage1_mf[a1] += learning_rate * (delta_stage1 + lambda_param * delta_stage2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```