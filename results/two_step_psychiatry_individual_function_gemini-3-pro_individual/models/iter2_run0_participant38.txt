Here are 3 new cognitive models inspired by reinforcement learning theories in decision-making neuroscience.

### Model 1: Hybrid Learner (Model-Based + Model-Free)
This is the classic "Daw et al. (2011)" style model. It assumes the brain uses two parallel systems: a Model-Free (MF) system that learns from direct experience (TD learning) and a Model-Based (MB) system that uses knowledge of the transition structure to plan. The final choice is a weighted combination of both.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Learner.
    
    The agent computes Stage 1 values by mixing a Model-Based (MB) valuation
    (using the transition matrix) and a Model-Free (MF) valuation.
    Stage 2 is purely Model-Free.

    Parameters:
    learning_rate: [0, 1] - Rate for updating MF Q-values.
    beta: [0, 10] - Inverse temperature for softmax.
    w: [0, 1] - Weighting parameter (0 = Pure MF, 1 = Pure MB).
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition matrix as described in task (A->X common, U->Y common)
    # 0 -> 0 (70%), 0 -> 1 (30%)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2) 
    q_stage2_mf = np.zeros((2, 2)) # 2 states (planets), 2 actions (aliens)

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        # 1. Calculate Model-Based values: V_MB = T * max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Mix MB and MF values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # 3. Softmax choice
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = action_1[trial]
        p_choice_1[trial] = probs_1[a1]
        
        state_idx = state[trial] # Planet arrived at

        # --- Stage 2 Policy ---
        # Standard softmax on stage 2 Q-values
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = action_2[trial]
        p_choice_2[trial] = probs_2[a2]
        
        r = reward[trial]
  
        # --- Updates ---
        # Standard SARSA/TD updates for MF values
        
        # TD error for stage 1 (using stage 2 value as the target)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # TD error for stage 2 (using reward as the target)
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Choice Kernel (Perseveration) + Model-Free
This model tests the hypothesis that the participant isn't using complex planning, but rather a simple Model-Free strategy combined with a "Choice Kernel." The Choice Kernel captures the tendency to repeat (or switch) recently chosen actions regardless of reward (perseveration).

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Learner with Choice Perseveration (Choice Kernel).
    
    Includes a 'stickiness' parameter that biases the agent to repeat 
    the previous Stage 1 choice, regardless of the outcome.

    Parameters:
    learning_rate: [0, 1] - Update rate for Q-values.
    beta: [0, 10] - Inverse temperature.
    perseveration: [0, 5] - Bonus added to the previously chosen action (positive = repeat).
    """
    learning_rate, beta, perseveration = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track the previous action (initialize to -1 or handle first trial)
    last_action_1 = -1

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        # Add perseveration bonus to the Q-values before softmax
        q_net = q_stage1_mf.copy()
        if last_action_1 != -1:
            q_net[last_action_1] += perseveration
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = action_1[trial]
        p_choice_1[trial] = probs_1[a1]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = action_2[trial]
        p_choice_2[trial] = probs_2[a2]
        
        r = reward[trial]
  
        # --- Updates ---
        # TD(0) updates (simple Q-learning/SARSA)
        
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Update history
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Asymmetric Learning Rates (Positive vs Negative RPE)
This model investigates if the participant learns differently from positive surprises (better than expected) versus negative surprises (worse than expected). This is common in clinical populations (e.g., depression or addiction). It is a pure Model-Free learner but splits the learning rate.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Learner with Asymmetric Learning Rates.
    
    Uses separate learning rates for positive and negative prediction errors.
    This captures biases towards learning from gains vs. losses.

    Parameters:
    alpha_pos: [0, 1] - Learning rate for positive prediction errors (delta > 0).
    alpha_neg: [0, 1] - Learning rate for negative prediction errors (delta < 0).
    beta: [0, 10] - Inverse temperature.
    """
    alpha_pos, alpha_neg, beta = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        a1 = action_1[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = action_2[trial]
        p_choice_2[trial] = probs_2[a2]
        
        r = reward[trial]
  
        # --- Updates ---
        
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        if delta_stage2 > 0:
            q_stage2_mf[state_idx, a2] += alpha_pos * delta_stage2
        else:
            q_stage2_mf[state_idx, a2] += alpha_neg * delta_stage2
            
        # Stage 1 Update
        # Note: We use the updated Stage 2 value for the Stage 1 target (SARSA style)
        # or the value before update. Here we use the value *before* update for consistency
        # with standard TD(0), but re-calculating delta based on stored Qs.
        # Actually, let's use the standard TD error definition:
        # Target is Q_stage2(state, a2) which we just updated? 
        # Standard TD(0) usually uses the value of the state landed in.
        
        # Let's use the Q-value of the action actually taken in stage 2 as the target for stage 1.
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        
        if delta_stage1 > 0:
            q_stage1_mf[a1] += alpha_pos * delta_stage1
        else:
            q_stage1_mf[a1] += alpha_neg * delta_stage1
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```