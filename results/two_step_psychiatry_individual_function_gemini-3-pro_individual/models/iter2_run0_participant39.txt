Here are three new cognitive models that explore different mechanisms than the ones previously tried, focusing on eligibility traces, separate learning rates for different stages, and a hybrid model-based/model-free architecture with a mixing parameter.

### Model 1: TD(1) / Eligibility Trace Model
This model uses an eligibility trace (specifically TD(1)) to update the first-stage values directly based on the final reward, rather than just bootstrapping from the second stage. This captures the idea that the first action is directly reinforced by the final outcome, skipping the intermediate state value update lag.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    TD(1) / Direct Reinforcement Model.
    Instead of standard TD(0) learning where Stage 1 is updated by Stage 2's value,
    this model updates Stage 1 values directly based on the final reward received.
    This is equivalent to an eligibility trace with lambda=1.

    Parameters:
    learning_rate: [0, 1] - Update rate for Q-values.
    beta: [0, 10] - Inverse temperature for softmax choice.
    """
    learning_rate, beta = model_parameters
    n_trials = len(action_1)
  
    # Q-values initialization
    q_stage1 = np.zeros(2)      # Values for choosing Spaceship A or U
    q_stage2 = np.zeros((2, 2)) # Values for aliens (Planet X: W/S, Planet Y: P/H)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Observe state transition
        state_idx = state[trial] # 0 for Planet X, 1 for Planet Y

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning ---
        # Update Stage 2 value based on reward (standard Q-learning)
        r = reward[trial]
        chosen_s2_val = q_stage2[state_idx, action_2[trial]]
        delta_stage2 = r - chosen_s2_val
        q_stage2[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Update Stage 1 value directly based on the REWARD (TD(1)-like behavior)
        # In standard TD(0), we update towards the state value of stage 2.
        # Here, we update towards the actual reward obtained.
        chosen_s1_val = q_stage1[action_1[trial]]
        delta_stage1 = r - chosen_s1_val
        q_stage1[action_1[trial]] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Separate Learning Rates for Stage 1 and Stage 2
This model hypothesizes that learning happens at different speeds for the high-level spaceship choice (Stage 1) versus the specific alien choice (Stage 2). For example, the participant might learn which alien is generous quickly, but be slow to update their spaceship preference, or vice versa.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Dual Learning Rate Model (Stage-Specific).
    Uses distinct learning rates for the first stage (spaceship choice)
    and the second stage (alien choice).
    
    Parameters:
    lr_stage1: [0, 1] - Learning rate for the spaceship choice.
    lr_stage2: [0, 1] - Learning rate for the alien choice.
    beta: [0, 10] - Inverse temperature.
    """
    lr_stage1, lr_stage2, beta = model_parameters
    n_trials = len(action_1)
  
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning ---
        # Stage 1 update: Driven by the difference between Stage 2 value and Stage 1 value
        # Using lr_stage1
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr_stage1 * delta_stage1

        # Stage 2 update: Driven by reward prediction error
        # Using lr_stage2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr_stage2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model-Based / Model-Free (Classic Daw et al.)
This is the canonical "Hybrid" model for this task. It assumes the agent computes values in two ways: "Model-Free" (MF), relying on cached reward history, and "Model-Based" (MB), using the transition matrix to plan. The final decision is a weighted mix of both. This introduces the parameter `w` (mixing weight).

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Model.
    The agent maintains both a Model-Free (TD) value and a Model-Based value
    for the first stage choice. The final choice probability is determined by
    a weighted combination of these Q-values.

    Parameters:
    learning_rate: [0, 1] - Update rate for MF values.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - Weighting parameter (0 = Pure MF, 1 = Pure MB).
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition structure: 
    # Spaceship 0 -> Planet 0 (70%), Planet 1 (30%)
    # Spaceship 1 -> Planet 0 (30%), Planet 1 (70%)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # Stage 2 values are shared by MB and MF
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):

        # --- Calculate Model-Based Values ---
        # MB value for Stage 1 is the expected value of the best option at Stage 2,
        # weighted by the transition probabilities.
        max_q_stage2 = np.max(q_stage2_mf, axis=1) # Best value available at each planet
        q_stage1_mb = transition_matrix @ max_q_stage2

        # --- Combined Net Value ---
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)

        # --- Stage 1 Policy ---
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        # Stage 2 is purely model-free (no further planning steps)
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning (Model-Free Updates) ---
        # Update Stage 1 MF value (SARSA-style update using Stage 2 value)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1

        # Update Stage 2 MF value based on reward
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```