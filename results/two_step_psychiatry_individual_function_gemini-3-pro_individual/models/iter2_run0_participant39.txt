Here are 3 new cognitive models based on the two-step task.

### Model 1: Perseveration + Model-Free Learning
This model hypothesizes that the participant's behavior is driven by a simple model-free reinforcement learning process, augmented by a tendency to repeat the previous choice ("perseveration"). Given the participant data shows repeated choices of the same option, this model explicitly captures that "stickiness" rather than complex planning.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Model 1: Model-Free Learner with Choice Perseveration.
    This model assumes the agent learns values via TD learning but has an 
    explicit bias to repeat the previous stage-1 action.
    
    Bounds:
    learning_rate: [0, 1] - Learning rate for Q-value updates.
    beta: [0, 10] - Inverse temperature for softmax.
    perseveration: [0, 5] - Bonus added to the Q-value of the previously chosen action.
    """
    learning_rate, beta, perseveration = model_parameters
    n_trials = len(action_1)
  
    # Initialize Q-values
    q_stage1_mf = np.zeros(2) 
    q_stage2_mf = np.zeros((2, 2)) 
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1 # No previous action on first trial

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        # Add perseveration bonus to the previously chosen action
        q_net_stage1 = q_stage1_mf.copy()
        if last_action_1 != -1:
            q_net_stage1[last_action_1] += perseveration

        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        a1 = action_1[trial]
        last_action_1 = a1 # Update last action

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]

        # --- Learning ---
        # SARSA-style update for Stage 1 (using Q-value of stage 2 choice)
        # Note: Standard MF often uses max Q(s', a'), here we use the chosen Q(s', a2)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Separate Learning Rates for Positive/Negative Prediction Errors
This model investigates if the participant learns differently from success (getting a coin) versus failure (no coin). Given the data shows a string of successes followed by a failure, this asymmetry might explain how quickly they would switch strategies after the failure. It uses a pure Model-Free framework to keep the parameter count low while adding this nuance.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model 2: Asymmetric Learning Rates (Positive vs Negative PE).
    Splits the learning rate into alpha_pos (for positive prediction errors) 
    and alpha_neg (for negative prediction errors).
    
    Bounds:
    alpha_pos: [0, 1] - Learning rate for positive prediction errors.
    alpha_neg: [0, 1] - Learning rate for negative prediction errors.
    beta: [0, 10] - Inverse temperature for softmax.
    """
    alpha_pos, alpha_neg, beta = model_parameters
    n_trials = len(action_1)
  
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        a1 = action_1[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]

        # --- Learning ---
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        if delta_stage1 > 0:
            q_stage1_mf[a1] += alpha_pos * delta_stage1
        else:
            q_stage1_mf[a1] += alpha_neg * delta_stage1

        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        if delta_stage2 > 0:
            q_stage2_mf[state_idx, a2] += alpha_pos * delta_stage2
        else:
            q_stage2_mf[state_idx, a2] += alpha_neg * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Eligibility Trace (TD(lambda)) Learner
This model assumes that the reward at the second stage directly reinforces the first stage choice, bridging the gap between the two steps more effectively than simple 1-step TD learning. This is implemented via an eligibility trace parameter `lambda`. If `lambda` is high, the reward at the end of the trial strongly updates the spaceship choice directly, bypassing the need for the second-stage Q-value to be accurate first.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model 3: TD(lambda) Eligibility Trace Learner.
    Uses an eligibility trace to allow the Stage 2 reward to directly 
    influence the Stage 1 value update, bridging the temporal gap.
    
    Bounds:
    learning_rate: [0, 1] - Learning rate.
    beta: [0, 10] - Inverse temperature.
    lambda_param: [0, 1] - Eligibility trace decay parameter (0=TD(0), 1=Monte Carlo).
    """
    learning_rate, beta, lambda_param = model_parameters
    n_trials = len(action_1)
  
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        a1 = action_1[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]

        # --- Learning with Eligibility Traces ---
        # 1. Calculate prediction error at stage 1
        delta_1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        
        # 2. Update Stage 1 Q-value based on immediate transition
        q_stage1_mf[a1] += learning_rate * delta_1
        
        # 3. Calculate prediction error at stage 2
        delta_2 = r - q_stage2_mf[state_idx, a2]
        
        # 4. Update Stage 2 Q-value
        q_stage2_mf[state_idx, a2] += learning_rate * delta_2
        
        # 5. Eligibility Trace Update: Apply Stage 2 error to Stage 1 choice
        # The trace for state 1 decays by lambda.
        q_stage1_mf[a1] += learning_rate * lambda_param * delta_2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```