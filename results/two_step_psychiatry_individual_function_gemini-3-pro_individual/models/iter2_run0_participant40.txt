Here are three new cognitive models based on the two-step task structure. These models explore different mechanisms: pure model-based learning, a hybrid model with a fixed mixing weight, and a model with separate learning rates for the two stages.

### Model 1: Pure Model-Based Learner
This model assumes the participant fully utilizes the transition structure of the task. Instead of learning the value of the spaceships (stage 1) directly from reward prediction errors, it calculates the value of a spaceship by combining the known transition probabilities with the learned values of the planets (stage 2).

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Based Learner.
    
    This model assumes the participant calculates the value of stage 1 actions 
    by planning forward using the transition matrix and the learned values of 
    stage 2 states. It does not maintain separate cached Q-values for stage 1.

    Parameters:
    - learning_rate: [0, 1] Rate at which stage 2 Q-values are updated.
    - beta: [0, 10] Inverse temperature for softmax choice.
    """
    learning_rate, beta = model_parameters
    n_trials = len(action_1)

    # Fixed transition matrix as described in task: A->X (0.7), U->Y (0.7)
    # Rows: Actions (0, 1), Cols: States (0, 1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    # Only Stage 2 Q-values are learned directly from reward
    q_stage2_mf = np.zeros((2, 2)) 

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Policy (Model-Based) ---
        # Calculate expected value of each state (planet)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        
        # Calculate Q-value for stage 1 actions by integrating transition probs
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning ---
        # Only Stage 2 values are updated via TD error
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Learner (Model-Free + Model-Based) with Choice Perseveration
This model combines both model-free (habitual) and model-based (planning) value estimates. It introduces a weighting parameter `w` to balance them. Additionally, it includes a perseveration parameter `p` to capture the tendency to repeat the previous stage 1 choice regardless of value, a common phenomenon in this task.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner with Choice Perseveration.
    
    Combines Model-Based and Model-Free values for the first stage choice.
    Also includes a "stickiness" parameter for the first stage action.

    Parameters:
    - learning_rate: [0, 1] Rate for updating Q-values.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight (0 = Pure MF, 1 = Pure MB).
    - p: [0, 5] Perseveration bonus for repeating the previous stage 1 choice.
    """
    learning_rate, beta, w, p = model_parameters
    n_trials = len(action_1)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1 # Indicator for no previous action

    for trial in range(n_trials):

        # --- Stage 1 Policy (Hybrid) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combine MF and MB values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add perseveration bonus
        if last_action_1 != -1:
            q_net[last_action_1] += p
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_action_1 = action_1[trial]
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning ---
        # Update Stage 1 MF value (SARSA-like update using Stage 2 value)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1

        # Update Stage 2 MF value
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Dual Learning Rate Model-Free Learner
This model is a pure model-free learner (like the best model so far), but it hypothesizes that learning happens at different speeds for the first stage (spaceships) and the second stage (aliens). This captures the possibility that the participant updates their preferences for the immediate outcome (aliens/coins) differently than for the predictive cue (spaceships).

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Dual Learning Rate Model-Free Learner.
    
    Assumes purely model-free learning but allows for different learning rates
    for the first stage (action selection) and the second stage (reward receipt).

    Parameters:
    - alpha1: [0, 1] Learning rate for stage 1 (spaceship choice).
    - alpha2: [0, 1] Learning rate for stage 2 (alien choice).
    - beta: [0, 10] Inverse temperature.
    """
    alpha1, alpha2, beta = model_parameters
    n_trials = len(action_1)

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning ---
        # Stage 1 update uses alpha1
        # Note: We use the value of the CHOSEN second stage action as the target
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += alpha1 * delta_stage1

        # Stage 2 update uses alpha2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += alpha2 * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```