Here are three new cognitive models. These models explore different mechanisms: Model-Based learning, a Hybrid learner (mixing Model-Based and Model-Free), and a model that incorporates "eligibility traces" (TD(1)) to link outcomes directly to initial choices.

### Model 1: Pure Model-Based Learning
This model assumes the participant fully understands the task structure. Instead of learning the value of spaceships directly from reward history (like the Model-Free `cognitive_model1` you provided), it calculates the value of a spaceship by combining the learned values of the aliens (at the second stage) with the known transition probabilities of the spaceships.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Based Reinforcement Learning.
    
    This model computes the value of the first-stage actions (spaceships) by planning
    forward using the transition matrix and the learned values of the second-stage 
    states (planets/aliens). It assumes fixed transition probabilities.

    Parameters:
    - learning_rate: [0, 1] Rate at which second-stage Q-values are updated.
    - beta: [0, 10] Inverse temperature for softmax choice.
    """
    learning_rate, beta = model_parameters
    n_trials = len(action_1)
    
    # Fixed transition matrix as per task description:
    # Row 0: Spaceship A -> [70% Planet X, 30% Planet Y]
    # Row 1: Spaceship U -> [30% Planet X, 70% Planet Y]
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    # Q-values for the second stage (aliens) only.
    # We do not store Q-values for stage 1; we compute them on the fly.
    q_stage2_mf = np.zeros((2, 2)) # (State/Planet, Action/Alien)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Policy (Model-Based) ---
        # Calculate the max value available at each planet
        max_q_stage2 = np.max(q_stage2_mf, axis=1) 
        
        # Bellman equation: V(action1) = Sum(P(state|action1) * max(Q(state, action2)))
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        a1 = action_1[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]

        # --- Updates ---
        # In a pure model-based agent, we only update the immediate reward estimates (Stage 2)
        # The Stage 1 values update automatically because they are derived from Stage 2.
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Learner (Model-Based + Model-Free)
This is the classic "Two-Step Task" model. It assumes the participant's brain runs two systems in parallel: a habitual system (Model-Free) and a planning system (Model-Based). The final decision at the first stage is a weighted combination of both value estimates.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Learner.
    
    This model maintains both MF and MB value estimates for the first stage.
    The final choice probability is determined by a weighted sum of these values,
    controlled by the parameter 'w'.
    
    Parameters:
    - learning_rate: [0, 1] Update rate for Q-values.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weighting parameter. 0 = Pure MF, 1 = Pure MB.
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)      # Learned via TD error
    q_stage2_mf = np.zeros((2, 2)) # Learned via Reward error
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Policy (Hybrid) ---
        # 1. Calculate MB values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Combine MF and MB values
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        a1 = action_1[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]

        # --- Updates ---
        # Update Stage 1 MF values based on the value of the state we arrived at
        # (Standard SARSA-style update)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

        # Update Stage 2 MF values based on reward
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: TD(1) / Eligibility Trace Learner
This is a Model-Free learner, but unlike the standard TD(0) (your `cognitive_model1`), this model allows the reward at the very end of the trial to directly update the value of the *first* choice (the spaceship). This is achieved via an "eligibility trace" parameter (`lambda_eligibility`), effectively bridging the gap between the final outcome and the initial decision without needing a transition model.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    TD(lambda) Model-Free Learner (Eligibility Traces).
    
    This model allows the reward received at the second stage to directly influence
    the value of the first stage choice, mediated by the eligibility trace parameter (lambda).
    If lambda is 1, the full reward prediction error at stage 2 is applied to stage 1.
    
    Parameters:
    - learning_rate: [0, 1] Update rate.
    - beta: [0, 10] Inverse temperature.
    - lambda_eligibility: [0, 1] Decay rate of eligibility trace. 
                          0 = TD(0) (standard MF), 1 = Monte Carlo-like updates.
    """
    learning_rate, beta, lambda_eligibility = model_parameters
    n_trials = len(action_1)

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        a1 = action_1[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]

        # --- Updates ---
        
        # 1. Prediction error at stage 1 (transition to stage 2)
        delta_1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        # Update stage 1 immediately based on what we see at stage 2
        q_stage1_mf[a1] += learning_rate * delta_1
        
        # 2. Prediction error at stage 2 (receiving reward)
        delta_2 = r - q_stage2_mf[state_idx, a2]
        # Update stage 2 normally
        q_stage2_mf[state_idx, a2] += learning_rate * delta_2
        
        # 3. Eligibility Trace Update:
        # Pass the stage 2 error back to stage 1, scaled by lambda
        q_stage1_mf[a1] += learning_rate * lambda_eligibility * delta_2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```