Here are three new cognitive models exploring different mechanisms for learning and decision-making in the two-step task.

### Model 1: Perseveration + Model-Based/Model-Free Hybrid
This model extends the standard hybrid learner by adding a "perseveration" parameter. Participants often have a tendency to repeat their previous choice regardless of the outcome (stickiness), or conversely, to switch. This parameter captures that raw behavioral inertia alongside the sophisticated reinforcement learning.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner with Choice Perseveration.
    
    Adds a 'stickiness' parameter to the standard hybrid model. This accounts for
    a tendency to repeat (or switch) the previous stage-1 choice regardless of value.

    Bounds:
    - learning_rate: [0, 1] Update rate for Q-values.
    - beta: [0, 10] Inverse temperature for softmax.
    - w: [0, 1] Mixing weight (1=MB, 0=MF).
    - perseveration: [-5, 5] Bonus added to the previously chosen action (positive=sticky, negative=switchy).
    """
    learning_rate, beta, w, perseveration = model_parameters
    n_trials = len(action_1)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)       
    q_stage2_mf = np.zeros((2, 2))  
    
    # Track previous choice (initialized to -1 so it doesn't affect trial 0)
    last_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Calculate net Q-value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add perseveration bonus if applicable
        if last_action_1 != -1:
            q_net[last_action_1] += perseveration

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Record this action for the next trial's perseveration
        last_action_1 = action_1[trial]
        
        state_idx = state[trial] 

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        # TD(0) update for Stage 1 (Model-Free)
        # Note: In standard hybrid models, stage 1 MF is often updated via TD-lambda or direct reward,
        # but here we use the standard Q-learning structure requested by the template.
        chosen_stage2_value = q_stage2_mf[state_idx, action_2[trial]]
        delta_stage1 = chosen_stage2_value - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1

        # TD(0) update for Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Separate Learning Rates for Positive and Negative Prediction Errors
This model investigates asymmetry in learning. It hypothesizes that the participant might update their beliefs differently when they are pleasantly surprised (positive prediction error) versus when they are disappointed (negative prediction error). This is a purely Model-Free implementation to isolate the learning rate effects.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Asymmetric Learning Rate Model (Dual Alpha).
    
    This model allows for different learning rates depending on whether the 
    prediction error is positive (better than expected) or negative (worse than expected).
    It operates purely in a model-free manner (no transition matrix usage).

    Bounds:
    - alpha_pos: [0, 1] Learning rate for positive prediction errors.
    - alpha_neg: [0, 1] Learning rate for negative prediction errors.
    - beta: [0, 10] Inverse temperature.
    """
    alpha_pos, alpha_neg, beta = model_parameters
    n_trials = len(action_1)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)       
    q_stage2_mf = np.zeros((2, 2))  

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        # Pure model-free choice
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial] 

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        
        # Stage 1 Update
        chosen_stage2_value = q_stage2_mf[state_idx, action_2[trial]]
        delta_stage1 = chosen_stage2_value - q_stage1_mf[action_1[trial]]
        
        if delta_stage1 > 0:
            q_stage1_mf[action_1[trial]] += alpha_pos * delta_stage1
        else:
            q_stage1_mf[action_1[trial]] += alpha_neg * delta_stage1

        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        if delta_stage2 > 0:
            q_stage2_mf[state_idx, action_2[trial]] += alpha_pos * delta_stage2
        else:
            q_stage2_mf[state_idx, action_2[trial]] += alpha_neg * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Eligibility Trace (TD-Lambda) Learner
Standard Q-learning updates Stage 1 based on the *value* of Stage 2. However, a more direct reinforcement link is often observed: the reward received at the very end reinforces *both* the Stage 2 choice *and* the Stage 1 choice that led there. This model uses an eligibility trace parameter (`lambda`) to modulate how much the final reward directly updates the first-stage choice.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    TD(Lambda) Learner.
    
    Instead of only updating Stage 1 based on the Stage 2 state-value, this model
    allows the final reward outcome to 'propagate' back to the first stage choice directly.
    Lambda controls the decay of the eligibility trace.
    
    Bounds:
    - learning_rate: [0, 1]
    - beta: [0, 10]
    - lambda_param: [0, 1] Decay factor. 0 = TD(0)/Standard Q-learning, 1 = Monte Carlo.
    """
    learning_rate, beta, lambda_param = model_parameters
    n_trials = len(action_1)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)       
    q_stage2_mf = np.zeros((2, 2))  

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial] 

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        
        # 1. Calculate Prediction Error at Stage 1 (based on arrival at Stage 2)
        # We use the max value of the next state as the proxy for V(s')
        v_stage2 = np.max(q_stage2_mf[state_idx])
        delta_stage1 = v_stage2 - q_stage1_mf[action_1[trial]]
        
        # Update Stage 1 immediately based on transition
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # 2. Calculate Prediction Error at Stage 2 (based on reward)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Update Stage 2
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # 3. Eligibility Trace Update for Stage 1
        # The Stage 2 prediction error (delta_stage2) propagates back to Stage 1 
        # scaled by lambda. This connects the final reward to the first choice.
        q_stage1_mf[action_1[trial]] += learning_rate * lambda_param * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```