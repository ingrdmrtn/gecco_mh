Here are three new cognitive models based on the two-step task structure, exploring different mechanisms than the ones previously tested.

### Model 1: Hybrid Model-Based / Model-Free (MB/MF)
This is the classic "Daw et al. (2011)" hybrid model. It assumes the agent computes values in two ways: a Model-Free (MF) way (habitual, learning from direct experience) and a Model-Based (MB) way (planning using the transition matrix). A mixing parameter `w` controls the balance between these two systems.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Reinforcement Learning.
    
    The agent values Stage 1 actions as a weighted combination of:
    1. Model-Based (MB): Calculated using the transition matrix and Stage 2 values.
    2. Model-Free (MF): Learned directly from prediction errors (TD-learning).
    
    Bounds:
    learning_rate: [0, 1] - Learning rate for value updates.
    beta: [0, 10] - Inverse temperature for softmax choice.
    w: [0, 1] - Weighting parameter (0 = pure MF, 1 = pure MB).
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition matrix: A->X (common), U->Y (common)
    # Assuming indices: 0->0 (0.7), 0->1 (0.3), 1->0 (0.3), 1->1 (0.7)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)       # Model-free values for stage 1
    q_stage2 = np.zeros((2, 2))     # Values for stage 2 (shared by MB and MF)

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        # 1. Calculate Model-Based values
        # Max value achievable in each state (greedy planning)
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Combine MB and MF values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Transition
        state_idx = state[trial] # 0 or 1

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # SARSA / TD(1) style update for Stage 1 MF
        # Note: In the classic hybrid model, Stage 1 MF is often updated by the Stage 2 Q-value 
        # (TD(0)) or the final reward (TD(1)). Here we use a simple TD(1) approach where 
        # the Stage 1 value is updated directly by the final reward, skipping the Stage 2 prediction error.
        
        # Update Stage 1 MF using the final reward (TD(1)-like simplification often used)
        delta_stage1_mf = reward[trial] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1_mf
        
        # Update Stage 2 (used by both MB and MF)
        delta_stage2 = reward[trial] - q_stage2[state_idx, action_2[trial]]
        q_stage2[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Separate Learning Rates for Stage 1 and Stage 2
Standard models often assume a single learning rate for the whole task. However, Stage 1 (choosing a spaceship) is more abstract and distal from the reward than Stage 2 (choosing an alien). This model proposes that the participant learns about the immediate reward context (Stage 2) at a different rate than the spaceship context (Stage 1).

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free RL with Separate Learning Rates (Dual-Alpha).
    
    This model assumes the participant updates their valuation of the Spaceships (Stage 1)
    with a different flexibility/speed than they update their valuation of the Aliens (Stage 2).
    
    Bounds:
    alpha1: [0, 1] - Learning rate for Stage 1 (Spaceships).
    alpha2: [0, 1] - Learning rate for Stage 2 (Aliens).
    beta: [0, 10] - Inverse temperature.
    """
    alpha1, alpha2, beta = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # Stage 1 is updated based on the value of the state reached (TD(0))
        # This is a standard Q-learning update for Stage 1
        delta_stage1 = q_stage2[state_idx, action_2[trial]] - q_stage1[action_1[trial]]
        q_stage1[action_1[trial]] += alpha1 * delta_stage1
        
        # Stage 2 is updated based on the reward received
        delta_stage2 = reward[trial] - q_stage2[state_idx, action_2[trial]]
        q_stage2[state_idx, action_2[trial]] += alpha2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Asymmetric Learning Rates (Positive vs. Negative Prediction Error)
This model investigates valence-dependent learning. It posits that the participant updates their beliefs differently when they are pleasantly surprised (positive prediction error, "better than expected") versus when they are disappointed (negative prediction error, "worse than expected"). This is a common finding in clinical psychiatry (e.g., depression or optimism bias).

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free RL with Asymmetric Learning Rates (Pos/Neg PE).
    
    The agent uses different learning rates depending on whether the 
    Outcome was better (Positive PE) or worse (Negative PE) than expected.
    This applies to the final reward update.
    
    Bounds:
    alpha_pos: [0, 1] - Learning rate for positive prediction errors.
    alpha_neg: [0, 1] - Learning rate for negative prediction errors.
    beta: [0, 10] - Inverse temperature.
    """
    alpha_pos, alpha_neg, beta = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # Note: We apply asymmetry primarily to the REWARD update (Stage 2), 
        # as this is where the explicit positive/negative valence is strongest.
        # Stage 1 uses a simple average of the two alphas for simplicity, 
        # or could be modeled with just one. Here we use the mean.
        alpha_avg = (alpha_pos + alpha_neg) / 2.0
        
        delta_stage1 = q_stage2[state_idx, action_2[trial]] - q_stage1[action_1[trial]]
        q_stage1[action_1[trial]] += alpha_avg * delta_stage1
        
        # Stage 2 Update with Asymmetry
        delta_stage2 = reward[trial] - q_stage2[state_idx, action_2[trial]]
        
        if delta_stage2 >= 0:
            q_stage2[state_idx, action_2[trial]] += alpha_pos * delta_stage2
        else:
            q_stage2[state_idx, action_2[trial]] += alpha_neg * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```