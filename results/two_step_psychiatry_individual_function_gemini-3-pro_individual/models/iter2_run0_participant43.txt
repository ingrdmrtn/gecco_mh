Here are three cognitive models implemented as Python functions, following the provided template and specifications.

### Model 1: Pure Model-Free Learner (TD Learning)
This model ignores the transition structure (the map of spaceships to planets). Instead, it relies entirely on Temporal Difference (TD) learning. It updates the value of the spaceship chosen in Stage 1 based on the value of the planet reached in Stage 2, and updates the planet value based on the reward received.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free Learner (TD Learning).
    
    This model relies solely on cached values (Model-Free) updated via 
    Temporal Difference errors. It does not use the transition matrix to 
    infer values. Stage 1 values are updated towards Stage 2 values, 
    and Stage 2 values are updated towards rewards.

    Parameters:
    - learning_rate: [0, 1] Rate at which Q-values are updated.
    - beta: [0, 10] Inverse temperature for softmax choice (exploration/exploitation).
    """
    learning_rate, beta = model_parameters
    n_trials = len(action_1)
  
    # Initialize values
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values for Stage 1 (Spaceships) and Stage 2 (Planets)
    q_stage1_mf = np.zeros(2)      # Values for Spaceship 0, Spaceship 1
    q_stage2_mf = np.zeros((2, 2)) # Values for (Planet 0, Alien 0/1), (Planet 1, Alien 0/1)
    # Note: Standard task usually tracks Q(Planet) not Q(Alien), but template implies aliens.
    # Assuming standard simplified 2-step where Q_stage2 is Q(Planet), 
    # but using (2,2) to match template implication of choice at stage 2.

    for trial in range(n_trials):

        # --- Policy for the first choice (Stage 1) ---
        # Pure MF uses q_stage1_mf directly
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial] # Planet arrived at

        # --- Policy for the second choice (Stage 2) ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updating ---
        # 1. Update Stage 1 Q-value based on Stage 2 Q-value (TD-0)
        # The value of the chosen spaceship is updated toward the value of the chosen alien/planet state
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # 2. Update Stage 2 Q-value based on Reward
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Learner (Model-Based + Model-Free)
This is the classic "Two-Step" model (Daw et al., 2011). It calculates a Model-Based value (using the transition matrix) and a Model-Free value (using direct experience). The final decision at Stage 1 is a weighted combination of both strategies.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner (Model-Based + Model-Free).
    
    This model computes a weighted average of Model-Based (MB) and Model-Free (MF) 
    values to drive the Stage 1 choice. 
    MB values are computed analytically using the transition matrix.
    MF values are learned via TD updates.

    Parameters:
    - learning_rate: [0, 1] Update rate for MF values.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weighting parameter. 1 = Pure MB, 0 = Pure MF.
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Policy for the first choice (Stage 1) ---
        # 1. Calculate MB values: Transition Matrix * Max Stage 2 Values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Calculate Net values: Weighted sum of MB and MF
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Policy for the second choice (Stage 2) ---
        # Stage 2 is purely model-free (choosing the best alien)
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updating ---
        # Update MF Stage 1
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Update MF Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Asymmetric Learning Rate Model-Free
This model modifies the standard Model-Free learner to have different sensitivities to positive outcomes (rewards) versus negative outcomes (omissions). This is often relevant in psychiatry to distinguish between reward sensitivity and punishment/omission sensitivity.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Asymmetric Learning Rate Model-Free Learner.
    
    This model uses separate learning rates for positive prediction errors 
    (better than expected) and negative prediction errors (worse than expected).
    This captures potential biases in how the participant processes wins vs losses.

    Parameters:
    - alpha_pos: [0, 1] Learning rate for positive prediction errors (delta > 0).
    - alpha_neg: [0, 1] Learning rate for negative prediction errors (delta < 0).
    - beta: [0, 10] Inverse temperature.
    """
    alpha_pos, alpha_neg, beta = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Policy for the first choice ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Policy for the second choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updating ---
        
        # Stage 2 Update (Reward driven)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        if delta_stage2 >= 0:
            q_stage2_mf[state_idx, action_2[trial]] += alpha_pos * delta_stage2
        else:
            q_stage2_mf[state_idx, action_2[trial]] += alpha_neg * delta_stage2

        # Stage 1 Update (Value driven)
        # Note: We use the *updated* stage 2 value to drive stage 1 update (SARSA-like logic for consistency)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        # Apply asymmetric rates to Stage 1 update as well
        if delta_stage1 >= 0:
            q_stage1_mf[action_1[trial]] += alpha_pos * delta_stage1
        else:
            q_stage1_mf[action_1[trial]] += alpha_neg * delta_stage1
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```