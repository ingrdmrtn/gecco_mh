Here are three new cognitive models based on the two-step task paradigm, exploring mechanisms beyond standard model-free/model-based hybrids and simple perseveration.

### Model 1: Asymmetric Learning Rates (Reward vs. Punishment)
This model hypothesizes that the participant learns differently from positive outcomes (finding gold) versus negative outcomes (no gold). This is often observed in clinical populations where sensitivity to reward and punishment differs. It splits the learning rate into `alpha_rew` and `alpha_pun`.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free learner with Asymmetric Learning Rates for Reward and Punishment.
    
    This model updates Q-values with different learning rates depending on whether
    the prediction error is positive (better than expected) or negative (worse than expected).
    
    Parameters:
    alpha_rew: [0,1] Learning rate for positive prediction errors.
    alpha_pun: [0,1] Learning rate for negative prediction errors.
    beta: [0,10] Inverse temperature (softmax sensitivity).
    """
    alpha_rew, alpha_pun, beta = model_parameters
    n_trials = len(action_1)

    # Initialize Q-values
    q_stage1 = np.zeros(2)      # Values for choosing Spaceship A or U
    q_stage2 = np.zeros((2, 2)) # Values for aliens on Planet X and Y

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial] # Planet arrived at

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        # Calculate Stage 2 Prediction Error (Reward - Expectation)
        delta_stage2 = reward[trial] - q_stage2[state_idx, action_2[trial]]
        
        # Apply asymmetric learning to Stage 2
        eff_alpha2 = alpha_rew if delta_stage2 >= 0 else alpha_pun
        q_stage2[state_idx, action_2[trial]] += eff_alpha2 * delta_stage2
        
        # Calculate Stage 1 Prediction Error (TD(0): Value of State 2 - Value of Action 1)
        # Note: We use the *updated* stage 2 value or the value of the state chosen
        delta_stage1 = q_stage2[state_idx, action_2[trial]] - q_stage1[action_1[trial]]
        
        # Apply asymmetric learning to Stage 1
        eff_alpha1 = alpha_rew if delta_stage1 >= 0 else alpha_pun
        q_stage1[action_1[trial]] += eff_alpha1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Decay-Based Learning (Forgetting)
This model introduces a memory decay parameter (`decay`). Instead of just updating the chosen action, unchosen actions slowly decay back to a baseline (0 or 0.5). This captures the idea that if an alien isn't visited for a while, the participant becomes less certain about its value or assumes the value "drifts."

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free learner with Memory Decay (Forgetting).
    
    Unchosen actions decay toward 0.5 (uncertainty/baseline) at each step.
    This simulates working memory limitations or the assumption of environmental volatility.
    
    Parameters:
    learning_rate: [0,1] Standard Q-learning rate.
    beta: [0,10] Inverse temperature.
    decay: [0,1] Rate at which unchosen Q-values decay to 0.5. (0 = no decay, 1 = instant reset).
    """
    learning_rate, beta, decay = model_parameters
    n_trials = len(action_1)

    q_stage1 = np.zeros(2) + 0.5 # Start at 0.5 for symmetry
    q_stage2 = np.zeros((2, 2)) + 0.5

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        
        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2[state_idx, action_2[trial]]
        q_stage2[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Decay unchosen stage 2 actions (for the current state)
        unchosen_a2 = 1 - action_2[trial]
        q_stage2[state_idx, unchosen_a2] += decay * (0.5 - q_stage2[state_idx, unchosen_a2])
        
        # Also decay actions in the unvisited state? 
        # A simpler version decays ALL unchosen terminal values.
        unvisited_state = 1 - state_idx
        q_stage2[unvisited_state, :] += decay * (0.5 - q_stage2[unvisited_state, :])

        # Stage 1 Update
        delta_stage1 = q_stage2[state_idx, action_2[trial]] - q_stage1[action_1[trial]]
        q_stage1[action_1[trial]] += learning_rate * delta_stage1
        
        # Decay unchosen stage 1 action
        unchosen_a1 = 1 - action_1[trial]
        q_stage1[unchosen_a1] += decay * (0.5 - q_stage1[unchosen_a1])

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Pure Model-Based with Exploration Bonus
This model abandons the hybrid approach and uses a pure Model-Based strategy for Stage 1 (planning based on the transition matrix), but adds an `exploration_bonus` parameter. This bonus increases the value of actions that have not been chosen recently, encouraging the agent to check the other spaceship or other aliens.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Based learner with an Exploration Bonus.
    
    This model calculates Stage 1 values strictly by planning through the transition matrix.
    It adds an exploration bonus to actions based on how long ago they were last selected.
    
    Parameters:
    learning_rate: [0,1] Update rate for the Stage 2 (Alien) values.
    beta: [0,10] Inverse temperature.
    expl_bonus: [0,2] Weight added to actions based on 'time since last taken'.
    """
    learning_rate, beta, expl_bonus = model_parameters
    n_trials = len(action_1)
    
    # Fixed transition structure of the task (70% common)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Q-values for Stage 2 (aliens) only need to be learned
    q_stage2 = np.zeros((2, 2))
    
    # Counters for exploration (time since last chosen)
    # Initialize with 1 to avoid zero division, though logic handles add-on
    time_since_s1 = np.ones(2) 
    time_since_s2 = np.ones((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        
        # --- Stage 1: Model-Based Planning ---
        # 1. Determine max value available in each state (greedy expectation)
        max_q_stage2 = np.max(q_stage2, axis=1)
        
        # 2. Compute MB value: Sum(Transition_Prob * State_Value)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 3. Add exploration bonus: Bonus * log(time_since_chosen)
        # This is a form of Upper Confidence Bound or uncertainty bonus
        # We use a simple linear scaler for stability: expl_bonus * (time_since / 10)
        q_stage1_eff = q_stage1_mb + (expl_bonus * (time_since_s1 / 10.0))

        exp_q1 = np.exp(beta * q_stage1_eff)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        # Add exploration bonus to stage 2 choices
        q_stage2_eff = q_stage2[state_idx] + (expl_bonus * (time_since_s2[state_idx] / 10.0))
        
        exp_q2 = np.exp(beta * q_stage2_eff)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        # Update Stage 2 values based on reward
        delta_stage2 = reward[trial] - q_stage2[state_idx, action_2[trial]]
        q_stage2[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # --- Update Exploration Counters ---
        # Increment all counters
        time_since_s1 += 1
        time_since_s2 += 1
        
        # Reset counters for actions taken
        time_since_s1[action_1[trial]] = 0
        time_since_s2[state_idx, action_2[trial]] = 0

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```