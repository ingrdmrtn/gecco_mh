Here are three new cognitive models that introduce distinct mechanisms not yet fully covered by the previous parameter combinations.

### Model 1: Hybrid Model with Perseveration
This model combines Model-Based (planning) and Model-Free (habit) learning, but crucially adds a "perseveration" parameter. This parameter captures the tendency to repeat the previous choice regardless of reward, a common behavior in reinforcement learning tasks. This is distinct from the pure `w` hybrid model because it adds a primitive "stickiness" to the choice policy.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MF/MB Model with Choice Perseveration.
    
    This model blends Model-Based (planning using the transition matrix) and 
    Model-Free (TD learning) values. It adds a 'perseveration' parameter 
    to the first-stage choice, which biases the agent to repeat their 
    previous spaceship choice regardless of the outcome.

    Parameters:
    learning_rate: [0, 1] - Update rate for Q-values.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - Weighting parameter (1 = fully Model-Based, 0 = fully Model-Free).
    perseveration: [0, 5] - Bonus added to the Q-value of the previously chosen action.
    """
    learning_rate, beta, w, perseveration = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track previous action for perseveration (initialize to -1 or handle first trial)
    last_action_1 = -1

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        # 1. Calculate Model-Based values: V(state) = max(Q(state, action))
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Integrate MF and MB values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # 3. Add perseveration bonus
        if last_action_1 != -1:
            q_net[last_action_1] += perseveration
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # TD(0) update for Stage 1 MF value
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # TD(0) update for Stage 2 MF value
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Update last action
        last_action_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Separate Learning Rates for Positive and Negative Prediction Errors
This model investigates asymmetry in learning. It hypothesizes that the participant might update their beliefs differently when they are surprised by a good outcome (positive prediction error) versus a bad outcome (negative prediction error). This is a pure Model-Free learner but with higher fidelity in the learning mechanism.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Q-Learning with Asymmetric Learning Rates.
    
    This model uses standard temporal difference learning but applies different 
    learning rates depending on whether the prediction error is positive 
    (better than expected) or negative (worse than expected). This captures 
    optimism/pessimism biases.

    Parameters:
    alpha_pos: [0, 1] - Learning rate for positive prediction errors.
    alpha_neg: [0, 1] - Learning rate for negative prediction errors.
    beta: [0, 10] - Inverse temperature.
    lambda_eligibility: [0, 1] - Eligibility trace decay.
    """
    alpha_pos, alpha_neg, beta, lambda_eligibility = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        lr_1 = alpha_pos if delta_stage1 > 0 else alpha_neg
        q_stage1_mf[action_1[trial]] += lr_1 * delta_stage1
        
        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        lr_2 = alpha_pos if delta_stage2 > 0 else alpha_neg
        q_stage2_mf[state_idx, action_2[trial]] += lr_2 * delta_stage2
        
        # Eligibility Trace Update (Stage 1 updated by Stage 2 PE)
        # Note: We use the learning rate associated with the Stage 2 error here
        q_stage1_mf[action_1[trial]] += lr_2 * lambda_eligibility * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Model-Based with Forgetting
This model assumes the participant uses a Model-Based strategy (planning), but their memory of the value of the second-stage aliens decays over time. If they haven't visited an alien recently, its value slowly reverts to a neutral point (0.5). This captures memory limitations or the assumption of environmental volatility.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Based Learner with Passive Forgetting.
    
    This agent calculates values using the transition matrix (Model-Based).
    However, Q-values for the second stage (aliens) decay toward a neutral 
    value (0.5) on every trial, regardless of whether they were visited. 
    This simulates memory decay or a belief that unvisited options change.

    Parameters:
    learning_rate: [0, 1] - Update rate for chosen options.
    beta: [0, 10] - Inverse temperature.
    decay_rate: [0, 1] - Rate at which unchosen Q-values revert to 0.5.
    """
    learning_rate, beta, decay_rate = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Initialize Q-values at 0.5 (neutral expectation)
    q_stage2_mf = np.ones((2, 2)) * 0.5

    for trial in range(n_trials):

        # --- Stage 1 Policy (Pure Model-Based) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # 1. Decay ALL Stage 2 values towards 0.5
        q_stage2_mf = (1 - decay_rate) * q_stage2_mf + decay_rate * 0.5

        # 2. Active learning update for the chosen alien (correcting the decay for the chosen one)
        # We re-apply the learning update on top of the decayed value
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```