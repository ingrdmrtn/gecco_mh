Here are three new cognitive models that explore different mechanisms for learning and decision-making in the two-step task. These models introduce variations in how prediction errors are processed and how model-based vs. model-free values are integrated.

### Model 1: Asymmetric Learning Rates (Positive vs. Negative)
This model hypothesizes that the participant learns differently from positive outcomes (finding gold) versus negative outcomes (no gold). This is a common finding in reinforcement learning where "confirmation bias" or risk aversion can lead to different update speeds for gains and losses.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Asymmetric Learning Rates for Positive/Negative Prediction Errors.
    
    This model separates the learning rate into two components: one for positive 
    prediction errors (better than expected) and one for negative prediction errors 
    (worse than expected). This captures potential biases in learning from gains vs. losses.

    Parameters:
    alpha_pos: [0, 1] Learning rate for positive prediction errors (RPE > 0).
    alpha_neg: [0, 1] Learning rate for negative prediction errors (RPE < 0).
    beta: [0, 10] Inverse temperature for softmax.
    w: [0, 1] Weight for Model-Based control (1 = Pure MB, 0 = Pure MF).
    """
    alpha_pos, alpha_neg, beta, w = model_parameters
    n_trials = len(action_1)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)       
    q_stage2_mf = np.zeros((2, 2))  

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial] 

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates with Asymmetric Learning ---
        
        # Stage 1 Update (TD(0))
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        lr_s1 = alpha_pos if delta_stage1 > 0 else alpha_neg
        q_stage1_mf[action_1[trial]] += lr_s1 * delta_stage1

        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        lr_s2 = alpha_pos if delta_stage2 > 0 else alpha_neg
        q_stage2_mf[state_idx, action_2[trial]] += lr_s2 * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Eligibility Traces (TD(lambda) / Direct Reinforcement)
This model introduces an eligibility trace parameter (`lambda`). Instead of only updating the first-stage value based on the second-stage value (temporal difference), the first-stage choice is also directly reinforced by the final reward outcome. This bridges the gap between pure Model-Free TD learning and Monte Carlo methods, allowing the final reward to "leak" back to the first choice immediately.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Eligibility Traces (TD-lambda).
    
    Incorporates an eligibility trace parameter (lambda_decay). This allows the reward 
    obtained at the second stage to directly influence the value of the first-stage 
    choice, rather than just propagating through the second-stage value estimate.
    High lambda makes the agent learn faster about the first stage from final outcomes.

    Parameters:
    learning_rate: [0, 1] Update rate for Q-values.
    beta: [0, 10] Inverse temperature for softmax.
    w: [0, 1] Weight for Model-Based control.
    lambda_decay: [0, 1] Eligibility trace decay factor (0 = TD(0), 1 = Monte Carlo).
    """
    learning_rate, beta, w, lambda_decay = model_parameters
    n_trials = len(action_1)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)       
    q_stage2_mf = np.zeros((2, 2))  

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial] 

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates with Eligibility Trace ---
        
        # Calculate prediction errors
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Update Stage 2 (Standard TD)
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Update Stage 1: Standard TD part + Eligibility Trace part
        # The Stage 1 value is updated by its own prediction error, PLUS
        # a fraction (lambda) of the stage 2 prediction error.
        q_stage1_mf[action_1[trial]] += learning_rate * (delta_stage1 + lambda_decay * delta_stage2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Separate Learning Rates for Stage 1 and Stage 2
This model posits that learning happens at different speeds for the two stages of the task. The participant might be very quick to update which alien gives gold (`lr_stage2`) but slow to update the value of the spaceships (`lr_stage1`), or vice versa. This decouples the plasticity of the high-level plan from the low-level execution.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Separate Learning Rates for Stage 1 and Stage 2.
    
    This model assumes the participant might have different plasticity for the 
    spaceship choice (Stage 1) versus the alien choice (Stage 2). For example, 
    they might learn the reward probabilities of aliens quickly but integrate 
    that information into spaceship values slowly.

    Parameters:
    lr_stage1: [0, 1] Learning rate for the first decision stage (Spaceships).
    lr_stage2: [0, 1] Learning rate for the second decision stage (Aliens).
    beta: [0, 10] Inverse temperature for softmax.
    w: [0, 1] Weight for Model-Based control.
    """
    lr_stage1, lr_stage2, beta, w = model_parameters
    n_trials = len(action_1)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)       
    q_stage2_mf = np.zeros((2, 2))  

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial] 

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates with Separate Learning Rates ---
        
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr_stage1 * delta_stage1

        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr_stage2 * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```