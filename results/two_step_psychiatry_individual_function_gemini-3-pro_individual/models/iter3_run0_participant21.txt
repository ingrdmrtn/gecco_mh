Here are three new cognitive models based on the two-step decision task. These models explore different mechanisms such as perseverance (stickiness), separate learning rates for positive/negative outcomes, and a hybrid model-based/model-free architecture.

### Model 1: Stickiness (Perseverance) Model
This model extends the basic reinforcement learning framework by adding a "stickiness" parameter. This parameter captures the tendency of participants to repeat their previous first-stage choice regardless of the reward history, a common behavioral phenomenon in this task.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Q-Learning with Choice Stickiness.

    This model adds a perseverance (stickiness) bonus to the Q-values of the 
    previously chosen action at the first stage. This accounts for habit-like 
    repetition of choices independent of reward history.

    Parameters:
    learning_rate: [0, 1] - Rate at which Q-values are updated.
    beta: [0, 10] - Inverse temperature for softmax choice rule.
    stickiness: [0, 5] - Bonus added to the Q-value of the previously chosen action.
    """
    learning_rate, beta, stickiness = model_parameters
    n_trials = len(action_1)
  
    # Initialize Q-values
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_action_1 = -1 # No previous action for the first trial

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Add stickiness bonus to the Q-values temporarily for decision making
        q_net_stage1 = q_stage1_mf.copy()
        if prev_action_1 != -1:
            q_net_stage1[prev_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Store current action for next trial's stickiness
        prev_action_1 = action_1[trial]
        
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        # TD(0) update for Stage 1
        # Note: Stickiness is a bias in choice, not learned value, so we update the raw Q
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # TD(0) update for Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Asymmetric Learning Rates (Positive/Negative)
This model hypothesizes that the participant learns differently from positive outcomes (finding gold) versus negative outcomes (no gold). This is often observed in clinical populations where sensitivity to reward or punishment differs.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Q-Learning with Asymmetric Learning Rates.

    This model uses two separate learning rates: one for positive prediction errors 
    (when outcome > expectation) and one for negative prediction errors 
    (when outcome < expectation).

    Parameters:
    alpha_pos: [0, 1] - Learning rate for positive prediction errors.
    alpha_neg: [0, 1] - Learning rate for negative prediction errors.
    beta: [0, 10] - Inverse temperature for softmax.
    """
    alpha_pos, alpha_neg, beta = model_parameters
    n_trials = len(action_1)
  
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        if delta_stage1 > 0:
            q_stage1_mf[action_1[trial]] += alpha_pos * delta_stage1
        else:
            q_stage1_mf[action_1[trial]] += alpha_neg * delta_stage1

        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        if delta_stage2 > 0:
            q_stage2_mf[state_idx, action_2[trial]] += alpha_pos * delta_stage2
        else:
            q_stage2_mf[state_idx, action_2[trial]] += alpha_neg * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model-Based / Model-Free
This model combines Model-Free (habitual) and Model-Based (goal-directed) value estimation. The `w_mb` parameter controls the weighting between these two systems. While you have seen this combination before, this implementation strictly adheres to the template structure provided in the prompt, calculating the MB values explicitly via the transition matrix and mixing them at the decision stage.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Reinforcement Learning.

    This model calculates a weighted average of Model-Free (MF) and Model-Based (MB) 
    values for the first-stage decision. 
    - MF values are learned via TD error.
    - MB values are computed using the known transition matrix and max Stage 2 values.

    Parameters:
    learning_rate: [0, 1] - Learning rate for MF Q-values.
    beta: [0, 10] - Inverse temperature for softmax.
    w_mb: [0, 1] - Weight of Model-Based control (0 = Pure MF, 1 = Pure MB).
    """
    learning_rate, beta, w_mb = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition matrix as per task description (70% common, 30% rare)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # 1. Calculate Model-Based Value (Bellman equation)
        # Q_MB(s1, a) = sum(P(s2|s1,a) * max_a'(Q(s2, a')))
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Mix MF and MB values
        q_net_stage1 = (1 - w_mb) * q_stage1_mf + w_mb * q_stage1_mb

        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        # Stage 2 is purely model-free (terminal state)
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        # Update MF values
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```