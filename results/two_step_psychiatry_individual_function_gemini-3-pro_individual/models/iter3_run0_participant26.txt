Here are three new cognitive models for the two-step task. These models explore different hypotheses about how the participant might be learning, specifically focusing on separate learning rates for positive/negative outcomes, a pure model-based strategy, and a hybrid strategy with eligibility traces.

### Model 1: Asymmetric Learning Rates (Pos/Neg)
This model hypothesizes that the participant learns differently from wins (getting a coin) versus losses (getting nothing). This is a common finding in reinforcement learning studies where "optimism" or "pessimism" biases the update rule.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free RL with Asymmetric Learning Rates.

    Hypothesis: The participant updates their value estimates differently depending on 
    whether the outcome was positive (reward=1) or negative (reward=0).
    
    Parameters:
    alpha_pos: [0, 1] - Learning rate for positive prediction errors (wins).
    alpha_neg: [0, 1] - Learning rate for negative prediction errors (losses).
    beta: [0, 10] - Inverse temperature for softmax choice.
    """
    alpha_pos, alpha_neg, beta = model_parameters
    n_trials = len(action_1)
  
    # Initialize Q-values
    q_stage1_mf = np.zeros(2)      # Q-values for Stage 1 actions (Spaceships)
    q_stage2_mf = np.zeros((2, 2)) # Q-values for Stage 2 actions (Aliens) given state (Planet)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial] # 0 or 1 (Planet X or Y)

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # Stage 1 Update (TD-0 using Stage 2 Q-value as proxy for reward)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        if delta_stage1 > 0:
            q_stage1_mf[action_1[trial]] += alpha_pos * delta_stage1
        else:
            q_stage1_mf[action_1[trial]] += alpha_neg * delta_stage1

        # Stage 2 Update (Actual Reward)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        if delta_stage2 > 0:
            q_stage2_mf[state_idx, action_2[trial]] += alpha_pos * delta_stage2
        else:
            q_stage2_mf[state_idx, action_2[trial]] += alpha_neg * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Pure Model-Based Learning
This model tests the hypothesis that the participant is strictly "Model-Based." Instead of caching values for the spaceships (Model-Free), they calculate the value of a spaceship on the fly by combining the known transition probabilities with the learned values of the aliens.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Based Reinforcement Learning.

    Hypothesis: The participant calculates the value of stage 1 actions (spaceships) 
    by combining a learned model of the transition structure (assumed fixed here as 70/30)
    with the learned values of the stage 2 states (planets/aliens).
    
    Parameters:
    learning_rate: [0, 1] - Update rate for Stage 2 Q-values (aliens).
    beta: [0, 10] - Inverse temperature for softmax choice.
    """
    learning_rate, beta = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition matrix (70% common, 30% rare)
    # Row 0: Spaceship A -> [Planet X, Planet Y]
    # Row 1: Spaceship U -> [Planet X, Planet Y]
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # We only need to learn Stage 2 values; Stage 1 values are computed.
    q_stage2_mb = np.zeros((2, 2)) 

    for trial in range(n_trials):

        # --- Stage 1 Policy (Model-Based Calculation) ---
        # Value of a planet is the max value of the aliens on it
        max_q_stage2 = np.max(q_stage2_mb, axis=1) 
        
        # Value of a spaceship is the weighted sum of planet values
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mb[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # Only Stage 2 is updated directly from reward. 
        # Stage 1 values change implicitly because q_stage2_mb changes.
        delta_stage2 = reward[trial] - q_stage2_mb[state_idx, action_2[trial]]
        q_stage2_mb[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Model-Free with Eligibility Traces (TD(lambda))
This model introduces an eligibility trace parameter (`lambda`). In standard TD learning (Model 1), the reward at the second stage updates the first stage only via the second stage's *estimate*. With eligibility traces, the reward at the end of the trial can "reach back" and directly update the first stage choice, bridging the temporal gap.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free TD(lambda) Learning.

    Hypothesis: The participant uses eligibility traces to update the first-stage choice 
    based on the final outcome, rather than just the second-stage value estimate.
    
    Parameters:
    learning_rate: [0, 1] - Update rate for Q-values.
    beta: [0, 10] - Inverse temperature for softmax choice.
    lambd: [0, 1] - Eligibility trace decay parameter (0 = TD(0), 1 = Monte Carlo).
    """
    learning_rate, beta, lambd = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # 1. Prediction error at stage 1 (based on stage 2 value)
        delta1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        # 2. Prediction error at stage 2 (based on reward)
        delta2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Update Stage 1: standard TD update + eligibility trace from Stage 2 error
        # The trace for Stage 1 action is 1 at step 1, then decays by lambda at step 2.
        # So the Stage 1 value absorbs the immediate error (delta1) AND a portion of the future error (delta2).
        q_stage1_mf[action_1[trial]] += learning_rate * (delta1 + lambd * delta2)

        # Update Stage 2: Standard TD update
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```