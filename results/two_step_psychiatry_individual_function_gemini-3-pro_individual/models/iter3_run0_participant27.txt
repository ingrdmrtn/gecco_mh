Here are three cognitive models based on the two-step task paradigm. They range from a pure Model-Free learner to a Hybrid learner, and finally a learner with separate learning rates for positive and negative outcomes.

### Model 1: Pure Model-Free Reinforcement Learning (TD(1))
This model assumes the participant does not use the transition structure of the task (the map of spaceships to planets) at all. Instead, it learns the value of the first-stage actions (spaceships) directly from the final reward, effectively "caching" the value of the spaceship based on the outcome, regardless of the intermediate planet. This is a standard TD(1) or Monte Carlo-like update for the first stage.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free Reinforcement Learning (TD(1)).
    
    The agent ignores the transition structure (transition matrix).
    It updates the first-stage action values directly based on the final reward obtained.
    This captures a simple "habitual" strategy where actions are repeated if rewarded.

    Parameters:
    - learning_rate: [0, 1] Rate at which Q-values are updated.
    - beta: [0, 10] Inverse temperature for softmax choice.
    - eligibility_trace: [0, 1] How much credit the first stage gets from the second stage reward.
    """
    learning_rate, beta, eligibility_trace = model_parameters
    n_trials = len(action_1)

    # Q-values initialized to 0.5 (neutral)
    q_stage1_mf = np.zeros(2) + 0.5
    q_stage2_mf = np.zeros((2, 2)) + 0.5
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial] # Planet arrived at

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        # Prediction error at second stage (Reward - Q_stage2)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Update Stage 2 value
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Update Stage 1 value directly from the stage 2 prediction error (TD(1) / eligibility trace logic)
        # In a strict TD(0) model, we'd update based on Q_stage2. Here we allow direct reward influence.
        q_stage1_mf[action_1[trial]] += learning_rate * eligibility_trace * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model-Based / Model-Free Learner
This is the classic "Daw et al. (2011)" style model. It assumes the participant's brain computes two different values for the first-stage choice: a Model-Based value (planning using the transition matrix) and a Model-Free value (learning from experience). The final choice is a weighted combination of these two systems, controlled by a mixing parameter `w`.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Learner.
    
    The agent computes a weighted sum of Model-Based (MB) and Model-Free (MF) values
    for the first-stage choice. The MB system uses the transition matrix and max Q-values
    from stage 2. The MF system uses TD(0) learning.

    Parameters:
    - learning_rate: [0, 1] Learning rate for MF values (both stages).
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weighting parameter. 1 = Pure Model-Based, 0 = Pure Model-Free.
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # 1. Calculate Model-Based values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Combine MB and MF values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # TD(0) update for Stage 1 (SARSA-style)
        # Note: We use the value of the CHOSEN state 2 option, not the max (SARSA vs Q-learning)
        # This is standard in many 2-step implementations to capture actual experience.
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # TD update for Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Asymmetric Learning Rate Model (Risk Sensitivity)
This model investigates if the participant learns differently from gains (positive prediction errors) versus losses (negative prediction errors). It is a pure Model-Free learner but splits the learning rate into `alpha_pos` and `alpha_neg`. This is useful if a participant is very quick to adopt a winning strategy but slow to abandon it, or vice versa.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Asymmetric Learning Rate Model (Model-Free).
    
    This model allows for different learning rates for positive and negative prediction errors.
    This captures risk-seeking or risk-averse learning dynamics (e.g., learning more from
    wins than losses). It assumes a simple TD(1) structure for stage 1 updating.

    Parameters:
    - alpha_pos: [0, 1] Learning rate for positive prediction errors (RPE > 0).
    - alpha_neg: [0, 1] Learning rate for negative prediction errors (RPE < 0).
    - beta: [0, 10] Inverse temperature.
    """
    alpha_pos, alpha_neg, beta = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # Calculate Prediction Error at outcome
        rpe = reward[trial] - q_stage2[state_idx, action_2[trial]]
        
        # Determine which learning rate to use
        lr = alpha_pos if rpe >= 0 else alpha_neg
        
        # Update Stage 2
        q_stage2[state_idx, action_2[trial]] += lr * rpe
        
        # Update Stage 1 (Direct reinforcement from outcome)
        # We use the same RPE to update the first stage choice directly
        rpe_s1 = reward[trial] - q_stage1[action_1[trial]]
        lr_s1 = alpha_pos if rpe_s1 >= 0 else alpha_neg
        q_stage1[action_1[trial]] += lr_s1 * rpe_s1
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```