Here are 3 new cognitive models exploring different mechanisms and parameter combinations not yet listed in the feedback.

### Model 1: Asymmetric Learning Rates for Stage 1 vs Stage 2
This model tests the hypothesis that the participant learns at different speeds for the two different stages of the task. The first stage involves abstract spaceship choices leading to planets, while the second stage involves concrete alien choices leading to immediate reward. It is plausible that learning rates differ between these contexts. It uses a standard hybrid structure (mixing Model-Based and Model-Free) but splits the learning rate.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Separate Learning Rates for Stages:
    Differentiates between learning from the transition (Stage 1) and learning 
    from the immediate reward (Stage 2).
    
    Parameters:
    lr_stage1: [0, 1] Learning rate for Stage 1 (Spaceship choice) MF updates.
    lr_stage2: [0, 1] Learning rate for Stage 2 (Alien choice) Q-value updates.
    beta: [0, 10] Inverse temperature for both stages.
    w: [0, 1] Weight mixing MB (1) and MF (0).
    """
    lr_stage1, lr_stage2, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combine MB and MF values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Update Stage 1 MF values using lr_stage1
        # Note: Standard TD(0) update for stage 1 uses the value of the state reached
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr_stage1 * delta_stage1
        
        # Update Stage 2 values using lr_stage2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr_stage2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Eligibility Trace Decay (TD(lambda))
This model introduces an eligibility trace parameter (`lam`). Instead of strictly separating the updates (where Stage 1 update depends on Stage 2 value, and Stage 2 update depends on reward), this model allows the Stage 1 value to be updated directly by the final reward prediction error (`delta_stage2`) to a degree determined by lambda. This captures the intuition that the initial choice is reinforced by the final outcome directly, not just by the intermediate state value.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    TD(lambda) Model:
    Uses an eligibility trace (lambda) to allow the final reward prediction error
    to directly update the first-stage choice, bridging the gap between action and outcome.
    
    Parameters:
    learning_rate: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Weight mixing MB (1) and MF (0).
    lam: [0, 1] Eligibility trace decay parameter (lambda). 
                0 = TD(0) (pure split), 1 = Monte Carlo (full propagation).
    """
    learning_rate, beta, w, lam = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # TD(1) / Eligibility Trace Logic
        # 1. Calculate prediction error at stage 1 (transition to state)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        # 2. Calculate prediction error at stage 2 (reward receipt)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Update Stage 1: It gets its own error PLUS a decayed portion of the stage 2 error
        q_stage1_mf[action_1[trial]] += learning_rate * (delta_stage1 + lam * delta_stage2)
        
        # Update Stage 2: Standard update
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Separate Inverse Temperatures (Beta) for Stage 1 vs Stage 2
This model suggests that the level of exploration/exploitation differs between the two stages. Participants might be very decisive (high beta) when choosing a spaceship but more exploratory (low beta) when choosing an alien, or vice versa. This decouples the noise parameter for the two distinct decisions.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Separate Noise Parameters (Beta):
    Allows for different levels of exploration/exploitation randomness 
    at the spaceship choice stage versus the alien choice stage.
    
    Parameters:
    learning_rate: [0, 1] Learning rate.
    beta_1: [0, 10] Inverse temperature for Stage 1 (Spaceship).
    beta_2: [0, 10] Inverse temperature for Stage 2 (Alien).
    w: [0, 1] Weight mixing MB (1) and MF (0).
    """
    learning_rate, beta_1, beta_2, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Use beta_1 for first choice
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        # Use beta_2 for second choice
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Standard Updates
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```