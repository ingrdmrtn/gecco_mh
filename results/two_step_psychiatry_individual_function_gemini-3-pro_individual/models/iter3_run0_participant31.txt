Here are three new cognitive models based on the provided template and task description. These models explore different strategies: Model-Based learning, a Hybrid learner (mixing Model-Based and Model-Free), and a Model-Free learner with eligibility traces (TD(1)).

### Model 1: Pure Model-Based Learner
This model assumes the participant builds a mental map of the task structure. Instead of learning the value of spaceships directly from reward history (like the Model-Free learner), it learns the value of the aliens (stage 2) and uses the known transition probabilities (70/30) to calculate the expected value of each spaceship (stage 1).

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Based Learner.
    
    This model calculates the value of stage 1 actions (spaceships) by combining
    learned stage 2 values (aliens) with the known transition matrix. It assumes
    the participant understands the transition structure (0.7/0.3).

    Parameters:
    learning_rate: [0,1] Rate at which stage 2 Q-values are updated.
    beta: [0,10] Inverse temperature for softmax choice.
    """
    learning_rate, beta = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition matrix: Row 0 -> Planet 0 (0.7), Planet 1 (0.3); Row 1 -> Planet 0 (0.3), Planet 1 (0.7)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values for stage 2 (Aliens on Planet X and Y)
    q_stage2_mf = np.zeros((2, 2)) 

    for trial in range(n_trials):

        # --- Stage 1 Policy (Model-Based) ---
        # Calculate max value available on each planet
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        # Compute Q-values for Stage 1 by projecting Stage 2 values through the transition matrix
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial] # The planet actually reached

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # Note: In a pure Model-Based agent, we only update the Stage 2 values based on reward.
        # Stage 1 values are re-computed on the fly next trial using the matrix.
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Learner (MB + MF)
This is the classic "Daw et al. (2011)" style model. It assumes the participant uses a weighted combination of Model-Free (habitual) and Model-Based (goal-directed) systems to make the first-stage choice. A mixing parameter `w` determines which system dominates.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner (Model-Based + Model-Free).
    
    This model computes stage 1 values as a weighted sum of Model-Based (planning)
    and Model-Free (TD learning) estimates.
    
    Parameters:
    learning_rate: [0,1] Update rate for Q-values.
    beta: [0,10] Inverse temperature for softmax.
    w: [0,1] Mixing weight. 1 = Pure Model-Based, 0 = Pure Model-Free.
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)      # Model-Free values for Stage 1
    q_stage2_mf = np.zeros((2, 2)) # Values for Stage 2 (shared by MB and MF logic)

    for trial in range(n_trials):

        # --- Stage 1 Policy (Hybrid) ---
        # 1. Calculate Model-Based values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Combine MB and MF values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # Update Stage 1 MF value using the value of the state reached (SARSA-style or Q-learning)
        # Here using the value of the chosen option in stage 2 as the target
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Update Stage 2 value based on reward
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Model-Free with Eligibility Traces (TD(1))
This model is purely Model-Free but more sophisticated than simple TD(0). It uses an eligibility trace parameter `lambda_eligibility`. When a reward is received at the end of the trial, it updates not just the second-stage choice, but "passes back" the reward prediction error to the first-stage choice as well. This allows the first stage to learn directly from the final outcome, skipping the intermediate state value.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Learner with Eligibility Traces (TD(lambda)).
    
    This model allows the reward at the second stage to directly influence the 
    value of the first stage choice, modulated by an eligibility parameter (lambda).
    If lambda is 1, the first stage learns directly from the reward. If 0, it behaves like TD(0).

    Parameters:
    learning_rate: [0,1] Update rate.
    beta: [0,10] Inverse temperature.
    lambda_eligibility: [0,1] Eligibility trace decay.
    """
    learning_rate, beta, lambda_eligibility = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # 1. Prediction error at stage 1 (based on stage 2 value)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # 2. Prediction error at stage 2 (based on reward)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # 3. Eligibility Trace Update: Apply Stage 2 error to Stage 1 choice
        # This bridges the gap between Action 1 and Reward
        q_stage1_mf[action_1[trial]] += learning_rate * lambda_eligibility * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```