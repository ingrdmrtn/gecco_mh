Here are 3 new cognitive models exploring different mechanisms for learning and decision-making in the two-step task.

### Model 1: Hybrid Model with Separate Learning Rates for Stage 1 and Stage 2
This model hypothesizes that the participant might learn at different speeds for the high-level planning stage (Stage 1 choice between spaceships) versus the direct reward-learning stage (Stage 2 choice between aliens). This separation can capture if a participant is quick to update their alien preferences but slow to update their spaceship preferences (or vice versa).

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free RL with Separate Stage Learning Rates.
    
    This model allows for different learning rates for the first stage (spaceship choice)
    and the second stage (alien choice), testing if the participant updates values
    differently based on the hierarchical level of the decision.

    Parameters:
    lr_stage1: [0, 1] - Learning rate for Stage 1 (MF) Q-values.
    lr_stage2: [0, 1] - Learning rate for Stage 2 Q-values.
    beta: [0, 10] - Inverse temperature for softmax choice.
    w: [0, 1] - Weighting parameter (0 = pure Model-Free, 1 = pure Model-Based).
    """
    lr_stage1, lr_stage2, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combine MB and MF values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Update Stage 1 MF values using lr_stage1
        # Note: Standard MF update uses the value of the state reached (q_stage2_mf)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr_stage1 * delta_stage1
        
        # Update Stage 2 values using lr_stage2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr_stage2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Pure Model-Based with Forgetting (Decay)
This model assumes the participant is purely Model-Based (no Model-Free caching at Stage 1), but their memory of the alien values (Stage 2 Q-values) decays over time. Instead of just learning from rewards, unchosen options slowly revert to 0 or decay, representing a "use it or lose it" memory trace for the changing reward probabilities.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Based RL with Value Decay (Forgetting).
    
    Assumes the participant is fully Model-Based (w=1 fixed implictly) but experiences
    memory decay on the Q-values of the aliens. This helps capture the
    changing nature of the reward probabilities by reducing the influence of old interactions.

    Parameters:
    learning_rate: [0, 1] - Rate at which Q-values are updated from rewards.
    beta: [0, 10] - Inverse temperature for softmax choice.
    decay_rate: [0, 1] - Rate at which unchosen/all Q-values decay toward 0 each trial.
    """
    learning_rate, beta, decay_rate = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Only Stage 2 Q-values are needed for a pure MB agent to calculate Stage 1 values
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice (Pure Model-Based)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Update chosen option
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Apply decay to ALL values (passive forgetting)
        # This pulls values slightly towards 0 every trial
        q_stage2_mf *= (1 - decay_rate)
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model with Outcome-Specific Learning Rates (Reward vs. No Reward)
This model investigates if the participant learns differently from positive outcomes (finding gold) versus negative outcomes (no gold). This asymmetry is often seen in clinical populations (e.g., depression or addiction) where sensitivity to reward or punishment differs. This is applied specifically to the Stage 2 updates, which drive the entire system.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free RL with Asymmetric Learning Rates (Reward vs No-Reward).
    
    Splits the learning rate into alpha_win (for rewarded trials) and alpha_loss 
    (for unrewarded trials) to test for valence-dependent learning asymmetries.

    Parameters:
    alpha_win: [0, 1] - Learning rate when reward is received (1).
    alpha_loss: [0, 1] - Learning rate when no reward is received (0).
    beta: [0, 10] - Inverse temperature for softmax choice.
    w: [0, 1] - Weighting parameter (0 = pure Model-Free, 1 = pure Model-Based).
    """
    alpha_win, alpha_loss, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Determine effective learning rate based on outcome
        if reward[trial] > 0:
            current_lr = alpha_win
        else:
            current_lr = alpha_loss

        # Stage 1 MF update (using the same LR for consistency with the outcome that drove the update)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += current_lr * delta_stage1
        
        # Stage 2 update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += current_lr * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```