Here are three new cognitive models that explore different mechanisms than the pure Model-Free TD learner provided in the feedback. These models introduce Model-Based learning, a hybrid architecture, and a distinct perseverance mechanism.

### Model 1: Pure Model-Based Learner
This model assumes the participant builds an internal map of the task structure (Transition Matrix) and calculates values by forward planning (Bellman equation), rather than caching values through reward prediction errors. It learns the transition probabilities dynamically.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Based Learner with Dynamic Transition Learning.
    
    This model computes Stage 1 values by planning forward using a learned 
    transition matrix and the max Stage 2 values. It does not use TD errors 
    for Stage 1. It updates the transition matrix based on observed state transitions.

    Parameters:
    learning_rate: [0, 1] - Rate at which Stage 2 Q-values are updated.
    beta: [0, 10] - Inverse temperature for softmax choice.
    transition_lr: [0, 1] - Learning rate for updating the transition matrix estimate.
    """
    learning_rate, beta, transition_lr = model_parameters
    n_trials = len(action_1)
  
    # Initialize transition matrix (State 0 | Action 0, State 0 | Action 1)
    # Rows: Action (0 or 1), Cols: Next State (0 or 1)
    # Initial belief is uniform or slight bias towards instructions
    transition_counts = np.array([[0.5, 0.5], [0.5, 0.5]]) 
    
    # Q-values for stage 2 (Aliens)
    q_stage2_mf = np.zeros((2, 2)) # (State, Alien)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Decision (Model-Based Planning) ---
        # Normalize counts to get probabilities
        row_sums = transition_counts.sum(axis=1, keepdims=True)
        trans_probs = transition_counts / row_sums
        
        # Calculate MB values: Q_MB(s1, a1) = sum(P(s2|s1,a1) * max(Q(s2, :)))
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = trans_probs @ max_q_stage2
        
        # Softmax for Stage 1
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Decision ---
        state_idx = state[trial]
        
        # Softmax for Stage 2
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]

        # 1. Update Stage 2 Q-values (Model-Free)
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # 2. Update Transition Matrix (Model-Based Learning)
        # We increase the count for the observed transition
        # Instead of just adding 1, we use a learning rate approach to approximate 
        # a moving window or forgetting factor: New = Old + lr * (Target - Old)
        # Here we implement a simple counter update weighted by transition_lr
        
        # Create a one-hot vector for the observed state
        observed_transition = np.zeros(2)
        observed_transition[state_idx] = 1.0
        
        # Update the row corresponding to the chosen action
        transition_counts[a1] = (1 - transition_lr) * transition_counts[a1] + transition_lr * observed_transition

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model-Based / Model-Free Learner
This is the classic "Daw" model architecture, where the agent calculates both a Model-Based value (planning) and a Model-Free value (TD learning) for the first stage, and combines them using a weighting parameter `w`.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner (MB/MF Mixture).
    
    This model computes Stage 1 values as a weighted sum of Model-Based (planning)
    and Model-Free (TD) values.
    
    Parameters:
    learning_rate: [0, 1] - Rate for updating MF Q-values.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - Weight parameter. 1 = Pure Model-Based, 0 = Pure Model-Free.
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition matrix as per task instructions (70/30)
    # Row 0: Action 0 (A) -> [Prob State 0, Prob State 1]
    # Row 1: Action 1 (B) -> [Prob State 0, Prob State 1]
    # Assuming A->X(0) is common (0.7) and B->Y(1) is common (0.7)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):

        # --- Stage 1 Decision ---
        # 1. Calculate Model-Based Value (Planning)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Combine MB and MF values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Decision ---
        state_idx = state[trial]
        
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]

        # TD Error Stage 2
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # TD Error Stage 1 (SARSA style update using Stage 2 value)
        # Note: In standard Hybrid models, the MF update for stage 1 uses the 
        # value of the state actually reached (q_stage2_mf[state_idx, a2])
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        
        # Update Stage 1 MF value (using eligibility trace lambda=1 implicitly for simplicity in this variant)
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Choice Kernel Perseverance (Habit) Model
This model adds a "Choice Kernel" (or stickiness) to the pure Model-Free learner. This captures the tendency to repeat the previously chosen action regardless of reward history (motor perseveration), which is distinct from reward-based reinforcement.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Learner with Choice Perseverance (Stickiness).
    
    This model includes a 'choice kernel' that tracks the frequency of recent
    choices. This adds a bias to repeat (or switch) actions independent of reward.
    
    Parameters:
    learning_rate: [0, 1] - Rate for Q-value updates.
    beta: [0, 10] - Inverse temperature.
    perseverance: [0, 5] - Strength of the "stickiness" bias. Positive = repeat, Negative = switch.
    choice_trace_decay: [0, 1] - Rate at which the choice trace decays (alpha_k).
    """
    learning_rate, beta, perseverance, choice_trace_decay = model_parameters
    n_trials = len(action_1)
  
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Choice trace (kernel) for Stage 1 actions
    choice_trace = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # Value is Q-value + Perseverance Bonus
        net_value_1 = q_stage1_mf + (perseverance * choice_trace)
        
        exp_q1 = np.exp(beta * net_value_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Decision ---
        # (Standard softmax on Q-values)
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]

        # Update Choice Trace
        # Decay all traces
        choice_trace *= (1 - choice_trace_decay)
        # Increment trace for chosen action
        choice_trace[a1] += 1.0

        # Update Q-values (Standard TD)
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # TD(1) style update for Stage 1
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```