Here are three new cognitive models exploring different mechanisms for learning and decision-making in the two-step task. These models move beyond simple Model-Free (MF) or Model-Based (MB) distinctions to look at separate learning rates for different stages and eligibility traces.

### Model 1: Hybrid Learner with Eligibility Traces (Lambda)
This model implements a classic reinforcement learning algorithm known as SARSA($\lambda$). Instead of just updating the immediately preceding state-action pair, it uses an "eligibility trace" ($\lambda$) to update the Stage 1 choice based on the final reward received at Stage 2. This allows the Stage 1 value to be updated directly by the final outcome, bridging the gap between the first action and the reward.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Model 1: Hybrid Learner with Eligibility Traces (SARSA-Lambda).
    Uses an eligibility trace parameter (lambda) to allow the Stage 2 reward 
    to directly influence the Stage 1 value update, bridging the temporal gap.
    
    Bounds:
    learning_rate: [0, 1] - Learning rate for value updates.
    beta: [0, 10] - Inverse temperature for softmax.
    lambda_param: [0, 1] - Eligibility trace decay parameter (0 = TD(0), 1 = Monte Carlo).
    """
    learning_rate, beta, lambda_param = model_parameters
    n_trials = len(action_1)

    # Q-values initialization
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        state_idx = state[trial]

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]

        # --- Learning Updates ---
        # Prediction error at Stage 1 (TD error) based on Stage 2 Q-value
        delta_1 = q_stage2[state_idx, a2] - q_stage1[a1]
        
        # Prediction error at Stage 2 based on Reward
        delta_2 = r - q_stage2[state_idx, a2]
        
        # Update Stage 2 Q-value (standard TD update)
        q_stage2[state_idx, a2] += learning_rate * delta_2
        
        # Update Stage 1 Q-value
        # It gets the immediate TD error (delta_1) PLUS a portion of the Stage 2 error (delta_2)
        # scaled by lambda. If lambda=1, Stage 1 learns fully from the final reward.
        q_stage1[a1] += learning_rate * (delta_1 + lambda_param * delta_2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Dual Learning Rates (Stage-Specific Learning)
Standard models often assume a single learning rate for the whole task. However, Stage 1 (choosing a spaceship) and Stage 2 (choosing an alien) are cognitively distinct. Stage 2 is a "bandit" problem (immediate reward), while Stage 1 is a planning problem. This model allows the participant to learn at different speeds for the high-level planning stage (`lr_stage1`) versus the low-level reward stage (`lr_stage2`).

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model 2: Dual Learning Rates (Stage-Specific Learning).
    Assumes the participant might update their beliefs about spaceships (Stage 1)
    at a different rate than their beliefs about aliens (Stage 2).
    
    Bounds:
    lr_stage1: [0, 1] - Learning rate for Stage 1 (spaceships).
    lr_stage2: [0, 1] - Learning rate for Stage 2 (aliens).
    beta: [0, 10] - Inverse temperature for softmax.
    """
    lr_stage1, lr_stage2, beta = model_parameters
    n_trials = len(action_1)

    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        state_idx = state[trial]

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]

        # --- Learning Updates ---
        
        # Stage 1 Update: Driven by the transition to the Stage 2 state value
        # Uses specific Stage 1 learning rate
        delta_1 = q_stage2[state_idx, a2] - q_stage1[a1]
        q_stage1[a1] += lr_stage1 * delta_1
        
        # Stage 2 Update: Driven by the reward
        # Uses specific Stage 2 learning rate
        delta_2 = r - q_stage2[state_idx, a2]
        q_stage2[state_idx, a2] += lr_stage2 * delta_2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Asymmetric Learning Rates (Positive vs. Negative)
Participants often weigh positive outcomes (gains) differently from negative outcomes (omissions/losses). This model implements "confirmation bias" or asymmetric updating in the second stage. It splits the learning rate into `alpha_pos` (for rewarded trials) and `alpha_neg` (for unrewarded trials), applying this asymmetry to the Stage 2 updates where the actual reward is received.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model 3: Asymmetric Learning Rates (Positive vs. Negative).
    Differentiates learning from positive rewards (alpha_pos) vs 
    lack of rewards (alpha_neg) at the second stage.
    
    Bounds:
    alpha_pos: [0, 1] - Learning rate when reward is received (R=1).
    alpha_neg: [0, 1] - Learning rate when reward is not received (R=0).
    beta: [0, 10] - Inverse temperature for softmax.
    """
    alpha_pos, alpha_neg, beta = model_parameters
    n_trials = len(action_1)

    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        state_idx = state[trial]

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]

        # --- Learning Updates ---
        
        # Determine which learning rate to use based on outcome
        if r > 0:
            current_lr = alpha_pos
        else:
            current_lr = alpha_neg

        # Stage 1 Update (TD-0 style using the chosen LR for consistency, though debated)
        delta_1 = q_stage2[state_idx, a2] - q_stage1[a1]
        q_stage1[a1] += current_lr * delta_1
        
        # Stage 2 Update (The primary site of reward learning)
        delta_2 = r - q_stage2[state_idx, a2]
        q_stage2[state_idx, a2] += current_lr * delta_2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```