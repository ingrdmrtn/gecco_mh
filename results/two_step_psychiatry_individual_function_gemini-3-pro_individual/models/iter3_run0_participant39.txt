Here are three new cognitive models exploring different mechanisms for learning and decision-making in the two-step task. These models introduce variations in how value is updated (e.g., forgetting), how model-based and model-free systems interact, and how reward prediction errors are processed.

### Model 1: Hybrid Learner with Decay
This model combines model-based (MB) and model-free (MF) learning but adds a "decay" parameter. In standard Q-learning, unchosen actions retain their old values indefinitely. This model assumes that the values of unvisited states or unchosen actions slowly decay back to a baseline (0.5), reflecting a forgetting process or an assumption of environmental volatility.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model (MB/MF) with Value Decay.
    Combines model-based and model-free value estimation. Crucially, it includes a 
    decay parameter that pulls all Q-values (both stages) toward a neutral value (0.5) 
    on every trial, simulating forgetting or environmental volatility.

    Parameters:
    learning_rate: [0, 1] - Rate of learning from prediction errors.
    beta: [0, 10] - Inverse temperature for softmax choice.
    w: [0, 1] - Weighting between Model-Based (1) and Model-Free (0) control.
    decay_rate: [0, 1] - Rate at which Q-values decay to 0.5 (0 = no decay, 1 = instant reset).
    """
    learning_rate, beta, w, decay_rate = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition matrix as per task description (A->X=0.7, U->Y=0.7)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Initialize Q-values at 0.5 (neutral expectation)
    q_stage1_mf = np.ones(2) * 0.5
    q_stage2_mf = np.ones((2, 2)) * 0.5

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based Value Calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax Policy Stage 1
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial] # 0 for Planet X, 1 for Planet Y

        # --- Stage 2 Choice ---
        # Standard Model-Free Q-learning for Stage 2
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # SARSA(0) / TD(0) update for Stage 1 MF
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Reward Prediction Error for Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # --- Decay Step ---
        # All Q-values decay towards 0.5
        q_stage1_mf = (1 - decay_rate) * q_stage1_mf + decay_rate * 0.5
        q_stage2_mf = (1 - decay_rate) * q_stage2_mf + decay_rate * 0.5

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Asymmetric Learning Rates (Positive vs Negative)
This model focuses on "Confirmation Bias" or "Optimism/Pessimism" in learning. It hypothesizes that the participant updates their value estimates differently depending on whether the outcome was better than expected (positive prediction error) or worse than expected (negative prediction error). This is a purely Model-Free learner.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Learner with Asymmetric Update Rates.
    Uses two different learning rates: one for positive prediction errors (better than expected)
    and one for negative prediction errors (worse than expected). This captures biases 
    like optimism or pessimism.

    Parameters:
    alpha_pos: [0, 1] - Learning rate for positive prediction errors (RPE > 0).
    alpha_neg: [0, 1] - Learning rate for negative prediction errors (RPE < 0).
    beta: [0, 10] - Inverse temperature.
    eligibility_trace: [0, 1] - How much credit the Stage 1 choice gets for the Stage 2 reward (lambda).
    """
    alpha_pos, alpha_neg, beta, eligibility_trace = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        
        # 1. Stage 1 TD Error (State Prediction Error)
        # The 'reward' for stage 1 is the value of the state we landed in (Q_stage2)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        # Update Stage 1 based on sign of error
        lr_1 = alpha_pos if delta_stage1 > 0 else alpha_neg
        q_stage1_mf[action_1[trial]] += lr_1 * delta_stage1

        # 2. Stage 2 TD Error (Reward Prediction Error)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Update Stage 2 based on sign of error
        lr_2 = alpha_pos if delta_stage2 > 0 else alpha_neg
        q_stage2_mf[state_idx, action_2[trial]] += lr_2 * delta_stage2
        
        # 3. Eligibility Trace Update (Direct reinforcement of Stage 1 choice by Stage 2 reward)
        # This allows the final reward to 'leak' back to the first choice directly
        q_stage1_mf[action_1[trial]] += lr_2 * eligibility_trace * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Inverse Temperature Modulation (Exploration Bonus)
This model assumes the participant's "randomness" (beta) is not constant. Instead, the inverse temperature is modulated by the conflict or uncertainty of the choice. Specifically, here we test a mechanism where the `beta` for the second stage is different from the first stage, effectively allowing different levels of exploration/exploitation at different distances from the reward.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Learner with Stage-Specific Exploration.
    Instead of a single inverse temperature (beta), this model allows for different
    levels of exploration at Stage 1 (choosing the spaceship) versus Stage 2 
    (choosing the alien). This tests if the participant is more precise/greedy 
    closer to the reward.

    Parameters:
    learning_rate: [0, 1] - Update rate.
    beta_1: [0, 10] - Inverse temperature for Stage 1 choice.
    beta_2: [0, 10] - Inverse temperature for Stage 2 choice.
    eligibility_trace: [0, 1] - Eligibility trace (lambda) connecting Stage 2 reward to Stage 1.
    """
    learning_rate, beta_1, beta_2, eligibility_trace = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy (Uses beta_1) ---
        exp_q1 = np.exp(beta_1 * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy (Uses beta_2) ---
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # TD(1) / Eligibility Trace approach
        
        # Prediction Error 1: Difference between value of state arrived at and state left
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Prediction Error 2: Difference between reward received and value of alien chosen
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Eligibility Trace: Apply the Stage 2 error to the Stage 1 choice as well
        q_stage1_mf[action_1[trial]] += learning_rate * eligibility_trace * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```