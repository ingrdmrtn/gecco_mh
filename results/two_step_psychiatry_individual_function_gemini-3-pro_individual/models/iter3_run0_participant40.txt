Here are 3 new cognitive models that explore different mechanisms of learning and decision-making in the two-step task.

### Model 1: Hybrid Learner with Eligibility Traces (TD(lambda))
This model introduces an eligibility trace parameter (`lambda`). In standard TD learning (TD(0)), the stage 1 value is updated based on the stage 2 value. With eligibility traces, the stage 1 value is also directly updated by the final reward, bridging the gap between the initial choice and the outcome. This is a common variation in reinforcement learning to handle credit assignment.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner with Eligibility Traces (TD(lambda)).

    This model uses a decay parameter (lambda) to allow the final reward 
    to directly influence the update of the first-stage choice, bridging 
    the gap between action 1 and the final outcome more effectively than 
    simple one-step backup.

    Parameters:
    - learning_rate: [0, 1] Rate at which Q-values are updated.
    - beta: [0, 10] Inverse temperature for softmax choice.
    - lambda_param: [0, 1] Eligibility trace decay parameter. 
      (0 = pure TD learning, 1 = Monte Carlo-like update).
    """
    learning_rate, beta, lambda_param = model_parameters
    n_trials = len(action_1)

    # Initialize Q-values
    q_stage1_mf = np.zeros(2) 
    q_stage2_mf = np.zeros((2, 2)) 

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        state_idx = state[trial]
        chosen_a1 = action_1[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        chosen_a2 = action_2[trial]
        r = reward[trial]

        # --- Learning ---
        # Prediction error at stage 1 (driven by stage 2 value)
        delta_stage1 = q_stage2_mf[state_idx, chosen_a2] - q_stage1_mf[chosen_a1]
        
        # Prediction error at stage 2 (driven by reward)
        delta_stage2 = r - q_stage2_mf[state_idx, chosen_a2]

        # Update Stage 2 value
        q_stage2_mf[state_idx, chosen_a2] += learning_rate * delta_stage2
        
        # Update Stage 1 value:
        # It gets the immediate TD error (delta_stage1) PLUS a portion of the 
        # second stage error (delta_stage2) scaled by lambda.
        q_stage1_mf[chosen_a1] += learning_rate * (delta_stage1 + lambda_param * delta_stage2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Model-Based Learner with Perseverance
This model assumes the participant uses a Model-Based strategy (calculating Stage 1 values based on the transition matrix and Stage 2 maximums) but is also subject to "choice perseverance" (or stickiness). Perseverance is a tendency to repeat the previous action regardless of reward, a common heuristic found in human and animal behavior data.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Based Learner with Choice Perseverance (Stickiness).
    
    This model calculates stage 1 values by planning through the transition matrix 
    (Model-Based). It adds a perseverance bonus to the previously chosen action
    at stage 1, capturing the tendency to repeat choices regardless of value.

    Parameters:
    - learning_rate: [0, 1] Rate at which stage 2 Q-values are updated.
    - beta: [0, 10] Inverse temperature for softmax choice.
    - perseverance: [0, 5] Bonus added to the logits of the previously chosen action.
    """
    learning_rate, beta, perseverance = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure: A->X(0.7), U->Y(0.7) usually
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage2_mf = np.zeros((2, 2)) # Model-free values at stage 2 are necessary for MB planning

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_action_1 = -1 # Indicator for previous action

    for trial in range(n_trials):

        # --- Stage 1 Choice (Model-Based) ---
        # Calculate Model-Based values: Transition * Max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Calculate logits
        logits_1 = beta * q_stage1_mb
        
        # Add perseverance bonus if not the first trial
        if prev_action_1 != -1:
            logits_1[prev_action_1] += perseverance
            
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Store current action for next trial's perseverance
        prev_action_1 = action_1[trial]

        # --- Stage 2 Choice ---
        state_idx = state[trial]
        chosen_a2 = action_2[trial]
        
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[chosen_a2]

        # --- Learning ---
        # Update Stage 2 values based on reward (needed for the MB projection next turn)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, chosen_a2]
        q_stage2_mf[state_idx, chosen_a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Asymmetric Learning Rates (Positive vs. Negative)
This model is a Model-Free learner that updates values differently depending on whether the prediction error is positive (better than expected) or negative (worse than expected). This accounts for potential optimism or pessimism biases in how the participant processes rewards and punishments.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Learner with Asymmetric Learning Rates (Pos/Neg).

    This model splits the learning rate into two separate parameters:
    alpha_pos for positive prediction errors and alpha_neg for negative 
    prediction errors. This captures biases where participants might learn 
    faster from gains than losses (or vice versa).

    Parameters:
    - alpha_pos: [0, 1] Learning rate for positive prediction errors.
    - alpha_neg: [0, 1] Learning rate for negative prediction errors.
    - beta: [0, 10] Inverse temperature for softmax choice.
    """
    alpha_pos, alpha_neg, beta = model_parameters
    n_trials = len(action_1)

    q_stage1_mf = np.zeros(2) 
    q_stage2_mf = np.zeros((2, 2)) 

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        state_idx = state[trial]
        chosen_a1 = action_1[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        chosen_a2 = action_2[trial]
        r = reward[trial]

        # --- Learning ---
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, chosen_a2] - q_stage1_mf[chosen_a1]
        if delta_stage1 > 0:
            q_stage1_mf[chosen_a1] += alpha_pos * delta_stage1
        else:
            q_stage1_mf[chosen_a1] += alpha_neg * delta_stage1

        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[state_idx, chosen_a2]
        if delta_stage2 > 0:
            q_stage2_mf[state_idx, chosen_a2] += alpha_pos * delta_stage2
        else:
            q_stage2_mf[state_idx, chosen_a2] += alpha_neg * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```