Here are three new cognitive models that explore different mechanisms for learning and decision-making in the two-step task, distinct from the standard hybrid-with-perseveration model.

### Model 1: Asymmetric Learning Rates (Reward vs. Punishment)
This model hypothesizes that the participant learns differently from positive outcomes (finding gold) versus negative outcomes (no gold). Standard models use a single learning rate, but individuals often display a positivity bias (learning more from gains) or a negativity bias (learning more from the absence of reward). This is a purely Model-Free approach to keep parameters low while testing this specific asymmetry.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Learner with Asymmetric Learning Rates.
    
    Distinguishes between learning from positive prediction errors (better than expected)
    and negative prediction errors (worse than expected). This captures potential
    optimism or pessimism biases.

    Bounds:
    - alpha_pos: [0, 1] Learning rate for positive prediction errors (delta > 0).
    - alpha_neg: [0, 1] Learning rate for negative prediction errors (delta < 0).
    - beta: [0, 10] Inverse temperature for softmax.
    """
    alpha_pos, alpha_neg, beta = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Standard Q-learning initialization
    q_stage1_mf = np.zeros(2)       
    q_stage2_mf = np.zeros((2, 2))  

    for trial in range(n_trials):
        # --- Stage 1 Choice (Model-Free only) ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        state_idx = state[trial] 

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Stage 2 Update ---
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Apply asymmetric learning rate based on sign of delta
        lr_2 = alpha_pos if delta_stage2 > 0 else alpha_neg
        q_stage2_mf[state_idx, action_2[trial]] += lr_2 * delta_stage2

        # --- Stage 1 Update (TD-learning) ---
        # The reward for stage 1 is the value of the state reached in stage 2
        chosen_stage2_value = q_stage2_mf[state_idx, action_2[trial]]
        delta_stage1 = chosen_stage2_value - q_stage1_mf[action_1[trial]]
        
        lr_1 = alpha_pos if delta_stage1 > 0 else alpha_neg
        q_stage1_mf[action_1[trial]] += lr_1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Eligibility Trace (TD(lambda)) Learner
This model introduces an eligibility trace parameter (`lambda`). In standard Q-learning (TD(0)), the Stage 1 value is updated based on the Stage 2 value. In Monte Carlo (TD(1)), it is updated based on the final reward. An eligibility trace allows the Stage 1 choice to be updated directly by the final reward to a degree controlled by `lambda`. This captures how much credit is assigned to the first action for the final outcome directly, bypassing the intermediate state value estimate.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    TD(lambda) Learner.
    
    Uses an eligibility trace (lambda) to allow the final reward to directly influence 
    the update of the first-stage action values, bridging the gap between Model-Free 
    TD learning and Monte Carlo updating.

    Bounds:
    - learning_rate: [0, 1] Update rate.
    - beta: [0, 10] Inverse temperature.
    - lam: [0, 1] Eligibility trace decay parameter (lambda). 0 = standard TD, 1 = Monte Carlo.
    """
    learning_rate, beta, lam = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1 = np.zeros(2)       
    q_stage2 = np.zeros((2, 2))  

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        state_idx = state[trial] 

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        # 1. Calculate prediction error at stage 2 (reward prediction error)
        delta_2 = reward[trial] - q_stage2[state_idx, action_2[trial]]
        
        # 2. Calculate prediction error at stage 1 (state prediction error)
        # Note: We use the value of the chosen state/action in stage 2 as the target
        delta_1 = q_stage2[state_idx, action_2[trial]] - q_stage1[action_1[trial]]

        # Update Stage 2 value normally
        q_stage2[state_idx, action_2[trial]] += learning_rate * delta_2

        # Update Stage 1 value:
        # It gets updated by its own error (delta_1) PLUS a portion of the stage 2 error (delta_2)
        # mediated by lambda. This connects the final reward back to the first choice.
        q_stage1[action_1[trial]] += learning_rate * (delta_1 + lam * delta_2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Learner with Separate Learning Rates
While the feedback indicated avoiding the standard `learning_rate, beta, w` combination, a critical variation in the literature is whether the Model-Based and Model-Free systems learn at the same speed. This model proposes a Hybrid architecture where the Model-Free system (which relies on habits/caching) might learn slowly (`lr_mf`), while the Model-Based system (which updates the values of the aliens at the end) might learn quickly (`lr_mb`). This disentangles the learning process of the two systems.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner with Separate Learning Rates.
    
    Allows the Model-Free (habitual) and Model-Based (goal-directed) systems to update
    their value estimates at different rates. For example, the MB system might track 
    rapid changes in alien reward probabilities better than the MF system.

    Bounds:
    - lr_mf: [0, 1] Learning rate for Model-Free Q-values.
    - lr_mb: [0, 1] Learning rate for Model-Based values (Stage 2 values).
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight (1=MB, 0=MF).
    """
    lr_mf, lr_mb, beta, w = model_parameters
    n_trials = len(action_1)
    
    # Fixed transition structure for MB
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)       
    q_stage2_mf = np.zeros((2, 2))  
    
    # MB values track the stage 2 values explicitly
    v_stage2_mb = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # MB calculation: Bellman equation using transition matrix and current Stage 2 values
        max_v_stage2 = np.max(v_stage2_mb, axis=1)
        q_stage1_mb = transition_matrix @ max_v_stage2

        # Hybrid mixing
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        state_idx = state[trial] 

        # --- Stage 2 Choice ---
        # Usually assumed to be the same for both systems (just maximizing immediate reward)
        # We use the MB values here as the primary driver for stage 2 or an average, 
        # but standard implementations often just use one set of values for stage 2. 
        # Here we mix them to be consistent with the hybrid logic.
        q_stage2_net = w * v_stage2_mb[state_idx] + (1 - w) * q_stage2_mf[state_idx]
        
        exp_q2 = np.exp(beta * q_stage2_net)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        
        # 1. Model-Free Updates (using lr_mf)
        # Stage 1 MF update (TD)
        chosen_stage2_val_mf = q_stage2_mf[state_idx, action_2[trial]]
        delta_stage1 = chosen_stage2_val_mf - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr_mf * delta_stage1
        
        # Stage 2 MF update
        delta_stage2_mf = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr_mf * delta_stage2_mf

        # 2. Model-Based Updates (using lr_mb)
        # The MB system only needs to update the values of the terminal states (aliens)
        delta_stage2_mb = reward[trial] - v_stage2_mb[state_idx, action_2[trial]]
        v_stage2_mb[state_idx, action_2[trial]] += lr_mb * delta_stage2_mb

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```