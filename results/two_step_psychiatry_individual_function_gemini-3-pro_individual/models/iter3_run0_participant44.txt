Here are three new cognitive models exploring alternative parameter combinations and mechanisms not yet tested, specifically focusing on eligibility traces, separate learning rates for the two stages, and a hybrid model-based/model-free approach with a specific mixing parameter.

### Model 1: TD(1) Learning (Eligibility Traces)
This model implements a strict Temporal Difference learning algorithm with eligibility traces (controlled by `lambda_eligibility`). Instead of just updating the first stage based on the second stage's value (TD(0)), the reward at the end of the trial can "propagate" back to update the first stage action directly. This is a classic Model-Free approach that bridges the gap between simple Q-learning and Monte Carlo methods.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    TD(lambda) Model-Free Learner.
    Uses an eligibility trace to allow the final reward to directly influence
    the first-stage value update, bridging the gap between Stage 1 and outcome.
    
    Parameters:
    learning_rate: [0,1] Learning rate for value updates.
    beta: [0,10] Inverse temperature for softmax choice.
    lambda_eligibility: [0,1] Eligibility trace decay parameter (0 = TD(0), 1 = Monte Carlo).
    """
    learning_rate, beta, lambda_eligibility = model_parameters
    n_trials = len(action_1)

    # Q-values
    q_stage1 = np.zeros(2)     # Values for choosing Spaceship A or U
    q_stage2 = np.zeros((2, 2)) # Values for choosing aliens on Planet X or Y

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        s_idx = state[trial] # 0 or 1 (Planet X or Y)

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]

        # --- Updates with Eligibility Trace ---
        
        # 1. Prediction Error at Stage 1 (TD error 1)
        # The value of the state reached (max Q or chosen Q) minus current estimate
        # Standard SARSA-like update uses Q(s', a')
        delta1 = q_stage2[s_idx, a2] - q_stage1[a1]
        
        # 2. Prediction Error at Stage 2 (TD error 2)
        delta2 = r - q_stage2[s_idx, a2]

        # Update Stage 2 value
        q_stage2[s_idx, a2] += learning_rate * delta2
        
        # Update Stage 1 value
        # It gets the immediate TD error (delta1) plus a portion of the second stage error (delta2)
        # scaled by lambda. If lambda=1, the full reward error propagates back.
        q_stage1[a1] += learning_rate * (delta1 + lambda_eligibility * delta2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Separate Stage Learning Rates
This model hypothesizes that the participant learns at different speeds for the two different stages of the task. It uses `lr_stage1` for updating spaceship preferences and `lr_stage2` for updating alien preferences. This is plausible because Stage 1 is about transition structures (or caching values from them), while Stage 2 is a direct bandit task.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Dual Learning Rate Model-Free Learner.
    Assumes the participant learns value updates at different rates for 
    the first stage (choosing spaceships) vs the second stage (choosing aliens).
    
    Parameters:
    lr_stage1: [0,1] Learning rate for Stage 1 Q-values.
    lr_stage2: [0,1] Learning rate for Stage 2 Q-values.
    beta: [0,10] Inverse temperature for softmax choice (shared).
    """
    lr_stage1, lr_stage2, beta = model_parameters
    n_trials = len(action_1)

    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        s_idx = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]

        # --- Updates ---
        
        # Stage 1 Update: Driven by the value of the state reached
        # We use SARSA style: Q(s1,a1) moves toward Q(s2, a2)
        delta_stage1 = q_stage2[s_idx, a2] - q_stage1[a1]
        q_stage1[a1] += lr_stage1 * delta_stage1
        
        # Stage 2 Update: Driven by reward
        delta_stage2 = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += lr_stage2 * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model-Based / Model-Free (Classic "Daw" Model)
This is the canonical explanation for behavior in this task. It calculates two values for Stage 1: a Model-Free value (based on direct experience) and a Model-Based value (calculated using the transition matrix and Stage 2 values). The final choice is a weighted mix of these two, controlled by the parameter `w`.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Learner.
    Stage 1 values are a weighted combination of MF (TD learning) and MB 
    (planning using the transition matrix) values.
    
    Parameters:
    learning_rate: [0,1] Update rate for MF values (and MB stage 2 values).
    beta: [0,10] Inverse temperature.
    w: [0,1] Weighting parameter. 0 = Pure Model-Free, 1 = Pure Model-Based.
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition matrix as described in task (0.7 common, 0.3 rare)
    # Row 0: Spaceship A -> [70% Planet X, 30% Planet Y]
    # Row 1: Spaceship U -> [30% Planet X, 70% Planet Y]
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # Used for both MF path and MB calculation

    for trial in range(n_trials):
        
        # --- Stage 1 Policy Construction ---
        
        # 1. Model-Based Value Calculation
        # V_MB(s1) = Sum( P(s2|s1) * max(Q_stage2(s2, :)) )
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Hybrid Value
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Choice Probability
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        s_idx = state[trial]

        # --- Stage 2 Policy Construction ---
        # Stage 2 is purely model-free (simple bandit) in this paradigm
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]
  
        # --- Updates ---
        
        # Update Stage 1 MF value (SARSA)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Update Stage 2 MF value (Reward Prediction Error)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```