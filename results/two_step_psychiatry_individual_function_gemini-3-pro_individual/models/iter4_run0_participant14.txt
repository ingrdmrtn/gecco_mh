Here are three new cognitive models based on the two-step task framework. These models explore different hypotheses about how the participant combines model-based (planning) and model-free (habitual) learning strategies.

### Model 1: Hybrid Model-Based / Model-Free (Classic Daw et al. 2011)
This is the canonical model for this task. It assumes the participant uses a weighted combination of Model-Based (planning using the transition matrix) and Model-Free (TD learning) values to make the first-stage choice. The parameter `w` controls the balance: `w=1` is pure model-based, `w=0` is pure model-free.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Reinforcement Learning Model.
    
    This model assumes the agent computes both Model-Based (MB) values (using the 
    transition structure) and Model-Free (MF) values (using direct experience).
    The stage 1 choice is a weighted mix of these two valuations.
    
    Parameters:
    learning_rate: [0, 1] - Update rate for Q-values.
    beta: [0, 10] - Inverse temperature for softmax choice.
    w: [0, 1] - Weighting parameter (0 = Pure MF, 1 = Pure MB).
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition matrix as described in task: A->X (0.7), U->Y (0.7)
    # Rows: Spaceship 0 (A), Spaceship 1 (U)
    # Cols: Planet 0 (X), Planet 1 (Y)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)      # MF values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # MF values for stage 2 (Aliens)

    for trial in range(n_trials):
        
        # --- Stage 1 Policy ---
        # 1. Calculate Model-Based values for Stage 1
        # MB value = Transition Probability * Max Value of Next Stage
        max_q_stage2 = np.max(q_stage2_mf, axis=1) # Max value available on each planet
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Combine MB and MF values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # 3. Softmax choice
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial] # Actual planet arrived at

        # --- Stage 2 Policy ---
        # Standard softmax on stage 2 values
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning Updates ---
        # 1. Update Stage 1 MF value (TD(0) update)
        # Note: In the full Daw model, there is often an eligibility trace (lambda).
        # Here we use a simpler TD(0) update for Stage 1 MF as per the prompt's template style.
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # 2. Update Stage 2 MF value (Reward prediction error)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Model-Free with Eligibility Traces (TD($\lambda$))
This model is purely model-free (no transition matrix knowledge) but adds an eligibility trace parameter `lambda_eligibility`. This allows the reward received at the second stage to directly influence the value of the first-stage choice (Spaceship) within the same trial, bridging the gap between the final outcome and the initial decision more effectively than simple TD(0).

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Reinforcement Learning with Eligibility Traces (TD-Lambda).
    
    This model updates the first-stage choice based on the second-stage reward
    via an eligibility trace, allowing 'direct' reinforcement of the spaceship 
    choice by the gold coins, bypassing the intermediate state value to some extent.
    
    Parameters:
    learning_rate: [0, 1] - Update rate.
    beta: [0, 10] - Inverse temperature.
    lambda_eligibility: [0, 1] - Decay rate of eligibility trace (0 = TD(0), 1 = Monte Carlo).
    """
    learning_rate, beta, lambda_eligibility = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning Updates ---
        
        # 1. Prediction error at stage 1 (Transition from Spaceship -> Planet)
        # The value of the spaceship is updated towards the value of the chosen alien
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # 2. Prediction error at stage 2 (Outcome)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # 3. Eligibility Trace Update
        # The Stage 1 choice ALSO gets updated by the Stage 2 prediction error,
        # scaled by lambda. If lambda=1, the spaceship value is updated fully by the coin outcome.
        q_stage1_mf[action_1[trial]] += learning_rate * lambda_eligibility * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model with Choice Stickiness (Perseveration)
This extends the hybrid model (Model 1) by adding a `stickiness` parameter. Participants often repeat their previous choice regardless of reward (perseveration) or switch (alternation). This parameter captures raw motor repetition bias at the first stage, which is a common nuisance variable in this task that can mask learning effects.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free with Choice Stickiness.
    
    Adds a 'stickiness' parameter to the standard hybrid model. This accounts for 
    the tendency to repeat the same stage-1 choice (spaceship) regardless of 
    value, often due to motor perseveration or cognitive inertia.
    
    Parameters:
    learning_rate: [0, 1] - Update rate.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - Weighting parameter (0 = Pure MF, 1 = Pure MB).
    stickiness: [0, 5] - Bonus added to the logits of the previously chosen action.
    """
    learning_rate, beta, w, stickiness = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1 # Initialize 'previous choice' marker

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combined Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add Stickiness Bonus
        # We calculate logits (unnormalized log probabilities)
        logits = beta * q_net
        if last_action_1 != -1:
            logits[last_action_1] += stickiness
            
        # Softmax with stickiness
        exp_q1 = np.exp(logits)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Update tracker for next trial
        last_action_1 = action_1[trial] 
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```