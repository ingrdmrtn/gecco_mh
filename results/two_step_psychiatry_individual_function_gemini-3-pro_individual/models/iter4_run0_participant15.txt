Here are three new cognitive models based on the provided template, exploring parameter combinations and mechanisms distinct from those previously tested (such as asymmetric learning rates, memory decay, and stage-specific exploration).

### Model 1: Hybrid Model with Asymmetric Learning Rates
This model hypothesizes that the participant learns differently from positive prediction errors (better than expected) versus negative prediction errors (worse than expected). This is often observed in clinical populations where sensitivity to reward or punishment is imbalanced.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MF/MB Model with Asymmetric Learning Rates.
    
    This model distinguishes between learning from positive prediction errors 
    (alpha_pos) and negative prediction errors (alpha_neg). It allows the 
    agent to be more sensitive to rewards than punishments, or vice versa.

    Parameters:
    alpha_pos: [0, 1] - Learning rate for positive prediction errors (delta > 0).
    alpha_neg: [0, 1] - Learning rate for negative prediction errors (delta < 0).
    beta: [0, 10] - Inverse temperature (exploration/exploitation trade-off).
    w: [0, 1] - Weighting parameter (1 = fully Model-Based, 0 = fully Model-Free).
    """
    alpha_pos, alpha_neg, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combine MF and MB values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Update Stage 1
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        if delta_stage1 > 0:
            q_stage1_mf[action_1[trial]] += alpha_pos * delta_stage1
        else:
            q_stage1_mf[action_1[trial]] += alpha_neg * delta_stage1

        # Update Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        if delta_stage2 > 0:
            q_stage2_mf[state_idx, action_2[trial]] += alpha_pos * delta_stage2
        else:
            q_stage2_mf[state_idx, action_2[trial]] += alpha_neg * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model with Passive Memory Decay
This model introduces a `decay_rate` parameter. It posits that the participant's value estimates are not perfectly maintained; Q-values for all options (both chosen and unchosen) slowly decay toward zero over time. This simulates forgetting or a "center-bias" where values regress to neutrality if not reinforced.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MF/MB Model with Passive Memory Decay.
    
    In addition to standard hybrid learning, this model implements a decay 
    mechanism where all Q-values regress towards 0 on every trial. This 
    simulates forgetting or the fleeting nature of value estimation in 
    changing environments.

    Parameters:
    learning_rate: [0, 1] - Update rate for Q-values.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - Weighting parameter (1 = MB, 0 = MF).
    decay_rate: [0, 1] - Rate at which stored Q-values decay to 0 each trial.
    """
    learning_rate, beta, w, decay_rate = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Apply passive decay to ALL Q-values before specific updating
        q_stage1_mf *= (1 - decay_rate)
        q_stage2_mf *= (1 - decay_rate)

        # Standard Updates
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1

        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model with Stage-Specific Exploration
This model assumes that the level of "randomness" or exploration differs between the two stages. Stage 1 (Spaceships) is an abstract planning stage, while Stage 2 (Aliens) is a concrete bandit task. The participant might be more consistent in one stage than the other.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MF/MB Model with Stage-Specific Inverse Temperatures.
    
    This model separates the 'beta' parameter into two: one for the 
    first-stage choice (Spaceships) and one for the second-stage choice 
    (Aliens). This accounts for different levels of exploration/exploitation 
    at different levels of the decision hierarchy.

    Parameters:
    learning_rate: [0, 1] - Update rate for Q-values.
    beta_1: [0, 10] - Inverse temperature for Stage 1 (Spaceships).
    beta_2: [0, 10] - Inverse temperature for Stage 2 (Aliens).
    w: [0, 1] - Weighting parameter (1 = MB, 0 = MF).
    """
    learning_rate, beta_1, beta_2, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Use beta_1 for the first stage
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        qs_current_state = q_stage2_mf[state_idx]
        
        # Use beta_2 for the second stage
        exp_q2 = np.exp(beta_2 * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1

        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```