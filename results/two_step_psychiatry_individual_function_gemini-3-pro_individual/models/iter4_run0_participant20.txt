Here are three new cognitive models for the two-step task. These models explore different mechanisms for learning and choice, distinct from the feedback provided.

### Model 1: Asymmetric Learning Rates (Positive vs. Negative Prediction Errors)
This model hypothesizes that the participant updates their beliefs differently depending on whether the outcome was better or worse than expected. This is often observed in clinical populations where sensitivity to reward (positive PE) and punishment/omission (negative PE) differs.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Asymmetric Learning Rates Model (Model-Free).
    
    This model assumes the participant is purely model-free but has different 
    learning rates for positive and negative prediction errors. This captures 
    potential biases in how good vs. bad news is integrated.

    Parameters:
    alpha_pos: [0,1] - Learning rate for positive prediction errors (reward > expectation).
    alpha_neg: [0,1] - Learning rate for negative prediction errors (reward < expectation).
    beta: [0,10] - Inverse temperature for softmax choice.
    """
    alpha_pos, alpha_neg, beta = model_parameters
    n_trials = len(action_1)
  
    # Pure Model-Free setup, so transition matrix is not used for value calculation
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial] # 0 or 1 (Planet X or Y)

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # Stage 1 Update (SARSA-style TD(0) for simplicity in this MF model)
        # Prediction error driven by the value of the state landed in
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        if delta_stage1 > 0:
            q_stage1_mf[action_1[trial]] += alpha_pos * delta_stage1
        else:
            q_stage1_mf[action_1[trial]] += alpha_neg * delta_stage1

        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        if delta_stage2 > 0:
            q_stage2_mf[state_idx, action_2[trial]] += alpha_pos * delta_stage2
        else:
            q_stage2_mf[state_idx, action_2[trial]] += alpha_neg * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Learner with Eligibility Traces (TD(lambda))
This model introduces an eligibility trace parameter (`lam`). Instead of just updating the second-stage value based on reward, the reward prediction error at the second stage "back-propagates" to update the first-stage choice immediately. This connects the final outcome directly to the initial spaceship choice, bridging the gap between the two steps.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner with Eligibility Traces (TD-lambda) and Mixing Weight.
    
    Combines Model-Based (MB) planning with a Model-Free (MF) system that uses 
    eligibility traces. The trace parameter (lam) allows the stage 2 reward 
    prediction error to directly update stage 1 values.

    Parameters:
    learning_rate: [0,1] - Update rate for Q-values.
    beta: [0,10] - Inverse temperature.
    w: [0,1] - Weighting parameter (0 = Pure MF, 1 = Pure MB).
    lam: [0,1] - Eligibility trace decay (0 = TD(0), 1 = Monte Carlo/full backprop).
    """
    learning_rate, beta, w, lam = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Choice (Hybrid) ---
        # Model-Based Value
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combined Value
        q_hybrid = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_hybrid)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # 1. Stage 1 prediction error (TD error at transition)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # 2. Stage 2 prediction error (Reward error)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # 3. Eligibility Trace Update: Apply stage 2 error to stage 1 choice
        # Scaled by lambda. If lambda > 0, the final reward reinforces the first choice.
        q_stage1_mf[action_1[trial]] += learning_rate * lam * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Value Forgetting / Decay
This model assumes that participants are not perfect integrators. While they learn from new experiences, the values of unchosen options (or simply all options) decay over time toward a neutral point (0.5 or 0). This "forgetting" mechanism can capture the volatility of human memory or a belief that the environment is rapidly changing.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner with Value Decay (Forgetting).
    
    Includes a decay parameter that pulls Q-values towards 0 (or a neutral point) 
    on every trial. This represents forgetting or a belief in environmental volatility.
    
    Parameters:
    learning_rate: [0,1] - Update rate for Q-values.
    beta: [0,10] - Inverse temperature.
    w: [0,1] - Weighting parameter (0 = Pure MF, 1 = Pure MB).
    decay_rate: [0,1] - Rate at which unvisited/all state values decay toward 0.
    """
    learning_rate, beta, w, decay_rate = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # Initialized at 0, representing neutral/unknown

    for trial in range(n_trials):

        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_hybrid = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)

        exp_q1 = np.exp(beta * q_hybrid)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1

        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # --- Decay ---
        # Apply decay to all Q-values to simulate forgetting
        q_stage1_mf *= (1 - decay_rate)
        q_stage2_mf *= (1 - decay_rate)


    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```