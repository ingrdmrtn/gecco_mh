Here are 3 new cognitive models exploring different mechanisms for learning and decision-making in the two-step task.

### Model 1: Hybrid Model with Eligibility Traces (TD($\lambda$))
This model introduces an eligibility trace parameter $\lambda$ (`lambda_elig`). Instead of just updating the first-stage value based on the second-stage value (TD(0)), this model allows the final reward to directly influence the first-stage choice values, effectively bridging the gap between the two stages more strongly. This is a form of Model-Free learning that can sometimes mimic Model-Based behavior.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free RL with Eligibility Traces (TD-Lambda).
    
    Incorporates an eligibility trace parameter `lambda_elig` which allows the 
    reward at the second stage to directly update the first-stage Q-values, 
    bridging the temporal gap.

    Parameters:
    learning_rate: [0, 1] Update rate for Q-values.
    beta: [0, 10] Inverse temperature for softmax.
    w: [0, 1] Weight for Model-Based control (1 = Pure MB, 0 = Pure MF).
    lambda_elig: [0, 1] Eligibility trace decay parameter. 0 = TD(0), 1 = Monte Carlo.
    """
    learning_rate, beta, w, lambda_elig = model_parameters
    n_trials = len(action_1)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)       
    q_stage2_mf = np.zeros((2, 2))  

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial] 

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        # Prediction error for stage 1 (driven by stage 2 value)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Prediction error for stage 2 (driven by reward)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Eligibility trace update: pass stage 2 error back to stage 1
        q_stage1_mf[action_1[trial]] += learning_rate * lambda_elig * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Separate Learning Rates for Stage 1 and Stage 2
Standard models assume a single learning rate for both the high-level spaceship choice and the low-level alien choice. This model proposes that participants might learn at different speeds for these two distinct contexts. For instance, the values of aliens (Stage 2) might be updated rapidly based on direct rewards, while the values of spaceships (Stage 1) might be updated more slowly as they are one step removed.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free RL with Separate Stage Learning Rates.
    
    Distinguishes between learning at the first stage (spaceship choice) and 
    the second stage (alien choice).

    Parameters:
    lr_stage1: [0, 1] Learning rate for Stage 1 Q-values.
    lr_stage2: [0, 1] Learning rate for Stage 2 Q-values.
    beta: [0, 10] Inverse temperature for softmax.
    w: [0, 1] Weight for Model-Based control.
    """
    lr_stage1, lr_stage2, beta, w = model_parameters
    n_trials = len(action_1)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)       
    q_stage2_mf = np.zeros((2, 2))  

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2

        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial] 

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        # Stage 1 update uses lr_stage1
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr_stage1 * delta_stage1

        # Stage 2 update uses lr_stage2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr_stage2 * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Value-Dependent Learning Rate (Surprise Modulation)
This model implements a dynamic learning rate mechanism (often called a Pearce-Hall-like mechanism). The core idea is that agents learn more when they are "surprised." Here, the learning rate for the current trial is modulated by the magnitude of the prediction error from the *previous* trial. If the previous outcome was unexpected (large error), the agent becomes more sensitive to new information (higher effective learning rate).

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free RL with Surprise-Modulated Learning Rate (Pearce-Hall style).
    
    The learning rate is dynamic: it increases when the previous prediction error 
    (surprise) was high. This model is purely Model-Free (w=0 implied) to focus 
    on the learning dynamics.

    Parameters:
    base_lr: [0, 1] Baseline learning rate.
    surprise_sensitivity: [0, 5] How strongly previous errors increase the learning rate.
    beta: [0, 10] Inverse temperature for softmax.
    decay: [0, 1] Decay rate for the surprise signal (moving average of absolute error).
    """
    base_lr, surprise_sensitivity, beta, decay = model_parameters
    n_trials = len(action_1)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)       
    q_stage2_mf = np.zeros((2, 2))  
    
    # Initialize surprise signal (smoothed absolute prediction error)
    surprise_signal = 0.0

    for trial in range(n_trials):
        
        # Calculate dynamic learning rate based on surprise
        # Bounded between 0 and 1
        current_lr = base_lr + surprise_sensitivity * surprise_signal
        current_lr = np.clip(current_lr, 0, 1)

        # --- Stage 1 Policy ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial] 

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += current_lr * delta_stage1

        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += current_lr * delta_stage2
        
        # Update surprise signal (moving average of absolute error from stage 2)
        # We focus on stage 2 error as it reflects the primary reward outcome
        surprise_signal = (1 - decay) * surprise_signal + decay * np.abs(delta_stage2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```