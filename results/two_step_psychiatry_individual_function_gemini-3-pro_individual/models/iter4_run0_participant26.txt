Here are three new cognitive models exploring different mechanisms for learning and decision-making in the two-step task.

### Model 1: Hybrid Learner (Model-Based + Model-Free)
This model combines model-based (planning using the transition structure) and model-free (learning from direct experience) systems. A mixing parameter `w` determines the balance between the two systems for the first-stage choice. This is a classic formulation for this task.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Learner.

    Hypothesis: The participant uses a mixture of Model-Based (planning using the 
    transition matrix) and Model-Free (TD learning) strategies to make choices.
    
    Parameters:
    learning_rate: [0, 1] - Update rate for Q-values.
    beta: [0, 10] - Inverse temperature for softmax choice.
    w: [0, 1] - Weighting parameter. 1 = Pure Model-Based, 0 = Pure Model-Free.
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition matrix as described in task: A->X (0->0) is common (0.7)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2) # Model-free Q-values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Q-values for stage 2 (Aliens)

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        # Model-Based Value: V_MB(s1) = T * max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Value: Q_net = w * Q_MB + (1-w) * Q_MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial] # Actual planet arrived at

        # --- Stage 2 Policy ---
        # Standard softmax on Stage 2 Q-values
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning Updates ---
        # TD(1) update for Stage 1 MF value (using the actual outcome at stage 2)
        # Note: Often TD(1) or TD(0) is used. Here we use a simple SARSA-like update
        # driven by the second stage value estimate.
        
        # Update Stage 1 MF Q-value using the value of the state reached
        # Q_MF(s1, a1) <- Q_MF(s1, a1) + alpha * (Q_stage2(s2, a2) - Q_MF(s1, a1))
        # This is effectively TD(0) from the perspective of the first step
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1

        # Update Stage 2 Q-value based on reward
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Eligibility Trace Learner (TD(lambda))
This model assumes the participant is purely Model-Free but uses an eligibility trace `lambda` to connect the final reward back to the first-stage choice. This allows the reward obtained at the end of the trial to directly influence the value of the spaceship chosen at the beginning, bridging the gap between the two stages.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    TD(lambda) Learner.

    Hypothesis: The participant is a model-free learner who updates first-stage values 
    based on the final reward, mediated by an eligibility trace parameter (lambda). 
    This allows direct credit assignment from reward to the spaceship choice.
    
    Parameters:
    learning_rate: [0, 1] - Update rate for Q-values.
    beta: [0, 10] - Inverse temperature for softmax choice.
    lam: [0, 1] - Eligibility trace decay (lambda). 
                  0 = TD(0) (only update from next state value), 
                  1 = Monte Carlo (update fully from reward).
    """
    learning_rate, beta, lam = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning Updates ---
        
        # 1. Calculate Prediction Error at Stage 2 (Reward - Expected)
        delta_2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # 2. Calculate Prediction Error at Stage 1 (Value of S2 - Value of S1)
        delta_1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        # Update Stage 2 Q-value
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_2
        
        # Update Stage 1 Q-value
        # The update includes the immediate TD error (delta_1) 
        # PLUS the trace of the second stage error (lam * delta_2)
        q_stage1_mf[action_1[trial]] += learning_rate * (delta_1 + lam * delta_2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Asymmetric Learning Rates (Pos/Neg)
This model posits that the participant learns differently from positive outcomes (finding gold) versus negative outcomes (no gold). It splits the learning rate into `alpha_pos` and `alpha_neg`. This asymmetry can explain biases like optimism or pessimism in value estimation.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Learner with Asymmetric Learning Rates.

    Hypothesis: The participant updates their value estimates differently depending on 
    whether the prediction error is positive (better than expected) or negative 
    (worse than expected).
    
    Parameters:
    alpha_pos: [0, 1] - Learning rate for positive prediction errors.
    alpha_neg: [0, 1] - Learning rate for negative prediction errors.
    beta: [0, 10] - Inverse temperature for softmax choice.
    """
    alpha_pos, alpha_neg, beta = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning Updates ---
        
        # Stage 1 Update
        delta_1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        if delta_1 > 0:
            q_stage1_mf[action_1[trial]] += alpha_pos * delta_1
        else:
            q_stage1_mf[action_1[trial]] += alpha_neg * delta_1

        # Stage 2 Update
        delta_2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        if delta_2 > 0:
            q_stage2_mf[state_idx, action_2[trial]] += alpha_pos * delta_2
        else:
            q_stage2_mf[state_idx, action_2[trial]] += alpha_neg * delta_2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```