Here are three new cognitive models exploring different mechanisms for learning and decision-making in the two-step task.

### Model 1: Hybrid Model-Based / Model-Free (Dyna-style)
This model implements the classic "hybrid" architecture often used for this task. It assumes the agent maintains both Model-Based (planning) and Model-Free (habitual) values for the first-stage choice. A mixing parameter `w` determines the balance between these two systems. This addresses the possibility that the participant isn't purely planning (Model-Based) or purely reactive (Model-Free), but a mix of both.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Reinforcement Learning.
    
    The agent computes Stage 1 values as a weighted combination of:
    1. Model-Based (MB): Calculated using the transition matrix and max Stage 2 values.
    2. Model-Free (MF): Learned via TD errors from Stage 2 transitions.
    
    Parameters:
    - learning_rate: [0, 1] Update rate for Q-values.
    - beta: [0, 10] Inverse temperature for softmax.
    - w: [0, 1] Weighting parameter (0 = Pure MF, 1 = Pure MB).
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)       # Model-free values for stage 1
    q_stage2_mf = np.zeros((2, 2))  # Model-free values for stage 2 (aliens)

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # 1. Calculate Model-Based values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Combine MB and MF values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # 3. Softmax choice probability
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial] # Actual planet arrived at

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        # 1. Update Stage 1 MF value (TD(0) style, driven by value of state reached)
        # Note: In full SARSA(lambda) this might use the Q-value of the next action, 
        # but often in this task, the max Q of the next state is used for the stage 1 update target.
        # Here we use the value of the *chosen* second stage action as the target (SARSA-like).
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # 2. Update Stage 2 MF value
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Pure Model-Free with Perseverance
This model hypothesizes that the participant is not doing complex planning (Model-Based) but is instead a simple Model-Free learner who has a "stickiness" or perseverance bias. They tend to repeat the last action they took at the first stage, regardless of reward. This is a common heuristic in human and animal behavior that can sometimes look like learning but is actually just repetition.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free RL with Choice Perseverance (Stickiness).
    
    The agent learns via standard TD errors but adds a 'stickiness' bonus
    to the action chosen on the previous trial at Stage 1.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - stickiness: [0, 5] Bonus added to the previously chosen action logit.
    """
    learning_rate, beta, stickiness = model_parameters
    n_trials = len(action_1)
  
    # No transition matrix needed for pure MF
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1 # Initialize with no previous action

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # Add stickiness to the Q-values before softmax
        logits_1 = beta * q_stage1_mf.copy()
        if last_action_1 != -1:
            logits_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1)) # Subtract max for numerical stability
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Store action for next trial's stickiness
        last_action_1 = action_1[trial]
        state_idx = state[trial]

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        # TD(1) / Lambda=1 logic implies the reward at stage 2 fully propagates back to stage 1
        # This is equivalent to updating stage 1 with the final reward directly.
        
        # Update Stage 1
        delta_stage1 = reward[trial] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Update Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Asymmetric Learning Rates (Positive/Negative)
This model investigates if the participant learns differently from success (getting a coin) versus failure (getting nothing). Standard RL models use a single learning rate, but humans often display "optimism bias" (learning more from rewards) or "pessimism bias" (learning more from punishments/omissions). This is a pure Model-Free architecture but with split alpha.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free RL with Asymmetric Learning Rates (Pos/Neg).
    
    The agent uses different learning rates depending on whether the 
    prediction error (delta) is positive (better than expected) or 
    negative (worse than expected).
    
    Parameters:
    - alpha_pos: [0, 1] Learning rate for positive prediction errors.
    - alpha_neg: [0, 1] Learning rate for negative prediction errors.
    - beta: [0, 10] Inverse temperature.
    """
    alpha_pos, alpha_neg, beta = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        # We assume direct reinforcement of Stage 1 by the reward (like Model 2)
        # to focus on the asymmetry of the reward impact.
        
        # Calculate Delta Stage 1 (using direct reward)
        delta_1 = reward[trial] - q_stage1_mf[action_1[trial]]
        
        # Apply asymmetric learning rate for Stage 1
        if delta_1 >= 0:
            q_stage1_mf[action_1[trial]] += alpha_pos * delta_1
        else:
            q_stage1_mf[action_1[trial]] += alpha_neg * delta_1
            
        # Calculate Delta Stage 2
        delta_2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Apply asymmetric learning rate for Stage 2
        if delta_2 >= 0:
            q_stage2_mf[state_idx, action_2[trial]] += alpha_pos * delta_2
        else:
            q_stage2_mf[state_idx, action_2[trial]] += alpha_neg * delta_2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```