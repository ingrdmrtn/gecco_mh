Here are three cognitive models based on the two-step task structure, ranging from a pure model-based learner to a hybrid learner, and finally a model with separate learning rates for positive and negative outcomes.

### Model 1: Pure Model-Based Learner
This model assumes the participant does not rely on direct reinforcement of the first-stage actions (Model-Free). Instead, they only learn the values of the aliens (second stage) and combine this with their knowledge of the transition structure (Spaceship -> Planet probabilities) to plan their first choice.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Based Learner.
    
    This model assumes the agent learns the values of the second-stage options (aliens)
    via TD learning, but calculates the first-stage values strictly by planning 
    through the transition matrix (V_stage1 = Transition * Max_Q_stage2).
    It ignores the direct experience of Stage 1 Q-values.

    Parameters:
    learning_rate: [0,1] Rate at which second-stage Q-values are updated.
    beta: [0,10] Inverse temperature for softmax choice.
    """
    learning_rate, beta = model_parameters
    n_trials = len(action_1)

    # Fixed transition matrix as per task description
    # Row 0 (Spaceship A) -> [0.7 Planet X, 0.3 Planet Y]
    # Row 1 (Spaceship U) -> [0.3 Planet X, 0.7 Planet Y]
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Stage 2 values: 2 Planets x 2 Aliens
    q_stage2_mf = np.zeros((2, 2)) 

    for trial in range(n_trials):
        # --- Stage 1 Decision (Model-Based Planning) ---
        # Calculate expected value of each planet (max of available aliens)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        # Plan: Multiply transition probs by best values at destination
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial] # Actual planet reached

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning ---
        # Only Stage 2 is learned from reward
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Note: No Stage 1 update here because this model is Pure MB.

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model-Based / Model-Free Learner
This is the classic "Daw et al. (2011)" style model. It assumes the participant uses a weighted combination of Model-Free (habitual) and Model-Based (planning) systems to make the first-stage choice. A mixing parameter `w` controls the balance between the two systems.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Learner.
    
    This model computes Q-values for the first stage using both:
    1. A Model-Free (MF) system: TD(0) updates based on the second stage value.
    2. A Model-Based (MB) system: Planning using the transition matrix.
    The final choice is a weighted mix of both Q-values.

    Parameters:
    learning_rate: [0,1] Rate at which Q-values are updated.
    beta: [0,10] Inverse temperature for softmax.
    w: [0,1] Weighting parameter (0 = Pure MF, 1 = Pure MB).
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # 1. Calculate Model-Based values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Mix MF and MB values
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial] 

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning ---
        # Update Stage 1 MF value using the value of the state reached (TD(0))
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1

        # Update Stage 2 MF value using the reward
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Asymmetric Learning Rate Model (Pure Model-Free)
This model is a variation of the pure Model-Free learner (TD(0)), but it posits that the participant learns differently from positive outcomes (finding gold) versus negative outcomes (no gold). This asymmetry is often observed in clinical populations or specific reinforcement learning contexts.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Asymmetric Learning Rate Model (Model-Free).
    
    This is a TD(0) learner that uses separate learning rates for positive prediction errors 
    (better than expected) and negative prediction errors (worse than expected).
    It does not use model-based planning.

    Parameters:
    alpha_pos: [0,1] Learning rate for positive prediction errors (delta > 0).
    alpha_neg: [0,1] Learning rate for negative prediction errors (delta < 0).
    beta: [0,10] Inverse temperature for softmax.
    """
    alpha_pos, alpha_neg, beta = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial] 

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning ---
        
        # Stage 1 Update
        # Note: We use the Q-value of the chosen 2nd stage action as the target
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        if delta_stage1 > 0:
            q_stage1_mf[action_1[trial]] += alpha_pos * delta_stage1
        else:
            q_stage1_mf[action_1[trial]] += alpha_neg * delta_stage1

        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        if delta_stage2 > 0:
            q_stage2_mf[state_idx, action_2[trial]] += alpha_pos * delta_stage2
        else:
            q_stage2_mf[state_idx, action_2[trial]] += alpha_neg * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```