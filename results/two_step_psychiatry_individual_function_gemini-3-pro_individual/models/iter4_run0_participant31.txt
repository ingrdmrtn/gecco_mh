Here are three new cognitive models for the two-step decision task. These models introduce variations in how learning rates are applied (e.g., separate rates for chosen vs. unchosen options) and how rewards are processed (e.g., outcome sensitivity), avoiding the exact parameter combinations previously attempted.

### Model 1: Hybrid Model with Separate Learning Rates for Unchosen Options
This model hypothesizes that participants might update the value of the option they *didn't* choose differently from the one they did. This is often called a "counterfactual" update or a decay for the unchosen path.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Unchosen Option Decay:
    Standard hybrid MB/MF model, but unchosen options at the second stage 
    decay towards 0 (or update differently) based on a separate learning rate.
    
    Parameters:
    lr_chosen: [0, 1] Learning rate for the chosen option.
    lr_unchosen: [0, 1] Decay rate for the unchosen option (pulls Q towards 0).
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Weight mixing MB (1) and MF (0).
    """
    lr_chosen, lr_unchosen, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # 2 states (planets), 2 actions (aliens)

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial] # Actual planet arrived at

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updating ---
        
        # Stage 1 Update (TD(0))
        # Note: We use the value of the state arrived at (max_q_stage2[state_idx]) or the Q-value of action taken.
        # Standard formulation uses the max of the next stage.
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr_chosen * delta_stage1
        
        # Stage 2 Update
        # Update chosen alien
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr_chosen * delta_stage2
        
        # Update unchosen alien (decay or counterfactual update)
        unchosen_action_2 = 1 - action_2[trial]
        # Here we assume unchosen options decay toward 0.5 (uncertainty) or 0.
        # Let's simply decay toward 0 for this model implementation.
        q_stage2_mf[state_idx, unchosen_action_2] += lr_unchosen * (0 - q_stage2_mf[state_idx, unchosen_action_2])
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model with Reward Scaling (Subjective Utility)
This model introduces a `reward_scaling` parameter. Instead of treating the reward (0 or 1) as an objective mathematical input, the participant might perceive the reward of '1' as having a variable subjective magnitude. This affects the size of the prediction error.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Reward Scaling:
    The effective reward is scaled by a parameter 'rho'. This captures 
    subjective valuation of the coin outcome.
    
    Parameters:
    learning_rate: [0, 1] Standard learning rate.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Weight mixing MB (1) and MF (0).
    rho: [0, 5] Reward scaling factor (subjective utility of 1 coin).
    """
    learning_rate, beta, w, rho = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updating ---
        effective_reward = reward[trial] * rho
        
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = effective_reward - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Pure Model-Free with Eligibility Traces (TD(1))
This model abandons the hybrid approach (setting w=0 implicitly) and focuses purely on temporal difference learning mechanisms. It uses an eligibility trace parameter (`lambda`) to determine how much the second-stage reward directly updates the first-stage choice, bridging the gap between the two steps without a model-based transition matrix.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free TD(lambda):
    No model-based component. Uses eligibility traces to update Stage 1 values 
    based on Stage 2 outcomes directly.
    
    Parameters:
    learning_rate: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    elig_trace: [0, 1] Eligibility trace decay (lambda). 
                 0 = TD(0) (only update from next state value), 
                 1 = TD(1) (update fully from final reward).
    """
    learning_rate, beta, elig_trace = model_parameters
    n_trials = len(action_1)
  
    # No transition matrix used here (Pure MF)
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updating ---
        
        # 1. Prediction error at Stage 1 (transition to Stage 2)
        # Value of state 2 is approximated by the Q-value of the chosen action in state 2
        delta_1 = q_stage2[state_idx, action_2[trial]] - q_stage1[action_1[trial]]
        
        # 2. Prediction error at Stage 2 (reward receipt)
        delta_2 = reward[trial] - q_stage2[state_idx, action_2[trial]]
        
        # Update Stage 2 Q-values
        q_stage2[state_idx, action_2[trial]] += learning_rate * delta_2
        
        # Update Stage 1 Q-values
        # It gets the immediate error (delta_1) plus a portion of the second stage error (delta_2)
        # mediated by the eligibility trace.
        q_stage1[action_1[trial]] += learning_rate * (delta_1 + elig_trace * delta_2)
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```