Here are three new cognitive models that explore different mechanisms for learning and decision-making in the two-step task. These models introduce variations in how eligibility traces are used, how learning rates might differ for unchosen options, and how uncertainty might guide exploration.

### Model 1: Hybrid Model with Eligibility Traces (TD($\lambda$))
This model introduces an eligibility trace parameter $\lambda$. Instead of updating the first-stage Model-Free value only based on the immediate transition to the second stage, the reward received at the second stage "drifts back" to update the first stage choice directly. This bridges the gap between the two stages more effectively than simple TD(0).

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Eligibility Traces (TD-lambda).
    
    Uses an eligibility trace to allow the Stage 2 reward to directly influence 
    the Stage 1 Model-Free Q-values, bridging the temporal gap.

    Parameters:
    learning_rate: [0, 1] - Learning rate for value updates.
    beta: [0, 10] - Inverse temperature for softmax choice.
    w: [0, 1] - Weighting between Model-Based (1) and Model-Free (0).
    lambda_param: [0, 1] - Eligibility trace decay parameter (0 = TD(0), 1 = Monte Carlo).
    """
    learning_rate, beta, w, lambda_param = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        chosen_a1 = action_1[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        chosen_a2 = action_2[trial]

        # --- Updates ---
        # 1. Prediction error at stage 1 (based on stage 2 value)
        delta_stage1 = q_stage2_mf[state_idx, chosen_a2] - q_stage1_mf[chosen_a1]
        q_stage1_mf[chosen_a1] += learning_rate * delta_stage1
        
        # 2. Prediction error at stage 2 (based on reward)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, chosen_a2]
        q_stage2_mf[state_idx, chosen_a2] += learning_rate * delta_stage2
        
        # 3. Eligibility trace update: Pass stage 2 error back to stage 1
        q_stage1_mf[chosen_a1] += learning_rate * lambda_param * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model with Forgetting (Decay)
This model assumes that the participant doesn't just learn about chosen options, but also "forgets" the value of unchosen options. This is implemented as a decay toward a neutral value (0 or 0.5, here assumed 0 for simplicity in Q-learning context) for actions not taken. This mimics memory decay or a passive "return to baseline."

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Passive Decay (Forgetting).
    
    Unchosen options decay towards 0, simulating memory loss or passive forgetting.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate for chosen options.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - Model-Based / Model-Free weight.
    decay_rate: [0, 1] - Rate at which unchosen Q-values decay to 0.
    """
    learning_rate, beta, w, decay_rate = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        chosen_a1 = action_1[trial]
        unchosen_a1 = 1 - chosen_a1

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        chosen_a2 = action_2[trial]
        unchosen_a2 = 1 - chosen_a2

        # --- Updates ---
        # Standard updates for chosen options
        delta_stage1 = q_stage2_mf[state_idx, chosen_a2] - q_stage1_mf[chosen_a1]
        q_stage1_mf[chosen_a1] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, chosen_a2]
        q_stage2_mf[state_idx, chosen_a2] += learning_rate * delta_stage2
        
        # Decay for unchosen options
        q_stage1_mf[unchosen_a1] *= (1 - decay_rate)
        q_stage2_mf[state_idx, unchosen_a2] *= (1 - decay_rate)
        
        # Note: The unvisited state at stage 2 is typically not decayed in standard models 
        # because it wasn't available, but some versions decay all unchosen Qs. 
        # Here we decay the unchosen action in the *visited* state.

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Pure Model-Free with Experience-Weighted Attraction (EWA) style Counterfactuals
This model discards the Model-Based component entirely to focus on a sophisticated Model-Free mechanism. It implements a form of "fictitious play" or counterfactual updating at the second stage. If the participant receives a reward (1) for one alien, they assume the other alien would have given 0 (and vice versa), updating the *unchosen* option with a separate learning rate.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free with Counterfactual Updating.
    
    Updates the unchosen option at the second stage based on the assumption that
    rewards are anti-correlated (if I got 1, the other was likely 0).
    
    Parameters:
    lr_chosen: [0, 1] - Learning rate for the chosen action.
    lr_unchosen: [0, 1] - Learning rate for the unchosen action (counterfactual).
    beta: [0, 10] - Inverse temperature.
    perseveration: [0, 5] - Choice stickiness for stage 1.
    """
    lr_chosen, lr_unchosen, beta, perseveration = model_parameters
    n_trials = len(action_1)
  
    # No transition matrix needed for pure MF
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        q_net_1 = q_stage1_mf.copy()
        if last_action_1 != -1:
            q_net_1[last_action_1] += perseveration

        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        chosen_a1 = action_1[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        chosen_a2 = action_2[trial]
        unchosen_a2 = 1 - chosen_a2

        # --- Updates ---
        # Stage 1 MF Update (Standard TD)
        delta_stage1 = q_stage2_mf[state_idx, chosen_a2] - q_stage1_mf[chosen_a1]
        q_stage1_mf[chosen_a1] += lr_chosen * delta_stage1
        
        # Stage 2 MF Update (Chosen)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, chosen_a2]
        q_stage2_mf[state_idx, chosen_a2] += lr_chosen * delta_stage2
        
        # Stage 2 MF Update (Unchosen / Counterfactual)
        # Assume reward structure is roughly conserved: if reward=1, unchosen=0.
        # This is a heuristic often used in bandit tasks.
        counterfactual_reward = 0 # Simple assumption: unchosen yields 0
        delta_stage2_unchosen = counterfactual_reward - q_stage2_mf[state_idx, unchosen_a2]
        q_stage2_mf[state_idx, unchosen_a2] += lr_unchosen * delta_stage2_unchosen
        
        last_action_1 = chosen_a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```