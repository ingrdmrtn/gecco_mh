Here are three cognitive models proposing different mechanisms for how the participant makes decisions in the two-step task.

### Model 1: Hybrid Model-Based/Model-Free Learner
This model posits that the participant uses a mixture of two strategies: a Model-Based (MB) strategy that uses the transition matrix to plan ahead, and a Model-Free (MF) strategy that learns from direct experience. A mixing parameter `w` determines the balance between these two systems.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Learner.
    
    This model combines Model-Based (planning using the transition matrix) and 
    Model-Free (TD learning) value estimation. The first-stage choice is a 
    weighted combination of both systems.
    
    Parameters:
    - learning_rate: Rate at which Q-values update [0, 1].
    - beta: Inverse temperature for softmax choice [0, 10].
    - w: Weighting parameter [0, 1]. 0 = Pure MF, 1 = Pure MB.
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition matrix (70% common, 30% rare)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)       # Model-free values for spaceships
    q_stage2_mf = np.zeros((2, 2))  # Model-free values for aliens (planets x aliens)

    for trial in range(n_trials):
        # --- STAGE 1 CHOICE ---
        # Model-Based Value: V(planet) = max(Q(alien))
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value: Q_net = w * Q_MB + (1-w) * Q_MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial] # Planet arrived at

        # --- STAGE 2 CHOICE ---
        # Standard softmax on stage 2 Q-values
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- LEARNING ---
        # 1. Update Stage 2 Q-values (TD error from reward)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # 2. Update Stage 1 MF Q-values (TD error from Stage 2 value)
        # Note: We use the value of the state actually reached (q_stage2_mf[state_idx, action_2])
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Perseveration Model (Habitual Repeater)
This model assumes the participant is primarily Model-Free but has a tendency to repeat their previous Stage 1 choice regardless of the outcome ("perseveration"). This captures motor habits or cognitive inertia often seen in behavioral tasks.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Learner with Perseveration.
    
    Adds a 'stickiness' parameter to the standard TD learner. The model is biased
    to repeat the action taken on the previous trial at Stage 1, independent of reward.
    
    Parameters:
    - learning_rate: Rate at which Q-values update [0, 1].
    - beta: Inverse temperature for softmax choice [0, 10].
    - perseveration: Bias added to the previously chosen action's Q-value [-2, 2].
                     (Positive = stickiness, Negative = alternation)
    """
    learning_rate, beta, perseveration = model_parameters
    n_trials = len(action_1)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track previous action (initialize to -1 or None, handled in loop)
    last_action_1 = -1

    for trial in range(n_trials):
        
        # --- STAGE 1 CHOICE ---
        # Add perseveration bonus to the Q-values temporarily for choice calculation
        current_q_stage1 = q_stage1_mf.copy()
        if last_action_1 != -1:
            current_q_stage1[last_action_1] += perseveration

        exp_q1 = np.exp(beta * current_q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- STAGE 2 CHOICE ---
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- LEARNING ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1

        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Update history
        last_action_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Distinct Learning Rates (Stage 1 vs Stage 2)
This model hypothesizes that the participant updates their beliefs about the two stages of the task at different speeds. For example, they might learn the value of aliens (Stage 2) quickly but are slower to update the value of spaceships (Stage 1), or vice versa. This is a pure Model-Free learner but with split learning rates.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Learner with Distinct Learning Rates.
    
    Uses separate learning rates for the first stage (spaceships) and the 
    second stage (aliens). This accounts for different plasticity or attention
    allocated to the two levels of the hierarchy.
    
    Parameters:
    - lr_stage1: Learning rate for spaceship choices [0, 1].
    - lr_stage2: Learning rate for alien choices [0, 1].
    - beta: Inverse temperature for softmax choice [0, 10].
    """
    lr_stage1, lr_stage2, beta = model_parameters
    n_trials = len(action_1)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- STAGE 1 CHOICE ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- STAGE 2 CHOICE ---
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- LEARNING ---
        # Update Stage 1 using lr_stage1
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr_stage1 * delta_stage1

        # Update Stage 2 using lr_stage2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr_stage2 * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```