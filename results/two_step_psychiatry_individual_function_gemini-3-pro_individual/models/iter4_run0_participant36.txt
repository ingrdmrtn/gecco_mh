Here are three new cognitive models based on the two-step task framework, exploring different mechanisms like mixture models (hybrid MB/MF) and choice perseverance.

### Model 1: Hybrid Model-Based / Model-Free Learner
This model posits that the participant uses a weighted combination of both strategies. They maintain a Model-Free value for the first stage (learning directly from outcomes) and calculate a Model-Based value (planning using the transition matrix). A mixing parameter `w` controls the balance between these two systems.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Learner.
    
    This model assumes the agent computes both a Model-Based (MB) value via planning
    and a Model-Free (MF) value via direct reinforcement for the first-stage choice.
    The final choice is a weighted mixture of these two values.

    Parameters:
    learning_rate: [0,1] Learning rate for Q-value updates.
    beta: [0,10] Inverse temperature for softmax choice.
    w: [0,1] Weighting parameter (0 = Pure MF, 1 = Pure MB).
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)

    # Transition matrix: P(Planet X | Ship A), P(Planet Y | Ship A), etc.
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Q-values
    q_stage1_mf = np.zeros(2)       # Model-free values for stage 1 (Ships)
    q_stage2_mf = np.zeros((2, 2))  # Values for stage 2 (Aliens on Planets)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):

        # --- Stage 1 Choice ---
        # 1. Calculate Model-Based values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Combine MB and MF values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # 3. Softmax
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        # --- Stage 2 Choice ---
        state_idx = state[trial]
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning ---
        # Update Stage 1 MF value (TD(0) update using Stage 2 value as proxy)
        # Note: We use the value of the CHOSEN state in stage 2
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1

        # Update Stage 2 MF value (Reward prediction error)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Model-Free Learner with Eligibility Traces (TD(1))
This model is purely Model-Free but uses an eligibility trace (parameter `lambda`). Instead of just updating the first stage based on the second stage value, the reward outcome propagates all the way back to the first stage choice directly. This allows the first stage to learn directly from the final reward, bypassing the second stage value estimate to some degree.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Learner with Eligibility Traces (TD(lambda)).
    
    This model updates the first-stage values based on a combination of the
    second-stage value estimate and the final reward outcome. The lambda parameter
    controls how much the final reward directly influences the first-stage update.

    Parameters:
    learning_rate: [0,1] Learning rate for Q-value updates.
    beta: [0,10] Inverse temperature for softmax choice.
    lambd: [0,1] Eligibility trace decay (0 = TD(0), 1 = Monte Carlo).
    """
    learning_rate, beta, lambd = model_parameters
    n_trials = len(action_1)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):

        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        # --- Stage 2 Choice ---
        state_idx = state[trial]
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning ---
        # 1. Prediction error at stage 1 (driven by stage 2 value)
        delta_1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        # 2. Prediction error at stage 2 (driven by reward)
        delta_2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Update Stage 2
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_2
        
        # Update Stage 1: It gets its own error (delta_1) PLUS a fraction of the stage 2 error (delta_2)
        # This effectively links the reward back to the first choice.
        q_stage1_mf[action_1[trial]] += learning_rate * (delta_1 + lambd * delta_2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Learner with Choice Perseverance
This model extends the Hybrid (MB/MF) framework by adding a "stickiness" or perseverance parameter. Participants often have a tendency to repeat their previous choice regardless of reward (motor perseverance) or cognitive inertia. This model adds a bias term to the Q-values of the previously chosen action.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner with Choice Perseverance (Stickiness).
    
    Combines Model-Based and Model-Free learning, but adds a perseverance bonus
    to the action chosen in the immediately preceding trial. This captures the
    tendency to repeat actions regardless of value.

    Parameters:
    learning_rate: [0,1] Learning rate.
    beta: [0,10] Inverse temperature.
    w: [0,1] MB/MF weight (0=MF, 1=MB).
    pers: [0,5] Perseverance bonus (added to Q-value of repeated choice).
    """
    learning_rate, beta, w, pers = model_parameters
    n_trials = len(action_1)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1 # Initialize with an impossible action

    for trial in range(n_trials):

        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add perseverance bonus if applicable
        if last_action_1 != -1:
            q_net[last_action_1] += pers
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Store action for next trial's perseverance
        last_action_1 = action_1[trial]

        # --- Stage 2 Choice ---
        state_idx = state[trial]
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1

        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```