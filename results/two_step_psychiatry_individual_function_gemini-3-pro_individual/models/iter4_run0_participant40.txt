Here are three new cognitive models for the two-step decision task. These models introduce different mechanisms for value estimation and choice, specifically focusing on perseveration, separate learning rates for positive/negative outcomes, and a mix of model-based and model-free strategies.

### Model 1: Perseveration Model
This model extends the standard temporal difference learner by adding a "perseveration" parameter. This captures the tendency of participants to repeat their previous first-stage choice regardless of the reward, a common phenomenon in this task that is distinct from value-based learning.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Learner with Choice Perseveration.

    This model includes a 'stickiness' or perseveration parameter (p) which 
    biases the first-stage choice towards the action taken on the immediately 
    preceding trial, independent of reward history.

    Parameters:
    - learning_rate: [0, 1] Rate at which Q-values are updated.
    - beta: [0, 10] Inverse temperature for softmax choice.
    - perseveration: [-5, 5] Bias for repeating the previous choice. 
      (Positive values = stickiness, Negative values = switching).
    """
    learning_rate, beta, perseveration = model_parameters
    n_trials = len(action_1)

    q_stage1_mf = np.zeros(2) 
    q_stage2_mf = np.zeros((2, 2)) 

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Track the previous action (initialize to -1 or handle first trial separately)
    prev_action_1 = -1

    for trial in range(n_trials):
        
        # --- Stage 1 Choice ---
        # Add perseveration bonus to the Q-value of the previous action
        q_stage1_effective = q_stage1_mf.copy()
        if prev_action_1 != -1:
            q_stage1_effective[prev_action_1] += perseveration
            
        exp_q1 = np.exp(beta * q_stage1_effective)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        state_idx = state[trial]
        chosen_a1 = action_1[trial]
        prev_action_1 = chosen_a1 # Update for next trial

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        chosen_a2 = action_2[trial]
        r = reward[trial]

        # --- Learning (SARSA / TD) ---
        # Prediction error for stage 1
        delta_stage1 = q_stage2_mf[state_idx, chosen_a2] - q_stage1_mf[chosen_a1]
        q_stage1_mf[chosen_a1] += learning_rate * delta_stage1

        # Prediction error for stage 2
        delta_stage2 = r - q_stage2_mf[state_idx, chosen_a2]
        q_stage2_mf[state_idx, chosen_a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Asymmetric Learning Rates
This model hypothesizes that the participant learns differently from positive outcomes (finding gold) versus negative/neutral outcomes (no gold/loss). It splits the learning rate into `alpha_pos` and `alpha_neg`.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Dual Learning Rate Model (Asymmetric Update).

    This model assumes participants update their value estimates differently 
    depending on whether the prediction error is positive (better than expected) 
    or negative (worse than expected).

    Parameters:
    - alpha_pos: [0, 1] Learning rate for positive prediction errors.
    - alpha_neg: [0, 1] Learning rate for negative prediction errors.
    - beta: [0, 10] Inverse temperature for softmax choice.
    """
    alpha_pos, alpha_neg, beta = model_parameters
    n_trials = len(action_1)

    q_stage1_mf = np.zeros(2) 
    q_stage2_mf = np.zeros((2, 2)) 

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):

        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        state_idx = state[trial]
        chosen_a1 = action_1[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        chosen_a2 = action_2[trial]
        r = reward[trial]

        # --- Learning ---
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[state_idx, chosen_a2]
        lr_2 = alpha_pos if delta_stage2 > 0 else alpha_neg
        q_stage2_mf[state_idx, chosen_a2] += lr_2 * delta_stage2
        
        # Stage 1 Update
        # Note: We use the updated stage 2 value for the stage 1 target here (SARSA-like)
        delta_stage1 = q_stage2_mf[state_idx, chosen_a2] - q_stage1_mf[chosen_a1]
        lr_1 = alpha_pos if delta_stage1 > 0 else alpha_neg
        q_stage1_mf[chosen_a1] += lr_1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model-Based / Model-Free
This is the classic "Hybrid" model for this task. It assumes the participant's first-stage choice is a weighted combination of a Model-Free (MF) system (which learns simply by TD errors) and a Model-Based (MB) system (which calculates values using the known transition matrix).

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Learner.

    The first-stage Q-values are a weighted sum of Model-Free values (learned 
    via TD) and Model-Based values (calculated using the transition matrix 
    and max stage 2 values).

    Parameters:
    - learning_rate: [0, 1] Rate at which MF Q-values are updated.
    - beta: [0, 10] Inverse temperature for softmax choice.
    - w: [0, 1] Weighting parameter (0 = Pure MF, 1 = Pure MB).
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition structure: A->X (70%), U->Y (70%)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # Used by both MB and MF systems

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):

        # --- Stage 1 Choice ---
        # 1. Calculate Model-Based Values
        # Q_MB(s1, a) = sum(P(s2|s1,a) * max_a' Q_MF(s2, a'))
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Combine MB and MF values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        state_idx = state[trial]
        chosen_a1 = action_1[trial]

        # --- Stage 2 Choice ---
        # Stage 2 is purely model-free in this standard hybrid formulation
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        chosen_a2 = action_2[trial]
        r = reward[trial]
  
        # --- Learning ---
        # Update Stage 2 values (MF)
        delta_stage2 = r - q_stage2_mf[state_idx, chosen_a2]
        q_stage2_mf[state_idx, chosen_a2] += learning_rate * delta_stage2
        
        # Update Stage 1 values (MF only)
        # Note: MB values don't need updating as transition matrix is fixed
        delta_stage1 = q_stage2_mf[state_idx, chosen_a2] - q_stage1_mf[chosen_a1]
        q_stage1_mf[chosen_a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```