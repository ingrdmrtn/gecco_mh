Here are three new cognitive models based on the two-step task paradigm.

### Model 1: Hybrid Model-Based / Model-Free Learner
This is the classic "Daw et al. (2011)" style model. It assumes the participant uses a mixture of two strategies:
1.  **Model-Free (MF):** Learns values based on reward prediction errors (habitual).
2.  **Model-Based (MB):** Calculates values by planning forward using the known transition matrix (goal-directed).

The final choice at Stage 1 is a weighted combination of these two valuations.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Learner.
    
    Combines habitual (MF) and goal-directed (MB) strategies.
    Stage 1 Q-values are a weighted sum of MF and MB values.
    Stage 2 is purely MF (since it's the final step).
    
    Parameters:
    - learning_rate: [0, 1] Update rate for Q-values.
    - beta: [0, 10] Inverse temperature for softmax.
    - w: [0, 1] Weighting parameter (0 = pure MF, 1 = pure MB).
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)
    
    # Fixed transition matrix as per task description (A->X common, U->Y common)
    # Rows: Actions (0, 1), Cols: States (0, 1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- STAGE 1 CHOICE ---
        # Calculate Model-Based Values for Stage 1
        # V_MB(s1, a) = Sum P(s2|s1,a) * max_a' Q(s2, a')
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value: Q_net = w * Q_MB + (1-w) * Q_MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial] # Actual planet arrived at
        a1 = action_1[trial]
        
        # --- STAGE 2 CHOICE ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]

        # --- UPDATING ---
        # Update Stage 1 MF value using TD(1) logic (or TD(0) to best stage 2 option)
        # Here we use the standard TD-error driven update
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Update Stage 2 MF value
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Perseveration Model (Model-Free + Stickiness)
Often, participants simply repeat their previous choice regardless of reward ("stickiness" or perseveration). This model adds a parameter `perseveration` to the standard Model-Free learner. If `perseveration` is positive, the model is more likely to repeat the last action taken at Stage 1.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Learner with Choice Perseveration (Stickiness).
    
    Adds a bias to repeat the previous Stage 1 action, independent of reward history.
    
    Parameters:
    - learning_rate: [0, 1] Update rate for Q-values.
    - beta: [0, 10] Inverse temperature.
    - perseveration: [0, 5] Bonus added to the Q-value of the previously chosen action.
    """
    learning_rate, beta, perseveration = model_parameters
    n_trials = len(action_1)

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) 
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1 # Indicator for previous action

    for trial in range(n_trials):
        # --- STAGE 1 CHOICE ---
        # Add stickiness bonus
        q_net = q_stage1_mf.copy()
        if last_action_1 != -1:
            q_net[last_action_1] += perseveration
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Store for next trial
        last_action_1 = action_1[trial]
        
        state_idx = state[trial]
        a1 = action_1[trial]

        # --- STAGE 2 CHOICE ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]

        # --- UPDATING ---
        # Standard SARSA/TD updates
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Model-Based with Eligibility Traces (TD(lambda))
This model introduces an eligibility trace parameter `lam` (lambda). In the two-step task, this controls how much the Stage 2 reward directly updates the Stage 1 choice.
- If `lam` = 0: Stage 1 is updated only by the value of the state arrived at (TD(0)).
- If `lam` = 1: Stage 1 is updated directly by the final reward (Monte Carlo / TD(1)).
This allows the model to capture direct reinforcement of the first choice by the final outcome, bypassing the intermediate state value.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Reinforcement Learning with Eligibility Traces (TD(lambda)).
    
    Allows the reward at the end of the trial to directly influence the 
    Stage 1 choice, bridging the gap between the two stages.
    
    Parameters:
    - learning_rate: [0, 1] Update rate.
    - beta: [0, 10] Inverse temperature.
    - lam: [0, 1] Eligibility trace decay (0 = TD(0), 1 = Monte Carlo).
    """
    learning_rate, beta, lam = model_parameters
    n_trials = len(action_1)

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- STAGE 1 CHOICE ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        a1 = action_1[trial]

        # --- STAGE 2 CHOICE ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]

        # --- UPDATING ---
        # 1. Prediction error at Stage 2 (reward prediction error)
        # The agent arrived at state_idx, chose a2, and got r.
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        
        # 2. Prediction error at Stage 1 (state prediction error)
        # The agent chose a1 and arrived at state_idx (value is Q(state_idx, a2))
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]

        # Update Stage 2 Q-value
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Update Stage 1 Q-value
        # It gets the immediate Stage 1 error PLUS a portion (lambda) of the Stage 2 error
        q_stage1_mf[a1] += learning_rate * (delta_stage1 + lam * delta_stage2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```