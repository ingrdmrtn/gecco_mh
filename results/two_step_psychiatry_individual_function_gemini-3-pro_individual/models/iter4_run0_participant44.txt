Here are three new cognitive models that explore different mechanisms for learning and decision-making in the two-step task. These models introduce variations on pure model-based learning, a hybrid model with eligibility traces, and a model with separate learning rates for positive and negative outcomes.

### Model 1: Pure Model-Based Reinforcement Learning
This model assumes the participant builds an internal model of the task structure (the transition matrix) and plans their first-stage choices by calculating the expected value of the second stage. Unlike the hybrid models often used, this one relies *entirely* on the calculated model-based values for the first stage choice, ignoring any direct caching of first-stage action values (TD-learning).

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Based Reinforcement Learning.
    
    This model assumes the participant calculates the value of stage 1 actions
    by combining the known transition probabilities with the learned values
    of the stage 2 states. It does not maintain a separate model-free value
    for stage 1 actions (q_stage1_mf is unused for decision making).

    Bounds:
    learning_rate: [0,1] - Update rate for stage 2 values.
    beta: [0,10] - Inverse temperature for softmax.
    """
    learning_rate, beta = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition matrix as per task description (A->X common, U->Y common)
    # Rows: Action 0 (A), Action 1 (U)
    # Cols: State 0 (X), State 1 (Y)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values for the second stage (aliens)
    q_stage2_mf = np.zeros((2, 2)) + 0.5 # Initialize with neutral expectation

    for trial in range(n_trials):

        # --- Policy for the first choice (Model-Based) ---
        # Calculate max value available in each state (planning)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        
        # Bellman equation: Q_MB(a1) = sum(P(s2|a1) * max(Q(s2, a2)))
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]

        # --- Policy for the second choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # --- Value Updating ---
        # Only stage 2 values are updated via TD error.
        # Stage 1 values are re-computed at the start of next trial based on these.
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] = q_stage2_mf[state_idx, a2] + learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Model-Free RL with Eligibility Traces (TD(lambda))
This model introduces an eligibility trace parameter (`lambda`). Instead of just updating the second stage based on reward and the first stage based on the second stage's value (TD-0), the reward received at the end of the trial "feeds back" to update the first-stage choice directly. This allows the model to capture the direct association between the first choice and the final reward, bridging the gap between the two stages.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free RL with Eligibility Traces (TD-lambda).
    
    This model allows the reward at the second stage to directly influence
    the value of the first stage action, mediated by an eligibility trace
    parameter (lambda_param). This is a more sophisticated model-free approach
    than simple TD-0.

    Bounds:
    learning_rate: [0,1]
    beta: [0,10]
    lambda_param: [0,1] - Eligibility trace decay factor.
    """
    learning_rate, beta, lambda_param = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        
        a1 = action_1[trial]
        state_idx = state[trial]
        a2 = action_2[trial]
        r = reward[trial]

        # --- Policy for the first choice ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Policy for the second choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # --- Value Updating ---
        
        # 1. Prediction error at stage 1 (transition to stage 2)
        # Note: In SARSA/TD(lambda), we usually use Q(s', a') for the bootstrap.
        # Here we use the value of the state chosen (Q(s, a2)) as the proxy for V(s').
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        
        # Update stage 1 based on transition
        q_stage1_mf[a1] = q_stage1_mf[a1] + learning_rate * delta_stage1
        
        # 2. Prediction error at stage 2 (reward receipt)
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        
        # Update stage 2 based on reward
        q_stage2_mf[state_idx, a2] = q_stage2_mf[state_idx, a2] + learning_rate * delta_stage2
        
        # 3. Eligibility Trace Update for Stage 1
        # The stage 1 action also learns from the stage 2 prediction error, scaled by lambda
        q_stage1_mf[a1] = q_stage1_mf[a1] + learning_rate * lambda_param * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Asymmetric Learning Rate Model (Risk Sensitivity)
This model posits that the participant learns differently from positive outcomes (gains) versus negative outcomes (absence of reward). This is often observed in clinical populations or specific personality traits. It uses two separate learning rates: `alpha_pos` for when the prediction error is positive, and `alpha_neg` for when it is negative. It uses a pure model-free architecture to isolate this learning rate effect.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Asymmetric Learning Rate Model (Risk Sensitivity).
    
    This model differentiates between learning from "good news" (positive prediction errors)
    and "bad news" (negative prediction errors). It uses two separate learning rates.
    It operates on a Model-Free architecture.

    Bounds:
    alpha_pos: [0,1] - Learning rate for positive prediction errors.
    alpha_neg: [0,1] - Learning rate for negative prediction errors.
    beta: [0,10]
    """
    alpha_pos, alpha_neg, beta = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        
        a1 = action_1[trial]
        state_idx = state[trial]
        a2 = action_2[trial]
        r = reward[trial]

        # --- Policy for the first choice ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Policy for the second choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # --- Value Updating ---
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        if delta_stage1 > 0:
            q_stage1_mf[a1] = q_stage1_mf[a1] + alpha_pos * delta_stage1
        else:
            q_stage1_mf[a1] = q_stage1_mf[a1] + alpha_neg * delta_stage1
            
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        if delta_stage2 > 0:
            q_stage2_mf[state_idx, a2] = q_stage2_mf[state_idx, a2] + alpha_pos * delta_stage2
        else:
            q_stage2_mf[state_idx, a2] = q_stage2_mf[state_idx, a2] + alpha_neg * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```