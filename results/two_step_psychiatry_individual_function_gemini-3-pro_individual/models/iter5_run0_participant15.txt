Here are the 3 proposed cognitive models.

### Model 1: Hybrid Model with Stage-Specific Learning Rates
This model hypothesizes that the participant learns the value of the spaceships (Stage 1, which has stable transition probabilities but indirect rewards) at a different rate than they learn the value of the aliens (Stage 2, which has volatile rewards).

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF Model with Stage-Specific Learning Rates.
    
    This model separates the learning rate for the first stage (spaceships) 
    and the second stage (aliens). This accounts for the possibility that 
    the participant updates values differently for the stable transition 
    structure versus the fluctuating reward structure.

    Parameters:
    alpha_1: [0, 1] - Learning rate for Stage 1 (Spaceship choice).
    alpha_2: [0, 1] - Learning rate for Stage 2 (Alien choice).
    beta: [0, 10] - Inverse temperature (exploration/exploitation trade-off).
    w: [0, 1] - Weighting parameter (1 = fully Model-Based, 0 = fully Model-Free).
    """
    alpha_1, alpha_2, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action

    for trial in range(n_trials):

        # --- Policy for the first choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Weighted combination of MF and MB values
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Policy for the second choice ---
        # Standard Softmax on Stage 2 Q-values
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Value Updating ---
        # TD(0) update for Stage 1 using alpha_1
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += alpha_1 * delta_stage1
        
        # TD(0) update for Stage 2 using alpha_2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += alpha_2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model with Passive Decay (Forgetting)
This model introduces a memory decay mechanism. It assumes that the values of aliens (Stage 2 options) that are *not* visited slowly decay towards zero. This captures the cognitive cost of maintaining precise value estimates for unvisited states over time.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF Model with Passive Value Decay.
    
    This model assumes that the Q-values of unchosen actions in Stage 2 
    decay over time. If an alien is not visited, its expected value 
    shrinks by a factor of (1 - decay_rate), representing forgetting.

    Parameters:
    learning_rate: [0, 1] - Update rate for chosen actions.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - Weighting parameter (1 = fully Model-Based, 0 = fully Model-Free).
    decay_rate: [0, 1] - Rate at which unchosen Stage 2 values decay toward 0.
    """
    learning_rate, beta, w, decay_rate = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Policy for the first choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Policy for the second choice ---
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Value Updating ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # --- Decay for unchosen Stage 2 actions ---
        # Apply decay to the unchosen action in the current state
        unchosen_action_2 = 1 - action_2[trial]
        q_stage2_mf[state_idx, unchosen_action_2] *= (1.0 - decay_rate)
        
        # Also apply decay to both actions in the *other* state (since neither was visited)
        other_state_idx = 1 - state_idx
        q_stage2_mf[other_state_idx, :] *= (1.0 - decay_rate)
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model with Stage-Specific Inverse Temperatures
This model posits that the participant's level of "decision noise" (randomness) differs between the two stages. They might be very exploratory when choosing spaceships (Stage 1) but very exploitative when choosing aliens (Stage 2), or vice versa.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF Model with Stage-Specific Inverse Temperatures.
    
    This model allows for different levels of choice stochasticity (beta) 
    at Stage 1 and Stage 2. This captures the idea that the participant 
    might explore more aggressively in one stage while exploiting known 
    values in the other.

    Parameters:
    learning_rate: [0, 1] - Update rate for Q-values.
    beta_1: [0, 10] - Inverse temperature for Stage 1 (Spaceships).
    beta_2: [0, 10] - Inverse temperature for Stage 2 (Aliens).
    w: [0, 1] - Weighting parameter (1 = fully Model-Based, 0 = fully Model-Free).
    """
    learning_rate, beta_1, beta_2, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Policy for the first choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Use beta_1 for the first stage decision
        exp_q1 = np.exp(beta_1 * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Policy for the second choice ---
        qs_current_state = q_stage2_mf[state_idx]
        
        # Use beta_2 for the second stage decision
        exp_q2 = np.exp(beta_2 * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Value Updating ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```