Here are three new cognitive models based on the two-step task structure, exploring parameter combinations and mechanisms not yet listed in the feedback.

### Model 1: Hybrid Learner with Asymmetric Updating
This model hypothesizes that the participant integrates Model-Based and Model-Free values (Hybrid) but updates their value estimates differently depending on whether the prediction error is positive (better than expected) or negative (worse than expected). This "optimism bias" or "pessimism bias" is captured by splitting the learning rate.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner with Asymmetric Updating (Positive/Negative Learning Rates).
    
    Combines MB and MF strategies, but learns from positive prediction errors 
    at a different rate than negative prediction errors.

    Parameters:
    alpha_pos: [0,1] - Learning rate for positive prediction errors.
    alpha_neg: [0,1] - Learning rate for negative prediction errors.
    beta: [0,10] - Inverse temperature (exploration/exploitation).
    w: [0,1] - Weighting parameter (0 = Pure MF, 1 = Pure MB).
    """
    alpha_pos, alpha_neg, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Mix MB and MF values
        q_hybrid = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_hybrid)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Update Stage 1
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        lr_1 = alpha_pos if delta_stage1 > 0 else alpha_neg
        q_stage1_mf[action_1[trial]] += lr_1 * delta_stage1
        
        # Update Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        lr_2 = alpha_pos if delta_stage2 > 0 else alpha_neg
        q_stage2_mf[state_idx, action_2[trial]] += lr_2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Learner with Win-Stay Bonus
Unlike standard perseveration (stickiness regardless of outcome), this model implements a "Win-Stay" heuristic. The participant receives a bonus added to the value of the previously chosen spaceship *only if* that choice resulted in a reward. This captures a specific type of reinforcement-driven repetition within a hybrid framework.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner with Win-Stay Bonus.
    
    Standard Hybrid model, but adds a 'bonus' value to the previously chosen 
    Stage 1 action ONLY if the previous trial was rewarded. 
    This captures outcome-dependent stickiness.

    Parameters:
    learning_rate: [0,1] - Update rate for Q-values.
    beta: [0,10] - Inverse temperature.
    w: [0,1] - Weighting parameter (0 = Pure MF, 1 = Pure MB).
    win_bonus: [0,5] - Value added to previous choice if rewarded.
    """
    learning_rate, beta, w, win_bonus = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    last_reward = 0

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_hybrid = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Apply Win-Stay Bonus
        bonus_vec = np.zeros(2)
        if last_action_1 != -1 and last_reward == 1:
            bonus_vec[last_action_1] = win_bonus
            
        exp_q1 = np.exp(beta * (q_hybrid + bonus_vec))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Update Stage 1
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Update Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        last_action_1 = action_1[trial]
        last_reward = reward[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Model-Free with Eligibility Traces and Stage-Specific Noise
This model abandons the Hybrid/Model-Based component (`w=0`) to focus on a sophisticated Model-Free architecture. It uses eligibility traces (`lambda`) to allow the Stage 2 reward to directly update Stage 1 values. Crucially, it acknowledges that the decision noise (temperature) might differ between the strategic Stage 1 choice and the reactive Stage 2 choice.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Learner with Eligibility Traces and Dual Betas.
    
    Pure Model-Free (no MB component), but uses eligibility traces (lambda)
    to update Stage 1 values based on Stage 2 RPEs. 
    Uses separate inverse temperatures for Stage 1 and Stage 2.

    Parameters:
    learning_rate: [0,1] - Update rate.
    beta_1: [0,10] - Inverse temperature for Stage 1 choice.
    beta_2: [0,10] - Inverse temperature for Stage 2 choice.
    lam: [0,1] - Eligibility trace decay (0 = TD(0), 1 = Monte Carlo).
    """
    learning_rate, beta_1, beta_2, lam = model_parameters
    n_trials = len(action_1)
  
    # No transition matrix needed for Pure MF
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice (Pure MF, using beta_1)
        exp_q1 = np.exp(beta_1 * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice (Using beta_2)
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Update Stage 1 (TD(0) part)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Update Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Eligibility Trace Update (TD(lambda) part): 
        # Update Stage 1 choice again based on Stage 2 outcome
        q_stage1_mf[action_1[trial]] += learning_rate * lam * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```