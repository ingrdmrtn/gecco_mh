Here are 3 new cognitive models based on the two-step task structure and the provided participant data.

### Model 1: Hybrid Learner with Separate Learning Rates for Positive/Negative Prediction Errors
This model hypothesizes that the participant learns differently from good outcomes (rewards) versus bad outcomes (omissions). This asymmetry is often observed in clinical populations or specific cognitive styles. It uses a hybrid Model-Based (MB) and Model-Free (MF) architecture, weighted by `w`.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid learner with asymmetric learning rates for positive and negative prediction errors.
    
    Parameters:
    alpha_pos: [0,1] - Learning rate for positive prediction errors (RPE > 0).
    alpha_neg: [0,1] - Learning rate for negative prediction errors (RPE < 0).
    beta: [0,10] - Inverse temperature for softmax.
    w: [0,1] - Weighting parameter (0 = Pure Model-Free, 1 = Pure Model-Based).
    """
    alpha_pos, alpha_neg, beta, w = model_parameters
    n_trials = len(action_1)

    # Fixed transition matrix for the model-based component
    # Rows: Choice 0/1, Cols: State 0/1. 
    # Usually Choice 0 -> State 0 (0.7), Choice 1 -> State 1 (0.7)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based Value Calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1) # Max value in each state
        q_stage1_mb = transition_matrix @ max_q_stage2

        # Integrated Value (Hybrid)
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        a1 = action_1[trial]
        s_idx = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        a2 = action_2[trial]
        r = reward[trial]

        # --- Learning ---
        # Stage 2 RPE
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        # Apply asymmetric learning rates for Stage 2
        lr_2 = alpha_pos if delta_stage2 > 0 else alpha_neg
        q_stage2_mf[s_idx, a2] += lr_2 * delta_stage2

        # Stage 1 RPE (SARSA-style TD(0) update based on Stage 2 value)
        # Note: In standard hybrid models, MF Q1 is updated by the Q2 value of the state reached
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        
        # Apply asymmetric learning rates for Stage 1
        lr_1 = alpha_pos if delta_stage1 > 0 else alpha_neg
        q_stage1_mf[a1] += lr_1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Pure Model-Based Learner with Transition Learning
This model assumes the participant is purely Model-Based (w=1) but does *not* assume the transition probabilities are fixed at 0.7/0.3. Instead, the participant actively learns the transition structure of the spaceships (`transition_lr`) alongside the reward values. This captures the behavior of participants who might be skeptical of the instructions or sensitive to transition fluctuations.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Based Learner that dynamically learns the transition matrix.
    
    Parameters:
    learning_rate: [0,1] - Learning rate for reward values (Q-values).
    transition_lr: [0,1] - Learning rate for updating transition probabilities.
    beta: [0,10] - Inverse temperature.
    """
    learning_rate, transition_lr, beta = model_parameters
    n_trials = len(action_1)

    # Initialize transition matrix (start with uniform priors or instructions)
    # T[action, state] = probability of reaching state given action
    # Initializing at 0.5 implies no initial knowledge
    T = np.ones((2, 2)) * 0.5 

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage2 = np.zeros((2, 2)) # State x Action

    for trial in range(n_trials):
        # --- Stage 1 Choice (Pure Model-Based) ---
        max_q_stage2 = np.max(q_stage2, axis=1)
        
        # Q_MB(a1) = Sum(P(s|a1) * max(Q(s, a2)))
        q_stage1_mb = np.zeros(2)
        q_stage1_mb[0] = T[0, 0] * max_q_stage2[0] + T[0, 1] * max_q_stage2[1]
        q_stage1_mb[1] = T[1, 0] * max_q_stage2[0] + T[1, 1] * max_q_stage2[1]

        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        a1 = action_1[trial]
        s_idx = state[trial] # Actual state reached

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        a2 = action_2[trial]
        r = reward[trial]

        # --- Learning ---
        
        # 1. Update Reward Values
        q_stage2[s_idx, a2] += learning_rate * (r - q_stage2[s_idx, a2])

        # 2. Update Transition Matrix estimates
        # We observed transition a1 -> s_idx.
        # Update the row for action a1 towards the observed state outcome (1)
        # and away from the unobserved state outcome (0)
        
        # Create a one-hot vector for the observed state
        observed_transition = np.zeros(2)
        observed_transition[s_idx] = 1.0 
        
        # Simple delta rule for probability estimation
        T[a1] += transition_lr * (observed_transition - T[a1])
        
        # Normalize to ensure probabilities sum to 1
        T[a1] /= np.sum(T[a1])

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Model-Free Learner with "Win-Stay, Lose-Shift" Heuristic
This model combines standard reinforcement learning (Model-Free) with a simple heuristic strategy often seen in animal and human behavior: "Win-Stay, Lose-Shift" (WSLS). The `wsls_weight` parameter dictates how much the previous outcome directly biases the next Stage 1 choice, bypassing value calculation.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Learner augmented with a Win-Stay, Lose-Shift (WSLS) heuristic bias.
    
    Parameters:
    learning_rate: [0,1] - Update rate for Q-values.
    beta: [0,10] - Inverse temperature.
    wsls_weight: [0,5] - Strength of the WSLS bias added to the Q-values.
    """
    learning_rate, beta, wsls_weight = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    # Track previous trial info for WSLS
    prev_a1 = -1
    prev_reward = -1

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Calculate base Q-values
        net_q1 = q_stage1.copy()
        
        # Apply WSLS Bias if not the first trial
        if trial > 0:
            if prev_reward == 1:
                # Win-Stay: Boost value of previous action
                net_q1[prev_a1] += wsls_weight
            else:
                # Lose-Shift: Boost value of the OTHER action (or punish previous)
                # Equivalently, we can just subtract weight from prev_a1
                net_q1[prev_a1] -= wsls_weight

        exp_q1 = np.exp(beta * net_q1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        a1 = action_1[trial]
        s_idx = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        a2 = action_2[trial]
        r = reward[trial]

        # --- Learning ---
        # Standard TD(1) / SARSA updates
        delta_stage2 = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += learning_rate * delta_stage2
        
        # Direct reinforcement of Stage 1 choice by reward (Model-Free)
        delta_stage1 = r - q_stage1[a1] # Simple MF update using final reward
        q_stage1[a1] += learning_rate * delta_stage1
        
        # Store history
        prev_a1 = a1
        prev_reward = r

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```