Here are three new cognitive models exploring different mechanisms for learning and decision-making in the two-step task.

### Model 1: Hybrid Model-Based/Model-Free with Eligibility Traces
This model implements the classic hybrid theory where behavior is a mix of Model-Based (planning) and Model-Free (habitual) systems. It uses an eligibility trace parameter (`lambda`) to modulate how much the stage-2 outcome directly reinforces the stage-1 choice, effectively bridging the two stages in the model-free system.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Reinforcement Learning with Eligibility Traces.
    
    This model assumes the agent uses a weighted combination of:
    1. A Model-Based (MB) controller that plans using the transition matrix.
    2. A Model-Free (MF) controller that learns from direct experience (TD learning).
    
    The 'w' parameter controls the balance between these systems.
    The 'lambda_eligibility' parameter controls the strength of the eligibility trace,
    allowing the stage-2 reward to directly update stage-1 values in the MF system.

    Parameters:
    learning_rate: [0, 1] - Rate at which Q-values are updated.
    beta: [0, 10] - Inverse temperature for softmax choice.
    w: [0, 1] - Weighting parameter (0 = Pure MF, 1 = Pure MB).
    lambda_eligibility: [0, 1] - Eligibility trace decay (0 = TD(0), 1 = Monte Carlo).
    """
    learning_rate, beta, w, lambda_eligibility = model_parameters
    n_trials = len(action_1)
    
    # Fixed transition matrix as per task description:
    # A -> X (0.7), U -> Y (0.7) usually. 
    # State 0 is Planet X, State 1 is Planet Y.
    # Action 0 is Spaceship A, Action 1 is Spaceship U.
    # trans[0,0]=0.7 (A->X), trans[0,1]=0.3 (A->Y)
    # trans[1,0]=0.3 (U->X), trans[1,1]=0.7 (U->Y)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2) 
    q_stage2_mf = np.zeros((2, 2)) 
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        
        # --- Stage 1 Decision ---
        
        # Model-Based Value Calculation
        # Maximize over stage 2 options for each state
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        # Bellman equation using known transition structure
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Q-value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Transition ---
        state_idx = state[trial] # 0 or 1
        a1 = action_1[trial]
        
        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]
        
        # --- Learning ---
        
        # Stage 1 TD Error (SARSA style for stage 1 value)
        # Note: In standard TD(0), we update Q1 based on Q2.
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Stage 2 TD Error
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Eligibility Trace Update for Stage 1
        # The stage 1 choice is also reinforced by the stage 2 prediction error,
        # scaled by lambda. This allows the reward at the end to "reach back" to the start.
        q_stage1_mf[a1] += learning_rate * lambda_eligibility * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Separate Learning Rates for Positive and Negative Prediction Errors
This model investigates valence-dependent learning. It posits that the participant might learn differently from "good news" (positive prediction errors, e.g., getting gold) versus "bad news" (negative prediction errors, e.g., getting nothing or less than expected). This asymmetry is a common finding in psychiatric computational modeling.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Q-Learning with Asymmetric Learning Rates.
    
    This model splits the learning rate into two:
    1. alpha_pos: Used when the prediction error is positive (better than expected).
    2. alpha_neg: Used when the prediction error is negative (worse than expected).
    
    This captures potential biases like optimism (ignoring negative errors) or 
    pessimism/anxiety (overweighting negative errors).

    Parameters:
    alpha_pos: [0, 1] - Learning rate for positive prediction errors.
    alpha_neg: [0, 1] - Learning rate for negative prediction errors.
    beta: [0, 10] - Inverse temperature for softmax choice.
    """
    alpha_pos, alpha_neg, beta = model_parameters
    n_trials = len(action_1)

    q_stage1_mf = np.zeros(2) 
    q_stage2_mf = np.zeros((2, 2)) 
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):

        # --- Stage 1 Decision ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        state_idx = state[trial]
        a1 = action_1[trial]

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]

        # --- Learning ---
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        if delta_stage1 > 0:
            q_stage1_mf[a1] += alpha_pos * delta_stage1
        else:
            q_stage1_mf[a1] += alpha_neg * delta_stage1

        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        if delta_stage2 > 0:
            q_stage2_mf[state_idx, a2] += alpha_pos * delta_stage2
        else:
            q_stage2_mf[state_idx, a2] += alpha_neg * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Model-Based with Forgetting
This model assumes the participant uses a purely Model-Based strategy (planning forward using the transition structure) but has imperfect memory. The Q-values for the second stage decay toward a neutral value (0.5) on every trial. This "forgetting" mechanism tests if the participant struggles to maintain value estimates over time, a feature relevant to working memory limitations.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Based Learning with Forgetting (Decay).
    
    This model assumes the agent relies entirely on the transition matrix to compute
    stage 1 values. However, the stage 2 values (alien values) are subject to 
    passive decay (forgetting) on every trial.
    
    Unchosen options decay toward the initial/neutral value (0.5).

    Parameters:
    learning_rate: [0, 1] - Rate at which chosen Q-values are updated.
    beta: [0, 10] - Inverse temperature for softmax choice.
    decay_rate: [0, 1] - Rate at which all Q-values decay to 0.5 (1 = instant decay, 0 = no decay).
    """
    learning_rate, beta, decay_rate = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Initialize Q-values at 0.5 (neutral expectation for probability 0-1)
    q_stage2_mf = np.full((2, 2), 0.5) 
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):

        # --- Stage 1 Decision (Pure Model-Based) ---
        # Calculate MB values on the fly based on current Stage 2 estimates
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        state_idx = state[trial]
        a1 = action_1[trial]

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]

        # --- Learning ---
        
        # 1. Decay ALL stage 2 values toward 0.5 before update
        # This represents the passage of time/memory loss
        q_stage2_mf = q_stage2_mf * (1 - decay_rate) + 0.5 * decay_rate
        
        # 2. Update the specific chosen alien using prediction error
        # Note: We do not update a separate q_stage1_mf because this is a pure MB agent
        # derived from q_stage2_mf
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```