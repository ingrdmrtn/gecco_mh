Here are 3 new cognitive models based on the provided participant data and task structure.

### Model 1: Hybrid Learner with Eligibility Traces (TD($\lambda$))
This model introduces an eligibility trace parameter ($\lambda$). Instead of just updating the first stage based on the second stage's value (TD(0)), this model allows the final reward to directly influence the first-stage Q-values to a degree determined by lambda. This bridges the gap between pure model-free TD learning and Monte Carlo updates.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner with Eligibility Traces (TD(lambda)).
    
    Hypothesis: The participant updates first-stage values not just based on the 
    second-stage state value, but also partly based on the final reward received, 
    mediated by an eligibility trace parameter lambda.

    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    lam: [0, 1] - Eligibility trace decay parameter (0 = TD(0), 1 = Monte Carlo).
    """
    learning_rate, beta, lam = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Model-free Q-values
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial] # 0 or 1 (Planets X or Y)

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning ---
        # Prediction Error 1: Difference between Stage 2 value and Stage 1 estimate
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        # Prediction Error 2: Difference between Reward and Stage 2 estimate
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Update Stage 2 Q-values (standard TD)
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Update Stage 1 Q-values with Eligibility Trace
        # It gets the immediate TD error (delta1) plus a weighted portion of the second error (delta2)
        q_stage1_mf[action_1[trial]] += learning_rate * (delta_stage1 + lam * delta_stage2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Model-Based / Model-Free Hybrid with Separate Learning Rates
This model implements the classic "Daw et al. (2011)" hybrid structure but specifically tests the hypothesis that the participant learns at different speeds for the two stages. For example, learning about the aliens (Stage 2) might happen faster or slower than learning about the spaceships (Stage 1).

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Separate Learning Rates and Mixing Weight.
    
    Hypothesis: The participant uses a weighted combination of Model-Based (planning) 
    and Model-Free (habitual) values, but updates the Q-values for Stage 1 and 
    Stage 2 with distinct learning rates.

    Bounds:
    lr_stage1: [0, 1] - Learning rate for first stage transitions.
    lr_stage2: [0, 1] - Learning rate for second stage rewards.
    beta: [0, 10]
    w: [0, 1] - Weighting parameter (0 = pure MF, 1 = pure MB).
    """
    lr_stage1, lr_stage2, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # Used for both MF and MB calculations

    for trial in range(n_trials):

        # --- Stage 1 Choice (Hybrid) ---
        # 1. Calculate Model-Based Value
        # Max value of each state in stage 2
        max_q_stage2 = np.max(q_stage2_mf, axis=1) 
        # Bellman equation: Transition Matrix * Max Next State Values
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Combine MB and MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        # Update Stage 1 MF (using lr_stage1)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr_stage1 * delta_stage1
        
        # Update Stage 2 MF/MB-base (using lr_stage2)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr_stage2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Pure Model-Based with Forgetting (Decay)
This model assumes the participant is purely Model-Based (planning based on the transition structure) but has imperfect memory. Values for unchosen aliens "decay" back to a neutral prior (0.5) over time. This captures the idea that if you don't visit an alien for a while, you become less certain about its payout, effectively forgetting what you learned.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Based Learner with Memory Decay.
    
    Hypothesis: The participant relies entirely on the transition structure (Model-Based),
    but value estimates for unvisited states decay toward a neutral value (0.5) 
    on every trial.

    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    decay_rate: [0, 1] - Rate at which unchosen values revert to 0.5.
    """
    learning_rate, beta, decay_rate = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Initialize values at 0.5 (neutral expectation)
    q_stage2_values = np.ones((2, 2)) * 0.5

    for trial in range(n_trials):

        # --- Stage 1 Choice (Pure Model-Based) ---
        max_q_stage2 = np.max(q_stage2_values, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_values[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning and Decay ---
        # 1. Update the chosen alien
        chosen_alien = action_2[trial]
        pe = reward[trial] - q_stage2_values[state_idx, chosen_alien]
        q_stage2_values[state_idx, chosen_alien] += learning_rate * pe
        
        # 2. Decay unchosen aliens toward 0.5
        for s in range(2):
            for a in range(2):
                if not (s == state_idx and a == chosen_alien):
                    q_stage2_values[s, a] += decay_rate * (0.5 - q_stage2_values[s, a])
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```