Here are three new cognitive models exploring different mechanisms of learning and decision-making in the two-step task.

### Model 1: Hybrid Model with Separate Learning Rates
This model implements the classic hybrid reinforcement learning theory often applied to this task. It assumes the agent computes both a Model-Free (MF) value (based on direct reinforcement history) and a Model-Based (MB) value (based on the transition structure) for the first-stage choice. Crucially, this version allows for distinct learning rates for the first and second stages, reflecting the hypothesis that learning dynamics might differ between the abstract spaceship choice and the concrete alien reward learning.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MF/MB Model with Separate Learning Rates.
    
    Combines Model-Free (MF) and Model-Based (MB) values for the first-stage choice using a weighting parameter 'w'.
    Uses separate learning rates for stage 1 (spaceship choice) and stage 2 (alien choice).
    
    Parameters:
    - lr_1: [0, 1] Learning rate for stage 1 MF values.
    - lr_2: [0, 1] Learning rate for stage 2 MF values.
    - beta: [0, 10] Inverse temperature for softmax choice.
    - w: [0, 1] Weighting parameter (0 = Pure MF, 1 = Pure MB).
    """
    lr_1, lr_2, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2) # MF value for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Value for stage 2 (aliens)

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        # 1. Calculate Model-Based Value: Transition * Max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Integrate MF and MB values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # 3. Softmax
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial] # Actual planet arrived at

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # Stage 1 MF update (SARSA-like logic using stage 2 value)
        # Note: In standard hybrid models, stage 1 MF is often updated by TD(1) or TD(0). 
        # Here we use a direct TD(0) update from the value of the state reached.
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr_1 * delta_stage1
        
        # Stage 2 MF update (Reward prediction error)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr_2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Perseveration Model (Pure Model-Based + Stickiness)
This model assumes the participant is fundamentally Model-Based (planning based on the transition structure) but exhibits "choice stickiness" or perseveration. Regardless of reward history, humans often tend to repeat their previous motor action. This model adds a `perseveration` parameter that biases the first-stage choice probability toward the spaceship chosen on the immediately preceding trial.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Based Reinforcement Learning with Choice Perseveration.
    
    The agent calculates values based on the transition matrix (Model-Based).
    However, the decision is biased by a tendency to repeat the previous stage-1 action
    (perseveration), regardless of the outcome.
    
    Parameters:
    - learning_rate: [0, 1] Rate at which second-stage Q-values are updated.
    - beta: [0, 10] Inverse temperature for softmax choice.
    - perseveration: [0, 5] Bonus added to the log-odds of repeating the previous stage-1 choice.
    """
    learning_rate, beta, perseveration = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage2_mf = np.zeros((2, 2))
    
    # Store previous action for perseveration logic (initialized to -1 or None)
    last_action_1 = -1

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Calculate logits (values before exponentiation)
        logits = beta * q_stage1_mb
        
        # Add perseveration bonus if applicable
        if last_action_1 != -1:
            logits[last_action_1] += perseveration
            
        exp_q1 = np.exp(logits)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # Only Stage 2 is learned via RPE in a Pure MB model
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Update history
        last_action_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Asymmetric Learning Rate Model (Pos/Neg Bias)
This model investigates valence-dependent learning. It posits that the participant might update their expectations differently depending on whether the outcome was better than expected (positive prediction error) or worse than expected (negative prediction error). This is implemented by splitting the learning rate into `alpha_pos` and `alpha_neg` for the second-stage updates, while keeping a pure model-free architecture for simplicity in the first stage.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free TD(1) with Asymmetric Learning Rates.
    
    The agent learns via direct reinforcement (Model-Free) but updates values differently
    depending on whether the prediction error is positive (better than expected) or 
    negative (worse than expected).
    
    Parameters:
    - alpha_pos: [0, 1] Learning rate for positive prediction errors.
    - alpha_neg: [0, 1] Learning rate for negative prediction errors.
    - beta: [0, 10] Inverse temperature.
    - lambda_decay: [0, 1] Eligibility trace decay (TD-lambda logic) connecting stage 2 reward to stage 1 choice.
    """
    alpha_pos, alpha_neg, beta, lambda_decay = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        
        # 1. Prediction Error at Stage 2 (Reward - Expectation)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # 2. Select Learning Rate based on sign of delta
        lr_eff = alpha_pos if delta_stage2 > 0 else alpha_neg
        
        # 3. Update Stage 2 Value
        q_stage2_mf[state_idx, action_2[trial]] += lr_eff * delta_stage2
        
        # 4. Update Stage 1 Value via Eligibility Trace
        # In TD(1) or TD(lambda), the stage 1 value is updated by the stage 2 RPE directly
        # scaled by lambda.
        q_stage1_mf[action_1[trial]] += lr_eff * lambda_decay * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```