Here are 3 new cognitive models based on the two-step task. These models explore different mechanisms for learning and decision-making, specifically focusing on perseveration, separate learning rates for positive/negative outcomes, and a hybrid model-based/model-free architecture with distinct stage-specific learning.

### Model 1: Hybrid Model with Perseveration
This model combines Model-Based (MB) and Model-Free (MF) reinforcement learning strategies but adds a "perseveration" parameter. This parameter captures the tendency to repeat the previous Stage 1 choice regardless of the outcome, a common behavioral tick in decision-making tasks.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free RL with Perseveration.
    
    This model assumes the agent uses a weighted combination of model-based
    planning (using the transition matrix) and model-free learning (TD learning).
    It also includes a perseveration bonus, where the agent is more likely to 
    repeat their last choice at stage 1.

    Parameters:
    learning_rate: [0,1] - Rate at which Q-values are updated.
    beta: [0,10] - Inverse temperature for softmax choice.
    w: [0,1] - Weighting parameter (0 = Pure MF, 1 = Pure MB).
    perseveration: [0,5] - Bonus added to the previous stage 1 action (stickiness).
    """
    learning_rate, beta, w, perseveration = model_parameters
    n_trials = len(action_1)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Q-values
    q_stage1_mf = np.zeros(2)       # Model-free values for Stage 1
    q_stage2_mf = np.zeros((2, 2))  # Values for Stage 2 (Aliens)
    
    last_action_1 = -1 # Initialize with an impossible action

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        # 1. Model-Based Value: Transition * Max(Stage 2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Combined Value: w * MB + (1-w) * MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # 3. Add Perseveration Bonus
        if last_action_1 != -1:
            q_net[last_action_1] += perseveration
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial] # Planet reached

        # --- Stage 2 Policy ---
        q_values_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * q_values_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        # Update Stage 1 MF (TD(0))
        # Note: In standard hybrid models, Stage 1 MF is often updated via TD(1) or eligibility traces.
        # Here we use a direct update towards the value of the chosen stage 2 state.
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Update Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        last_action_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Asymmetric Learning Rates (Positive vs Negative)
This model is purely model-free (TD learning) but distinguishes between learning from positive prediction errors (rewards better than expected) and negative prediction errors (rewards worse than expected). This captures the phenomenon where agents might learn faster from gains than losses, or vice versa.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free RL with Asymmetric Learning Rates.
    
    This model relies solely on Temporal Difference learning but uses two 
    different learning rates: one for positive prediction errors (alpha_pos) 
    and one for negative prediction errors (alpha_neg).

    Parameters:
    alpha_pos: [0,1] - Learning rate for positive prediction errors (delta > 0).
    alpha_neg: [0,1] - Learning rate for negative prediction errors (delta < 0).
    beta: [0,10] - Inverse temperature.
    lambda_eligibility: [0,1] - Eligibility trace decay (how much stage 2 outcome affects stage 1).
    """
    alpha_pos, alpha_neg, beta, lambda_eligibility = model_parameters
    n_trials = len(action_1)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        chosen_a1 = action_1[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        chosen_a2 = action_2[trial]

        # --- Updates ---
        
        # 1. Prediction Error Stage 2 (Reward - Q_stage2)
        delta2 = reward[trial] - q_stage2[state_idx, chosen_a2]
        
        # Update Stage 2 Q-value
        lr2 = alpha_pos if delta2 > 0 else alpha_neg
        q_stage2[state_idx, chosen_a2] += lr2 * delta2
        
        # 2. Prediction Error Stage 1 (Q_stage2 - Q_stage1)
        # We use the value of the state we actually landed in
        delta1 = q_stage2[state_idx, chosen_a2] - q_stage1[chosen_a1]
        
        # Update Stage 1 Q-value (TD(0) part)
        lr1 = alpha_pos if delta1 > 0 else alpha_neg
        q_stage1[chosen_a1] += lr1 * delta1
        
        # 3. Eligibility Trace Update (TD(1) part)
        # The stage 2 RPE also updates stage 1, scaled by lambda
        q_stage1[chosen_a1] += lambda_eligibility * lr2 * delta2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Decoupled Stage 1 and Stage 2 Learning (Model-Based)
This model assumes a Model-Based architecture but posits that the learning rate for the second stage (learning alien values) is distinct from the learning rate used to update the first stage values (specifically, how quickly the computed MB values update or drift). However, to fit the constraints and logic of MB, we interpret this as having a specific learning rate for the aliens (`lr_aliens`) and a separate decay or "forgetting" rate for the unchosen options, or simply a distinct parameter configuration focused on the second stage value estimation.

Here, we implement a version where the focus is on the `learning_rate` for the aliens, but we introduce a `forgetting_rate` parameter. This implies that while we learn about the chosen alien, the value of the unchosen alien slowly decays back to a neutral prior (0 or 0.5), representing memory loss or uncertainty increase over time.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Based RL with Forgetting.
    
    This is a model-based learner that calculates Stage 1 values via the 
    transition matrix. However, it includes a 'forgetting' mechanism for Stage 2.
    When an alien is NOT chosen, its Q-value decays towards 0. This captures 
    the difficulty of maintaining precise value estimates for unvisited states.

    Parameters:
    learning_rate: [0,1] - Update rate for the chosen alien.
    forgetting_rate: [0,1] - Decay rate for the unchosen alien's value.
    beta_1: [0,10] - Inverse temperature for Stage 1.
    beta_2: [0,10] - Inverse temperature for Stage 2.
    """
    learning_rate, forgetting_rate, beta_1, beta_2 = model_parameters
    n_trials = len(action_1)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage2_mf = np.zeros((2, 2)) 

    for trial in range(n_trials):
        
        # --- Stage 1 Policy (Model Based) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2

        exp_q1 = np.exp(beta_1 * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        q_values_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta_2 * q_values_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        chosen_alien = action_2[trial]
        unchosen_alien = 1 - chosen_alien
        
        # Update chosen alien
        delta = reward[trial] - q_stage2_mf[state_idx, chosen_alien]
        q_stage2_mf[state_idx, chosen_alien] += learning_rate * delta
        
        # Decay unchosen alien (Forgetting)
        # Value decays towards 0
        q_stage2_mf[state_idx, unchosen_alien] *= (1 - forgetting_rate)
        
        # Note: We also decay the aliens on the OTHER planet since we didn't visit them
        other_planet = 1 - state_idx
        q_stage2_mf[other_planet, :] *= (1 - forgetting_rate)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```