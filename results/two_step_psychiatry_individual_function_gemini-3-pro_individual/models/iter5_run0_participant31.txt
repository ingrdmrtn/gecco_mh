Here are 3 new cognitive models as Python functions, designed to explore mechanisms distinct from the previous attempts listed in the feedback.

### Model 1: Hybrid Model with Memory Decay
This model introduces a `decay_rate` parameter. In standard Q-learning, values persist indefinitely until updated. However, biological memory fades. This model posits that the value of *unchosen* options (and unvisited states) slowly decays toward zero (or a neutral point) on every trial, representing forgetfulness or a "use-it-or-lose-it" heuristic.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Memory Decay:
    Standard Hybrid (MB/MF) model, but Q-values for unchosen options 
    decay over time, simulating forgetting.
    
    Parameters:
    learning_rate: [0, 1] Learning rate for Q-values.
    beta: [0, 10] Inverse temperature (softness of choice).
    w: [0, 1] Weight mixing MB (1) and MF (0).
    decay_rate: [0, 1] Rate at which unchosen Q-values decay toward 0.
    """
    learning_rate, beta, w, decay_rate = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Update chosen Stage 1
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Decay unchosen Stage 1
        unchosen_1 = 1 - action_1[trial]
        q_stage1_mf[unchosen_1] *= (1 - decay_rate)

        # Update chosen Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Decay unchosen Stage 2 (in current state)
        unchosen_2 = 1 - action_2[trial]
        q_stage2_mf[state_idx, unchosen_2] *= (1 - decay_rate)
        
        # Decay Stage 2 values in the unvisited state (both options decay)
        unvisited_state = 1 - state_idx
        q_stage2_mf[unvisited_state, :] *= (1 - decay_rate)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model with Subjective Transition Beliefs
Standard models assume the participant knows the transition matrix is exactly `[[0.7, 0.3], [0.3, 0.7]]`. This model replaces the fixed matrix with a parameterized belief `subj_trans_prob`. This allows the model to capture if a participant believes the transitions are more deterministic (e.g., 0.9/0.1) or more random (e.g., 0.55/0.45) than reality, which heavily impacts the Model-Based value calculation.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Subjective Transition Beliefs:
    The participant estimates the Model-Based values using a subjective 
    probability for the 'common' transition, rather than the objective 0.7.
    
    Parameters:
    learning_rate: [0, 1] Learning rate for Q-values.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Weight mixing MB (1) and MF (0).
    subj_trans_prob: [0, 1] Subjective probability of the common transition (A->X, U->Y).
    """
    learning_rate, beta, w, subj_trans_prob = model_parameters
    n_trials = len(action_1)
    
    # Construct subjective transition matrix based on parameter
    # Row 0: Space A -> [Prob X, Prob Y]
    # Row 1: Space U -> [Prob X, Prob Y]
    # Assuming symmetry in belief (A->X is same prob as U->Y)
    p = subj_trans_prob
    transition_matrix = np.array([[p, 1-p], [1-p, p]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model with Reward-Dependent Perseverance
The "Best Model" so far used a single `perseverance` parameter. This model refines that mechanism by splitting perseverance into two separate parameters: `pers_win` (stickiness after a reward) and `pers_loss` (stickiness after no reward). This captures "Win-Stay, Lose-Shift" dynamics explicitly combined with the Hybrid RL machinery. It tests if the participant's habit of repeating a choice is contingent on the previous outcome.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Reward-Dependent Perseverance:
    Distinguishes between 'stickiness' after a rewarded trial vs 
    'stickiness' after an unrewarded trial.
    
    Parameters:
    learning_rate: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Weight mixing MB (1) and MF (0).
    pers_win: [0, 5] Perseverance bonus applied if the previous trial was rewarded.
    pers_loss: [0, 5] Perseverance bonus applied if the previous trial was unrewarded.
    """
    learning_rate, beta, w, pers_win, pers_loss = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_choice = -1
    last_reward_val = 0

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Apply reward-dependent perseverance
        q_decision = q_net.copy()
        if last_choice != -1:
            if last_reward_val > 0:
                q_decision[last_choice] += pers_win
            else:
                q_decision[last_choice] += pers_loss

        exp_q1 = np.exp(beta * q_decision)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Store for next trial
        last_choice = action_1[trial]
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        last_reward_val = reward[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```