Here are three new cognitive models for the two-step decision task, implemented as Python functions.

### Model 1: Subjective Transition Model-Based Learner
This model assumes the participant relies on a Model-Based strategy (planning) but may have an incorrect or subjective belief about the spaceship transition probabilities, rather than the true 0.7/0.3 structure.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Subjective Transition Model-Based Learner.
    
    This model assumes the participant is a Model-Based learner who calculates 
    Stage 1 values based on the expected values of Stage 2. However, unlike the 
    standard model which assumes a fixed 0.7 transition probability, this model 
    treats the participant's belief about the transition probability as a free parameter.
    They do not learn Stage 1 Q-values from experience (Pure MB at Stage 1).
    
    Parameters:
    - learning_rate: [0, 1] Update rate for Stage 2 Q-values (Aliens).
    - beta: [0, 10] Inverse temperature for softmax choices.
    - transition_belief: [0, 1] The participant's subjective probability that Spaceship A goes to Planet X 
                                (and U to Y). 0.5 implies random, >0.5 implies correct structure, <0.5 implies reversal.
    """
    learning_rate, beta, transition_belief = model_parameters
    n_trials = len(action_1)
  
    # We construct the subjective matrix based on the parameter
    # Row 0: Spaceship A (0) -> [Prob(Planet 0), Prob(Planet 1)]
    # Row 1: Spaceship U (1) -> [Prob(Planet 0), Prob(Planet 1)]
    subj_transition_matrix = np.array([
        [transition_belief, 1.0 - transition_belief], 
        [1.0 - transition_belief, transition_belief]
    ])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2) # Not used for choice, but kept for template structure consistency
    q_stage2_mf = np.zeros((2, 2)) # Values for Aliens

    for trial in range(n_trials):

        # policy for the first choice
        # Pure Model-Based calculation using subjective transitions
        max_q_stage2 = np.max(q_stage2_mf, axis=1) # Value of best alien on each planet
        q_stage1_subjective = subj_transition_matrix @ max_q_stage2
        
        exp_q1 = np.exp(beta * q_stage1_subjective)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Update Stage 2 values (MF)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Note: Stage 1 MF values are not updated or used in this Pure Subjective MB model.

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Model-Free Q-Learning with Biased Initialization
This model assumes the participant starts the task with a strong initial preference (bias) for one spaceship over the other, which may explain early perseveration before learning takes over.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Q-Learning with Biased Initialization.
    
    This model assumes the participant starts the task with a pre-existing preference 
    (bias) for one spaceship over the other, rather than starting with neutral values. 
    This bias decays or is overwritten as they learn from feedback.
    
    Parameters:
    - learning_rate: [0, 1] Update rate for Q-values.
    - beta: [0, 10] Inverse temperature for softmax choices.
    - initial_bias: [-1, 1] Initial value bias added to Spaceship A (0) and subtracted from Spaceship U (1).
                            Positive values favor A initially.
    """
    learning_rate, beta, initial_bias = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Initialize Stage 1 values with bias
    # Assuming neutral is 0.0, we offset the starting values.
    q_stage1_mf = np.array([0.0 + initial_bias, 0.0 - initial_bias])
    
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        # Pure MF with initialization bias
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Model-Free Q-Learning with Uncertainty-Driven Exploration
This model posits that the participant is motivated to explore options they haven't chosen recently. It adds an "exploration bonus" to the Q-value of a spaceship proportional to the time since it was last selected.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Q-Learning with Uncertainty-Driven Exploration.
    
    This model adds an exploration bonus to options in Stage 1 based on how long 
    it has been since they were last chosen. This encourages the participant to 
    sample the unchosen spaceship to reduce uncertainty.
    
    Parameters:
    - learning_rate: [0, 1] Update rate for Q-values.
    - beta: [0, 10] Inverse temperature for softmax choices.
    - exploration_bonus: [0, 1] Bonus added to the Q-value per trial since last selection.
    """
    learning_rate, beta, exploration_bonus = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track time since last choice for Stage 1 options
    time_since_choice = np.zeros(2)

    for trial in range(n_trials):

        # policy for the first choice
        # Add exploration bonus to the Q-values based on time since last choice
        q_stage1_augmented = q_stage1_mf + (exploration_bonus * time_since_choice)
        
        exp_q1 = np.exp(beta * q_stage1_augmented)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Update counters
        time_since_choice += 1
        time_since_choice[action_1[trial]] = 0 # Reset counter for the chosen option
  
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```