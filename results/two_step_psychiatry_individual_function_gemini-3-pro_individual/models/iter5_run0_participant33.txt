Here are three new cognitive models based on the two-step task structure. These models explore different mechanisms like eligibility traces, separate learning rates for different stages, and distinct sensitivity to rewards versus non-rewards.

### Model 1: TD(1) / Eligibility Trace Model
This model replaces the standard TD(0) update with a TD(1) mechanism (or simplified eligibility trace). Instead of just updating the first stage based on the second stage's value estimate, the reward received at the very end directly reinforces the first-stage choice. This creates a stronger link between the initial action and the final outcome, bypassing the intermediate value estimate to some degree.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    TD(1) / Eligibility Trace Model.
    
    This model assumes that the reward received at the second stage directly 
    reinforces the first-stage choice, rather than just updating the second-stage 
    value which then propagates back. It uses a 'lambda' parameter (here implicitly 1 
    for the direct reward connection) to bridge the delay.
    
    Parameters:
    - learning_rate: Rate at which Q-values update [0, 1].
    - beta: Inverse temperature for softmax choice [0, 10].
    - lambda_trace: Decay factor for eligibility traces [0, 1].
      If 0, it behaves like TD(0) (standard Q-learning). 
      If 1, the full reward error propagates back to stage 1.
    """
    learning_rate, beta, lambda_trace = model_parameters
    n_trials = len(action_1)
  
    # Standard transition matrix (unused in pure MF, but good for reference)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # Planets x Aliens

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        # Pure Model-Free policy for stage 1
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        
        # Calculate Stage 2 Prediction Error (RPE)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Calculate Stage 1 Prediction Error (based on stage 2 value)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]

        # Update Stage 2 value
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Update Stage 1 value:
        # Standard TD(0) part: learning_rate * delta_stage1
        # Eligibility trace part: learning_rate * lambda_trace * delta_stage2
        # This allows the final reward prediction error to directly affect stage 1.
        q_stage1_mf[action_1[trial]] += learning_rate * (delta_stage1 + lambda_trace * delta_stage2)
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Separate Learning Rates for Stage 1 and Stage 2
Standard models often assume a single learning rate for the whole task. However, the cognitive demands of Stage 1 (choosing a spaceship with probabilistic transitions) and Stage 2 (choosing an alien with direct rewards) are different. This model allows the participant to learn the values of the spaceships at a different speed than the values of the aliens.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Dual Learning Rate Model-Free Learner.
    
    This model posits that learning happens at different speeds for the two stages.
    Stage 1 involves abstract transition probabilities, while Stage 2 involves
    concrete rewards.
    
    Parameters:
    - alpha1: Learning rate for the first stage (spaceships) [0, 1].
    - alpha2: Learning rate for the second stage (aliens) [0, 1].
    - beta: Inverse temperature for softmax choice [0, 10].
    """
    alpha1, alpha2, beta = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        
        # Stage 1 update: Driven by the value of the state we arrived at
        # We use the value of the CHOSEN option in stage 2 as the target for stage 1
        # (SARSA-like update for stage 1 based on stage 2 Q-value)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += alpha1 * delta_stage1
        
        # Stage 2 update: Driven by the reward
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += alpha2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Asymmetric Learning Rates (Positive vs. Negative RPE)
This model investigates whether the participant learns differently from success (getting a coin) versus failure (getting nothing). It splits the learning rate into `alpha_pos` (for positive prediction errors) and `alpha_neg` (for negative prediction errors). This is often observed in clinical populations where sensitivity to reward or punishment may be unbalanced.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Asymmetric Learning Rate Model (Pos/Neg RPE).
    
    This model allows for different learning rates depending on whether the 
    prediction error is positive (better than expected) or negative (worse than expected).
    This applies to the second stage updates where concrete rewards occur.
    
    Parameters:
    - alpha_pos: Learning rate for positive prediction errors [0, 1].
    - alpha_neg: Learning rate for negative prediction errors [0, 1].
    - beta: Inverse temperature for softmax choice [0, 10].
    """
    alpha_pos, alpha_neg, beta = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        
        # Stage 2 Prediction Error
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Apply asymmetric learning rates for Stage 2
        if delta_stage2 > 0:
            q_stage2_mf[state_idx, action_2[trial]] += alpha_pos * delta_stage2
        else:
            q_stage2_mf[state_idx, action_2[trial]] += alpha_neg * delta_stage2

        # Stage 1 Prediction Error
        # For simplicity, we use the average of alpha_pos and alpha_neg for stage 1, 
        # or just alpha_pos since stage 1 updates are usually driven by value differences not raw rewards.
        # Here we apply the same logic: is the transition better or worse than expected?
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        if delta_stage1 > 0:
             q_stage1_mf[action_1[trial]] += alpha_pos * delta_stage1
        else:
             q_stage1_mf[action_1[trial]] += alpha_neg * delta_stage1
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```