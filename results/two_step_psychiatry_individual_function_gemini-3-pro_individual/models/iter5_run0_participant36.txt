Here are three new cognitive models based on the provided template and task description. These models explore mechanisms distinct from the standard Model-Based/Hybrid configurations, focusing on eligibility traces, choice perseveration (stickiness), and asymmetric learning rates.

### Model 1: TD($\lambda$) Learner (Eligibility Traces)
This model implements a Model-Free learner with an eligibility trace parameter ($\lambda$). Unlike a simple TD(0) learner which only updates the first stage based on the second stage's value, this model allows the final reward to directly influence the value of the first-stage choice. This captures the intuition that the outcome "ripples back" to the start state.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    TD(lambda) Model-Free Learner.
    Uses an eligibility trace (lambda) to allow the final reward to directly
    influence the update of the Stage 1 choice, bridging the gap between
    the two stages.

    Parameters:
    learning_rate: [0,1] Weight of prediction error updates.
    beta: [0,10] Inverse temperature for softmax.
    lambda_trace: [0,1] Eligibility trace decay. 0 = TD(0) (SARSA), 1 = Monte Carlo.
    """
    learning_rate, beta, lambda_trace = model_parameters
    n_trials = len(action_1)
  
    # transition_matrix is not used for pure MF, but kept for template consistency
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]]) 
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)      # Values for Spaceships A(0) and U(1)
    q_stage2_mf = np.zeros((2, 2)) # Values for Aliens (Planet 0: 0,1; Planet 1: 0,1)

    for trial in range(n_trials):

        # policy for the first choice
        # Pure Model-Free decision making for Stage 1
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial] # Planet arrived at

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Calculate Prediction Errors
        # RPE 1: Difference between value of Stage 2 state and Stage 1 choice
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        # RPE 2: Difference between Reward and Stage 2 choice value
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # ACTION VALUE UPDATING FOR CHOICE 1
        # Update Stage 1 using both immediate RPE and scaled final RPE (eligibility trace)
        q_stage1_mf[action_1[trial]] += learning_rate * (delta_stage1 + lambda_trace * delta_stage2)

        # ACTION VALUE UPDATING FOR CHOICE 2
        # Standard TD update for the second stage
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Model-Free Learner with Choice Stickiness
This model adds a "stickiness" or perseveration parameter. In decision-making tasks, participants often have a tendency to repeat their previous choice regardless of reward history (motor perseveration) or switch less often than optimal. This model accounts for that bias in the first stage.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Learner with Choice Stickiness.
    Biases the decision at Stage 1 towards repeating the previously chosen
    spaceship, regardless of the value associated with it.

    Parameters:
    learning_rate: [0,1] Rate of value updating.
    beta: [0,10] Inverse temperature for softmax.
    stickiness: [0,5] Bonus added to the log-odds of repeating the last Stage 1 choice.
    """
    learning_rate, beta, stickiness = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1 # Initialize with no previous action

    for trial in range(n_trials):

        # policy for the first choice
        # Calculate base logits from Q-values
        logits_1 = beta * q_stage1_mf
        
        # Add stickiness bonus if a previous action exists
        if last_action_1 != -1:
            logits_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        # ACTION VALUE UPDATING FOR CHOICE 1
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # ACTION VALUE UPDATING FOR CHOICE 2
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Update tracker for next trial
        last_action_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Asymmetric Learning Rate Model
This model hypothesizes that the participant learns differently from positive prediction errors (unexpected rewards) versus negative prediction errors (unexpected omissions of reward). This asymmetry (optimism vs. pessimism) is a common trait in reinforcement learning phenotypes.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Asymmetric Learning Rate Model.
    Uses separate learning rates for positive prediction errors (better than expected)
    and negative prediction errors (worse than expected).

    Parameters:
    lr_pos: [0,1] Learning rate for positive prediction errors (delta > 0).
    lr_neg: [0,1] Learning rate for negative prediction errors (delta < 0).
    beta: [0,10] Inverse temperature for softmax.
    """
    lr_pos, lr_neg, beta = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        # ACTION VALUE UPDATING FOR CHOICE 1
        # Use lr_pos if delta is positive, lr_neg otherwise
        effective_lr_1 = lr_pos if delta_stage1 > 0 else lr_neg
        q_stage1_mf[action_1[trial]] += effective_lr_1 * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # ACTION VALUE UPDATING FOR CHOICE 2
        # Use lr_pos if delta is positive, lr_neg otherwise
        effective_lr_2 = lr_pos if delta_stage2 > 0 else lr_neg
        q_stage2_mf[state_idx, action_2[trial]] += effective_lr_2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```