Here are three new cognitive models designed to capture different aspects of the participant's behavior, specifically focusing on mechanisms that might explain the high degree of repetition seen in the data (choosing 0, 0, 0 repeatedly).

### Model 1: Asymmetric Learning Rates (Positive/Negative) with Stage-Specific Updates
This model hypothesizes that the participant learns differently from positive outcomes (getting a coin) versus negative outcomes (getting nothing), and that this learning asymmetry might differ between the first stage (spaceship choice) and the second stage (alien choice). This is a classic "risk-sensitive" RL approach. Given the data shows a long string of successes followed by a failure, splitting learning rates by valence (pos/neg) is a strong candidate for explaining why behavior might persist or change.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Asymmetric Learning Rates (Positive vs Negative).
    This model allows for different update speeds depending on whether the 
    prediction error was positive (better than expected) or negative (worse than expected).
    
    Parameters:
    alpha_pos: [0, 1] - Learning rate for positive prediction errors.
    alpha_neg: [0, 1] - Learning rate for negative prediction errors.
    beta: [0, 10] - Inverse temperature for softmax choice.
    """
    alpha_pos, alpha_neg, beta = model_parameters
    n_trials = len(action_1)
  
    # Initialize Q-values
    # Stage 1: 2 choices (Spaceships A, U)
    q_stage1_mf = np.zeros(2) 
    # Stage 2: 2 states (Planets X, Y) x 2 choices (Aliens)
    q_stage2_mf = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial] # 0 or 1 (Planets)

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # TD Error Stage 1: Driven by the value of the state we landed in
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        # Apply asymmetric learning rate for Stage 1
        if delta_stage1 > 0:
            q_stage1_mf[action_1[trial]] += alpha_pos * delta_stage1
        else:
            q_stage1_mf[action_1[trial]] += alpha_neg * delta_stage1

        # TD Error Stage 2: Driven by the actual reward received
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Apply asymmetric learning rate for Stage 2
        if delta_stage2 > 0:
            q_stage2_mf[state_idx, action_2[trial]] += alpha_pos * delta_stage2
        else:
            q_stage2_mf[state_idx, action_2[trial]] += alpha_neg * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model with Separate Eligibility Traces for MB and MF
This model introduces a specific parameter `lambda_eligibility` that controls how much credit the first-stage action gets for the second-stage reward directly (Model-Free eligibility trace), combined with a standard Model-Based / Model-Free weighting `w`. The "tried so far" list includes eligibility traces but not in combination with the `w` parameter which arbitrates between MB and MF control. This model tests if the participant is a mixture of a planner (MB) and a habit-learner (MF) who assigns credit backward in time.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF with Eligibility Traces.
    Combines Model-Based planning (using transition matrix) and Model-Free learning 
    (using eligibility traces to update stage 1 Q-values based on stage 2 reward).
    
    Parameters:
    learning_rate: [0, 1] - Update rate for value estimation.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - Weighting parameter (0 = pure MF, 1 = pure MB).
    lambda_eligibility: [0, 1] - Decay factor for eligibility traces (credit assignment).
    """
    learning_rate, beta, w, lambda_eligibility = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition structure for Model-Based component
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):

        # --- Stage 1 Policy (Hybrid) ---
        # Model-Based Value: Expected value of next states weighted by transitions
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Q-value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # Stage 1 MF update (TD(0) part)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Stage 2 MF update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Eligibility Trace update: Update Stage 1 based on Stage 2 RPE
        # This connects the final reward back to the first choice directly
        q_stage1_mf[action_1[trial]] += learning_rate * lambda_eligibility * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Model-Free RL with "Win-Stay, Lose-Shift" Heuristic Bias
This model adds a heuristic bias parameter rather than a generic perseveration parameter. Instead of just "sticking" to the last choice regardless of outcome, this model implements a "Win-Stay, Lose-Shift" (WSLS) logic as a bias on top of standard Q-learning. If the previous trial was rewarded, the bias increases the probability of repeating the action. If it was unrewarded, the bias might decrease it (or not increase it). This is distinct from simple perseveration which is reward-agnostic.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free RL with Win-Stay, Lose-Shift (WSLS) Bias.
    Adds a bias to the Q-values based on the previous trial's outcome.
    If the previous trial was a 'Win' (reward=1), a bonus is added to the chosen action.
    If it was a 'Loss' (reward=0), the values rely purely on Q-learning.
    
    Parameters:
    learning_rate: [0, 1] - Update rate.
    beta: [0, 10] - Inverse temperature.
    wsls_bias: [0, 5] - Additional value added to the previous action if it was rewarded.
    """
    learning_rate, beta, wsls_bias = model_parameters
    n_trials = len(action_1)
  
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1
    last_reward = 0

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        # Calculate effective Q-values with WSLS bias
        q_stage1_effective = q_stage1_mf.copy()
        
        # If we have a history and the last trial was a win, boost that action
        if last_action_1 != -1 and last_reward == 1:
            q_stage1_effective[last_action_1] += wsls_bias
            
        exp_q1 = np.exp(beta * q_stage1_effective)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        # Note: WSLS usually applies most strongly to the first "high level" choice, 
        # but we use standard softmax here for stage 2.
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # Standard SARSA/Q-learning updates
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1

        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Store history for next trial's bias
        last_action_1 = action_1[trial]
        last_reward = reward[trial]
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```