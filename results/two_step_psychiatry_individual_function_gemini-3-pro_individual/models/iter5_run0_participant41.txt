Here are three new cognitive models that explore different mechanisms for learning and decision-making in the two-step task, distinct from the standard hybrid model provided in the feedback.

### Model 1: Perseveration + Model-Based Learning
This model modifies the standard hybrid approach by adding a "perseveration" parameter. Participants often have a tendency to repeat their previous choice regardless of reward (stickiness). This model tests if the participant's behavior is better explained by a mix of goal-directed (Model-Based) planning and a simple heuristic to repeat the last action, rather than a Model-Free habitual system.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Model-Based Learner with Choice Perseveration.
    
    This model assumes the participant is primarily Model-Based (planning using the transition matrix)
    but has a 'stickiness' or perseveration bias where they tend to repeat their last Stage 1 choice.
    It does not use a Stage 1 Model-Free component.
    
    Parameters:
    - learning_rate: [0, 1] Update rate for Stage 2 Q-values.
    - beta: [0, 10] Inverse temperature for softmax.
    - perseveration: [0, 5] Bonus added to the Q-value of the previously chosen action (0 = no stickiness).
    """
    learning_rate, beta, perseveration = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Only Stage 2 Q-values are learned directly from reward
    q_stage2_mf = np.zeros((2, 2)) 
    
    # Track the previous action for perseveration (initialize to -1 or None)
    last_action_1 = -1

    for trial in range(n_trials):

        # --- Stage 1 Policy (Model-Based + Perseveration) ---
        # Calculate Model-Based values: V(S') = max(Q(S', a'))
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        # Q_MB(S, a) = T(S, a, S') * V(S')
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Add perseveration bonus to the previously chosen action
        q_net = q_stage1_mb.copy()
        if last_action_1 != -1:
            q_net[last_action_1] += perseveration
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Update last action
        last_action_1 = action_1[trial]
        state_idx = state[trial]

        # --- Stage 2 Policy (Standard Softmax) ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        # No Stage 1 MF update here, as this model relies on MB + Stickiness for Stage 1.
        
        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Separate Learning Rates for Positive and Negative Prediction Errors
This model investigates asymmetry in learning. It hypothesizes that the participant might update their beliefs differently when they receive a reward (positive prediction error) versus when they get nothing (negative prediction error). This is often observed in clinical populations where sensitivity to reward or punishment differs. It uses a pure Model-Free strategy for simplicity to isolate the learning rate effect.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free Learner with Asymmetric Learning Rates.
    
    This model distinguishes between learning from positive outcomes (getting gold)
    and negative/neutral outcomes (getting nothing).
    
    Parameters:
    - alpha_pos: [0, 1] Learning rate for positive prediction errors (reward > expectation).
    - alpha_neg: [0, 1] Learning rate for negative prediction errors (reward < expectation).
    - beta: [0, 10] Inverse temperature for softmax.
    - lambda_decay: [0, 1] Eligibility trace decay (allow Stage 2 reward to update Stage 1).
    """
    alpha_pos, alpha_neg, beta, lambda_decay = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        a1 = action_1[trial]
        a2 = action_2[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        
        # Stage 1 PE: Difference between Stage 2 value and Stage 1 value
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        
        # Choose alpha based on sign of PE
        lr_1 = alpha_pos if delta_stage1 > 0 else alpha_neg
        q_stage1_mf[a1] += lr_1 * delta_stage1
        
        # Stage 2 PE: Difference between reward and Stage 2 value
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        
        # Choose alpha based on sign of PE
        lr_2 = alpha_pos if delta_stage2 > 0 else alpha_neg
        q_stage2_mf[state_idx, a2] += lr_2 * delta_stage2
        
        # Eligibility Trace: Update Stage 1 again based on Stage 2 outcome
        # This connects the final reward back to the first choice
        q_stage1_mf[a1] += lambda_decay * lr_2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: "Win-Stay, Lose-Shift" (WSLS) Heuristic with Noise
This model represents a simpler, heuristic-based strategy often used as a baseline. Instead of maintaining Q-values over long horizons, the agent simply repeats a choice if it was rewarded (Win-Stay) and switches if unrewarded (Lose-Shift). This version applies WSLS logic to *both* stages but includes a noise parameter to capture stochasticity.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Win-Stay, Lose-Shift (WSLS) Heuristic Model.
    
    Instead of reinforcement learning, this model assumes the participant follows a simple rule:
    - If rewarded: Repeat the same choices next time (Win-Stay).
    - If unrewarded: Switch choices (Lose-Shift).
    
    This is applied locally. For Stage 1, "Win" depends on the final reward.
    For Stage 2, "Win" also depends on the final reward.
    
    Parameters:
    - prob_stay_win: [0, 1] Probability of repeating a choice after a win.
    - prob_shift_lose: [0, 1] Probability of switching a choice after a loss.
    - epsilon: [0, 1] Random noise (lapse rate) applied to decisions.
    """
    prob_stay_win, prob_shift_lose, epsilon = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Store history
    last_action_1 = -1
    last_action_2_at_state = np.full(2, -1) # Track last action at specific planet
    last_reward = -1 # Result of the previous trial

    for trial in range(n_trials):
        
        # --- Stage 1 Policy ---
        # Default random probabilities
        probs_1 = np.array([0.5, 0.5])
        
        if last_action_1 != -1:
            if last_reward == 1:
                # Win-Stay
                probs_1[last_action_1] = prob_stay_win
                probs_1[1 - last_action_1] = 1 - prob_stay_win
            else:
                # Lose-Shift
                probs_1[last_action_1] = 1 - prob_shift_lose
                probs_1[1 - last_action_1] = prob_shift_lose
        
        # Apply epsilon noise (mixture with uniform distribution)
        probs_1 = (1 - epsilon) * probs_1 + epsilon * 0.5
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_action_1 = action_1[trial]
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        probs_2 = np.array([0.5, 0.5])
        prev_a2 = last_action_2_at_state[state_idx]
        
        # Note: We only have a history for this specific state if we've been here before.
        # However, simplistic WSLS usually just looks at the very last trial regardless of state,
        # or tracks state-specific history. Here we track state-specific history for Stage 2 accuracy.
        
        if prev_a2 != -1:
             # We need to know if the LAST time we were at this state, did we win?
             # This simple model assumes the global last_reward acts as the proxy for "feeling good",
             # or we could track reward history per state. 
             # Let's use global last_reward as a simplified "mood" heuristic common in WSLS analysis.
            if last_reward == 1:
                probs_2[prev_a2] = prob_stay_win
                probs_2[1 - prev_a2] = 1 - prob_stay_win
            else:
                probs_2[prev_a2] = 1 - prob_shift_lose
                probs_2[1 - prev_a2] = prob_shift_lose

        # Apply epsilon noise
        probs_2 = (1 - epsilon) * probs_2 + epsilon * 0.5
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Update history
        last_action_2_at_state[state_idx] = action_2[trial]
        last_reward = reward[trial]
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```