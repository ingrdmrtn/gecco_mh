Here are three new cognitive models based on the two-step task template.

### Model 1: Hybrid Learner with Eligibility Traces
This model introduces an eligibility trace parameter (`lambda_eligibility`). In standard Model-Free learning (TD(0)), the first-stage choice is updated only based on the value of the second-stage state. In this model, the reward received at the end of the trial can also directly update the first-stage choice (TD(1) or Monte Carlo), scaled by `lambda`. This bridges the gap between the two stages.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner with Eligibility Traces (Lambda).
    
    Incorporates an eligibility trace parameter (lambda). This allows the Stage 1 
    Model-Free values to be updated not just by the Stage 2 state value prediction, 
    but also by the final reward prediction error, scaled by lambda.
    
    Bounds:
    - learning_rate: [0, 1]
    - beta: [0, 10]
    - w: [0, 1] (Weight for Model-Based, 1=MB, 0=MF)
    - lambda_eligibility: [0, 1] (0 = pure TD(0), 1 = Monte Carlo-like update for stage 1)
    """
    learning_rate, beta, w, lambda_eligibility = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combine MB and MF values
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Update Stage 1 MF value using Stage 2 value (TD(0))
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Update Stage 2 MF value using Reward
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Eligibility Trace: Pass Stage 2 prediction error back to Stage 1
        # This allows the final reward to directly influence the start state value
        q_stage1_mf[action_1[trial]] += learning_rate * lambda_eligibility * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Learner with Separate Stage Temperatures
This model posits that the level of exploration vs. exploitation differs between the planning stage (Stage 1) and the harvesting stage (Stage 2). It uses two separate inverse temperature parameters (`beta_1` and `beta_2`).

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner with Separate Stage Temperatures.
    
    Allows for different levels of exploration/randomness (beta) at Stage 1 
    (spaceship choice) versus Stage 2 (alien choice).
    
    Bounds:
    - learning_rate: [0, 1]
    - beta_1: [0, 10] (Inverse temp for Stage 1)
    - beta_2: [0, 10] (Inverse temp for Stage 2)
    - w: [0, 1] (Weight for Model-Based)
    """
    learning_rate, beta_1, beta_2, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice using beta_1
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_1 * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice using beta_2
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Learner with Passive Decay
This model incorporates a forgetting mechanism. At the end of every trial, all Q-values decay towards zero. This accounts for the participant's uncertainty increasing over time for options they have not visited recently, or a belief that the environment is volatile.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner with Passive Decay.
    
    Q-values for all actions decay toward 0 on every trial. This accounts for
    forgetting or a prior belief that values drift/degrade over time.
    
    Bounds:
    - learning_rate: [0, 1]
    - beta: [0, 10]
    - w: [0, 1]
    - decay_rate: [0, 1] (Fraction of value lost per trial)
    """
    learning_rate, beta, w, decay_rate = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Apply passive decay to all values
        q_stage1_mf *= (1 - decay_rate)
        q_stage2_mf *= (1 - decay_rate)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```