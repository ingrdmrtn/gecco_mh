Here are three new cognitive models exploring different mechanisms for learning and decision-making in the two-step task.

### Model 1: Hybrid Model with Separate Learning Rates for MB/MF
This model implements the classic hybrid reinforcement learning theory (Daw et al., 2011), but unlike previous attempts that might have used a single learning rate, this version allows the Model-Based (MB) and Model-Free (MF) systems to learn at different speeds. The Model-Based system learns the transition structure (though here fixed for simplicity in value calculation, the values it computes are distinct), while the Model-Free system learns from direct experience. The `w` parameter controls the balance between these two distinct value estimates.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF Model with Separate Learning Rates.
    Allows the model-free and model-based value estimates to update at different rates.
    
    Bounds:
    lr_mf: [0, 1] - Learning rate for model-free updates (Stage 1 and Stage 2).
    lr_mb: [0, 1] - Learning rate for the second-stage values used specifically by the MB planner.
    beta: [0, 10] - Inverse temperature for softmax.
    w: [0, 1] - Weighting parameter (0 = Pure MF, 1 = Pure MB).
    """
    lr_mf, lr_mb, beta, w = model_parameters
    n_trials = len(action_1)
  
    # Transition matrix assumed fixed/known for the MB calculation component
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # MF values
    q_mf_s1 = np.zeros(2)
    q_mf_s2 = np.zeros((2, 2)) # Used for MF system
    
    # MB values (we track a separate set of Q-values for the MB system to allow distinct learning rates)
    q_mb_s2 = np.zeros((2, 2)) 

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based Value: V_MB(s1) = T(s1, s') * max_a Q_MB(s', a)
        max_q_mb_s2 = np.max(q_mb_s2, axis=1)
        q_net_mb = transition_matrix @ max_q_mb_s2
        
        # Model-Free Value is just the stored Q-value
        q_net_mf = q_mf_s1
        
        # Hybrid Value
        q_net = w * q_net_mb + (1 - w) * q_net_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Policy ---
        # Usually only MF or simple Q-learning drives Stage 2 choice
        state_idx = state[trial]
        # We use the MF values for stage 2 choice (common assumption as MB just plans over them)
        # But here we average them or just use the one being updated by reward directly.
        # Let's use the average to be consistent with the hybrid nature.
        q_s2_integrated = w * q_mb_s2[state_idx] + (1 - w) * q_mf_s2[state_idx]
        
        exp_q2 = np.exp(beta * q_s2_integrated)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        # 1. MF Updates (TD-0 and SARSA-like for stage 1)
        # Stage 1 MF update (using stage 2 value as proxy for reward)
        delta_s1 = q_mf_s2[state_idx, action_2[trial]] - q_mf_s1[action_1[trial]]
        q_mf_s1[action_1[trial]] += lr_mf * delta_s1
        
        # Stage 2 MF update (using actual reward)
        delta_s2_mf = reward[trial] - q_mf_s2[state_idx, action_2[trial]]
        q_mf_s2[state_idx, action_2[trial]] += lr_mf * delta_s2_mf
        
        # 2. MB Updates
        # The MB system updates its internal model of stage 2 values based on reward
        delta_s2_mb = reward[trial] - q_mb_s2[state_idx, action_2[trial]]
        q_mb_s2[state_idx, action_2[trial]] += lr_mb * delta_s2_mb

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Model-Free with Outcome-Specific Perseverance
Standard perseverance ("stickiness") repeats the last chosen action regardless of the outcome. This model introduces **outcome-specific perseverance**, hypothesizing that participants might be more likely to repeat a Stage 1 choice if they were rewarded, or perhaps if they reached the common state, distinct from reinforcement learning. This captures a heuristic strategy like "Win-Stay, Lose-Shift" operating alongside Q-learning.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free RL with Outcome-Specific Perseverance.
    Adds a 'stickiness' bonus to the previous Stage 1 action, but the magnitude
    of this bonus depends on whether the previous trial was rewarded.
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    pers_rew: [0, 5] - Perseverance bonus if previous trial was rewarded.
    pers_unrew: [0, 5] - Perseverance bonus if previous trial was unrewarded.
    """
    learning_rate, beta, pers_rew, pers_unrew = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    last_action_1 = -1
    last_reward = 0

    for trial in range(n_trials):

        # --- Stage 1 Choice ---
        q_hybrid = q_stage1.copy()
        
        # Apply outcome-dependent perseverance
        if last_action_1 != -1:
            if last_reward == 1:
                q_hybrid[last_action_1] += pers_rew
            else:
                q_hybrid[last_action_1] += pers_unrew

        exp_q1 = np.exp(beta * q_hybrid)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Record for next trial
        last_action_1 = action_1[trial]
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # Stage 1 Update (TD)
        delta_stage1 = q_stage2[state_idx, action_2[trial]] - q_stage1[action_1[trial]]
        q_stage1[action_1[trial]] += learning_rate * delta_stage1
        
        # Stage 2 Update (Reward Prediction Error)
        delta_stage2 = reward[trial] - q_stage2[state_idx, action_2[trial]]
        q_stage2[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        last_reward = reward[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Dynamic Transition Learning (Model-Based)
Most models assume the participant knows the transition matrix (0.7/0.3) is fixed. This model assumes the participant is actively **learning the transition probabilities** between Stage 1 and Stage 2. It is a Model-Based learner that updates its belief about `P(State|Action)` trial-by-trial using a delta rule, rather than using the static true probabilities.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Based RL with Dynamic Transition Learning.
    The agent learns the transition matrix T(s1, s2) online instead of assuming it is fixed.
    
    Bounds:
    lr_value: [0, 1] - Learning rate for Q-values (Stage 2).
    lr_trans: [0, 1] - Learning rate for the transition matrix probabilities.
    beta: [0, 10] - Inverse temperature.
    """
    lr_value, lr_trans, beta = model_parameters
    n_trials = len(action_1)
  
    # Initialize transition estimates to uniform (0.5) or slightly biased if preferred.
    # Structure: [Action_Stage1][Resulting_State]
    # transitions[0, 0] is P(State 0 | Action 0)
    # transitions[0, 1] is P(State 1 | Action 0) etc.
    trans_probs = np.ones((2, 2)) * 0.5 
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Only Stage 2 Q-values are needed for pure MB planning
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy (Model-Based Planning) ---
        # Calculate expected value of each Stage 1 action using CURRENT transition beliefs
        # V(a1) = P(s_0|a1)*max(Q(s_0)) + P(s_1|a1)*max(Q(s_1))
        
        qs_stage1_calculated = np.zeros(2)
        max_q_s2 = np.max(q_stage2, axis=1) # [max_val_state0, max_val_state1]
        
        for a1 in range(2):
            # The probability of reaching state 0 given a1
            p_s0 = trans_probs[a1, 0]
            # The probability of reaching state 1 given a1
            p_s1 = trans_probs[a1, 1]
            
            qs_stage1_calculated[a1] = p_s0 * max_q_s2[0] + p_s1 * max_q_s2[1]

        exp_q1 = np.exp(beta * qs_stage1_calculated)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        current_action_1 = action_1[trial]
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Value Update ---
        # Update Stage 2 values based on reward
        delta_val = reward[trial] - q_stage2[state_idx, action_2[trial]]
        q_stage2[state_idx, action_2[trial]] += lr_value * delta_val
        
        # --- Transition Probability Update ---
        # Update belief about P(State | Action 1)
        # We observed 'state_idx' after taking 'current_action_1'
        # Create a one-hot vector for the observed outcome
        outcome_vec = np.zeros(2)
        outcome_vec[state_idx] = 1.0
        
        # Delta rule for transitions: Old_Prob + lr * (Actual_Outcome - Old_Prob)
        # Note: This ensures probabilities sum to 1 if initialized that way and lr is consistent
        trans_probs[current_action_1] += lr_trans * (outcome_vec - trans_probs[current_action_1])
        
        # Normalize to ensure numerical stability (though delta rule usually keeps sum=1)
        trans_probs[current_action_1] /= np.sum(trans_probs[current_action_1])

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```