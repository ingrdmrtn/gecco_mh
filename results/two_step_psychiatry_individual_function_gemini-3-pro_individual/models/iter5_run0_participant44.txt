Here are three new cognitive models exploring different mechanisms for learning and decision-making in the two-step task.

### Model 1: Hybrid Learner with Separate Learning Rates for Positive and Negative Prediction Errors (Asymmetric Learning)
This model extends the classic hybrid (Model-Based + Model-Free) approach by allowing the agent to learn differently from "good" surprises (positive prediction errors) versus "bad" surprises (negative prediction errors). This asymmetry is often observed in clinical populations.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid learner (MB/MF) with asymmetric learning rates for positive/negative RPEs.
    Allows different updates for "good" vs "bad" outcomes.

    Parameters:
    alpha_pos: [0,1] Learning rate for positive prediction errors.
    alpha_neg: [0,1] Learning rate for negative prediction errors.
    beta: [0,10] Inverse temperature (softness of choice).
    w: [0,1] Weighting parameter (0 = pure Model-Free, 1 = pure Model-Based).
    """
    alpha_pos, alpha_neg, beta, w = model_parameters
    n_trials = len(action_1)

    # Transition matrix (fixed structure as per task description)
    # 0 -> [0.7, 0.3], 1 -> [0.3, 0.7]
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # 2 planets, 2 aliens

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based Value Calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2

        # Integrated Value
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf

        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        state_idx = state[trial]
        a1 = action_1[trial]
        
        # --- Stage 2 Choice ---
        # Standard Q-learning choice at stage 2
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        a2 = action_2[trial]
        r = reward[trial]

        # --- Updating ---
        # Stage 2 RPE
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        
        # Asymmetric update for Stage 2
        if delta_stage2 >= 0:
            q_stage2_mf[state_idx, a2] += alpha_pos * delta_stage2
        else:
            q_stage2_mf[state_idx, a2] += alpha_neg * delta_stage2

        # Stage 1 RPE (TD(0) update using updated stage 2 value)
        # Note: In standard TD(1) or eligibility trace models, we might update stage 1 based on the reward.
        # Here we use the standard TD chain: Stage 1 updates towards Stage 2 value.
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        
        # Asymmetric update for Stage 1
        if delta_stage1 >= 0:
            q_stage1_mf[a1] += alpha_pos * delta_stage1
        else:
            q_stage1_mf[a1] += alpha_neg * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Model-Free Learner with Outcome-Specific Perseveration
This model tests the hypothesis that perseveration (stickiness) is not just about repeating the last action, but specifically about repeating actions that were *rewarded* versus those that were *unrewarded*. It separates the "stickiness" parameter into a general repetition bias and a reward-specific repetition bias.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free learner with Outcome-Specific Perseveration.
    Distinguishes between general stickiness and stickiness enhanced by reward.

    Parameters:
    learning_rate: [0,1] Update rate.
    beta: [0,10] Inverse temperature.
    pers_stick: [0,5] General bonus for repeating the last action taken (regardless of outcome).
    pers_win_stay: [0,5] Additional bonus for repeating the last action IF it was rewarded.
    """
    learning_rate, beta, pers_stick, pers_win_stay = model_parameters
    n_trials = len(action_1)

    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    last_action_1 = -1
    last_action_2 = -1
    last_reward = 0

    for trial in range(n_trials):
        state_idx = state[trial]

        # --- Stage 1 Choice ---
        q_eff_1 = q_stage1.copy()
        
        # Apply perseveration bonuses
        if last_action_1 != -1:
            q_eff_1[last_action_1] += pers_stick
            if last_reward == 1:
                q_eff_1[last_action_1] += pers_win_stay

        exp_q1 = np.exp(beta * q_eff_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]

        # --- Stage 2 Choice ---
        q_eff_2 = q_stage2[state_idx].copy()
        
        # Note: For Stage 2, perseveration is usually state-specific.
        # We check if we were in this state last time we acted.
        # For simplicity in this structure, we often just use the very last action if applicable,
        # but standard implementations might track 'last_action_per_state'. 
        # Here we apply a simple heuristic: if the previous trial ended in this state, apply bonus.
        # However, typically 'last_action_2' is tracked globally or per state. 
        # Let's assume global stickiness for simplicity or stickiness to the specific button pressed.
        if last_action_2 != -1:
             q_eff_2[last_action_2] += pers_stick
             if last_reward == 1:
                 q_eff_2[last_action_2] += pers_win_stay

        exp_q2 = np.exp(beta * q_eff_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        a2 = action_2[trial]
        r = reward[trial]

        # --- Updating ---
        # SARSA / Q-learning updates
        delta_stage1 = q_stage2[state_idx, a2] - q_stage1[a1]
        q_stage1[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2[state_idx, a2]
        q_stage2[state_idx, a2] += learning_rate * delta_stage2

        last_action_1 = a1
        last_action_2 = a2
        last_reward = r

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Learner with Inverse Temperature (Beta) Modulation by Conflict/Uncertainty
This model posits that decision noise (exploration) is not constant. Instead, the inverse temperature `beta` is modulated by the "conflict" or value difference between options. When values are close (high conflict), the agent might become more deterministic (focusing harder) or more random (giving up). Here we model it such that `beta` scales with the absolute difference in values, simulating a dynamic exploration rate.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid learner where Beta is modulated by value difference (conflict).
    Base beta is scaled: beta_effective = beta_base * (1 + beta_slope * |V_left - V_right|).
    
    Parameters:
    learning_rate: [0,1] Update rate.
    beta_base: [0,10] Baseline inverse temperature.
    beta_slope: [0,5] Sensitivity of beta to value difference.
    w: [0,1] Weighting parameter (0 = pure Model-Free, 1 = pure Model-Based).
    """
    learning_rate, beta_base, beta_slope, w = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Calculate value difference (conflict)
        val_diff_1 = np.abs(q_net_1[0] - q_net_1[1])
        beta_eff_1 = beta_base * (1 + beta_slope * val_diff_1)
        
        exp_q1 = np.exp(beta_eff_1 * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        a1 = action_1[trial]

        # --- Stage 2 Choice ---
        q_vals_2 = q_stage2_mf[state_idx]
        
        val_diff_2 = np.abs(q_vals_2[0] - q_vals_2[1])
        beta_eff_2 = beta_base * (1 + beta_slope * val_diff_2)

        exp_q2 = np.exp(beta_eff_2 * q_vals_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]

        # --- Updating ---
        # Standard TD updates
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```