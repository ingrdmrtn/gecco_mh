Here are three new cognitive models for the two-step task. These models explore different mechanisms such as separate learning rates for the two stages, stickiness to the second-stage choice, and a distinct "common vs. rare" update heuristic.

### Cognitive Model 1: Separate Stage Learning Rates
This model hypothesizes that the participant learns at different speeds for the first stage (spaceship choice) versus the second stage (alien choice). This is biologically plausible as the first stage involves transition structure learning (or caching values), while the second stage is purely reward-based bandit learning.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MF/MB Model with Separate Stage Learning Rates.
    
    This model allows for different learning rates for the first-stage (spaceship)
    value updates and the second-stage (alien) value updates. It assumes the 
    participant might update their valuation of the spaceships (alpha1) differently 
    than they update their valuation of the specific aliens (alpha2).

    Parameters:
    alpha1: [0, 1] - Learning rate for stage 1 (model-free) Q-values.
    alpha2: [0, 1] - Learning rate for stage 2 Q-values.
    beta: [0, 10] - Inverse temperature (softmax randomness).
    w: [0, 1] - Weighting parameter (1 = fully Model-Based, 0 = fully Model-Free).
    """
    alpha1, alpha2, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combined value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        # Stage 1 update uses alpha1
        # Note: TD(0) update. The target is the value of the state we landed in.
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += alpha1 * delta_stage1

        # Stage 2 update uses alpha2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += alpha2 * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2: Second-Stage "Stickiness" (Response Stickiness)
Previous models explored perseveration on the first choice (repeating the spaceship). This model explores perseveration on the *second* choice (repeating the specific alien). If a participant picks Alien A on Planet X, they might be biased to pick Alien A again if they return to Planet X, regardless of reward history.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MF/MB Model with Second-Stage Choice Stickiness.
    
    This model includes a 'stickiness' parameter for the second stage (alien choice).
    It captures the tendency to repeat the same action (alien choice) within a 
    specific state (planet) if visited again, independent of the reward received.

    Parameters:
    learning_rate: [0, 1] - Update rate for Q-values.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - Weighting parameter (1 = fully Model-Based, 0 = fully Model-Free).
    stickiness_2: [0, 5] - Bonus added to the Q-value of the previously chosen alien in that state.
    """
    learning_rate, beta, w, stickiness_2 = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track the last action taken in each state (0 or 1). Initialize to -1 (none).
    last_action_in_state = np.array([-1, -1]) 

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy with Stickiness ---
        qs_current_state = q_stage2_mf[state_idx].copy()
        
        # Add stickiness bonus if we have visited this state before
        if last_action_in_state[state_idx] != -1:
            qs_current_state[last_action_in_state[state_idx]] += stickiness_2
            
        exp_q2 = np.exp(beta * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1

        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Record action for stickiness in future trials
        last_action_in_state[state_idx] = action_2[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3: "Common-Transition" Heuristic Model
Instead of a full Model-Based calculation, some participants use a simpler heuristic: if they get a reward after a Common transition, they stay with the spaceship. If they get a reward after a Rare transition, they switch spaceships (because the other spaceship is the "common" path to the rewarding planet). This model replaces the complex `w` weighting with a direct `heuristic_strength` parameter that modifies the first-stage Q-values based on the previous trial's outcome type.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free with 'Common-Transition' Heuristic Update.
    
    This model is fundamentally Model-Free but adds a heuristic update to the 
    first-stage values. If the previous trial was a Rare transition and resulted 
    in a Reward, the agent boosts the value of the *unchosen* spaceship (the one 
    that commonly leads to the rewarding state), mimicking model-based behavior 
    without the full planning computation.

    Parameters:
    learning_rate: [0, 1] - Standard TD learning rate.
    beta: [0, 10] - Inverse temperature.
    heuristic_strength: [0, 2] - Strength of the heuristic update applied to the 'other' action.
    """
    learning_rate, beta, heuristic_strength = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy (Pure MF) ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Standard Updates ---
        # Standard TD(1) / SARSA-like update chain
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1

        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # --- Heuristic Update ---
        # Determine if transition was common or rare
        # Action 0 -> State 0 (Common), Action 1 -> State 1 (Common)
        is_common = (action_1[trial] == state_idx)
        
        if reward[trial] == 1:
            if not is_common:
                # Rare transition + Reward: The *other* spaceship is actually the good one.
                # Boost the unchosen action.
                other_action = 1 - action_1[trial]
                q_stage1_mf[other_action] += heuristic_strength
            else:
                # Common transition + Reward: The chosen spaceship is good.
                # Boost the chosen action (reinforcing the standard MF update).
                q_stage1_mf[action_1[trial]] += heuristic_strength

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```