Here are three cognitive models exploring different mechanisms of learning and decision-making in the two-step task.

### Model 1: Hybrid Learner with Mixing Parameter `w`
This model is the classic "Daw et al. (2011)" hybrid model. It assumes the participant uses a weighted combination of Model-Based (planning using the transition matrix) and Model-Free (trial-and-error) values for the first-stage decision. The parameter `w` controls the balance: `w=1` is purely Model-Based, `w=0` is purely Model-Free.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Learner.
    
    This model computes Q-values using both a Model-Based (MB) strategy (using the 
    known transition matrix) and a Model-Free (MF) strategy (TD learning). 
    The first-stage choice is a weighted mix of these two valuations.
    
    Parameters:
    learning_rate: [0, 1] - Update rate for Q-values.
    beta: [0, 10] - Inverse temperature for softmax.
    w: [0, 1] - Mixing weight. 1 = Pure MB, 0 = Pure MF.
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition matrix: Row 0 is Spaceship A -> [Planet X, Planet Y]
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)       # MF values for stage 1
    q_stage2_mf = np.zeros((2, 2))  # MF values for stage 2 (aliens)

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # Model-Based Calculation: V_MB(s1) = T * max(Q(s2))
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value: Q_net = w * Q_MB + (1-w) * Q_MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax Choice 1
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial] # Planet arrived at

        # --- Stage 2 Decision ---
        # Standard Softmax on Stage 2 Q-values
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        # 1. Update Stage 2 values (SARSA/Q-learning at terminal state)
        # RPE at stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # 2. Update Stage 1 MF values (TD(1) logic: update based on reward)
        # Note: Often TD(1) is used in simple hybrid models for stability
        delta_stage1 = reward[trial] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Perseveration Learner (Stickiness)
This model extends the basic learning framework by adding a "choice stickiness" or perseveration parameter (`pers`). This parameter accounts for the tendency to repeat the previous action regardless of reward history (motor perseveration) or to switch actions. This is often a strong signal in human and animal data.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Learner with Choice Perseveration (Stickiness).
    
    This model is a Model-Free learner (TD(1)) but adds a 'perseveration' bonus
    to the action taken on the previous trial. This captures the tendency to 
    repeat choices regardless of value.

    Parameters:
    learning_rate: [0, 1] - Update rate for Q-values.
    beta: [0, 10] - Inverse temperature.
    pers: [-5, 5] - Perseveration parameter. Positive = sticky, Negative = switching.
    """
    learning_rate, beta, pers = model_parameters
    n_trials = len(action_1)

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1 # Initialize with an impossible action

    for trial in range(n_trials):

        # --- Stage 1 Decision ---
        # Calculate logits: Value * beta + Perseveration Bonus
        logits_1 = beta * q_stage1_mf.copy()
        if last_action_1 != -1:
            logits_1[last_action_1] += pers
            
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Record action for next trial's stickiness
        last_action_1 = action_1[trial]
        
        state_idx = state[trial]

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # TD(1) update for Stage 1
        delta_stage1 = reward[trial] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Separate Learning Rates for Stage 1 and Stage 2
This model acknowledges that learning dynamics might differ between the abstract choice of spaceships (Stage 1) and the concrete choice of aliens (Stage 2). It uses two distinct learning rates: `lr_1` for updating spaceship values and `lr_2` for updating alien values. This is a pure Model-Free approach but allows for different timescales of integration.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Learner with Separate Learning Rates.
    
    This model assumes the participant learns about Spaceships (Stage 1) and 
    Aliens (Stage 2) at different speeds. It uses TD(1) logic but separates
    the alpha parameter.

    Parameters:
    lr_1: [0, 1] - Learning rate for Stage 1 (Spaceships).
    lr_2: [0, 1] - Learning rate for Stage 2 (Aliens).
    beta: [0, 10] - Inverse temperature.
    """
    lr_1, lr_2, beta = model_parameters
    n_trials = len(action_1)

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):

        # --- Stage 1 Decision ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        # Update Stage 2 (Aliens) using lr_2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr_2 * delta_stage2
        
        # Update Stage 1 (Spaceships) using lr_1
        # Using the final reward as the proxy for value (TD(1))
        delta_stage1 = reward[trial] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr_1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```