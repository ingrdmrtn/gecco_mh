Here are three new cognitive models based on the two-step decision task, exploring different mechanisms than those already tested.

### Model 1: Hybrid Learner with Separate Learning Rates for Stages
This model hypothesizes that the participant might learn the value of spaceships (Stage 1) and aliens (Stage 2) at different speeds. For example, Stage 2 values are direct reward associations, while Stage 1 values are more abstract predictions of future value.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner with Separate Learning Rates for Stages.
    
    This model assumes the participant learns Stage 1 (spaceship) values and 
    Stage 2 (alien) values at different rates. It combines Model-Based (MB) 
    and Model-Free (MF) control via a mixing weight 'w'.

    Parameters:
    lr_stage1: [0,1] - Learning rate for Stage 1 (spaceships).
    lr_stage2: [0,1] - Learning rate for Stage 2 (aliens).
    beta: [0,10] - Inverse temperature for softmax choice.
    w: [0,1] - Mixing weight (0 = pure MF, 1 = pure MB).
    """
    lr_stage1, lr_stage2, beta, w = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition matrix as per task description (70% common, 30% rare)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)      # Model-free values for Stage 1
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for Stage 2

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based value calculation: V(s') = max_a Q(s', a)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated value
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial] # The planet actually reached

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning ---
        # Update Stage 2 values (Aliens) using specific Stage 2 learning rate
        # Prediction Error 2
        pe2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr_stage2 * pe2
        
        # Update Stage 1 values (Spaceships) using specific Stage 1 learning rate
        # TD(0) update: The value of the spaceship is updated toward the value of the alien chosen
        # Note: In standard TD, this is usually Q(s1, a1) += alpha * (Q(s2, a2) - Q(s1, a1))
        pe1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr_stage1 * pe1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Model-Free Learner with Outcome-Specific Learning Rates
This model suggests the participant might be biased in how they process wins versus losses (asymmetric learning). They might update their beliefs strongly after finding gold but ignore empty planets, or vice versa.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Learner with Outcome-Specific Learning Rates (Asymmetric Learning).
    
    This model differentiates learning from positive prediction errors (better than expected)
    vs negative prediction errors (worse than expected).

    Parameters:
    alpha_pos: [0,1] - Learning rate for positive prediction errors.
    alpha_neg: [0,1] - Learning rate for negative prediction errors.
    beta: [0,10] - Inverse temperature.
    """
    alpha_pos, alpha_neg, beta = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning ---
        # Stage 2 Update
        pe2 = reward[trial] - q_stage2[state_idx, action_2[trial]]
        lr2 = alpha_pos if pe2 > 0 else alpha_neg
        q_stage2[state_idx, action_2[trial]] += lr2 * pe2
        
        # Stage 1 Update (TD-learning)
        # Using the value of the chosen second stage action as the target
        pe1 = q_stage2[state_idx, action_2[trial]] - q_stage1[action_1[trial]]
        lr1 = alpha_pos if pe1 > 0 else alpha_neg
        q_stage1[action_1[trial]] += lr1 * pe1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Model-Based Learner with Transition Learning
Instead of assuming the transition matrix (70/30) is fixed and known perfectly, this model assumes the participant learns the transition structure of the spaceships over time. It is a pure Model-Based agent, but one that updates its mental map of the world.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Based Learner with Transition Learning.
    
    The agent does not use model-free caching for Stage 1. Instead, it estimates 
    the probability of Spaceship A -> Planet X (and U -> Y) based on experience.
    
    Parameters:
    learning_rate: [0,1] - Learning rate for reward values (Stage 2).
    trans_lr: [0,1] - Learning rate for transition probabilities.
    beta: [0,10] - Inverse temperature.
    """
    learning_rate, trans_lr, beta = model_parameters
    n_trials = len(action_1)
  
    # Initialize transition estimate (start with flat priors 0.5 or weak prior)
    # trans_prob[0] is P(Planet 0 | Spaceship 0)
    # trans_prob[1] is P(Planet 1 | Spaceship 1)
    trans_probs = np.array([0.5, 0.5]) 
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage2 = np.zeros((2, 2)) # Values of aliens

    for trial in range(n_trials):
        # --- Stage 1 Choice (Model-Based Calculation) ---
        # V(Planet 0) = max(Q(Planet 0, Alien 0), Q(Planet 0, Alien 1))
        v_planet_0 = np.max(q_stage2[0])
        v_planet_1 = np.max(q_stage2[1])
        
        # Q_MB(Spaceship 0) = P(P0|S0)*V(P0) + P(P1|S0)*V(P1)
        q_mb_0 = trans_probs[0] * v_planet_0 + (1 - trans_probs[0]) * v_planet_1
        
        # Q_MB(Spaceship 1) = P(P1|S1)*V(P1) + P(P0|S1)*V(P0)
        # Note: trans_probs[1] tracks P(P1|S1), so (1-trans_probs[1]) is P(P0|S1)
        q_mb_1 = (1 - trans_probs[1]) * v_planet_0 + trans_probs[1] * v_planet_1
        
        q_stage1 = np.array([q_mb_0, q_mb_1])

        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Choice ---
        state_idx = state[trial]
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning ---
        
        # 1. Update Transition Probabilities
        # If we took Spaceship 0 (action_1 == 0) and landed on Planet 0 (state == 0), outcome is 1.
        # If we took Spaceship 0 and landed on Planet 1, outcome is 0.
        if action_1[trial] == 0:
            outcome = 1 if state[trial] == 0 else 0
            trans_probs[0] += trans_lr * (outcome - trans_probs[0])
        elif action_1[trial] == 1:
            outcome = 1 if state[trial] == 1 else 0
            trans_probs[1] += trans_lr * (outcome - trans_probs[1])

        # 2. Update Reward Values (Q-learning at Stage 2)
        pe = reward[trial] - q_stage2[state_idx, action_2[trial]]
        q_stage2[state_idx, action_2[trial]] += learning_rate * pe

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```