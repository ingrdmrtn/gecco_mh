Here are three new cognitive models exploring different mechanisms for learning and decision-making in the two-step task, distinct from the feedback provided.

### Model 1: Hybrid Learner with Separate Learning Rates for Stages
This model hypothesizes that the participant learns at different speeds for the first-stage decision (which spaceship to choose) versus the second-stage decision (which alien to ask). This is plausible because the first stage involves transition structures while the second stage is purely reward-based. It combines Model-Based (MB) and Model-Free (MF) control via a weighting parameter `w`.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF Learner with Separate Learning Rates.

    Hypothesis: The participant integrates model-based and model-free values but learns
    action-values for the spaceship choice (stage 1) at a different rate than alien values (stage 2).

    Parameters:
    lr_stage1: [0, 1] - Learning rate for first-stage Q-values (MF).
    lr_stage2: [0, 1] - Learning rate for second-stage Q-values.
    beta: [0, 10] - Inverse temperature for softmax choice.
    w: [0, 1] - Mixing weight (0 = pure Model-Free, 1 = pure Model-Based).
    """
    lr_stage1, lr_stage2, beta, w = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition matrix for the model-based component
    # Rows: Actions (Spaceships), Cols: States (Planets)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)      # MF values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Values for stage 2 (Aliens)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based Value Calculation
        # Max Q-value available at stage 2 for each state
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        # Expected value of stage 1 actions based on transition structure
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value Integration
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # TD(0) update for Stage 1 (Model-Free) using specific lr_stage1
        # Note: Standard TD uses the value of the next state (q_stage2_mf)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr_stage1 * delta_stage1
        
        # TD(0) update for Stage 2 using specific lr_stage2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr_stage2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Model-Based Learner with Eligibility Traces (TD(lambda))
This model assumes the participant is purely Model-Free but uses eligibility traces (TD-lambda). Instead of just updating the immediately preceding state, the reward signal propagates back to the first-stage choice with a decay factor `lam`. This allows the first-stage choice to be reinforced directly by the reward outcome, bridging the gap between the two stages more effectively than simple TD(0).

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Learner with Eligibility Traces (TD-lambda).

    Hypothesis: The participant uses model-free reinforcement learning, but the 
    reward obtained at the end of the trial reinforces the first-stage choice 
    directly, mediated by an eligibility trace parameter lambda.

    Parameters:
    learning_rate: [0, 1] - Update rate for Q-values.
    beta: [0, 10] - Inverse temperature for softmax choice.
    lam: [0, 1] - Eligibility trace decay parameter (lambda). 
                  0 = TD(0), 1 = Monte Carlo-like updates.
    """
    learning_rate, beta, lam = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # Prediction error at stage 2 (reward - expectation)
        delta_2 = reward[trial] - q_stage2[state_idx, action_2[trial]]
        
        # Prediction error at stage 1 (value of next state - expectation)
        delta_1 = q_stage2[state_idx, action_2[trial]] - q_stage1[action_1[trial]]

        # Update Stage 2 value
        q_stage2[state_idx, action_2[trial]] += learning_rate * delta_2
        
        # Update Stage 1 value
        # The update includes the immediate error (delta_1) PLUS 
        # the discounted error from stage 2 (lam * delta_2)
        q_stage1[action_1[trial]] += learning_rate * (delta_1 + lam * delta_2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Reward-Dependent Learning Rates (Asymmetric Updating)
This model posits that the participant learns differently from positive outcomes (finding gold) versus negative outcomes (no gold). This asymmetry is often observed in clinical populations or specific cognitive styles. It applies separate learning rates for positive prediction errors (`alpha_pos`) and negative prediction errors (`alpha_neg`).

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Asymmetric Model-Free Learner (Reward-Dependent Learning Rates).

    Hypothesis: The participant updates their value estimates differently depending on 
    whether the outcome was better than expected (positive prediction error) or 
    worse than expected (negative prediction error).

    Parameters:
    alpha_pos: [0, 1] - Learning rate for positive prediction errors.
    alpha_neg: [0, 1] - Learning rate for negative prediction errors.
    beta: [0, 10] - Inverse temperature for softmax choice.
    """
    alpha_pos, alpha_neg, beta = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        lr_1 = alpha_pos if delta_stage1 > 0 else alpha_neg
        q_stage1_mf[action_1[trial]] += lr_1 * delta_stage1

        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        lr_2 = alpha_pos if delta_stage2 > 0 else alpha_neg
        q_stage2_mf[state_idx, action_2[trial]] += lr_2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```