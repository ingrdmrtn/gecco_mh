Here are 3 new cognitive models as Python functions.

### Model 1: Perseveration + Model-Based RL
This model adds a "perseveration" parameter to the standard Model-Based approach. The participant data shows a strong tendency to repeat the same action (Spaceship 0, Alien 0) repeatedly. A simple RL model might explain this via high value, but a perseveration bias explicitly models the tendency to repeat the previous choice regardless of reward, which is common in human decision-making.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Model-Based RL with Perseveration.
    
    This model assumes the agent calculates values based on the transition structure (Model-Based),
    but biases its decision at the first stage based on what it chose in the previous trial.
    Positive 'pers' makes the agent sticky; negative 'pers' makes them switchy.

    Parameters:
    - learning_rate: [0, 1] Update rate for second-stage Q-values.
    - beta: [0, 10] Inverse temperature for softmax.
    - pers: [0, 5] Perseveration bonus added to the Q-value of the previously chosen action.
    """
    learning_rate, beta, pers = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values for the second stage (aliens)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track previous choice for perseveration (initialize to -1 or None)
    prev_choice_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Policy (Model-Based + Perseveration) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Add perseveration bonus
        logits_1 = q_stage1_mb.copy()
        if prev_choice_1 != -1:
            logits_1[prev_choice_1] += pers
            
        exp_q1 = np.exp(beta * logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy (Standard Softmax) ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # Update Stage 2 values based on reward
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Store choice for next trial's perseveration
        prev_choice_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Pure Model-Free TD(0) (Sarsa-style)
This model ignores the transition matrix entirely and learns purely from experience using a standard Temporal Difference (TD) algorithm without eligibility traces. It treats the first stage value as a prediction of the second stage value (state-action-reward-state-action). This is the "habitual" system often contrasted with the model-based system.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free TD(0) Learning.
    
    The agent learns values for Stage 1 and Stage 2 independently based on direct experience.
    Stage 1 values are updated by the value of the state reached in Stage 2 (TD error).
    Stage 2 values are updated by the reward received.
    It does not use the transition matrix (0.7/0.3).

    Parameters:
    - lr: [0, 1] Learning rate for both stages.
    - beta: [0, 10] Inverse temperature for softmax.
    """
    lr, beta = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Model-Free Q-values
    q_stage1_mf = np.zeros(2)      # Values for Spaceships A, U
    q_stage2_mf = np.zeros((2, 2)) # Values for Aliens (Planet X: W, S; Planet Y: P, H)

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial] # Actual planet arrived at

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # TD(0) update for Stage 1: Driven by the value of the chosen option in Stage 2
        # Note: In standard TD(0)/SARSA, we use Q(s', a').
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr * delta_stage1
        
        # TD(0) update for Stage 2: Driven by reward
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model-Based / Model-Free with Separate Learning Rates
Standard hybrid models often use a weighting parameter `w` to mix MB and MF values. However, they usually share a single learning rate. This model proposes that the brain might learn at different speeds for the first stage (spaceship choice) versus the second stage (alien choice), while mixing MB and MF strategies.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF with Stage-Specific Learning Rates.
    
    This model combines Model-Based and Model-Free values using a weighting parameter 'w'.
    Crucially, it allows for different learning rates for the first stage (transition prediction/value)
    and the second stage (reward prediction), capturing potential differences in plasticity 
    between abstract choices and concrete reward outcomes.

    Parameters:
    - lr_stage1: [0, 1] Learning rate for Stage 1 MF values.
    - lr_stage2: [0, 1] Learning rate for Stage 2 values.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weighting parameter (0 = Pure MF, 1 = Pure MB).
    """
    lr_stage1, lr_stage2, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy (Hybrid) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combined value: w * MB + (1-w) * MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # Update Stage 1 MF value using Stage 1 specific learning rate
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr_stage1 * delta_stage1
        
        # Update Stage 2 value using Stage 2 specific learning rate
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr_stage2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```