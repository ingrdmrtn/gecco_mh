Here are three new cognitive models for the two-step decision task, exploring different mechanisms like Model-Based learning, hybrid strategies, and distinct learning rates for different stages.

### Model 1: Pure Model-Based Learner
This model assumes the participant builds a map of the task structure. Instead of learning the value of the first-stage spaceships directly from experience, it calculates their value by combining the known transition probabilities with the learned values of the second-stage aliens.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Based Learner.
    
    This model calculates the value of first-stage actions (spaceships) by combining
    learned second-stage values (aliens) with the known transition matrix. It plans
    forward rather than caching values.

    Parameters:
    learning_rate: [0,1] Rate at which second-stage Q-values are updated.
    beta: [0,10] Inverse temperature for softmax.
    """
    learning_rate, beta = model_parameters
    n_trials = len(action_1)

    # Transition matrix: A(0)->X(0) is 0.7, A(0)->Y(1) is 0.3, etc.
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # We only need to learn Q-values for the second stage (aliens)
    q_stage2_mf = np.zeros((2, 2)) 

    for trial in range(n_trials):

        # --- Policy for the first choice (Model-Based) ---
        # Value of a spaceship = P(Planet X|Spaceship) * max(Val_X) + P(Planet Y|Spaceship) * max(Val_Y)
        max_q_stage2 = np.max(q_stage2_mf, axis=1) # Max value available at each planet
        q_stage1_mb = transition_matrix @ max_q_stage2 # Expected value calculation
        
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Policy for the second choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # No Stage 1 update needed for pure MB (it's calculated on the fly).
        
        # Stage 2 update (Standard Q-learning)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Learner (Model-Based + Model-Free)
This model represents the classic "Two-Step" analysis approach. It assumes the participant's first-stage choice is a weighted mixture of a Model-Based strategy (planning) and a Model-Free strategy (habit). A mixing parameter `w` determines the balance between these two systems.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Learner.
    
    The first-stage Q-values are a weighted sum of Model-Based values (computed via 
    transitions) and Model-Free values (computed via TD errors).

    Parameters:
    learning_rate: [0,1] Rate at which Q-values are updated.
    beta: [0,10] Inverse temperature for softmax.
    w: [0,1] Weight parameter. 1 = Pure Model-Based, 0 = Pure Model-Free.
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)      # Model-free values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Values for stage 2

    for trial in range(n_trials):

        # --- Policy for the first choice ---
        # Calculate Model-Based values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combine MB and MF values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Policy for the second choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # Stage 1 MF update (TD(0) style, updated by value of state reached)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Stage 2 update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Model-Free Learner with Eligibility Traces (TD(1))
This model is purely model-free but adds an "eligibility trace" parameter `lambda_eligibility`. Unlike the simple TD(0) model which only updates the first stage based on the second stage's value, this model allows the final reward to directly "propagate back" to update the first-stage choice. This bridges the gap between the action and the outcome.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Learner with Eligibility Traces (TD(lambda)).
    
    Allows the reward received at the end of the trial to directly influence the 
    value of the first-stage choice, mediated by the eligibility trace parameter.
    
    Parameters:
    learning_rate: [0,1] Rate at which Q-values are updated.
    beta: [0,10] Inverse temperature for softmax.
    lambda_eligibility: [0,1] Eligibility trace decay. 0 = TD(0), 1 = Monte Carlo.
    """
    learning_rate, beta, lambda_eligibility = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Policy for the first choice ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Policy for the second choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # 1. Prediction error at stage 1 (transition to stage 2)
        # Note: We use the value of the *chosen* second stage action for TD(0) part
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # 2. Prediction error at stage 2 (reward receipt)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # 3. Eligibility Trace Update
        # The stage 1 choice is also updated by the stage 2 prediction error, scaled by lambda
        q_stage1_mf[action_1[trial]] += learning_rate * lambda_eligibility * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```