Here are three new cognitive models based on the two-step task, exploring different mechanisms than the ones listed in the feedback.

### Model 1: Hybrid Model with Separate Learning Rates for Model-Based and Model-Free
This model posits that the Model-Based and Model-Free systems might learn at different speeds. For instance, the transition structure (MB) is often learned or conceptualized differently than the direct caching of reward values (MF). While the standard task assumes a fixed transition matrix, this model allows the *value updating* associated with the two systems to have distinct sensitivities (learning rates) to prediction errors, effectively decoupling the speed of updating stage 1 MF values from stage 2 values (which feed into the MB calculation).

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Separate Learning Rates for Stage 1 (MF) and Stage 2.
    
    This model decouples the learning rate for the first stage (pure MF) from the 
    second stage. Since the Model-Based value is derived from Stage 2 values, 
    `lr_stage2` implicitly controls the rate at which the Model-Based valuations update, 
    while `lr_stage1` controls the Model-Free Stage 1 updating.

    Parameters:
    lr_stage1: [0, 1] - Learning rate for Stage 1 Model-Free Q-values.
    lr_stage2: [0, 1] - Learning rate for Stage 2 Q-values (which feed into MB calc).
    beta: [0, 10] - Inverse temperature for softmax choice.
    w: [0, 1] - Weighting parameter (0 = pure Model-Free, 1 = pure Model-Based).
    perseveration: [0, 5] - Choice stickiness for Stage 1.
    """
    lr_stage1, lr_stage2, beta, w, perseveration = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        # Model-Based Value: Derived from current Stage 2 values + transition matrix
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Net Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add perseveration bonus
        if last_action_1 != -1:
            q_net[last_action_1] += perseveration

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updating ---
        # Stage 1 MF Update: Driven by Stage 2 value (TD(0)-like logic for MF path)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr_stage1 * delta_stage1
        
        # Stage 2 Update: Driven by Reward
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr_stage2 * delta_stage2
        
        last_action_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model with Reward-Dependent Learning Rates
This model investigates if the participant learns differently from positive outcomes (finding gold) versus negative outcomes (no gold). This "asymmetric learning" is a common finding in reinforcement learning tasks, suggesting distinct neural mechanisms for processing gains and losses (or lack of gains).

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Asymmetric Learning Rates (Positive vs Negative Prediction Errors).
    
    This model splits the learning rate into two parameters: one for when the 
    prediction error is positive (better than expected) and one for when it is 
    negative (worse than expected). This applies to the Stage 2 updates primarily,
    as that is where the external reward is received.

    Parameters:
    alpha_pos: [0, 1] - Learning rate for positive prediction errors.
    alpha_neg: [0, 1] - Learning rate for negative prediction errors.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - Weighting parameter (0 = pure Model-Free, 1 = pure Model-Based).
    perseveration: [0, 5] - Choice stickiness for Stage 1.
    """
    alpha_pos, alpha_neg, beta, w, perseveration = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_action_1 != -1:
            q_net[last_action_1] += perseveration
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updating ---
        
        # Stage 1 Update (using average alpha for simplicity in transfer, or could split too)
        # Here we use the specific alpha based on the sign of delta_stage1
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        lr_1 = alpha_pos if delta_stage1 > 0 else alpha_neg
        q_stage1_mf[action_1[trial]] += lr_1 * delta_stage1
        
        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        lr_2 = alpha_pos if delta_stage2 > 0 else alpha_neg
        q_stage2_mf[state_idx, action_2[trial]] += lr_2 * delta_stage2
        
        last_action_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model with Inverse Temperature (Beta) Modulation by Stage
This model proposes that the exploration/exploitation balance differs between the high-level planning stage (Stage 1) and the concrete implementation stage (Stage 2). Participants might be more exploratory when choosing a spaceship (Stage 1) but more exploitative (deterministic) when choosing an alien (Stage 2), or vice versa.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Separate Inverse Temperatures (Beta) for Stage 1 and Stage 2.
    
    Instead of a single noise parameter, this model allows the stochasticity of choice
    to vary between the spaceship choice (Stage 1) and the alien choice (Stage 2).

    Parameters:
    learning_rate: [0, 1] - Learning rate.
    beta_1: [0, 10] - Inverse temperature for Stage 1 choice.
    beta_2: [0, 10] - Inverse temperature for Stage 2 choice.
    w: [0, 1] - Weighting parameter (0 = pure Model-Free, 1 = pure Model-Based).
    perseveration: [0, 5] - Choice stickiness for Stage 1.
    """
    learning_rate, beta_1, beta_2, w, perseveration = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):

        # --- Stage 1 Policy (Uses beta_1) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_action_1 != -1:
            q_net[last_action_1] += perseveration
            
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy (Uses beta_2) ---
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updating ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        last_action_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```