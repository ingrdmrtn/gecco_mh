Here are three new cognitive models exploring different mechanisms in the two-step task, distinct from the previous TD(1) and basic Q-learning approaches.

### Model 1: Hybrid Model with Separate Learning Rates
This model implements the classic "Hybrid" reinforcement learning theory often used for this task. It assumes participants use a mixture of Model-Based (planning using the transition structure) and Model-Free (habitual caching of values) strategies. Crucially, this version allows for different learning rates for the first and second stages, hypothesizing that the abstract choice of a spaceship might be learned at a different speed than the concrete choice of an alien.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free RL with Separate Learning Rates.
    
    This model combines a Model-Free (MF) system, which learns values based on 
    direct reinforcement, and a Model-Based (MB) system, which computes values 
    by planning through the transition matrix. It distinguishes learning speeds 
    between the high-level planning stage (Stage 1) and the low-level action stage (Stage 2).

    Parameters:
    - lr_stage1: Learning rate for stage 1 MF values [0, 1].
    - lr_stage2: Learning rate for stage 2 MF values [0, 1].
    - beta: Inverse temperature for softmax choice [0, 10].
    - w: Mixing weight [0, 1]. 0 = Pure MF, 1 = Pure MB.
    """
    lr_stage1, lr_stage2, beta, w = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition matrix as per task description (70% common, 30% rare)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)      # Model-free values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for stage 2 (Planets x Aliens)

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        # Calculate Model-Based values: Transition prob * Max Q(stage 2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combine MF and MB values
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updating ---
        # Standard Q-learning (SARSA-like logic for stage 1, Q-learning for stage 2)
        
        # Update Stage 1 MF value using the value of the state actually reached
        # (This is a common variation where Stage 1 tracks the value of the chosen path)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr_stage1 * delta_stage1
        
        # Update Stage 2 MF value based on reward
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr_stage2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Perseveration Model
This model tests the hypothesis that choices are driven not just by value learning, but by a simple tendency to repeat (or switch) previous actions regardless of the outcome. This "perseveration" bias is common in human and animal data. It adds a "stickiness" parameter `pers_k` that boosts the value of the previously chosen action at stage 1.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Q-Learning with Choice Perseveration (Stickiness).
    
    This model assumes the participant is purely Model-Free but exhibits 
    a bias to repeat their previous Stage 1 choice, regardless of reward.
    This captures motor perseveration or cognitive inertia.

    Parameters:
    - learning_rate: Rate at which Q-values update [0, 1].
    - beta: Inverse temperature for softmax choice [0, 10].
    - pers_k: Perseveration bonus [0, 5]. Positive values encourage repetition, 
      negative values encourage switching.
    """
    learning_rate, beta, pers_k = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track previous choice (initialized to -1 or handled in loop)
    last_choice_1 = -1

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        # Add perseveration bonus to the Q-values temporarily for decision making
        q_stage1_biased = q_stage1_mf.copy()
        if last_choice_1 != -1:
            q_stage1_biased[last_choice_1] += pers_k

        exp_q1 = np.exp(beta * q_stage1_biased)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Store choice for next trial
        last_choice_1 = action_1[trial]
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updating ---
        # Standard TD(0) updates
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Asymmetric Learning Rate Model (Positive vs Negative)
This model investigates valence-dependent learning. It hypothesizes that the participant learns differently from positive prediction errors (better than expected outcomes) versus negative prediction errors (worse than expected outcomes). This is biologically motivated by dopamine signaling differences in D1 (Go) vs D2 (NoGo) pathways.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Asymmetric Learning Rate Model (Pos/Neg RPE).
    
    This model splits the learning rate into two components: one for positive 
    prediction errors (learning_rate_pos) and one for negative prediction 
    errors (learning_rate_neg). This allows the model to capture optimism 
    or pessimism biases in updating values.

    Parameters:
    - lr_pos: Learning rate for positive prediction errors [0, 1].
    - lr_neg: Learning rate for negative prediction errors [0, 1].
    - beta: Inverse temperature for softmax choice [0, 10].
    """
    lr_pos, lr_neg, beta = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updating ---
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        if delta_stage1 > 0:
            q_stage1_mf[action_1[trial]] += lr_pos * delta_stage1
        else:
            q_stage1_mf[action_1[trial]] += lr_neg * delta_stage1
        
        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        if delta_stage2 > 0:
            q_stage2_mf[state_idx, action_2[trial]] += lr_pos * delta_stage2
        else:
            q_stage2_mf[state_idx, action_2[trial]] += lr_neg * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```