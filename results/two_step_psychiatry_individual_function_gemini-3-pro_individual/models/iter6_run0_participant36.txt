Here are 3 new cognitive models for the two-step decision task, exploring different mechanisms than the pure model-based learner provided in the feedback.

### Model 1: Hybrid Learner (Model-Based + Model-Free)
This model combines Model-Based (planning) and Model-Free (habitual) systems. It uses a weighting parameter `w` to arbitrate between the two valuation systems for the first-stage choice. This is the classic "Daw et al. (2011)" style hybrid model structure.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Learner.
    Combines a planning-based valuation (MB) with a temporal-difference valuation (MF)
    for the first-stage choice.

    Parameters:
    learning_rate: [0,1] Update rate for Q-values.
    beta: [0,10] Inverse temperature for softmax.
    w: [0,1] Weighting parameter (0 = Pure MF, 1 = Pure MB).
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)       # Model-free values for Spaceships
    q_stage2_mf = np.zeros((2, 2))  # Model-free values for Aliens (Planets)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based Value: Planning using transition matrix + max stage 2 value
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value: Weighted sum of MB and MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        # Standard softmax on stage 2 Q-values
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # SARSA / TD(0) updates
        
        # Stage 1 MF Update: driven by the value of the state actually reached
        # Note: Standard TD(0) often uses q_stage2_mf[state_idx, action_2[trial]] as target
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Stage 2 MF Update: driven by reward
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Pure Model-Free with Eligibility Traces (TD(1))
This model ignores the transition structure entirely. Instead of just updating the second stage (Alien) and the first stage (Spaceship) independently, it uses an "eligibility trace" (or direct update) where the reward received at the second stage directly reinforces the first stage choice. This allows the model to learn about the first stage choice based on the final outcome, bypassing the intermediate state value.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free Learner with Eligibility Traces (TD(lambda=1)).
    The reward received at the second stage directly updates the Q-value of the 
    first-stage choice, treating the two steps as a single chain.

    Parameters:
    learning_rate: [0,1] Update rate.
    beta: [0,10] Inverse temperature.
    lambda_trace: [0,1] Eligibility trace decay (0 = TD(0), 1 = Monte Carlo).
    """
    learning_rate, beta, lambda_trace = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Choice ---
        # Pure MF choice
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        
        # Prediction Error at Stage 2 (Reward - Q_stage2)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Prediction Error at Stage 1 (Q_stage2 - Q_stage1)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]

        # Update Stage 2
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Update Stage 1:
        # 1. Direct TD update (TD(0) part): learning_rate * delta_stage1
        # 2. Eligibility trace update (TD(1) part): learning_rate * lambda * delta_stage2
        # This propagates the RPE from stage 2 back to stage 1
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1 + (learning_rate * lambda_trace * delta_stage2)
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Perseveration Model (Model-Based + Choice Stickiness)
This model extends the "best model so far" (Pure Model-Based) by adding a "stickiness" or perseveration parameter. Participants often repeat their previous choice regardless of reward (or switch), due to motor heuristics or cognitive inertia. This model captures that tendency explicitly.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Based Learner with Perseveration (Stickiness).
    Adds a bias to repeat the previous Stage 1 choice, independent of value.

    Parameters:
    learning_rate: [0,1] Update rate for second-stage Q-values.
    beta: [0,10] Inverse temperature.
    perseveration: [0,5] Bonus added to the logits of the previously chosen action.
                   (Positive = sticky, Negative = alternation).
    """
    learning_rate, beta, perseveration = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage2_mf = np.zeros((2, 2)) 
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1 # Initialize with no previous action

    for trial in range(n_trials):

        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Calculate logits
        logits_1 = beta * q_stage1_mb
        
        # Add perseveration bonus if a previous action exists
        if last_action_1 != -1:
            logits_1[last_action_1] += perseveration
            
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Store current action for next trial
        last_action_1 = action_1[trial]
        
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        # Standard softmax
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```