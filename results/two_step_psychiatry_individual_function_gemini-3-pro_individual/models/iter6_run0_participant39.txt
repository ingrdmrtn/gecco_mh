Here are three new cognitive models based on the two-step task structure, exploring different mechanisms for learning and decision-making.

### Model 1: Hybrid Learner with Separate Learning Rates for Stages
This model extends the standard hybrid (MB/MF) approach by acknowledging that learning about the value of spaceships (Stage 1) might happen at a different rate than learning about the value of aliens (Stage 2). For instance, the second stage is a direct bandit task, while the first stage is predictive.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid model with distinct learning rates for Stage 1 and Stage 2 updates.
    This allows the agent to update values for the immediate reward context (aliens)
    differently from the predictive context (spaceships).

    Parameters:
    lr_stage1: [0, 1] - Learning rate for Stage 1 Q-values (TD-learning).
    lr_stage2: [0, 1] - Learning rate for Stage 2 Q-values (Reward learning).
    beta: [0, 10] - Inverse temperature for softmax choice.
    w: [0, 1] - Weighting parameter (0 = pure Model-Free, 1 = pure Model-Based).
    """
    lr_stage1, lr_stage2, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)  # Model-free values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for stage 2 (Aliens)

    for trial in range(n_trials):

        # --- Stage 1 Policy (Hybrid) ---
        # 1. Calculate Model-Based values: V_MB(s1) = T * max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Combine MB and MF values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # 3. Softmax
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # Update Stage 1 MF values using TD-error and specific lr_stage1
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr_stage1 * delta_stage1
        
        # Update Stage 2 MF values using Reward prediction error and specific lr_stage2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr_stage2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Model-Based Learner with Forgetting (Decay)
This model assumes the agent relies purely on a Model-Based strategy (calculating values based on the transition structure) but has imperfect memory. Unchosen options in the second stage "decay" back to a neutral value (0.5), simulating memory loss or uncertainty growing over time for unvisited states.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Based learner with value decay (forgetting).
    Values for unchosen aliens decay toward 0.5 on every trial.
    
    Parameters:
    learning_rate: [0, 1] - Rate of updating values from rewards.
    beta: [0, 10] - Inverse temperature.
    decay_rate: [0, 1] - Rate at which unchosen Q-values decay to 0.5 (0=no decay, 1=instant forget).
    """
    learning_rate, beta, decay_rate = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Initialize Q-values at 0.5 (assumption of neutral expectation)
    q_stage2_mf = np.ones((2, 2)) * 0.5

    for trial in range(n_trials):

        # --- Stage 1 Policy (Pure Model-Based) ---
        # Calculate expected value of each spaceship based on current alien values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # 1. Update chosen alien
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # 2. Decay unchosen aliens across both planets
        for s in range(2):
            for a in range(2):
                if s == state_idx and a == action_2[trial]:
                    continue # Skip the chosen one
                # Decay towards 0.5
                q_stage2_mf[s, a] += decay_rate * (0.5 - q_stage2_mf[s, a])
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Asymmetric Learning Rate (Confirmation Bias)
This model posits that the participant updates their beliefs differently depending on whether the outcome was "good" (positive prediction error) or "bad" (negative prediction error). This is often called confirmation bias or asymmetric learning, where an agent might learn more from success than failure (or vice versa).

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free learner with Asymmetric Learning Rates (Confirmation Bias).
    Uses different learning rates for positive and negative prediction errors.
    
    Parameters:
    alpha_pos: [0, 1] - Learning rate for positive prediction errors (better than expected).
    alpha_neg: [0, 1] - Learning rate for negative prediction errors (worse than expected).
    beta: [0, 10] - Inverse temperature.
    """
    alpha_pos, alpha_neg, beta = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy (Model-Free) ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        if delta_stage1 > 0:
            q_stage1_mf[action_1[trial]] += alpha_pos * delta_stage1
        else:
            q_stage1_mf[action_1[trial]] += alpha_neg * delta_stage1

        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        if delta_stage2 > 0:
            q_stage2_mf[state_idx, action_2[trial]] += alpha_pos * delta_stage2
        else:
            q_stage2_mf[state_idx, action_2[trial]] += alpha_neg * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```