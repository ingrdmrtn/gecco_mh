Here are three new cognitive models exploring different mechanisms for learning and decision-making in the two-step task.

### Model 1: Hybrid Learner with Separate Learning Rates for Stages
This model builds on the classic hybrid (Model-Based + Model-Free) architecture but acknowledges that learning dynamics might differ between the abstract first stage (choosing a spaceship) and the concrete second stage (choosing an alien). It uses a mixing parameter `w` to blend MB and MF values at stage 1, but crucially, it uses distinct learning rates for stage 1 and stage 2 updates.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Model 1: Hybrid Learner with Separate Learning Rates.
    This model combines Model-Based (MB) and Model-Free (MF) strategies.
    It differentiates plasticity between the spaceship choice (stage 1) 
    and the alien choice (stage 2).
    
    Bounds:
    lr_1: [0, 1] - Learning rate for stage 1 Q-values.
    lr_2: [0, 1] - Learning rate for stage 2 Q-values.
    beta: [0, 10] - Inverse temperature for softmax.
    w: [0, 1] - Mixing weight (0 = pure MF, 1 = pure MB).
    """
    lr_1, lr_2, beta, w = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition matrix for the model-based component
    # (0.7 probability of common transition)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2) # MF values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # MF values for stage 2

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        # 1. Calculate Model-Based values
        # Max value of next stage states
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        # Bellman equation using known transition structure
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Combine MB and MF values
        q_stage1_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_stage1_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        a1 = action_1[trial]
        a2 = action_2[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        r = reward[trial]

        # --- Updating ---
        # Update Stage 1 MF values using TD-error from stage 2 transition
        # Note: We use lr_1 here
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr_1 * delta_stage1
        
        # Update Stage 2 MF values using reward
        # Note: We use lr_2 here
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += lr_2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Model-Free Learner with Eligibility Traces (TD(lambda))
This model assumes the participant is a pure Model-Free learner but uses eligibility traces to solve the credit assignment problem. Instead of just updating the immediately preceding state, the reward at the end of the trial "drifts back" to update the stage 1 choice directly, weighted by a decay parameter `lambda_decay`. This creates a direct link between the final reward and the initial spaceship choice without explicit model-based planning.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model 2: TD(lambda) Learner.
    Uses eligibility traces to update stage 1 values based on the final reward,
    bridging the gap between the spaceship choice and the outcome.
    
    Bounds:
    learning_rate: [0, 1] - Learning rate.
    beta: [0, 10] - Inverse temperature.
    lambda_decay: [0, 1] - Decay rate of the eligibility trace (0 = TD(0), 1 = Monte Carlo).
    """
    learning_rate, beta, lambda_decay = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        a1 = action_1[trial]
        a2 = action_2[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        r = reward[trial]

        # --- Updating with Eligibility Traces ---
        # 1. Calculate Prediction Errors
        # Error at transition from stage 1 to stage 2
        delta_1 = q_stage2[state_idx, a2] - q_stage1[a1]
        # Error at outcome
        delta_2 = r - q_stage2[state_idx, a2]
        
        # 2. Update Stage 2 (standard TD update)
        q_stage2[state_idx, a2] += learning_rate * delta_2
        
        # 3. Update Stage 1
        # It gets updated by its own immediate error (delta_1)
        # AND a portion of the stage 2 error (delta_2) carried back by lambda
        q_stage1[a1] += learning_rate * (delta_1 + lambda_decay * delta_2)
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Asymmetric Learning Rates (Reward vs. No-Reward)
This model investigates valence-dependent learning. It posits that the participant updates their expectations differently when they receive a coin (positive prediction error) versus when they receive nothing (negative prediction error). This asymmetry is applied to the second stage updates where the reward is received. It assumes a Model-Free architecture for simplicity to isolate the valence effect.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model 3: Asymmetric Learning Rates (Pos/Neg).
    This model separates learning rates for positive outcomes (finding gold)
    and negative outcomes (finding nothing/omission).
    
    Bounds:
    alpha_pos: [0, 1] - Learning rate when RPE is positive (reward received).
    alpha_neg: [0, 1] - Learning rate when RPE is negative (no reward).
    beta: [0, 10] - Inverse temperature.
    """
    alpha_pos, alpha_neg, beta = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        a1 = action_1[trial]
        a2 = action_2[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        r = reward[trial]

        # --- Updating ---
        
        # Stage 2 Prediction Error
        delta_2 = r - q_stage2[state_idx, a2]
        
        # Select learning rate based on sign of prediction error
        # Note: Since reward is 0 or 1, and Q is usually 0-1, 
        # r=1 usually implies delta > 0, r=0 usually implies delta < 0.
        if delta_2 >= 0:
            effective_lr = alpha_pos
        else:
            effective_lr = alpha_neg
            
        q_stage2[state_idx, a2] += effective_lr * delta_2
        
        # Stage 1 Update (Standard TD)
        # We use the average of the two alphas for the transition update 
        # as there is no extrinsic reward here, just value transfer.
        avg_lr = (alpha_pos + alpha_neg) / 2
        delta_1 = q_stage2[state_idx, a2] - q_stage1[a1]
        q_stage1[a1] += avg_lr * delta_1
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```