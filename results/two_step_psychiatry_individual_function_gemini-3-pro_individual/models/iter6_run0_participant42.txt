Here are three new cognitive models for the two-step decision task, designed to explore mechanisms not fully covered by the previous parameter combinations (like distinct inverse temperatures for stages, mixture models of strategies, or separate learning rates for first vs. second stage updates).

### Model 1: Hybrid Model with Independent Stage Temperatures
This model implements the classic hybrid reinforcement learning agent (Daw et al., 2011) which mixes Model-Based (MB) and Model-Free (MF) values. However, unlike previous attempts that used a single `beta` for both stages, this model separates the exploration/exploitation trade-off for the first stage (spaceship choice) and the second stage (alien choice). The first stage is a complex conflict between MB and MF systems, while the second stage is purely MF, so they may require different sensitivities to value differences.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based/Model-Free RL with Independent Stage Temperatures.
    
    This model assumes the participant mixes MB and MF strategies at the first stage
    weighted by `w`. Crucially, it allows for different "inverse temperatures" (beta)
    at Stage 1 and Stage 2, acknowledging that the first choice (planning) might
    be noisier or more deterministic than the second choice (simple bandit).

    Bounds:
    learning_rate: [0, 1] - Learning rate for value updates.
    beta1: [0, 10] - Inverse temperature for Stage 1 (Spaceship choice).
    beta2: [0, 10] - Inverse temperature for Stage 2 (Alien choice).
    w: [0, 1] - Weighting parameter (0 = Pure MF, 1 = Pure MB).
    """
    learning_rate, beta1, beta2, w = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition matrix as per task description (A->X 0.7, U->Y 0.7)
    # Rows: Spaceships (0, 1), Cols: Planets (0, 1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)      # Model-free values for spaceships
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for aliens (State x Action)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based valuation: V_MB(s1) = sum(P(s2|s1) * max_a(Q(s2, a)))
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid valuation
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax selection for Stage 1 using beta1
        exp_q1 = np.exp(beta1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        # Simple Model-Free selection using beta2
        exp_q2 = np.exp(beta2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning ---
        # SARSA(0) / TD(0) updates
        
        # Stage 1 MF update (using Q-value of chosen stage 2 action as target)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Stage 2 MF update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Pure Model-Free with Separate Learning Rates for Stages
This model drops the model-based component entirely but hypothesizes that learning happens at different speeds for the two stages. The first stage involves learning the value of a spaceship (which is an abstract predictor of future states), while the second stage involves learning the direct reward probability of an alien. These distinct cognitive demands might be supported by different learning rates.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free RL with Separate Learning Rates for Stage 1 and Stage 2.
    
    This model assumes no model-based planning. Instead, it focuses on the 
    temporal difference learning dynamics, allowing the agent to update 
    spaceship values (Stage 1) at a different rate than alien values (Stage 2).
    
    Bounds:
    alpha1: [0, 1] - Learning rate for Stage 1 updates (Spaceships).
    alpha2: [0, 1] - Learning rate for Stage 2 updates (Aliens).
    beta: [0, 10] - Inverse temperature (shared across stages).
    """
    alpha1, alpha2, beta = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        
        # Update Stage 1 value using alpha1
        # TD error based on the value of the state reached
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += alpha1 * delta_stage1
        
        # Update Stage 2 value using alpha2
        # TD error based on reward received
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += alpha2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model with Choice Perseverance (Stickiness)
Previous feedback indicated interest in perseverance. This model integrates the classic Hybrid (MB/MF) structure with a general choice perseverance parameter (stickiness) on the first stage. Unlike the "Outcome-Specific" model in the feedback, this stickiness is independent of reward historyâ€”it captures a basic motor or cognitive inertia to repeat the last spaceship choice regardless of the outcome.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based/Model-Free RL with Choice Perseverance (Stickiness).
    
    Combines the weighted Model-Based/Model-Free valuation with a 'stickiness'
    parameter. This parameter adds a bonus to the Q-value of the previously 
    chosen spaceship, capturing a tendency to repeat choices regardless of reward.
    
    Bounds:
    learning_rate: [0, 1] - Learning rate.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - Weighting parameter (0 = Pure MF, 1 = Pure MB).
    stickiness: [0, 5] - Bonus added to the previously chosen action (Stage 1).
    """
    learning_rate, beta, w, stickiness = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        # Calculate MB values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combine MF and MB
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Apply stickiness bonus to the net Q-values
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Store for next trial
        last_action_1 = action_1[trial]
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```