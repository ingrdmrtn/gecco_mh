Here are three new cognitive models for the two-step decision task, exploring different mechanisms and parameter combinations not yet fully tested in the provided list.

### Model 1: Hybrid Learner with Separate Model-Based/Model-Free Learning Rates
This model implements the classic hybrid reinforcement learning theory but specifically tests the hypothesis that the brain might update model-free (habitual) and model-based (goal-directed) values at different speeds. Instead of a single mixing weight `w` or a single learning rate, it uses separate learning rates for the two systems to see if one is more volatile than the other.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner with Separate Model-Based and Model-Free Learning Rates.
    
    This model assumes the participant maintains two separate value estimates:
    1. A Model-Free (MF) value based on direct reinforcement history.
    2. A Model-Based (MB) value calculated from the transition matrix and second-stage values.
    Crucially, it allows these two systems to learn at different rates.
    
    Parameters:
    lr_mf: [0,1] Learning rate for Model-Free TD updates.
    lr_mb: [0,1] Learning rate for Model-Based updates (updating the second-stage values used for planning).
    beta: [0,10] Inverse temperature for softmax choice.
    w: [0,1] Mixing weight (0 = pure MF, 1 = pure MB).
    """
    lr_mf, lr_mb, beta, w = model_parameters
    n_trials = len(action_1)
    
    # Fixed transition matrix for the task (70% common, 30% rare)
    # Row 0: Space A -> [Planet X (70%), Planet Y (30%)]
    # Row 1: Space B -> [Planet X (30%), Planet Y (70%)]
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)       # MF values for stage 1
    q_stage2 = np.zeros((2, 2))     # Values for stage 2 (shared/base for MB calculation)
    
    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        
        # Model-Based value calculation: V_MB(s1) = T * max(Q(s2))
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated value: Q_net = w * Q_MB + (1-w) * Q_MF
        q_net_s1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_s1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        s_idx = state[trial] # 0 or 1 (Planet X or Y)
        
        # --- Stage 2 Decision ---
        # Stage 2 is purely model-free (terminal state)
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]
        
        # --- Learning ---
        
        # 1. Model-Free Update for Stage 1 (TD(1) style - direct reinforcement from outcome)
        # Note: In standard TD(1), we update Q(s1) based on reward r.
        # Delta = r - Q_MF(s1)
        delta_mf = r - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr_mf * delta_mf
        
        # 2. Model-Based Update for Stage 2 (Planning values)
        # The MB system relies on accurate Stage 2 values to plan.
        delta_mb = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += lr_mb * delta_mb

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

```

### Model 2: Delta-Rule Model with Forgetting (Decay)
This model posits that learning isn't just about updating the chosen option; it's also about the passive decay of non-chosen options. In volatile environments (like the drifting reward probabilities in this task), older information becomes less reliable. This model introduces a decay parameter that pulls unchosen action values toward zero (or a baseline), effectively modeling "forgetting."

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Learner with Passive Decay (Forgetting).
    
    Standard Q-learning updates the chosen action. This model adds a decay 
    process where unchosen actions slowly revert to 0 (or lose intensity), 
    mimicking memory trace decay.
    
    Parameters:
    learning_rate: [0,1] Update rate for chosen actions.
    decay_rate: [0,1] Rate at which unchosen action values decay toward 0.
    beta: [0,10] Inverse temperature.
    """
    learning_rate, decay_rate, beta = model_parameters
    n_trials = len(action_1)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        s_idx = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]

        # --- Updates with Decay ---
        
        # Update Stage 1
        # Chosen option: Standard TD update
        # We use a simple TD(1) approximation here connecting Stage 1 to Reward directly for MF simplicity
        q_stage1[a1] += learning_rate * (r - q_stage1[a1])
        
        # Unchosen option: Decay
        unchosen_a1 = 1 - a1
        q_stage1[unchosen_a1] *= (1 - decay_rate)
        
        # Update Stage 2
        # Chosen option
        q_stage2[s_idx, a2] += learning_rate * (r - q_stage2[s_idx, a2])
        
        # Unchosen option at the current state
        unchosen_a2 = 1 - a2
        q_stage2[s_idx, unchosen_a2] *= (1 - decay_rate)
        
        # Note: We do not decay the *other* state's values in this specific implementation 
        # as the participant was not there, but one could argue for that as well. 
        # Here we stick to local decay.

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Risk-Sensitive Reinforcement Learning
This model investigates whether the participant is sensitive not just to the expected value (mean) of the reward, but also to the variance or "risk." It introduces a risk-sensitivity parameter `rho`. If `rho` is positive, the participant is risk-seeking (values variance); if negative, risk-averse. This is often modeled by transforming the prediction error.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Risk-Sensitive Reinforcement Learning.
    
    This model transforms the prediction error based on a risk parameter `rho`.
    Instead of V <- V + alpha * delta, it uses:
    V <- V + alpha * (delta * (1 +/- rho))
    effectively weighing positive and negative prediction errors differently.
    
    Parameters:
    learning_rate: [0,1] Base learning rate.
    beta: [0,10] Inverse temperature.
    rho: [0,1] Risk sensitivity parameter. 
         This implementation treats rho as an asymmetry parameter:
         If delta > 0 (outcome better than expected), weight by (1 + rho).
         If delta < 0 (outcome worse than expected), weight by (1 - rho).
         If rho > 0, gains are emphasized (risk seeking/optimistic).
         If rho < 0, losses are emphasized (risk averse/pessimistic).
         Note: To keep bounds [0,1], we map input `rho` to range [-1, 1] internally 
         or we can define rho as [0,1] and say rho > 0.5 is optimistic. 
         Here, let's use rho directly in [0,1] as a "Gain Emphasis" factor.
         Weight for positive PE = 2 * rho.
         Weight for negative PE = 2 * (1 - rho).
         (If rho=0.5, weights are 1.0 and 1.0 -> standard Q learning).
    """
    learning_rate, beta, rho = model_parameters
    n_trials = len(action_1)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    # Calculate weights based on rho
    # rho = 0.5 -> neutral
    # rho > 0.5 -> optimistic (weights positive PE more)
    # rho < 0.5 -> pessimistic (weights negative PE more)
    w_pos = 2 * rho
    w_neg = 2 * (1 - rho)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        s_idx = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]

        # --- Risk-Sensitive Updates ---
        
        # Stage 2 Update
        delta2 = r - q_stage2[s_idx, a2]
        if delta2 >= 0:
            q_stage2[s_idx, a2] += learning_rate * w_pos * delta2
        else:
            q_stage2[s_idx, a2] += learning_rate * w_neg * delta2
            
        # Stage 1 Update (TD(1) logic for simplicity in this MF model)
        delta1 = r - q_stage1[a1]
        if delta1 >= 0:
            q_stage1[a1] += learning_rate * w_pos * delta1
        else:
            q_stage1[a1] += learning_rate * w_neg * delta1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```