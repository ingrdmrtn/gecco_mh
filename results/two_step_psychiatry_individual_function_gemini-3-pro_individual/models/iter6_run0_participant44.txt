Here are three new cognitive models based on the provided template and data structure.

### Model 1: Hybrid Model-Based/Model-Free with Perseverance
This model combines Model-Based (planning) and Model-Free (habitual) value estimation. Crucially, it adds a **Perseverance** (stickiness) parameter. This accounts for the common human tendency to repeat the previous choice regardless of reward or state, which is distinct from reinforcement learning.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Reinforcement Learning with Perseverance.
    
    This model posits that the participant's choice is a weighted mixture of 
    Model-Based (planning using the transition matrix) and Model-Free values.
    It includes a 'perseverance' bonus that increases the likelihood of 
    repeating the immediately preceding Stage 1 action.

    Parameters:
    learning_rate: [0,1] - Update rate for Q-values.
    beta: [0,10] - Inverse temperature for softmax.
    w: [0,1] - Weighting parameter (0 = Pure MF, 1 = Pure MB).
    perseverance: [0,5] - Bonus added to the logits of the previously chosen action.
    """
    learning_rate, beta, w, perseverance = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Model-Free values
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action

    prev_action_1 = -1

    for trial in range(n_trials):

        # --- Policy for the first choice ---
        # 1. Model-Based Value: Max Q2 projected through transition matrix
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Hybrid Value: Weighted sum
        q_hybrid = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # 3. Perseverance Bonus
        logits = beta * q_hybrid
        if prev_action_1 != -1:
            logits[prev_action_1] += perseverance

        exp_q1 = np.exp(logits)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        a1 = action_1[trial]
        prev_action_1 = a1 # Store for next trial

        # --- Policy for the second choice ---
        # Standard Softmax on Stage 2 MF values
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]

        # --- Updating ---
        # SARSA-style TD error for Stage 1 (Model-Free only)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] = q_stage1_mf[a1] + learning_rate * delta_stage1
        
        # Reward Prediction Error for Stage 2
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] = q_stage2_mf[state_idx, a2] + learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Model-Free TD(lambda) with Eligibility Traces
This model ignores Model-Based planning but introduces an **Eligibility Trace** parameter (`lambda`). In standard TD-0 (the "Best Model" so far), the Stage 1 value is only updated by the value of the Stage 2 state. In TD($\lambda$), the Stage 1 value is *also* updated by the final reward received at Stage 2. This allows the reward to "propagate" back to the first choice faster.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Reinforcement Learning with Eligibility Traces (TD-Lambda).
    
    This model assumes the participant is purely model-free but uses eligibility 
    traces to credit the first stage choice with the final outcome. 
    If lambda > 0, the reward prediction error at the second stage (delta_stage2) 
    propagates back to update the first stage Q-values directly.

    Parameters:
    learning_rate: [0,1] - Update rate for Q-values.
    beta: [0,10] - Inverse temperature for softmax.
    lambda_param: [0,1] - Eligibility trace decay parameter (0=TD-0, 1=Monte Carlo).
    """
    learning_rate, beta, lambda_param = model_parameters
    n_trials = len(action_1)
  
    # We ignore the transition matrix for pure MF
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Policy for the first choice ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        a1 = action_1[trial]

        # --- Policy for the second choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]

        # --- Updating ---
        
        # 1. Prediction error at Stage 1 (Action 1 -> State 2)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        
        # 2. Prediction error at Stage 2 (Action 2 -> Reward)
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        
        # Update Stage 2 (Standard)
        q_stage2_mf[state_idx, a2] = q_stage2_mf[state_idx, a2] + learning_rate * delta_stage2
        
        # Update Stage 1 (TD-Lambda logic)
        # The update includes the immediate error (delta1) AND a weighted portion of the future error (delta2)
        combined_error = delta_stage1 + (lambda_param * delta_stage2)
        q_stage1_mf[a1] = q_stage1_mf[a1] + learning_rate * combined_error
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Dual-Learning Rate Model-Free
This model hypothesizes that the participant learns about the spaceships (Stage 1) and the aliens (Stage 2) at different speeds. It splits the learning rate into `alpha_1` (for spaceships) and `alpha_2` (for aliens). This is useful if the participant believes the spaceship transitions are stable but the alien coin probabilities are volatile.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Dual-Learning Rate Model-Free Reinforcement Learning.
    
    This model assumes the participant is model-free but maintains separate 
    learning rates for the first stage (spaceships) and the second stage (aliens).
    This allows the model to capture differences in plasticity between the two 
    levels of the task hierarchy.

    Parameters:
    alpha_1: [0,1] - Learning rate for Stage 1 (Spaceships).
    alpha_2: [0,1] - Learning rate for Stage 2 (Aliens).
    beta: [0,10] - Inverse temperature for softmax.
    """
    alpha_1, alpha_2, beta = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Policy for the first choice ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        a1 = action_1[trial]

        # --- Policy for the second choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]
  
        # --- Updating ---
        
        # Update Stage 1 using alpha_1
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] = q_stage1_mf[a1] + alpha_1 * delta_stage1
        
        # Update Stage 2 using alpha_2
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] = q_stage2_mf[state_idx, a2] + alpha_2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```