Here are three new cognitive models based on the two-step task structure, introducing variations in learning rates and parameter combinations not previously explored in the feedback list.

### Model 1: Hybrid Model with Separate Learning Rates for Stages
This model distinguishes between the learning rate used for the first-stage (spaceship choice) and the second-stage (alien choice). This hypothesis suggests that participants might update their values differently based on the immediate transition versus the final reward outcome.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Separate Learning Rates for Stages.
    
    This model assumes the participant learns at different speeds for the 
    first-stage prediction error (transition structure) and the second-stage 
    prediction error (reward outcome).
    
    Parameters:
    lr_stage1: [0, 1] - Learning rate for first-stage (TD(0) update).
    lr_stage2: [0, 1] - Learning rate for second-stage (reward update).
    beta: [0, 10] - Inverse temperature for softmax choice.
    w: [0, 1] - Weighting parameter (1 = fully Model-Based, 0 = fully Model-Free).
    """
    lr_stage1, lr_stage2, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # policy for the second choice
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Update stage 1 using lr_stage1
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr_stage1 * delta_stage1
        
        # Update stage 2 using lr_stage2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr_stage2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Pure Model-Free with Eligibility Trace and Forgetting
This model abandons the Model-Based component (`w=0`) to focus purely on Model-Free mechanisms. It introduces a decay/forgetting rate for unchosen actions, combined with an eligibility trace parameter. This tests if the participant is purely reactive but has memory decay for options they don't explore.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free with Eligibility Trace and Passive Decay.
    
    This model is purely Model-Free (no transition matrix usage). It updates 
    values using TD-lambda (eligibility traces) and includes a decay parameter 
    where unchosen action values slowly rot back toward 0.
    
    Parameters:
    learning_rate: [0, 1] - Update rate for chosen actions.
    beta: [0, 10] - Inverse temperature.
    lambda_eligibility: [0, 1] - Eligibility trace parameter connecting stage 2 reward to stage 1 choice.
    decay_rate: [0, 1] - Rate at which unchosen Q-values decay to 0.5 (neutral).
    """
    learning_rate, beta, lambda_eligibility, decay_rate = model_parameters
    n_trials = len(action_1)
  
    # No transition matrix used here (Pure MF)
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2) + 0.5 # Initialize to neutral
    q_stage2_mf = np.zeros((2, 2)) + 0.5 # Initialize to neutral

    for trial in range(n_trials):

        # policy for the first choice (Pure MF)
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # policy for the second choice
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Prediction errors
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Update chosen Stage 1
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1 + learning_rate * lambda_eligibility * delta_stage2
        
        # Decay unchosen Stage 1
        unchosen_1 = 1 - action_1[trial]
        q_stage1_mf[unchosen_1] = (1 - decay_rate) * q_stage1_mf[unchosen_1] + decay_rate * 0.5
        
        # Update chosen Stage 2
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Decay unchosen Stage 2 (in the current state only)
        unchosen_2 = 1 - action_2[trial]
        q_stage2_mf[state_idx, unchosen_2] = (1 - decay_rate) * q_stage2_mf[state_idx, unchosen_2] + decay_rate * 0.5
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model with Stickiness (Perseveration) on Both Stages
Previous feedback explored perseveration on the first stage. This model extends choice stickiness to the second stage as well. It assumes that participants might simply repeat their choice of alien (if they land on the same planet again) regardless of value, in addition to repeating their spaceship choice.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Multi-Stage Stickiness.
    
    This model includes "stickiness" (perseveration) parameters for BOTH 
    the first stage (spaceship) and the second stage (alien). 
    
    Parameters:
    learning_rate: [0, 1] - Update rate.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - MB/MF weighting.
    stick_1: [0, 5] - Bonus added to previously chosen spaceship.
    stick_2: [0, 5] - Bonus added to previously chosen alien (context-dependent).
    """
    learning_rate, beta, w, stick_1, stick_2 = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    last_action_1 = -1
    # Track last action taken in specific states (aliens chosen per planet)
    last_action_2_per_state = [-1, -1] 

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Apply stickiness to stage 1
        if last_action_1 != -1:
            q_net_1[last_action_1] += stick_1
            
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # policy for the second choice
        q_net_2 = q_stage2_mf[state_idx].copy()
        
        # Apply stickiness to stage 2 if we have visited this state before
        if last_action_2_per_state[state_idx] != -1:
             q_net_2[last_action_2_per_state[state_idx]] += stick_2

        exp_q2 = np.exp(beta * q_net_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Update
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Update history
        last_action_1 = action_1[trial]
        last_action_2_per_state[state_idx] = action_2[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```