Here are three new cognitive models based on the two-step task structure, exploring different mechanisms for value updating and decision-making.

### Model 1: Hybrid Model-Based/Model-Free with Eligibility Traces (TD($\lambda$))
This model introduces an eligibility trace parameter (`lambda_param`). Instead of purely updating based on the final reward (TD(1)) or just the next state's value (TD(0)), this model allows the agent to weigh the direct reinforcement against the temporal difference error from the second stage. It combines this with a weighted mixture of Model-Based (MB) and Model-Free (MF) control.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free RL with Eligibility Traces (TD(lambda)).
    
    This model mixes model-based planning (using the transition matrix) and 
    model-free learning. The model-free component uses an eligibility trace 
    parameter (lambda) to modulate how much the stage 1 value is updated by 
    the stage 2 prediction error versus the final reward outcome.

    Parameters:
    learning_rate: [0, 1] - Learning rate for Q-value updates.
    beta: [0, 10] - Inverse temperature for softmax choice.
    w: [0, 1] - Weighting parameter (0 = Pure MF, 1 = Pure MB).
    lambda_param: [0, 1] - Eligibility trace decay (0 = TD(0), 1 = TD(1)/Monte Carlo).
    """
    learning_rate, beta, w, lambda_param = model_parameters
    n_trials = len(action_1)
    
    # Fixed transition matrix as described in task: 
    # Row 0 (Spaceship A) -> 0.7 Planet X, 0.3 Planet Y
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)       # Model-free values for Stage 1
    q_stage2_mf = np.zeros((2, 2))  # Values for Stage 2 (Aliens)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # Model-Based Value Calculation: V_MB(s1) = T * max(Q(s2))
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value: Q_net = w * Q_MB + (1-w) * Q_MF
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax for Stage 1
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Transition ---
        state_idx = state[trial] # Planet arrived at
        
        # --- Stage 2 Decision ---
        # Standard Model-Free choice at Stage 2
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning ---
        # 1. Prediction Error at Stage 1 (TD(0) component)
        # The difference between the value of the state we landed in and the spaceship we chose
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        # 2. Prediction Error at Stage 2
        # The difference between the reward received and the alien chosen
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Update Stage 2 values (standard Rescorla-Wagner)
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Update Stage 1 values using Eligibility Trace logic
        # Update = alpha * (delta_1 + lambda * delta_2)
        q_stage1_mf[action_1[trial]] += learning_rate * (delta_stage1 + lambda_param * delta_stage2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Pure Model-Based with Perseverance
This model assumes the participant relies exclusively on the transition structure (Model-Based) to calculate expected values for the first stage. However, it adds a "perseverance" (or stickiness) parameter. This captures the tendency to repeat the previous choice regardless of reward history, a common phenomenon in sequential decision-making tasks.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Based RL with Choice Perseverance (Stickiness).
    
    This model calculates stage 1 values purely by projecting stage 2 values 
    through the transition matrix. It ignores direct model-free reinforcement 
    of stage 1 actions. However, it includes a 'stickiness' parameter that 
    biases the agent to repeat the immediately preceding stage 1 choice.

    Parameters:
    learning_rate: [0, 1] - Rate for updating stage 2 Q-values.
    beta: [0, 10] - Inverse temperature.
    stickiness: [0, 5] - Bonus added to the log-odds of repeating the last choice.
    """
    learning_rate, beta, stickiness = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Only need Stage 2 values for pure MB (Stage 1 is derived)
    q_stage2_mf = np.zeros((2, 2)) 
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1 # Initialize with an impossible action

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # Calculate MB values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Apply Stickiness
        # We add the stickiness bonus directly to the Q-value (or logit) of the previous choice
        q_stage1_effective = q_stage1_mb.copy()
        if last_action_1 != -1:
            q_stage1_effective[last_action_1] += stickiness
        
        exp_q1 = np.exp(beta * q_stage1_effective)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Record this action for the next trial
        last_action_1 = action_1[trial]
        
        # --- Transition ---
        state_idx = state[trial]

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning ---
        # Update Stage 2 values based on reward
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Note: No Stage 1 update loop here because it's Pure MB (values are derived dynamically)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Asymmetric Learning Rates (Positive/Negative) Model-Free
This model posits that the participant is purely Model-Free (ignoring transition probabilities) but processes positive and negative prediction errors differently. It uses two separate learning rates: `alpha_pos` for better-than-expected outcomes and `alpha_neg` for worse-than-expected outcomes. This can explain risk-seeking or risk-averse behaviors driven by how strongly good vs. bad news is integrated.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free RL with Asymmetric Learning Rates (Pos/Neg).
    
    This model is a TD(1) Model-Free learner that updates values based on 
    the final reward. Crucially, it uses different learning rates for 
    positive prediction errors (alpha_pos) and negative prediction errors 
    (alpha_neg), allowing for confirmation bias or pessimism.

    Parameters:
    alpha_pos: [0, 1] - Learning rate for positive prediction errors (delta > 0).
    alpha_neg: [0, 1] - Learning rate for negative prediction errors (delta < 0).
    beta: [0, 10] - Inverse temperature.
    """
    alpha_pos, alpha_neg, beta = model_parameters
    n_trials = len(action_1)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning ---
        # We use a TD(1)-like update logic where stage 1 is updated by the final reward
        
        # Update Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        if delta_stage2 >= 0:
            q_stage2_mf[state_idx, action_2[trial]] += alpha_pos * delta_stage2
        else:
            q_stage2_mf[state_idx, action_2[trial]] += alpha_neg * delta_stage2
            
        # Update Stage 1 (Direct Reinforcement from Reward)
        delta_stage1 = reward[trial] - q_stage1_mf[action_1[trial]]
        if delta_stage1 >= 0:
            q_stage1_mf[action_1[trial]] += alpha_pos * delta_stage1
        else:
            q_stage1_mf[action_1[trial]] += alpha_neg * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```