Here are 3 new cognitive models for the two-step decision task.

### Model 1: Hybrid MB/MF with Separate Stage Sensitivity
This model hypothesizes that the level of "exploration noise" (inverse temperature) differs between the high-level planning stage (Stage 1) and the low-level bandit stage (Stage 2). For instance, participants might be more precise in their bandit choices (Stage 2) while being more exploratory or noisy in their spaceship choices (Stage 1).

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF with Separate Stage Inverse Temperatures.
    
    Allows for different levels of choice stochasticity (beta) at Stage 1 
    (choosing a spaceship) versus Stage 2 (choosing an alien).
    
    Parameters:
    learning_rate: [0, 1] Update rate for Q-values.
    beta_stage1: [0, 10] Inverse temperature for Stage 1 choice.
    beta_stage2: [0, 10] Inverse temperature for Stage 2 choice.
    w: [0, 1] Weight for Model-Based control (1 = Pure MB, 0 = Pure MF).
    """
    learning_rate, beta_stage1, beta_stage2, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Use beta_stage1 for the first decision
        exp_q1 = np.exp(beta_stage1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        # Use beta_stage2 for the second decision
        exp_q2 = np.exp(beta_stage2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid MB/MF with Memory Decay
This model incorporates a forgetting mechanism. In complex tasks, participants may not maintain Q-values indefinitely. This model assumes that the values of unchosen actions "decay" or fade over time (towards 0), representing passive forgetting or a return to a baseline expectation.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF with Q-value Memory Decay.
    
    Introduces a decay parameter. On each trial, the Q-values of unchosen 
    actions decay towards zero, simulating forgetting in working memory.
    
    Parameters:
    learning_rate: [0, 1] Update rate for chosen Q-values.
    beta: [0, 10] Inverse temperature for softmax.
    w: [0, 1] Weight for Model-Based control.
    decay_rate: [0, 1] Rate at which unchosen Q-values decay to 0 (0 = no decay, 1 = instant forget).
    """
    learning_rate, beta, w, decay_rate = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Update Stage 1
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Decay unchosen Stage 1
        unchosen_1 = 1 - action_1[trial]
        q_stage1_mf[unchosen_1] *= (1 - decay_rate)

        # Update Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Decay unchosen Stage 2 (in the current state)
        unchosen_2 = 1 - action_2[trial]
        q_stage2_mf[state_idx, unchosen_2] *= (1 - decay_rate)
        
        # Decay Stage 2 values for the unvisited state (both actions)
        unvisited_state = 1 - state_idx
        q_stage2_mf[unvisited_state, :] *= (1 - decay_rate)
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid MB/MF with Dual Perseveration
This model extends the concept of "stickiness" to both stages of the task. It posits that motor perseveration or habit formation occurs not just when choosing a spaceship, but also when choosing an alien within a specific planet context.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF with Dual Perseveration (Stage 1 and Stage 2).
    
    Includes separate stickiness parameters for the first choice (spaceship)
    and the second choice (alien), capturing habit formation at both levels.
    
    Parameters:
    learning_rate: [0, 1] Update rate for Q-values.
    beta: [0, 10] Inverse temperature for softmax.
    w: [0, 1] Weight for Model-Based control.
    pers_1: [0, 5] Perseveration bonus for Stage 1 (Spaceship).
    pers_2: [0, 5] Perseveration bonus for Stage 2 (Alien).
    """
    learning_rate, beta, w, pers_1, pers_2 = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    # Track last action taken in each state (planet) for Stage 2 perseveration
    last_action_2_per_state = [-1, -1] 

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net
        if last_action_1 != -1:
            logits_1[last_action_1] += pers_1
            
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # policy for the second choice
        logits_2 = beta * q_stage2_mf[state_idx]
        
        # Apply perseveration if we have visited this state before
        if last_action_2_per_state[state_idx] != -1:
            logits_2[last_action_2_per_state[state_idx]] += pers_2
            
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Update history
        last_action_1 = action_1[trial]
        last_action_2_per_state[state_idx] = action_2[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```