Here are three new cognitive models exploring different mechanisms for learning and decision-making in the two-step task.

### Model 1: Asymmetric Learning Rates (Positive vs. Negative Prediction Errors)
This model investigates whether the participant learns differently from positive outcomes (better than expected) versus negative outcomes (worse than expected). This is common in psychiatric populations where reward sensitivity might differ from punishment sensitivity.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free RL with Asymmetric Learning Rates for Positive and Negative Prediction Errors.
    
    This model splits the learning rate into two separate parameters:
    1. alpha_pos: Learning rate for positive prediction errors (RPE > 0).
    2. alpha_neg: Learning rate for negative prediction errors (RPE < 0).
    
    This allows the model to capture biases towards optimism (learning more from gains)
    or pessimism (learning more from disappointments).

    Parameters:
    alpha_pos: [0, 1] - Learning rate for positive RPEs.
    alpha_neg: [0, 1] - Learning rate for negative RPEs.
    beta: [0, 10] - Inverse temperature for softmax choice.
    w: [0, 1] - Weighting parameter (0 = Pure MF, 1 = Pure MB).
    """
    alpha_pos, alpha_neg, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        state_idx = state[trial]
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]

        # --- Policy for the first choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Policy for the second choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # --- Update Stage 1 (TD-0) ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        if delta_stage1 > 0:
            q_stage1_mf[a1] += alpha_pos * delta_stage1
        else:
            q_stage1_mf[a1] += alpha_neg * delta_stage1
            
        # --- Update Stage 2 ---
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        if delta_stage2 > 0:
            q_stage2_mf[state_idx, a2] += alpha_pos * delta_stage2
        else:
            q_stage2_mf[state_idx, a2] += alpha_neg * delta_stage2
        
        # Note: No eligibility trace in this specific variant to isolate asymmetry effects.

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Choice Perseveration with Hybrid Control
This model adds a "stickiness" or perseveration parameter. Participants often have a tendency to repeat their previous choice regardless of reward (motor perseveration) or switch frequently. This model captures that low-level bias alongside the high-level planning.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model (MB/MF) with Choice Perseveration (Stickiness).
    
    Includes a 'perseveration' parameter that biases the stage-1 choice 
    towards the action taken on the immediately preceding trial.
    
    Parameters:
    learning_rate: [0, 1] - Rate at which Q-values are updated.
    beta: [0, 10] - Inverse temperature for softmax choice.
    w: [0, 1] - Weighting parameter (0 = Pure MF, 1 = Pure MB).
    perseveration: [-5, 5] - Bonus added to the logits of the previously chosen action.
                             Positive values = repeat choice; Negative = switch.
    """
    learning_rate, beta, w, perseveration = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1 # Indicator for previous action

    for trial in range(n_trials):
        state_idx = state[trial]
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]

        # --- Policy for the first choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add perseveration bonus to the logits directly
        logits = beta * q_net
        if last_action_1 != -1:
            logits[last_action_1] += perseveration
            
        exp_q1 = np.exp(logits)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Policy for the second choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # --- Update Stage 1 ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # --- Update Stage 2 ---
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Update eligibility trace (SARSA-like update for stage 1 based on stage 2 outcome)
        q_stage1_mf[a1] += learning_rate * 1.0 * delta_stage2 # Fixed lambda=1 for this model variant

        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Separate Inverse Temperatures (Beta) for Stage 1 and Stage 2
This model tests the hypothesis that the level of exploration/exploitation differs between the two stages. Stage 1 involves planning and uncertainty about transitions, while Stage 2 is a simple bandit task. Participants might be more precise (higher beta) in Stage 2 than Stage 1.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Separate Inverse Temperatures (Betas) for Stage 1 and Stage 2.
    
    This model allows the randomness of choice behavior to differ between the planning stage 
    (Stage 1) and the bandit stage (Stage 2).
    
    Parameters:
    learning_rate: [0, 1] - Rate at which Q-values are updated.
    beta_1: [0, 10] - Inverse temperature for Stage 1 choice.
    beta_2: [0, 10] - Inverse temperature for Stage 2 choice.
    w: [0, 1] - Weighting parameter (0 = Pure MF, 1 = Pure MB).
    """
    learning_rate, beta_1, beta_2, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        state_idx = state[trial]
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]

        # --- Policy for the first choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Use beta_1 here
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Policy for the second choice ---
        # Use beta_2 here
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # --- Update Stage 1 ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # --- Update Stage 2 ---
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Fixed lambda=1 eligibility trace
        q_stage1_mf[a1] += learning_rate * 1.0 * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```