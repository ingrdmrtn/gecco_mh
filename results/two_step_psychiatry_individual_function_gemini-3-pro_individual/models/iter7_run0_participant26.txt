Here are 3 new cognitive models implemented as Python functions.

### Model 1: Pure Model-Based with Stage 2 Forgetting
This model assumes the participant is purely Model-Based (planning forward using the transition matrix) and relies entirely on their current estimate of the aliens' values. However, it incorporates a "forgetting" parameter for the Stage 2 values. Since the aliens' reward probabilities drift slowly, older knowledge becomes less reliable. The participant decays the value of the *unchosen* alien on every trial.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Based RL with Passive Forgetting.
    
    Hypothesis: The participant calculates Stage 1 values purely based on the 
    known transition matrix and current Stage 2 values (Model-Based). 
    Crucially, values for unchosen aliens 'decay' back to a neutral prior 
    because the participant knows the environment is non-stationary.

    Parameters:
    learning_rate: [0, 1] - Rate of updating chosen Stage 2 Q-values.
    beta: [0, 10] - Inverse temperature for softmax.
    decay: [0, 1] - Rate at which unchosen Stage 2 values decay toward 0.5.
                    (0 = no forgetting, 1 = instant forgetting).
    """
    learning_rate, beta, decay = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # We maintain Stage 2 values (aliens), but Stage 1 is derived dynamically
    q_stage2_mf = np.full((2, 2), 0.5) # Initialize at 0.5 (neutral)

    for trial in range(n_trials):

        # policy for the first choice
        # Pure Model-Based: V(State) = Transition * Max(Q_Stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Action Value Updating
        # Note: In Pure MB, we do not update a cached Stage 1 Q-value via TD errors.
        # We only update the Stage 2 estimates which drive the MB calculation.
        
        # Update Chosen Stage 2 Value
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Decay Unchosen Stage 2 Value
        unchosen_action_2 = 1 - action_2[trial]
        q_stage2_mf[state_idx, unchosen_action_2] += decay * (0.5 - q_stage2_mf[state_idx, unchosen_action_2])

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Dual-Rate Model-Free RL
This model posits that the participant uses a Model-Free strategy (learning values from experience without using the transition matrix), but they learn at different speeds for the two stages. Stage 1 involves stable spaceship transitions, while Stage 2 involves drifting alien rewards. The participant may have a higher learning rate for the immediate alien rewards and a lower (or different) one for the spaceship choices.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Dual-Rate Model-Free RL.

    Hypothesis: The participant is purely Model-Free (TD learning) but utilizes 
    distinct learning rates for the two stages. This reflects the structural 
    difference between choosing a path (Stage 1) and harvesting a reward (Stage 2).

    Parameters:
    lr_stage1: [0, 1] - Learning rate for the first decision (Spaceships).
    lr_stage2: [0, 1] - Learning rate for the second decision (Aliens).
    beta: [0, 10] - Inverse temperature for softmax choice (shared).
    """
    lr_stage1, lr_stage2, beta = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Action Value Updating
        
        # TD(0) update for Stage 1 using the value of the state reached
        # Note: We use the Q-value of the chosen stage 2 action as the proxy for state value V(s')
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr_stage1 * delta_stage1
        
        # Prediction Error update for Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr_stage2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Model-Free RL with Outcome-Dependent Switching (Win-Stay Lose-Shift)
This model extends standard Model-Free learning by adding a heuristic bias based on the *previous outcome*. Unlike simple perseveration (which repeats choices regardless of reward), this model implements a "Win-Stay, Lose-Shift" bias. If the previous trial was rewarded, the participant gets a "bonus" to repeating the Stage 1 choice. If unrewarded, they get a "penalty" (encouraging a switch), on top of the standard Q-value learning.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free RL with Outcome-Dependent Switching (Win-Stay Lose-Shift).

    Hypothesis: The participant combines standard reinforcement learning with a 
    heuristic outcome-based strategy. If the previous trial yielded gold, 
    they are biased to repeat the Stage 1 choice. If it yielded nothing, 
    they are biased to switch, independent of the actual Q-value magnitude.

    Parameters:
    learning_rate: [0, 1] - Update rate for Q-values.
    beta: [0, 10] - Inverse temperature.
    wsls_bias: [0, 5] - Magnitude of the bias. Added to previous choice logits if 
                        rewarded, subtracted if unrewarded.
    """
    learning_rate, beta, wsls_bias = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    last_action_1 = -1
    last_reward = 0

    for trial in range(n_trials):

        # policy for the first choice
        # Calculate effective Q-values (Base Q + Heuristic Bias)
        q_effective = q_stage1_mf.copy()
        
        if last_action_1 != -1:
            if last_reward == 1:
                # Win-Stay: Boost the previous action
                q_effective[last_action_1] += wsls_bias
            else:
                # Lose-Shift: Penalize the previous action
                q_effective[last_action_1] -= wsls_bias

        exp_q1 = np.exp(beta * q_effective)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Action Value Updating
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Update history for next trial
        last_action_1 = action_1[trial]
        last_reward = reward[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```