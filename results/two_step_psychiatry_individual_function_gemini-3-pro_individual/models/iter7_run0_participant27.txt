Here are three new cognitive models exploring different mechanisms for learning and decision-making in the two-step task.

### Model 1: Asymmetric Learning Rates (Positive vs. Negative Prediction Errors)
This model hypothesizes that the participant learns differently from positive outcomes (rewards) compared to negative outcomes (omissions). This asymmetry is a common finding in reinforcement learning literature, suggesting different neural mechanisms for "better than expected" vs "worse than expected" updates.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free RL with Asymmetric Learning Rates.
    
    This model allows for different learning rates for positive prediction errors (learning_rate_pos)
    and negative prediction errors (learning_rate_neg). This captures potential biases
    towards learning from rewards vs. omissions.

    Parameters:
    - learning_rate_pos: [0, 1] Learning rate for positive prediction errors (RPE > 0).
    - learning_rate_neg: [0, 1] Learning rate for negative prediction errors (RPE < 0).
    - beta: [0, 10] Inverse temperature for softmax.
    """
    learning_rate_pos, learning_rate_neg, beta = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Model-free Q-values
    q_stage1_mf = np.zeros(2)      # Values for Spaceship A vs U
    q_stage2_mf = np.zeros((2, 2)) # Values for Aliens (2 planets x 2 aliens)

    for trial in range(n_trials):

        # --- Stage 1 Policy (Model-Free) ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial] # 0 for Planet X, 1 for Planet Y

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updating ---
        # Stage 1 RPE (TD-error using Stage 2 value as proxy for reward)
        # Note: Standard TD(0) uses V(s'), here we use Q(s', a') of the chosen action
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        if delta_stage1 > 0:
            q_stage1_mf[action_1[trial]] += learning_rate_pos * delta_stage1
        else:
            q_stage1_mf[action_1[trial]] += learning_rate_neg * delta_stage1

        # Stage 2 RPE
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        if delta_stage2 > 0:
            q_stage2_mf[state_idx, action_2[trial]] += learning_rate_pos * delta_stage2
        else:
            q_stage2_mf[state_idx, action_2[trial]] += learning_rate_neg * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model with Mixing Parameter (w) and Decay
This model combines Model-Based (planning using the transition matrix) and Model-Free (habitual) value estimation. It introduces a `decay` parameter, representing a forgetting process where unchosen Q-values slowly return to zero (or decay towards a baseline). This accounts for memory limitations or the assumption of environmental volatility.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF Model with Passive Decay.
    
    Combines Model-Based and Model-Free values using a mixing weight 'w'.
    Additionally, unchosen actions decay towards 0, simulating forgetting.

    Parameters:
    - learning_rate: [0, 1] Update rate for Q-values.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weight for Model-Based control (1=Pure MB, 0=Pure MF).
    - decay: [0, 1] Rate at which all Q-values decay per trial (1 = no decay, 0 = instant forgetting).
    """
    learning_rate, beta, w, decay = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy (Hybrid) ---
        # 1. Calculate MB values: Transition Matrix * Max Stage 2 Values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Mix MB and MF values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updating ---
        # TD(1) / SARSA-like update for Stage 1 MF
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Standard Q-learning update for Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # --- Decay ---
        # Apply decay to all Q-values (both stages) to simulate forgetting
        q_stage1_mf *= decay
        q_stage2_mf *= decay
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Softmax Temperature Modulation (Exploration Bonus)
This model tests the hypothesis that the participant's exploration changes over time or based on recent history. Instead of a fixed `beta`, the inverse temperature is modulated by an `exploration_factor`. While often exploration *decreases* over time, in this simplified static version, we add a parameter that allows the second stage to be more deterministic (or noisy) than the first stage, effectively having two different betas.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free RL with Separate Stage Temperatures.
    
    This model assumes the level of decision noise (exploration) differs between 
    Stage 1 (choosing a spaceship) and Stage 2 (choosing an alien).
    This might reflect that Stage 1 is more abstract/strategic while Stage 2 is more concrete.

    Parameters:
    - learning_rate: [0, 1] Update rate.
    - beta_1: [0, 10] Inverse temperature for Stage 1 choice.
    - beta_2: [0, 10] Inverse temperature for Stage 2 choice.
    """
    learning_rate, beta_1, beta_2 = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        # Uses beta_1
        exp_q1 = np.exp(beta_1 * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        # Uses beta_2
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updating ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```