Here are three new cognitive models based on the two-step task structure.

### Model 1: Hybrid with Stage-Specific Perseverance
This model hypothesizes that "stickiness" (perseverance) works differently during the navigation phase (Stage 1) compared to the bandit phase (Stage 2). Participants might have a strong motor habit for the first choice (Spaceship) but be more flexible—or differently sticky—when choosing Aliens, potentially due to the different cognitive load or visual context at each stage.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Stage-Specific Perseverance.
    Distinguishes between perseverance (habit) at the spaceship choice (Stage 1)
    versus the alien choice (Stage 2).

    Parameters:
    learning_rate: [0, 1] Learning rate for value updates.
    beta: [0, 10] Inverse temperature (exploration/exploitation).
    w: [0, 1] Weight mixing MB (1) and MF (0).
    pers_1: [0, 5] Perseverance bonus for Stage 1 (Spaceship choice).
    pers_2: [0, 5] Perseverance bonus for Stage 2 (Alien choice).
    """
    learning_rate, beta, w, pers_1, pers_2 = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    last_choice_1 = -1
    # Track last choice for Stage 2 separately for each state (Planet X vs Planet Y)
    last_choice_2 = [-1, -1] 

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid weighted value
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Apply Stage 1 Perseverance
        if last_choice_1 != -1:
            q_net_1[last_choice_1] += pers_1

        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # policy for the second choice
        q_net_2 = q_stage2_mf[state_idx].copy()
        
        # Apply Stage 2 Perseverance (specific to the current planet)
        if last_choice_2[state_idx] != -1:
            q_net_2[last_choice_2[state_idx]] += pers_2

        exp_q2 = np.exp(beta * q_net_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # Update history
        last_choice_1 = action_1[trial]
        last_choice_2[state_idx] = action_2[trial]
  
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid with Asymmetric Learning and Perseverance
This model combines hybrid control with asymmetric learning rates (positive vs. negative prediction errors) and a general perseverance bonus. It posits that the participant learns differently from "wins" (finding gold) versus "losses" (no gold), while simultaneously exhibiting a baseline tendency to repeat previous actions regardless of the outcome.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Asymmetric Learning Rates and Perseverance.
    Allows for different learning speeds from positive vs negative prediction errors,
    plus a general stickiness bias.

    Parameters:
    lr_pos: [0, 1] Learning rate when prediction error is positive (better than expected).
    lr_neg: [0, 1] Learning rate when prediction error is negative (worse than expected).
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Weight mixing MB (1) and MF (0).
    pers: [0, 5] General perseverance bonus applied to the previously chosen option.
    """
    lr_pos, lr_neg, beta, w, pers = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_choice_1 = -1

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Apply Perseverance
        if last_choice_1 != -1:
            q_net[last_choice_1] += pers

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_choice_1 = action_1[trial]
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Update Stage 1 (TD learning)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        eff_lr_1 = lr_pos if delta_stage1 > 0 else lr_neg
        q_stage1_mf[action_1[trial]] += eff_lr_1 * delta_stage1
        
        # Update Stage 2 (Reward learning)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        eff_lr_2 = lr_pos if delta_stage2 > 0 else lr_neg
        q_stage2_mf[state_idx, action_2[trial]] += eff_lr_2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid with Subjective Transition Belief and Perseverance
This model acknowledges that while the true transition probabilities are 0.7/0.3, the participant may believe the structure is more deterministic (e.g., 0.9) or more random (e.g., 0.55). It treats the transition probability used in the Model-Based calculation as a free parameter, while also including a perseverance parameter to account for choice inertia.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Subjective Transition Belief and Perseverance.
    The participant uses an internal, subjective transition probability 
    for Model-Based planning, rather than the true objective probability.

    Parameters:
    learning_rate: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Weight mixing MB (1) and MF (0).
    subj_prob: [0, 1] The participant's subjective belief of the common transition probability (e.g., believing it's 0.9 instead of 0.7).
    pers: [0, 5] Perseverance bonus.
    """
    learning_rate, beta, w, subj_prob, pers = model_parameters
    n_trials = len(action_1)
  
    # Overwrite the objective matrix with the subjective one
    # If subj_prob is the probability of Common, 1-subj_prob is Rare.
    transition_matrix = np.array([[subj_prob, 1.0 - subj_prob], 
                                  [1.0 - subj_prob, subj_prob]])
                                  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_choice_1 = -1

    for trial in range(n_trials):

        # policy for the first choice
        # MB calculation uses the SUBJECTIVE transition matrix
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_choice_1 != -1:
            q_net[last_choice_1] += pers

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_choice_1 = action_1[trial]
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```