Here are three new cognitive models for the two-step task, exploring different mechanisms and parameter combinations not previously listed in the "tried so far" section.

### Model 1: Hybrid Learner with Separate Learning Rates for First and Second Stages
This model hypothesizes that the participant might update their values at different rates for the spaceship choice (Stage 1) versus the alien choice (Stage 2). This is biologically plausible as Stage 1 involves abstract planning/transitions, while Stage 2 involves direct reward feedback. It combines model-based and model-free influences using a mixing weight `w`.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner with Separate Learning Rates for Stages.
    
    This model separates the learning rate for the first stage (transitions) 
    and the second stage (direct rewards), combined with a hybrid MB/MF architecture.

    Parameters:
    - alpha1: [0, 1] Learning rate for Stage 1 (TD updates from state transitions).
    - alpha2: [0, 1] Learning rate for Stage 2 (Reward prediction errors).
    - beta: [0, 10] Inverse temperature for softmax.
    - w: [0, 1] Weighting parameter (0 = Pure Model-Free, 1 = Pure Model-Based).
    """
    alpha1, alpha2, beta, w = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition matrix for the Model-Based component
    # (0->0: 0.7, 0->1: 0.3; 1->0: 0.3, 1->1: 0.7)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)      # MF values for Stage 1
    q_stage2_mf = np.zeros((2, 2)) # MF values for Stage 2 (States x Actions)

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # Model-Based Value Calculation: V(S') = max(Q_MF(S', a'))
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Policy
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Decision ---
        # Only Model-Free applies here
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        # Stage 1 Update (TD(0)): Uses alpha1
        # Prediction error: Value of state arrived at - Value of state left
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += alpha1 * delta_stage1
        
        # Stage 2 Update: Uses alpha2
        # Prediction error: Reward received - Value of state/action pair
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += alpha2 * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Model-Free Learner with Outcome-Specific "Stickiness"
This model investigates if the participant has a tendency to repeat choices based specifically on whether they were rewarded or unrewarded, but distinct from standard reinforcement learning. Unlike a standard "win-stay/lose-shift" heuristic, this adds specific "stickiness" bonuses to the Q-values based on the *previous* outcome, allowing for different degrees of perseveration after wins versus losses.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Learner with Reward-Dependent Stickiness.
    
    Adds a 'stickiness' bonus to the previously chosen action, but the magnitude
    of this bonus depends on whether the previous trial was rewarded.

    Parameters:
    - learning_rate: [0, 1] Standard Q-learning rate.
    - beta: [0, 10] Inverse temperature.
    - stick_rew: [0, 5] Bonus added to previous Stage 1 choice if last trial had Reward=1.
    - stick_unrew: [0, 5] Bonus added to previous Stage 1 choice if last trial had Reward=0.
    """
    learning_rate, beta, stick_rew, stick_unrew = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1
    prev_reward = 0

    for trial in range(n_trials):

        # --- Stage 1 Decision ---
        # Calculate effective Q-values including stickiness
        q_stage1_effective = q_stage1_mf.copy()
        
        if prev_action_1 != -1:
            if prev_reward == 1:
                q_stage1_effective[prev_action_1] += stick_rew
            else:
                q_stage1_effective[prev_action_1] += stick_unrew

        exp_q1 = np.exp(beta * q_stage1_effective)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Decision ---
        # Standard Softmax
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        # Standard TD(0) / SARSA-like updates
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Store history for next trial's stickiness
        prev_action_1 = action_1[trial]
        prev_reward = reward[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Learner with Forgetting (Decay) on Unchosen Options
This model posits that while the participant learns about chosen options, they simultaneously "forget" the value of options they do not choose. The values of unchosen actions decay toward a neutral point (0) or the initial value. This captures the phenomenon where memory traces of unvisited states/actions fade over time.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner with Forgetting (Decay) for Unchosen Actions.
    
    Chosen actions update via learning_rate. Unchosen actions decay towards 0 
    by a factor of (1 - decay_rate).

    Parameters:
    - learning_rate: [0, 1] Update rate for chosen actions.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] MB/MF weighting.
    - decay_rate: [0, 1] Rate at which unchosen Q-values decay to 0 (0=no decay, 1=instant reset).
    """
    learning_rate, beta, w, decay_rate = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning & Forgetting ---
        
        # 1. Update CHOSEN Stage 1
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # 2. Decay UNCHOSEN Stage 1
        unchosen_1 = 1 - action_1[trial]
        q_stage1_mf[unchosen_1] *= (1 - decay_rate)

        # 3. Update CHOSEN Stage 2 (in the visited state)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # 4. Decay UNCHOSEN Stage 2 (in the visited state)
        unchosen_2 = 1 - action_2[trial]
        q_stage2_mf[state_idx, unchosen_2] *= (1 - decay_rate)
        
        # Note: We typically do not decay the *unvisited* state in Stage 2 
        # because the participant wasn't there to make a choice, 
        # though variations of this model might decay everything.

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```