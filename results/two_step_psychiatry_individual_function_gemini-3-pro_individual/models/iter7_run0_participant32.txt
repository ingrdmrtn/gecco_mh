Here are three new cognitive models based on the two-step task structure, exploring different mechanisms for learning and decision-making.

### Model 1: Asymmetric Learning Rates (Positive vs. Negative Prediction Errors)
This model hypothesizes that the participant might learn differently from "good news" (positive prediction errors) versus "bad news" (negative prediction errors). This asymmetry is a common finding in reinforcement learning studies involving reward processing.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Asymmetric Learning Rates.
    
    Distinguishes between learning from positive prediction errors (better than expected)
    and negative prediction errors (worse than expected).

    Parameters:
    alpha_pos: [0, 1] - Learning rate for positive prediction errors (RPE > 0).
    alpha_neg: [0, 1] - Learning rate for negative prediction errors (RPE < 0).
    beta: [0, 10] - Inverse temperature for softmax choice.
    w: [0, 1] - Weighting parameter (0 = pure Model-Free, 1 = pure Model-Based).
    """
    alpha_pos, alpha_neg, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial] # Actual state reached (0 or 1)

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning ---
        # Stage 1 Update (TD(0) style for MF)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        if delta_stage1 > 0:
            q_stage1_mf[action_1[trial]] += alpha_pos * delta_stage1
        else:
            q_stage1_mf[action_1[trial]] += alpha_neg * delta_stage1
        
        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        if delta_stage2 > 0:
            q_stage2_mf[state_idx, action_2[trial]] += alpha_pos * delta_stage2
        else:
            q_stage2_mf[state_idx, action_2[trial]] += alpha_neg * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Separate Model-Based and Model-Free Inverse Temperatures
This model posits that the reliability or confidence placed in the Model-Based system versus the Model-Free system might differ. Instead of a single mixing weight `w`, this model assigns distinct `beta` (inverse temperature) parameters to the MB and MF values at the first stage. This allows the model to capture if a participant is very precise with their MF values but noisy with MB planning (or vice versa).

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Dual-Beta Hybrid Model.
    
    Instead of a weighting parameter 'w', this model uses separate inverse temperatures
    for the model-based and model-free components at Stage 1. This allows for independent
    scaling of the influence of habitual vs. planned values.

    Parameters:
    learning_rate: [0, 1] - Learning rate for Q-values.
    beta_mb: [0, 10] - Inverse temperature for Model-Based values at Stage 1.
    beta_mf: [0, 10] - Inverse temperature for Model-Free values at Stage 1.
    beta_2: [0, 10] - Inverse temperature for Stage 2 choices.
    """
    learning_rate, beta_mb, beta_mf, beta_2 = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combine values directly in the exponent with separate betas
        # Note: We don't use 'w' here. The relative magnitude of betas determines the weight.
        logits_1 = (beta_mb * q_stage1_mb) + (beta_mf * q_stage1_mf)
        
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning ---
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Eligibility Trace (TD(lambda)) with Forgetting
This model implements a TD($\lambda$) approach where the update of the first-stage value by the second-stage reward is modulated by an eligibility parameter $\lambda$. Crucially, it adds a decay/forgetting parameter for unchosen options, suggesting that Q-values for actions not taken might drift back toward neutral (0 or 0.5) over time.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    TD(lambda) Model with Passive Forgetting.
    
    Includes an eligibility trace parameter (lambda) to link Stage 2 rewards directly
    to Stage 1 choices, and a decay parameter that causes unchosen action values 
    to slowly regress to zero (forgetting).

    Parameters:
    learning_rate: [0, 1] - Learning rate for Q-values.
    beta: [0, 10] - Inverse temperature for softmax choice.
    lam: [0, 1] - Eligibility trace parameter (0 = TD(0), 1 = Monte Carlo).
    decay: [0, 1] - Decay rate for unchosen actions (0 = no decay, 1 = instant reset).
    """
    learning_rate, beta, lam, decay = model_parameters
    n_trials = len(action_1)
  
    # Pure Model-Free formulation (w=0 implicit) but with eligibility traces
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning with Eligibility Trace ---
        
        # 1. Prediction error at stage 1 (based on stage 2 value)
        delta1 = q_stage2[state_idx, action_2[trial]] - q_stage1[action_1[trial]]
        q_stage1[action_1[trial]] += learning_rate * delta1
        
        # 2. Prediction error at stage 2 (based on reward)
        delta2 = reward[trial] - q_stage2[state_idx, action_2[trial]]
        q_stage2[state_idx, action_2[trial]] += learning_rate * delta2
        
        # 3. Eligibility trace: Update Stage 1 value again based on Stage 2 RPE
        q_stage1[action_1[trial]] += learning_rate * lam * delta2

        # --- Forgetting / Decay for unchosen options ---
        # Decay Stage 1 unchosen
        unchosen_1 = 1 - action_1[trial]
        q_stage1[unchosen_1] *= (1 - decay)
        
        # Decay Stage 2 unchosen (in the visited state)
        unchosen_2 = 1 - action_2[trial]
        q_stage2[state_idx, unchosen_2] *= (1 - decay)
        
        # Note: We usually don't decay the unvisited state in Stage 2 in this specific setup,
        # but one could argue for it. Here we only decay the unchosen option in the active state.

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```