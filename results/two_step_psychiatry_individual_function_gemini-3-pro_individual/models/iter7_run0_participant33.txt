Here are three new cognitive models exploring different mechanisms for learning and decision-making in the two-step task.

### Model 1: Hybrid Learner with Separate Learning Rates
This model implements the classic hybrid reinforcement learning theory often used for this task (Daw et al., 2011). It combines Model-Free (MF) learning, which reinforces actions based on reward history, and Model-Based (MB) learning, which plans using the transition structure. Crucially, this version allows for different learning rates for the first stage (where the transition matters) and the second stage (where only the reward matters), testing if the participant learns about spaceship values at a different speed than alien values.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Learner with Separate Learning Rates.
    
    This model assumes the participant uses a weighted combination of:
    1. Model-Based (MB) values: Calculated by planning through the transition matrix.
    2. Model-Free (MF) values: Learned directly from experience (TD learning).
    
    It allows for distinct learning rates for stage 1 (spaceships) and stage 2 (aliens).
    
    Parameters:
    - lr_1: Learning rate for stage 1 MF values [0, 1].
    - lr_2: Learning rate for stage 2 MF values [0, 1].
    - beta: Inverse temperature for softmax choice [0, 10].
    - w: Weighting parameter [0, 1]. 
         0 = Pure Model-Free, 1 = Pure Model-Based.
    """
    lr_1, lr_2, beta, w = model_parameters
    n_trials = len(action_1)

    # Fixed transition matrix: P(Planet | Spaceship)
    # Row 0: Spaceship 0 -> [0.7 Planet 0, 0.3 Planet 1]
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)       # Model-free values for spaceships
    q_stage2_mf = np.zeros((2, 2))  # Model-free values for aliens (Planets x Aliens)

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        
        # Calculate Model-Based values for Stage 1
        # Max value of each state (planet) based on current Stage 2 MF values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combined value: w * MB + (1-w) * MF
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial] # The planet we actually arrived at

        # --- Stage 2 Decision ---
        
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning ---
        
        # Prediction error at stage 2 (Reward - Expected Value)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Update Stage 2 MF values (Aliens)
        q_stage2_mf[state_idx, action_2[trial]] += lr_2 * delta_stage2
        
        # Prediction error at stage 1 (Value of state reached - Value of spaceship chosen)
        # Note: Standard TD(0) uses the value of the state reached (q_stage2_mf)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        # Update Stage 1 MF values (Spaceships)
        q_stage1_mf[action_1[trial]] += lr_1 * delta_stage1
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Perseveration / Stickiness Model
This model tests the hypothesis that the participant isn't just driven by reward, but also has a simple bias to repeat their previous choice regardless of the outcome ("choice stickiness"). This is a common heuristic in human decision-making. It augments a standard Model-Free learner with a parameter that adds a "bonus" to the previously chosen action at the first stage.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Learner with Choice Perseveration (Stickiness).
    
    This model assumes the participant is a standard TD learner but has an 
    additional bias to repeat the action taken on the previous trial, 
    regardless of whether it was rewarded.
    
    Parameters:
    - learning_rate: Rate at which Q-values update [0, 1].
    - beta: Inverse temperature for softmax choice [0, 10].
    - pers_w: Perseveration weight [0, 5]. 
              Positive values increase probability of repeating the last choice.
    """
    learning_rate, beta, pers_w = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1 # Initialize with no previous action

    for trial in range(n_trials):

        # --- Stage 1 Decision ---
        # Calculate effective Q-values including perseveration bonus
        q_stage1_effective = q_stage1_mf.copy()
        
        if last_action_1 != -1:
            q_stage1_effective[last_action_1] += pers_w

        exp_q1 = np.exp(beta * q_stage1_effective)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Decision ---
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning ---
        # Standard SARSA / TD(0) updates
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Store action for next trial's perseveration
        last_action_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Asymmetric Learning Rates (Positive vs. Negative)
This model investigates if the participant learns differently from success (getting a coin) versus failure (getting zero coins). It splits the learning rate into `alpha_pos` (for positive prediction errors) and `alpha_neg` (for negative prediction errors). This asymmetry is a well-documented phenomenon in psychiatry and neuroscience, often linked to dopamine signaling.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Learner with Asymmetric Learning Rates (Pos/Neg).
    
    This model allows for different learning speeds depending on whether the 
    outcome was better than expected (positive prediction error) or worse 
    than expected (negative prediction error).
    
    Parameters:
    - alpha_pos: Learning rate for positive prediction errors [0, 1].
    - alpha_neg: Learning rate for negative prediction errors [0, 1].
    - beta: Inverse temperature for softmax choice [0, 10].
    """
    alpha_pos, alpha_neg, beta = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Decision ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Decision ---
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning ---
        
        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        if delta_stage2 >= 0:
            q_stage2_mf[state_idx, action_2[trial]] += alpha_pos * delta_stage2
        else:
            q_stage2_mf[state_idx, action_2[trial]] += alpha_neg * delta_stage2
            
        # Stage 1 Update
        # Using the updated stage 2 value to drive stage 1 (TD-like)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        if delta_stage1 >= 0:
            q_stage1_mf[action_1[trial]] += alpha_pos * delta_stage1
        else:
            q_stage1_mf[action_1[trial]] += alpha_neg * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```