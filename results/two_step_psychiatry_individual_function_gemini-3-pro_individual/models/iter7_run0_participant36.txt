Here are three new cognitive models that explore different mechanisms for learning and decision-making in the two-step task.

### Model 1: Hybrid Learner with Distinct Learning Rates
This model implements the classic hybrid reinforcement learning theory (Daw et al., 2011), combining Model-Free (MF) and Model-Based (MB) systems. Crucially, it differentiates the learning rate for the first stage (where only transitions are observed initially) versus the second stage (where actual rewards are received), allowing the model to be more sensitive to updates at one level than the other.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner with Distinct Learning Rates.
    Combines Model-Based (MB) and Model-Free (MF) values using a weighting parameter 'w'.
    Uses separate learning rates for Stage 1 (transition prediction/TD(1)) and Stage 2 (reward).

    Parameters:
    lr_stage1: [0,1] Learning rate for Stage 1 MF values.
    lr_stage2: [0,1] Learning rate for Stage 2 MF values.
    beta: [0,10] Inverse temperature.
    w: [0,1] Weighting parameter (0 = Pure MF, 1 = Pure MB).
    """
    lr_stage1, lr_stage2, beta, w = model_parameters
    n_trials = len(action_1)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based Value: Transition * Max(Stage 2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        # --- Transition ---
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning ---
        # 1. Update Stage 2 values based on reward (using lr_stage2)
        pe_2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr_stage2 * pe_2
        
        # 2. Update Stage 1 MF values (TD(1)-like update)
        # We update Stage 1 MF based on the reward received at Stage 2
        pe_1 = reward[trial] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr_stage1 * pe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Pure Model-Free with Eligibility Traces (TD(lambda))
This model abandons the model-based planning component entirely and focuses on the temporal credit assignment problem in reinforcement learning. It uses an eligibility trace parameter (`lambda`) to determine how much the reward at the second stage should reinforce the choice made at the first stage. This captures "caching" behavior where good outcomes reinforce the entire chain of actions leading to them, regardless of transition probability.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free Learner with Eligibility Traces (TD(lambda)).
    Updates values based on temporal difference errors. The lambda parameter 
    controls how much the Stage 2 prediction error updates Stage 1 values.

    Parameters:
    learning_rate: [0,1] General learning rate.
    beta: [0,10] Inverse temperature.
    lambda_param: [0,1] Eligibility trace decay (0 = TD(0), 1 = TD(1)/Monte Carlo).
    """
    learning_rate, beta, lambda_param = model_parameters
    n_trials = len(action_1)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        state_idx = state[trial]
        chosen_a1 = action_1[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        chosen_a2 = action_2[trial]
        r = reward[trial]

        # --- Learning (TD(lambda)) ---
        
        # Prediction Error 1: Difference between Stage 2 value and Stage 1 value
        pe_1 = q_stage2[state_idx, chosen_a2] - q_stage1[chosen_a1]
        
        # Prediction Error 2: Difference between Reward and Stage 2 value
        pe_2 = r - q_stage2[state_idx, chosen_a2]

        # Update Stage 2 value
        q_stage2[state_idx, chosen_a2] += learning_rate * pe_2
        
        # Update Stage 1 value:
        # It gets updated by PE1 (immediate next step)
        # AND a fraction (lambda) of PE2 (the reward step)
        q_stage1[chosen_a1] += learning_rate * (pe_1 + lambda_param * pe_2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Learner with Choice Perseveration
This model adds a "stickiness" or perseveration parameter to the standard hybrid model. Humans often repeat their previous choice regardless of reward or model-based inference (habitual responding or motor perseveration). This model captures that tendency by adding a bonus to the Q-value of the action taken on the immediately preceding trial.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner with Choice Perseveration.
    Standard Hybrid (MF/MB) model augmented with a 'perseveration' parameter
    that biases the agent to repeat the previous Stage 1 choice.

    Parameters:
    learning_rate: [0,1] Learning rate.
    beta: [0,10] Inverse temperature.
    w: [0,1] Weighting (0=MF, 1=MB).
    perseveration: [0,5] Bonus added to the previously chosen Stage 1 action.
    """
    learning_rate, beta, w, perseveration = model_parameters
    n_trials = len(action_1)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1 # Indicator for previous choice

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add perseveration bonus
        if last_action_1 != -1:
            q_net[last_action_1] += perseveration

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        # Store this action for next trial's perseveration
        last_action_1 = action_1[trial]
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning ---
        # Update Stage 2
        pe_2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * pe_2
        
        # Update Stage 1 MF (TD(1) style assumption for simplicity in this variant)
        pe_1 = reward[trial] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * pe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```