Here are three new cognitive models based on the two-step task structure, exploring different mechanisms than the standard Model-Based/Model-Free hybrid or simple perseveration models.

### Model 1: Asymmetric Learning Rates (Positive vs. Negative Prediction Errors)
This model posits that the participant updates their value estimates differently depending on whether the outcome was better than expected (positive prediction error) or worse than expected (negative prediction error). This is often observed in clinical populations where sensitivity to reward or punishment differs.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Model 1: Asymmetric Learning Rates (pos/neg).
    This model assumes the agent learns values via TD learning but uses different
    learning rates for positive and negative prediction errors.
    
    Bounds:
    alpha_pos: [0, 1] - Learning rate for positive prediction errors.
    alpha_neg: [0, 1] - Learning rate for negative prediction errors.
    beta: [0, 10] - Inverse temperature for softmax.
    """
    alpha_pos, alpha_neg, beta = model_parameters
    n_trials = len(action_1)

    q_stage1_mf = np.zeros(2) 
    q_stage2_mf = np.zeros((2, 2)) 
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        # Policy for Stage 1
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        a1 = action_1[trial]

        # Policy for Stage 2
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]

        # Stage 1 Update (TD Learning)
        # Note: In pure MF, stage 1 is updated by the value of stage 2 choice
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        if delta_stage1 > 0:
            q_stage1_mf[a1] += alpha_pos * delta_stage1
        else:
            q_stage1_mf[a1] += alpha_neg * delta_stage1

        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        if delta_stage2 > 0:
            q_stage2_mf[state_idx, a2] += alpha_pos * delta_stage2
        else:
            q_stage2_mf[state_idx, a2] += alpha_neg * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Decay-Based Reinforcement Learning
This model introduces a memory decay parameter. Instead of just updating the chosen action, unchosen actions slowly decay back to a baseline (assumed to be 0 or 0.5, here treated as decaying toward 0 for simplicity). This captures the phenomenon where participants "forget" the value of options they haven't explored in a while, or their confidence in those values erodes.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model 2: Decay-Based Reinforcement Learning.
    Standard TD learning, but values for unchosen actions decay over time,
    representing forgetting or uncertainty growth.
    
    Bounds:
    learning_rate: [0, 1] - Learning rate for Q-value updates.
    beta: [0, 10] - Inverse temperature for softmax.
    decay_rate: [0, 1] - Rate at which unchosen Q-values decay toward 0 (1=no decay, 0=instant forget).
    """
    learning_rate, beta, decay_rate = model_parameters
    n_trials = len(action_1)

    q_stage1_mf = np.zeros(2) 
    q_stage2_mf = np.zeros((2, 2)) 
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        # Policy Stage 1
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        a1 = action_1[trial]
        
        # Policy Stage 2
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]

        # Update Stage 1
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Decay unchosen stage 1
        unchosen_a1 = 1 - a1
        q_stage1_mf[unchosen_a1] *= decay_rate

        # Update Stage 2
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Decay unchosen stage 2 (in the current state)
        unchosen_a2 = 1 - a2
        q_stage2_mf[state_idx, unchosen_a2] *= decay_rate
        # Note: We do not decay the unvisited state's Q-values in this specific implementation 
        # to keep param count low, though one could.

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: "Win-Stay, Lose-Shift" Heuristic Hybrid
This model combines standard Reinforcement Learning with a heuristic "Win-Stay, Lose-Shift" (WSLS) strategy applied specifically to the second stage. While RL slowly integrates value, WSLS is a reactive mechanism that immediately boosts the probability of repeating a rewarded action or switching from an unrewarded one, regardless of the integrated Q-value.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model 3: Hybrid RL with Stage 2 Win-Stay Lose-Shift (WSLS) bias.
    The agent uses RL for values, but the Stage 2 choice is biased by the 
    outcome of the *immediately preceding* trial if the state matches.
    
    Bounds:
    learning_rate: [0, 1] - Learning rate for Q-value updates.
    beta: [0, 10] - Inverse temperature for softmax.
    wsls_strength: [0, 5] - Strength of the heuristic bias added to Q-values.
    """
    learning_rate, beta, wsls_strength = model_parameters
    n_trials = len(action_1)

    q_stage1_mf = np.zeros(2) 
    q_stage2_mf = np.zeros((2, 2)) 
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Track previous outcome for WSLS logic
    # (state, action, reward)
    last_s2_context = None 

    for trial in range(n_trials):
        # Policy Stage 1 (Pure RL here)
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        a1 = action_1[trial]
        
        # Policy Stage 2 (RL + WSLS Bias)
        q_net_stage2 = q_stage2_mf[state_idx].copy()
        
        # Apply WSLS if we were in this state recently
        if last_s2_context is not None:
            prev_state, prev_action, prev_reward = last_s2_context
            if prev_state == state_idx:
                if prev_reward == 1:
                    # Win-Stay: boost previous action
                    q_net_stage2[prev_action] += wsls_strength
                else:
                    # Lose-Shift: boost the OTHER action (equivalent to penalizing previous)
                    q_net_stage2[1 - prev_action] += wsls_strength

        exp_q2 = np.exp(beta * q_net_stage2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]

        # Updates
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Save context for next trial's heuristic
        last_s2_context = (state_idx, a2, r)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```