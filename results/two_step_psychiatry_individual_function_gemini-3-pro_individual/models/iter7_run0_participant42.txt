Here are three new cognitive models that explore different mechanisms for learning and decision-making in the two-step task, avoiding the exact parameter combinations previously tested while adhering to the template structure.

### Model 1: Hybrid Learner with Separate Learning Rates for Stage 1 and Stage 2
This model posits that the participant might learn at different speeds for the high-level spaceship choice (Stage 1) versus the specific alien choice (Stage 2). The "w" parameter mixes Model-Based (transition-aware) and Model-Free (experience-based) values for the first stage.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid learner with distinct learning rates for Stage 1 and Stage 2.
    This allows the model to capture if the participant updates their spaceship preferences
    faster or slower than their alien preferences.
    
    Bounds:
    alpha_stage1: [0, 1] - Learning rate for Stage 1 (TD(1) update).
    alpha_stage2: [0, 1] - Learning rate for Stage 2 (Reward update).
    beta: [0, 10] - Inverse temperature for softmax.
    w: [0, 1] - Weighting parameter (0 = pure Model-Free, 1 = pure Model-Based).
    """
    alpha_stage1, alpha_stage2, beta, w = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition matrix (70% common, 30% rare)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)      # Model-free values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for stage 2 (aliens)

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # Model-Based Value: V(s') = max(Q(s', a'))
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value: w * MB + (1-w) * MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial] # 0 or 1 (Planets)

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # Stage 1 MF update (TD(1) style - driven by final reward)
        # Note: Standard MF often updates stage 1 based on stage 2 value (TD(0)) or reward (TD(1)).
        # Here we use a direct update from reward to Stage 1 Q-value for simplicity in this variant.
        delta_stage1 = reward[trial] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += alpha_stage1 * delta_stage1
        
        # Stage 2 MF update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += alpha_stage2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Model-Based Learner with Transition Learning
Instead of assuming the transition matrix is fixed at 70/30, this model learns the transition probabilities from experience. It is a pure Model-Based agent (no "w" parameter mixing MF) but includes a parameter to control how quickly it updates its beliefs about spaceship-planet transitions.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Based learner that dynamically learns the transition matrix.
    It updates state transition probabilities based on observed spaceship-planet outcomes.
    
    Bounds:
    learning_rate: [0, 1] - Learning rate for Q-values (aliens).
    lr_trans: [0, 1] - Learning rate for the transition matrix probabilities.
    beta: [0, 10] - Inverse temperature.
    """
    learning_rate, lr_trans, beta = model_parameters
    n_trials = len(action_1)
  
    # Initialize transition matrix (start with uniform prior or standard assumption)
    # Rows: Action 1 (Spaceships), Cols: State (Planets)
    # Initialize at 0.5 to represent uncertainty, or 0.7 if assuming prior knowledge.
    # We start at 0.5 (maximum entropy) to let the data drive the structure.
    trans_probs = np.array([[0.5, 0.5], [0.5, 0.5]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage2_mf = np.zeros((2, 2)) # Values for aliens

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # Calculate expected value of each spaceship based on current transition beliefs
        # V(planet) = max(Q(planet, alien))
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        
        # Q_MB(spaceship) = P(planet A|spaceship) * V(planet A) + P(planet B|spaceship) * V(planet B)
        q_stage1_mb = trans_probs @ max_q_stage2
        
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        
        # 1. Update Transition Matrix based on where we actually ended up
        # We increase prob of the observed transition and normalize
        # State is 0 or 1. Action 1 is 0 or 1.
        # Simple delta rule for probability estimation
        target = np.zeros(2)
        target[state_idx] = 1.0 # The planet we actually reached
        
        # Update the row corresponding to the chosen spaceship
        trans_probs[action_1[trial]] += lr_trans * (target - trans_probs[action_1[trial]])
        
        # Ensure rows sum to 1 (though delta rule on one-hot target usually preserves this roughly, explicit norm is safer)
        trans_probs[action_1[trial]] /= np.sum(trans_probs[action_1[trial]])

        # 2. Update Stage 2 Q-values
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Model-Free Learner with Decay
This model implements a pure Model-Free strategy (TD-learning) but introduces a "decay" parameter. Unchosen options slowly decay back to 0 (forgetting), which captures the phenomenon where participants lose confidence in options they haven't explored recently.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free TD Learner with value decay (forgetting).
    Values of unchosen actions decay toward 0 on every trial.
    
    Bounds:
    learning_rate: [0, 1] - Learning rate for chosen actions.
    beta: [0, 10] - Inverse temperature.
    decay: [0, 1] - Rate at which unchosen action values decay (0 = no decay, 1 = instant forgetting).
    """
    learning_rate, beta, decay = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        
        # Calculate prediction errors
        # TD(0) update for Stage 1: driven by the value of the state reached (Planet)
        v_state_reached = np.max(q_stage2[state_idx]) # or use q of chosen action
        delta_stage1 = v_state_reached - q_stage1[action_1[trial]]
        
        # Update Stage 1 Chosen
        q_stage1[action_1[trial]] += learning_rate * delta_stage1
        # Decay Stage 1 Unchosen
        unchosen_1 = 1 - action_1[trial]
        q_stage1[unchosen_1] *= (1 - decay)

        # Update Stage 2 Chosen
        delta_stage2 = reward[trial] - q_stage2[state_idx, action_2[trial]]
        q_stage2[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Decay Stage 2 Unchosen (only for the current planet visited)
        unchosen_2 = 1 - action_2[trial]
        q_stage2[state_idx, unchosen_2] *= (1 - decay)
        
        # Optional: Decay the unvisited planet's aliens? 
        # Usually standard decay models apply to all unchosen options.
        # Here we decay the aliens on the OTHER planet too.
        other_state = 1 - state_idx
        q_stage2[other_state, :] *= (1 - decay)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```