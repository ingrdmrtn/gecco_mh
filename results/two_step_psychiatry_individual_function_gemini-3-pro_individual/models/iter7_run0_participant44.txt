Here are three new cognitive models for the two-step decision task, defined as Python functions.

### Model 1: TD(Î») Model-Free Learning
This model hypothesizes that the participant does not use a map of the task (the transition matrix) but relies on a Model-Free strategy enhanced by **eligibility traces**. The `lambda_coeff` parameter allows the outcome at the second stage (reward) to directly update the value of the first stage choice, bridging the temporal gap between the initial decision and the final result.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free TD(lambda) Reinforcement Learning.
    
    This model assumes the participant uses a pure model-free strategy (no transition matrix),
    but uses eligibility traces (lambda) to allow the final reward to directly reinforce 
    the Stage 1 choice, bridging the gap between Stage 1 and the outcome.

    Parameters:
    learning_rate: [0,1] - Update rate for Q-values.
    beta: [0,10] - Inverse temperature for softmax.
    lambda_coeff: [0,1] - Eligibility trace decay parameter (0 = TD(0), 1 = Monte Carlo-like).
    """
    learning_rate, beta, lambda_coeff = model_parameters
    n_trials = len(action_1)
    
    # Q-values initialization
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2)) # State x Action
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        # Policy for the first choice (Pure MF)
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        s_idx = state[trial]
        
        # Policy for the second choice
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]
        
        # Action Value Updating
        
        # 1. TD(0) error for stage 1: Difference between Stage 2 value and Stage 1 expectation
        delta1 = q_stage2[s_idx, a2] - q_stage1[a1]
        q_stage1[a1] = q_stage1[a1] + learning_rate * delta1
        
        # 2. TD error for stage 2: Difference between Reward and Stage 2 expectation
        delta2 = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] = q_stage2[s_idx, a2] + learning_rate * delta2
        
        # 3. Eligibility trace update: Apply Stage 2 error to Stage 1 value, scaled by lambda
        q_stage1[a1] = q_stage1[a1] + learning_rate * lambda_coeff * delta2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Asymmetric Learning (Valence-Dependent)
This model investigates whether the participant learns differently from "good news" versus "bad news." It uses separate learning rates for positive prediction errors (outcomes better than expected) and negative prediction errors (outcomes worse than expected). This is frequently used to model biases in psychiatric populations (e.g., pessimism in depression).

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Asymmetric Model-Free Reinforcement Learning.
    
    This model assumes the participant learns differently from positive prediction errors 
    (outcomes better than expected) versus negative prediction errors (outcomes worse than expected).
    This captures valence-dependent learning biases.

    Parameters:
    alpha_pos: [0,1] - Learning rate for positive prediction errors (delta > 0).
    alpha_neg: [0,1] - Learning rate for negative prediction errors (delta < 0).
    beta: [0,10] - Inverse temperature for softmax.
    """
    alpha_pos, alpha_neg, beta = model_parameters
    n_trials = len(action_1)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        # Policy for the first choice
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        s_idx = state[trial]
        
        # Policy for the second choice
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]
        
        # Action Value Updating with Asymmetric Rates
        
        # Stage 1 Update
        delta1 = q_stage2[s_idx, a2] - q_stage1[a1]
        lr1 = alpha_pos if delta1 > 0 else alpha_neg
        q_stage1[a1] = q_stage1[a1] + lr1 * delta1
        
        # Stage 2 Update
        delta2 = r - q_stage2[s_idx, a2]
        lr2 = alpha_pos if delta2 > 0 else alpha_neg
        q_stage2[s_idx, a2] = q_stage2[s_idx, a2] + lr2 * delta2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model with Passive Decay
This model modifies the standard Hybrid (Model-Based/Model-Free) approach by adding a **decay** mechanism. Instead of "persevering" (sticking to choices), this model assumes that learned values for all options naturally fade (forgetting) over time toward zero. This captures recency effects where older information becomes less influential.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free RL with Passive Decay.
    
    Combines Model-Based planning with Model-Free learning. Additionally, 
    all Q-values (both stages) decay towards zero on each trial, 
    simulating forgetting or a recency bias.

    Parameters:
    learning_rate: [0,1] - Update rate for Q-values.
    beta: [0,10] - Inverse temperature for softmax.
    w: [0,1] - Weighting parameter (0 = Pure MF, 1 = Pure MB).
    decay_rate: [0,1] - Rate at which Q-values decay to 0 on every trial.
    """
    learning_rate, beta, w, decay_rate = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        # Policy for the first choice (Hybrid)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_hybrid = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_hybrid)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        s_idx = state[trial]
        
        # Policy for the second choice (MF only)
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]
        
        # Action Value Updating
        delta1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] = q_stage1_mf[a1] + learning_rate * delta1
        
        delta2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] = q_stage2_mf[s_idx, a2] + learning_rate * delta2
        
        # Passive Decay: All values fade slightly
        q_stage1_mf = q_stage1_mf * (1 - decay_rate)
        q_stage2_mf = q_stage2_mf * (1 - decay_rate)
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```