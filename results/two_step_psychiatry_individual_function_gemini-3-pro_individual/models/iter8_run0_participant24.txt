Here are 3 new cognitive models exploring different mechanisms and parameter combinations not yet fully tested in the provided history.

### Model 1: Hybrid Model with Choice Perseveration
This model extends the standard hybrid Model-Based/Model-Free framework by adding a perseveration parameter. This parameter captures the tendency of participants to repeat their previous first-stage choice, regardless of reward history, which is a common phenomenon in this task.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free RL with Choice Perseveration.
    
    This model combines model-based planning and model-free learning but adds a 
    'perseveration' bonus to the previously chosen action at stage 1. This accounts 
    for habitual repetition ("stickiness") often seen in human data.

    Parameters:
    learning_rate: [0, 1] - Rate at which Q-values are updated.
    beta: [0, 10] - Inverse temperature for softmax choice.
    w: [0, 1] - Weighting parameter (0 = Pure MF, 1 = Pure MB).
    perseveration: [-5, 5] - Bonus added to the Q-value of the previous stage 1 choice.
                             (Positive = stickiness, Negative = alternation)
    """
    learning_rate, beta, w, perseveration = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Initialize Q-values
    q_stage1_mf = np.zeros(2) # MF values for stage 1 (actions A, U)
    q_stage2_mf = np.zeros((2, 2)) # MF values for stage 2 (States X,Y; Actions L,R)
    
    # Track previous action for perseveration
    prev_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based Value: V_MB(s1) = T * max(Q_MF(s2))
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Q-value: Weighted sum of MB and MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add perseveration bonus if not the first trial
        if prev_action_1 != -1:
            q_net[prev_action_1] += perseveration

        # Softmax choice for stage 1
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial] # Observed state (Planet X or Y)
        a1 = action_1[trial]
        prev_action_1 = a1 # Update for next trial

        # --- Stage 2 Policy ---
        # Standard Softmax on stage 2 Q-values
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]

        # --- Updating ---
        # SARSA / TD(0) update for Stage 1 MF values
        # Note: In standard 2-step implementations, Stage 1 MF is often updated 
        # using the Stage 2 value (Q(s2, a2)) rather than the max (Q-learning).
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # TD(0) update for Stage 2 MF values
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Note: Eligibility traces (lambda) are implicitly 0 here as requested 
        # by the exclusion of that parameter combination.

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Pure Model-Based with Forgetting
This model hypothesizes that the participant relies entirely on a Model-Based strategy (planning) but suffers from memory decay. The Q-values at the second stage (the aliens) decay back to a neutral value (0.5) on every trial. This tests the hypothesis that suboptimal performance is due to memory limitations rather than a lack of model-based planning.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Based RL with Forgetting (Decay).
    
    This model assumes the agent is purely Model-Based (w=1 implicitly) regarding 
    structure learning, but their memory of the reward probabilities at the second stage 
    decays over time.
    
    Parameters:
    learning_rate: [0, 1] - Rate at which second-stage Q-values are updated.
    beta: [0, 10] - Inverse temperature for softmax choice.
    decay_rate: [0, 1] - Rate at which unchosen Q-values decay to 0.5.
                         (0 = no decay, 1 = instant forgetting).
    """
    learning_rate, beta, decay_rate = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values for stage 2 (aliens). Initialized to 0.5 (neutral expectation).
    q_stage2 = np.ones((2, 2)) * 0.5

    for trial in range(n_trials):
        # --- Stage 1 Policy (Pure MB) ---
        # Calculate expected value of each spaceship based on transition matrix
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]
  
        # --- Updating ---
        # Update chosen alien
        delta_stage2 = r - q_stage2[state_idx, a2]
        q_stage2[state_idx, a2] += learning_rate * delta_stage2
        
        # Decay ALL Q-values towards 0.5 (passive forgetting)
        # This applies to both the chosen and unchosen options to simulate global memory decay
        q_stage2 = (1 - decay_rate) * q_stage2 + decay_rate * 0.5

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Separate Learning Rates for First and Second Stages (Pure MF)
This model investigates if the participant learns the value of spaceships (Stage 1) and aliens (Stage 2) at different speeds. It is a pure Model-Free learner (no transition matrix usage) but allows for distinct plasticity in the two stages. This helps differentiate if the participant is slow to change spaceship preference but fast to track alien rewards, or vice versa.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free RL with Separate Stage Learning Rates.
    
    This model assumes no knowledge of the transition structure (Pure MF).
    However, it allows for different learning rates for the first stage (spaceships)
    and the second stage (aliens).
    
    Parameters:
    lr_stage1: [0, 1] - Learning rate for spaceship values.
    lr_stage2: [0, 1] - Learning rate for alien values.
    beta: [0, 10] - Inverse temperature for softmax choice.
    """
    lr_stage1, lr_stage2, beta = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy (Pure MF) ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]
  
        # --- Updating ---
        # Update Stage 1 based on Stage 2 value (TD chain)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr_stage1 * delta_stage1
        
        # Update Stage 2 based on reward
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += lr_stage2 * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```