Here are three new cognitive models based on the provided template and participant data, exploring different mechanisms than those already tested.

### Model 1: Hybrid Model with Eligibility Traces (TD($\lambda$))
This model introduces an eligibility trace parameter (`lambda_eligibility`). Instead of just doing a one-step update or a purely model-based update, this model allows the Stage 2 reward to directly influence the Stage 1 Q-value via a trace. This is a classic way to bridge the gap between model-free and model-based-like behavior without a full transition matrix calculation, often called TD($\lambda$). It tests if the participant credits the first-stage choice directly for the second-stage outcome.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    TD(lambda) Model.

    Hypothesis: The participant uses temporal difference learning with eligibility traces.
    Instead of just updating Stage 1 based on Stage 2's value (TD(0)), the Stage 2 prediction error 
    also propagates back to update Stage 1 directly, scaled by lambda.
    
    Parameters:
    learning_rate: [0, 1] - Update rate for Q-values.
    beta: [0, 10] - Inverse temperature for softmax choice.
    lambda_eligibility: [0, 1] - Decay factor for eligibility traces (0 = TD(0), 1 = Monte Carlo).
    """
    learning_rate, beta, lambda_eligibility = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # TD(0) error for stage 1
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        # Standard update for stage 1 based on stage 2 value
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1

        # TD error for stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Update stage 2
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Eligibility trace update: Pass stage 2 error back to stage 1 choice
        q_stage1_mf[action_1[trial]] += learning_rate * lambda_eligibility * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Model-Based with Forgetting
This model assumes the participant is fully Model-Based (calculating values based on the transition matrix) but suffers from memory decay. The Q-values for the aliens (Stage 2) decay back to a neutral value (0.5) on every trial. This tests the hypothesis that the participant relies on the structure of the task (Model-Based) but their knowledge of the specific reward probabilities is volatile.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Based Model with Passive Forgetting.

    Hypothesis: The participant calculates Stage 1 values using the transition matrix (Model-Based).
    However, their memory of the specific alien rewards (Stage 2 Q-values) decays over time 
    towards a neutral prior (0.5).
    
    Parameters:
    learning_rate: [0, 1] - Update rate for Q-values.
    beta: [0, 10] - Inverse temperature for softmax choice.
    forgetting_rate: [0, 1] - Rate at which unchosen Q-values decay to 0.5.
    """
    learning_rate, beta, forgetting_rate = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Initialize Stage 2 values at 0.5 (neutral expectation)
    q_stage2_mf = np.ones((2, 2)) * 0.5

    for trial in range(n_trials):

        # policy for the first choice (Pure Model-Based)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Update chosen Stage 2 value
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Forgetting: Decay ALL Stage 2 Q-values towards 0.5
        # This simulates memory loss between trials
        q_stage2_mf = (1 - forgetting_rate) * q_stage2_mf + forgetting_rate * 0.5
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Separate Learning Rates for Positive/Negative Prediction Errors
This model is a purely Model-Free learner (SARSA-like logic) but distinguishes between "good news" (positive prediction errors) and "bad news" (negative prediction errors). This asymmetry is often observed in psychiatric populations or specific personality traits where individuals might learn faster from punishment than reward, or vice versa.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Asymmetric Learning Rate Model (Dual Alpha).

    Hypothesis: The participant learns differently from positive outcomes (better than expected)
    versus negative outcomes (worse than expected). This is applied to both stages.
    
    Parameters:
    alpha_pos: [0, 1] - Learning rate for positive prediction errors (delta > 0).
    alpha_neg: [0, 1] - Learning rate for negative prediction errors (delta < 0).
    beta: [0, 10] - Inverse temperature for softmax choice.
    """
    alpha_pos, alpha_neg, beta = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Update Stage 1
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        lr_1 = alpha_pos if delta_stage1 > 0 else alpha_neg
        q_stage1_mf[action_1[trial]] += lr_1 * delta_stage1

        # Update Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        lr_2 = alpha_pos if delta_stage2 > 0 else alpha_neg
        q_stage2_mf[state_idx, action_2[trial]] += lr_2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```