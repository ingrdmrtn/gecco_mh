Here are three cognitive models implementing different strategies for the two-step task. They build upon the template and explore different weightings of model-based vs. model-free learning, as well as distinct learning rates for the two stages.

### Model 1: Hybrid Learner with Mixing Parameter `w`
This model combines Model-Based (planning using the transition matrix) and Model-Free (TD learning) strategies. The parameter `w` controls the balance between these two systems for the first-stage choice.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner combining Model-Based and Model-Free values.
    
    This model computes Q-values for the first stage as a weighted sum of 
    Model-Based (MB) and Model-Free (MF) values. 
    MB values plan using the transition matrix.
    MF values learn from direct experience (TD(1) logic implicitly).

    Parameters:
    learning_rate: [0,1] Update rate for Q-values.
    beta: [0,10] Inverse temperature for softmax choice.
    w: [0,1] Weighting parameter (0 = Pure MF, 1 = Pure MB).
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition matrix as per task description
    # Row 0 (Spaceship 0) -> [0.7 to Planet 0, 0.3 to Planet 1]
    # Row 1 (Spaceship 1) -> [0.3 to Planet 0, 0.7 to Planet 1]
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State (Planet) x Action (Alien)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # 1. Calculate Model-Based values
        # Max value achievable at stage 2 for each planet
        max_q_stage2 = np.max(q_stage2_mf, axis=1) 
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Combine MB and MF values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updating ---
        # Update Stage 1 MF value (TD(1)-like update based on Stage 2 value)
        # Note: In standard hybrid models, stage 1 MF is often updated by the 
        # value of the *chosen* second stage option, not just the state value.
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Update Stage 2 MF value based on reward
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Separate Learning Rates for Stage 1 and Stage 2
This model is a pure Model-Free learner (like the feedback example), but it acknowledges that learning dynamics might differ between the abstract choice of spaceships (Stage 1) and the concrete choice of aliens (Stage 2). It uses two distinct learning rates.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Dual Learning Rate Model-Free Learner.
    
    This model assumes the participant learns about spaceships (Stage 1) 
    at a different rate than they learn about aliens (Stage 2).
    It is purely model-free (no transition matrix usage).

    Parameters:
    lr_stage1: [0,1] Learning rate for the first stage choice.
    lr_stage2: [0,1] Learning rate for the second stage choice.
    beta: [0,10] Inverse temperature for softmax.
    """
    lr_stage1, lr_stage2, beta = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updating ---
        # Stage 1 update uses lr_stage1
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr_stage1 * delta_stage1
        
        # Stage 2 update uses lr_stage2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr_stage2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Learner with Choice Perseverance
This extends the hybrid model by adding a perseverance parameter `p`. This parameter captures the tendency to repeat the previous Stage 1 choice regardless of reward (habitual repetition), a common phenomenon in reinforcement learning tasks.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner with Choice Perseverance (Stickiness).
    
    Adds a 'stickiness' bonus to the action taken in the previous trial,
    biasing the agent to repeat choices.
    
    Parameters:
    learning_rate: [0,1] Update rate for Q-values.
    beta: [0,10] Inverse temperature.
    w: [0,1] Weighting (0 = Pure MF, 1 = Pure MB).
    perseverance: [0,5] Bonus added to the previously chosen action's value.
    """
    learning_rate, beta, w, perseverance = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1 # Initialize with an impossible action

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combine MB and MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add perseverance bonus
        if last_action_1 != -1:
            q_net[last_action_1] += perseverance

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Store current action for next trial's perseverance
        last_action_1 = action_1[trial]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updating ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```