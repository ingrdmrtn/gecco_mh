Here are 3 new cognitive models based on the Two-Step task structure.

### Model 1: Rare Transition Dampening
This model hypothesizes that participants may treat "rare" transitions (e.g., choosing Spaceship A but ending up at Planet Y) as "noise" or "errors" in the environment. Consequently, they may dampen the Model-Free learning signal (prediction error) generated from these trials, relying less on them to update their Stage 1 values compared to common transitions.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Rare Transition Dampening Model:
    Modulates the Model-Free learning rate for Stage 1 updates based on whether
    the transition was Common or Rare. Rare transitions produce smaller updates.
    
    Parameters:
    learning_rate: [0, 1] Learning rate for Q-value updates.
    beta: [0, 10] Inverse temperature (exploration/exploitation).
    w: [0, 1] Weight mixing MB (1) and MF (0).
    pers: [0, 5] General perseverance bonus for repeating the last choice.
    rare_damp: [0, 1] Multiplier for the learning rate during rare transitions (0=ignore rare, 1=treat same as common).
    """
    learning_rate, beta, w, pers, rare_damp = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_choice = -1

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Apply perseverance
        if last_choice != -1:
            q_net[last_choice] += pers

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_choice = action_1[trial]
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updating ---
        # Determine if transition was common or rare
        # 0->0 and 1->1 are common (0.7). 0->1 and 1->0 are rare (0.3).
        is_common = (action_1[trial] == state_idx)
        
        # Calculate effective learning rate for Stage 1
        current_lr_stage1 = learning_rate if is_common else (learning_rate * rare_damp)

        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += current_lr_stage1 * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Dual-Beta with Perseverance
This model posits that the level of exploration/randomness differs between the high-level planning stage (Stage 1) and the immediate consumption stage (Stage 2). A participant might be very strategic about choosing a spaceship (high beta 1) but more exploratory when choosing an alien (low beta 2), or vice versa. It includes a perseverance parameter to account for basic repetition bias.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Dual-Beta Model with Perseverance:
    Uses separate inverse temperature parameters for Stage 1 and Stage 2 choices,
    acknowledging that exploration noise may differ between planning and bandit stages.
    
    Parameters:
    learning_rate: [0, 1] Learning rate.
    beta_1: [0, 10] Inverse temperature for Stage 1 (Spaceship choice).
    beta_2: [0, 10] Inverse temperature for Stage 2 (Alien choice).
    w: [0, 1] Weight mixing MB (1) and MF (0).
    pers: [0, 5] Perseverance bonus.
    """
    learning_rate, beta_1, beta_2, w, pers = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_choice = -1

    for trial in range(n_trials):

        # --- Stage 1 Policy (Uses beta_1) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_choice != -1:
            q_net[last_choice] += pers

        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_choice = action_1[trial]
        state_idx = state[trial]

        # --- Stage 2 Policy (Uses beta_2) ---
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updating ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Choice Kernel (Decaying Perseverance)
Instead of a simple "repeat last choice" bonus, this model maintains a "Choice Kernel"â€”an exponentially decaying trace of past choices. This allows the model to capture longer-term habit formation (e.g., if a participant chooses 'A' 10 times in a row, they are much more "stuck" than if they chose it only once).

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Choice Kernel Model:
    Replaces simple perseverance with a 'Choice Kernel' that tracks the history 
    of recent choices using an exponential moving average. 
    
    Parameters:
    learning_rate: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Weight mixing MB (1) and MF (0).
    pers_weight: [0, 5] Weight of the choice kernel added to Q-values (strength of habit).
    pers_decay: [0, 1] Decay rate of the choice kernel (0=remember only last, 1=never forget).
    """
    learning_rate, beta, w, pers_weight, pers_decay = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Choice kernel tracks frequency of past choices [Choice 0, Choice 1]
    choice_kernel = np.zeros(2)

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add Choice Kernel influence to decision values
        q_decision = q_net + (pers_weight * choice_kernel)

        exp_q1 = np.exp(beta * q_decision)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Update Choice Kernel (Exponential Moving Average)
        # 1. Decay both traces
        choice_kernel = choice_kernel * pers_decay
        # 2. Reinforce the chosen action
        choice_kernel[action_1[trial]] += (1 - pers_decay)
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updating ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```