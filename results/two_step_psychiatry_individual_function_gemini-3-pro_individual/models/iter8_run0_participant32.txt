Here are 3 new cognitive models exploring different mechanisms and parameter combinations not yet fully covered in the provided history.

### Model 1: Hybrid Model with Separate Learning Rates for Reward and No-Reward
This model investigates whether the participant learns differently from positive outcomes (finding gold) versus negative outcomes (no gold), often referred to as asymmetric learning rates. This is applied within the standard hybrid Model-Based/Model-Free framework.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF Model with Asymmetric Learning Rates (Positive/Negative).

    This model allows for different learning rates depending on whether the 
    outcome was a reward or an omission. This captures potential biases 
    towards learning more from success or failure.

    Parameters:
    alpha_pos: [0, 1] - Learning rate for positive prediction errors (reward > expected).
    alpha_neg: [0, 1] - Learning rate for negative prediction errors (reward < expected).
    beta: [0, 10] - Inverse temperature for softmax choice.
    w: [0, 1] - Weighting parameter (0 = pure Model-Free, 1 = pure Model-Based).
    """
    alpha_pos, alpha_neg, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # Stage 1 Update (TD(0) style)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        # Apply asymmetric learning rate based on sign of prediction error
        lr_1 = alpha_pos if delta_stage1 > 0 else alpha_neg
        q_stage1_mf[action_1[trial]] += lr_1 * delta_stage1
        
        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        # Apply asymmetric learning rate based on sign of prediction error
        lr_2 = alpha_pos if delta_stage2 > 0 else alpha_neg
        q_stage2_mf[state_idx, action_2[trial]] += lr_2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model with Eligibility Traces (TD(lambda))
This model introduces an eligibility trace parameter (`lambda`). Instead of just updating the first stage value based on the second stage value (TD(0)), this model allows the reward at the end of the trial to directly influence the first-stage choice value, bridging the gap between stages more effectively.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF Model with Eligibility Traces (TD-Lambda).

    Uses an eligibility trace parameter (lambda) to modulate how much the 
    Stage 2 reward prediction error updates the Stage 1 Q-values directly.
    This creates a blend between TD(0) and Monte Carlo updating for the MF system.

    Parameters:
    learning_rate: [0, 1] - Global learning rate.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - Weighting parameter (0 = pure Model-Free, 1 = pure Model-Based).
    lambda_param: [0, 1] - Eligibility trace decay (0 = TD(0), 1 = full propagation).
    """
    learning_rate, beta, w, lambda_param = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # 1. Stage 1 prediction error (TD error at step 1)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # 2. Stage 2 prediction error (TD error at step 2)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # 3. Eligibility Trace Update: Pass Stage 2 error back to Stage 1 choice
        # The Stage 1 action is 'eligible' to be updated by the final reward error
        q_stage1_mf[action_1[trial]] += learning_rate * lambda_param * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Pure Model-Free with Decay
This model tests the hypothesis that the participant is not using a Model-Based strategy at all (no `w` parameter, or effectively `w=0`), but instead relies on a pure Model-Free strategy with a memory decay component. Unchosen actions' Q-values decay back to 0 (or a baseline), representing forgetting.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free Q-Learning with Passive Decay.

    Assumes no model-based planning (w is effectively 0). Instead, it includes
    a decay parameter that causes Q-values of unchosen actions to slowly 
    return to 0.5 (neutral prob), simulating forgetting or regression to mean.

    Parameters:
    learning_rate: [0, 1] - Rate of updating chosen actions.
    beta: [0, 10] - Inverse temperature.
    decay: [0, 1] - Rate at which unchosen Q-values decay toward 0.
    perseveration: [0, 5] - Sticky choice bonus for Stage 1.
    """
    learning_rate, beta, decay, perseveration = model_parameters
    n_trials = len(action_1)
  
    # No transition matrix needed for pure MF
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Choice (Pure MF) ---
        q_net = q_stage1_mf.copy()
        
        if last_action_1 != -1:
            q_net[last_action_1] += perseveration

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # Standard SARSA/Q-learning updates
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # --- Decay for unchosen actions ---
        # Stage 1
        unchosen_1 = 1 - action_1[trial]
        q_stage1_mf[unchosen_1] *= (1 - decay)
        
        # Stage 2 (decay the unchosen option in the *current* state)
        unchosen_2 = 1 - action_2[trial]
        q_stage2_mf[state_idx, unchosen_2] *= (1 - decay)
        
        last_action_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```