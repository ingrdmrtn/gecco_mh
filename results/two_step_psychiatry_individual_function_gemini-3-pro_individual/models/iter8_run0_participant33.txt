Here are three new cognitive models for the two-step task, implemented as standalone Python functions.

### Model 1: Choice Stickiness (Perseveration)
This model extends standard Model-Free Q-learning by adding a "stickiness" parameter. This captures the tendency of participants to repeat their previous first-stage choice regardless of the outcome (motor perseveration), a common phenomenon in this task.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Model with Choice Stickiness (Perseveration).
    
    This model assumes the participant is biased to repeat their previous 
    first-stage action. This 'stickiness' is added to the action logits 
    before the softmax step.
    
    Parameters:
    - learning_rate: Rate at which Q-values update [0, 1].
    - beta: Inverse temperature for softmax choice [0, 10].
    - stickiness: Bonus added to the logit of the previously chosen action [0, 5].
    """
    learning_rate, beta, stickiness = model_parameters
    n_trials = len(action_1)
    
    # Initialize Q-values
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # Planets x Aliens
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_action_1 = -1
    
    for trial in range(n_trials):
        
        # Policy for the first choice
        # Calculate logits based on Q-values
        logits_1 = beta * q_stage1_mf
        
        # Add stickiness bonus to previous action if it exists
        if prev_action_1 != -1:
            logits_1[prev_action_1] += stickiness
            
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        
        # Policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # Action value updating
        # Stage 2 RPE
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Stage 1 RPE (SARSA-style, using value of stage 2 choice)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Store action for next trial's stickiness
        prev_action_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Asymmetric Learning Rates (Optimism Bias)
This model allows for different learning rates depending on whether the prediction error is positive (better than expected) or negative (worse than expected). This can capture optimistic (learning more from wins) or pessimistic (learning more from losses) biases.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Asymmetric Learning Rate Model.
    
    Uses separate learning rates for positive and negative prediction errors.
    This captures biases in how participants update their beliefs following 
    rewards versus omissions.
    
    Parameters:
    - alpha_pos: Learning rate for positive prediction errors (RPE > 0) [0, 1].
    - alpha_neg: Learning rate for negative prediction errors (RPE < 0) [0, 1].
    - beta: Inverse temperature for softmax choice [0, 10].
    """
    alpha_pos, alpha_neg, beta = model_parameters
    n_trials = len(action_1)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        
        # Policy for the first choice
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        
        # Policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # Action value updating
        
        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        lr_2 = alpha_pos if delta_stage2 > 0 else alpha_neg
        q_stage2_mf[state_idx, action_2[trial]] += lr_2 * delta_stage2
        
        # Stage 1 Update
        # Using the value of the chosen stage 2 state-action as the target
        target_val = q_stage2_mf[state_idx, action_2[trial]]
        delta_stage1 = target_val - q_stage1_mf[action_1[trial]]
        lr_1 = alpha_pos if delta_stage1 > 0 else alpha_neg
        q_stage1_mf[action_1[trial]] += lr_1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Forgetful Q-Learning
This model incorporates a decay parameter. Since the reward probabilities in the task drift slowly over time, older values become unreliable. This model decays the Q-values of *unchosen* options towards 0, effectively "forgetting" old information to adapt to the non-stationary environment.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Forgetful Q-Learning / Decay Model.
    
    In addition to standard reinforcement learning updates for chosen actions,
    the Q-values of unchosen actions decay toward zero on each trial. This
    helps the agent adapt to the drifting reward probabilities described in the task.
    
    Parameters:
    - learning_rate: Rate at which chosen Q-values update [0, 1].
    - beta: Inverse temperature for softmax choice [0, 10].
    - decay_rate: Rate at which unchosen Q-values decay [0, 1].
    """
    learning_rate, beta, decay_rate = model_parameters
    n_trials = len(action_1)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        
        # Policy for the first choice
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        
        # Policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # Action value updating
        
        # Stage 2 Update (Chosen)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Stage 2 Decay (Unchosen)
        unchosen_a2 = 1 - action_2[trial]
        q_stage2_mf[state_idx, unchosen_a2] *= (1 - decay_rate)
        
        # Stage 1 Update (Chosen)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Stage 1 Decay (Unchosen)
        unchosen_a1 = 1 - action_1[trial]
        q_stage1_mf[unchosen_a1] *= (1 - decay_rate)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```