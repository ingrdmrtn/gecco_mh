Here are three new cognitive models exploring different mechanisms for learning and decision-making in the two-step task.

### Model 1: Hybrid Learner with Choice Perseveration
This model combines Model-Based (planning) and Model-Free (habitual) value estimation. Crucially, it adds a **perseveration parameter (`p`)** to the first-stage choice. This captures the tendency of participants to repeat their previous spaceship choice regardless of reward, a common "motor" or "habit" bias often observed in this task.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner with Choice Perseveration.
    Combines Model-Based and Model-Free learning with a tendency to repeat
    the previous Stage 1 choice (perseveration).

    Parameters:
    learning_rate: [0,1] Update rate for Q-values.
    beta: [0,10] Inverse temperature for softmax.
    w: [0,1] Weighting parameter (0 = pure MF, 1 = pure MB).
    p: [0,5] Perseveration bonus added to the previously chosen action.
    """
    learning_rate, beta, w, p = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1 # Indicator for no previous action

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        # 1. Model-Based Value: Plan using transition matrix and max stage 2 values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Hybrid Value: Weighted mix
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # 3. Add Perseveration Bonus
        if last_action_1 != -1:
            q_net[last_action_1] += p

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Record this action for the next trial's perseveration
        last_action_1 = action_1[trial] 
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # SARSA(0) / TD(0) update for Stage 1 MF
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Q-Learning update for Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Model-Free Learner with Eligibility Traces (TD(lambda))
This model is a pure Model-Free learner but uses **eligibility traces (`lambda_trace`)** to connect the final reward back to the first-stage choice more efficiently. Instead of just updating the second stage and doing a one-step update for the first stage, the reward prediction error at the second stage "leaks" back to update the first stage choice directly. This allows the model to learn long chains of cause-and-effect without explicit planning.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Learner with Eligibility Traces (TD(lambda)).
    Uses eligibility traces to update Stage 1 values based on Stage 2 outcomes directly.

    Parameters:
    lr: [0,1] Learning rate.
    beta: [0,10] Inverse temperature.
    lambda_trace: [0,1] Decay rate of eligibility trace (0 = TD(0), 1 = Monte Carlo).
    """
    lr, beta, lambda_trace = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        chosen_a1 = action_1[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        chosen_a2 = action_2[trial]
  
        # --- Updates with Eligibility Trace ---
        
        # 1. Prediction Error at Stage 1 transition
        # Value of state arrived at (max Q) minus value of action taken
        # Note: Standard Q-learning uses max over next state for the target
        v_stage2_state = np.max(q_stage2[state_idx]) 
        delta_1 = v_stage2_state - q_stage1[chosen_a1]
        
        # Update Stage 1 immediately based on transition
        q_stage1[chosen_a1] += lr * delta_1
        
        # 2. Prediction Error at Stage 2 outcome
        delta_2 = reward[trial] - q_stage2[state_idx, chosen_a2]
        
        # Update Stage 2
        q_stage2[state_idx, chosen_a2] += lr * delta_2
        
        # Update Stage 1 AGAIN based on Stage 2 error (eligibility trace)
        # The trace for action_1 is 1.0 immediately after choice, decayed by lambda
        q_stage1[chosen_a1] += lr * lambda_trace * delta_2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Separate Learning Rates for Positive/Negative Prediction Errors
This model investigates valence-dependent learning. It posits that the participant might update their beliefs differently when they are positively surprised (get a reward they didn't expect) versus negatively surprised (don't get a reward they expected). This is implemented via two learning rates: `lr_pos` and `lr_neg`. It uses a pure Model-Based framework to keep the parameter count low while testing this specific hypothesis.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Based Learner with Asymmetric Learning Rates.
    Updates Q-values differently depending on whether the prediction error
    is positive (better than expected) or negative (worse than expected).

    Parameters:
    lr_pos: [0,1] Learning rate for positive prediction errors.
    lr_neg: [0,1] Learning rate for negative prediction errors.
    beta: [0,10] Inverse temperature.
    """
    lr_pos, lr_neg, beta = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Store values for the aliens
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy (Pure Model-Based) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates with Asymmetric Learning Rates ---
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Select learning rate based on sign of prediction error
        if delta_stage2 >= 0:
            effective_lr = lr_pos
        else:
            effective_lr = lr_neg
            
        q_stage2_mf[state_idx, action_2[trial]] += effective_lr * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```