Here are the 3 new cognitive models based on the task description and the constraints provided.

### Model 1: Adaptive Model-Based RL
This model assumes the participant is using a Model-Based strategy, but unlike the standard model which assumes fixed transition probabilities (70/30), this agent **learns the transition structure** of the spaceships over time. They update their internal model of how likely Spaceship A is to go to Planet X vs Y based on observation.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Adaptive Model-Based RL.
    
    The agent is purely Model-Based but does not assume the fixed 70/30 transition 
    probabilities. Instead, it learns the transition matrix (T) dynamically 
    based on experienced transitions.
    
    Parameters:
    lr_reward: [0, 1] - Learning rate for Stage 2 values (aliens).
    lr_transition: [0, 1] - Learning rate for updating the Stage 1 transition matrix.
    beta: [0, 10] - Inverse temperature for softmax choice.
    """
    lr_reward, lr_transition, beta = model_parameters
    n_trials = len(action_1)

    # Initialize transition estimates (start uniform 0.5)
    # est_trans_probs[action_1, next_state]
    est_trans_probs = np.ones((2, 2)) * 0.5 
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values for Stage 2 (Aliens)
    q_stage2_mf = np.zeros((2, 2)) # State x Action

    for trial in range(n_trials):
        
        # --- Stage 1 Policy (Model-Based) ---
        # Calculate V_MB for Stage 2 states: max value available in each state
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        
        # Q_MB(action) = Sum( P(state|action) * V(state) )
        q_stage1_mb = est_trans_probs @ max_q_stage2
        
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial] # The planet we actually arrived at
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        
        # 1. Update Transition Matrix (Model Learning)
        # We observed action_1[trial] -> state_idx
        # Increase prob of observed transition, decrease others
        # We perform a delta rule update on the row corresponding to the chosen action
        target_trans = np.zeros(2)
        target_trans[state_idx] = 1.0
        
        curr_trans = est_trans_probs[action_1[trial]]
        est_trans_probs[action_1[trial]] += lr_transition * (target_trans - curr_trans)
        
        # 2. Update Stage 2 Values (Reward Learning)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr_reward * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid RL with Choice Kernel (Trace-Based Habits)
This model combines Model-Based and Model-Free learning (Hybrid), but adds a **Choice Kernel** mechanism. Unlike simple "perseveration" which only looks at the *last* trial, a Choice Kernel builds a decaying trace of past choices. This simulates a stronger, longer-term habit formation that competes with value-based decisions.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF RL with Choice Kernel.
    
    Includes a 'Choice Kernel' that tracks the frequency of past choices.
    This creates a bias towards frequently chosen actions (habit) that 
    decays slowly over time, distinct from reward-driven learning.
    
    Parameters:
    learning_rate: [0, 1] - Update rate for Q-values.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - Weighting parameter (0=Pure MF, 1=Pure MB).
    ck_decay: [0, 1] - Decay rate of the choice kernel (how fast habits fade).
    ck_weight: [0, 5] - Weight of the choice kernel in the decision (strength of habit).
    """
    learning_rate, beta, w, ck_decay, ck_weight = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition matrix for the MB component
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Choice Kernel for Stage 1 actions
    choice_kernel = np.zeros(2)

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        # 1. Model-Based Value
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Hybrid Value
        q_hybrid = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # 3. Add Choice Kernel (Habit)
        q_final = q_hybrid + (ck_weight * choice_kernel)
        
        exp_q1 = np.exp(beta * q_final)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Value Updates ---
        
        # MF Stage 1 Update (SARSA-style for MF component)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # MF Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # --- Choice Kernel Update ---
        # Update trace: The chosen action strengthens, unchosen decays
        # CK(t+1) = CK(t) * (1-decay) + decay * Indicator(choice)
        # This implementation simplifies to: Decay everything, add 1 to chosen
        choice_kernel = choice_kernel * (1 - ck_decay)
        choice_kernel[action_1[trial]] += 1.0 # Add "strength" to chosen action

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid RL with Asymmetric Learning Rates (Dual Alpha)
This model posits that the participant learns differently from positive outcomes (wins) versus negative outcomes (omission of reward). It combines this asymmetric learning with the Hybrid Model-Based/Model-Free architecture.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF RL with Asymmetric Learning (Dual Alpha).
    
    The agent balances Model-Based and Model-Free strategies, but updates 
    values using different learning rates depending on whether the prediction 
    error was positive (better than expected) or negative (worse than expected).
    
    Parameters:
    alpha_pos: [0, 1] - Learning rate for positive prediction errors.
    alpha_neg: [0, 1] - Learning rate for negative prediction errors.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - Weighting parameter (0=Pure MF, 1=Pure MB).
    """
    alpha_pos, alpha_neg, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Asymmetric Updates ---
        
        # Stage 1 MF Update
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        lr_1 = alpha_pos if delta_stage1 > 0 else alpha_neg
        q_stage1_mf[action_1[trial]] += lr_1 * delta_stage1
        
        # Stage 2 MF Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        lr_2 = alpha_pos if delta_stage2 > 0 else alpha_neg
        q_stage2_mf[state_idx, action_2[trial]] += lr_2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```