Here are three new cognitive models for the two-step decision task.

### Model 1: Hybrid Learner with Perseveration
This model integrates Model-Based (planning) and Model-Free (habitual) value estimation, modulated by a mixing weight `w`. Crucially, it adds a `perseveration` parameter, which captures the tendency to repeat the previous Stage 1 choice regardless of reward history. This combination is often considered the "gold standard" for this task but was not listed as a combined configuration in the feedback.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Model 1: Hybrid Learner (MB/MF) with Choice Perseveration.
    Combines model-based planning and model-free learning, plus a sticky choice bias.
    
    Bounds:
    learning_rate: [0, 1] - Rate of Q-value updating.
    beta: [0, 10] - Inverse temperature for softmax.
    w: [0, 1] - Weighting parameter (0 = Pure MF, 1 = Pure MB).
    perseveration: [0, 5] - Bonus added to the previously chosen Stage 1 action.
    """
    learning_rate, beta, w, perseveration = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2) # Model-free values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Values for stage 2 (aliens)

    last_action_1 = -1

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combine MB and MF values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add perseveration bonus
        if last_action_1 != -1:
            q_net[last_action_1] += perseveration
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        last_action_1 = action_1[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Prediction errors and updates
        # Stage 1 MF Update (TD-0 style, linking Stage 1 to Stage 2 value)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Stage 2 MF Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Dual-Beta Model-Free Learner
This model hypothesizes that the participant is purely Model-Free but exhibits different levels of exploration/exploitation noise at the two stages of the task. For instance, they might be very decisive about which spaceship to take (Stage 1) but more random when choosing an alien (Stage 2), or vice versa.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model 2: Dual-Beta Model-Free Learner.
    Assumes pure model-free learning but uses distinct inverse temperatures (betas)
    for the first and second stage choices.
    
    Bounds:
    learning_rate: [0, 1] - Rate of Q-value updating.
    beta_1: [0, 10] - Inverse temperature for Stage 1 (Spaceship choice).
    beta_2: [0, 10] - Inverse temperature for Stage 2 (Alien choice).
    """
    learning_rate, beta_1, beta_2 = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        # Note: q_stage1_mb is calculated for template consistency but unused (Pure MF)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Use beta_1 for stage 1
        exp_q1 = np.exp(beta_1 * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        # Use beta_2 for stage 2
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Prediction errors and updates
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Model-Free Learner with Memory Decay
This model introduces a `decay` parameter. In addition to learning from experienced rewards, the model assumes that the values of *unchosen* actions (both spaceships and aliens) slowly decay towards zero. This mimics a forgetting process or a prior belief that options not recently visited may lose value over time.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model 3: Model-Free Learner with Memory Decay.
    Chosen actions update via learning_rate. Unchosen actions decay towards 0.
    
    Bounds:
    learning_rate: [0, 1] - Rate of Q-value updating for chosen actions.
    beta: [0, 10] - Inverse temperature for softmax.
    decay: [0, 1] - Rate at which unchosen action values decay toward 0.
    """
    learning_rate, beta, decay = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        # Note: q_stage1_mb is calculated for template consistency but unused (Pure MF)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Identify chosen and unchosen indices
        a1_chosen = action_1[trial]
        a1_unchosen = 1 - a1_chosen
        
        a2_chosen = action_2[trial]
        a2_unchosen = 1 - a2_chosen

        # Update Stage 1
        delta_stage1 = q_stage2_mf[state_idx, a2_chosen] - q_stage1_mf[a1_chosen]
        q_stage1_mf[a1_chosen] += learning_rate * delta_stage1
        # Decay unchosen Stage 1
        q_stage1_mf[a1_unchosen] *= (1.0 - decay)

        # Update Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2_chosen]
        q_stage2_mf[state_idx, a2_chosen] += learning_rate * delta_stage2
        # Decay unchosen Stage 2 (only for the current state visited)
        q_stage2_mf[state_idx, a2_unchosen] *= (1.0 - decay)
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```