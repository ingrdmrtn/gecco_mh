Here are 3 new cognitive models based on the two-step task structure, introducing mechanisms not listed in the previous feedback (such as outcome-dependent mixing weights, stage-2 specific perseveration, and transition-type learning modulation).

### Model 1: Outcome-Dependent Arbitration
This model hypothesizes that the balance between Model-Based (planning) and Model-Free (habit) systems is not static, but depends on the previous outcome. For example, an agent might rely more on habits (`w_win`) after a success ("don't fix what isn't broken") but switch to planning (`w_loss`) after a failure ("I need to re-evaluate").

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Outcome-Dependent Arbitration Model.
    
    The mixing weight 'w' (arbitration between MB and MF) changes based on 
    whether the previous trial was rewarded. This captures strategy switching 
    triggered by success or failure.

    Bounds:
    - learning_rate: [0, 1]
    - beta: [0, 10]
    - w_win: [0, 1] Weight for MB system after a Reward.
    - w_loss: [0, 1] Weight for MB system after No Reward (or first trial).
    """
    learning_rate, beta, w_win, w_loss = model_parameters
    n_trials = len(action_1)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)       
    q_stage2_mf = np.zeros((2, 2))  
    
    # Track the outcome of the previous trial to determine w
    last_reward = 0 

    for trial in range(n_trials):
        
        # Determine current mixing weight based on previous outcome
        if last_reward == 1:
            current_w = w_win
        else:
            current_w = w_loss

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2

        # Mix MB and MF using the outcome-dependent w
        q_net = current_w * q_stage1_mb + (1 - current_w) * q_stage1_mf

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        state_idx = state[trial] 

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updating ---
        # Update Stage 1 MF
        chosen_stage2_value = q_stage2_mf[state_idx, action_2[trial]]
        delta_stage1 = chosen_stage2_value - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1

        # Update Stage 2 MF
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Store reward for next trial's arbitration
        last_reward = reward[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Stage 2 Choice Stickiness
Previous models explored stickiness (perseveration) on the first choice (Spaceship). This model introduces stickiness on the *second* choice (Alien). Participants often develop motor habits or preferences for specific aliens within a planet, independent of the reward value. This adds a "bonus" to the Q-value of the alien chosen last time the participant visited that specific planet.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner with Stage 2 Stickiness.
    
    Incorporates a perseveration bonus specifically for the second-stage choice (Aliens).
    If the agent visits a planet, they are more likely to choose the same alien 
    they chose the last time they were at this specific planet.

    Bounds:
    - learning_rate: [0, 1]
    - beta: [0, 10]
    - w: [0, 1]
    - stick_s2: [-5, 5] Bonus added to the previously chosen alien (per state).
    """
    learning_rate, beta, w, stick_s2 = model_parameters
    n_trials = len(action_1)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)       
    q_stage2_mf = np.zeros((2, 2))  
    
    # Track last choice made in each state (Planet 0 and Planet 1)
    # Initialize with -1 (no previous choice)
    last_choice_per_state = [-1, -1]

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2

        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        state_idx = state[trial] 

        # --- Stage 2 Policy ---
        # Calculate values for selection
        q_stage2_selection = q_stage2_mf[state_idx].copy()
        
        # Apply stickiness if we have visited this state before
        prev_action = last_choice_per_state[state_idx]
        if prev_action != -1:
            q_stage2_selection[prev_action] += stick_s2

        exp_q2 = np.exp(beta * q_stage2_selection)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # Update history for this state
        last_choice_per_state[state_idx] = action_2[trial]

        # --- Updating ---
        chosen_stage2_value = q_stage2_mf[state_idx, action_2[trial]]
        delta_stage1 = chosen_stage2_value - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1

        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Rare Transition Modulation
This model posits that the Model-Free system is not entirely blind to the transition structure. While it doesn't build a map, it may dampen learning from "Rare" transitions. If a transition is unexpected (Rare), the agent might treat the subsequent reward prediction error as "noise" or a "fluke" and reduce the learning rate for the Stage 1 update, preventing the habit system from updating strongly on unrepresentative trials.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner with Rare Transition Modulation.
    
    The Model-Free update for the first stage is modulated by the transition type.
    If the transition was 'Rare' (unexpected), the learning rate is scaled by 
    'rare_mod'. This allows the MF system to ignore or amplify outliers caused 
    by stochastic transitions.

    Bounds:
    - learning_rate: [0, 1] Base learning rate.
    - beta: [0, 10]
    - w: [0, 1]
    - rare_mod: [0, 2] Multiplier for learning rate on Rare transitions (0=ignore, 1=same, >1=amplify).
    """
    learning_rate, beta, w, rare_mod = model_parameters
    n_trials = len(action_1)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)       
    q_stage2_mf = np.zeros((2, 2))  

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2

        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        state_idx = state[trial] 

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updating ---
        
        # Determine if transition was Common or Rare
        # Action 0 -> Common State 0. Action 1 -> Common State 1.
        is_common = (action_1[trial] == state_idx)
        
        # Calculate effective learning rate for Stage 1
        if is_common:
            effective_lr = learning_rate
        else:
            effective_lr = learning_rate * rare_mod

        chosen_stage2_value = q_stage2_mf[state_idx, action_2[trial]]
        delta_stage1 = chosen_stage2_value - q_stage1_mf[action_1[trial]]
        
        # Apply modulated learning rate
        q_stage1_mf[action_1[trial]] += effective_lr * delta_stage1

        # Stage 2 update uses standard learning rate (outcome is reliable)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```