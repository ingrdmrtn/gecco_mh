Here are three new cognitive models for the two-step decision task.

### Model 1: Outcome-Dependent Exploration
This model hypothesizes that the participant's level of decision noise (exploration vs. exploitation) changes based on the immediate previous outcome. A win might lead to more deterministic choices (higher beta), while a loss might trigger more random exploration (lower beta).

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free RL with Outcome-Dependent Exploration.
    The inverse temperature (beta) depends on the previous trial's outcome.
    If the previous trial was rewarded, the agent uses `beta_win`.
    If unrewarded, they use `beta_lose`. This captures dynamic shifts in 
    exploration/exploitation strategies.

    Bounds:
    learning_rate: [0, 1]
    beta_win: [0, 10]
    beta_lose: [0, 10]
    """
    learning_rate, beta_win, beta_lose = model_parameters
    n_trials = len(action_1)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    # Initialize previous reward (0 implies 'loss' state or neutral start)
    last_reward = 0 

    for trial in range(n_trials):
        # Select beta based on previous outcome
        current_beta = beta_win if last_reward == 1 else beta_lose

        # Stage 1 Choice
        exp_q1 = np.exp(current_beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # Stage 2 Choice
        exp_q2 = np.exp(current_beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # Learning (Standard TD)
        # Stage 1 Update (SARSA-style target from Stage 2 Q-value)
        delta_stage1 = q_stage2[state_idx, action_2[trial]] - q_stage1[action_1[trial]]
        q_stage1[action_1[trial]] += learning_rate * delta_stage1
        
        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2[state_idx, action_2[trial]]
        q_stage2[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        last_reward = reward[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model with Eligibility Traces
This model combines Model-Based planning with a Model-Free system that utilizes eligibility traces (TD-Lambda). The eligibility trace allows the reward received at the end of the trial (Stage 2) to directly influence the value of the starting choice (Stage 1), bridging the temporal gap more effectively than simple TD(0).

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Eligibility Traces (TD-Lambda).
    Combines Model-Based planning with a Model-Free system that uses eligibility traces.
    The eligibility trace (lambda) allows the Stage 2 reward prediction error to directly
    update Stage 1 values, scaled by lambda.
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    w: [0, 1] - Weight for Model-Based values (0=Pure MF, 1=Pure MB)
    lambda_eligibility: [0, 1] - Decay factor for eligibility traces connecting Stage 2 to Stage 1.
    """
    learning_rate, beta, w, lambda_eligibility = model_parameters
    n_trials = len(action_1)
    
    # Fixed transition knowledge for the Model-Based component
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Model-Based Valuation ---
        # MB uses the max of the MF Q-values from the second stage to plan
        max_q_stage2 = np.max(q_stage2_mf, axis=1) 
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # --- Hybrid Integration ---
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Stage 1 Choice
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # Stage 2 Choice (Purely driven by MF values in standard hybrid models)
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        # Stage 1 RPE (Prediction error between Stage 1 value and Stage 2 value)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        # Stage 2 RPE (Prediction error between Stage 2 value and Reward)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Update Stage 2
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Update Stage 1 
        # With eligibility trace: Stage 1 is updated by its own error PLUS the discounted error from Stage 2
        q_stage1_mf[action_1[trial]] += learning_rate * (delta_stage1 + lambda_eligibility * delta_stage2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Model-Free with Counterfactual Stage 2 Update
This model assumes that when participants choose an alien and receive a reward (or lack thereof), they also update their belief about the *unchosen* alien. Specifically, they update the unchosen option towards the opposite outcome (e.g., if chosen=0, assume unchosen=1), representing a "fictitious play" or counterfactual reasoning strategy.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free RL with Counterfactual Updating.
    In Stage 2 (the planets), the agent updates the value of the chosen alien
    based on the reward received. Additionally, the agent updates the *unchosen*
    alien based on a counterfactual assumption that the rewards are anti-correlated
    (i.e., if I got 0, the other likely had 1).
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    cf_learning_rate: [0, 1] - Learning rate for the unchosen option.
    """
    learning_rate, beta, cf_learning_rate = model_parameters
    n_trials = len(action_1)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        # Stage 1 Choice
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # Stage 2 Choice
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # Updates
        delta_stage1 = q_stage2[state_idx, action_2[trial]] - q_stage1[action_1[trial]]
        q_stage1[action_1[trial]] += learning_rate * delta_stage1
        
        # Stage 2 Update (Chosen)
        chosen_a2 = action_2[trial]
        delta_stage2 = reward[trial] - q_stage2[state_idx, chosen_a2]
        q_stage2[state_idx, chosen_a2] += learning_rate * delta_stage2
        
        # Stage 2 Update (Unchosen / Counterfactual)
        unchosen_a2 = 1 - chosen_a2
        # Assume the unchosen option would have yielded (1 - reward)
        counterfactual_reward = 1 - reward[trial]
        delta_cf = counterfactual_reward - q_stage2[state_idx, unchosen_a2]
        q_stage2[state_idx, unchosen_a2] += cf_learning_rate * delta_cf

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```