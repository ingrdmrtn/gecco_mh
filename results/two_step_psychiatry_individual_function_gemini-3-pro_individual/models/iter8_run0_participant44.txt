Here are 3 new cognitive models based on the two-step task structure, introducing mechanisms for asymmetric learning, stage-specific exploration, and memory decay.

### Model 1: Asymmetric Learning Rates (Win/Loss)
This model hypothesizes that the participant learns differently from positive outcomes (finding gold) versus negative outcomes (no gold). This is biologically inspired by separate dopaminergic pathways for positive and negative prediction errors.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF model with Asymmetric Learning Rates.
    
    Distinguishes between learning from positive rewards (alpha_pos) and 
    negative/zero rewards (alpha_neg). This captures potential biases 
    towards learning more from wins or losses.

    Parameters:
    alpha_pos: [0,1] - Learning rate when reward is received (1).
    alpha_neg: [0,1] - Learning rate when no reward is received (0).
    beta: [0,10] - Inverse temperature for softmax.
    w: [0,1] - Weighting parameter (0 = Pure MF, 1 = Pure MB).
    """
    alpha_pos, alpha_neg, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid value integration
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Select learning rate based on the trial outcome (reward)
        # If the trial was a "win", we use alpha_pos for all updates this trial
        # If "loss", we use alpha_neg.
        current_alpha = alpha_pos if reward[trial] == 1 else alpha_neg

        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += current_alpha * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += current_alpha * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Stage-Specific Exploration (Dual Beta)
This model suggests that the level of "noise" or exploration differs between the two stages. Stage 1 involves abstract planning (Spaceships -> Planets), while Stage 2 is a concrete bandit task (Aliens -> Gold). The participant may be more decisive in one stage than the other.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF model with Stage-Specific Exploration parameters.
    
    Allows for different inverse temperatures (beta) for Stage 1 and Stage 2 choices.
    The participant might be more exploratory in the spaceship choice but 
    more exploitative in the alien choice, or vice versa.

    Parameters:
    learning_rate: [0,1] - Update rate for Q-values.
    beta_1: [0,10] - Inverse temperature for Stage 1 (Spaceship choice).
    beta_2: [0,10] - Inverse temperature for Stage 2 (Alien choice).
    w: [0,1] - Weighting parameter (0 = Pure MF, 1 = Pure MB).
    """
    learning_rate, beta_1, beta_2, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Use beta_1 for the first stage
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        # Use beta_2 for the second stage
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Learning with Memory Decay
This model incorporates a forgetting mechanism. While chosen actions are updated via reinforcement learning, unchosen actions slowly decay toward zero. This accounts for the possibility that the participant loses confidence in options they haven't sampled recently.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF model with Memory Decay for Unchosen Options.
    
    Q-values for actions NOT chosen decay towards 0 at a rate defined by 'decay'.
    This simulates forgetting or a loss of confidence in unsampled options over time.

    Parameters:
    learning_rate: [0,1] - Update rate for Q-values of chosen actions.
    beta: [0,10] - Inverse temperature for softmax.
    w: [0,1] - Weighting parameter (0 = Pure MF, 1 = Pure MB).
    decay: [0,1] - Rate at which unchosen Q-values decay to 0 (0=no decay, 1=instant forget).
    """
    learning_rate, beta, w, decay = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Standard Updates for Chosen Actions
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Decay for Unchosen Actions
        # Stage 1 unchosen
        unchosen_1 = 1 - action_1[trial]
        q_stage1_mf[unchosen_1] *= (1 - decay)
        
        # Stage 2 unchosen (only for the state visited)
        unchosen_2 = 1 - action_2[trial]
        q_stage2_mf[state_idx, unchosen_2] *= (1 - decay)
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```