Here are three new cognitive models for the two-step task. These models explore different mechanisms such as separate learning rates for model-free/model-based components, inverse temperature scaling, and outcome-specific perseveration.

### Model 1: Hybrid Model with Separate Learning Rates for Model-Based and Model-Free
This model modifies the standard hybrid architecture by allowing the Model-Free (MF) and Model-Based (MB) systems to learn at different speeds. In standard models, `learning_rate` often applies to all value updates. Here, we hypothesize that the second-stage values (which drive the MB system's planning) might be updated at a different rate (`lr_mb`) than the first-stage MF values (`lr_mf`), reflecting different cognitive costs or plasticity in these systems.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Separate MF and MB Learning Rates.
    
    This model distinguishes between the learning rate used for the first-stage 
    Model-Free values (habitual caching of spaceship values) and the learning 
    rate used for second-stage values (which feed into Model-Based planning).

    Parameters:
    lr_mf: [0, 1] - Learning rate for first-stage Model-Free Q-values.
    lr_mb: [0, 1] - Learning rate for second-stage Q-values (used for MB planning).
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - Weighting parameter (1 = fully Model-Based, 0 = fully Model-Free).
    """
    lr_mf, lr_mb, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combine MB and MF values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # policy for the second choice
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Update Stage 1 MF values using the specific MF learning rate
        # Note: We use the value of the state reached (q_stage2_mf) as the TD target
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr_mf * delta_stage1
        
        # Update Stage 2 values (which drive MB planning) using the specific MB/Stage 2 rate
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr_mb * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model with Beta Scaling (Stage 2 Confidence)
Standard models assume the same level of exploration/exploitation noise (`beta`) at both decision stages. However, participants might be more decisive (higher beta) or more random (lower beta) once they reach the planet stage compared to the initial spaceship choice. This model introduces `beta_scale`, a multiplier for the second-stage inverse temperature, allowing the model to capture differential decisiveness between the abstract first stage and the concrete second stage.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Beta Scaling for Second Stage.
    
    This model assumes the inverse temperature (exploration/exploitation balance)
    might differ between the two stages. It uses a base beta for stage 1 and 
    scales it by `beta_scale` for stage 2.

    Parameters:
    learning_rate: [0, 1] - Update rate for Q-values.
    beta: [0, 10] - Base inverse temperature (Stage 1).
    w: [0, 1] - Weighting parameter (1 = fully Model-Based, 0 = fully Model-Free).
    beta_scale: [0, 5] - Multiplier for beta at Stage 2 (beta_2 = beta * beta_scale).
    """
    learning_rate, beta, w, beta_scale = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        # Use scaled beta for the second stage
        beta_2 = beta * beta_scale
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta_2 * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model with Reward-Dependent Perseveration
Simple perseveration assumes a tendency to repeat the last choice regardless of the outcome. However, "Win-Stay, Lose-Shift" strategies suggest that perseveration might be modulated by the previous reward. This model splits perseveration into two parameters: `pers_rew` (tendency to repeat after a reward) and `pers_unrew` (tendency to repeat after no reward). This captures more nuanced primitive strategies that might coexist with MB/MF learning.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Reward-Dependent Perseveration.
    
    This model adds a bias to the first-stage choice based on the previous action,
    but the magnitude of this bias depends on whether the previous trial was 
    rewarded or unrewarded.

    Parameters:
    learning_rate: [0, 1] - Update rate for Q-values.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - Weighting parameter (1 = fully Model-Based, 0 = fully Model-Free).
    pers_rew: [0, 2] - Perseveration bonus if the previous trial was rewarded.
    pers_unrew: [0, 2] - Perseveration bonus if the previous trial was unrewarded.
    """
    learning_rate, beta, w, pers_rew, pers_unrew = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    last_reward = 0

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Apply reward-dependent perseveration
        if last_action_1 != -1:
            if last_reward == 1:
                q_net[last_action_1] += pers_rew
            else:
                q_net[last_action_1] += pers_unrew

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        last_action_1 = action_1[trial]
        last_reward = reward[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```