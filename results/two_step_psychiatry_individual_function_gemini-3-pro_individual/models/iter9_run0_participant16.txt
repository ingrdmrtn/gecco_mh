Here are three new cognitive models that explore different mechanisms and parameter combinations than those previously tested.

### Model 1: Hybrid Model with Separate Learning Rates for Stage 1 and Stage 2
This model hypothesizes that the participant might update their values at different rates for the spaceship choice (Stage 1) versus the alien choice (Stage 2). This is often plausible because the second stage is a direct bandit task with immediate feedback, while the first stage is more abstract. It uses a mixing parameter `w` to blend Model-Based (MB) and Model-Free (MF) values.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF model with separate learning rates for Stage 1 and Stage 2.
    
    This model allows for different plasticity in the two stages of the task.
    It combines model-based and model-free values using a mixing weight 'w'.
    
    Parameters:
    lr_stage1: [0, 1] - Learning rate for first-stage (spaceship) values.
    lr_stage2: [0, 1] - Learning rate for second-stage (alien) values.
    beta: [0, 10] - Inverse temperature for softmax.
    w: [0, 1] - Mixing weight (0 = Pure MF, 1 = Pure MB).
    """
    lr_stage1, lr_stage2, beta, w = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition matrix as per task description (A->X common, U->Y common)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # 2 planets, 2 aliens per planet

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        # Model-Based Value: Transition * Max(Stage 2 values)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # Update Stage 2 values (Alien choice)
        # Standard Q-learning
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr_stage2 * delta_stage2
        
        # Update Stage 1 values (Spaceship choice)
        # Using TD(0) update: difference between Stage 2 value and Stage 1 value
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr_stage1 * delta_stage1
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Pure Model-Based with Choice Perseverance
This model assumes the participant is purely Model-Based (ignoring Model-Free caching at stage 1) but exhibits "perseverance" (or stickiness). This means they have a tendency to repeat their previous Stage 1 choice regardless of the reward, captured by a `perseverance` parameter. This is a common heuristic in behavioral data.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Based model with Choice Perseverance.
    
    This model assumes the agent calculates values based solely on the transition 
    structure (MB) but has a bias to repeat the previous first-stage action.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate for second-stage values.
    beta: [0, 10] - Inverse temperature for softmax.
    perseverance: [0, 5] - Bonus added to the Q-value of the previously chosen action.
    """
    learning_rate, beta, perseverance = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Only need Stage 2 values, Stage 1 is calculated on the fly via MB
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1 # Initialize with an impossible action

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Add perseverance bonus
        q_net = q_stage1_mb.copy()
        if last_action_1 != -1:
            q_net[last_action_1] += perseverance
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Store action for next trial's perseverance
        last_action_1 = action_1[trial]
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # Update Stage 2 values based on reward
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model with Asymmetric Learning Rates (Positive vs Negative)
This model introduces asymmetric updating for positive and negative prediction errors. The participant might learn more aggressively from rewards (positive RPE) than from omissions (negative RPE), or vice versa. This is applied to the second-stage values, which then propagate up to the first stage via the hybrid mechanism.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF model with Asymmetric Learning Rates (Pos/Neg).
    
    This model differentiates between learning from positive prediction errors
    (getting gold) and negative prediction errors (getting nothing).
    
    Parameters:
    lr_pos: [0, 1] - Learning rate for positive prediction errors (RPE > 0).
    lr_neg: [0, 1] - Learning rate for negative prediction errors (RPE < 0).
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - Mixing weight (0 = Pure MF, 1 = Pure MB).
    """
    lr_pos, lr_neg, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # Calculate RPE for Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Apply asymmetric learning rate
        if delta_stage2 >= 0:
            eff_lr = lr_pos
        else:
            eff_lr = lr_neg
            
        q_stage2_mf[state_idx, action_2[trial]] += eff_lr * delta_stage2
        
        # Update Stage 1 MF values (using TD(0) logic, assuming symmetric LR here for simplicity/param count)
        # We use the average of lr_pos and lr_neg for the stage 1 update to save a parameter slot
        avg_lr = (lr_pos + lr_neg) / 2.0
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += avg_lr * delta_stage1
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```