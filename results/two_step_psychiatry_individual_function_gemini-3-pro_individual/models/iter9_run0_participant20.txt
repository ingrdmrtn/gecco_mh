Here are 3 new cognitive models based on the two-step task structure. These models explore memory decay, stage-specific learning rates combined with stickiness, and reward-dependent perseveration heuristics.

### Model 1: Hybrid Model with Q-Value Decay (Forgetting)
This model introduces a **decay** parameter. In standard RL, Q-values remain constant until the action is chosen again. In this model, the Q-values of *unchosen* actions passively decay toward zero on every trial. This mimics memory trace decay or a "use-it-or-lose-it" heuristic, which can be crucial in volatile environments where old information becomes obsolete.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Passive Q-value Decay.
    
    Incorporates a forgetting mechanism where unchosen action values decay 
    towards 0 on each trial. This captures the cognitive cost of maintaining 
    value representations over time.

    Parameters:
    learning_rate: [0, 1] Standard Rescorla-Wagner update rate.
    beta: [0, 10] Inverse temperature for softmax.
    w: [0, 1] Weight for Model-Based control (1 = Pure MB, 0 = Pure MF).
    decay_rate: [0, 1] Rate at which unchosen Q-values decay to 0 (0 = no decay, 1 = instant forgetting).
    """
    learning_rate, beta, w, decay_rate = model_parameters
    n_trials = len(action_1)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)       
    q_stage2_mf = np.zeros((2, 2))  

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Policy ---
        state_idx = state[trial]
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        # 1. Decay unchosen Stage 1 values
        unchosen_1 = 1 - action_1[trial]
        q_stage1_mf[unchosen_1] *= (1 - decay_rate)
        
        # 2. Decay unchosen Stage 2 values (for the current state)
        unchosen_2 = 1 - action_2[trial]
        q_stage2_mf[state_idx, unchosen_2] *= (1 - decay_rate)
        # Note: We could also decay the unvisited state's Q-values, but standard decay usually applies to available options.

        # 3. Standard RL Updates
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1

        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Dual-Learning Rate Hybrid with Perseveration
This model acknowledges that learning dynamics often differ between the abstract first stage (choosing a spaceship) and the concrete second stage (choosing an alien). It assigns separate learning rates to each stage. Additionally, it includes a perseveration parameter, combining distinct learning speeds with the tendency to repeat choices.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Separate Stage Learning Rates and Perseveration.
    
    Allows for different plasticity in Stage 1 vs Stage 2, plus a 
    stickiness bias. This accounts for the possibility that the participant 
    learns direct alien values (Stage 2) at a different speed than spaceship 
    values (Stage 1).

    Parameters:
    lr_stage1: [0, 1] Learning rate for the first decision stage.
    lr_stage2: [0, 1] Learning rate for the second decision stage.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Model-Based weight.
    perseveration: [0, 5] Bonus added to the logits of the previously chosen action.
    """
    lr_stage1, lr_stage2, beta, w, perseveration = model_parameters
    n_trials = len(action_1)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)       
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net
        # Apply perseveration bonus
        if last_action_1 != -1:
            logits_1[last_action_1] += perseveration
            
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Policy ---
        state_idx = state[trial]
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        # Update Stage 1 with specific learning rate
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr_stage1 * delta_stage1

        # Update Stage 2 with specific learning rate
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr_stage2 * delta_stage2
        
        last_action_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model with Outcome-Dependent Perseveration
Standard perseveration assumes the agent repeats choices regardless of the outcome ("habit"). This model splits perseveration into "Win-Stay" and "Lose-Stay" components. It tests if the participant's stickiness is driven by the previous reward (heuristic strategy) or if they are simply sticky regardless of success.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Reward-Dependent Perseveration (Win-Stay/Lose-Stay).
    
    Instead of a single 'stickiness' parameter, this model applies a different 
    bias depending on whether the previous trial was rewarded. This captures 
    heuristic 'Win-Stay, Lose-Shift' dynamics overlaid on the RL process.

    Parameters:
    learning_rate: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Model-Based weight.
    persev_win: [0, 5] Stickiness bonus if the previous trial was Rewarded (Win-Stay).
    persev_loss: [0, 5] Stickiness bonus if the previous trial was Unrewarded (Lose-Stay).
    """
    learning_rate, beta, w, persev_win, persev_loss = model_parameters
    n_trials = len(action_1)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)       
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    last_reward = 0

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net
        
        # Apply outcome-dependent perseveration
        if last_action_1 != -1:
            if last_reward == 1:
                logits_1[last_action_1] += persev_win
            else:
                logits_1[last_action_1] += persev_loss
            
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Policy ---
        state_idx = state[trial]
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1

        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        last_action_1 = action_1[trial]
        last_reward = reward[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```