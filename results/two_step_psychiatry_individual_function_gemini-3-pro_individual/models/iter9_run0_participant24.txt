Here are three new cognitive models based on the provided template and participant data structure.

### Model 1: Dual Beta Hybrid Model
This model hypothesizes that the participant's exploration-exploitation trade-off differs between the two stages. They may be more precise (higher beta) in the simple bandit task (Stage 2) while being more exploratory or noisy (lower beta) in the planning task (Stage 1), or vice versa.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Dual Beta Hybrid Model.
    
    Distinguishes between the decision noise (inverse temperature) in Stage 1 
    (choosing a spaceship) and Stage 2 (choosing an alien).
    
    Parameters:
    learning_rate: [0, 1] - Update rate for Q-values.
    beta_1: [0, 10] - Inverse temperature for Stage 1 choice.
    beta_2: [0, 10] - Inverse temperature for Stage 2 choice.
    w: [0, 1] - Balance between Model-Based (1) and Model-Free (0) control.
    """
    learning_rate, beta_1, beta_2, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Policy for the first choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Weighted combination of MB and MF values
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax using beta_1
        exp_q1 = np.exp(beta_1 * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Policy for the second choice ---
        # Softmax using beta_2 (Stage 2 is purely model-free in this paradigm)
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]

        # Stage 1 Update (SARSA-style: using the value of the state-action actually selected in stage 2)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Stage 2 Update (Reward prediction error)
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Memory Decay Hybrid Model
This model introduces a forgetting mechanism. In addition to learning from rewards, the participant's value estimates decay over time towards zero. This accounts for memory limitations or a prior belief that the environment is volatile and unvisited options lose value.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Memory Decay Hybrid Model.
    
    Incorporates a passive decay parameter. On every trial, all Q-values 
    decay slightly towards 0, simulating forgetting or volatility beliefs.
    
    Parameters:
    learning_rate: [0, 1] - Rate of learning from prediction errors.
    beta: [0, 10] - Inverse temperature for softmax choice.
    w: [0, 1] - Weighting parameter (0 = Pure MF, 1 = Pure MB).
    decay_rate: [0, 1] - Rate at which stored Q-values decay to 0 each trial.
    """
    learning_rate, beta, w, decay_rate = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Policy for the first choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Policy for the second choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]

        # Standard MF updates
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # --- Memory Decay ---
        # All Q-values decay towards 0
        q_stage1_mf *= (1 - decay_rate)
        q_stage2_mf *= (1 - decay_rate)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Dual Learning Rate with Eligibility Trace
This model allows for different learning rates in Stage 1 and Stage 2, combined with an eligibility trace. This captures the possibility that the participant learns the value of spaceships (Stage 1) at a different speed than the value of aliens (Stage 2), while still allowing the final reward to influence the first stage choice via the eligibility trace.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Dual Learning Rate with Eligibility Trace.
    
    Allows separate learning rates for Stage 1 (spaceships) and Stage 2 (aliens),
    plus an eligibility trace that allows the Stage 2 reward to update Stage 1
    values directly.
    
    Parameters:
    lr_stage1: [0, 1] - Learning rate for Stage 1 updates.
    lr_stage2: [0, 1] - Learning rate for Stage 2 updates.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - MB/MF weighting.
    lambda_eligibility: [0, 1] - Strength of eligibility trace connecting R to Stage 1.
    """
    lr_stage1, lr_stage2, beta, w, lambda_eligibility = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Policy for the first choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Policy for the second choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]

        # Update Stage 1 using Stage 1 Learning Rate
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr_stage1 * delta_stage1
        
        # Update Stage 2 using Stage 2 Learning Rate
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += lr_stage2 * delta_stage2
        
        # Eligibility Trace: Update Stage 1 again based on Stage 2 RPE
        # We use lr_stage1 for the scaling as it modifies a Stage 1 value
        q_stage1_mf[a1] += lr_stage1 * lambda_eligibility * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```