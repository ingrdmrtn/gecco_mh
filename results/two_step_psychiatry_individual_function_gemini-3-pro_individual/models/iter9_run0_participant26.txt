def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free RL with Perseveration.
    
    This model assumes the participant uses a combination of two systems:
    1. A Model-Based (MB) system that uses the known transition matrix to plan.
    2. A Model-Free (MF) system that learns values from direct experience (TD learning).
    The choices are a weighted mixture of these systems. Additionally, a perseveration 
    bonus is added to repeat the previous Stage 1 choice ("stickiness").

    Parameters:
    learning_rate: [0, 1] - Update rate for Q-values (Stage 2 and MF Stage 1).
    beta: [0, 10] - Inverse temperature for softmax choice (exploration/exploitation).
    w: [0, 1] - Weighting parameter (0 = Pure MF, 1 = Pure MB).
    perseveration: [0, 5] - Bonus added to the Q-value of the previously chosen action at Stage 1.
    """
    learning_rate, beta, w, perseveration = model_parameters
    n_trials = len(action_1)
    
    # Transition matrix (70% common, 30% rare)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2) # Model-Free values for Stage 1
    q_stage2_mf = np.zeros((2, 2)) # Values for Stage 2 (shared resource for MB/MF)
    
    last_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        
        # 1. Model-Based Value Calculation
        # V(state) = max_a Q(state, a)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        # Q_MB(a) = sum P(s'|a) * V(s')
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Hybrid Value Calculation
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # 3. Add Perseveration
        if last_action_1 != -1:
            q_net[last_action_1] += perseveration
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        
        # MF Stage 1 Update (TD(0))
        # Update Q_MF(s1, a1) towards Q(s2, a2)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Stage 2 Update
        # Update Q(s2, a2) towards reward
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        last_action_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free TD(Lambda) with Perseveration.
    
    This model relies purely on Model-Free reinforcement learning but uses eligibility 
    traces (lambda) to bridge the two stages. This allows the reward received at Stage 2 
    to directly update the value of the Stage 1 choice. It also includes choice perseveration.

    Parameters:
    learning_rate: [0, 1] - Update rate for Q-values.
    beta: [0, 10] - Inverse temperature for softmax choice.
    lambda_eligibility: [0, 1] - Eligibility trace decay (0 = TD(0), 1 = Monte Carlo).
    perseveration: [0, 5] - Bonus added to the Q-value of the previously chosen action at Stage 1.
    """
    learning_rate, beta, lambda_eligibility, perseveration = model_parameters
    n_trials = len(action_1)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        q_net = q_stage1_mf.copy()
        
        # Add Perseveration
        if last_action_1 != -1:
            q_net[last_action_1] += perseveration
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        
        # 1. Prediction error at Stage 1 (State 1 -> State 2)
        # Value of chosen S1 action vs Value of chosen S2 action
        delta_1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        # Standard TD update for S1
        q_stage1_mf[action_1[trial]] += learning_rate * delta_1
        
        # 2. Prediction error at Stage 2 (State 2 -> Reward)
        delta_2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Standard TD update for S2
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_2
        
        # 3. Eligibility Trace Update for S1
        # The S1 choice is also eligible for the S2 reward error, scaled by lambda
        q_stage1_mf[action_1[trial]] += learning_rate * lambda_eligibility * delta_2
        
        last_action_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Asymmetric Learning Rate Model-Free RL with Perseveration.
    
    This model posits that the participant learns differently from positive prediction errors 
    (better than expected) versus negative prediction errors (worse than expected). 
    It includes choice perseveration to account for behavioral stickiness.
    
    Parameters:
    alpha_pos: [0, 1] - Learning rate for positive prediction errors (delta > 0).
    alpha_neg: [0, 1] - Learning rate for negative prediction errors (delta < 0).
    beta: [0, 10] - Inverse temperature for softmax choice.
    perseveration: [0, 5] - Bonus added to the Q-value of the previously chosen action at Stage 1.
    """
    alpha_pos, alpha_neg, beta, perseveration = model_parameters
    n_trials = len(action_1)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        q_net = q_stage1_mf.copy()
        
        # Add Perseveration
        if last_action_1 != -1:
            q_net[last_action_1] += perseveration
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        
        # Stage 1 Update
        delta_1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        # Select learning rate based on sign of delta
        lr_1 = alpha_pos if delta_1 > 0 else alpha_neg
        q_stage1_mf[action_1[trial]] += lr_1 * delta_1
        
        # Stage 2 Update
        delta_2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        # Select learning rate based on sign of delta
        lr_2 = alpha_pos if delta_2 > 0 else alpha_neg
        q_stage2_mf[state_idx, action_2[trial]] += lr_2 * delta_2
        
        last_action_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss