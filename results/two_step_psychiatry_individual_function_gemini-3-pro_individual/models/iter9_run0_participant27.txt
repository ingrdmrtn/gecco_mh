Here are the 3 proposed cognitive models.

### Model 1: Hybrid MB/MF with Perseveration
This model extends the standard hybrid Model-Based/Model-Free architecture by adding a **perseveration** parameter. While the agent weighs model-based and model-free values using `w`, they also have a tendency to repeat their previous Stage 1 choice regardless of reward history (motor perseveration), captured by the parameter `pers`.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF with Perseveration.
    
    Combines Model-Based and Model-Free values using a mixing weight 'w',
    but adds a 'stickiness' or perseveration bonus to the previously chosen
    Stage 1 action.
    
    Parameters:
    - learning_rate: [0, 1] Update rate for Q-values.
    - beta: [0, 10] Inverse temperature for softmax.
    - w: [0, 1] Mixing weight. 1 = Pure MB, 0 = Pure MF.
    - pers: [0, 5] Perseveration bonus added to the previous action's value.
    """
    learning_rate, beta, w, pers = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track previous action for perseveration (initialize to -1 or None)
    last_action_1 = -1

    for trial in range(n_trials):

        # --- Policy for the first choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Weighted mixture of MB and MF
        q_hybrid = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add perseveration bonus if a previous action exists
        if last_action_1 != -1:
            q_hybrid[last_action_1] += pers

        exp_q1 = np.exp(beta * q_hybrid)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Store current action for next trial's perseveration
        last_action_1 = action_1[trial]
        state_idx = state[trial]

        # --- Policy for the second choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # TD(0) update for Stage 1 MF
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # TD(0) update for Stage 2 (used by both MB and MF logic)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid MB/MF with Asymmetric Learning Rates
This model hypothesizes that the participant learns differently from positive prediction errors (rewards better than expected) versus negative prediction errors (rewards worse than expected). It applies a standard hybrid `w` structure but splits the learning rate into `lr_pos` and `lr_neg`.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF with Asymmetric Learning Rates.
    
    Uses separate learning rates for positive and negative prediction errors.
    This captures biases where an agent might learn quickly from rewards but 
    slowly from lack of rewards (or vice versa).
    
    Parameters:
    - lr_pos: [0, 1] Learning rate when prediction error is positive.
    - lr_neg: [0, 1] Learning rate when prediction error is negative.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight (MB vs MF).
    """
    lr_pos, lr_neg, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Policy for the first choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_hybrid = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_hybrid)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # --- Policy for the second choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # Stage 1 MF Update with asymmetry
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        if delta_stage1 > 0:
            q_stage1_mf[action_1[trial]] += lr_pos * delta_stage1
        else:
            q_stage1_mf[action_1[trial]] += lr_neg * delta_stage1
            
        # Stage 2 Update with asymmetry
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        if delta_stage2 > 0:
            q_stage2_mf[state_idx, action_2[trial]] += lr_pos * delta_stage2
        else:
            q_stage2_mf[state_idx, action_2[trial]] += lr_neg * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid MB/MF with Passive Decay
This model introduces a **decay** parameter. In dynamic environments, old information becomes stale. While the chosen option is updated via the learning rate, the *unchosen* options in Stage 2 (the aliens not visited) decay toward 0. This allows the agent to "forget" old values, promoting re-exploration of neglected options or simply accounting for volatility.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF with Passive Decay of Unchosen Options.
    
    Standard Hybrid model, but Q-values for unchosen aliens decay over time.
    This helps the model adapt to the slowly changing reward probabilities 
    described in the task by reducing the influence of outdated experiences.
    
    Parameters:
    - learning_rate: [0, 1] Update rate for chosen options.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight.
    - decay: [0, 1] Decay rate for unchosen Stage 2 options (0=no decay, 1=instant forget).
    """
    learning_rate, beta, w, decay = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Policy for the first choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_hybrid = w * q_stage1_mb + (1 - w) * q_stage1_mf

        exp_q1 = np.exp(beta * q_hybrid)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # --- Policy for the second choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # --- Decay Unchosen Stage 2 Options ---
        # We iterate through all planets (s) and aliens (a)
        for s in range(2):
            for a in range(2):
                # If this specific alien was NOT the one chosen on this trial
                if not (s == state_idx and a == action_2[trial]):
                    q_stage2_mf[s, a] *= (1 - decay)
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```