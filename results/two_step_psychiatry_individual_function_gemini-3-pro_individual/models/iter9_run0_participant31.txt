Here are three new cognitive models that explore different mechanisms for solving the two-step task, specifically focusing on the balance between Model-Based (planning) and Model-Free (habitual) learning, as well as distinct learning rates for different stages.

### Model 1: Hybrid Model-Based/Model-Free Learner
This model implements the classic "hybrid" architecture often used for this task. It assumes decision-making is a mixture of a Model-Based system (which uses the transition matrix to plan) and a Model-Free system (which learns from direct experience). A weighting parameter `w` controls the trade-off.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Learner.
    
    This model computes Q-values for the first stage as a weighted average of 
    Model-Based (MB) and Model-Free (MF) values. 
    - MB values are derived by multiplying the transition matrix by the max Q-values of the second stage.
    - MF values are learned via temporal difference updates.
    
    Parameters:
    learning_rate: [0,1] Rate at which Q-values are updated (applied to both stages).
    beta: [0,10] Inverse temperature for softmax choice rule.
    w: [0,1] Weighting parameter. 0 = Pure MF, 1 = Pure MB.
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition matrix as per task description (A->X common, U->Y common)
    # Rows: Choice (A, U), Cols: State (X, Y)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)      # Model-free values for first stage
    q_stage2_mf = np.zeros((2, 2)) # Values for second stage (aliens)

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # Model-Based Calculation: V_MB(s1) = T * max(Q(s2))
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value: Q_net = w * Q_MB + (1-w) * Q_MF
        q_net_stage1 = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Decision ---
        state_idx = state[trial]
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning Updates ---
        # Stage 1 MF Update (TD(1) style - driven by second stage value)
        # Note: In simple hybrid models, stage 1 MF is often updated by the Q-value of the chosen 2nd stage action
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Stage 2 MF Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Model-Free Learner with Eligibility Traces (TD($\lambda$))
This model is purely Model-Free but incorporates an eligibility trace parameter `lambda`. This allows the reward received at the second stage to directly influence the value of the first-stage choice, bridging the gap between the action and the outcome more effectively than simple TD(0).

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Learner with Eligibility Traces (TD(lambda)).
    
    This model allows the reward at the end of the trial to 'propagate' back 
    to update the first-stage choice directly, scaled by lambda.
    
    Parameters:
    learning_rate: [0,1] Rate at which Q-values are updated.
    beta: [0,10] Inverse temperature for softmax.
    lam: [0,1] Eligibility trace parameter (lambda). 0 = TD(0), 1 = Monte Carlo.
    """
    learning_rate, beta, lam = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning Updates ---
        # 1. Prediction error at stage 2 transition
        # delta1 = V(state2) - V(state1)
        delta1 = q_stage2[state_idx, action_2[trial]] - q_stage1[action_1[trial]]
        
        # Update stage 1 immediately based on stage 2 value expectation
        q_stage1[action_1[trial]] += learning_rate * delta1
        
        # 2. Prediction error at reward
        # delta2 = Reward - V(state2)
        delta2 = reward[trial] - q_stage2[state_idx, action_2[trial]]
        
        # Update stage 2
        q_stage2[state_idx, action_2[trial]] += learning_rate * delta2
        
        # Update stage 1 again based on the reward prediction error, scaled by lambda
        # This is the eligibility trace mechanism connecting reward to the first choice
        q_stage1[action_1[trial]] += learning_rate * lam * delta2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Dual Learning Rate Model-Free Learner
This model posits that the participant might learn about the high-level choices (spaceships) at a different speed than the low-level choices (aliens). It splits the learning rate into `alpha1` (for stage 1) and `alpha2` (for stage 2), remaining purely model-free.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Dual Learning Rate Model-Free Learner.
    
    This model assumes the participant learns the value of Spaceships (Stage 1)
    and Aliens (Stage 2) with distinct learning rates. This captures potential 
    differences in volatility estimation or attention between stages.
    
    Parameters:
    alpha1: [0,1] Learning rate for Stage 1 (Spaceships).
    alpha2: [0,1] Learning rate for Stage 2 (Aliens).
    beta: [0,10] Inverse temperature for softmax.
    """
    alpha1, alpha2, beta = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning Updates ---
        # Stage 1 Update: Driven by the value of the state reached (SARSA-like logic for transition)
        delta_stage1 = q_stage2[state_idx, action_2[trial]] - q_stage1[action_1[trial]]
        q_stage1[action_1[trial]] += alpha1 * delta_stage1
        
        # Stage 2 Update: Driven by the actual reward received
        delta_stage2 = reward[trial] - q_stage2[state_idx, action_2[trial]]
        q_stage2[state_idx, action_2[trial]] += alpha2 * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```