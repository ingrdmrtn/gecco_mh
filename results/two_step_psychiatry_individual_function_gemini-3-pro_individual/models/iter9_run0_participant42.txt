Here are three new cognitive models for the two-step task. These models explore different mechanisms for valuation and learning, specifically focusing on how model-based and model-free systems might interact or how learning rates might differ based on prediction errors.

### Model 1: Hybrid Model with Independent Learning Rates for Stages
This model implements the classic Hybrid Model-Based / Model-Free architecture (Daw et al., 2011), but with a twist: it allows for different learning rates for the first-stage (abstract choice) and second-stage (concrete outcome) updates. This tests the hypothesis that learning about the value of spaceships (Stage 1) happens at a different speed than learning about the value of aliens (Stage 2).

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based/Model-Free RL with separate learning rates for each stage.
    This model assumes the brain might update high-level choices (Stage 1) 
    at a different rate than low-level concrete outcomes (Stage 2).
    
    Bounds:
    alpha1: [0, 1] - Learning rate for Stage 1 (Model-Free TD error).
    alpha2: [0, 1] - Learning rate for Stage 2 (Reward prediction error).
    beta: [0, 10] - Inverse temperature for softmax.
    w: [0, 1] - Weighting parameter (0 = Pure MF, 1 = Pure MB).
    """
    alpha1, alpha2, beta, w = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition matrix as per task description (70% common, 30% rare)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)  # Model-free values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for stage 2 (also used by MB)

    for trial in range(n_trials):

        # --- POLICY FOR THE FIRST CHOICE ---
        # Model-Based valuation: V_MB(s1) = T * max(Q(s2))
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid valuation: Q_net = w * Q_MB + (1-w) * Q_MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial] # 0 or 1 (Planet X or Y)

        # --- POLICY FOR THE SECOND CHOICE ---
        # Standard softmax on stage 2 values
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- ACTION VALUE UPDATING ---
        
        # Stage 1 Update (TD-0): Uses alpha1
        # Delta = Q_S2(chosen) - Q_S1(chosen)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += alpha1 * delta_stage1
        
        # Stage 2 Update (Reward PE): Uses alpha2
        # Delta = Reward - Q_S2(chosen)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += alpha2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Pearce-Hall Hybrid Model (Dynamic Learning Rate)
This model implements a hybrid architecture where the learning rate is not static. Instead, it follows the logic of the Pearce-Hall (1980) model, where the learning rate ("associability") increases when prediction errors are high (surprise) and decreases when the environment is predictable. This allows the agent to adapt quickly to changes in reward probabilities.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Pearce-Hall style dynamic learning rate (associability).
    The learning rate for Stage 2 updates is determined by the magnitude of 
    recent prediction errors.
    
    Bounds:
    k: [0, 1] - Scaling parameter for the dynamic learning rate.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - Weighting parameter (0 = Pure MF, 1 = Pure MB).
    decay: [0, 1] - Decay rate for the associability trace.
    """
    k, beta, w, decay = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Initialize associability (dynamic alpha)
    associability = np.ones((2, 2)) * 0.5 

    for trial in range(n_trials):

        # --- POLICY FOR THE FIRST CHOICE ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- POLICY FOR THE SECOND CHOICE ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- ACTION VALUE UPDATING ---
        
        # Stage 1 Update (Standard TD, fixed small rate for stability here implicit or we could add param)
        # To keep params <= 5, we use 'k' as a proxy for base learning rate or fix stage 1 lr.
        # Let's use 'k' as the base learning rate for stage 1.
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += k * delta_stage1
        
        # Stage 2 Update (Dynamic PH Learning Rate)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Current learning rate is the associability
        curr_alpha = associability[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += curr_alpha * delta_stage2
        
        # Update Associability: alpha_new = (1-decay)*alpha_old + decay*|delta|
        # If error is high, associability increases.
        associability[state_idx, action_2[trial]] = (1 - decay) * curr_alpha + decay * k * np.abs(delta_stage2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model with Choice Kernel (Repetition Bias)
This model adds a "Choice Kernel" to the hybrid architecture. Unlike simple stickiness which often just looks at the immediately previous trial (n-1), a choice kernel maintains a running average of past choices. This captures a longer-term tendency to repeat (or alternate) choices regardless of reward, often interpreted as motor perseveration or habit formation independent of value.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with a Choice Kernel (CK).
    The CK tracks the frequency of past choices and adds a bias to the decision 
    process, distinct from value-based learning.
    
    Bounds:
    learning_rate: [0, 1] - Learning rate for Q-values.
    beta: [0, 10] - Inverse temperature (value sensitivity).
    w: [0, 1] - Model-based weight.
    ck_lr: [0, 1] - Learning rate for the choice kernel (how fast the bias decays/updates).
    ck_weight: [0, 5] - Weight of the choice kernel in the decision (positive = perseveration).
    """
    learning_rate, beta, w, ck_lr, ck_weight = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Choice Kernel for Stage 1 actions (tracks frequency of choosing A vs U)
    choice_kernel = np.zeros(2) 

    for trial in range(n_trials):

        # --- POLICY FOR THE FIRST CHOICE ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combine MB, MF, and Choice Kernel
        # The kernel acts as an additive bias to the logits
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Logits = beta * Value + ck_weight * Kernel
        logits = beta * q_net + ck_weight * choice_kernel
        
        exp_q1 = np.exp(logits)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- POLICY FOR THE SECOND CHOICE ---
        # Standard softmax (Kernel only applied to Stage 1 here to save params/complexity)
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- ACTION VALUE UPDATING ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # --- CHOICE KERNEL UPDATING ---
        # Update trace: K(a) <- K(a) + alpha_k * (1 - K(a)) for chosen
        #               K(a) <- K(a) + alpha_k * (0 - K(a)) for unchosen
        # Simplified vector update:
        choice_vector = np.zeros(2)
        choice_vector[action_1[trial]] = 1
        choice_kernel += ck_lr * (choice_vector - choice_kernel)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```