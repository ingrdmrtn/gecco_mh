Here are three new cognitive models based on the two-step task framework, exploring different mechanisms like decay, separate learning rates for stages, and eligibility traces.

### Model 1: Decay-Based Reinforcement Learning
This model introduces a forgetting mechanism. Instead of just updating the chosen action's value, unchosen actions slowly decay back to a neutral value (0). This simulates memory decay or a "use-it-or-lose-it" heuristic, which is distinct from standard Q-learning.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Decay-Based Reinforcement Learning Model.
    
    This model assumes that action values decay over time if not selected.
    It operates purely in a Model-Free manner but adds a decay parameter
    that pulls all Q-values towards 0 on every trial.

    Parameters:
    learning_rate: [0,1] - Update rate for chosen Q-values.
    beta: [0,10] - Inverse temperature for softmax.
    decay_rate: [0,1] - Rate at which Q-values decay toward 0 each trial.
    """
    learning_rate, beta, decay_rate = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition matrix for the task structure (though not used for MB planning here)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Initialize Q-values
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Choice ---
        # Standard Model-Free choice
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        a1 = action_1[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]
  
        # --- Updates ---
        
        # 1. Decay all values before specific updates
        q_stage1_mf *= (1 - decay_rate)
        q_stage2_mf *= (1 - decay_rate)

        # 2. TD(0) Updates
        # Stage 1 update (using the value of the state we actually landed in)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] = q_stage1_mf[a1] + learning_rate * delta_stage1
        
        # Stage 2 update
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] = q_stage2_mf[state_idx, a2] + learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Separate Stage Learning Rates (Dual-Alpha)
This model posits that learning happens at different speeds for the first stage (spaceship choice) and the second stage (alien choice). The first stage is an abstract transition prediction, while the second stage is a direct reward interaction. Participants often treat these levels of hierarchy differently.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Dual-Alpha Model-Free Learning.
    
    This model separates the learning process for Stage 1 (Spaceship choice)
    and Stage 2 (Alien choice). It hypothesizes that the credit assignment 
    for the transition structure (Stage 1) occurs at a different rate than 
    the direct reward learning (Stage 2).

    Parameters:
    alpha_stage1: [0,1] - Learning rate for the first decision stage.
    alpha_stage2: [0,1] - Learning rate for the second decision stage.
    beta: [0,10] - Inverse temperature for softmax.
    """
    alpha_stage1, alpha_stage2, beta = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        a1 = action_1[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]
  
        # --- Updates ---
        
        # Update Stage 1 using alpha_stage1
        # Note: We use the value of the second stage choice as the target (SARSA-style logic for this step)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] = q_stage1_mf[a1] + alpha_stage1 * delta_stage1
        
        # Update Stage 2 using alpha_stage2
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] = q_stage2_mf[state_idx, a2] + alpha_stage2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model with Eligibility Trace (TD(1))
This model implements a hybrid Model-Based/Model-Free architecture but replaces the standard TD(0) update for the Model-Free component with an eligibility trace mechanism (TD(lambda) where lambda=1). This allows the reward at the end of the trial to directly update the Stage 1 choice, rather than waiting for the value to propagate back via the Stage 2 Q-value in subsequent trials.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Direct Reward Learning (Eligibility Trace).
    
    This model combines Model-Based planning with a Model-Free system that
    uses an eligibility trace (effectively TD(1) for the first stage). 
    This allows the Stage 1 value to be updated directly by the final reward 
    obtained in Stage 2, bypassing the need for the Stage 2 Q-value to act 
    as the proxy.

    Parameters:
    learning_rate: [0,1] - Update rate.
    beta: [0,10] - Inverse temperature.
    w: [0,1] - Weighting (0 = Pure MF, 1 = Pure MB).
    lambda_eligibility: [0,1] - Scaling factor for the direct reward update to Stage 1.
    """
    learning_rate, beta, w, lambda_eligibility = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Choice ---
        # Calculate Model-Based values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybridize
        q_hybrid = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_hybrid)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        a1 = action_1[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]
  
        # --- Updates ---
        
        # Standard TD(0) update for Stage 2
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 Update: Two components
        # 1. TD(0) part: driven by the value of the state we landed in
        delta_stage1_td0 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        
        # 2. TD(1) part / Eligibility: driven by the actual reward received
        # We model this by allowing the reward error to propagate directly to stage 1
        # scaled by lambda_eligibility.
        direct_reward_update = r - q_stage1_mf[a1]
        
        # Combined update: The lambda parameter blends how much we rely on the 
        # immediate next state value vs the final outcome.
        # If lambda_eligibility is 0, it's pure TD(0). If 1, it's closer to Monte Carlo.
        combined_update = (1 - lambda_eligibility) * delta_stage1_td0 + lambda_eligibility * direct_reward_update
        
        q_stage1_mf[a1] += learning_rate * combined_update
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```