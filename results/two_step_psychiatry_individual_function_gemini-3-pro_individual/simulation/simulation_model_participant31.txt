Here is the simulation code corresponding to the Pure Model-Free Learner (TD(0)) model provided.

```python
import numpy as np

def simulate_model(
    n_trials,
    parameters,
    drift1,
    drift2,
    drift3,
    drift4):
    """
    Simulates choices and rewards using the Pure Model-Free Learner (TD(0)).
    
    This model learns Q-values for both stages using direct prediction error updates.
    It does not use the transition matrix to plan. It updates the first stage 
    value based on the value of the state reached in the second stage.

    Parameters:
        n_trials (int): Number of trials to simulate.
        parameters (list): [learning_rate, beta]
        drift1, drift2, drift3, drift4 (np.ndarray): Trial-wise reward probabilities.

    Returns:
        stage1_choice (np.ndarray): First-stage choices (0 or 1).
        state2 (np.ndarray): Second-stage states (0 or 1).
        stage2_choice (np.ndarray): Second-stage choices (0 or 1).
        reward (np.ndarray): Rewards (0 or 1).
    """
    
    # Unpack parameters based on the input model definition
    learning_rate, beta = parameters
    
    # Initialize Q-values
    # q_stage1_mf: Values for Spaceship A (0) and U (1)
    q_stage1_mf = np.zeros(2)      
    # q_stage2_mf: Values for Planet X (aliens 0,1) and Planet Y (aliens 0,1)
    # Note: In the fitting code, state was 0 or 1. Here we map 0->Planet X, 1->Planet Y.
    q_stage2_mf = np.zeros((2, 2)) 

    # Transition matrix for the environment (standard 2-step task structure)
    # 70% common transition, 30% rare transition
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Initialize storage arrays
    stage1_choice = np.zeros(n_trials, dtype=int)
    state2 = np.zeros(n_trials, dtype=int)
    stage2_choice = np.zeros(n_trials, dtype=int)
    reward = np.zeros(n_trials, dtype=int)
    
    # Random Number Generator
    rng = np.random.default_rng()

    for t in range(n_trials):
        # Current trial reward probabilities
        # state 0 corresponds to drift1/drift2, state 1 corresponds to drift3/drift4
        current_reward_probs = [[drift1[t], drift2[t]], 
                                [drift3[t], drift4[t]]]

        # --- Stage 1 Decision ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        # Make choice 1
        a1 = rng.choice([0, 1], p=probs_1)
        
        # Determine transition to Stage 2
        # a1=0 -> high prob of state 0, a1=1 -> high prob of state 1
        s2 = rng.choice([0, 1], p=transition_matrix[a1])
        
        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        # Make choice 2
        a2 = rng.choice([0, 1], p=probs_2)
        
        # --- Reward ---
        r = int(rng.random() < current_reward_probs[s2][a2])
        
        # --- Learning Updates ---
        
        # TD(0) Update for Stage 1
        # The value of the first stage choice is updated based on the value 
        # of the *state* reached in the second stage (specifically, the Q-value of the action taken there)
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

        # TD(0) Update for Stage 2
        delta_stage2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += learning_rate * delta_stage2
        
        # Store data
        stage1_choice[t] = a1
        state2[t] = s2
        stage2_choice[t] = a2
        reward[t] = r

    return stage1_choice, state2, stage2_choice, reward
```