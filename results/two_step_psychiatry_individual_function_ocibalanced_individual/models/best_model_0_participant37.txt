def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free TD Learner.
    
    This model updates values based on Temporal Difference (TD) learning.
    It does not use a transition matrix (model-based planning).
    
    Parameters:
    - learning_rate (alpha): Rate at which Q-values are updated [0, 1].
    - beta: Inverse temperature for softmax choice [0, 10].
    - elig_trace (lambda): Eligibility trace parameter connecting stage 2 to stage 1 [0, 1].
    """
    learning_rate, beta, elig_trace = model_parameters
    n_trials = len(action_1)


    q_stage1_mf = np.zeros(2) + 0.5 

    q_stage2_mf = np.zeros((2, 2)) + 0.5 

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):

        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)

        if action_1[trial] != -1:
            p_choice_1[trial] = probs_1[int(action_1[trial])]
            a1 = int(action_1[trial])
        else:
            p_choice_1[trial] = 1.0 # Ignore missing data
            a1 = 0 # Dummy for update logic if needed, though we skip updates usually

        if state[trial] != -1:
            s_idx = int(state[trial])
            exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
            probs_2 = exp_q2 / np.sum(exp_q2)
            
            if action_2[trial] != -1:
                p_choice_2[trial] = probs_2[int(action_2[trial])]
                a2 = int(action_2[trial])
                r = reward[trial]




                pe_1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]

                pe_2 = r - q_stage2_mf[s_idx, a2]

                q_stage2_mf[s_idx, a2] += learning_rate * pe_2


                q_stage1_mf[a1] += learning_rate * (pe_1 + elig_trace * pe_2)
                
            else:
                p_choice_2[trial] = 1.0
        else:
            p_choice_2[trial] = 1.0

    eps = 1e-10

    valid_trials_mask = (action_1 != -1) & (state != -1) & (action_2 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials_mask] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_trials_mask] + eps)))
    return log_loss