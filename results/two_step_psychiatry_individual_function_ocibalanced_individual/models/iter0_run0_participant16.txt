Here are three cognitive models based on the provided template, ranging from pure model-free learning to a hybrid model-based/model-free approach.

### Model 1: Pure Model-Free TD Learning (SARSA-like)
This model assumes the participant ignores the transition structure (which spaceship goes to which planet) and learns purely from experience. It uses Temporal Difference (TD) learning to update values. It updates the first-stage value based on the second-stage value (TD(0)), and the second-stage value based on the reward.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free Learner (TD Learning).
    
    This model assumes the agent learns values purely through trial-and-error 
    using Temporal Difference updates, ignoring the transition structure of the task.
    It updates Stage 1 values based on the value of the chosen Stage 2 state-action pair.

    Parameters:
    learning_rate: [0, 1] Rate at which prediction errors update value estimates.
    beta: [0, 10] Inverse temperature for softmax choice (exploration-exploitation trade-off).
    """
    learning_rate, beta = model_parameters
    n_trials = len(action_1)
  
    # Initialize values
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)       # Values for Spaceship A vs U
    q_stage2_mf = np.zeros((2, 2))  # Values for Aliens (Planet X: W,S; Planet Y: P,H)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Softmax policy based on model-free Q-values
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Observe state (Planet)
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        # Softmax policy based on second-stage Q-values for the current planet
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning ---
        # 1. Update Stage 1 value using the value of the CHOSEN option in Stage 2 (SARSA-style TD(0))
        # Prediction Error: (Value of state 2 action) - (Value of state 1 action)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # 2. Update Stage 2 value using the reward
        # Prediction Error: Reward - (Value of state 2 action)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Pure Model-Based Learner
This model assumes the participant calculates the value of the spaceships (Stage 1) strictly by planning forward. They use the known transition probabilities and the learned values of the aliens (Stage 2) to compute the expected value of each spaceship. They do *not* maintain a separate cache of Stage 1 values (no `q_stage1_mf` used for choice).

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Based Learner.
    
    This model computes Stage 1 values by planning forward using the transition matrix 
    and the max value available at Stage 2. It does not cache Stage 1 values via TD learning.

    Parameters:
    learning_rate: [0, 1] Rate at which second-stage values are updated from rewards.
    beta: [0, 10] Inverse temperature for softmax choice.
    """
    learning_rate, beta = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition structure: A->X (0.7), U->Y (0.7)
    # Rows: Spaceships (0, 1), Cols: Planets (0, 1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage2_mf = np.zeros((2, 2)) # Learned values of aliens

    for trial in range(n_trials):

        # --- Stage 1 Choice ---
        # Calculate Model-Based values: Transition Probability * Max Value of Next State
        max_q_stage2 = np.max(q_stage2_mf, axis=1) # Best alien on Planet X and Planet Y
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        # Note: In a pure model-based agent, we only learn the values of the final states (Stage 2)
        # The Stage 1 values are re-computed every time step based on the model.
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model-Based / Model-Free Learner (Daw et al. 2011)
This is the standard "Two-Step" model. The agent maintains both a Model-Free value (learned via TD) and a Model-Based value (computed via planning) for the first stage. The final choice is a weighted combination of both systems, controlled by a mixing parameter `w`.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Learner.
    
    This model combines both learning strategies. Stage 1 choices are a weighted sum 
    of Model-Based planning and Model-Free caching.

    Parameters:
    learning_rate: [0, 1] Rate for updating values.
    beta: [0, 10] Inverse temperature for choice.
    w: [0, 1] Weighting parameter. 1 = Pure Model-Based, 0 = Pure Model-Free.
    lambda_eligibility: [0, 1] Eligibility trace decay. Controls how much the Stage 1 choice is reinforced by the final reward directly.
    """
    learning_rate, beta, w, lambda_eligibility = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Choice ---
        # 1. Calculate Model-Based Value
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Combine MB and MF values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        # 1. Update Stage 1 MF value based on Stage 2 value (TD(0))
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # 2. Update Stage 2 MF value based on Reward
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # 3. Eligibility Trace Update (TD(1) aspect)
        # This allows the reward to propagate directly back to the Stage 1 choice
        q_stage1_mf[action_1[trial]] += learning_rate * lambda_eligibility * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```