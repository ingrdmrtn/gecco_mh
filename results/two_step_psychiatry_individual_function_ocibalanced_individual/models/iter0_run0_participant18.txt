Here are three cognitive models ranging from simple model-free learning to a hybrid model-based/model-free approach, and finally a model with distinct learning rates for positive and negative outcomes.

### Model 1: Pure Model-Free (TD-Learning)
This model assumes the participant ignores the transition structure of the task (the probabilities of spaceships going to planets) and learns purely from experience using Temporal Difference (TD) learning. They update the value of the spaceship based on the value of the planet they landed on, and update the planet value based on the reward received.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free TD Learner.
    This model assumes the agent learns values purely through experience without 
    using a model of the transition probabilities. It uses TD(0) updates.
    
    Parameters:
    learning_rate: [0, 1] - Rate at which Q-values are updated (alpha).
    beta: [0, 10] - Inverse temperature for softmax choice rule.
    """
    learning_rate, beta = model_parameters
    n_trials = len(action_1)
  
    # Initialize Q-values
    # Stage 1: Values for the two spaceships (0 and 1)
    q_stage1_mf = np.zeros(2) 
    # Stage 2: Values for the aliens (2 planets x 2 aliens per planet)
    q_stage2_mf = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice (Spaceship) ---
        # Calculate softmax probabilities for stage 1
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        # Store probability of the chosen action
        # Note: Input data uses -1 for missing data, we skip or handle carefully.
        # Assuming valid indices 0 or 1 for this logic.
        if action_1[trial] != -1:
            p_choice_1[trial] = probs_1[action_1[trial]]
        else:
            p_choice_1[trial] = 1.0 # Ignore missing data trial in likelihood

        # If data is missing/invalid, skip update
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
            continue

        a1 = action_1[trial]
        s_idx = state[trial] # 0 or 1 (Planet X or Y)
        a2 = action_2[trial] # 0 or 1 (Alien)
        r = reward[trial]

        # --- Stage 2 Choice (Alien) ---
        # Calculate softmax probabilities for stage 2 given the state (planet)
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # TD Error 1: Difference between Stage 2 value and Stage 1 value
        # In pure MF (SARSA-like here), we use the value of the chosen stage 2 option
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        
        # Update Stage 1 Q-value
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # TD Error 2: Difference between Reward and Stage 2 value
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        # Update Stage 2 Q-value
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    # Sum logs, ignoring trials where prob was set to 1.0 due to missing data
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model-Based / Model-Free
This is the classic "Daw" model logic. The agent maintains model-free values (from experience) but also calculates model-based values by multiplying the best Stage 2 values by the known transition matrix (70/30). A mixing parameter `w` determines the balance between these two systems for the first-stage choice.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Learner.
    Combines a model-free TD learner with a model-based planner that uses the 
    transition matrix to compute expected values.
    
    Parameters:
    learning_rate: [0, 1] - Alpha for updating Q-values.
    beta: [0, 10] - Inverse temperature for softmax.
    w: [0, 1] - Weighting parameter (0 = Pure MF, 1 = Pure MB).
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition matrix: Row 0 -> [P(Planet0|Ship0), P(Planet1|Ship0)]
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based Value Calculation: 
        # Max value of each planet * probability of reaching that planet
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Q-value: Weighted sum of MB and MF
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        if action_1[trial] != -1:
            p_choice_1[trial] = probs_1[action_1[trial]]
        else:
            p_choice_1[trial] = 1.0

        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
            continue
            
        a1 = action_1[trial]
        s_idx = state[trial]
        a2 = action_2[trial]
        r = reward[trial]

        # --- Stage 2 Choice ---
        # Standard softmax on MF values (MB doesn't apply at the terminal step)
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Update MF values (MB values are derived, not updated directly)
        
        # Stage 1 MF Update
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Stage 2 MF Update
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Asymmetric Learning Rates (Positive/Negative)
This model is a Model-Free learner that updates its values differently depending on whether the prediction error is positive (better than expected) or negative (worse than expected). This captures potential biases towards optimism or pessimism.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Asymmetric Learning Rate Model (Model-Free).
    Uses different learning rates for positive prediction errors (alpha_pos) 
    and negative prediction errors (alpha_neg).
    
    Parameters:
    alpha_pos: [0, 1] - Learning rate for positive prediction errors.
    alpha_neg: [0, 1] - Learning rate for negative prediction errors.
    beta: [0, 10] - Inverse temperature.
    """
    alpha_pos, alpha_neg, beta = model_parameters
    n_trials = len(action_1)
  
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        if action_1[trial] != -1:
            p_choice_1[trial] = probs_1[action_1[trial]]
        else:
            p_choice_1[trial] = 1.0

        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
            continue

        a1 = action_1[trial]
        s_idx = state[trial]
        a2 = action_2[trial]
        r = reward[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # --- Updates ---
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        if delta_stage1 > 0:
            q_stage1_mf[a1] += alpha_pos * delta_stage1
        else:
            q_stage1_mf[a1] += alpha_neg * delta_stage1
            
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        if delta_stage2 > 0:
            q_stage2_mf[s_idx, a2] += alpha_pos * delta_stage2
        else:
            q_stage2_mf[s_idx, a2] += alpha_neg * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```