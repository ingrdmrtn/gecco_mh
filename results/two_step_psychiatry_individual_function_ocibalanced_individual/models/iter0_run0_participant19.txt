Here are three cognitive models based on the two-step reinforcement learning framework, ranging from a basic model-free learner to a hybrid model-based/model-free learner with complex update mechanics.

### Model 1: Pure Model-Free Learner (TD-Learning)
This model assumes the participant ignores the transition structure (which spaceship goes to which planet) and learns purely from experienced rewards using Temporal Difference (TD) learning. It treats the first stage choice as leading directly to the second stage value, without calculating expected values based on transition probabilities.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free Learner using SARSA(lambda) logic with lambda=1 (direct reinforcement).
    This model assumes the agent learns values for stage 1 and stage 2 purely from experience,
    ignoring the transition structure of the task.

    Parameters:
    - learning_rate: [0, 1] Rate at which Q-values are updated.
    - beta: [0, 10] Inverse temperature for softmax choice rule (exploration/exploitation).
    """
    learning_rate, beta = model_parameters
    n_trials = len(action_1)
  
    # Initialize Q-values
    # q_stage1_mf: Expected value of choosing spaceship 0 or 1
    q_stage1_mf = np.zeros(2) 
    # q_stage2_mf: Expected value of choosing alien 0 or 1 given planet 0 or 1 (2x2 matrix)
    q_stage2_mf = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # Calculate probabilities for choosing spaceship 0 or 1
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Observe state (planet) arrival
        state_idx = state[trial]

        # --- Stage 2 Decision ---
        # Calculate probabilities for choosing alien 0 or 1 at the current planet
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning / Updates ---
        
        # Prediction Error 2: Difference between received reward and expected value of the alien
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Prediction Error 1: Difference between value of state reached (Q2) and chosen spaceship (Q1)
        # This is standard TD-learning: the value of the second stage choice reinforces the first stage choice.
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]

        # Update Stage 1 Q-values
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Update Stage 2 Q-values
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model-Based / Model-Free Learner
This is the classic "Daw et al. (2011)" style model. It combines a Model-Based (MB) system, which plans using the transition matrix, with a Model-Free (MF) system. A weighting parameter `w` determines the balance between these two systems during the first-stage decision.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based/Model-Free Learner.
    Combines a habitual (MF) system with a planning (MB) system using a weighting parameter.
    
    Parameters:
    - learning_rate: [0, 1] Update rate for Q-values.
    - beta: [0, 10] Inverse temperature for softmax.
    - w: [0, 1] Weighting parameter (0 = Pure MF, 1 = Pure MB).
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition matrix as described in task (0.7 common, 0.3 rare)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Decision ---
        # Model-Based Value: Expected value of best action at next state, weighted by transition probability
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value: Weighted sum of MF and MB values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Decision ---
        # Standard softmax on Stage 2 Q-values
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        # MF Stage 1 Error: Value of next state - Value of current state
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        # MF Stage 2 Error: Reward - Value of current state
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Update Stage 1 MF values
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Update Stage 2 MF values (used by both MB and MF systems)
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Learner with Choice Perseverance
This model extends the hybrid learner by adding a "perseverance" parameter. This accounts for the tendency of participants to repeat their previous Stage 1 choice regardless of reward (a "sticky" choice bias), which is a common phenomenon in this specific task.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based/Model-Free Learner with Choice Perseverance (Stickiness).
    Adds a parameter to account for the tendency to repeat the previous first-stage action.
    
    Parameters:
    - learning_rate: [0, 1] Update rate for Q-values.
    - beta: [0, 10] Inverse temperature for softmax.
    - w: [0, 1] Weighting parameter (0 = Pure MF, 1 = Pure MB).
    - perseverance: [0, 5] Bonus added to the Q-value of the previously chosen action.
    """
    learning_rate, beta, w, perseverance = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track previous choice for perseverance
    last_action_1 = -1

    for trial in range(n_trials):

        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add perseverance bonus if a previous action exists
        if last_action_1 != -1:
            q_net[last_action_1] += perseverance

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Store current action for next trial's perseverance bonus
        last_action_1 = action_1[trial]
        state_idx = state[trial]

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```