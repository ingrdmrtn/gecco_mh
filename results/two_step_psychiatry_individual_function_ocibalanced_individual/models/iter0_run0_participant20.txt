Here are three cognitive models representing different hypotheses about how the participant learns in this two-step task.

### Model 1: Pure Model-Free Learner (TD-Learning)
This model assumes the participant does not use a map of the task structure (transitions between spaceships and planets). Instead, they learn purely through trial and error using Temporal Difference (TD) learning. They update the value of the spaceship based on the value of the planet they land on, and update the planet choice based on the reward received.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free Learner (SARSA-like TD learning).
    
    This model assumes the participant learns values for both stages purely from 
    experienced reward prediction errors, without using the transition matrix 
    to calculate model-based values.
    
    Parameters:
    - learning_rate: [0,1] Rate at which Q-values are updated.
    - beta: [0,10] Inverse temperature for softmax choice rule.
    """
    learning_rate, beta = model_parameters
    n_trials = len(action_1)
  
    # Q-values for Stage 1 (Spaceships: 0 or 1)
    q_stage1_mf = np.zeros(2) # Initial values 0.5 for neutrality
    
    # Q-values for Stage 2 (Planets: 0 or 1, Aliens: 0 or 1)
    # Shape: (2 planets, 2 aliens)
    q_stage2_mf = np.zeros((2, 2)) 

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):

        # --- Stage 1 Choice (Spaceship) ---
        # Calculate probabilities using Softmax on Model-Free Q-values
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Observe state (Planet)
        state_idx = state[trial] # 0 or 1

        # --- Stage 2 Choice (Alien) ---
        # Calculate probabilities for the specific planet we are on
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Value Updating ---
        
        # Prediction Error 1: Difference between value of chosen spaceship and value of chosen alien
        # Note: This is a SARSA-style update where we use the Q-value of the action actually taken in stage 2
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Prediction Error 2: Difference between reward received and value of chosen alien
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model-Based/Model-Free Learner
This is the classic "Two-Step" model. It assumes the participant uses a mixture of two strategies: a Model-Based strategy (planning using the transition matrix) and a Model-Free strategy (habitual learning). A weighting parameter `w` determines the balance between these two systems during the first-stage choice.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Learner.
    
    This model combines model-free TD learning with model-based planning.
    Stage 1 values are a weighted sum of MF values (experienced) and MB values 
    (calculated via transition matrix).
    
    Parameters:
    - learning_rate: [0,1] Update rate for Q-values.
    - beta: [0,10] Inverse temperature for softmax.
    - w: [0,1] Weighting parameter (0 = Pure MF, 1 = Pure MB).
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition matrix as described in task (70% common, 30% rare)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Choice ---
        # Calculate Model-Based values: V(planet) * P(planet|spaceship)
        # We assume the value of a planet is the max Q-value available there
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Q-value is weighted sum of MB and MF
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        # Stage 2 is purely model-free (choosing the best alien on the current planet)
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Value Updating ---
        # Update Stage 1 MF value using TD(1) / direct reinforcement from the second stage value
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Update Stage 2 MF value based on reward
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Learner with Choice Perseveration
This model extends the Hybrid learner by adding a "stickiness" or perseveration parameter. This captures the tendency of participants to simply repeat their previous motor action (spaceship choice) regardless of the value or reward, a common phenomenon in this task.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner with Choice Perseveration (Stickiness).
    
    Adds a 'stickiness' parameter to the hybrid model. This parameter captures 
    the tendency to repeat the previous Stage 1 choice regardless of reward history.
    
    Parameters:
    - learning_rate: [0,1] Update rate.
    - beta: [0,10] Inverse temperature.
    - w: [0,1] MB/MF weighting.
    - stickiness: [0,5] Bonus added to the Q-value of the previously chosen action.
    """
    learning_rate, beta, w, stickiness = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track previous action for stickiness
    last_action = -1 

    for trial in range(n_trials):

        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combine MB and MF
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Add Stickiness Bonus
        # If this isn't the first trial, add bonus to the previously chosen index
        if last_action != -1:
            q_net[last_action] += stickiness

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Update last action
        last_action = action_1[trial]
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Value Updating ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```