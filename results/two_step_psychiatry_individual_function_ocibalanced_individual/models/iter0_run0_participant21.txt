Here are three cognitive models based on the two-step task framework. Each model explores a different hypothesis about how participants learn and make decisions, ranging from pure model-free learning to a hybrid model-based/model-free approach, and finally adding a perseverance mechanism.

### Model 1: Pure Model-Free TD Learning (SARSA)
This model assumes the participant does not use knowledge of the transition structure (the map of spaceships to planets). Instead, they learn the value of the first-stage actions solely through temporal difference (TD) errors, chaining the value of the second stage back to the first. This is a classic "habitual" learner.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free Learner (SARSA-style updates).
    
    Hypothesis: The participant learns values purely through experience without 
    using a mental map of the transition probabilities. Stage 1 values are updated 
    based on the value of the state reached in Stage 2.

    Parameters:
    learning_rate: [0,1] Rate at which Q-values are updated.
    beta: [0,10] Inverse temperature for softmax choice (exploration/exploitation).
    """
    learning_rate, beta = model_parameters
    n_trials = len(action_1)
  
    # Q-values initialization
    # Stage 1: 2 actions (Spaceships)
    q_stage1_mf = np.zeros(2) 
    # Stage 2: 2 states (Planets) x 2 actions (Aliens)
    q_stage2_mf = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # Softmax policy for spaceship choice
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Determine which planet (state) was reached
        state_idx = state[trial]
        a1 = action_1[trial]

        # --- Stage 2 Decision ---
        # Softmax policy for alien choice given the planet
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[trial]
        p_choice_2[trial] = probs_2[a2]
        
        r = reward[trial]

        # --- Learning Updates ---
        
        # Prediction Error 1: Difference between value of chosen stage 2 option and chosen stage 1 option
        # Note: This uses SARSA logic (using Q(s', a') instead of max Q(s', a'))
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] = q_stage1_mf[a1] + learning_rate * delta_stage1
        
        # Prediction Error 2: Difference between reward received and chosen stage 2 option
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] = q_stage2_mf[state_idx, a2] + learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model-Based / Model-Free Learner
This is the standard "Daw et al. (2011)" style model. It assumes the participant calculates Stage 1 values in two ways: 
1.  **Model-Based (MB):** Using the known transition matrix (70/30) and the max values of the planets to plan ahead.
2.  **Model-Free (MF):** Learning from direct experience (TD learning).
A mixing parameter (`w`) determines the balance between these two systems.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Learner.
    
    Hypothesis: Decision making is a weighted combination of a planning system 
    (Model-Based) which uses the transition matrix, and a habitual system 
    (Model-Free) which relies on TD errors.

    Parameters:
    learning_rate: [0,1] Update rate for Q-values.
    beta: [0,10] Inverse temperature for softmax.
    w: [0,1] Weighting parameter (0 = Pure MF, 1 = Pure MB).
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition matrix as per task description
    # Row 0: Spaceship A -> [70% Planet X, 30% Planet Y]
    # Row 1: Spaceship U -> [30% Planet X, 70% Planet Y]
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # Used by both MF and MB for terminal values

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        
        # 1. Calculate Model-Based values
        # Max value achievable at each planet
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        # Expected value of spaceship based on transitions
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Combine with Model-Free values
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Policy
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        a1 = action_1[trial]

        # --- Stage 2 Decision ---
        # Standard softmax on the second stage values
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[trial]
        p_choice_2[trial] = probs_2[a2]
        
        r = reward[trial]
  
        # --- Learning Updates ---
        
        # TD(1) / SARSA style update for stage 1 MF
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] = q_stage1_mf[a1] + learning_rate * delta_stage1
        
        # Standard Q-learning update for stage 2
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] = q_stage2_mf[state_idx, a2] + learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Learner with Choice Stickiness (Perseverance)
This model extends the hybrid approach by adding a "stickiness" or "perseverance" parameter. Participants often have a tendency to repeat their previous Stage 1 choice regardless of reward or model-based predictions. This model captures that motor or cognitive inertia.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner with Choice Perseverance (Stickiness).
    
    Hypothesis: In addition to MB/MF learning, the participant has a tendency 
    to repeat the previously chosen spaceship (Stage 1 action), captured by 
    a perseverance bonus.

    Parameters:
    learning_rate: [0,1] Update rate for Q-values.
    beta: [0,10] Inverse temperature.
    w: [0,1] Weighting between MB and MF.
    pers: [0,5] Perseverance bonus added to the previously chosen action's value.
    """
    learning_rate, beta, w, pers = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track previous choice (initialize as -1 or None, handle in loop)
    last_action_1 = -1

    for trial in range(n_trials):

        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Calculate net Q-value
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Add perseverance bonus to the previously chosen action
        # We create a temporary Q vector for decision making so we don't corrupt the learned Qs
        q_decision = q_net.copy()
        if last_action_1 != -1:
            q_decision[last_action_1] += pers
            
        exp_q1 = np.exp(beta * q_decision)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Record this action for the next trial's perseverance bonus
        last_action_1 = action_1[trial]
        
        state_idx = state[trial]
        a1 = action_1[trial]

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[trial]
        p_choice_2[trial] = probs_2[a2]
  
        r = reward[trial]
        
        # --- Learning Updates ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] = q_stage1_mf[a1] + learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] = q_stage2_mf[state_idx, a2] + learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```