Here are three cognitive models based on the two-step reinforcement learning framework. They progress from a simple Model-Free learner to a pure Model-Based learner, and finally a Hybrid model that combines both strategies.

### Model 1: Pure Model-Free Learner (TD-Learning)
This model assumes the participant learns purely through trial and error, ignoring the transition structure of the task (i.e., they don't use the knowledge that Spaceship A usually goes to Planet X). It uses Temporal Difference (TD) learning to update values.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free Learner using SARSA/TD(0) logic.
    This model updates stage 1 values based on the value of the state reached in stage 2,
    without considering transition probabilities.

    Parameters:
    learning_rate: [0,1] - Rate at which Q-values are updated (alpha).
    beta: [0,10] - Inverse temperature for softmax choice (exploration/exploitation).
    """
    learning_rate, beta = model_parameters
    n_trials = len(action_1)
  
    # Initialize probabilities and Q-values
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values for Stage 1: 2 spaceships
    q_stage1_mf = np.zeros(2) 
    # Q-values for Stage 2: 2 planets x 2 aliens
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # Calculate probabilities for stage 1 using Softmax on Model-Free values
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        # Store probability of the chosen action
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Determine which planet (state) was reached
        state_idx = state[trial]

        # --- Stage 2 Decision ---
        # Calculate probabilities for stage 2 using Softmax
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        # Store probability of the chosen alien
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning / Updates ---
        
        # Prediction error for Stage 1: Driven by the value of the *chosen* option in Stage 2
        # (Standard TD-learning: V(s1) moves toward V(s2))
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Prediction error for Stage 2: Driven by the reward received
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Pure Model-Based Learner
This model assumes the participant plans their first action by calculating the expected value of the second stage and multiplying it by the known transition matrix (70/30). It does not maintain a separate cache of model-free values for the first stage.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Based Learner.
    This model calculates Stage 1 values strictly by projecting Stage 2 values 
    through the transition matrix.

    Parameters:
    learning_rate: [0,1] - Rate at which Stage 2 Q-values are updated.
    beta: [0,10] - Inverse temperature for softmax choice.
    """
    learning_rate, beta = model_parameters
    n_trials = len(action_1)
  
    # Known transition structure: Row 0 is Spaceship A, Row 1 is Spaceship U
    # Col 0 is Planet X, Col 1 is Planet Y
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Only need Q-values for Stage 2 (aliens), Stage 1 is derived
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Decision ---
        # Calculate Model-Based values: Max value of each planet weighted by transition prob
        max_q_stage2 = np.max(q_stage2_mf, axis=1) # Max value available on Planet X and Planet Y
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning / Updates ---
        
        # No Stage 1 update needed, as q_stage1_mb is re-calculated every trial based on q_stage2
        
        # Update Stage 2 values based on reward
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Learner (Model-Based + Model-Free)
This model represents the classic "Two-Step" analysis. It assumes the participant uses a weighted combination of Model-Free (habitual) and Model-Based (goal-directed) values to make the first-stage decision. It includes a weighting parameter `w`.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Learner.
    Combines habitual (MF) and planning (MB) values for the first stage choice.
    
    Parameters:
    learning_rate: [0,1] - Update rate for Q-values.
    beta: [0,10] - Inverse temperature.
    w: [0,1] - Mixing weight. 0 = Pure MF, 1 = Pure MB.
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Decision ---
        # 1. Calculate Model-Based values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Combine MF and MB values using weight w
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning / Updates ---
        
        # Update Stage 1 MF value (TD-Learning)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Update Stage 2 value (Reward Learning)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```