Here are three cognitive models representing different hypotheses about how the participant learns and makes decisions.

### Model 1: Pure Model-Based Reinforcement Learning
This model assumes the participant builds a mental map of the task structure (Model-Based). They calculate the value of the first-stage spaceships based on the known transition probabilities to the planets and the learned values of the aliens on those planets. They do not use a "habitual" (Model-Free) cache for the first stage.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Based Reinforcement Learning.
    
    This model assumes the agent calculates the value of stage 1 actions solely 
    by planning forward using the transition matrix and stage 2 values.
    
    Parameters:
    learning_rate: [0, 1] Rate at which stage 2 values are updated.
    beta: [0, 10] Inverse temperature for softmax choice (exploration/exploitation).
    """
    learning_rate, beta = model_parameters
    n_trials = len(action_1)
  
    # Transition matrix: A -> X (0.7), U -> Y (0.7)
    # Rows: Action 1 (Spaceship 0/1), Cols: State (Planet 0/1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values for Stage 2 (aliens): 2 states (planets) x 2 actions (aliens)
    q_stage2_mf = np.zeros((2, 2)) # Initialized to 0.5 (neutral) usually, but 0 is fine given data range

    for trial in range(n_trials):

        # --- STAGE 1 CHOICE (Model-Based) ---
        # Calculate max value available at each planet
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        
        # Bellman equation: Q_MB(s1, a1) = sum(P(s2|s1,a1) * max(Q(s2, a2)))
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial] # Actual planet arrived at

        # --- STAGE 2 CHOICE ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- LEARNING ---
        # No Stage 1 update needed for Pure MB, as it is derived dynamically.
        
        # Update Stage 2 values based on reward (Model-Free update at second stage)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Pure Model-Free Reinforcement Learning (TD-Learning)
This model assumes the participant ignores the transition structure (the probabilities of spaceships going to planets). Instead, they learn the value of the spaceships directly from experience using Temporal Difference (TD) learning. If they get a reward, they reinforce the spaceship they chose, regardless of whether a "rare" or "common" transition occurred.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free Reinforcement Learning (TD(0)).
    
    This model relies on caching values for actions at both stages. Stage 1 values
    are updated based on the value of the state reached in Stage 2 (TD error).
    
    Parameters:
    learning_rate: [0, 1] Rate at which Q-values are updated.
    beta: [0, 10] Inverse temperature for softmax choice.
    eligibility: [0, 1] Eligibility trace (lambda) - how much the stage 2 reward directly affects stage 1.
    """
    learning_rate, beta, eligibility = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)      # Value of spaceships
    q_stage2_mf = np.zeros((2, 2)) # Value of aliens

    for trial in range(n_trials):

        # --- STAGE 1 CHOICE (Model-Free) ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- STAGE 2 CHOICE ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- LEARNING ---
        
        # Prediction Error 1: Difference between value of Stage 2 state and Stage 1 action
        # Note: Standard SARSA/Q-learning uses Q(s', a'), here we use Q(s', a_chosen)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Prediction Error 2: Difference between Reward and Stage 2 action value
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Eligibility Trace: Allow the Stage 2 RPE to also update Stage 1 choice
        q_stage1_mf[action_1[trial]] += learning_rate * eligibility * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model-Based / Model-Free (Daw et al. 2011)
This is the classic "Two-Step" model. It posits that the brain computes *both* a Model-Based value (using the transition matrix) and a Model-Free value (from direct experience) for the first stage. The final decision is a weighted combination of these two systems, controlled by a weighting parameter `w`.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Model.
    
    Combines MB and MF values for the first stage decision using a weighting parameter w.
    
    Parameters:
    learning_rate: [0, 1] Update rate for Q-values.
    beta: [0, 10] Softmax inverse temperature.
    w: [0, 1] Weighting parameter. 1 = Pure MB, 0 = Pure MF.
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- STAGE 1 CHOICE (Hybrid) ---
        # 1. Calculate Model-Based Value
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Integrate MB and MF values
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- STAGE 2 CHOICE ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- LEARNING ---
        
        # Update MF Stage 1 value (TD learning)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Update MF Stage 2 value (Reward learning)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Direct reinforcement of Stage 1 from Stage 2 reward (TD(1) logic often included in hybrid models)
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```