Here are three cognitive models representing different strategies participants often use in the two-step task. They range from pure model-free learning to a hybrid model-based/model-free approach, and finally a model that considers perseveration.

### Model 1: Pure Model-Free TD Learning (SARSA)
This model assumes the participant does not understand the transition structure (common vs. rare transitions). They simply learn the value of the first-stage spaceships based on the eventual reward, chaining values backward using Temporal Difference (TD) learning. They update values based on the specific path taken.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free TD Learning (SARSA-like).
    
    This model assumes the agent learns values for Stage 1 and Stage 2 purely through 
    reward prediction errors, without using a model of the transition probabilities.
    It updates Stage 1 values based on the value of the chosen Stage 2 state-action pair.

    Parameters:
    - learning_rate: Rate at which Q-values are updated [0, 1].
    - beta: Inverse temperature for softmax choice rule [0, 10].
    """
    learning_rate, beta = model_parameters
    n_trials = len(action_1)
    
    # Q-values initialization
    # Stage 1: 2 actions (Spaceship 0, Spaceship 1)
    q_stage1 = np.zeros(2) 
    # Stage 2: 2 states (Planet 0, Planet 1) x 2 actions (Alien 0, Alien 1)
    q_stage2 = np.zeros((2, 2)) 
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Determine which planet (state) was reached
        state_idx = state[trial]
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Value Updating (TD Learning) ---
        
        # Prediction error for Stage 1: driven by the value of the chosen Stage 2 option
        # This is the "SARSA" style update: Q(s1, a1) moves toward Q(s2, a2)
        delta_stage1 = q_stage2[state_idx, action_2[trial]] - q_stage1[action_1[trial]]
        q_stage1[action_1[trial]] += learning_rate * delta_stage1
        
        # Prediction error for Stage 2: driven by the actual reward received
        delta_stage2 = reward[trial] - q_stage2[state_idx, action_2[trial]]
        q_stage2[state_idx, action_2[trial]] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model-Based / Model-Free Learning
This is the classic "Daw et al. (2011)" style model. It posits that the brain computes two values for the first stage: a Model-Free (MF) value based on direct experience, and a Model-Based (MB) value calculated by planning through the transition matrix. The final choice is a weighted mixture of both strategies governed by a weighting parameter `w`.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Reinforcement Learning.
    
    The agent combines Model-Free (MF) learning (TD) with Model-Based (MB) planning.
    MB values are computed using the fixed transition matrix and the max Q-values 
    of the second stage.

    Parameters:
    - learning_rate: Update rate for Q-values [0, 1].
    - beta: Inverse temperature for softmax [0, 10].
    - w: Weighting parameter balancing MB (1) and MF (0) control [0, 1].
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition matrix: Row 0 -> [Common to 0, Rare to 1], Row 1 -> [Rare to 0, Common to 1]
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # MF values for stage 2 are used for both MF and MB calculations

    for trial in range(n_trials):

        # --- Stage 1 Choice ---
        # 1. Calculate Model-Based values
        # Max value of each state in stage 2
        max_q_stage2 = np.max(q_stage2_mf, axis=1) 
        # Bellman equation: Transition Probability * Max Future Value
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Combine MB and MF values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        # Standard softmax on Stage 2 Q-values
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Value Updating ---
        # TD(1) / SARSA-like update for Stage 1 MF values
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Standard Rescorla-Wagner update for Stage 2 values
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model with Choice Perseveration
This model extends the hybrid approach by adding a "perseveration" parameter. Humans often have a "stickiness" bias where they tend to repeat their previous Stage 1 choice regardless of reward (or switch away from it). This model captures that tendency, which often improves model fit significantly on this task.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free RL with Choice Perseveration.
    
    Adds a 'stickiness' parameter to the hybrid model. This accounts for the 
    tendency to repeat the previous Stage 1 action regardless of the outcome.

    Parameters:
    - learning_rate: Update rate for Q-values [0, 1].
    - beta: Inverse temperature for softmax [0, 10].
    - w: Weighting parameter balancing MB (1) and MF (0) control [0, 1].
    - pers: Perseveration bonus added to the previously chosen action [0, 1].
    """
    learning_rate, beta, w, pers = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track previous choice for perseveration (initialize as -1 or None)
    last_choice = -1

    for trial in range(n_trials):

        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combine MB and MF values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add perseveration bonus to the net Q-value of the previous action
        if last_choice != -1:
            q_net[last_choice] += pers
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Update last_choice for next trial
        last_choice = action_1[trial]
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Value Updating ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```