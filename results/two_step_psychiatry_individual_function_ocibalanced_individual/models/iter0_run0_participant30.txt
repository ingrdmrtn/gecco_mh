Here are three cognitive models based on the two-step task literature, ranging from a pure Model-Free learner to a hybrid Model-Based/Model-Free learner, and finally a model with a "perseveration" bias.

### Model 1: Pure Model-Free Learner (TD-Learning)
This model assumes the participant does not use knowledge of the transition structure (the spaceship-planet probabilities). Instead, they learn the value of the first-stage spaceships solely based on the eventual reward, chaining values backward using Temporal Difference (TD) learning. They update the first-stage value based on the second-stage value (TD(0)).

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free Learner (TD-Learning).
    
    This model learns action values solely through reward prediction errors.
    Stage 1 values are updated towards Stage 2 values (TD-learning), and Stage 2 values
    are updated towards the reward. It ignores the transition matrix structure.

    Parameters:
    - learning_rate: Rate at which Q-values are updated [0, 1].
    - beta: Inverse temperature for softmax choice [0, 10].
    """
    learning_rate, beta = model_parameters
    n_trials = len(action_1)
    
    # Initialize Q-values
    # q_stage1_mf: Values for the two spaceships (0 or 1)
    q_stage1_mf = np.zeros(2)  # Initialize at 0 or 0.5 (neutral)
    # q_stage2_mf: Values for the two aliens (0 or 1) at each of the two planets (0 or 1)
    q_stage2_mf = np.zeros((2, 2)) 

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice (Spaceship) ---
        # Softmax policy based on Model-Free Q-values only
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Observe state (planet)
        state_idx = state[trial] # 0 or 1

        # --- Stage 2 Choice (Alien) ---
        # Softmax policy based on Q-values for the current planet
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning ---
        
        # Prediction Error 1: Difference between Stage 2 value and Stage 1 value
        # The agent updates the value of the chosen spaceship based on the value of the chosen alien
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Prediction Error 2: Difference between Reward and Stage 2 value
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model-Based / Model-Free Learner
This is the classic "Two-Step" model. It assumes the participant uses a weighted combination of Model-Free (habitual) and Model-Based (goal-directed) planning. The Model-Based system calculates the value of a spaceship by multiplying the known transition probabilities by the best values available on the resulting planets. The parameter `w` controls the balance between these two systems.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Learner.
    
    Combines a Model-Free system (TD learning) with a Model-Based system (using the
    transition matrix to plan). A weighting parameter `w` determines the mix.

    Parameters:
    - learning_rate: Rate at which Q-values are updated [0, 1].
    - beta: Inverse temperature for softmax choice [0, 10].
    - w: Weighting parameter [0, 1]. 0 = Pure MF, 1 = Pure MB.
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition matrix as described in task (Spaceship 0 -> Planet 0 commonly, etc.)
    # We assume the agent knows this structure perfectly.
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        
        # Model-Based Value Calculation:
        # Max value available at each planet (state)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        # Expected value of spaceship based on transitions * max_planet_value
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Value: Weighted sum of MB and MF values
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        # Standard Q-learning choice at the second stage
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        # Update MF values (MB values are derived dynamically, not stored/updated directly here)
        
        # TD(0) update for Stage 1
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Reward update for Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model with Choice Perseveration
This extends the Hybrid model by adding a "perseveration" parameter. Participants often have a tendency to repeat their last choice regardless of reward (motor perserveration) or switch. This model adds a "stickiness" bonus to the previously chosen spaceship in the first stage.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Learner with Choice Perseveration.
    
    Adds a 'stickiness' or perseveration parameter to the hybrid model.
    This accounts for the tendency to repeat the previous Stage 1 action 
    regardless of the outcome.

    Parameters:
    - learning_rate: Rate at which Q-values are updated [0, 1].
    - beta: Inverse temperature for softmax choice [0, 10].
    - w: Weighting parameter [0, 1]. 0 = Pure MF, 1 = Pure MB.
    - perseveration: Bonus added to the logits of the previously chosen action [-2, 2] (approx range).
                     (Here bounds should be handled by optimizer, usually small magnitude).
    """
    learning_rate, beta, w, perseveration = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track previous action (initialize to -1 or handle first trial separately)
    last_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Calculate logits (values before exponentiation)
        logits = beta * q_net
        
        # Add perseveration bonus if not the first trial
        if last_action_1 != -1:
            logits[last_action_1] += perseveration
            
        exp_q1 = np.exp(logits)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Update last action
        last_action_1 = action_1[trial]
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```