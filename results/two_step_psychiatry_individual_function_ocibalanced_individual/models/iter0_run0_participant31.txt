Here are three cognitive models for the two-step decision task. They progress from a purely Model-Free learner to a Hybrid learner (the standard for this task), and finally a Hybrid learner with separate learning rates for the two stages.

### Model 1: Pure Model-Free Learner (TD-0)
This model assumes the participant does not use knowledge of the transition structure (probabilities of spaceships going to planets). It learns strictly from reward prediction errors at each stage. It treats the first-stage values as predictions of the second-stage values (TD-learning).

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free (TD-0) learner.
    
    This model ignores the transition structure (70/30) and learns purely 
    through temporal difference errors. It updates stage 1 values based on 
    the value of the stage 2 state reached, and stage 2 values based on rewards.

    Parameters:
    - learning_rate: [0, 1] Rate at which Q-values are updated.
    - beta: [0, 10] Inverse temperature for softmax choice (exploration/exploitation).
    """
    learning_rate, beta = model_parameters
    n_trials = len(action_1)
  
    # Q-values initialization
    # Stage 1: 2 choices (Spaceships 0 or 1)
    q_stage1_mf = np.zeros(2) 
    # Stage 2: 2 states (Planets 0 or 1) x 2 choices (Aliens 0 or 1)
    q_stage2_mf = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Softmax policy based on Model-Free values only
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Observe state transition
        state_idx = state[trial] # 0 or 1

        # --- Stage 2 Choice ---
        # Softmax policy based on Stage 2 Q-values for the current planet
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning ---
        # 1. Prediction error for Stage 1: Difference between value of state reached and value of chosen spaceship
        # Note: We use the value of the chosen action in stage 2 (SARSA-like) or max (Q-learning). 
        # Here we use the value of the chosen action in stage 2 as the proxy for state value.
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1

        # 2. Prediction error for Stage 2: Difference between reward and value of chosen alien
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model-Based / Model-Free Learner
This is the canonical model for this task. It assumes the participant mixes two strategies: a Model-Based strategy (calculating expected value using the transition matrix) and a Model-Free strategy (learning from direct experience). A mixing parameter `w` determines the balance.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free learner.
    
    The agent calculates stage 1 values as a weighted sum of Model-Based (MB) 
    and Model-Free (MF) values. MB values are derived by multiplying the 
    known transition matrix by the max stage 2 values.

    Parameters:
    - learning_rate: [0, 1] Rate at which MF Q-values are updated.
    - beta: [0, 10] Inverse temperature for softmax choice.
    - w: [0, 1] Weighting parameter. 1 = Pure Model-Based, 0 = Pure Model-Free.
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition matrix: Row 0 -> Planet 0 (70%), Planet 1 (30%)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Choice ---
        # Calculate Model-Based values: Transition Prob * Max Value of Next State
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combine MF and MB values using weight w
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        # Standard softmax on stage 2 values
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        # Update Stage 1 MF values (TD-learning)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Update Stage 2 MF values (Reward prediction error)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Learner with Separate Choice Perseverance
This model extends the Hybrid learner by adding a "perseverance" parameter. Participants often have a tendency to repeat their previous choice regardless of reward (stickiness). This model captures that behavioral nuisance parameter in the first stage.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF learner with Choice Perseverance (Stickiness).
    
    Adds a 'perseverance' bonus to the action taken on the immediately 
    preceding trial, capturing the tendency to repeat keys/choices.

    Parameters:
    - learning_rate: [0, 1] Rate at which Q-values are updated.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weighting parameter (MB vs MF).
    - p_stick: [0, 5] Perseverance bonus added to the previous choice's value.
    """
    learning_rate, beta, w, p_stick = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track the previous action (initialize with -1 or handle first trial separately)
    last_action = -1

    for trial in range(n_trials):

        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Add perseverance bonus if it's not the first trial
        if last_action != -1:
            q_net[last_action] += p_stick
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Store current action for next trial's perseverance
        last_action = action_1[trial]
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```