Here are three cognitive models based on the two-step task paradigm. They progress from a basic Model-Free/Model-Based hybrid to models incorporating perseveration and separate learning rates for positive/negative outcomes.

### Model 1: Classic Hybrid Model-Based / Model-Free Reinforcement Learning
This model implements the canonical Daw et al. (2011) hybrid architecture. It assumes the participant's first-stage choice is a weighted combination of a Model-Based (planning) system and a Model-Free (habitual) system.

*   **Model-Based (MB):** Calculates values based on the known transition structure (70/30) and the maximum value available at the second stage.
*   **Model-Free (MF):** Updates values based purely on prediction errors (TD learning).
*   **Mixing:** The parameter `w` controls the balance between these two systems.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Reinforcement Learning Model.
    
    This model assumes the agent combines model-based planning (using the known 
    transition matrix) and model-free learning (using temporal difference errors) 
    to make decisions.
    
    Parameters:
    learning_rate: [0,1] - Update rate for Q-values (alpha).
    beta_1: [0,10] - Inverse temperature for stage 1 choice softmax.
    beta_2: [0,10] - Inverse temperature for stage 2 choice softmax.
    w: [0,1] - Weighting parameter (0 = Pure Model-Free, 1 = Pure Model-Based).
    """
    learning_rate, beta_1, beta_2, w = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition matrix as per task description: 
    # Spaceship 0 -> Planet 0 (0.7), Spaceship 1 -> Planet 1 (0.7)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values initialization
    q_stage1_mf = np.zeros(2)      # Model-free values for stage 1 (2 spaceships)
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for stage 2 (2 planets x 2 aliens)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based Value Calculation: 
        # Value of spaceship = P(planet|spaceship) * max(Value of aliens on planet)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value: Weighted sum of MB and MF values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax choice for Stage 1
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Policy ---
        # Standard Model-Free choice based on the state (planet) arrived at
        curr_state = state[trial]
        exp_q2 = np.exp(beta_2 * q_stage2_mf[curr_state, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning / Updating ---
        
        # Stage 1 MF Update (SARSA-style TD(0) for first step)
        # Note: In pure MF, the value of stage 1 is updated by the value of the *chosen* option in stage 2
        # However, standard implementations often use the max or the specific choice. 
        # Here we use the value of the chosen second-stage action as the target.
        chosen_q2_val = q_stage2_mf[curr_state, action_2[trial]]
        delta_stage1 = chosen_q2_val - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Stage 2 MF Update (Reward prediction error)
        delta_stage2 = reward[trial] - chosen_q2_val
        q_stage2_mf[curr_state, action_2[trial]] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Model-Free with Choice Perseveration
This model drops the Model-Based component to focus on a "habitual" learner but adds a **perseveration** parameter. In many reinforcement learning tasks, participants exhibit a "stickiness" or tendency to repeat their previous choice regardless of reward.

*   **Perseveration (`p`):** A bonus added to the Q-value of the spaceship chosen on the *previous* trial.
*   **Lambda (`lam`):** An eligibility trace parameter that allows the stage 2 reward to update the stage 1 choice directly (TD(lambda)).

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free RL with Eligibility Traces and Choice Perseveration.
    
    This model relies purely on model-free learning but includes:
    1. An eligibility trace (lambda) allowing the final reward to influence the first-stage choice.
    2. A perseveration bonus (p) capturing the tendency to repeat the previous first-stage choice.
    
    Parameters:
    learning_rate: [0,1] - Update rate.
    beta: [0,10] - Inverse temperature (shared for both stages).
    lam: [0,1] - Eligibility trace decay (0 = TD(0), 1 = Monte Carlo).
    persev: [0,5] - Perseveration bonus added to the previously chosen stage 1 action.
    """
    learning_rate, beta, lam, persev = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    last_action_1 = -1 # Initialize with an impossible action

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        # Calculate net values including perseveration bonus
        q_stage1_net = q_stage1.copy()
        if last_action_1 != -1:
            q_stage1_net[last_action_1] += persev
            
        exp_q1 = np.exp(beta * q_stage1_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Store action for next trial's perseveration
        last_action_1 = action_1[trial]
        curr_state = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2[curr_state, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        # Prediction error at stage 1 (driven by value of state arrived at)
        # We use the value of the chosen action in stage 2 as the proxy for state value
        val_stage2 = q_stage2[curr_state, action_2[trial]]
        delta_stage1 = val_stage2 - q_stage1[action_1[trial]]
        
        # Update Stage 1 (TD(0) part)
        q_stage1[action_1[trial]] += learning_rate * delta_stage1
        
        # Prediction error at stage 2 (driven by reward)
        delta_stage2 = reward[trial] - val_stage2
        
        # Update Stage 2
        q_stage2[curr_state, action_2[trial]] += learning_rate * delta_stage2
        
        # Eligibility Trace Update:
        # The Stage 1 choice also learns from the Stage 2 prediction error, scaled by lambda
        q_stage1[action_1[trial]] += learning_rate * lam * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Asymmetric Learning Rates (Positive/Negative)
This model investigates if the participant learns differently from gains (finding gold) versus losses (no gold). It splits the learning rate into `alpha_pos` and `alpha_neg`. This is often useful in clinical populations or reward-processing tasks where sensitivity to reward and punishment differs. It uses a simplified hybrid structure (fixed weight `w` implied or simplified to pure MB/MF depending on fit, here we stick to a pure Model-Free structure to isolate the learning rate effect within the parameter limit).

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Asymmetric Learning Rate Model (Dual Alpha).
    
    This model posits that the participant updates their value expectations differently 
    depending on whether the outcome was better than expected (positive prediction error) 
    or worse than expected (negative prediction error).
    
    Parameters:
    alpha_pos: [0,1] - Learning rate for positive prediction errors.
    alpha_neg: [0,1] - Learning rate for negative prediction errors.
    beta: [0,10] - Inverse temperature.
    """
    alpha_pos, alpha_neg, beta = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        curr_state = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2[curr_state, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        
        # 1. Stage 1 Update
        # Using the value of the chosen stage 2 option as the target
        target_val_s2 = q_stage2[curr_state, action_2[trial]]
        delta_1 = target_val_s2 - q_stage1[action_1[trial]]
        
        # Select learning rate based on sign of error
        if delta_1 > 0:
            q_stage1[action_1[trial]] += alpha_pos * delta_1
        else:
            q_stage1[action_1[trial]] += alpha_neg * delta_1
            
        # 2. Stage 2 Update
        delta_2 = reward[trial] - target_val_s2
        
        # Select learning rate based on sign of error
        if delta_2 > 0:
            q_stage2[curr_state, action_2[trial]] += alpha_pos * delta_2
        else:
            q_stage2[curr_state, action_2[trial]] += alpha_neg * delta_2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```