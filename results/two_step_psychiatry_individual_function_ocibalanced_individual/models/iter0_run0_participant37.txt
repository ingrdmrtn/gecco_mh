Here are three cognitive models explaining participant behavior in the two-step task. They range from a purely Model-Free learner to a hybrid Model-Based/Model-Free learner, and finally one that incorporates habit/perseveration.

### Model 1: Pure Model-Free Learner (TD-Learning)
This model assumes the participant ignores the transition structure of the task (the common vs. rare paths). It learns the value of the first-stage spaceships solely based on the reward received at the very end, effectively treating the two steps as a single chain of prediction errors (TD-learning). It does not calculate expected values based on transition probabilities.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free TD Learner.
    
    This model updates values based on Temporal Difference (TD) learning.
    It does not use a transition matrix (model-based planning).
    
    Parameters:
    - learning_rate (alpha): Rate at which Q-values are updated [0, 1].
    - beta: Inverse temperature for softmax choice [0, 10].
    - elig_trace (lambda): Eligibility trace parameter connecting stage 2 to stage 1 [0, 1].
    """
    learning_rate, beta, elig_trace = model_parameters
    n_trials = len(action_1)
    
    # Q-values initialization
    # Stage 1: 2 spaceships
    q_stage1_mf = np.zeros(2) + 0.5 
    # Stage 2: 2 planets x 2 aliens
    q_stage2_mf = np.zeros((2, 2)) + 0.5 

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        # Avoid index errors with -1 data (missing trials)
        if action_1[trial] != -1:
            p_choice_1[trial] = probs_1[int(action_1[trial])]
            a1 = int(action_1[trial])
        else:
            p_choice_1[trial] = 1.0 # Ignore missing data
            a1 = 0 # Dummy for update logic if needed, though we skip updates usually

        # --- Stage 2 Choice ---
        if state[trial] != -1:
            s_idx = int(state[trial])
            exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
            probs_2 = exp_q2 / np.sum(exp_q2)
            
            if action_2[trial] != -1:
                p_choice_2[trial] = probs_2[int(action_2[trial])]
                a2 = int(action_2[trial])
                r = reward[trial]
                
                # --- Learning ---
                # Prediction Error 1: Difference between Stage 2 value and Stage 1 value
                # Note: In pure TD(lambda), the value of the state arrived at is used.
                # Here we use Q-value of chosen action (SARSA-like).
                pe_1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
                
                # Prediction Error 2: Difference between Reward and Stage 2 value
                pe_2 = r - q_stage2_mf[s_idx, a2]
                
                # Update Stage 2 Q-values
                q_stage2_mf[s_idx, a2] += learning_rate * pe_2
                
                # Update Stage 1 Q-values
                # Stage 1 is updated by its own PE plus the eligibility trace of the second stage PE
                q_stage1_mf[a1] += learning_rate * (pe_1 + elig_trace * pe_2)
                
            else:
                p_choice_2[trial] = 1.0
        else:
            p_choice_2[trial] = 1.0

    eps = 1e-10
    # Filter out missing trials from loss calculation
    valid_trials_mask = (action_1 != -1) & (state != -1) & (action_2 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials_mask] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_trials_mask] + eps)))
    return log_loss
```

### Model 2: Hybrid Model-Based / Model-Free Learner
This is the classic "Two-Step" model. It assumes the participant uses a mixture of two systems:
1.  **Model-Based (MB):** Uses the known transition matrix (0.7/0.3) to calculate the value of spaceships based on the current values of the aliens.
2.  **Model-Free (MF):** Learns spaceship values directly from experience.
The parameter `w` controls the balance between these two systems.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Learner.
    
    Combines a model-based planner (using transition matrix) and a model-free learner.
    
    Parameters:
    - learning_rate: Update rate for Q-values [0, 1].
    - beta: Inverse temperature [0, 10].
    - w: Weighting parameter. 0 = Pure MF, 1 = Pure MB [0, 1].
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition matrix: Row=Action, Col=State. 
    # Action 0 -> State 0 (0.7), State 1 (0.3)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2) + 0.5
    q_stage2_mf = np.zeros((2, 2)) + 0.5 # Serves as values for both MB and MF estimation

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # Skip invalid data
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Policy for the first choice ---
        # Model-Based Value: Transition * Max(Stage2 Values)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Weighted integrated value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Policy for the second choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # --- Updates ---
        # Note: In the simplified hybrid model, we often just update Stage 1 MF 
        # using the simple TD error from the stage 2 state value.
        
        # Stage 1 Update (MF only)
        # We use Q(s, a2) as the target (SARSA style) or max Q(s) (Q-learning style).
        # Standard implementation often uses the value of the chosen stage 2 option.
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    valid_mask = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss
```

### Model 3: Hybrid Learner with Perseveration
This model extends the Hybrid learner by adding a "perseveration" parameter. Participants often tend to repeat their previous Stage 1 choice regardless of reward (stickiness). This model captures that behavioral tendency.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner with Choice Perseveration.
    
    Adds a 'stickiness' parameter to the hybrid model.
    
    Parameters:
    - learning_rate: [0, 1]
    - beta: [0, 10]
    - w: Model-based weight [0, 1]
    - pers: Perseveration bonus [0, 1] (can be higher, but keeping small for stability)
    """
    learning_rate, beta, w, pers = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2) + 0.5
    q_stage2_mf = np.zeros((2, 2)) + 0.5
    
    # Track previous choice for perseveration
    last_action_1 = -1

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            last_action_1 = -1 # Reset on missing data
            continue

        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Policy for the first choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Calculate net Q-value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add perseveration bonus to the previously chosen action
        # We work on a copy to not corrupt the actual Q-values for learning
        logits = beta * q_net
        if last_action_1 != -1:
            logits[last_action_1] += pers
            
        exp_q1 = np.exp(logits)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Policy for the second choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # --- Updates ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Update history
        last_action_1 = a1

    eps = 1e-10
    valid_mask = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss
```