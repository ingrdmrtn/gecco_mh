Here are three cognitive models representing different strategies for the two-step task. They range from a pure Model-Free learner to a pure Model-Based learner, and finally a hybrid model that combines both strategies.

### Model 1: Pure Model-Free Learner (TD-Learning)
This model assumes the participant ignores the transition structure of the task (common vs. rare transitions). It learns the value of the first-stage spaceships solely based on the eventual reward, using temporal difference (TD) learning. It does not calculate expected values based on transition probabilities.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free (TD-Learning) Model.
    This model updates values based on direct experience (SARSA-like updates),
    ignoring the task's transition structure.

    Parameters:
    learning_rate: [0,1] - Rate at which value estimates are updated (alpha).
    beta: [0,10] - Inverse temperature for softmax choice rule (exploration/exploitation).
    eligibility_trace: [0,1] - (lambda) How much the Stage 1 value is updated by the Stage 2 reward outcome.
    """
    learning_rate, beta, eligibility_trace = model_parameters
    n_trials = len(action_1)
  
    # Q-values for Stage 1: 2 options (Spaceships A, U)
    q_stage1_mf = np.zeros(2) 
    # Q-values for Stage 2: 2 states (Planets) x 2 options (Aliens)
    q_stage2_mf = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        # Map input actions (-1, 1) to indices (0, 1) if necessary, 
        # but assuming inputs are already 0/1 based on standard Python indexing or pre-processing.
        # If inputs are -1/1, we might need: a1_idx = 0 if action_1[trial] == -1 else 1
        # Based on the provided data snippet, inputs seem to be mixed (-1/1 or 0/1). 
        # Standardizing on 0/1 for array indexing:
        a1_idx = int(action_1[trial])
        if a1_idx == -1: a1_idx = 0 # Handle -1 as index 0
        
        p_choice_1[trial] = probs_1[a1_idx]
        
        # Determine state index
        state_idx = int(state[trial])
        if state_idx == -1: state_idx = 0

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2_idx = int(action_2[trial])
        if a2_idx == -1: a2_idx = 0
            
        p_choice_2[trial] = probs_2[a2_idx]

        # --- Learning Updates ---
        
        # Prediction Error 1: Difference between Stage 2 value and Stage 1 expectation
        # (Using the value of the chosen option in stage 2)
        delta_stage1 = q_stage2_mf[state_idx, a2_idx] - q_stage1_mf[a1_idx]
        
        # Update Stage 1 Value (TD(0) part)
        q_stage1_mf[a1_idx] += learning_rate * delta_stage1
        
        # Prediction Error 2: Difference between Reward and Stage 2 expectation
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2_idx]
        
        # Update Stage 2 Value
        q_stage2_mf[state_idx, a2_idx] += learning_rate * delta_stage2
        
        # Eligibility Trace Update: Propagate Stage 2 prediction error back to Stage 1
        # This allows the final reward to influence the first choice immediately
        q_stage1_mf[a1_idx] += learning_rate * eligibility_trace * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Pure Model-Based Learner
This model assumes the participant perfectly utilizes the transition matrix (70/30). It does not learn the value of Stage 1 actions directly from reward history. Instead, it learns the values of the Aliens (Stage 2) and calculates the value of Spaceships (Stage 1) by multiplying the maximum available value at each planet by the transition probability to get there.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Based Model.
    Calculates Stage 1 values by planning: using the known transition matrix 
    combined with learned Stage 2 values.

    Parameters:
    learning_rate: [0,1] - Rate at which Stage 2 (Alien) values are updated.
    beta: [0,10] - Inverse temperature for softmax choice rule.
    """
    learning_rate, beta = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition matrix: Row 0 -> Planet 0 (0.7), Planet 1 (0.3)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Q-values for Stage 2 only (MB computes Stage 1 on the fly)
    q_stage2_mf = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        
        # --- Stage 1 Choice (Model-Based Planning) ---
        # Value of a state is the max value available in that state
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        
        # Compute Stage 1 values: Expected value over future states
        # V(Action1) = P(State1|Action1)*Max(Q(State1)) + P(State2|Action1)*Max(Q(State2))
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1_idx = int(action_1[trial])
        if a1_idx == -1: a1_idx = 0
            
        p_choice_1[trial] = probs_1[a1_idx]
        
        state_idx = int(state[trial])
        if state_idx == -1: state_idx = 0

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2_idx = int(action_2[trial])
        if a2_idx == -1: a2_idx = 0
        
        p_choice_2[trial] = probs_2[a2_idx]
  
        # --- Learning Updates ---
        # Model-based agents only need to update the terminal values (Stage 2)
        # The Stage 1 values are re-computed every trial based on these.
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2_idx]
        q_stage2_mf[state_idx, a2_idx] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model-Based / Model-Free Learner
This is the classic "Daw 2011" style model. It assumes the participant uses a weighted combination of Model-Free (habitual) and Model-Based (planning) strategies for the first-stage choice. A mixing parameter `w` determines the balance between the two systems.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model (MB/MF).
    Combines Model-Based planning and Model-Free caching for Stage 1 choices.
    
    Parameters:
    learning_rate: [0,1] - Update rate for values.
    beta: [0,10] - Inverse temperature.
    w: [0,1] - Weighting parameter (0 = Pure MF, 1 = Pure MB).
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Model-Free values
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Note: We do not store persistent q_stage1_mb values because they are 
    # re-calculated every trial based on the current state of q_stage2_mf.

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):

        # --- Stage 1 Choice (Hybrid) ---
        
        # 1. Calculate Model-Based values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Integrate MF and MB values
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1_idx = int(action_1[trial])
        if a1_idx == -1: a1_idx = 0
            
        p_choice_1[trial] = probs_1[a1_idx]
        
        state_idx = int(state[trial])
        if state_idx == -1: state_idx = 0

        # --- Stage 2 Choice ---
        # Stage 2 is always just model-free (choosing best alien)
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2_idx = int(action_2[trial])
        if a2_idx == -1: a2_idx = 0
        
        p_choice_2[trial] = probs_2[a2_idx]
  
        # --- Learning Updates ---
        
        # MF Update for Stage 1 (SARSA)
        delta_stage1 = q_stage2_mf[state_idx, a2_idx] - q_stage1_mf[a1_idx]
        q_stage1_mf[a1_idx] += learning_rate * delta_stage1
        
        # MF Update for Stage 2 (Prediction Error from Reward)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2_idx]
        q_stage2_mf[state_idx, a2_idx] += learning_rate * delta_stage2
        
        # Eligibility trace (lambda=1 implicitly for this simplified hybrid model often used)
        # We update Stage 1 MF value again based on the final outcome
        q_stage1_mf[a1_idx] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```