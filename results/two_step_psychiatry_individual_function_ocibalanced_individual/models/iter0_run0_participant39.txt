Here are three cognitive models representing different strategies for the two-step decision task.

### Model 1: Model-Based / Model-Free Hybrid
This is the classic "Daw et al. (2011)" style hybrid model. It assumes the participant combines two systems: a Model-Free (MF) system that learns from direct experience (Temporal Difference learning) and a Model-Based (MB) system that plans using the transition structure of the task. A mixing parameter `w` controls the balance between these two systems.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Reinforcement Learning.
    
    This model assumes the agent uses a weighted combination of:
    1. A Model-Based (MB) value calculated from the transition matrix and stage 2 values.
    2. A Model-Free (MF) value learned via TD-learning from stage 1 choices.
    
    Bounds:
    learning_rate: [0, 1] - How quickly values update.
    beta: [0, 10] - Inverse temperature (exploitation vs exploration).
    w: [0, 1] - Weighting parameter (0 = Pure MF, 1 = Pure MB).
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition matrix as described in the task (A->X common, U->Y common)
    # Rows: Action 1 (0, 1), Cols: State (0, 1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)       # MF values for stage 1 actions
    q_stage2_mf = np.zeros((2, 2))  # Values for stage 2 (State x Action)

    for trial in range(n_trials):

        # --- POLICY FOR THE FIRST CHOICE ---
        # 1. Calculate Model-Based values
        max_q_stage2 = np.max(q_stage2_mf, axis=1) # Max value achievable in each state
        q_stage1_mb = transition_matrix @ max_q_stage2 # Expected value based on transitions

        # 2. Combine MF and MB values using weight w
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # 3. Softmax selection
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Observe state transition
        state_idx = state[trial]

        # --- POLICY FOR THE SECOND CHOICE ---
        # Standard Softmax on Stage 2 Q-values
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- ACTION VALUE UPDATING ---
        
        # 1. Update Stage 1 MF value (TD-error using Stage 2 value as proxy)
        # Note: In pure TD(0), we use the value of the *chosen* second stage option, 
        # or max of next stage. Here we use the chosen Q-value (SARSA-like logic common in this task).
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # 2. Update Stage 2 MF value (prediction error from reward)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Pure Model-Free with Eligibility Traces (TD(1))
This model assumes the participant does *not* use the transition probabilities (no Model-Based planning). Instead, it relies purely on Model-Free learning but uses an **eligibility trace** parameter (`lambda_eligibility`). This allows the reward received at the very end of the trial to directly reinforce the Stage 1 choice, bypassing the need to chain values through Stage 2. This is often called TD($\lambda$).

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free RL with Eligibility Traces (TD-lambda).
    
    This model does not use the transition matrix. Instead, it allows the reward
    at the second stage to directly update the first stage choice via an eligibility trace.
    
    Bounds:
    learning_rate: [0, 1] - Learning rate.
    beta: [0, 10] - Inverse temperature.
    lambda_eligibility: [0, 1] - Eligibility trace decay (0=TD(0), 1=Monte Carlo).
    """
    learning_rate, beta, lambda_eligibility = model_parameters
    n_trials = len(action_1)
  
    # No transition matrix used here (Pure MF)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- POLICY FOR THE FIRST CHOICE ---
        # Pure MF decision
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- POLICY FOR THE SECOND CHOICE ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- ACTION VALUE UPDATING ---
        
        # 1. Stage 1 Update: Standard TD error
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # 2. Stage 2 Update: Reward prediction error
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # 3. Eligibility Trace Update
        # The stage 1 choice is also updated by the stage 2 prediction error,
        # scaled by lambda. This connects the final reward to the first choice.
        q_stage1_mf[action_1[trial]] += learning_rate * lambda_eligibility * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Model-Based with Perseveration
This model is primarily Model-Based (it uses the transition structure) but includes a **perseveration** parameter. Perseveration captures the tendency to repeat the same Stage 1 action regardless of reward history or model-based values ("stickiness"). This is a common nuisance parameter in two-step tasks that often improves model fit significantly.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Based RL with Perseveration (Stickiness).
    
    This model relies on the transition structure (Model-Based) to determine
    Stage 1 values, but adds a 'stickiness' bonus to the action chosen
    on the previous trial.
    
    Bounds:
    learning_rate: [0, 1] - Learning rate for stage 2 values.
    beta: [0, 10] - Inverse temperature.
    perseveration: [0, 5] - Bonus added to the previously chosen action (can be positive or negative).
    """
    learning_rate, beta, perseveration = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # We only track Stage 2 values via learning. Stage 1 is derived.
    q_stage2_mf = np.zeros((2, 2))
    
    # Track previous action for perseveration (initialize to -1 or random)
    last_action_1 = -1

    for trial in range(n_trials):

        # --- POLICY FOR THE FIRST CHOICE ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Calculate net logits
        logits = beta * q_stage1_mb
        
        # Add perseveration bonus if it's not the first trial
        if last_action_1 != -1:
            logits[last_action_1] += perseveration
            
        exp_q1 = np.exp(logits)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- POLICY FOR THE SECOND CHOICE ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- ACTION VALUE UPDATING ---
        
        # Only Stage 2 needs direct updating from reward here, as Stage 1 is calculated
        # dynamically from the transition matrix and current Stage 2 values.
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Store action for next trial's perseveration
        last_action_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```