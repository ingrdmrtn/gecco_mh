Here are three cognitive models based on the two-step task paradigm. They range from a pure Model-Free learner to a hybrid Model-Based/Model-Free learner, and finally a model that incorporates habit (perseveration).

### Model 1: Pure Model-Free Learner (TD Learning)
This model assumes the participant does not use knowledge of the transition structure (which spaceship goes to which planet). Instead, they learn the value of spaceships and aliens purely through trial and error using Temporal Difference (TD) learning. They update the value of the chosen spaceship based on the value of the planet they arrived at, and the value of the chosen alien based on the reward received.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free Reinforcement Learning Model (TD-0).
    
    This model ignores the transition structure (Model-Based) and learns solely 
    from direct experience using Temporal Difference errors. It updates Stage 1 
    values based on the value of the Stage 2 state reached, and Stage 2 values 
    based on the reward received.
    
    Parameters:
    learning_rate: [0,1] - Rate at which value estimates are updated (alpha).
    beta: [0,10] - Inverse temperature for softmax choice rule (exploration/exploitation).
    """
    learning_rate, beta = model_parameters
    n_trials = len(action_1)
  
    # Initialize Q-values
    # Stage 1: 2 spaceships
    q_stage1_mf = np.zeros(2) 
    # Stage 2: 2 planets (states), 2 aliens (actions) per planet
    q_stage2_mf = np.zeros((2, 2)) 

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):

        # --- Stage 1 Choice ---
        # Calculate probabilities using Softmax on Model-Free Q-values
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        # Store probability of the observed action
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Determine which state (planet) was reached
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        # Calculate probabilities for the aliens on the current planet
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        # Store probability of the observed action
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning / Updating ---
        
        # Prediction Error 1: Difference between value of state reached (max Q at stage 2) and chosen spaceship
        # Note: Using max(Q_stage2) is Q-learning (off-policy), using Q_stage2[chosen] is SARSA. 
        # Standard 2-step models often use the value of the chosen option in stage 2 as the target.
        chosen_s2_value = q_stage2_mf[state_idx, action_2[trial]]
        delta_stage1 = chosen_s2_value - q_stage1_mf[action_1[trial]]
        
        # Update Stage 1 Q-value
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1

        # Prediction Error 2: Difference between reward received and chosen alien value
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Update Stage 2 Q-value
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model-Based / Model-Free Learner
This is the classic "Daw et al. (2011)" style model. It assumes the participant uses a mixture of two strategies:
1.  **Model-Based (MB):** Calculates the value of a spaceship by combining the transition probabilities (which are fixed/known here) with the values of the planets (aliens).
2.  **Model-Free (MF):** Learns spaceship values directly from experience.
A mixing parameter (`w`) determines the balance between these two strategies during the first-stage choice.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Reinforcement Learning Model.
    
    This model combines a Model-Based system (planning using the transition matrix)
    and a Model-Free system (TD learning). A mixing parameter 'w' controls the 
    influence of each system on the Stage 1 choice.
    
    Parameters:
    learning_rate: [0,1] - Update rate for Q-values.
    beta: [0,10] - Inverse temperature for softmax.
    w: [0,1] - Weighting parameter (0 = Pure Model-Free, 1 = Pure Model-Based).
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition matrix: Row=Spaceship, Col=Planet
    # Spaceship 0 (A) -> Planet 0 (X) w/ 0.7
    # Spaceship 1 (U) -> Planet 1 (Y) w/ 0.7
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)       # Model-Free values for Stage 1
    q_stage2_mf = np.zeros((2, 2))  # Values for Stage 2 (aliens)

    for trial in range(n_trials):

        # --- Stage 1 Choice ---
        # 1. Calculate Model-Based values
        # Value of a planet is the max value of the aliens on it
        max_q_stage2 = np.max(q_stage2_mf, axis=1) 
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Combine MB and MF values
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # 3. Softmax
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        # Standard softmax on stage 2 values
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        
        # TD Error 1: Target is the value of the chosen Stage 2 option
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # TD Error 2: Target is the reward
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model with Perseveration
This model extends the hybrid MB/MF model by adding a "perseveration" parameter (`p`). This parameter captures the tendency to repeat the previous Stage 1 choice (choosing the same spaceship again), regardless of reward history. This is a common phenomenon in human decision-making tasks where participants stick to a motor response.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Model with Choice Perseveration.
    
    Adds a 'perseveration' bonus to the previously chosen Stage 1 action.
    This accounts for motor repetition or "stickiness" independent of value learning.
    
    Parameters:
    learning_rate: [0,1] - Update rate.
    beta: [0,10] - Inverse temperature.
    w: [0,1] - MB/MF mixing weight.
    p: [0,5] - Perseveration strength (positive favors repetition).
    """
    learning_rate, beta, w, p = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track the previous choice (-1 indicates no previous choice)
    last_action_1 = -1

    for trial in range(n_trials):

        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combine MB and MF
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Add perseveration bonus to the previously chosen action
        # We create a temporary Q-vector for decision making so we don't corrupt the learned values
        q_decision = q_net.copy()
        if last_action_1 != -1:
            q_decision[last_action_1] += p
            
        exp_q1 = np.exp(beta * q_decision)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        
        # Update last action
        last_action_1 = action_1[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```