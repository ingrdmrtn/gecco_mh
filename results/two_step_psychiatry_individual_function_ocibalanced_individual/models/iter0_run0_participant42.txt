Here are three cognitive models proposing different mechanisms for how the participant makes decisions in this two-step task.

### Model 1: Hybrid Model-Based / Model-Free Reinforcement Learning
This is the standard "gold standard" model for this task (Daw et al., 2011). It assumes the participant uses a mixture of two strategies for the first-stage choice:
1.  **Model-Free (MF):** Just repeats what worked before (TD learning).
2.  **Model-Based (MB):** Uses knowledge of the transition structure (Spaceship A -> Planet X) to plan ahead based on the value of the second-stage aliens.
A mixing parameter `w` controls the balance between these two systems.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Reinforcement Learning Model.
    
    This model assumes the agent combines a model-free strategy (learning directly from rewards)
    and a model-based strategy (planning based on transition probabilities).
    
    Parameters:
    learning_rate: [0, 1] Rate at which Q-values are updated.
    beta: [0, 10] Inverse temperature for softmax choice (exploration/exploitation).
    w: [0, 1] Mixing weight. 0 = Pure Model-Free, 1 = Pure Model-Based.
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)
  
    # Transition matrix: T[action_1, state]
    # Row 0 (Spaceship A) -> 0.7 to Planet 0, 0.3 to Planet 1
    # Row 1 (Spaceship U) -> 0.3 to Planet 0, 0.7 to Planet 1
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)      # Model-free values for stage 1 (Spaceships)
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for stage 2 (Aliens at planets)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # 1. Calculate Model-Based values
        # Max value available at each planet
        max_q_stage2 = np.max(q_stage2_mf, axis=1) 
        # Weighted average of planet values based on transition probabilities
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Combine MB and MF values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # 3. Softmax choice probability
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial] # Planet arrived at

        # --- Stage 2 Policy ---
        # Standard softmax on the aliens at the current planet
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        # 1. Update Stage 1 MF (TD(0) update using Stage 2 value as proxy)
        # Note: In full TD(lambda) models, this might include an eligibility trace,
        # but here we use a simple TD update towards the value of the chosen stage 2 state.
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # 2. Update Stage 2 MF (Prediction error based on actual reward)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Pure Model-Free with Eligibility Traces (TD($\lambda$))
This model assumes the participant does *not* use a map of the task (no model-based planning). Instead, they rely entirely on temporal difference learning. Crucially, it includes an "eligibility trace" parameter $\lambda$. This parameter allows the reward received at the very end (from the alien) to directly reinforce the first-stage choice (the spaceship), bridging the gap between the two steps.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free TD(lambda) Learning.
    
    Uses eligibility traces to update first-stage values based on second-stage outcomes.
    
    Parameters:
    learning_rate: [0, 1] Rate at which Q-values are updated.
    beta: [0, 10] Inverse temperature for softmax choice.
    lam: [0, 1] Eligibility trace decay (lambda). Determines how much stage 2 reward updates stage 1 choice.
    """
    learning_rate, beta, lam = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Pure model-free choice
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        # Prediction Error 1: Difference between value of state 2 and expected value of state 1
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        # Update Stage 1 based on transition to Stage 2
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Prediction Error 2: Difference between reward and expected value of state 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Update Stage 2
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Eligibility Trace Update:
        # The Stage 1 choice is also updated by the Stage 2 prediction error (reward surprise),
        # scaled by lambda. This connects the final reward back to the spaceship choice.
        q_stage1_mf[action_1[trial]] += learning_rate * lam * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Choice Kernel (Perseveration) + Model-Free Learning
This model hypothesizes that the participant is influenced by simple repetition bias (perseveration). Regardless of reward, they might just like to repeat their previous key press. The model combines standard Model-Free learning with a "Choice Kernel" (CK) that tracks the frequency of recent choices. A positive `beta_k` means the participant tends to repeat choices (stickiness), while a negative one implies alternation.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Learning with Choice Kernel (Perseveration).
    
    Adds a 'stickiness' factor where the agent is biased towards repeating 
    recently chosen actions, independent of reward.
    
    Parameters:
    learning_rate: [0, 1] Value learning rate.
    beta: [0, 10] Inverse temperature for value-based choice.
    beta_k: [0, 10] Inverse temperature for choice kernel (stickiness/perseveration).
    k_rate: [0, 1] Decay rate for the choice trace (how fast the memory of past choices fades).
    """
    learning_rate, beta, beta_k, k_rate = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Choice kernels (track recent choice history)
    ck_stage1 = np.zeros(2)
    ck_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        # Combine Value (Q) and Habit (CK)
        net_value_1 = (beta * q_stage1_mf) + (beta_k * ck_stage1)
        
        exp_q1 = np.exp(net_value_1 - np.max(net_value_1)) # subtract max for numerical stability
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        net_value_2 = (beta * q_stage2_mf[state_idx]) + (beta_k * ck_stage2[state_idx])
        
        exp_q2 = np.exp(net_value_2 - np.max(net_value_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning (Values) ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # --- Learning (Choice Kernels) ---
        # Update choice trace: chosen action approaches 1, unchosen decays to 0
        # For Stage 1
        ck_stage1 *= (1 - k_rate)
        ck_stage1[action_1[trial]] += k_rate
        
        # For Stage 2
        ck_stage2[state_idx] *= (1 - k_rate)
        ck_stage2[state_idx, action_2[trial]] += k_rate
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```