Here are 3 new cognitive models based on the two-step task structure, exploring different mechanisms than the previous submissions.

### Model 1: Perseveration + Model-Free Learner
This model hypothesizes that the participant might simply repeat their previous choice regardless of the outcome ("perseveration"), in addition to standard model-free learning. This is common in simple reinforcement learning tasks where subjects get "stuck" on an option.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Learner with Choice Perseveration.
    
    This model assumes the participant learns values via simple TD learning (Model-Free)
    but has a bias to repeat the previously chosen stage-1 action, captured by a 
    perseveration parameter 'p'.

    Parameters:
    learning_rate: [0, 1] Rate for updating Q-values.
    beta: [0, 10] Inverse temperature for choice stochasticity.
    perseveration: [0, 5] Bonus added to the Q-value of the previously chosen action.
    """
    learning_rate, beta, perseveration = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Initialize Q-values
    q_stage1_mf = np.zeros(2)      # Values for Spaceships (0, 1)
    q_stage2_mf = np.zeros((2, 2)) # Values for Aliens (Planet 0: 0,1; Planet 1: 0,1)
    
    last_action_1 = -1 # No previous action at start

    for trial in range(n_trials):

        # --- Stage 1 Choice ---
        # Add perseveration bonus to the previously chosen action
        q_stage1_biased = q_stage1_mf.copy()
        if last_action_1 != -1:
            q_stage1_biased[last_action_1] += perseveration

        exp_q1 = np.exp(beta * q_stage1_biased)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Record this action for the next trial's perseveration
        last_action_1 = action_1[trial] 
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        # Update Stage 1 value based on Stage 2 value (SARSA-style TD(0))
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Update Stage 2 value based on Reward
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Separate Learning Rates for Positive/Negative Prediction Errors
This model investigates if the participant learns differently from success versus failure. It splits the learning rate into `alpha_pos` (for positive prediction errors) and `alpha_neg` (for negative prediction errors). This asymmetry is a well-documented phenomenon in reward learning.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Learner with Asymmetric Learning Rates.
    
    This model allows for different learning speeds depending on whether the outcome
    was better than expected (Positive RPE) or worse than expected (Negative RPE).

    Parameters:
    alpha_pos: [0, 1] Learning rate for positive prediction errors.
    alpha_neg: [0, 1] Learning rate for negative prediction errors.
    beta: [0, 10] Inverse temperature for choice stochasticity.
    """
    alpha_pos, alpha_neg, beta = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        if delta_stage1 > 0:
            q_stage1_mf[action_1[trial]] += alpha_pos * delta_stage1
        else:
            q_stage1_mf[action_1[trial]] += alpha_neg * delta_stage1
        
        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        if delta_stage2 > 0:
            q_stage2_mf[state_idx, action_2[trial]] += alpha_pos * delta_stage2
        else:
            q_stage2_mf[state_idx, action_2[trial]] += alpha_neg * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Pure Model-Based Learning with Forgetting
Unlike the hybrid model in the feedback, this model assumes the participant relies *entirely* on a planning strategy (Model-Based) for the first stage, but with a twist: they slowly "forget" the values of the second-stage options over time. This captures the difficulty of maintaining accurate value representations for all four aliens.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Based Learner with Value Decay (Forgetting).
    
    This model assumes the participant calculates Stage 1 values strictly by planning
    through the transition matrix (Model-Based). However, the stored values of the 
    aliens (Stage 2) decay toward 0 on every trial, representing memory loss.

    Parameters:
    learning_rate: [0, 1] Rate for updating Stage 2 values from reward.
    beta: [0, 10] Inverse temperature.
    decay_rate: [0, 1] Rate at which unchosen option values decay to 0 (1 = no decay, 0 = instant forgetting).
    """
    learning_rate, beta, decay_rate = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # We only track Stage 2 values directly. Stage 1 values are computed on the fly.
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Choice (Model-Based Planning) ---
        # Calculate expected value of each spaceship by combining transition probs with max potential alien value
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning and Decay ---
        
        # 1. Decay all Stage 2 values towards 0 before update
        q_stage2 *= decay_rate
        
        # 2. Update the specific alien chosen with the reward
        # (Note: Standard Q-learning update, but applied after decay)
        delta_stage2 = reward[trial] - q_stage2[state_idx, action_2[trial]]
        q_stage2[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```