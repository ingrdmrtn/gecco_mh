Here are the three cognitive models based on the provided template and task description.

### Model 1: TD($\lambda$) Learner
This model introduces an **eligibility trace** parameter ($\lambda$). In standard TD learning (Model-Free), the first stage is only updated based on the prediction of the second stage. In TD($\lambda$), the reward received at the very end of the trial "traces back" to update the first stage choice directly. This allows the agent to learn the value of the spaceship based on the final coins received, not just the planet reached.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    TD(lambda) Model-Free Learner.
    
    This model implements an eligibility trace. The Stage 1 value is updated 
    not only by the transition to Stage 2 (delta_stage1) but also by the 
    final reward prediction error (delta_stage2), scaled by lambda.
    
    Parameters:
    learning_rate: [0, 1] - Alpha for updating Q-values.
    beta: [0, 10] - Inverse temperature for softmax.
    lambda_eligibility: [0, 1] - Eligibility trace decay parameter.
    """
    learning_rate, beta, lambda_eligibility = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        # This model is purely Model-Free, so we ignore the MB calculation in the template
        # but we must compute probabilities for the likelihood.
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2 # Calculated but unused in choice
        
        # Use MF Q-values for choice
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        if action_1[trial] != -1:
            p_choice_1[trial] = probs_1[action_1[trial]]
        else:
            p_choice_1[trial] = 1.0 # Handle missing data
            
        state_idx = state[trial]

        # Handle missing data for updates
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
            continue

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        # We calculate delta_stage2 first to use it in the trace update for stage 1
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Update Stage 1: Standard TD update + Eligibility Trace from Stage 2
        q_stage1_mf[action_1[trial]] += learning_rate * (delta_stage1 + lambda_eligibility * delta_stage2)

        # Update Stage 2
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Choice Stickiness (Perseverance)
This model adds a **stickiness** parameter. Humans often exhibit "choice perseveration," where they tend to repeat their previous motor choice (e.g., pressing the left key again) regardless of whether it was rewarded. This model biases the Stage 1 decision probabilities toward the action taken on the previous trial.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Learner with Choice Stickiness (Perseverance).
    
    Adds a 'stickiness' bonus to the Q-value of the previously chosen 
    Stage 1 action, biasing the agent to repeat choices regardless of reward.
    
    Parameters:
    learning_rate: [0, 1] - Alpha for updating Q-values.
    beta: [0, 10] - Inverse temperature for softmax.
    stickiness: [0, 5] - Bonus added to the previous action's value (can be pos or neg).
    """
    learning_rate, beta, stickiness = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2 
        
        # Add stickiness bonus to the MF value of the previous action
        q_net = q_stage1_mf.copy()
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        if action_1[trial] != -1:
            p_choice_1[trial] = probs_1[action_1[trial]]
            last_action_1 = action_1[trial] # Update history
        else:
            p_choice_1[trial] = 1.0
            last_action_1 = -1
            
        state_idx = state[trial]

        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
            continue

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        # Update Stage 1
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Update Stage 2
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Dual Learning Rates (Stage-Specific Learning)
This model proposes that the participant learns at different speeds for the two stages. Stage 1 involves learning the value of spaceships (which is abstract and relies on transitions), while Stage 2 involves learning the value of aliens (which is direct reward feedback). This model uses `alpha_stage1` and `alpha_stage2`.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Dual Learning Rate Model.
    
    Uses separate learning rates for Stage 1 (Spaceships) and Stage 2 (Aliens).
    This accounts for potential differences in how participants update values
    based on transition predictions vs. direct reward feedback.
    
    Parameters:
    alpha_s1: [0, 1] - Learning rate for Stage 1 updates.
    alpha_s2: [0, 1] - Learning rate for Stage 2 updates.
    beta: [0, 10] - Inverse temperature for softmax.
    """
    alpha_s1, alpha_s2, beta = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Pure MF choice logic using Stage 1 Q-values
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        if action_1[trial] != -1:
            p_choice_1[trial] = probs_1[action_1[trial]]
        else:
            p_choice_1[trial] = 1.0
            
        state_idx = state[trial]

        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
            continue

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        # Update Stage 1 using alpha_s1
        q_stage1_mf[action_1[trial]] += alpha_s1 * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Update Stage 2 using alpha_s2
        q_stage2_mf[state_idx, action_2[trial]] += alpha_s2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```