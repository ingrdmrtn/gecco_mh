Here are three new cognitive models that explore different mechanisms for learning and decision-making in the two-step task. These models introduce concepts like eligibility traces, separate learning rates for stage 1 and 2, and a simple heuristic strategy (Win-Stay, Lose-Shift) combined with reinforcement learning.

### Model 1: TD(1) / Eligibility Trace Model
This model extends the standard Model-Free learning by using an eligibility trace parameter (`lambda`). Instead of just updating the stage 1 value based on the stage 2 value (TD(0)), the reward received at the very end "drifts back" to update the stage 1 choice directly. This captures the idea that the final outcome reinforces the initial decision.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    TD(lambda) Reinforcement Learning Model.
    
    This model uses an eligibility trace (lambda) to allow the final reward 
    to directly influence the value of the first-stage choice, bridging the 
    gap between the first action and the final outcome more effectively than 
    simple TD(0).

    Parameters:
    learning_rate: [0,1] - Update rate for Q-values.
    beta: [0,10] - Inverse temperature for choice softmax (shared for both stages).
    lam: [0,1] - Eligibility trace parameter (lambda). 0 = TD(0), 1 = Monte Carlo.
    """
    learning_rate, beta, lam = model_parameters
    n_trials = len(action_1)

    # Initialize Q-values
    q_stage1 = np.zeros(2)       # Values for spaceships
    q_stage2 = np.zeros((2, 2))  # Values for aliens (2 planets x 2 aliens)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        s2 = state[trial] # The planet arrived at

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2[s2, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]

        # --- Learning ---
        
        # Prediction error at stage 2 (Reward - Q_stage2)
        delta_2 = r - q_stage2[s2, a2]
        
        # Prediction error at stage 1 (Q_stage2 - Q_stage1)
        # Note: We use the value of the CHOSEN stage 2 action (SARSA-like logic)
        delta_1 = q_stage2[s2, a2] - q_stage1[a1]

        # Update Stage 2 Q-values (standard TD update)
        q_stage2[s2, a2] += learning_rate * delta_2

        # Update Stage 1 Q-values
        # The update includes the immediate stage 1 error (delta_1)
        # PLUS the stage 2 error scaled by lambda (eligibility trace)
        q_stage1[a1] += learning_rate * (delta_1 + lam * delta_2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Separate Stage Learning Rates
Standard models often assume a single learning rate for the whole task. However, learning about the stability of spaceships (Stage 1) might happen at a different speed than learning about the fluctuating alien rewards (Stage 2). This model separates `alpha_1` and `alpha_2`.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Dual Learning Rate Model-Free Model.
    
    This model assumes the participant learns the value of the spaceships (stage 1)
    and the value of the aliens (stage 2) at different rates. This helps capture
    differences in volatility or attention between the two stages.

    Parameters:
    alpha_1: [0,1] - Learning rate for Stage 1 (spaceships).
    alpha_2: [0,1] - Learning rate for Stage 2 (aliens).
    beta: [0,10] - Inverse temperature for softmax choices.
    """
    alpha_1, alpha_2, beta = model_parameters
    n_trials = len(action_1)

    # Initialize Q-values
    q_stage1 = np.zeros(2)       
    q_stage2 = np.zeros((2, 2))  

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        s2 = state[trial] 

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2[s2, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]

        # --- Learning ---
        
        # Stage 2 Update: Driven by Reward
        delta_2 = r - q_stage2[s2, a2]
        q_stage2[s2, a2] += alpha_2 * delta_2
        
        # Stage 1 Update: Driven by Stage 2 Value (Temporal Difference)
        # We use the value of the chosen state-action pair in stage 2 as the target
        target_val = q_stage2[s2, a2]
        delta_1 = target_val - q_stage1[a1]
        q_stage1[a1] += alpha_1 * delta_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model-Based/Model-Free with Choice Stickiness
This model brings back the hybrid structure (using the transition matrix) but adds a "stickiness" or perseverance parameter (`persev`). This parameter captures the tendency to simply repeat the previous motor choice at stage 1, regardless of reward history or model-based planning. This is a common behavioral artifact in these tasks.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF Model with Choice Perseverance (Stickiness).
    
    Combines Model-Based planning and Model-Free learning, but adds a 
    perseverance bonus to the previously chosen stage 1 action. This accounts 
    for motor repetition bias often seen in participants.

    Parameters:
    learning_rate: [0,1] - Update rate for Q-values.
    beta: [0,10] - Inverse temperature for choices.
    w: [0,1] - Weighting (0=MF, 1=MB).
    persev: [0,5] - Stickiness parameter (bonus added to previous choice).
    """
    learning_rate, beta, w, persev = model_parameters
    n_trials = len(action_1)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Q-values
    q_stage1_mf = np.zeros(2)      
    q_stage2_mf = np.zeros((2, 2)) 
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Track previous choice for stickiness (initialize as -1 or None)
    last_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # Calculate Model-Based Values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2

        # Combine MB and MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add Stickiness Bonus
        # We create a temporary Q-vector for decision making so we don't corrupt the actual learned values
        q_decision = q_net.copy()
        if last_action_1 != -1:
            q_decision[last_action_1] += persev

        exp_q1 = np.exp(beta * q_decision)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        s2 = state[trial]
        
        # Update last action
        last_action_1 = a1

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[s2, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]

        # --- Learning ---
        # MF Stage 1 Update
        delta_1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_1

        # MF Stage 2 Update
        delta_2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += learning_rate * delta_2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```