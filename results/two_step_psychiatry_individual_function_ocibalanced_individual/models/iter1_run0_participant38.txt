Here are three new cognitive models that explore different mechanisms for learning and decision-making in the two-step task. These models introduce distinct parameters like perseverance (stickiness) and separate learning rates for positive/negative outcomes, moving beyond the standard hybrid model.

### Model 1: Hybrid Model with Choice Perseverance ("Stickiness")
This model extends the standard hybrid learner by adding a "stickiness" parameter. This accounts for the tendency of participants to repeat their previous choice regardless of reward history, a common phenomenon in this task that can mask or enhance model-based/model-free effects.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Choice Perseverance.
    Adds a 'stickiness' parameter to the hybrid MB/MF architecture, capturing
    the tendency to repeat the previous Stage 1 action.
    
    Parameters:
    learning_rate: [0,1] - Update rate for values.
    beta: [0,10] - Inverse temperature.
    w: [0,1] - Weighting parameter (0 = Pure MF, 1 = Pure MB).
    stickiness: [0,5] - Bonus added to the Q-value of the previously chosen action.
    """
    learning_rate, beta, w, stickiness = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Track previous action for stickiness (initialize to -1 or None)
    prev_a1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2

        # Combine MB and MF values
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Add stickiness bonus to the previously chosen action
        if prev_a1 != -1:
            q_net[prev_a1] += stickiness
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        # Handle input data indexing (converting -1 to 0 if necessary for array indexing)
        a1_idx = int(action_1[trial])
        if a1_idx == -1: a1_idx = 0
            
        p_choice_1[trial] = probs_1[a1_idx]
        
        # Update previous action for next trial
        prev_a1 = a1_idx
        
        state_idx = int(state[trial])
        if state_idx == -1: state_idx = 0

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2_idx = int(action_2[trial])
        if a2_idx == -1: a2_idx = 0
        
        p_choice_2[trial] = probs_2[a2_idx]

        # --- Learning Updates ---
        # TD(0) update for Stage 1 (MF)
        delta_stage1 = q_stage2_mf[state_idx, a2_idx] - q_stage1_mf[a1_idx]
        q_stage1_mf[a1_idx] += learning_rate * delta_stage1

        # TD(0) update for Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2_idx]
        q_stage2_mf[state_idx, a2_idx] += learning_rate * delta_stage2

        # TD(1) / Eligibility Trace update for Stage 1 based on Stage 2 outcome
        q_stage1_mf[a1_idx] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Asymmetric Learning Rate Model
This model hypothesizes that the participant updates their beliefs differently depending on whether the outcome was better or worse than expected. It splits the learning rate into `alpha_pos` (for positive prediction errors) and `alpha_neg` (for negative prediction errors), applied within a standard Model-Free framework (TD(1)).

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Asymmetric Learning Rate Model (Model-Free).
    Uses separate learning rates for positive and negative prediction errors.
    This captures optimism/pessimism biases in learning.
    
    Parameters:
    alpha_pos: [0,1] - Learning rate for positive prediction errors (delta > 0).
    alpha_neg: [0,1] - Learning rate for negative prediction errors (delta < 0).
    beta: [0,10] - Inverse temperature.
    """
    alpha_pos, alpha_neg, beta = model_parameters
    n_trials = len(action_1)
  
    # Pure Model-Free setup
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1_idx = int(action_1[trial])
        if a1_idx == -1: a1_idx = 0
        p_choice_1[trial] = probs_1[a1_idx]
        
        state_idx = int(state[trial])
        if state_idx == -1: state_idx = 0

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2_idx = int(action_2[trial])
        if a2_idx == -1: a2_idx = 0
        p_choice_2[trial] = probs_2[a2_idx]

        # --- Learning Updates ---
        
        # 1. Stage 1 Update (TD(0))
        delta_stage1 = q_stage2_mf[state_idx, a2_idx] - q_stage1_mf[a1_idx]
        lr_s1 = alpha_pos if delta_stage1 > 0 else alpha_neg
        q_stage1_mf[a1_idx] += lr_s1 * delta_stage1

        # 2. Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2_idx]
        lr_s2 = alpha_pos if delta_stage2 > 0 else alpha_neg
        q_stage2_mf[state_idx, a2_idx] += lr_s2 * delta_stage2

        # 3. Stage 1 Update from Stage 2 outcome (TD(1) / Eligibility Trace)
        # We use the learning rate appropriate for the second stage error
        q_stage1_mf[a1_idx] += lr_s2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Decay-Based Model-Free Learner
This model introduces a memory decay parameter. Instead of just updating the chosen action's value, the values of unchosen actions "decay" or regress toward 0 (or a neutral value) on every trial. This simulates forgetting and encourages re-exploration of options that haven't been visited in a while.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Decay-Based Model-Free Learner.
    Unchosen action values decay toward 0 at each step, simulating forgetting.
    
    Parameters:
    learning_rate: [0,1] - Update rate for chosen actions.
    decay_rate: [0,1] - Rate at which unchosen Q-values decay toward 0.
    beta: [0,10] - Inverse temperature.
    lambda_trace: [0,1] - Eligibility trace strength (0 = TD(0), 1 = TD(1)).
    """
    learning_rate, decay_rate, beta, lambda_trace = model_parameters
    n_trials = len(action_1)
  
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1_idx = int(action_1[trial])
        if a1_idx == -1: a1_idx = 0
        p_choice_1[trial] = probs_1[a1_idx]
        
        state_idx = int(state[trial])
        if state_idx == -1: state_idx = 0

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2_idx = int(action_2[trial])
        if a2_idx == -1: a2_idx = 0
        p_choice_2[trial] = probs_2[a2_idx]

        # --- Learning Updates ---
        
        # Decay unchosen Stage 1 action
        unchosen_a1 = 1 - a1_idx
        q_stage1_mf[unchosen_a1] = (1 - decay_rate) * q_stage1_mf[unchosen_a1]
        
        # Decay unchosen Stage 2 action (in the current state)
        unchosen_a2 = 1 - a2_idx
        q_stage2_mf[state_idx, unchosen_a2] = (1 - decay_rate) * q_stage2_mf[state_idx, unchosen_a2]

        # Standard Updates
        delta_stage1 = q_stage2_mf[state_idx, a2_idx] - q_stage1_mf[a1_idx]
        q_stage1_mf[a1_idx] += learning_rate * delta_stage1

        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2_idx]
        q_stage2_mf[state_idx, a2_idx] += learning_rate * delta_stage2

        # Eligibility trace update modulated by lambda
        q_stage1_mf[a1_idx] += learning_rate * lambda_trace * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```