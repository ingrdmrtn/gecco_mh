Here are three new cognitive models based on the provided template and data structure. These models explore mechanisms distinct from the previously tried combinations (hybrid weights, eligibility traces, and perseveration).

### Model 1: Asymmetric Learning Rates (Positive vs. Negative)
This model hypothesizes that the participant learns differently from positive outcomes (finding gold) versus negative outcomes (no gold). This is often observed in clinical populations where sensitivity to reward and punishment differs. It uses a Model-Based planning strategy for Stage 1.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Model-Based RL with Asymmetric Learning Rates.
    
    This model assumes the participant uses the transition structure to plan (Model-Based),
    but updates the values of the aliens (Stage 2) with different learning rates
    depending on whether the outcome was a reward (1) or no reward (0).
    
    Bounds:
    alpha_pos: [0, 1] - Learning rate for positive prediction errors (Reward = 1).
    alpha_neg: [0, 1] - Learning rate for negative prediction errors (Reward = 0).
    beta: [0, 10] - Inverse temperature (randomness in choice).
    """
    alpha_pos, alpha_neg, beta = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values for the second stage (aliens)
    q_stage2_mf = np.zeros((2, 2)) # (State, Action)

    for trial in range(n_trials):

        # --- Policy for the first choice (Model-Based) ---
        # Plan based on expected value of best option in stage 2
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Policy for the second choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Action Value Updating ---
        # Calculate Prediction Error
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Select learning rate based on whether the prediction error is positive or negative
        # Note: Since rewards are binary 0 or 1, and Q is between 0 and 1,
        # delta is > 0 if reward is 1, and delta < 0 if reward is 0.
        if delta_stage2 >= 0:
            current_alpha = alpha_pos
        else:
            current_alpha = alpha_neg
            
        q_stage2_mf[state_idx, action_2[trial]] += current_alpha * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Stage-Specific Exploration (Dual Beta)
This model suggests that the level of "noise" or exploration differs between the two stages. Stage 1 involves planning and probabilistic transitions (higher cognitive load), while Stage 2 is a direct bandit task. The participant may be more decisive in one stage than the other.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Based RL with Stage-Specific Inverse Temperatures.
    
    This model separates the exploration parameters for Stage 1 (Spaceships)
    and Stage 2 (Aliens). It allows for the possibility that decision noise 
    differs between the planning stage and the terminal choice stage.
    
    Bounds:
    learning_rate: [0, 1] - Learning rate for stage 2 values.
    beta_1: [0, 10] - Inverse temperature for Stage 1 (Spaceship choice).
    beta_2: [0, 10] - Inverse temperature for Stage 2 (Alien choice).
    """
    learning_rate, beta_1, beta_2 = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Policy for the first choice (Model-Based) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Use Beta 1 for the first stage
        exp_q1 = np.exp(beta_1 * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Policy for the second choice ---
        # Use Beta 2 for the second stage
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Action Value Updating ---
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Memory Decay (Forgetting)
This model incorporates a forgetting mechanism. In complex tasks, participants may struggle to maintain precise value estimates for all options over time. This model assumes that Q-values for all aliens decay slightly towards 0 (or a neutral point) on every trial, representing memory loss, before the chosen option is updated.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Based RL with Value Decay (Forgetting).
    
    This model assumes that learned values for the aliens are not static.
    On every trial, all values decay towards 0 by a factor, simulating memory loss.
    This accounts for the difficulty of tracking 4 distinct changing probabilities.
    
    Bounds:
    learning_rate: [0, 1] - Update rate for the chosen option.
    beta: [0, 10] - Inverse temperature.
    decay_rate: [0, 1] - Rate at which stored values are forgotten (0 = no forgetting, 1 = instant forgetting).
    """
    learning_rate, beta, decay_rate = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Policy for the first choice (Model-Based) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Policy for the second choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Action Value Updating ---
        
        # First, apply decay to ALL values (forgetting step)
        q_stage2_mf = (1 - decay_rate) * q_stage2_mf
        
        # Then, apply standard reinforcement learning update to the CHOSEN option
        # Note: The chosen option has already decayed, so we update from that decayed state
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```