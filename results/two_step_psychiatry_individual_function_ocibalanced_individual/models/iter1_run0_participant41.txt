Here are three new cognitive models exploring different mechanisms for learning and decision-making in the two-step task.

### Model 1: Asymmetric Learning Rates (Positive vs. Negative Prediction Errors)
This model hypothesizes that the participant updates their value estimates differently depending on whether the outcome was better than expected (positive prediction error) or worse than expected (negative prediction error). This is a common finding in reinforcement learning where individuals may be more sensitive to rewards than punishments, or vice versa.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Q-Learning with Asymmetric Learning Rates.
    
    This model assumes purely model-free learning but splits the learning rate
    into two separate parameters: one for positive prediction errors (alpha_pos)
    and one for negative prediction errors (alpha_neg).
    
    Parameters:
    alpha_pos: [0,1] - Learning rate for positive prediction errors (RPE > 0).
    alpha_neg: [0,1] - Learning rate for negative prediction errors (RPE < 0).
    beta: [0,10] - Inverse temperature for softmax choice.
    """
    alpha_pos, alpha_neg, beta = model_parameters
    n_trials = len(action_1)
  
    # Transition matrix is not used for MB planning here, but kept for structure consistency
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values initialized to 0 (or 0.5 to represent neutral expectation)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Decision (Pure Model-Free) ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning Stage 1 ---
        # Using the value of the state reached (TD(0)-like update)
        # Note: In standard Daw tasks, Stage 1 is often updated via eligibility traces 
        # from the final reward, but here we follow the template structure of TD-error.
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        if delta_stage1 > 0:
            q_stage1_mf[action_1[trial]] += alpha_pos * delta_stage1
        else:
            q_stage1_mf[action_1[trial]] += alpha_neg * delta_stage1

        # --- Learning Stage 2 ---
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        if delta_stage2 > 0:
            q_stage2_mf[state_idx, action_2[trial]] += alpha_pos * delta_stage2
        else:
            q_stage2_mf[state_idx, action_2[trial]] += alpha_neg * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model with Eligibility Traces (TD($\lambda$))
Standard Q-learning updates Stage 1 values based on the value of the Stage 2 state. However, many agents update Stage 1 choices directly based on the final reward received at the end of the trial. This model introduces an eligibility trace parameter ($\lambda$) to control how much the Stage 1 choice is reinforced by the Stage 2 reward outcome directly.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model (MB/MF) with Eligibility Traces (Lambda).
    
    This model includes the standard MB/MF mixture but adds an eligibility trace
    parameter (lambda) to the Model-Free component. This allows the Stage 2 reward 
    prediction error to propagate back to update Stage 1 values within the same trial.
    
    Parameters:
    learning_rate: [0,1] - Update rate.
    beta: [0,10] - Inverse temperature.
    w: [0,1] - MB/MF mixing weight (0=Pure MF, 1=Pure MB).
    lam: [0,1] - Eligibility trace decay (0=No trace/TD(0), 1=Full trace/Monte Carlo).
    """
    learning_rate, beta, w, lam = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Decision (Hybrid) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        # 1. Prediction error at Stage 1 (Transition from Choice 1 -> State 2)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # 2. Prediction error at Stage 2 (Reward receipt)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # 3. Eligibility Trace Update: Propagate Stage 2 error back to Stage 1
        # The trace allows the reward outcome to influence the spaceship choice immediately
        q_stage1_mf[action_1[trial]] += learning_rate * lam * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Pure Model-Based with Forgetting
This model assumes the participant relies heavily on the structure of the task (Model-Based) but has imperfect memory for the values of the aliens. A decay parameter is introduced, which pulls the Q-values of unchosen aliens back toward a neutral value (0.5) on every trial. This simulates memory decay or a belief that unobserved options change over time.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Based Learning with Passive Forgetting (Decay).
    
    This model assumes the participant uses the transition matrix (Model-Based)
    to plan Stage 1 choices. However, for the Stage 2 values (aliens), 
    unchosen options decay toward a neutral value (0.5) at each step.
    
    Parameters:
    learning_rate: [0,1] - Update rate for chosen options.
    beta: [0,10] - Inverse temperature.
    decay: [0,1] - Forgetting rate (0=No forgetting, 1=Immediate reset to neutral).
    """
    learning_rate, beta, decay = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Initialize Q-values at 0.5 (neutral expectation for binary 0/1 rewards)
    q_stage2_mf = np.ones((2, 2)) * 0.5

    for trial in range(n_trials):

        # --- Stage 1 Decision (Pure Model-Based) ---
        # Calculate expected value of each spaceship based on current alien values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        # Update chosen alien
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # --- Forgetting / Decay ---
        # All unchosen aliens decay toward 0.5
        for s in range(2):
            for a in range(2):
                if not (s == state_idx and a == action_2[trial]):
                    q_stage2_mf[s, a] = (1 - decay) * q_stage2_mf[s, a] + decay * 0.5
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```