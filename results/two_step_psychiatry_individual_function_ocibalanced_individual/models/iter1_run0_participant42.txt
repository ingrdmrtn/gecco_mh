Here are three new cognitive models exploring different mechanisms for learning and decision-making in the two-step task.

### Model 1: Hybrid Model-Based/Model-Free with Separate Learning Rates
This model introduces separate learning rates for the first stage (spaceship choice) and the second stage (alien choice). This accounts for the possibility that the participant learns the value of immediate actions (aliens) at a different speed than the abstract value of the starting choices (spaceships), while mixing model-based and model-free valuations.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free learner with separate learning rates for each stage.
    
    This model assumes the participant might update their valuation of the aliens (stage 2)
    at a different rate than they update the valuation of the spaceships (stage 1).
    It combines model-based (planning) and model-free values for the first stage choice.

    Parameters:
    alpha1: [0, 1] Learning rate for stage 1 Q-values (spaceships).
    alpha2: [0, 1] Learning rate for stage 2 Q-values (aliens).
    beta: [0, 10] Inverse temperature for softmax choice.
    w: [0, 1] Weighting parameter (0 = pure Model-Free, 1 = pure Model-Based).
    """
    alpha1, alpha2, beta, w = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition matrix as per task description (A->X common, U->Y common)
    # 0 -> 0 (0.7), 0 -> 1 (0.3)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Decision ---
        # Calculate Model-Based Value: Q_MB = Transition * max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combined Value: w * Q_MB + (1-w) * Q_MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial] # Actual state arrived at

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        
        # Stage 1 Update (SARSA-style TD error using stage 2 value)
        # Note: We use the value of the CHOSEN action in stage 2, not max, for standard TD
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += alpha1 * delta_stage1
        
        # Stage 2 Update (Reward Prediction Error)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += alpha2 * delta_stage2
        
        # Note: No eligibility trace (lambda) here; strict separation of stages via alpha1/alpha2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Perseveration Model (Habitual Stickiness)
This model extends a standard reinforcement learning framework by adding a "stickiness" or perseveration parameter. This captures the tendency to repeat the previous Stage 1 choice regardless of reward history, a common phenomenon in this task that is distinct from value-based learning.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Reinforcement Learning with Choice Perseveration (Stickiness).
    
    Adds a bias to repeat the previous stage 1 choice, independent of reward.
    This helps explain why participants might stick to one spaceship even if
    rewards fluctuate.

    Parameters:
    learning_rate: [0, 1] Rate at which Q-values are updated.
    beta: [0, 10] Inverse temperature for softmax choice.
    perseveration: [0, 5] Bonus added to the Q-value of the previously chosen action.
    """
    learning_rate, beta, perseveration = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_choice_1 = -1 # Initialize with no previous choice

    for trial in range(n_trials):

        # --- Stage 1 Decision ---
        # Calculate effective Q-values including stickiness bonus
        q_stage1_effective = q_stage1_mf.copy()
        if last_choice_1 != -1:
            q_stage1_effective[last_choice_1] += perseveration

        exp_q1 = np.exp(beta * q_stage1_effective)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        # Standard TD(0) updates
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Update history
        last_choice_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Asymmetric Learning Rates (Positive/Negative)
This model posits that the participant learns differently from positive outcomes (finding gold) versus negative outcomes (no gold). This "positivity bias" or "negativity bias" is a common heuristic in probabilistic environments.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Learner with Asymmetric Learning Rates for Wins and Losses.
    
    Uses alpha_pos when the prediction error is positive (better than expected)
    and alpha_neg when the prediction error is negative (worse than expected).

    Parameters:
    alpha_pos: [0, 1] Learning rate for positive prediction errors.
    alpha_neg: [0, 1] Learning rate for negative prediction errors.
    beta: [0, 10] Inverse temperature for softmax choice.
    """
    alpha_pos, alpha_neg, beta = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Decision ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        if delta_stage1 > 0:
            q_stage1_mf[action_1[trial]] += alpha_pos * delta_stage1
        else:
            q_stage1_mf[action_1[trial]] += alpha_neg * delta_stage1

        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        if delta_stage2 > 0:
            q_stage2_mf[state_idx, action_2[trial]] += alpha_pos * delta_stage2
        else:
            q_stage2_mf[state_idx, action_2[trial]] += alpha_neg * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```