Here are three new cognitive models that explore different mechanisms and parameter combinations not yet tested, specifically focusing on separate learning rates for stages, separate temperatures for stages, and distinct reward sensitivity.

### Model 1: Stage-Specific Learning Rates
This model hypothesizes that the participant learns at different speeds for the first-stage decision (which spaceship to choose) versus the second-stage decision (which alien to ask). It uses a pure model-free architecture but splits the learning rate.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Dual Learning Rate Model-Free Learner.
    
    This model assumes the participant updates their expectations for Stage 1 (spaceships)
    at a different rate than Stage 2 (aliens). This captures potential differences in 
    volatility or attention between the abstract transition structure and the concrete reward structure.

    Parameters:
    alpha_1: [0, 1] Learning rate for Stage 1 (Spaceship choice).
    alpha_2: [0, 1] Learning rate for Stage 2 (Alien choice).
    beta: [0, 10] Inverse temperature for choice stochasticity (shared).
    """
    alpha_1, alpha_2, beta = model_parameters
    n_trials = len(action_1)
  
    # transition_matrix is not used in pure MF, but kept for structural consistency with template
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice (Pure Model-Free)
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Update Stage 1 using alpha_1
        # Using SARSA-style update: Q1(a1) <- Q1(a1) + alpha1 * (Q2(s, a2) - Q1(a1))
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += alpha_1 * delta_stage1
        
        # Update Stage 2 using alpha_2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += alpha_2 * delta_stage2
        
        # Eligibility trace update for Stage 1 driven by final reward (using alpha_1)
        q_stage1_mf[action_1[trial]] += alpha_1 * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Stage-Specific Exploration (Dual Beta)
This model posits that the "exploration vs. exploitation" trade-off differs between the two stages. The participant might be very consistent in picking their favorite alien (high beta) but more random in picking spaceships (low beta), or vice versa.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Dual Temperature Model-Based/Model-Free Hybrid.
    
    This model allows for different levels of choice stochasticity (exploration) 
    at Stage 1 vs Stage 2. It assumes a simplified hybrid structure where Stage 1
    relies on model-based values, while Stage 2 is model-free.

    Parameters:
    learning_rate: [0, 1] Learning rate for value updates.
    beta_1: [0, 10] Inverse temperature for Stage 1 choices.
    beta_2: [0, 10] Inverse temperature for Stage 2 choices.
    """
    learning_rate, beta_1, beta_2 = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # We track Q-values for stage 2 (aliens)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice: Model-Based
        # We calculate the expected value of Stage 1 based on the transition matrix and max Stage 2 values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Use beta_1 for Stage 1
        exp_q1 = np.exp(beta_1 * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice: Model-Free
        # Use beta_2 for Stage 2
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Update Stage 2 values
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Reward Sensitivity and Perseveration
This model introduces a `reward_sensitivity` parameter (scaling the impact of the reward outcome) and a simple `perseveration` parameter for the second stage. This tests if the participant over/under-values the coins relative to the learning process and if they have a "sticky" choice preference for aliens regardless of reward.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Reward Sensitivity and Stage 2 Perseveration Model.
    
    This model modifies the standard reinforcement learning update by scaling the 
    reward term (reward sensitivity) and adding a bias to repeat the previous 
    Stage 2 action (perseveration).

    Parameters:
    learning_rate: [0, 1] Rate for updating values.
    beta: [0, 10] Inverse temperature for choice.
    rho: [0, 10] Reward sensitivity. Scales the effective reward value (e.g., is a coin worth 1 unit or 5 units of utility?).
    perseveration: [0, 5] Bonus added to the Q-value of the previously chosen alien (Stage 2 choice stickiness).
    """
    learning_rate, beta, rho, perseveration = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track previous choice for perseveration (initialize to -1 or handle first trial separately)
    prev_action_2 = -1 
    prev_state = -1

    for trial in range(n_trials):

        # policy for the first choice (Standard MF)
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice with Perseveration
        # Create a temporary Q-table for decision making that includes the perseveration bonus
        q_stage2_decision = q_stage2_mf[state_idx, :].copy()
        
        # If we were in this state on the very last trial, apply stickiness to the action taken
        if trial > 0 and state_idx == prev_state:
             q_stage2_decision[prev_action_2] += perseveration

        exp_q2 = np.exp(beta * q_stage2_decision)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Update Stage 1
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Update Stage 2 with Reward Sensitivity (rho)
        # Effective reward is rho * reward
        effective_reward = rho * reward[trial]
        delta_stage2 = effective_reward - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Update eligibility trace
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage2
        
        # Store history for next trial
        prev_action_2 = action_2[trial]
        prev_state = state_idx

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```