Here are three new cognitive models exploring different mechanisms for learning and decision-making in the two-step task.

### Model 1: Hybrid Learner (Model-Based + Model-Free)
This model combines Model-Free (MF) learning (which reinforces actions based on direct reward history) and Model-Based (MB) planning (which uses the transition structure of the task to compute values). A mixing parameter `w` controls the balance between these two systems. This is a classic formulation for this task.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner (Model-Based + Model-Free).
    
    Combines a model-free system (TD learning) with a model-based system 
    (planning using the transition matrix). The final Stage 1 value is a 
    weighted sum of both.
    
    Parameters:
    learning_rate: [0, 1] - Alpha for updating Q-values.
    beta: [0, 10] - Inverse temperature for softmax.
    w: [0, 1] - Weighting parameter (0 = pure Model-Free, 1 = pure Model-Based).
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)
  
    # Transition matrix: Row 0 -> [P(State 0|Act 0), P(State 1|Act 0)]
    #                    Row 1 -> [P(State 0|Act 1), P(State 1|Act 1)]
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)      # Model-free Q-values for Stage 1
    q_stage2_mf = np.zeros((2, 2)) # Model-free Q-values for Stage 2 (Aliens)

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        
        # 1. Model-Based Value Calculation
        # Max value available at stage 2
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        # Expected value of stage 1 actions given transition probabilities
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Hybrid Value Calculation
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # 3. Action Selection
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        if action_1[trial] != -1:
            p_choice_1[trial] = probs_1[action_1[trial]]
        else:
            p_choice_1[trial] = 1.0 # Ignore missing data

        # Skip learning if data is missing
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
            continue
            
        state_idx = state[trial] # 0 or 1 (Planets)

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning / Updating ---
        
        # SARSA / TD(0) update for Stage 1 MF
        # The value of the chosen stage 1 action is updated toward the value of the chosen stage 2 state-action
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Prediction Error for Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Update Stage 2 MF values
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Asymmetric Learning Rates (Reward vs. Punishment)
This model hypothesizes that the participant learns differently from positive outcomes (finding gold) versus negative outcomes (no gold). It splits the learning rate `alpha` into `alpha_pos` and `alpha_neg`. This is a pure Model-Free learner but with sensitivity to valence.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Learner with Asymmetric Learning Rates.
    
    Uses separate learning rates for positive prediction errors (better than expected)
    and negative prediction errors (worse than expected).
    
    Parameters:
    alpha_pos: [0, 1] - Learning rate for positive prediction errors (reward > expectation).
    alpha_neg: [0, 1] - Learning rate for negative prediction errors (reward < expectation).
    beta: [0, 10] - Inverse temperature for softmax.
    """
    alpha_pos, alpha_neg, beta = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Decision ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        if action_1[trial] != -1:
            p_choice_1[trial] = probs_1[action_1[trial]]
        else:
            p_choice_1[trial] = 1.0

        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
            continue
            
        state_idx = state[trial]

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        if delta_stage1 > 0:
            q_stage1_mf[action_1[trial]] += alpha_pos * delta_stage1
        else:
            q_stage1_mf[action_1[trial]] += alpha_neg * delta_stage1
        
        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        if delta_stage2 > 0:
            q_stage2_mf[state_idx, action_2[trial]] += alpha_pos * delta_stage2
        else:
            q_stage2_mf[state_idx, action_2[trial]] += alpha_neg * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Model-Free Learner with Eligibility Traces (TD(lambda))
This model implements a temporal difference learning algorithm with eligibility traces. Instead of just updating the immediately preceding state-action pair, the reward at the end of the trial "trickles back" to update the Stage 1 choice directly, weighted by a decay parameter `lambda_eligibility`. This allows the Stage 1 choice to be reinforced by the final reward without relying strictly on the Stage 2 value estimate.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Learner with Eligibility Traces (TD-Lambda).
    
    The Stage 1 action value is updated not just by the transition to Stage 2,
    but also directly by the final reward, scaled by lambda. This bridges the 
    gap between the first choice and the final outcome.
    
    Parameters:
    learning_rate: [0, 1] - Alpha for updating Q-values.
    beta: [0, 10] - Inverse temperature for softmax.
    lambda_eligibility: [0, 1] - Decay factor for the eligibility trace.
    """
    learning_rate, beta, lambda_eligibility = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Decision ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        if action_1[trial] != -1:
            p_choice_1[trial] = probs_1[action_1[trial]]
        else:
            p_choice_1[trial] = 1.0

        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
            continue
            
        state_idx = state[trial]

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        
        # 1. Calculate Prediction Error at Stage 1 (Transition)
        # The "reward" for stage 1 is the value of the state we landed in (Stage 2)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        # Update Stage 1 based on immediate transition
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # 2. Calculate Prediction Error at Stage 2 (Outcome)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Update Stage 2 based on reward
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # 3. Eligibility Trace Update for Stage 1
        # The Stage 1 action also gets a bump from the Stage 2 prediction error,
        # scaled by lambda. If lambda=0, this is standard TD(0). If lambda=1, it's Monte Carlo.
        q_stage1_mf[action_1[trial]] += learning_rate * lambda_eligibility * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```