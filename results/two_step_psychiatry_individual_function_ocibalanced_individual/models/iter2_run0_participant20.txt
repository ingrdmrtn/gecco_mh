Here are 3 new cognitive models that explore different parameter configurations and mechanisms not fully covered in the previous attempts.

### Model 1: Hybrid Learner with Distinct Learning Rates for Positive/Negative Prediction Errors
This model hypothesizes that the participant learns differently from positive surprises (getting a coin) versus negative surprises (getting nothing). This asymmetry is often observed in clinical populations (e.g., depression or anxiety) and is captured by splitting the learning rate.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner with Asymmetric Learning Rates.
    
    This model separates the learning rate into two components: one for positive
    prediction errors (better than expected) and one for negative prediction errors
    (worse than expected). This captures biases in how good vs. bad news is integrated.
    
    Parameters:
    - learning_rate_pos: [0,1] Update rate for positive prediction errors.
    - learning_rate_neg: [0,1] Update rate for negative prediction errors.
    - beta: [0,10] Inverse temperature (choice consistency).
    - w: [0,1] Weighting parameter (0=Pure MF, 1=Pure MB).
    """
    learning_rate_pos, learning_rate_neg, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Q-value
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Value Updating ---
        # Stage 2 Update (Reward Prediction Error)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Apply asymmetric learning rates
        lr_2 = learning_rate_pos if delta_stage2 > 0 else learning_rate_neg
        q_stage2_mf[state_idx, action_2[trial]] += lr_2 * delta_stage2
        
        # Stage 1 Update (TD Prediction Error passing back from Stage 2)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        lr_1 = learning_rate_pos if delta_stage1 > 0 else learning_rate_neg
        q_stage1_mf[action_1[trial]] += lr_1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Model-Free Learner with Eligibility Traces (Sarsa($\lambda$))
Instead of the hybrid MB/MF architecture, this model assumes a pure Model-Free approach but uses eligibility traces (controlled by `lambda_decay`). This allows the reward at the end of the trial to directly influence the value of the first-stage choice, bridging the temporal gap without needing an explicit transition model.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Learner with Eligibility Traces (Sarsa-Lambda).
    
    This model does not use a Model-Based component (w is effectively 0). 
    Instead, it uses an eligibility trace parameter (lambda) to allow the 
    Stage 2 reward to update Stage 1 values directly.
    
    Parameters:
    - learning_rate: [0,1] Update rate.
    - beta: [0,10] Inverse temperature.
    - lambda_decay: [0,1] Decay rate of the eligibility trace. 
      (0 = TD(0), only immediate updates; 1 = Monte Carlo-like updates).
    """
    learning_rate, beta, lambda_decay = model_parameters
    n_trials = len(action_1)
  
    # No transition matrix needed for pure MF
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy (Pure MF) ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Value Updating ---
        # Sarsa(lambda) logic:
        # 1. Prediction error at stage 1 (difference between Q_stage2 and Q_stage1)
        delta_1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        # 2. Prediction error at stage 2 (difference between Reward and Q_stage2)
        delta_2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Update Stage 2 Q-value (standard TD)
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_2
        
        # Update Stage 1 Q-value
        # It gets updated by its own immediate TD error (delta_1)
        # PLUS the discounted error from stage 2 (lambda * delta_2)
        q_stage1_mf[action_1[trial]] += learning_rate * (delta_1 + lambda_decay * delta_2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Learner with Separate Inverse Temperatures (Beta)
Previous models assumed the "noise" or exploration (Beta) was constant across both stages of the task. This model proposes that decision-making precision might differ between the abstract spaceship choice (Stage 1) and the concrete alien choice (Stage 2).

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner with Stage-Specific Inverse Temperatures.
    
    This model allows for different levels of stochasticity (exploration/exploitation)
    at the two different stages of the task.
    
    Parameters:
    - learning_rate: [0,1] Update rate.
    - beta_1: [0,10] Inverse temperature for Stage 1 (Spaceships).
    - beta_2: [0,10] Inverse temperature for Stage 2 (Aliens).
    - w: [0,1] Weighting parameter (0=Pure MF, 1=Pure MB).
    """
    learning_rate, beta_1, beta_2, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy (Uses beta_1) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy (Uses beta_2) ---
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Value Updating ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```