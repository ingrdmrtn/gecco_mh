Here are three new cognitive models that explore different mechanisms for learning and decision-making in the two-step task, avoiding the exact parameter combinations previously tested.

### Model 1: Hybrid Learner with Separate Learning Rates for Stage 1 and Stage 2
This model hypothesizes that the participant learns the value of spaceships (Stage 1) at a different speed than the value of aliens (Stage 2). This is biologically plausible as Stage 1 involves abstract transition structures while Stage 2 involves direct reward feedback. It uses a mixing parameter `w` to combine Model-Based (planning) and Model-Free (habitual) value estimation.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner with Separate Learning Rates.
    Distinguishes between learning at the first stage (transitions/abstract values)
    and the second stage (direct reward values), combined with a model-based weight.

    Parameters:
    lr_stage1: [0, 1] - Learning rate for first-stage updates (TD(1)).
    lr_stage2: [0, 1] - Learning rate for second-stage updates (reward prediction error).
    beta: [0, 10] - Inverse temperature for softmax.
    w: [0, 1] - Weighting parameter (0 = pure Model-Free, 1 = pure Model-Based).
    """
    lr_stage1, lr_stage2, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Calculate Model-Based values: Transition * Max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrate Model-Free and Model-Based values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updating ---
        # Stage 2 RPE (used for both updates)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Update Stage 1 MF values using TD-lambda-like logic (TD(1) here)
        # We update Q1 based on the eventual reward, not just the transition
        q_stage1_mf[action_1[trial]] += lr_stage1 * (reward[trial] - q_stage1_mf[action_1[trial]])
        
        # Update Stage 2 values
        q_stage2_mf[state_idx, action_2[trial]] += lr_stage2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Model-Free Learner with Eligibility Traces (TD(lambda))
This model abandons the Model-Based component entirely but introduces an eligibility trace parameter (`lambda`). This allows the second-stage reward to directly influence the first-stage choice more effectively than simple one-step TD learning. It tests if the participant is a sophisticated habitual learner rather than a planner.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Learner with Eligibility Traces (TD-lambda).
    Uses an eligibility trace to modulate how much the second-stage reward 
    updates the first-stage value.

    Parameters:
    learning_rate: [0, 1] - Rate for updating Q-values.
    beta: [0, 10] - Inverse temperature for softmax.
    lam: [0, 1] - Eligibility trace decay parameter (lambda). 
                  0 = TD(0) (only immediate next state matters), 
                  1 = Monte Carlo (full reward propagates back).
    """
    learning_rate, beta, lam = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updating ---
        # Prediction error at stage 1 (based on value of state landed in)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        # Prediction error at stage 2 (based on reward)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Update Stage 2
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Update Stage 1: Includes immediate error + weighted portion of stage 2 error
        q_stage1_mf[action_1[trial]] += learning_rate * (delta_stage1 + lam * delta_stage2)
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Learner with Inverse Temperature Modulation (Exploration Bonus)
This model posits that the participant's exploration changes based on the first-stage choice. It introduces a parameter `beta_stick` which acts as a "decisiveness" or "confidence" booster. If the previous trial was rewarded, the agent might become more exploitative (higher beta) specifically for the previously chosen action, creating a dynamic interaction between learning and action selection distinct from simple additive stickiness.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner with Dynamic Beta (Win-Stay/Lose-Shift modulation).
    The inverse temperature (exploitation) increases if the previous trial 
    was rewarded, effectively modeling a 'hot hand' confidence effect.

    Parameters:
    learning_rate: [0, 1] - Rate for updating Q-values.
    beta_base: [0, 10] - Baseline inverse temperature.
    beta_boost: [0, 5] - Increase in beta after a rewarded trial.
    w: [0, 1] - Weighting parameter (0 = pure Model-Free, 1 = pure Model-Based).
    """
    learning_rate, beta_base, beta_boost, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_reward = 0

    for trial in range(n_trials):
        
        # Dynamic Beta: Be more deterministic if previously rewarded
        current_beta = beta_base + (beta_boost * prev_reward)

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(current_beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        # We use base beta for stage 2 to isolate the effect to the main strategic choice
        exp_q2 = np.exp(beta_base * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updating ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        prev_reward = reward[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```