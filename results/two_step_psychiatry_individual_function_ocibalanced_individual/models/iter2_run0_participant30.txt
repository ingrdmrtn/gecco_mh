Here are three new cognitive models based on the two-step task structure, designed to explore mechanisms distinct from the parameter combinations already tested.

### Model 1: Pure Model-Based with Perseveration
This model builds on the "Best Model" (Pure Model-Based) by adding a **perseveration** parameter. In two-step tasks, participants often exhibit a "stickiness" or motor repetition bias, where they are more likely to repeat their previous Stage 1 choice regardless of the reward. This model tests if the participant is a planner who simply has a habit of repeating keys.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Based Learner with Perseveration.
    
    This model plans Stage 1 choices based on the transition matrix and Stage 2 values,
    but adds a "perseveration" bonus to the previously chosen Stage 1 action.
    
    Parameters:
    - learning_rate: Rate at which Stage 2 Q-values are updated [0, 1].
    - beta: Inverse temperature for softmax choice [0, 10].
    - perseveration: Bonus added to the log-odds of repeating the previous Stage 1 choice [0, 5].
    """
    learning_rate, beta, perseveration = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # We only need Stage 2 MF values for a Model-Based learner
    q_stage2_mf = np.zeros((2, 2))
    
    # Track previous choice for perseveration
    prev_choice_1 = -1

    for trial in range(n_trials):

        # --- Policy for the first choice (Model-Based + Perseveration) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Calculate logits (values before exponentiation)
        logits_1 = beta * q_stage1_mb
        
        # Add perseveration bonus if it's not the first trial
        if prev_choice_1 != -1:
            logits_1[prev_choice_1] += perseveration
            
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Policy for the second choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # Update Stage 2 Q-values (Standard Rescorla-Wagner)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Store choice for next trial
        prev_choice_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Pure Model-Based with Passive Decay (Forgetting)
Since the aliens' reward probabilities change slowly, old information becomes stale. This model assumes the participant is a Model-Based learner who "forgets" the value of the unchosen alien on every trial. The value of the unchosen option decays toward a neutral point (0.5), allowing the model to adapt to non-stationarity better than standard learning alone.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Based Learner with Passive Decay.
    
    Stage 2 Q-values for the *unchosen* alien decay toward 0.5 (neutral probability)
    on every trial. This helps the model handle the non-stationary reward probabilities.
    
    Parameters:
    - learning_rate: Update rate for the chosen alien [0, 1].
    - beta: Inverse temperature for softmax choice [0, 10].
    - decay_rate: Rate at which unchosen values revert to 0.5 [0, 1].
    """
    learning_rate, beta, decay_rate = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Initialize at 0.5 (neutral expectation) rather than 0, as decay targets 0.5
    q_stage2_mf = np.ones((2, 2)) * 0.5

    for trial in range(n_trials):

        # --- Policy for the first choice (Pure Model-Based) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Policy for the second choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        chosen_alien = action_2[trial]
        unchosen_alien = 1 - chosen_alien
        
        # Update Chosen Alien (Rescorla-Wagner)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, chosen_alien]
        q_stage2_mf[state_idx, chosen_alien] += learning_rate * delta_stage2
        
        # Update Unchosen Alien (Passive Decay toward 0.5)
        # New_Value = Old_Value * (1 - decay) + Target * decay
        q_stage2_mf[state_idx, unchosen_alien] = (q_stage2_mf[state_idx, unchosen_alien] * (1 - decay_rate)) + (0.5 * decay_rate)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Model-Free TD($\lambda$) Learner
This model abandons the Model-Based component entirely to test a nuanced Model-Free hypothesis. Instead of a simple hybrid, this uses **eligibility traces** ($\lambda$). This connects the Stage 1 choice directly to the Stage 2 reward. If $\lambda > 0$, the Stage 1 choice is reinforced by the final coin outcome, not just the transition to the planet. This is distinct from previous models that used `w` (hybridization) or `alpha` variations.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free TD(lambda) Learner.
    
    Uses eligibility traces to update Stage 1 values based on the Stage 2 reward.
    This creates a direct link between the spaceship choice and the final coin, 
    bypassing the transition structure (Model-Free).
    
    Parameters:
    - learning_rate: Rate for TD updates [0, 1].
    - beta: Inverse temperature [0, 10].
    - lambda_trace: Eligibility trace parameter [0, 1]. 
                    0 = TD(0) (only immediate state updates), 
                    1 = Monte Carlo (Stage 1 updated fully by final reward).
    """
    learning_rate, beta, lambda_trace = model_parameters
    n_trials = len(action_1)
  
    # Model-Free values only
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Policy for the first choice (Model-Free) ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Policy for the second choice (Model-Free) ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates with Eligibility Trace ---
        
        # 1. Prediction error at Stage 1 (State 1 -> State 2)
        # Note: Reward at transition is 0, value is Q_stage2
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        # Update Stage 1 based on transition
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # 2. Prediction error at Stage 2 (State 2 -> Reward)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Update Stage 2 based on reward
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # 3. Eligibility Trace Update: 
        # Pass the Stage 2 prediction error back to Stage 1, scaled by lambda
        q_stage1_mf[action_1[trial]] += learning_rate * lambda_trace * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```