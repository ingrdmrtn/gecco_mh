Here are three new cognitive models that explore different mechanisms and parameter combinations, specifically focusing on eligibility traces, separate learning rates for stages, and forgetting mechanisms.

### Model 1: Hybrid MB/MF with Eligibility Traces (Lambda)
This model introduces an eligibility trace parameter `lam` (lambda). In standard Q-learning (TD(0)), the Stage 1 value is updated based on the Stage 2 value. With eligibility traces (TD($\lambda$)), the Stage 1 value is also directly updated by the final reward, scaled by lambda. This allows the agent to credit the first choice directly for the final outcome, bridging the gap between Model-Free and Model-Based logic in a computationally cheap way.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF Model with Eligibility Traces (TD-Lambda).
    
    This model uses a weighting parameter 'w' to mix model-based and model-free
    values. Crucially, it uses an eligibility trace 'lam' to allow the stage 1 
    model-free value to be updated directly by the stage 2 reward, not just 
    the stage 2 value prediction.

    Parameters:
    learning_rate: [0,1] - Update rate for Q-values.
    beta: [0,10] - Inverse temperature for choices.
    w: [0,1] - Weighting (0=MF, 1=MB).
    lam: [0,1] - Eligibility trace decay (0=TD(0), 1=Monte Carlo).
    """
    learning_rate, beta, w, lam = model_parameters
    n_trials = len(action_1)

    # Fixed transition matrix (70/30 common/rare)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    q_stage1_mf = np.zeros(2)      
    q_stage2_mf = np.zeros((2, 2)) 
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # Model-Based Value: Expected value of next stage based on transition matrix
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2

        # Integrated Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        s2 = state[trial] # Planet arrived at

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[s2, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]

        # --- Learning ---
        # Stage 1 Prediction Error (driven by Stage 2 value)
        delta_1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_1
        
        # Stage 2 Prediction Error (driven by reward)
        delta_2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += learning_rate * delta_2
        
        # Eligibility Trace Update:
        # Update Stage 1 value again based on the Stage 2 prediction error,
        # scaled by lambda. This connects the final reward to the first choice.
        q_stage1_mf[a1] += learning_rate * lam * delta_2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Distinct Learning Rates for Stage 1 and Stage 2
This model posits that learning happens at different speeds for the high-level planning (Stage 1 spaceship choice) versus the low-level bandit task (Stage 2 alien choice). The `alpha_1` parameter controls how quickly the agent updates values for spaceships, while `alpha_2` controls updates for aliens. This separates the timescales of learning without invoking complex model-based mechanics.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Q-Learning with Separate Stage Learning Rates.
    
    This model assumes the participant learns the value of the spaceships (Stage 1)
    at a different rate than they learn the value of the aliens (Stage 2). 
    It is purely model-free (no transition matrix usage).

    Parameters:
    alpha_1: [0,1] - Learning rate for Stage 1 (Spaceships).
    alpha_2: [0,1] - Learning rate for Stage 2 (Aliens).
    beta_1: [0,10] - Inverse temperature for Stage 1 choices.
    beta_2: [0,10] - Inverse temperature for Stage 2 choices.
    """
    alpha_1, alpha_2, beta_1, beta_2 = model_parameters
    n_trials = len(action_1)

    q_stage1 = np.zeros(2)      
    q_stage2 = np.zeros((2, 2)) 
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # Pure Model-Free choice
        exp_q1 = np.exp(beta_1 * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        s2 = state[trial] 

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta_2 * q_stage2[s2, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]

        # --- Learning ---
        # Update Stage 1 using alpha_1
        # Note: Standard SARSA-like update using Q-value of chosen next state
        delta_1 = q_stage2[s2, a2] - q_stage1[a1]
        q_stage1[a1] += alpha_1 * delta_1
        
        # Update Stage 2 using alpha_2
        delta_2 = r - q_stage2[s2, a2]
        q_stage2[s2, a2] += alpha_2 * delta_2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid MB/MF with Passive Forgetting
This model incorporates a `decay` parameter. In many reinforcement learning tasks, participants not only learn from current rewards but also slowly "forget" the values of unchosen options, which decay back toward zero (or a neutral value). This helps explain why participants might revisit options they haven't tried in a while, or why learned values don't persist indefinitely.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF Model with Passive Value Decay (Forgetting).
    
    In addition to standard hybrid learning, Q-values for unchosen options
    decay towards 0 on every trial. This simulates memory loss or a 
    'return to baseline' effect.

    Parameters:
    learning_rate: [0,1] - Update rate for chosen Q-values.
    beta: [0,10] - Inverse temperature for choices.
    w: [0,1] - Weighting (0=MF, 1=MB).
    decay: [0,1] - Rate at which unchosen values decay (1=no decay, 0=instant forgetting).
    """
    learning_rate, beta, w, decay = model_parameters
    n_trials = len(action_1)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    q_stage1_mf = np.zeros(2)      
    q_stage2_mf = np.zeros((2, 2)) 
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2

        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        s2 = state[trial]

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[s2, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]

        # --- Learning & Decay ---
        
        # 1. Decay all Stage 1 values
        q_stage1_mf *= decay
        # 2. Update chosen Stage 1 value (standard TD)
        # We add the decay effect first, then the learning update. 
        # (This implies the update is 'fresh' and resists decay for this step).
        delta_1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_1

        # 3. Decay all Stage 2 values
        q_stage2_mf *= decay
        # 4. Update chosen Stage 2 value
        delta_2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += learning_rate * delta_2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```