Here are three new cognitive models exploring different mechanisms for learning and decision-making in the two-step task.

### Model 1: Asymmetric Learning Rates (Positive vs. Negative Prediction Errors)
This model hypothesizes that the participant learns differently from positive surprises (better than expected) versus negative surprises (worse than expected). This asymmetry is a common finding in reinforcement learning literature, suggesting different neural sensitivities to gains and losses.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Asymmetric Learning Rates.
    
    Distinguishes between learning from positive prediction errors (better than expected)
    and negative prediction errors (worse than expected).
    
    Bounds:
    lr_pos: [0, 1] (Learning rate for positive prediction errors)
    lr_neg: [0, 1] (Learning rate for negative prediction errors)
    beta: [0, 10]
    w: [0, 1] (Mixing weight: 1=Model-Based, 0=Model-Free)
    """
    lr_pos, lr_neg, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial] # 0 or 1

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updating ---
        # Stage 1 Update (TD(0))
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        alpha_1 = lr_pos if delta_stage1 > 0 else lr_neg
        q_stage1_mf[action_1[trial]] += alpha_1 * delta_stage1
        
        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        alpha_2 = lr_pos if delta_stage2 > 0 else lr_neg
        q_stage2_mf[state_idx, action_2[trial]] += alpha_2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Eligibility Traces (TD(lambda) with pure Model-Free)
Instead of a hybrid model-based/model-free architecture, this model tests a pure Model-Free approach but with **Eligibility Traces** ($\lambda$). This allows the reward at the second stage to directly influence the value of the first-stage choice, bridging the gap between the two steps without explicit model-based planning.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free Model with Eligibility Traces (TD-Lambda).
    
    Uses an eligibility trace to update Stage 1 values based on the final reward,
    skipping the intermediate step's value. This allows long-range temporal credit assignment.
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    lambd: [0, 1] (Decay rate of eligibility trace)
    """
    learning_rate, beta, lambd = model_parameters
    n_trials = len(action_1)
  
    # No transition matrix needed for pure MF
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        # Pure Model-Free choice at stage 1
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updating with Eligibility Trace ---
        # 1. Prediction error at stage 1 (transition from Stage 1 to Stage 2)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # 2. Prediction error at stage 2 (Reward received)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # 3. Direct update of Stage 1 Q-value by Stage 2 RPE (Eligibility Trace)
        # The trace for the chosen stage 1 action is 1, decayed by lambda
        q_stage1_mf[action_1[trial]] += learning_rate * lambd * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Inverse Temperature Scaling (Exploration/Exploitation Trade-off)
This model introduces a separate inverse temperature parameter ($\beta$) for the second stage compared to the first stage. The logic is that Stage 1 involves planning and uncertainty about transitions, while Stage 2 is a simpler bandit task. Participants often exhibit different levels of exploration (randomness) in these two distinct contexts.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Distinct Stage Temperatures.
    
    Uses separate inverse temperature (beta) parameters for Stage 1 and Stage 2 choices.
    This accounts for potentially different exploration levels in the planning phase (Stage 1)
    versus the bandit phase (Stage 2).
    
    Bounds:
    learning_rate: [0, 1]
    beta_stage1: [0, 10] (Inverse temp for spaceship choice)
    beta_stage2: [0, 10] (Inverse temp for alien choice)
    w: [0, 1]
    """
    learning_rate, beta_stage1, beta_stage2, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Use beta_stage1
        exp_q1 = np.exp(beta_stage1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        # Use beta_stage2
        exp_q2 = np.exp(beta_stage2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updating ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```