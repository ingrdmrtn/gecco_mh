Here are three new cognitive models exploring different mechanisms for learning and decision-making in the two-step task. These models introduce variations in how model-based and model-free values are combined, how learning rates might differ by stage, and how rewards are processed.

### Model 1: Hybrid Model with Separate Learning Rates
This model implements the classic hybrid architecture (combining Model-Based and Model-Free reinforcement learning) but allows for distinct learning rates for the first-stage and second-stage updates. This captures the possibility that the participant learns about spaceship transitions and alien rewards at different speeds.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Learner with Separate Learning Rates.
    
    This model combines model-based planning (using a fixed transition matrix)
    with model-free TD learning. Crucially, it uses two different learning rates:
    one for the spaceship choice (stage 1) and one for the alien choice (stage 2).
    
    Parameters:
    alpha1: [0, 1] Learning rate for stage 1 (spaceship) updates.
    alpha2: [0, 1] Learning rate for stage 2 (alien) updates.
    beta: [0, 10] Inverse temperature for softmax choice.
    w: [0, 1] Weighting parameter (0 = pure Model-Free, 1 = pure Model-Based).
    """
    alpha1, alpha2, beta, w = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition structure as per task description
    # A (0) -> X (0) mostly, U (1) -> Y (1) mostly.
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Decision ---
        # Model-Based Value: Transition * Max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Value: w * MB + (1-w) * MF
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial] # Planet arrived at

        # --- Stage 2 Decision ---
        # Only Model-Free learning applies at the second stage
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        
        # Stage 1 Update (TD(0)): Driven by difference between Q_stage2 and Q_stage1
        # Uses alpha1
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += alpha1 * delta_stage1
        
        # Stage 2 Update: Driven by reward
        # Uses alpha2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += alpha2 * delta_stage2
        
        # Note: In this specific formulation (TD(0)), the stage 2 reward does not 
        # immediately propagate back to stage 1 Q-values via eligibility traces, 
        # maintaining the distinct separation of learning speeds.

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Pure Model-Free with Decay
This model relies entirely on Model-Free learning (TD(1) logic with lambda=1 implicitly) but introduces a *decay* parameter. This decay causes unchosen options to slowly regress toward zero (or forget their value), simulating memory limitations or a "use it or lose it" heuristic often seen in human reinforcement learning.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free Learner with Value Decay.
    
    This model assumes no knowledge of the transition structure (pure MF).
    However, it includes a decay parameter. On every trial, the Q-values of 
    unchosen actions decay towards 0. This simulates forgetting or 
    passive value erosion.
    
    Parameters:
    learning_rate: [0, 1] Update rate for chosen actions.
    beta: [0, 10] Inverse temperature.
    decay: [0, 1] Decay rate for unchosen actions (1 = no decay, 0 = instant forgetting).
    """
    learning_rate, beta, decay = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Decision ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning and Decay ---
        
        # 1. Decay all values first
        q_stage1_mf *= decay
        q_stage2_mf *= decay
        
        # 2. Update Chosen Stage 1 Value (TD(1) logic: direct reward update)
        # We update Q1 directly from the final reward, bypassing the stage 2 value diff
        # This is a common simplification in pure MF analysis of this task.
        q_stage1_mf[action_1[trial]] += learning_rate * (reward[trial] - q_stage1_mf[action_1[trial]])
        
        # 3. Update Chosen Stage 2 Value
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * (reward[trial] - q_stage2_mf[state_idx, action_2[trial]])
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model with Reward Magnitude Sensitivity (Risk)
This model is a hybrid learner that treats the reward not just as a scalar to be maximized, but applies a transformation via a power-law utility function. This allows the model to capture risk-seeking (convex) or risk-averse (concave) tendencies regarding the reward value, which modulates the impact of the prediction error.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Subjective Utility (Reward Sensitivity).
    
    Standard Hybrid learner, but the reward is transformed by a utility 
    parameter 'rho' before updating values. This captures non-linear 
    sensitivity to rewards (e.g., diminishing returns or risk sensitivity).
    
    Parameters:
    learning_rate: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Weighting (0=MF, 1=MB).
    rho: [0, 1] Reward sensitivity exponent. Utility = reward^rho.
    """
    learning_rate, beta, w, rho = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Value Updating ---
        
        # Transform reward into subjective utility
        # Since reward is binary 0 or 1, reward^rho is trivial (0 or 1)
        # UNLESS the input rewards are sums or magnitudes > 1. 
        # Assuming typical implementation where reward might be scaled or continuous in variations,
        # or simply to capture sensitivity if reward was e.g. 0.5 vs 1.0.
        # For strict binary 0/1, this parameter might act as a scaling factor on the positive outcome.
        # To make it robust for this specific prompt: we treat reward as magnitude.
        subjective_reward = reward[trial] ** rho
        
        # TD(0) update for Stage 1
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Update Stage 2 using Subjective Utility
        delta_stage2 = subjective_reward - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```