Here are three new cognitive models for the two-step task. These models explore different mechanisms such as hybrid Model-Based/Model-Free reinforcement learning, separate learning rates for the two stages, and asymmetric learning from positive vs. negative prediction errors.

### Model 1: Hybrid Model-Based / Model-Free Learner
This model implements the classic "Daw et al. (2011)" hybrid architecture. It assumes the participant's Stage 1 choice is a weighted mixture of a Model-Free (habitual) value and a Model-Based (goal-directed) value. The weighting is controlled by a parameter `w`.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Learner.
    
    Combines Model-Free (TD) learning with Model-Based (planning) values at Stage 1.
    The parameter 'w' controls the balance: w=0 is pure MF, w=1 is pure MB.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate for both stages.
    beta: [0, 10] - Inverse temperature for softmax.
    w: [0, 1] - Weighting parameter (0 = Model-Free, 1 = Model-Based).
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2) # Model-free Q-values for Stage 1
    q_stage2_mf = np.zeros((2, 2)) # Q-values for Stage 2 (aliens)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based Value Calculation: V(state) = max(Q_stage2(state, :))
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        # Q_MB(a1) = T(s|a1) * V(s)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value: Q_net = w * Q_MB + (1-w) * Q_MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        if action_1[trial] != -1:
            p_choice_1[trial] = probs_1[action_1[trial]]
        else:
            p_choice_1[trial] = 1.0

        state_idx = state[trial]

        # Handle missing data
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
            continue

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Value Updating ---
        # TD(1) update for Stage 1 (Model-Free)
        # Note: In pure MF, Q(s1,a1) is updated by the Q(s2,a2) actually chosen.
        # Often simplified to TD(0) or TD(lambda). Here we use a standard TD update structure.
        
        # Prediction Error 1: Value of state 2 - Value of action 1
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Prediction Error 2: Reward - Value of action 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Eligibility trace / TD(1) logic implies the stage 2 RPE also updates stage 1
        # For this specific implementation, we add the stage 2 RPE to stage 1 as well (TD(1)-like)
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage2 

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Dual Learning Rate Model (Stage-Specific Alphas)
This model hypothesizes that the participant learns at different speeds for the first-stage decision (spaceships) versus the second-stage decision (aliens). This captures the possibility that learning the structure of the transition (or the value of the spaceship) happens at a different rate than learning the fluctuating reward probabilities of the aliens.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Dual Learning Rate Model.
    
    Uses separate learning rates for Stage 1 (spaceships) and Stage 2 (aliens).
    This allows the model to capture different volatilities or learning speeds
    at different levels of the task hierarchy.
    
    Parameters:
    alpha_s1: [0, 1] - Learning rate for Stage 1 updates.
    alpha_s2: [0, 1] - Learning rate for Stage 2 updates.
    beta: [0, 10] - Inverse temperature for softmax.
    """
    alpha_s1, alpha_s2, beta = model_parameters
    n_trials = len(action_1)
  
    # Pure Model-Free setup for simplicity to isolate the effect of dual alphas
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        if action_1[trial] != -1:
            p_choice_1[trial] = probs_1[action_1[trial]]
        else:
            p_choice_1[trial] = 1.0
            
        state_idx = state[trial]

        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
            continue

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Value Updating ---
        
        # Stage 1 Update: Driven by the difference between Stage 2 Value and Stage 1 Value
        # Uses alpha_s1
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += alpha_s1 * delta_stage1
        
        # Stage 2 Update: Driven by Reward
        # Uses alpha_s2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += alpha_s2 * delta_stage2
        
        # Second stage RPE also updates first stage (TD-lambda logic where lambda=1)
        # We use alpha_s1 for this backward propagation
        q_stage1_mf[action_1[trial]] += alpha_s1 * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Asymmetric Learning Rate (Positive vs. Negative RPE)
This model investigates whether the participant learns differently from "good news" (positive prediction errors) versus "bad news" (negative prediction errors). This is often relevant in psychiatric populations where sensitivity to reward versus punishment may be unbalanced.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Asymmetric Learning Rate Model (Pos/Neg RPE).
    
    Applies different learning rates depending on whether the prediction error 
    (RPE) is positive (better than expected) or negative (worse than expected).
    
    Parameters:
    alpha_pos: [0, 1] - Learning rate for positive prediction errors.
    alpha_neg: [0, 1] - Learning rate for negative prediction errors.
    beta: [0, 10] - Inverse temperature for softmax.
    """
    alpha_pos, alpha_neg, beta = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        if action_1[trial] != -1:
            p_choice_1[trial] = probs_1[action_1[trial]]
        else:
            p_choice_1[trial] = 1.0
            
        state_idx = state[trial]

        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
            continue

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Value Updating ---
        
        # Calculate RPEs first
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Update Stage 1 based on transition RPE
        lr_s1 = alpha_pos if delta_stage1 > 0 else alpha_neg
        q_stage1_mf[action_1[trial]] += lr_s1 * delta_stage1
        
        # Update Stage 2 based on reward RPE
        lr_s2 = alpha_pos if delta_stage2 > 0 else alpha_neg
        q_stage2_mf[state_idx, action_2[trial]] += lr_s2 * delta_stage2
        
        # Update Stage 1 based on reward RPE (TD(1) logic)
        # We use the sign of the Stage 2 RPE to determine the learning rate here
        lr_s1_direct = alpha_pos if delta_stage2 > 0 else alpha_neg
        q_stage1_mf[action_1[trial]] += lr_s1_direct * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```