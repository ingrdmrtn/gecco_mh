Here are three new cognitive models exploring different mechanisms for learning and decision-making in the two-step task. These models introduce variations in how transition probabilities are learned, how habits persist, and how learning rates might differ based on model-based vs. model-free updates.

### Model 1: Dynamic Transition Learning (Model-Based with Adaptive Transitions)
This model relaxes the assumption that the participant perfectly knows the transition matrix (70/30). Instead, the participant learns the transition probabilities between Stage 1 (spaceships) and Stage 2 (planets) online. This is often more realistic, as participants must infer the structure of the task as they play.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Model-Based Learner with Dynamic Transition Learning.
    
    Instead of assuming a fixed 0.7/0.3 transition matrix, the agent learns
    the transition probabilities (alpha_trans) alongside value learning.
    
    Parameters:
    - lr_value: [0,1] Learning rate for Q-values (reward prediction errors).
    - lr_trans: [0,1] Learning rate for updating the transition matrix.
    - beta: [0,10] Inverse temperature for choice stochasticity.
    - w: [0,1] Weighting parameter (0=Pure MF, 1=Pure MB).
    """
    lr_value, lr_trans, beta, w = model_parameters
    n_trials = len(action_1)
  
    # Initialize transition counts or probabilities. 
    # We start with a uniform prior (0.5) to represent initial uncertainty,
    # or slight bias if instructed. Here we start neutral.
    # trans_prob[0, 0] is P(Planet 0 | Spaceship 0)
    trans_probs = np.array([[0.5, 0.5], [0.5, 0.5]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        
        # --- Stage 1 Policy ---
        # Model-Based Value: V(s') = max(Q(s', a'))
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        
        # Calculate MB values using the *learned* transition matrix
        q_stage1_mb = trans_probs @ max_q_stage2
        
        # Hybrid Value
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Policy ---
        state_idx = state[trial] # The planet actually reached
        
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        
        # 1. Update Transition Matrix (State Prediction Error)
        # Create a one-hot vector for the state we arrived at (0 or 1)
        outcome_vector = np.zeros(2)
        outcome_vector[state_idx] = 1.0
        
        # Update the row corresponding to the chosen spaceship (action_1)
        # Move the probability distribution towards the observed outcome
        trans_probs[action_1[trial]] += lr_trans * (outcome_vector - trans_probs[action_1[trial]])
        
        # 2. Update Action Values
        # Stage 1 MF update (TD(0))
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr_value * delta_stage1
        
        # Stage 2 MF update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr_value * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Choice Perseveration (Stickiness) on Both Stages
This model hypothesizes that the participant has a tendency to repeat their previous actions regardless of reward (perseveration) or switch actions (alternation). While previous models might have included simple stickiness, this model applies distinct stickiness parameters to the spaceship choice (Stage 1) and the alien choice (Stage 2), recognizing that motor repetition might differ from high-level strategy repetition.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner with Stage-Specific Choice Perseveration (Stickiness).
    
    Includes parameters to capture the tendency to repeat the last choice
    at Stage 1 and Stage 2 independently of value.
    
    Parameters:
    - learning_rate: [0,1] Value update rate.
    - beta: [0,10] Inverse temperature.
    - w: [0,1] Model-based weight.
    - stick_1: [0,5] Choice stickiness for Stage 1 (Spaceships).
    - stick_2: [0,5] Choice stickiness for Stage 2 (Aliens).
    """
    learning_rate, beta, w, stick_1, stick_2 = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track last choice for stickiness (initialize to -1 so no stickiness on trial 0)
    last_choice_1 = -1
    last_choice_2 = np.array([-1, -1]) # Track last choice per state (planet)

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Add stickiness bonus
        stick_bonus_1 = np.zeros(2)
        if last_choice_1 != -1:
            stick_bonus_1[last_choice_1] = stick_1
            
        exp_q1 = np.exp(beta * q_net + stick_bonus_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        # Add stickiness bonus specific to the planet we are currently on
        stick_bonus_2 = np.zeros(2)
        if last_choice_2[state_idx] != -1:
            stick_bonus_2[last_choice_2[state_idx]] = stick_2

        exp_q2 = np.exp(beta * q_stage2_mf[state_idx] + stick_bonus_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        # Update trackers
        last_choice_1 = action_1[trial]
        last_choice_2[state_idx] = action_2[trial]

        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Asymmetric Learning Rates (Confirmational Bias)
This model investigates whether the participant updates their beliefs differently when the outcome is better than expected (positive prediction error) versus worse than expected (negative prediction error). This "confirmational bias" or asymmetric updating is common in reinforcement learning tasks.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner with Asymmetric Learning Rates (Positive/Negative).
    
    This model separates the learning rate into two components: one for
    positive prediction errors (better than expected) and one for negative
    prediction errors (worse than expected).
    
    Parameters:
    - lr_pos: [0,1] Learning rate for positive prediction errors (delta > 0).
    - lr_neg: [0,1] Learning rate for negative prediction errors (delta < 0).
    - beta: [0,10] Inverse temperature.
    - w: [0,1] Weighting parameter (0=Pure MF, 1=Pure MB).
    """
    lr_pos, lr_neg, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        if delta_stage1 > 0:
            q_stage1_mf[action_1[trial]] += lr_pos * delta_stage1
        else:
            q_stage1_mf[action_1[trial]] += lr_neg * delta_stage1
            
        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        if delta_stage2 > 0:
            q_stage2_mf[state_idx, action_2[trial]] += lr_pos * delta_stage2
        else:
            q_stage2_mf[state_idx, action_2[trial]] += lr_neg * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```