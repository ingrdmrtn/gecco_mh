Here are three new cognitive models for the two-step decision task. These models explore different mechanisms of reinforcement learning, specifically focusing on how eligibility traces, separate learning rates for different stages, and mixtures of model-based/model-free strategies with specific decay properties might explain the data.

### Model 1: TD(1) Model-Free Learner (Eligibility Trace)
This model implements a Model-Free learner using Temporal Difference learning with an eligibility trace (TD(1)). Unlike the standard TD(0) (SARSA) approach which only updates the first-stage value based on the second-stage value, a TD(1) agent updates the first-stage value directly based on the final reward received. This essentially collapses the two steps into one for the purpose of credit assignment.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    TD(1) Model-Free Learner (Direct Reinforcement).
    Instead of standard TD(0) learning where Stage 1 is updated by Stage 2's value,
    this model updates Stage 1 values directly based on the final reward (lambda=1 equivalent).
    This represents an agent who ignores the intermediate state structure for learning.

    Parameters:
    learning_rate: [0, 1] - Rate for updating Q-values.
    beta: [0, 10] - Inverse temperature for softmax.
    """
    learning_rate, beta = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]]) # Not used in pure MF, but kept for structure
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        # Standard Softmax on Model-Free Q-values
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Update Stage 2 Q-values (Standard Rescorla-Wagner)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Update Stage 1 Q-values directly from the Reward (TD(1) logic)
        # Instead of using the difference between Q2 and Q1, we use Reward - Q1
        delta_stage1_direct = reward[trial] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1_direct

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model with Independent Forget Rate
This model combines Model-Based and Model-Free learning (controlled by `w`), but adds a distinct "forgetting" parameter (`decay`). In standard RL, unchosen options usually stay static. Here, Q-values for unchosen options decay toward 0 (or a neutral value) on every trial. This captures the phenomenon where participants lose confidence in options they haven't explored recently.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Passive Decay (Forgetting).
    Combines Model-Based and Model-Free influences, but adds a decay parameter
    where all Q-values slowly revert to 0 on every trial, representing memory loss.

    Parameters:
    learning_rate: [0, 1] - Rate for updating Q-values.
    beta: [0, 10] - Inverse temperature for softmax.
    w: [0, 1] - Weighting parameter (0 = pure MF, 1 = pure MB).
    decay: [0, 1] - Rate at which Q-values decay to 0 (1 = no decay, 0 = instant forgetting).
    """
    learning_rate, beta, w, decay = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        # Calculate Model-Based values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrate MB and MF
        q_integrated = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_integrated)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Update Stage 1 (SARSA-style TD(0))
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Update Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Apply Decay to ALL Q-values (passive forgetting)
        q_stage1_mf *= decay
        q_stage2_mf *= decay

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Dual-Learning Rate Model-Free
This model posits that learning happens at different speeds for the first stage (spaceship choice) and the second stage (alien choice). It is a pure Model-Free learner but acknowledges that the cognitive load or salience of the immediate reward (Stage 2) might differ from the delayed predictive value of the spaceship (Stage 1). It uses `lr_stage1` and `lr_stage2`.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Dual-Learning Rate Model-Free Learner.
    Uses separate learning rates for the first stage (spaceship choice)
    and the second stage (alien choice), reflecting different plasticity 
    or attention at different levels of the decision hierarchy.

    Parameters:
    lr_stage1: [0, 1] - Learning rate for the first decision stage.
    lr_stage2: [0, 1] - Learning rate for the second decision stage.
    beta: [0, 10] - Inverse temperature for softmax.
    """
    lr_stage1, lr_stage2, beta = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Update Stage 1: Driven by Stage 2 value difference
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr_stage1 * delta_stage1
        
        # Update Stage 2: Driven by Reward difference
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr_stage2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```