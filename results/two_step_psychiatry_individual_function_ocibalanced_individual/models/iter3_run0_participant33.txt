Here are 3 new cognitive models for the two-step task. These models explore mechanisms like forgetting, eligibility traces, and distinct learning rates for the two stages, avoiding the exact parameter combinations previously attempted.

### Model 1: Hybrid Model with Forgetting (Decay)
This model introduces a forgetting parameter `decay`. In many reinforcement learning contexts, values don't just update based on experience; they also decay back to a baseline (usually 0 or 0.5) over time if not visited. This captures the idea that memory of specific reward probabilities fades.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Passive Forgetting (Decay).
    
    This model includes a decay parameter that causes unchosen Q-values
    to slowly revert to zero on every trial. This mimics memory degradation.

    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    w: [0, 1] (Mixing weight: 1=Model-Based, 0=Model-Free)
    decay: [0, 1] (Rate at which unchosen Q-values decay to 0)
    """
    learning_rate, beta, w, decay = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Policy for Choice 1 ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Policy for Choice 2 ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        # 1. Decay all Q-values towards 0 before updating the current ones
        q_stage1_mf *= (1 - decay)
        q_stage2_mf *= (1 - decay)

        # 2. Standard TD Updates
        # Stage 1 update (TD(0))
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1

        # Stage 2 update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Model-Free with Eligibility Traces (TD(lambda)) and Separate Stage Betas
Instead of a hybrid model-based/model-free architecture, this model is purely Model-Free but uses `lambd` (eligibility trace) to allow the reward at the second stage to directly influence the first-stage choice. It also uses separate inverse temperatures (`beta_stage1`, `beta_stage2`) for the two stages, acknowledging that the exploration-exploitation trade-off might differ between selecting a spaceship and selecting an alien.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free TD(lambda) with Separate Exploration parameters.
    
    Uses an eligibility trace (lambda) to connect stage 2 rewards to stage 1 choices,
    without an explicit transition model. It allows different distinct 
    randomness (beta) in choices at stage 1 vs stage 2.

    Bounds:
    learning_rate: [0, 1]
    lambd: [0, 1] (Eligibility trace decay)
    beta_stage1: [0, 10]
    beta_stage2: [0, 10]
    """
    learning_rate, lambd, beta_stage1, beta_stage2 = model_parameters
    n_trials = len(action_1)
  
    # No transition matrix used here (Pure MF)
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Policy for Choice 1 ---
        # Pure MF Q-values used
        exp_q1 = np.exp(beta_stage1 * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Policy for Choice 2 ---
        exp_q2 = np.exp(beta_stage2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        # Prediction error at stage 1 (driven by value of state 2)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Prediction error at stage 2 (driven by reward)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Eligibility trace update: 
        # The stage 1 choice also learns from the stage 2 prediction error, scaled by lambda
        q_stage1_mf[action_1[trial]] += learning_rate * lambd * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model with Choice Perseveration (Stickiness)
This model adds a "perseveration" or "stickiness" parameter (`pers`). This captures the tendency of participants to repeat their previous choice regardless of reward history (motor perseveration) or to switch frequently. This is applied to the first-stage choice.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Choice Perseveration (Stickiness).
    
    Adds a bias to repeat (or switch) the previous stage-1 action, 
    independent of value learning.

    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    w: [0, 1]
    pers: [-5, 5] (Perseveration bias; positive = repeat, negative = switch)
    """
    learning_rate, beta, w, pers = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track previous choice (initialize to -1 or handle first trial separately)
    last_choice_1 = -1

    for trial in range(n_trials):

        # --- Policy for Choice 1 ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add perseveration bonus to the logits
        logits = beta * q_net
        if last_choice_1 != -1:
            logits[last_choice_1] += pers
            
        exp_q1 = np.exp(logits)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Policy for Choice 2 ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1

        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Update history
        last_choice_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```