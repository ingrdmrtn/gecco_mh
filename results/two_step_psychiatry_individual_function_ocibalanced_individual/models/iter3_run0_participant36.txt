Here are three new cognitive models exploring different parameter configurations and psychological mechanisms not exactly covered in the previous list.

### Model 1: Reward-Dependent Learning Rates
This model hypothesizes that the participant updates their values differently depending on whether the outcome was positive (winning a coin) or negative (getting nothing). This asymmetry is often observed in clinical populations or specific personality traits (e.g., optimism/pessimism bias). This is distinct from separating Model-Based/Model-Free learning rates; it separates based on *outcome valence*.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Asymmetric Learning Rates Model (Valence-Dependent).
    Uses different learning rates for positive (reward=1) and negative/neutral (reward=0) outcomes.
    
    Parameters:
    alpha_pos: [0,1] - Learning rate for rewarded trials.
    alpha_neg: [0,1] - Learning rate for unrewarded trials.
    beta: [0,10] - Inverse temperature (softmax sensitivity).
    w: [0,1] - Weighting parameter (0 = pure Model-Free, 1 = pure Model-Based).
    """
    alpha_pos, alpha_neg, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid value calculation
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Determine Learning Rate based on Reward ---
        # Note: In this task, rewards are typically 0 or 1. 
        # If reward > 0, use alpha_pos, else alpha_neg.
        current_alpha = alpha_pos if reward[trial] > 0 else alpha_neg

        # --- Value Updating ---
        # Stage 1 MF update (using TD(1) style logic passing reward back, or simple SARSA style)
        # Standard Daw implementation usually passes the Stage 2 Q-value back for the Stage 1 update.
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += current_alpha * delta_stage1
        
        # Stage 2 MF update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += current_alpha * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Eligibility Trace (TD-Lambda)
This model introduces an eligibility trace parameter `lam` (lambda). Instead of strictly separating Model-Based and Model-Free systems via a weighting parameter `w`, this model assumes a pure Model-Free system but allows the reward at the second stage to directly influence the first stage choice via an eligibility trace. This tests if the behavior can be explained by efficient Model-Free learning without explicit Model-Based planning.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    TD(Lambda) Model.
    A pure Model-Free learner that uses eligibility traces to update Stage 1 values 
    based on the final reward, bridging the gap between stages more effectively than simple Q-learning.
    
    Parameters:
    learning_rate: [0,1] - Update rate.
    beta: [0,10] - Inverse temperature.
    lam: [0,1] - Eligibility trace decay parameter (0 = simple Q-learning, 1 = Monte Carlo).
    """
    learning_rate, beta, lam = model_parameters
    n_trials = len(action_1)
  
    # No transition matrix needed for pure MF
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        chosen_a1 = action_1[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        chosen_a2 = action_2[trial]

        # --- Value Updating with Eligibility Trace ---
        
        # Prediction error at stage 1 (based on value of state 2)
        delta_1 = q_stage2[state_idx, chosen_a2] - q_stage1[chosen_a1]
        
        # Prediction error at stage 2 (based on reward)
        delta_2 = reward[trial] - q_stage2[state_idx, chosen_a2]

        # Update Stage 1:
        # It gets updated by its own error, PLUS the error from stage 2 discounted by lambda
        q_stage1[chosen_a1] += learning_rate * delta_1 + learning_rate * lam * delta_2
        
        # Update Stage 2:
        q_stage2[state_idx, chosen_a2] += learning_rate * delta_2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Separate Stage Temperatures (Exploration Asymmetry)
This model posits that the degree of exploration/exploitation differs between the first stage (spaceship choice) and the second stage (alien choice). The first stage is more abstract (choosing a path), while the second stage is proximal to the reward. This model uses two distinct `beta` parameters.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Dual-Beta Hybrid Model.
    Uses separate inverse temperature parameters for Stage 1 and Stage 2 choices,
    allowing for different levels of stochasticity (exploration) at different hierarchical levels.
    
    Parameters:
    learning_rate: [0,1] - Update rate.
    beta_1: [0,10] - Inverse temperature for Stage 1 (Spaceship).
    beta_2: [0,10] - Inverse temperature for Stage 2 (Alien).
    w: [0,1] - MB/MF mixing weight.
    """
    learning_rate, beta_1, beta_2, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy (Uses beta_1) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf

        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy (Uses beta_2) ---
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Value Updating ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```