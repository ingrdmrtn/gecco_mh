Here are three new cognitive models based on the two-step decision-making task. These models explore different mechanisms such as hybrid model-free/model-based learning, asymmetric learning rates for first and second stages, and separate learning rates for rewarded vs. unrewarded outcomes.

### Model 1: Hybrid Model-Based / Model-Free (The "Daw" Standard)
This is the canonical hybrid model often used to analyze this task. It assumes behavior is a weighted mixture of Model-Based (planning using the transition matrix) and Model-Free (TD learning) strategies. The parameter `w` controls the balance between these two systems.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free RL.
    
    This model computes Stage 1 values as a weighted sum of Model-Based (MB)
    and Model-Free (MF) values. 
    MB values are derived from the transition matrix and Stage 2 max values.
    MF values are learned via TD errors.
    
    Bounds:
    learning_rate: [0, 1] - Learning rate for value updates.
    beta: [0, 10] - Inverse temperature for softmax choice.
    w: [0, 1] - Weighting parameter (0 = Pure MF, 1 = Pure MB).
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)      # Model-free values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for stage 2 (aliens)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # 1. Model-Based Value: Transition * Max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Hybrid Value: w * MB + (1-w) * MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax choice 1
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial] # 0 or 1 (Planets)

        # --- Stage 2 Choice ---
        # Standard softmax on stage 2 Q-values
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        # TD Error Stage 1: Q2(chosen) - Q1(chosen)
        # Note: In the standard hybrid model, Q1_MF is often updated by Q2 value, not just reward.
        # But here we stick to the template's structure where Q1 is updated by the transition.
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # TD Error Stage 2: Reward - Q2(chosen)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Note: In full implementations, there is often an eligibility trace passing delta_stage2 back to stage 1.
        # Here we use the simpler TD(0) for Stage 1 MF as implied by the template structure.

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Separate Learning Rates for Stage 1 and Stage 2
It is biologically plausible that the rate of learning about the spaceship transitions (or the value of the spaceships) differs from the rate of learning about the aliens (the reward probabilities). This model introduces `alpha1` for the first stage and `alpha2` for the second stage, focusing on a pure Model-Free architecture to isolate this effect.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free RL with Separate Stage Learning Rates.
    
    This model assumes a Model-Free strategy (TD learning) but allows for 
    different plasticity in the first stage (spaceships) versus the second 
    stage (aliens). This captures if the participant learns about structural 
    transitions/values at a different speed than immediate rewards.
    
    Bounds:
    alpha1: [0, 1] - Learning rate for Stage 1 (Spaceships).
    alpha2: [0, 1] - Learning rate for Stage 2 (Aliens).
    beta: [0, 10] - Inverse temperature.
    """
    alpha1, alpha2, beta = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Choice (Pure Model-Free) ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        # Update Stage 1 using alpha1
        # The target for Stage 1 is the value of the state we landed in (Q_stage2 of chosen action)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += alpha1 * delta_stage1
        
        # Update Stage 2 using alpha2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += alpha2 * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Outcome-Specific Learning Rates (Win/Loss)
This model investigates valence-dependent learning. Participants often update their beliefs differently after a "win" (reward = 1) versus a "loss" (reward = 0). This model applies `alpha_win` when a reward is received and `alpha_loss` when no reward is received, applied specifically to the second stage where the reward is delivered. It uses a Model-Based strategy for the first stage to contrast with previous MF approaches.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Based RL with Asymmetric Win/Loss Learning Rates.
    
    This model uses Model-Based planning for Stage 1 choices.
    However, for the Stage 2 value updates (learning about aliens), it uses
    separate learning rates for rewarded trials (alpha_win) and unrewarded 
    trials (alpha_loss).
    
    Bounds:
    alpha_win: [0, 1] - Learning rate when reward == 1.
    alpha_loss: [0, 1] - Learning rate when reward == 0.
    beta: [0, 10] - Inverse temperature.
    """
    alpha_win, alpha_loss, beta = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # We only need Q-values for stage 2 (aliens) because Stage 1 is derived MB
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Choice (Pure Model-Based) ---
        # Calculate expected value based on transition matrix and current max Q2 values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        # Select learning rate based on outcome
        if reward[trial] == 1:
            current_alpha = alpha_win
        else:
            current_alpha = alpha_loss

        # Standard Q-learning update for Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += current_alpha * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```