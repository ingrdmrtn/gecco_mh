Here are three new cognitive models exploring different mechanisms for learning and decision-making in the two-step task.

### Model 1: Hybrid Learner with Eligibility Traces (TD($\lambda$))
This model introduces an eligibility trace parameter `lambda_param`. Instead of just updating the first stage based on the second stage's value (TD(0)), this model allows the reward at the second stage to directly influence the first-stage update, bridging the gap between the two steps more effectively. This is a classic reinforcement learning approach to the credit assignment problem in multi-step tasks.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner with Eligibility Traces (TD(lambda)).
    Uses an eligibility trace to allow the second-stage reward to directly 
    update the first-stage Q-value, bridging the temporal gap.
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    lambda_param: [0, 1] - Eligibility trace decay parameter (0 = TD(0), 1 = Monte Carlo).
    w: [0, 1] - Mixing weight (0 = pure MF, 1 = pure MB).
    """
    learning_rate, beta, lambda_param, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_integrated = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_integrated)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # TD(0) error at stage 1 (driven by state transition)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        # TD error at stage 2 (driven by reward)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Update Stage 2 Q-values (standard Q-learning)
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Update Stage 1 Q-values: 
        # The update includes the immediate TD error plus a portion of the second stage error
        # scaled by lambda. This effectively passes the reward back to stage 1.
        q_stage1_mf[action_1[trial]] += learning_rate * (delta_stage1 + lambda_param * delta_stage2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Dual-Update Asymmetric Learning Rate Model
This model posits that learning rates might differ depending on whether the outcome was better or worse than expected (Positive vs Negative prediction errors). However, unlike previous attempts that might split this globally, this model applies the asymmetry specifically to the **second stage** (where the reward is received), as this is the primary source of valence.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Dual-Update Asymmetric Learning Rate Model.
    Differentiates learning rates for positive and negative prediction errors
    specifically at the second stage where rewards are received.
    
    Bounds:
    lr_pos: [0, 1] - Learning rate for positive prediction errors (RPE > 0).
    lr_neg: [0, 1] - Learning rate for negative prediction errors (RPE < 0).
    beta: [0, 10]
    w: [0, 1]
    """
    lr_pos, lr_neg, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_integrated = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_integrated)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Standard SARSA/Q-learning update for stage 1 using average of pos/neg LR
        # We use a simple average LR for stage 1 to keep parameter count low, 
        # focusing asymmetry on the reward stage.
        avg_lr = (lr_pos + lr_neg) / 2.0
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += avg_lr * delta_stage1
        
        # Asymmetric update for stage 2 based on reward prediction error
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        if delta_stage2 >= 0:
            q_stage2_mf[state_idx, action_2[trial]] += lr_pos * delta_stage2
        else:
            q_stage2_mf[state_idx, action_2[trial]] += lr_neg * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Inverse Temperature (Beta) Decay Model
This model explores the idea of exploration-exploitation dynamics changing over time. Instead of a static `beta` (inverse temperature), the agent starts with a specific exploration level and becomes more exploitative (higher beta) or more exploratory (lower beta) over time, governed by a decay rate. This captures fatigue or increasing confidence.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Inverse Temperature (Beta) Decay Model.
    The inverse temperature parameter evolves over trials, allowing the 
    randomness of choice to change (e.g., settling into a strategy).
    
    Bounds:
    learning_rate: [0, 1]
    beta_initial: [0, 10] - Initial inverse temperature.
    beta_decay: [0, 1] - Multiplier for beta per trial (1 = constant, <1 = increasing noise).
    w: [0, 1]
    """
    learning_rate, beta_initial, beta_decay, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    current_beta = beta_initial

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_integrated = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Use current_beta which changes every trial
        exp_q1 = np.exp(current_beta * q_integrated)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(current_beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Update beta for the next trial
        # If beta_decay < 1, beta decreases (more noise/exploration)
        # If we wanted beta to increase (more exploitation), we could use different logic,
        # but here we model a simple multiplicative decay/drift.
        current_beta = current_beta * beta_decay

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```