Here are three new cognitive models that explore different mechanisms for learning and decision-making in the two-step task. These models introduce variations in how prediction errors are handled, how exploration is managed, and how stage-specific values are updated.

### Model 1: Hybrid Model with Separate Learning Rates for Stage 1 and Stage 2
This model acknowledges that learning dynamics might differ between the abstract choice of a spaceship (Stage 1) and the concrete choice of an alien for gold (Stage 2). By decoupling the learning rates, the model can capture if a participant updates their spaceship preferences faster or slower than their alien preferences.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Separate Learning Rates.
    
    This model separates the learning process for Stage 1 (spaceship choice)
    and Stage 2 (alien choice). This allows the model to capture different
    sensitivities to prediction errors at different levels of the task hierarchy.

    Bounds:
    lr_1: [0, 1] (Learning rate for Stage 1 updates)
    lr_2: [0, 1] (Learning rate for Stage 2 updates)
    beta: [0, 10] (Inverse temperature)
    w: [0, 1] (Mixing weight: 1=Model-Based, 0=Model-Free)
    """
    lr_1, lr_2, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        # Stage 1 update uses lr_1. Note: standard TD(0) update for Stage 1 MF
        # uses the value of the state reached (max_q_stage2[state_idx]) 
        # or the value of the action taken in stage 2. Here we use SARSA-style
        # update based on the Q-value of the action actually taken in stage 2.
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr_1 * delta_stage1

        # Stage 2 update uses lr_2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr_2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model with Eligibility Traces (Lambda) and Asymmetric Beta
This model introduces an eligibility trace parameter (`lambd`) which modulates how much the Stage 2 reward prediction error impacts the Stage 1 choice directly. Additionally, it uses separate inverse temperatures (`beta_1`, `beta_2`) for the two stages, assuming that the level of stochasticity (exploration/exploitation balance) might differ between the spaceship choice and the alien choice.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Eligibility Trace (Lambda) and Asymmetric Beta.
    
    This model implements a TD(lambda) style update where the Stage 2 reward 
    prediction error propagates back to Stage 1 values scaled by lambda.
    It also allows for different noise levels (beta) at each stage.

    Bounds:
    learning_rate: [0, 1]
    beta_1: [0, 10] (Inverse temperature for Stage 1)
    beta_2: [0, 10] (Inverse temperature for Stage 2)
    lambd: [0, 1] (Eligibility trace decay / credit assignment to Stage 1)
    w: [0, 1] (Mixing weight)
    """
    learning_rate, beta_1, beta_2, lambd, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        # 1. Calculate prediction errors
        # PE for Stage 1 transition
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        # PE for Stage 2 reward
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]

        # 2. Update Stage 2 values
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # 3. Update Stage 1 values
        # Stage 1 gets its own PE plus a fraction (lambda) of the Stage 2 PE
        q_stage1_mf[action_1[trial]] += learning_rate * (delta_stage1 + lambd * delta_stage2)
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Pure Model-Free with "Choice Stickiness" (Perseveration)
This model removes the Model-Based component entirely (`w` is effectively 0) to test a simpler hypothesis. Instead, it adds a "stickiness" parameter (`phi`). This parameter captures a tendency to repeat the previous Stage 1 action regardless of reward history (motor perseveration), a common phenomenon in reinforcement learning tasks.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free Model with Choice Stickiness (Perseveration).
    
    This model assumes no model-based planning (no 'w' parameter). 
    Instead, it includes a 'phi' parameter that biases the agent 
    to repeat the same Stage 1 choice made in the previous trial, 
    independent of value.

    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    phi: [0, 5] (Perseveration bonus: positive values encourage repetition)
    """
    learning_rate, beta, phi = model_parameters
    n_trials = len(action_1)
  
    # No transition matrix needed for pure MF
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1 # Indicator for no previous action

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        # Add stickiness bonus to Q-values before softmax
        q_stage1_biased = q_stage1_mf.copy()
        if last_action_1 != -1:
            q_stage1_biased[last_action_1] += phi
        
        exp_q1 = np.exp(beta * q_stage1_biased)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        # Standard TD(0) updates
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1

        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Store action for next trial's stickiness
        last_action_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```