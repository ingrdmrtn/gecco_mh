Here are three new cognitive models based on the two-step task structure, exploring mechanisms distinct from the previous feedback list.

### Model 1: Hybrid RL with Separate Learning Rates for Stages
This model implements the classic hybrid reinforcement learning theory (combining Model-Based and Model-Free control via a mixing weight `w`), but it introduces separate learning rates for the first-stage (Model-Free) and second-stage updates. This captures the idea that learning dynamics might differ between the abstract choice of a spaceship and the concrete choice of an alien.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid RL with Separate Learning Rates.
    
    Combines Model-Based (MB) and Model-Free (MF) values using a weighting parameter 'w'.
    Crucially, it uses distinct learning rates for Stage 1 (MF) and Stage 2 updates.
    
    Bounds:
    alpha_1: [0, 1] - Learning rate for Stage 1 MF values.
    alpha_2: [0, 1] - Learning rate for Stage 2 values.
    beta: [0, 10] - Inverse temperature (softmax sensitivity).
    w: [0, 1] - Weighting parameter (0 = Pure MF, 1 = Pure MB).
    """
    alpha_1, alpha_2, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2) # MF values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Values for stage 2 (aliens)

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        # Calculate Model-Based values for Stage 1
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combine MB and MF values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # Update Stage 1 MF value using the value of the state actually reached (TD(0)-like logic for stage 1)
        # Note: In standard hybrid models, stage 1 MF is often updated via eligibility traces or direct TD error.
        # Here we use the value of the chosen stage 2 option as the target.
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += alpha_1 * delta_stage1
        
        # Update Stage 2 value based on reward
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += alpha_2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Model-Free RL with Outcome-Specific Learning Rates (Reward vs. No-Reward)
This model is a pure Model-Free learner (no transition matrix usage) but distinguishes how it learns from positive outcomes (finding gold) versus negative outcomes (no gold). This asymmetry is often observed in clinical populations or specific cognitive phenotypes where sensitivity to reward and punishment differs.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free RL with Asymmetric Learning Rates (Reward vs. No-Reward).
    
    This model ignores the transition structure (Pure MF) but updates value estimates
    differently depending on whether a reward was received or not.
    
    Bounds:
    alpha_rew: [0, 1] - Learning rate when reward is 1.
    alpha_norew: [0, 1] - Learning rate when reward is 0.
    beta: [0, 10] - Inverse temperature.
    lambda_decay: [0, 1] - Eligibility trace decay parameter connecting stage 2 reward to stage 1 choice.
    """
    alpha_rew, alpha_norew, beta, lambda_decay = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy (Pure MF) ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # Determine which learning rate to use based on the outcome
        current_alpha = alpha_rew if reward[trial] == 1 else alpha_norew

        # Stage 2 Prediction Error
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Update Stage 2
        q_stage2_mf[state_idx, action_2[trial]] += current_alpha * delta_stage2
        
        # Update Stage 1 using eligibility trace logic (TD(lambda))
        # The Stage 1 value is updated by the Stage 2 prediction error, scaled by lambda
        q_stage1_mf[action_1[trial]] += current_alpha * lambda_decay * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Model-Based RL with Forgetting (Value Decay)
This model assumes the participant uses a Model-Based strategy (calculating Stage 1 values via the transition matrix) but introduces a "forgetting" or decay parameter. In every trial, unchosen options in Stage 2 slowly decay back to a neutral value (0.5), representing memory limitations or uncertainty estimation over time.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Based RL with Value Decay (Forgetting).
    
    Uses the transition matrix to compute Stage 1 values.
    Includes a decay parameter that pulls the values of unchosen Stage 2 options
    towards a neutral prior (0.5) on every trial.
    
    Bounds:
    learning_rate: [0, 1] - Learning rate for chosen options.
    beta: [0, 10] - Inverse temperature.
    decay_rate: [0, 1] - Rate at which unchosen values decay to 0.5.
    """
    learning_rate, beta, decay_rate = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Initialize values at 0.5 (neutral expectation of probability)
    q_stage2_mf = np.full((2, 2), 0.5)

    for trial in range(n_trials):

        # --- Stage 1 Policy (Model-Based) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # 1. Decay all Stage 2 values towards 0.5
        q_stage2_mf = (1 - decay_rate) * q_stage2_mf + decay_rate * 0.5
        
        # 2. Update the specific chosen option based on reward
        # (We re-apply the update on top of the decay for the chosen one)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```