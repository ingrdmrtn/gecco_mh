Here are three new cognitive models exploring different parameter combinations and mechanisms (eligibility traces, separate learning rates for stages, and distinct inverse temperatures), avoiding the exact combinations listed in the feedback history.

### Model 1: Hybrid Learner with Eligibility Traces (`lambda`)
This model introduces an eligibility trace parameter (`lambda_eligibility`) into the hybrid Model-Based/Model-Free framework. Instead of just updating the first stage based on the second stage's value (TD(0)), the reward at the second stage also directly influences the first stage Q-values, scaled by lambda (TD($\lambda$)).

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner with Eligibility Traces (TD-lambda).

    This model combines Model-Based and Model-Free learning but adds an eligibility 
    trace parameter. This allows the reward received at the second stage to directly 
    update the first stage Q-values, bridging the temporal gap more effectively 
    than standard TD(0) learning.

    Parameters:
    learning_rate: [0, 1] - Rate at which Q-values are updated.
    beta: [0, 10] - Inverse temperature for softmax choice rule.
    w: [0, 1] - Weighting parameter (0 = Pure MF, 1 = Pure MB).
    lambda_eligibility: [0, 1] - Decay factor for eligibility traces (0=TD(0), 1=Monte Carlo).
    """
    learning_rate, beta, w, lambda_eligibility = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action

    for trial in range(n_trials):
        # 1. Stage 1 Choice Policy (Hybrid)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf

        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        # Handle missing data or invalid trials
        if action_1[trial] == -1:
            p_choice_1[trial] = 1.0
            continue # Skip update if action is missing
        else:
            p_choice_1[trial] = probs_1[action_1[trial]]

        state_idx = state[trial]
        if state_idx == -1 or action_2[trial] == -1:
            continue

        # 2. Stage 2 Choice Policy
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # 3. Learning / Updates
        # Stage 1 Prediction Error (driven by Stage 2 Q-value)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Stage 2 Prediction Error (driven by Reward)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Eligibility Trace Update: Apply stage 2 error to stage 1 choice
        q_stage1_mf[action_1[trial]] += learning_rate * lambda_eligibility * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Pure Model-Free with Separate Stage Learning Rates
This model drops the model-based component entirely to focus on whether the participant learns at different speeds for the two different stages of the task (e.g., learning about spaceship reliability vs. alien generosity). It uses distinct learning rates `alpha_s1` and `alpha_s2` combined with a stickiness parameter.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free Learner with Separate Stage Learning Rates and Stickiness.

    This model assumes the participant does not use a map of the task (Model-Free only).
    However, it posits that the rate of learning differs between the first decision 
    (spaceships) and the second decision (aliens). It also includes choice stickiness.

    Parameters:
    alpha_s1: [0, 1] - Learning rate for Stage 1 (spaceships).
    alpha_s2: [0, 1] - Learning rate for Stage 2 (aliens).
    beta: [0, 10] - Inverse temperature for softmax choice rule.
    stickiness: [0, 5] - Tendency to repeat the previous Stage 1 choice.
    """
    alpha_s1, alpha_s2, beta, stickiness = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) 
    
    last_action_1 = -1

    for trial in range(n_trials):

        # 1. Stage 1 Choice Policy (Pure MF + Stickiness)
        q_net_stage1 = q_stage1_mf.copy()
        if last_action_1 != -1:
            q_net_stage1[last_action_1] += stickiness

        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        if action_1[trial] != -1:
            p_choice_1[trial] = probs_1[action_1[trial]]
            last_action_1 = action_1[trial]
        else:
            p_choice_1[trial] = 1.0
            last_action_1 = -1
            continue

        state_idx = state[trial]
        if state_idx == -1 or action_2[trial] == -1:
            continue

        # 2. Stage 2 Choice Policy
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # 3. Learning
        # Update Stage 1 using alpha_s1
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += alpha_s1 * delta_stage1

        # Update Stage 2 using alpha_s2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += alpha_s2 * delta_stage2
        
        # Note: No eligibility trace here, strictly TD(0) logic for this variant

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Learner with Separate Inverse Temperatures
This model is a Hybrid learner (like Model 1) but assumes that the exploration/exploitation trade-off differs between the two stages. For example, a participant might be very deterministic in choosing a spaceship (`beta_1`) but more exploratory when choosing an alien (`beta_2`), or vice versa.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner with Separate Inverse Temperatures (Dual Beta).

    This model uses the hybrid MB/MF architecture but allows for different 
    levels of stochasticity (randomness) in choices at Stage 1 versus Stage 2.
    This captures the possibility that decision noise is not constant across
    the different contexts of the task.

    Parameters:
    learning_rate: [0, 1] - Rate at which MF Q-values are updated.
    beta_1: [0, 10] - Inverse temperature for Stage 1 choices.
    beta_2: [0, 10] - Inverse temperature for Stage 2 choices.
    w: [0, 1] - Weighting parameter (0 = Pure MF, 1 = Pure MB).
    """
    learning_rate, beta_1, beta_2, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) 

    for trial in range(n_trials):

        # 1. Stage 1 Choice Policy (Hybrid, using beta_1)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf

        exp_q1 = np.exp(beta_1 * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        if action_1[trial] != -1:
            p_choice_1[trial] = probs_1[action_1[trial]]
        else:
            p_choice_1[trial] = 1.0
            continue

        state_idx = state[trial]
        if state_idx == -1 or action_2[trial] == -1:
            continue

        # 2. Stage 2 Choice Policy (using beta_2)
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # 3. Learning
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1

        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```