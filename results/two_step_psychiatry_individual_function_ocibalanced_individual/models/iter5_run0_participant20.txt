Here are three new cognitive models that explore different mechanisms than those previously tested, focusing on how rewards are processed and how model-based valuations are constructed.

### Model 1: Hybrid Learner with Reward Magnitude Scaling
This model introduces a `reward_scaling` parameter. Standard reinforcement learning assumes rewards are processed objectively (e.g., 0 or 1). However, participants might subjectively scale the utility of the reward (e.g., treating a win as much more significant than the math implies, or dampening it). This scaling affects the prediction error and subsequent Q-value updates.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner with Reward Magnitude Scaling.
    
    This model posits that the subjective utility of the reward is scaled by a parameter,
    affecting how strongly the agent learns from positive outcomes compared to the 
    standard numerical value.
    
    Parameters:
    - learning_rate: [0,1] Learning rate for value updates.
    - beta: [0,10] Inverse temperature for softmax choice.
    - w: [0,1] Weighting parameter (0=Pure MF, 1=Pure MB).
    - reward_scale: [0,5] Multiplier for the reward value (subjective utility).
    """
    learning_rate, beta, w, reward_scale = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid value construction
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning Updates ---
        # Scale the reward before calculating prediction error
        scaled_reward = reward[trial] * reward_scale
        
        # TD(1) style update for stage 1 based on stage 2 value
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Update stage 2 based on scaled reward
        delta_stage2 = scaled_reward - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Learner with "Forgetful" Q-values (Passive Decay)
This model introduces a `decay_rate` parameter that applies to *unchosen* options. In standard RL, unchosen options retain their value indefinitely until chosen again. This model assumes that memory traces for unvisited states or unchosen actions passively decay toward zero (or a baseline) over time, representing a forgetting process or a belief that the environment is volatile.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner with Passive Decay for Unchosen Options.
    
    Q-values for actions NOT taken decay towards 0 at each step. This captures
    forgetting or an assumption of environmental volatility.
    
    Parameters:
    - learning_rate: [0,1] Update rate for chosen actions.
    - beta: [0,10] Inverse temperature.
    - w: [0,1] Weighting parameter (0=Pure MF, 1=Pure MB).
    - decay: [0,1] Rate at which unchosen Q-values decay (1 = no decay, 0 = instant forgetting).
    """
    learning_rate, beta, w, decay = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning Updates ---
        # 1. Decay all Q-values first (passive forgetting)
        q_stage1_mf *= decay
        q_stage2_mf *= decay
        
        # 2. Standard Delta Updates (applied on top of decayed values)
        # Note: We re-add the decay loss to the chosen one effectively by just updating normally, 
        # or we treat the update as correcting the decayed value. Here we update the current value.
        
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Learner with Choice Perseveration (Stickiness) on Stage 1 Only
While global stickiness has been tried, this model isolates perseveration (`stick_1`) specifically to the first-stage choice (Spaceships). It hypothesizes that motor repetition or habit is stronger at the initial decision point (which is more abstract) than at the second stage (which is directly rewarded). It separates the "habit" of repeating a choice from the "learning" of value.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner with Stage 1 Choice Perseveration (Stickiness).
    
    Adds a 'stickiness' bonus to the Q-value of the spaceship chosen in the 
    previous trial, independent of reward history.
    
    Parameters:
    - learning_rate: [0,1] Learning rate.
    - beta: [0,10] Inverse temperature.
    - w: [0,1] Weighting parameter (0=Pure MF, 1=Pure MB).
    - stick_1: [0,5] Bonus added to the previously chosen stage 1 action.
    """
    learning_rate, beta, w, stick_1 = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track previous choice for stickiness (initialize as -1 or None logic)
    last_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Apply stickiness bonus to q_net temporarily for probability calculation
        q_net_sticky = q_net.copy()
        if last_action_1 != -1:
            q_net_sticky[last_action_1] += stick_1
        
        exp_q1 = np.exp(beta * q_net_sticky)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Update last action
        last_action_1 = action_1[trial]
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```