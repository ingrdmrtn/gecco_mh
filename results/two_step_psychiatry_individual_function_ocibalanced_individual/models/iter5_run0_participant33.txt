Here are three new cognitive models that explore different mechanisms and parameter combinations not yet fully tested in the provided history.

### Model 1: Hybrid Model with Eligibility Traces (TD($\lambda$)) and Separate Learning Rates
This model implements a classic TD($\lambda$) approach where the update of the first-stage value depends on the second-stage reward, mediated by an eligibility trace parameter $\lambda$. Crucially, it uses separate learning rates for the first and second stages, allowing the model to capture differences in plasticity between the abstract choice (spaceship) and the concrete choice (aliens).

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Eligibility Traces (TD-Lambda) and Separate Stage Learning Rates.
    
    This model uses separate learning rates for stage 1 and stage 2 updates.
    It also implements an eligibility trace (lambda) which allows the stage 2 reward
    to directly influence stage 1 values, bridging the gap between Model-Free and Model-Based logic.

    Bounds:
    lr_1: [0, 1] (Learning rate for stage 1)
    lr_2: [0, 1] (Learning rate for stage 2)
    beta: [0, 10] (Inverse temperature)
    lambd: [0, 1] (Eligibility trace decay parameter)
    w: [0, 1] (Mixing weight: 1=Model-Based, 0=Model-Free)
    """
    lr_1, lr_2, beta, lambd, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Value Updating ---
        # Prediction error at stage 1 (driven by value of state 2)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr_1 * delta_stage1

        # Prediction error at stage 2 (driven by reward)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr_2 * delta_stage2
        
        # Eligibility trace update: Stage 2 RPE also updates Stage 1 Q-value
        q_stage1_mf[action_1[trial]] += lr_1 * lambd * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Pure Model-Based with Perseveration and Separate Betas
This model assumes the participant is purely Model-Based ($w=1$) regarding the transition structure but relies on Q-learning for the second stage values. It removes the Model-Free Q-values for stage 1 entirely. Instead, it adds a "perseveration" parameter (stickiness to the previous choice) and allows for different exploration levels (betas) at the two stages.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Based with Perseveration and Stage-Specific Betas.
    
    This model assumes the agent always uses the transition matrix (w=1 fixed).
    It includes a perseveration bonus, where repeating the previous Stage 1 choice 
    is inherently favored. It also allows for different noise levels (beta) at each stage.

    Bounds:
    learning_rate: [0, 1]
    beta_1: [0, 10] (Inverse temperature for Stage 1)
    beta_2: [0, 10] (Inverse temperature for Stage 2)
    pers: [0, 1] (Perseveration bonus for repeating the last stage 1 choice)
    """
    learning_rate, beta_1, beta_2, pers = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Only need stage 2 Q-values, stage 1 is derived purely from MB calculation
    q_stage2_mf = np.zeros((2, 2))
    
    last_choice_1 = -1

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Add perseveration bonus
        logits_1 = beta_1 * q_stage1_mb
        if last_choice_1 != -1:
            logits_1[last_choice_1] += pers
            
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        last_choice_1 = action_1[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Value Updating ---
        # Standard Q-learning for the second stage
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Asymmetric Learning Rate (Positive/Negative) Model
This model hypothesizes that the participant learns differently from gains (positive prediction errors) versus losses (negative prediction errors). This is often observed in clinical populations. It applies this asymmetry to the second-stage updates, which propagate back to the first stage via a simple Model-Free mechanism (no mixing weight $w$, purely MF).

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free with Asymmetric Learning Rates (Pos/Neg).
    
    This model focuses on valence-dependent learning. It splits the learning rate
    into alpha_pos (for RPE > 0) and alpha_neg (for RPE < 0).
    It assumes a purely Model-Free strategy (w=0).

    Bounds:
    lr_pos: [0, 1] (Learning rate for positive prediction errors)
    lr_neg: [0, 1] (Learning rate for negative prediction errors)
    beta: [0, 10]
    lambd: [0, 1] (Eligibility trace)
    """
    lr_pos, lr_neg, beta, lambd = model_parameters
    n_trials = len(action_1)
  
    # No transition matrix needed for pure MF
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Value Updating ---
        
        # Stage 1 Update (TD(0) part)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        lr_s1 = lr_pos if delta_stage1 > 0 else lr_neg
        q_stage1_mf[action_1[trial]] += lr_s1 * delta_stage1

        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        lr_s2 = lr_pos if delta_stage2 > 0 else lr_neg
        q_stage2_mf[state_idx, action_2[trial]] += lr_s2 * delta_stage2
        
        # Stage 1 Update (TD(lambda) part)
        # We use the same asymmetry for the eligibility trace update
        q_stage1_mf[action_1[trial]] += lr_s2 * lambd * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```