Here are three new cognitive models exploring different mechanisms for learning and decision-making in the two-step task. These models introduce variations in how rewards are processed (magnitude vs. frequency), how learning rates might differ for the two stages, and how memory decay might affect value estimation.

### Model 1: Hybrid Learner with Separate Learning Rates for Stages
This model hypothesizes that the participant might update their expectations about the "spaceships" (Stage 1) at a different speed than they update their expectations about the "aliens" (Stage 2). For example, the values of aliens (which change slowly) might need a higher learning rate than the values of spaceships (which are abstract choices leading to states). It combines Model-Free (MF) and Model-Based (MB) control via a weighting parameter `w`.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner with Separate Stage Learning Rates.
    
    This model distinguishes between the plasticity of Stage 1 (abstract choices)
    and Stage 2 (concrete rewards). It uses a hybrid Model-Based/Model-Free 
    valuation for the first stage.

    Bounds:
    alpha1: [0, 1] - Learning rate for Stage 1 (Q_MF1).
    alpha2: [0, 1] - Learning rate for Stage 2 (Q_MF2).
    beta: [0, 10] - Inverse temperature for softmax.
    w: [0, 1] - Weighting parameter (0 = Pure MF, 1 = Pure MB).
    """
    alpha1, alpha2, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        # Model-Based Value: Transition * Max(Stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value: w * MB + (1-w) * MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        # Stage 1 Update (TD(0) style for MF component)
        # We use the value of the state we actually arrived at (max_q_stage2[state_idx]) 
        # as the target for the first stage MF update.
        delta_stage1 = max_q_stage2[state_idx] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += alpha1 * delta_stage1
        
        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += alpha2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Pure Model-Free with Forgetful Q-Learning
This model assumes the participant is purely Model-Free (ignores transition probabilities) but has a "leaky" memory. In standard Q-learning, unchosen options retain their values indefinitely. Here, unchosen options decay toward a neutral value (0.5), simulating forgetting or uncertainty growth over time. This captures the idea that if you haven't visited an alien in a while, your estimate of its gold becomes less certain.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free with Forgetful Q-Learning (Decay).
    
    Unchosen actions in Stage 2 decay toward a neutral prior (0.5) 
    rather than staying static. This simulates memory loss or 
    uncertainty over time.

    Bounds:
    learning_rate: [0, 1] - Standard learning rate for chosen actions.
    decay_rate: [0, 1] - Rate at which unchosen values decay to 0.5.
    beta: [0, 10] - Inverse temperature.
    """
    learning_rate, decay_rate, beta = model_parameters
    n_trials = len(action_1)
  
    # No transition matrix needed for Pure MF
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2) + 0.5 # Initialize at neutral
    q_stage2_mf = np.zeros((2, 2)) + 0.5 # Initialize at neutral

    for trial in range(n_trials):

        # --- Stage 1 Policy (Pure MF) ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        
        # Stage 1 Update (SARSA-like logic: using the value of the state reached)
        # The value of the first stage choice is updated by the value of the *chosen* second stage option
        chosen_q2_val = q_stage2_mf[state_idx, action_2[trial]]
        delta_stage1 = chosen_q2_val - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Stage 2 Update (Chosen)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Stage 2 Decay (Unchosen)
        # For the current state, decay the unchosen action
        unchosen_action_2 = 1 - action_2[trial]
        q_stage2_mf[state_idx, unchosen_action_2] += decay_rate * (0.5 - q_stage2_mf[state_idx, unchosen_action_2])
        
        # Also decay both options in the unvisited state
        unvisited_state = 1 - state_idx
        q_stage2_mf[unvisited_state, :] += decay_rate * (0.5 - q_stage2_mf[unvisited_state, :])
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Model-Based with "Win-Stay, Lose-Shift" Heuristic
This model posits that while the participant uses a sophisticated Model-Based plan for the first stage (calculating expected values based on transitions), their second-stage behavior is driven by a simpler heuristic: "Win-Stay, Lose-Shift" (WSLS). Instead of standard Q-learning for the second stage, the probability of repeating the last choice depends directly on the last outcome, modulated by a `stay_prob` parameter.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Based Stage 1 with Win-Stay Lose-Shift Stage 2.
    
    Stage 1 is calculated using Model-Based values derived from simple Q-learning
    estimates of the terminal states.
    Stage 2 is governed by a heuristic stickiness to the previous choice 
    conditional on reward (Win-Stay, Lose-Shift).

    Bounds:
    learning_rate: [0, 1] - Rate for tracking average value of stage 2 options (for MB calculation).
    beta: [0, 10] - Inverse temperature for Stage 1 choice.
    wsls_sensitivity: [0, 5] - How strongly the previous reward affects repetition probability in Stage 2.
    """
    learning_rate, beta, wsls_sensitivity = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # We still need to track values to inform the MB controller for Stage 1
    q_stage2_mf = np.zeros((2, 2)) 
    
    # Track previous choice/reward for WSLS
    last_action_2_per_state = [-1, -1] # For state 0 and state 1
    last_reward_per_state = [0, 0]

    for trial in range(n_trials):

        # --- Stage 1 Policy (Model-Based) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy (WSLS Heuristic) ---
        # Base logits (0)
        logits_2 = np.zeros(2)
        
        prev_a = last_action_2_per_state[state_idx]
        prev_r = last_reward_per_state[state_idx]
        
        if prev_a != -1:
            # If we won last time, add positive bonus to same action.
            # If we lost last time, subtract bonus (or add negative) to same action.
            # We map reward 0/1 to -1/+1 for the shift direction.
            direction = 1 if prev_r == 1 else -1
            logits_2[prev_a] += direction * wsls_sensitivity
            
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        # We update Q-values solely to feed the Model-Based controller in Stage 1
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Update history for WSLS
        last_action_2_per_state[state_idx] = action_2[trial]
        last_reward_per_state[state_idx] = reward[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```