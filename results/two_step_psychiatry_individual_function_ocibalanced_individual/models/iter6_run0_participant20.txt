Here are three new cognitive models exploring different parameter configurations and mechanisms for the two-step task.

### Model 1: Hybrid Learner with Separate Learning Rates for Stages
This model tests the hypothesis that the participant learns at different speeds for the high-level planning stage (choosing a spaceship) versus the low-level reward stage (choosing an alien). This is distinct from previous models that used split learning rates for positive/negative prediction errors or for value/transition learning.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner with Stage-Specific Learning Rates.
    
    This model posits that learning rates differ between the first stage (spaceship choice)
    and the second stage (alien choice), reflecting potentially different neural mechanisms
    or timescales for updating values at different levels of the task hierarchy.
    
    Bounds:
    - learning_rate_1: [0,1] Learning rate for Stage 1 updates.
    - learning_rate_2: [0,1] Learning rate for Stage 2 updates.
    - beta: [0,10] Inverse temperature (softmax sensitivity).
    - w: [0,1] Weighting parameter (0=Pure MF, 1=Pure MB).
    """
    learning_rate_1, learning_rate_2, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Update Stage 1 values using learning_rate_1
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate_1 * delta_stage1
        
        # Update Stage 2 values using learning_rate_2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate_2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Learner with Choice Stickiness (Perseveration)
This model introduces a single stickiness parameter that affects both stages. Stickiness captures the tendency to repeat the previous choice regardless of reward history, which is a common behavioral artifact in sequential decision-making tasks. This differs from previous attempts that might have separated stickiness by stage or not included it with the hybrid `w` parameter.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner with Global Choice Stickiness.
    
    Incorporates a 'stickiness' parameter that biases the agent to repeat the 
    same action taken on the previous trial, independent of value learning.
    
    Bounds:
    - learning_rate: [0,1] Value update rate.
    - beta: [0,10] Inverse temperature.
    - w: [0,1] Weighting parameter (0=Pure MF, 1=Pure MB).
    - stickiness: [0,5] Bonus added to the Q-value of the previously chosen action.
    """
    learning_rate, beta, w, stickiness = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track previous actions (initialized to -1 or None, handled in loop)
    prev_action_1 = -1
    prev_action_2 = np.array([-1, -1]) # Track prev action for each state separately

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Add stickiness bonus
        q_net_stick = q_net.copy()
        if prev_action_1 != -1:
            q_net_stick[prev_action_1] += stickiness

        exp_q1 = np.exp(beta * q_net_stick)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # policy for the second choice
        q_stage2_stick = q_stage2_mf[state_idx].copy()
        # Stickiness usually applies to the specific state context
        if prev_action_2[state_idx] != -1:
             q_stage2_stick[prev_action_2[state_idx]] += stickiness

        exp_q2 = np.exp(beta * q_stage2_stick)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Action Value Updating
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Update history
        prev_action_1 = action_1[trial]
        prev_action_2[state_idx] = action_2[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Learner with Reward Sensitivity
This model replaces the standard value update with one that scales the reward by a sensitivity parameter `rho`. This allows the model to capture subjective valuation of the reward (e.g., is a coin worth "1" or "10" in utility terms?), which effectively scales the learning signal magnitude.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner with Reward Sensitivity.
    
    Introduces a reward sensitivity parameter 'rho' that scales the effective
    reward received. This separates the learning rate (how fast expectations change)
    from the reward magnitude (how much the outcome matters).
    
    Bounds:
    - learning_rate: [0,1] Value update rate.
    - beta: [0,10] Inverse temperature.
    - w: [0,1] Weighting parameter (0=Pure MF, 1=Pure MB).
    - rho: [0,10] Reward sensitivity scaler.
    """
    learning_rate, beta, w, rho = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Action Value Updating
        # Note: Stage 1 update uses the raw Q-value difference, as it is an internal transfer
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Stage 2 update uses the SCALED reward
        effective_reward = reward[trial] * rho
        delta_stage2 = effective_reward - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```