Here are three new cognitive models based on the two-step task structure, introducing mechanisms like eligibility traces, separate learning rates for different stages, and hybrid model-based/model-free reinforcement learning.

### Model 1: Hybrid Model with Eligibility Trace (Sarsa($\lambda$)-like)
This model implements a hybrid strategy combining Model-Based (MB) and Model-Free (MF) learning. Crucially, it uses an eligibility trace parameter (`lambda_eligibility`) for the Model-Free component. Instead of just updating the first stage based on the second stage's value (TD(0)), the first stage is also updated directly by the final reward, scaled by $\lambda$. This allows the final outcome to "drift back" to the initial choice more effectively.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner with Eligibility Trace (Sarsa-lambda style).
    
    This model combines Model-Based (MB) planning with Model-Free (MF) learning.
    The MF component uses an eligibility trace (lambda) to allow the second-stage 
    reward to directly influence the first-stage value update, bridging the gap 
    between the initial choice and the final outcome.

    Parameters:
    learning_rate: [0, 1] - Rate for updating Q-values.
    beta: [0, 10] - Inverse temperature for softmax choice rule.
    w: [0, 1] - Weighting parameter (0 = Pure MF, 1 = Pure MB).
    lambda_eligibility: [0, 1] - Eligibility trace decay parameter. 
                                 Controls how much credit the first stage gets 
                                 from the final reward.
    """
    learning_rate, beta, w, lambda_eligibility = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition matrix as per task description (70% common, 30% rare)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        
        # --- Stage 1 Policy ---
        # Model-Based Value Calculation
        # Bellman equation: Q_MB(s1, a1) = sum(P(s2|s1,a1) * max(Q(s2, a2)))
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combined Value: Q_net = w * Q_MB + (1-w) * Q_MF
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial] # The planet actually reached

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updating ---
        
        # 1. Prediction Error at Stage 1 (TD(0) error)
        # The difference between expected value of stage 1 and value of state reached in stage 2
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        # 2. Prediction Error at Stage 2
        # The difference between expected value of stage 2 and actual reward
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Update Stage 2 Q-values
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Update Stage 1 Q-values
        # Standard TD update from stage transition + Eligibility trace update from final reward
        # This effectively is: Q1 += alpha * (delta1 + lambda * delta2)
        q_stage1_mf[action_1[trial]] += learning_rate * (delta_stage1 + lambda_eligibility * delta_stage2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Stage-Separated Learning Rates
This model hypothesizes that learning happens at different speeds for the abstract choice (Spaceship A vs U) versus the concrete choice (Alien 1 vs 2). It uses a standard hybrid architecture but splits the `learning_rate` into `lr_stage1` and `lr_stage2`. This allows the model to capture participants who might rapidly learn which alien is good but are slow to update their spaceship preference (or vice versa).

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner with Stage-Separated Learning Rates.
    
    This model allows for different plasticity in the two stages of the task.
    It posits that the rate of value updating for the abstract first-stage choice 
    might differ from the concrete second-stage choice.

    Parameters:
    lr_stage1: [0, 1] - Learning rate for the first stage (Spaceships).
    lr_stage2: [0, 1] - Learning rate for the second stage (Aliens/Planets).
    beta: [0, 10] - Inverse temperature for softmax.
    w: [0, 1] - Weighting parameter (0 = Pure MF, 1 = Pure MB).
    """
    lr_stage1, lr_stage2, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updating ---
        
        # Calculate RPEs
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Update Stage 1 using its specific learning rate
        q_stage1_mf[action_1[trial]] += lr_stage1 * delta_stage1
        
        # Update Stage 2 using its specific learning rate
        q_stage2_mf[state_idx, action_2[trial]] += lr_stage2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model with Stickiness and Eligibility Trace
This model combines the most effective mechanisms identified in literature: hybrid MB/MF control, choice perseveration ("stickiness"), and eligibility traces. It is a comprehensive model that tests if the participant uses model-based planning (`w`), repeats previous actions regardless of reward (`perseveration`), and allows reward information to flow back to the start state via eligibility (`lambda_eligibility`).

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner with Stickiness and Eligibility Trace.
    
    Combines three key mechanisms:
    1. Hybrid Model-Based/Model-Free valuation (weighted by w).
    2. Choice Perseveration (stickiness) on the first stage.
    3. Eligibility Trace (lambda) for more efficient MF credit assignment.

    Parameters:
    learning_rate: [0, 1] - Rate for updating Q-values.
    beta: [0, 10] - Inverse temperature for softmax.
    w: [0, 1] - Weighting parameter (0 = Pure MF, 1 = Pure MB).
    perseveration: [0, 5] - Bias to repeat the previous stage 1 action.
    lambda_eligibility: [0, 1] - Decay factor for the eligibility trace.
    """
    learning_rate, beta, w, perseveration, lambda_eligibility = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add Stickiness
        if last_action_1 != -1:
            q_net_stage1[last_action_1] += perseveration

        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_action_1 = action_1[trial]
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updating ---
        
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Update Stage 1 with eligibility trace
        q_stage1_mf[action_1[trial]] += learning_rate * (delta_stage1 + lambda_eligibility * delta_stage2)
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```