Here are three new cognitive models that explore different mechanisms than the ones previously tested. These models focus on reward sensitivity differences, eligibility trace variations, and differential learning rates for the two stages.

### Model 1: Hybrid Learner with Reward Sensitivity
This model modifies the standard hybrid learner by introducing a `reward_sensitivity` parameter. Instead of treating the reward as simply 0 or 1, the subjective value of the reward is scaled. This captures individual differences in how strongly a reward impacts the value update, which is distinct from the learning rate (how fast the value changes).

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner with Reward Sensitivity.
    Scales the reward magnitude before updating Q-values.
    
    Parameters:
    learning_rate: [0,1] - Update rate for Q-values.
    beta: [0,10] - Inverse temperature for softmax choice.
    w: [0,1] - MB/MF mixing weight (0=MF, 1=MB).
    reward_sensitivity: [0,5] - Scaling factor for the subjective value of the reward.
    """
    learning_rate, beta, w, reward_sensitivity = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Apply reward sensitivity
        subjective_reward = reward[trial] * reward_sensitivity

        # SARSA(0) / TD(0) logic for Stage 1 MF
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Update Stage 2
        delta_stage2 = subjective_reward - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Learner with Eligibility Trace (TD(1) vs TD(0))
This model introduces an eligibility trace parameter `lambda_trace` specifically for the Model-Free Stage 1 update. In standard TD(0), Stage 1 is updated by the value of Stage 2. In TD(1) (or Monte Carlo), Stage 1 is updated directly by the final reward. This parameter interpolates between updating based on the *expected* value of the next state versus the *actual* outcome received.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner with Eligibility Trace (Lambda).
    Allows the Stage 1 MF value to be updated by a mix of the Stage 2 value and the final reward.
    
    Parameters:
    learning_rate: [0,1] - Update rate.
    beta: [0,10] - Inverse temperature.
    w: [0,1] - MB/MF mixing weight.
    lambda_trace: [0,1] - Eligibility trace decay. 0 = TD(0) (update from next state value), 1 = Monte Carlo (update from reward).
    """
    learning_rate, beta, w, lambda_trace = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Stage 2 update (standard)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

        # Stage 1 MF update with eligibility trace
        # If lambda=0, update is driven by Q_stage2 (TD error).
        # If lambda=1, update is driven by Reward (direct reinforcement).
        # Note: We use the already updated q_stage2 for the TD(0) portion typically, 
        # or the pre-update. Here we implement the lambda return effectively.
        
        # TD(0) target: Value of the state we landed in (Q(s', a'))
        target_td0 = q_stage2_mf[state_idx, action_2[trial]] 
        
        # TD(1) target: The actual reward received
        target_td1 = reward[trial]
        
        combined_target = (1 - lambda_trace) * target_td0 + lambda_trace * target_td1
        
        delta_stage1 = combined_target - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Pure Model-Free with Separate Stage Learning Rates
This model abandons the hybrid MB/MF architecture (setting `w=0` implicitly) to focus purely on Model-Free dynamics, but it allows for different plasticity at different stages of the task. It hypothesizes that the participant might learn the value of spaceships (`lr_stage1`) at a different speed than they learn the value of aliens (`lr_stage2`).

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free Learner with Separate Stage Learning Rates.
    Assumes no model-based planning, but distinct learning rates for Stage 1 (Spaceships) and Stage 2 (Aliens).
    
    Parameters:
    lr_stage1: [0,1] - Learning rate for the first stage choice.
    lr_stage2: [0,1] - Learning rate for the second stage choice.
    beta: [0,10] - Inverse temperature.
    """
    lr_stage1, lr_stage2, beta = model_parameters
    n_trials = len(action_1)
  
    # transition_matrix is unused in pure MF, but defined for consistency with template structure
    # transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]]) 
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice (Pure MF)
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Update Stage 1 using lr_stage1
        # Using standard SARSA-style update: Q1(a1) <- Q1(a1) + lr1 * (Q2(s, a2) - Q1(a1))
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr_stage1 * delta_stage1
        
        # Update Stage 2 using lr_stage2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr_stage2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```