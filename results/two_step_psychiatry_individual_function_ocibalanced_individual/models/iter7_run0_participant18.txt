Here are three new cognitive models for the two-step decision task, implementing distinct mechanisms for learning and value updating while adhering to the specified constraints.

### Model 1: Hybrid Model with Independent Stage 2 Learning Rates
This model implements the classic hybrid reinforcement learning framework (mixing model-based and model-free values) but introduces separate learning rates for the second-stage states (Planets). This hypothesis suggests that participants might learn at different speeds depending on which planet they land on (e.g., one planet might be more volatile or salient).

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Independent Stage 2 Learning Rates.

    This model combines Model-Based (MB) and Model-Free (MF) learning.
    Crucially, it uses two different learning rates for the second stage:
    one for updates occurring in State 0 (Planet X) and another for State 1 (Planet Y).
    This accounts for potential asymmetries in learning speed between contexts.

    Parameters:
    lr_state0: [0, 1] - Learning rate for updates in State 0 (Planet X).
    lr_state1: [0, 1] - Learning rate for updates in State 1 (Planet Y).
    beta: [0, 10] - Inverse temperature for softmax choice rule.
    w: [0, 1] - Weighting parameter (0 = Pure MF, 1 = Pure MB).
    """
    lr_state0, lr_state1, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        # Model-Based Value: Transition * Max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value: w * MB + (1-w) * MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        if action_1[trial] != -1:
            p_choice_1[trial] = probs_1[action_1[trial]]
        else:
            p_choice_1[trial] = 1.0

        # Skip update if data is missing
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
            continue
            
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        # Select learning rate based on the current state (Planet)
        current_lr = lr_state0 if state_idx == 0 else lr_state1

        # Stage 1 MF Update (TD(0))
        # Note: In standard hybrid models, Stage 1 MF is often updated via TD(1) (eligibility traces)
        # or simple TD(0) from the stage 2 value. Here we use TD(0) from Stage 2 Q-value.
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += current_lr * delta_stage1
        
        # Stage 2 MF Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += current_lr * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Model-Free Learner with Counterfactual Updating
This model assumes a pure Model-Free strategy but introduces a "counterfactual" or "fictitious" update mechanism. When the participant receives a reward for a specific alien, they not only update the value of the chosen alien but also slightly update the unchosen alien in the opposite direction (or towards a mean), representing a belief that if one option is good, the other might be worse (or vice-versa).

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Learner with Counterfactual Updating.

    This model updates the chosen action normally based on reward prediction error.
    However, it also updates the UNCHOSEN action in the second stage using a 
    counterfactual learning rate. This captures the intuition that information 
    about one option might inform the value of the alternative.

    Parameters:
    learning_rate: [0, 1] - Learning rate for the chosen action.
    alpha_cf: [0, 1] - Learning rate for the unchosen action (counterfactual).
    beta: [0, 10] - Inverse temperature for softmax.
    """
    learning_rate, alpha_cf, beta = model_parameters
    n_trials = len(action_1)
  
    # Pure MF implies we don't use the transition matrix for planning, 
    # but we track transitions for the structure of the task.
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action

    for trial in range(n_trials):

        # --- Stage 1 Policy (Pure MF) ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        if action_1[trial] != -1:
            p_choice_1[trial] = probs_1[action_1[trial]]
        else:
            p_choice_1[trial] = 1.0

        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
            continue

        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        
        # Stage 1 Update: SARSA-style or simple TD
        # Updating Stage 1 Q-value based on Stage 2 Q-value of chosen action
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1

        # Stage 2 Update (Chosen)
        chosen_a = action_2[trial]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, chosen_a]
        q_stage2_mf[state_idx, chosen_a] += learning_rate * delta_stage2
        
        # Stage 2 Update (Unchosen / Counterfactual)
        # We assume the unchosen option decays towards 0 or updates negatively 
        # relative to the observed reward (assuming anti-correlation or decay).
        # Here we implement decay towards 0 weighted by alpha_cf.
        unchosen_a = 1 - chosen_a
        # Counterfactual update: "I didn't choose it, so I assume its value is 0" 
        # or simply forgetting.
        delta_cf = 0 - q_stage2_mf[state_idx, unchosen_a] 
        q_stage2_mf[state_idx, unchosen_a] += alpha_cf * delta_cf
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model with Choice Perseveration (Stickiness) on Stage 2
While stickiness is commonly modeled on the first stage (repeating the spaceship choice), this model hypothesizes that participants develop a "favorite alien" habit. It adds a stickiness bonus to the Q-values of the previously chosen alien *within* the specific planet context, combined with a hybrid learning structure.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Stage 2 Choice Perseveration.

    This model mixes Model-Based and Model-Free values (parameter w).
    It includes a stickiness parameter specifically for the second stage (aliens).
    If a participant chose Alien A on Planet X previously, they are more likely 
    to choose it again, regardless of reward history.

    Parameters:
    learning_rate: [0, 1] - Learning rate.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - MB/MF mixing weight.
    stick_s2: [0, 5] - Perseveration bonus for the repeated Stage 2 choice.
    """
    learning_rate, beta, w, stick_s2 = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track last choice made in each state (Planet 0 and Planet 1)
    # Initialize with -1 (no previous choice)
    last_choice_s2 = np.array([-1, -1]) 

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        if action_1[trial] != -1:
            p_choice_1[trial] = probs_1[action_1[trial]]
        else:
            p_choice_1[trial] = 1.0

        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
            continue
            
        state_idx = state[trial]

        # --- Stage 2 Policy with Stickiness ---
        # Copy Q-values to add temporary stickiness bonus
        q_stage2_temp = q_stage2_mf[state_idx].copy()
        
        # If we have a previous choice for this specific planet, add stickiness
        if last_choice_s2[state_idx] != -1:
            q_stage2_temp[last_choice_s2[state_idx]] += stick_s2

        exp_q2 = np.exp(beta * q_stage2_temp)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # Update choice history for this state
        last_choice_s2[state_idx] = action_2[trial]
  
        # --- Learning ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```