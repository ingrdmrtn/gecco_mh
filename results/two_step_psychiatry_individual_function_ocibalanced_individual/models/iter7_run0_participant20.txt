Here are three new cognitive models that explore different mechanisms and parameter combinations not yet fully covered in the provided history.

### Model 1: Hybrid Learner with Inverse Temperature scaling by Reward Rate
This model hypothesizes that the participant's exploration/exploitation balance (beta) is not static but dynamic, scaling based on the recent average reward rate. When the environment feels "rich" (high reward rate), they might exploit more (higher beta); when "lean," they explore more (lower beta).

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner with Dynamic Beta Scaling based on Average Reward.
    
    The inverse temperature (beta) is scaled by a moving average of the reward rate.
    This captures the idea that exploration/exploitation trade-offs change based
    on environmental richness.
    
    Parameters:
    - learning_rate: [0,1] Update rate for Q-values.
    - beta_base: [0,10] Baseline inverse temperature.
    - w: [0,1] Weighting parameter (0=Pure MF, 1=Pure MB).
    - reward_avg_lr: [0,1] Learning rate for the average reward tracker.
    """
    learning_rate, beta_base, w, reward_avg_lr = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Initialize average reward at 0.5 (neutral expectation)
    avg_reward = 0.5 

    for trial in range(n_trials):
        
        # Dynamic beta: scales with how 'good' the environment currently is
        # If avg_reward is high, beta increases (more exploitation).
        # We clamp it slightly to prevent explosion, though beta_base handles magnitude.
        # Simple scaling: beta_effective = beta_base * (0.5 + avg_reward)
        beta_eff = beta_base * (0.5 + avg_reward)

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta_eff * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta_eff * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Updates
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Update average reward tracker
        avg_reward += reward_avg_lr * (reward[trial] - avg_reward)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Learner with Choice Perseveration (Repetition Bias) on Stage 1
While general stickiness has been tried, this model specifically isolates **Stage 1 Choice Perseveration** (`stick_1`). It posits that the habit to repeat a spaceship choice is a distinct mechanism from the goal-directed or model-free value learning, often driven by motor heuristics or cognitive laziness, independent of the outcome.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner with Stage 1 Choice Perseveration (Repetition Bias).
    
    Includes a parameter specifically for repeating the previous Stage 1 choice,
    independent of reward history.
    
    Parameters:
    - learning_rate: [0,1] Update rate.
    - beta: [0,10] Inverse temperature.
    - w: [0,1] Weighting parameter (0=Pure MF, 1=Pure MB).
    - stick_1: [0,10] Bonus added to the Q-value of the previously chosen spaceship.
    """
    learning_rate, beta, w, stick_1 = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Add stickiness bonus to the previously selected action
        if last_action_1 != -1:
            q_net[last_action_1] += stick_1
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Updates
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        last_action_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Learner with Separate Learning Rates for MB and MF
Standard models assume the Model-Free (MF) and Model-Based (MB) systems update their internal representations at the same speed (or that MB is static). This model introduces `learning_rate_mf` for the direct temporal-difference updates and `learning_rate_mb` (applied here to the second stage values which feed into the MB calculation) to see if the participant learns the immediate alien values differently than the spaceship values.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner with Separate Learning Rates for Stage 1 (MF) and Stage 2 (MB input).
    
    Allows the model-free system (Stage 1 Q-values) and the values used for 
    model-based planning (Stage 2 Q-values) to be updated at different rates.
    
    Parameters:
    - lr_stg1: [0,1] Learning rate for Stage 1 MF values.
    - lr_stg2: [0,1] Learning rate for Stage 2 values (used by both MF and MB).
    - beta: [0,10] Inverse temperature.
    - w: [0,1] Weighting parameter.
    """
    lr_stg1, lr_stg2, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Updates
        # Stage 1 MF update uses its specific learning rate
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr_stg1 * delta_stage1
        
        # Stage 2 update uses its specific learning rate
        # Note: This affects both the MB calculation (via max_q_stage2 next trial)
        # and the MF path (via delta_stage1 next trial)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr_stg2 * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```