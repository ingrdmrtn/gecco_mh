Here are three new cognitive models that explore different mechanisms for learning and decision-making in the two-step task, avoiding the exact parameter combinations previously listed.

### Model 1: Hybrid RL with Separate Learning Rates for Stage 1 and Stage 2
This model posits that participants might update their values for the first-stage choice (spaceships) at a different speed than they update their values for the second-stage choice (aliens). This is a pure Model-Free approach but allows flexibility in how quickly the two different levels of the task structure are learned.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free RL with Separate Learning Rates.
    
    This model assumes a pure model-free strategy (ignoring transition structure)
    but allows for different plasticity in the first stage (spaceship choice)
    versus the second stage (alien choice).
    
    Bounds:
    alpha_1: [0, 1] - Learning rate for stage 1 (spaceships).
    alpha_2: [0, 1] - Learning rate for stage 2 (aliens).
    beta: [0, 10] - Inverse temperature for both stages.
    """
    alpha_1, alpha_2, beta = model_parameters
    n_trials = len(action_1)
  
    # Initialize Q-values
    q_stage1_mf = np.zeros(2)      # Values for spaceships (A vs U)
    q_stage2_mf = np.zeros((2, 2)) # Values for aliens (W/S on X, P/H on Y)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial] # 0 for Planet X, 1 for Planet Y

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updating ---
        # Note: In pure MF, stage 1 is updated by the value of the state reached (SARSA-like)
        # or the reward received. Here we use the standard TD(0) chain.
        
        # Prediction error for stage 1: driven by the value of the *chosen* option in stage 2
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += alpha_1 * delta_stage1
        
        # Prediction error for stage 2: driven by the actual reward
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += alpha_2 * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Model-Based RL with Forgetting (Decay) on Unchosen Options
This model assumes the participant uses a Model-Based strategy (calculating Stage 1 values based on the transition matrix) but suffers from memory decay. Crucially, the values of options *not* chosen in Stage 2 decay back toward 0 (or 0.5) over time. This captures the idea that knowledge about specific aliens becomes less reliable if they haven't been visited recently.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Based RL with Passive Decay on Unchosen Stage 2 Options.
    
    This model calculates Stage 1 values using the transition matrix (MB).
    However, for Stage 2 values, unchosen options decay towards 0 on every trial,
    representing forgetting.
    
    Bounds:
    learning_rate: [0, 1] - Learning rate for chosen stage 2 options.
    beta: [0, 10] - Inverse temperature.
    decay: [0, 1] - Rate at which unchosen Q-values decay (1 = no decay, 0 = instant forgetting).
    """
    learning_rate, beta, decay = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values for Stage 2 (Aliens)
    q_stage2_mf = np.zeros((2, 2)) 

    for trial in range(n_trials):

        # --- Stage 1 Policy (Model-Based) ---
        # Calculate expected value of each spaceship based on current alien values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updating ---
        chosen_a2 = action_2[trial]
        r = reward[trial]
        
        # Update the chosen alien
        delta_stage2 = r - q_stage2_mf[state_idx, chosen_a2]
        q_stage2_mf[state_idx, chosen_a2] += learning_rate * delta_stage2
        
        # Decay the unchosen aliens (all 3 other options)
        # We iterate through all states and actions
        for s in range(2):
            for a in range(2):
                if not (s == state_idx and a == chosen_a2):
                    q_stage2_mf[s, a] *= decay

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model with Mixing Weight `w` and Perseveration
This is the classic "Hybrid" model often used in the two-step literature, combining Model-Based and Model-Free values via a mixing parameter `w`. Additionally, it includes a perseveration parameter (stickiness) to account for motor repetition or cognitive inertia at the first stage. This combination (Hybrid + Stickiness) is distinct from previous "MB + Stickiness" or "Hybrid (no stickiness)" attempts.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid (MB/MF) Model with Mixing Weight and Perseveration.
    
    Combines Model-Based and Model-Free Q-values for the first stage choice
    using a mixing weight 'w'. Also includes 'perseveration' to bias
    repeating the previous Stage 1 choice.
    
    Bounds:
    learning_rate: [0, 1] - Learning rate for MF updates.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - Mixing weight (1 = Pure MB, 0 = Pure MF).
    perseveration: [0, 5] - Stickiness bonus for repeating Stage 1 choice.
    """
    learning_rate, beta, w, perseveration = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):

        # --- Stage 1 Policy (Hybrid) ---
        # 1. Calculate MB values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Mix MB and MF values
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # 3. Add perseveration bonus to logits (not Q-values directly)
        logits = beta * q_net
        if last_action_1 != -1:
            logits[last_action_1] += perseveration
            
        exp_q1 = np.exp(logits)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updating ---
        # TD(0) update for Stage 1 MF
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # TD(0) update for Stage 2 MF (used by both MB and MF systems)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        last_action_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```