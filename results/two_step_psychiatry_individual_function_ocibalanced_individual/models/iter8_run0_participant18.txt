Here are three new cognitive models exploring different mechanisms for learning and decision-making in the two-step task.

### Model 1: Hybrid Learner with Separate Learning Rates for First and Second Stage
This model introduces separate learning rates for the first stage (spaceship choice) and the second stage (alien choice). This allows the model to capture if a participant learns the value of the transitions (Stage 1) at a different speed than the value of the final rewards (Stage 2). It combines Model-Based (MB) and Model-Free (MF) valuation using a weighting parameter `w`.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner with Separate Learning Rates for First and Second Stage.
    
    This model assumes the participant learns Stage 1 values (transitions) and 
    Stage 2 values (rewards) at different rates. It combines MB and MF systems.

    Bounds:
    lr_s1: [0, 1] - Learning rate for Stage 1 (TD(1) update).
    lr_s2: [0, 1] - Learning rate for Stage 2 (Reward update).
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - Weighting parameter (0 = Pure MF, 1 = Pure MB).
    """
    lr_s1, lr_s2, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        # MB value: Transition * Max(Stage 2 Values)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        if action_1[trial] != -1:
            p_choice_1[trial] = probs_1[action_1[trial]]
        else:
            p_choice_1[trial] = 1.0
            continue # Skip update if missing data

        state_idx = state[trial]
        if state_idx == -1: continue

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        if action_2[trial] != -1:
            p_choice_2[trial] = probs_2[action_2[trial]]
        else:
            p_choice_2[trial] = 1.0
            continue

        # Updates
        # Stage 2 update (Reward prediction error)
        # Using specific learning rate lr_s2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr_s2 * delta_stage2
        
        # Stage 1 update (TD(1) style update using Stage 2 PE)
        # Using specific learning rate lr_s1
        q_stage1_mf[action_1[trial]] += lr_s1 * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Counterfactual Learner (Fictitious Play)
This model assumes that when a participant receives a reward (or lack thereof) from a chosen alien, they also update the value of the *unchosen* alien in that state, but with a "counterfactual" learning rate. This simulates the participant thinking, "If I had chosen the other alien, I might have gotten a different outcome," often assuming the unchosen option decays or updates towards a neutral prior.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Counterfactual Learner (Fictitious Play).
    
    Updates the chosen action based on reward, but also updates the unchosen
    action in the same state towards 0 (forgetting/counterfactual update).
    Pure Model-Free for Stage 1 logic to isolate the Stage 2 update mechanism.

    Bounds:
    learning_rate: [0, 1] - Learning rate for chosen actions.
    alpha_cf: [0, 1] - Counterfactual learning rate (decay for unchosen).
    beta: [0, 10] - Inverse temperature.
    """
    learning_rate, alpha_cf, beta = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # 2 planets, 2 aliens

    for trial in range(n_trials):

        # policy for the first choice (Pure MF here for simplicity)
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        if action_1[trial] != -1:
            p_choice_1[trial] = probs_1[action_1[trial]]
        else:
            p_choice_1[trial] = 1.0
            continue

        state_idx = state[trial]
        if state_idx == -1: continue

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        if action_2[trial] != -1:
            p_choice_2[trial] = probs_2[action_2[trial]]
        else:
            p_choice_2[trial] = 1.0
            continue

        # Updates
        # Stage 2 update: Chosen
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Stage 2 update: Unchosen (Counterfactual)
        # The unchosen option decays towards 0 (or assumes 0 reward)
        unchosen_action = 1 - action_2[trial]
        q_stage2_mf[state_idx, unchosen_action] += alpha_cf * (0 - q_stage2_mf[state_idx, unchosen_action])

        # Stage 1 update (TD(1))
        # Propagating the Stage 2 RPE back to Stage 1
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Asymmetric Learning Rates for Transitions (Common vs. Rare)
This model focuses on the distinction between common and rare transitions. It posits that the learning update at Stage 1 might be modulated differently depending on whether the transition experienced was "Common" (A->X or U->Y) or "Rare" (A->Y or U->X). This is a purely Model-Free learner but with transition-dependent learning rates, capturing "surprise" modulation.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Asymmetric Learning Rates for Transitions (Common vs. Rare).
    
    This model applies different learning rates for the Stage 1 update depending
    on whether the transition to the planet was Common or Rare.
    
    Bounds:
    lr_common: [0, 1] - Learning rate when transition was expected (Common).
    lr_rare: [0, 1] - Learning rate when transition was unexpected (Rare).
    beta: [0, 10] - Inverse temperature.
    """
    lr_common, lr_rare, beta = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    # Define common transitions: (Action 0 -> State 0) and (Action 1 -> State 1)
    # Rare: (Action 0 -> State 1) and (Action 1 -> State 0)
    # Note: State indices in data are usually 0 and 1.

    for trial in range(n_trials):

        # policy for the first choice
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        if action_1[trial] != -1:
            p_choice_1[trial] = probs_1[action_1[trial]]
        else:
            p_choice_1[trial] = 1.0
            continue

        state_idx = state[trial]
        if state_idx == -1: continue

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        if action_2[trial] != -1:
            p_choice_2[trial] = probs_2[action_2[trial]]
        else:
            p_choice_2[trial] = 1.0
            continue
        
        # Determine if transition was common or rare
        # 0.7 prob: 0->0, 1->1. 0.3 prob: 0->1, 1->0
        is_common = (action_1[trial] == state_idx)
        current_lr = lr_common if is_common else lr_rare

        # Updates
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Stage 2 update (standard)
        # Using lr_common as a baseline for stage 2 just to save parameters, 
        # or could use average. Here we use current_lr for consistency in the trial.
        q_stage2_mf[state_idx, action_2[trial]] += current_lr * delta_stage2
        
        # Stage 1 update (modulated by transition type)
        q_stage1_mf[action_1[trial]] += current_lr * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```