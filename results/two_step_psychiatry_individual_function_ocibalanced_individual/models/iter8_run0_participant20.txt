Here are three new cognitive models based on the two-step task structure, exploring mechanisms not yet covered in the feedback list.

### Model 1: Hybrid Learner with Inverse Temperature Modulation (Exploration Bonus)
This model introduces a mechanism where the inverse temperature (`beta`) is not static but is modulated by an "exploration bonus" parameter (`phi`). This parameter adjusts the temperature based on the trade-off between exploitation and exploration, specifically testing if the participant becomes more decisive (higher beta) or more random (lower beta) as the estimated value difference between options increases.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner with Value-Dependent Exploration (Inverse Temperature Modulation).
    
    Instead of a fixed beta, the effective beta scales with the magnitude of the 
    difference in values. This captures the idea that choices might be more stochastic 
    when values are similar and more deterministic when one option is clearly better.

    Parameters:
    - learning_rate: [0,1] Value update rate.
    - beta_0: [0,10] Baseline inverse temperature.
    - phi: [0,5] Modulation factor. If > 0, larger value differences lead to more deterministic choices.
    - w: [0,1] Model-based weight.
    """
    learning_rate, beta_0, phi, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Calculate value difference for Stage 1
        val_diff_1 = np.abs(q_net[0] - q_net[1])
        effective_beta_1 = beta_0 * (1 + phi * val_diff_1)
        
        exp_q1 = np.exp(effective_beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # policy for the second choice
        # Calculate value difference for Stage 2
        val_diff_2 = np.abs(q_stage2_mf[state_idx, 0] - q_stage2_mf[state_idx, 1])
        effective_beta_2 = beta_0 * (1 + phi * val_diff_2)
        
        exp_q2 = np.exp(effective_beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Update Stage 1 (TD(0) update towards Stage 2 value)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Update Stage 2 (TD(0) update towards Reward)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Learner with Asymmetric Transition Learning
Standard models assume the transition matrix is fixed at `[[0.7, 0.3], [0.3, 0.7]]`. However, participants might subjectively learn or distort these probabilities. This model introduces a parameter `trans_prob` that allows the agent's internal model of the transition structure to deviate from the true 0.7/0.3 split. It tests if the participant has an inaccurate model of the spaceship transitions.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner with Subjective Transition Probability.
    
    This model allows the agent to hold a subjective belief about the transition 
    probabilities (common vs. rare) that may differ from the true 0.7/0.3 structure.
    
    Parameters:
    - learning_rate: [0,1] Value update rate.
    - beta: [0,10] Inverse temperature.
    - w: [0,1] Model-based weight.
    - trans_prob: [0,1] Subjective probability of the common transition (e.g., 0.7 is veridical).
    """
    learning_rate, beta, w, trans_prob = model_parameters
    n_trials = len(action_1)
  
    # Subjective transition matrix based on the learned/estimated parameter
    # Row 0: Action A -> [Prob(X), Prob(Y)]
    # Row 1: Action U -> [Prob(X), Prob(Y)]
    # Assuming symmetry: P(X|A) = P(Y|U) = trans_prob
    transition_matrix = np.array([[trans_prob, 1 - trans_prob], 
                                  [1 - trans_prob, trans_prob]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        
        # The MB calculation uses the subjective transition matrix
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Update Stage 1
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Update Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Learner with Reward-Dependent "Win-Stay" Stickiness
While general stickiness (repeating the last choice regardless of outcome) has been tried, this model implements a specific "Win-Stay" heuristic parameter. This parameter adds a bonus to the previous choice *only if* the previous trial was rewarded. This captures a simpler, heuristic strategy often seen alongside sophisticated learning.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner with Reward-Dependent Stickiness (Win-Stay Bonus).
    
    Adds a 'stickiness' bonus to the Q-value of the previously chosen Stage 1 action,
    but ONLY if the previous trial resulted in a reward. This models a heuristic
    'Win-Stay' tendency separate from value learning.

    Parameters:
    - learning_rate: [0,1] Value update rate.
    - beta: [0,10] Inverse temperature.
    - w: [0,1] Model-based weight.
    - win_stay_bonus: [0,5] Additional value added to the previous choice if rewarded.
    """
    learning_rate, beta, w, win_stay_bonus = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track previous trial data
    prev_action_1 = -1
    prev_reward = 0

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Apply Win-Stay Bonus
        q_net_stick = q_net.copy()
        if trial > 0 and prev_reward == 1:
            q_net_stick[prev_action_1] += win_stay_bonus

        exp_q1 = np.exp(beta * q_net_stick)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Update history for next trial
        prev_action_1 = action_1[trial]
        prev_reward = reward[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```