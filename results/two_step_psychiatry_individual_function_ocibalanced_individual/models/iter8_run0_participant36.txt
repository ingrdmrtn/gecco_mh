Here are three new cognitive models for the two-step task.

### Model 1: Sophisticated Model-Free Learner (Eligibility Traces + Counterfactuals)
This model hypothesizes that participants do not use a model-based map (`w=0`), but instead rely on a sophisticated model-free system. It combines **Eligibility Traces** (allowing Stage 2 outcomes to update Stage 1 choices directly) with **Counterfactual Updating** in Stage 2 (updating the unchosen alien as if it would have yielded the opposite outcome).

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Sophisticated Model-Free Learner.
    Combines eligibility traces (lambda) with counterfactual updating (cf_lr).
    No Model-Based component is used.
    
    Parameters:
    learning_rate: [0,1] - Direct learning rate for chosen options.
    beta: [0,10] - Inverse temperature.
    lam: [0,1] - Eligibility trace decay (lambda).
    cf_lr: [0,1] - Counterfactual learning rate for unchosen options in Stage 2.
    """
    learning_rate, beta, lam, cf_lr = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # Policy for the first choice (Pure MF)
        # Note: We ignore MB calculation here as this is a pure MF model hypothesis
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # Policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Update Stage 1 (TD(1) / Eligibility Trace)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Update Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Counterfactual Update for Stage 2 unchosen option
        # Assume unchosen option would have yielded the opposite reward (1-R)
        # This bias is common in binary choice tasks
        unchosen_a2 = 1 - action_2[trial]
        delta_cf = (1 - reward[trial]) - q_stage2_mf[state_idx, unchosen_a2]
        q_stage2_mf[state_idx, unchosen_a2] += cf_lr * delta_cf

        # Apply Eligibility Trace to Stage 1
        q_stage1_mf[action_1[trial]] += learning_rate * lam * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Learner with Passive Decay (Forgetting)
This model adds a **Decay** mechanism to the standard Hybrid model. It posits that Q-values for unchosen options (in both Stage 1 and the visited state of Stage 2) decay towards zero over time, representing a forgetting process or a "leak" in memory, which allows the model to adapt to volatility by discarding old information.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner with Passive Decay.
    Unchosen options decay towards 0 at each step, simulating forgetting.
    
    Parameters:
    learning_rate: [0,1] - Update rate for chosen options.
    beta: [0,10] - Inverse temperature.
    w: [0,1] - Weighting between Model-Based (1) and Model-Free (0).
    decay: [0,1] - Decay rate for unchosen options (1 = instant forgetting, 0 = no forgetting).
    """
    learning_rate, beta, w, decay = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # Policy for the first choice (Hybrid)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # Policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Update Stage 1
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Decay unchosen Stage 1 option
        q_stage1_mf[1 - action_1[trial]] *= (1 - decay)

        # Update Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Decay unchosen Stage 2 option (in the current state)
        q_stage2_mf[state_idx, 1 - action_2[trial]] *= (1 - decay)
        
        # (Optional: One could also decay the unvisited state, but standard implementations 
        # often focus on the active set. Here we decay only the active state's unchosen option.)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Learner with Dual Stickiness (Stage 1 & Stage 2)
This model extends the concept of perseveration to both stages. It separates the tendency to repeat the abstract choice of spaceship (`stick_1`) from the tendency to repeat the motor response for the alien (`stick_2`). This allows the model to capture habit formation at different levels of the task hierarchy.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner with Dual Stickiness.
    Includes separate stickiness parameters for Stage 1 (Spaceship) and Stage 2 (Alien/Motor).
    
    Parameters:
    learning_rate: [0,1] - Update rate.
    beta: [0,10] - Inverse temperature.
    w: [0,1] - MB/MF weight.
    stick_1: [0,5] - Stickiness bonus for Stage 1 choice.
    stick_2: [0,5] - Stickiness bonus for Stage 2 choice (motor perseveration).
    """
    learning_rate, beta, w, stick_1, stick_2 = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track previous actions for stickiness
    prev_a1 = -1
    prev_a2 = -1

    for trial in range(n_trials):

        # Policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Apply Stage 1 Stickiness
        if trial > 0:
            q_net_1[prev_a1] += stick_1
            
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # Policy for the second choice
        q_net_2 = q_stage2_mf[state_idx].copy()
        
        # Apply Stage 2 Stickiness (Motor perseveration: repeating the same button 0 or 1)
        if trial > 0:
            q_net_2[prev_a2] += stick_2

        exp_q2 = np.exp(beta * q_net_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Updates
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1

        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Store actions for next trial
        prev_a1 = action_1[trial]
        prev_a2 = action_2[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```