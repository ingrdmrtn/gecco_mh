Here are three new cognitive models exploring different mechanisms for decision-making in the two-step task. These models introduce variations in how rewards are processed (magnitude vs. probability), how uncertainty is handled, and how model-based/model-free systems interact.

### Model 1: Hybrid Model with Separate Learning Rates for Stage 1 and Stage 2
This model hypothesizes that participants might update their expectations about the spaceships (Stage 1) at a different speed than they update their expectations about the aliens (Stage 2). The "best model so far" used a purely Model-Based approach for Stage 1, but this model allows for a hybrid Model-Free/Model-Based mixture where the Model-Free component has its own learning rate.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Distinct Learning Rates (alpha1, alpha2).
    
    This model assumes the participant learns Stage 1 (spaceship) values via a Model-Free
    prediction error (using alpha1) and Stage 2 (alien) values via a separate rate (alpha2).
    It mixes Model-Based and Model-Free values for the first choice using a weighting parameter 'w'.
    
    Bounds:
    alpha1: [0, 1] - Learning rate for Stage 1 (spaceships).
    alpha2: [0, 1] - Learning rate for Stage 2 (aliens).
    beta: [0, 10] - Inverse temperature (softmax sensitivity).
    w: [0, 1] - Weighting parameter (0 = pure Model-Free, 1 = pure Model-Based).
    """
    alpha1, alpha2, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2) # Model-free values for spaceships
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for aliens

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        # 1. Calculate Model-Based values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Mix MB and MF values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # 3. Softmax
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial] # Planet arrived at

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updating ---
        # Update Stage 1 MF value using the value of the state actually reached (SARSA-like logic for stage 1)
        # Note: Standard Daw 2011 uses Q(s2, a2) for the TD error at stage 1
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += alpha1 * delta_stage1
        
        # Update Stage 2 MF value based on reward
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += alpha2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Reward-Asymmetry Model (Positive vs Negative Prediction Error)
This model investigates if the participant learns differently from "disappointment" (negative prediction errors) versus "pleasant surprise" (positive prediction errors). This is often relevant in psychiatric populations where sensitivity to reward versus punishment/omission differs.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Reward-Asymmetry Model (Alpha_Pos vs Alpha_Neg).
    
    This model splits the learning rate based on the sign of the prediction error
    at the second stage (outcome). It assumes a purely Model-Based strategy for 
    Stage 1 planning (to reduce param count), but allows asymmetric updating of the 
    underlying reward probabilities.

    Bounds:
    alpha_pos: [0, 1] - Learning rate when Prediction Error is positive (better than expected).
    alpha_neg: [0, 1] - Learning rate when Prediction Error is negative (worse than expected).
    beta: [0, 10] - Inverse temperature.
    """
    alpha_pos, alpha_neg, beta = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values for the aliens (Stage 2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy (Pure Model-Based for simplicity) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updating ---
        # Calculate PE
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Asymmetric Update
        if delta_stage2 >= 0:
            q_stage2_mf[state_idx, action_2[trial]] += alpha_pos * delta_stage2
        else:
            q_stage2_mf[state_idx, action_2[trial]] += alpha_neg * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Counterfactual Update Model
This model posits that when a participant receives a reward (or lack thereof) from one alien, they also update the value of the *unchosen* alien on that same planet, assuming an inverse correlation or a "fictive" learning signal. For example, if Alien A gives 0, the participant might slightly increase the value of Alien B.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Counterfactual Update Model.
    
    In addition to standard learning, this model updates the unchosen option 
    at the second stage. If the chosen option is updated by alpha * PE, 
    the unchosen option drifts towards the opposite value or decays, 
    controlled by a 'counterfactual' learning rate (alpha_cf).
    
    Bounds:
    alpha: [0, 1] - Standard learning rate for chosen option.
    alpha_cf: [0, 1] - Counterfactual learning rate for unchosen option.
    beta: [0, 10] - Inverse temperature.
    """
    alpha, alpha_cf, beta = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage2_mf = np.zeros((2, 2)) # Initial values at 0 (or 0.5)

    for trial in range(n_trials):

        # --- Stage 1 Policy (Pure Model-Based) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updating ---
        # Chosen option update
        chosen_alien = action_2[trial]
        unchosen_alien = 1 - chosen_alien
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, chosen_alien]
        q_stage2_mf[state_idx, chosen_alien] += alpha * delta_stage2
        
        # Counterfactual update: 
        # Assume the unchosen option would have yielded the opposite reward (or average reward 0.5)
        # Here we assume a 'forgone' reward of (1 - actual_reward) implies a competitive structure,
        # or simply drift the unchosen one towards the unobserved outcome.
        forgone_reward = 1 - reward[trial] 
        delta_cf = forgone_reward - q_stage2_mf[state_idx, unchosen_alien]
        q_stage2_mf[state_idx, unchosen_alien] += alpha_cf * delta_cf
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```