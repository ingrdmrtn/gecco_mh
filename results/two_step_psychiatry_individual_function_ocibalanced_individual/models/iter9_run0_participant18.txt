Here are three new cognitive models based on the two-step decision task, introducing distinct mechanisms for learning and valuation while adhering to the parameter constraints and template structure.

### Model 1: Hybrid Learner with Reward-Scaling (Magnitude Sensitivity)
This model introduces a `reward_scaling` parameter. Instead of treating the reward simply as 0 or 1, the subjective utility of the reward is scaled. This captures the idea that some participants might be hypersensitive (or hyposensitive) to the feedback, effectively amplifying the prediction error. It combines Model-Based (MB) and Model-Free (MF) learning using a mixing weight `w`.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner with Reward Scaling.
    
    This model assumes a hybrid Model-Based/Model-Free architecture.
    Crucially, it includes a reward sensitivity parameter. The subjective
    value of the reward is scaled, which amplifies or dampens the 
    prediction error updates.

    Parameters:
    learning_rate: [0, 1] - Learning rate for Q-value updates.
    beta: [0, 10] - Inverse temperature for softmax.
    w: [0, 1] - Mixing weight. 0 = Pure MF, 1 = Pure MB.
    reward_scaling: [0, 5] - Multiplier for the reward signal (subjective utility).
    """
    learning_rate, beta, w, reward_scaling = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # Handle missing data
        if action_1[trial] == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # --- Stage 1 Policy ---
        # Model-Based Value: Transition * Max(Stage 2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Value: w * MB + (1-w) * MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        if state_idx == -1 or action_2[trial] == -1:
            p_choice_2[trial] = 1.0
            continue

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updating ---
        # Scale the reward
        subjective_reward = reward[trial] * reward_scaling

        # Stage 1 MF Update (TD(0) style for simplicity in this variant)
        # Note: Standard Daw task often uses lambda-eligibility traces, 
        # but here we use a direct update for variety in parameter space.
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Stage 2 MF Update
        delta_stage2 = subjective_reward - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Pure Model-Free with Asymmetric Learning Rates (Reward vs Punishment)
This model abandons the Model-Based component entirely to focus on how valuations change differently based on valence. It splits the learning rate into `alpha_plus` (learning from reward) and `alpha_minus` (learning from lack of reward). It also includes an eligibility trace `lambda_trace` to connect the second-stage outcome back to the first-stage choice.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free with Asymmetric Learning (Pos/Neg) and Eligibility Trace.
    
    This model ignores the transition structure (Pure MF) but differentiates 
    between learning from positive outcomes (alpha_plus) and negative outcomes 
    (alpha_minus). It uses an eligibility trace to update Stage 1 values based 
    on Stage 2 rewards.

    Parameters:
    alpha_plus: [0, 1] - Learning rate for rewarded trials (reward=1).
    alpha_minus: [0, 1] - Learning rate for unrewarded trials (reward=0).
    beta: [0, 10] - Inverse temperature.
    lambda_trace: [0, 1] - Eligibility trace decay (0 = TD(0), 1 = Monte Carlo).
    """
    alpha_plus, alpha_minus, beta, lambda_trace = model_parameters
    n_trials = len(action_1)
  
    # Only MF values needed
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        if action_1[trial] == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # --- Stage 1 Policy (Pure MF) ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        if state_idx == -1 or action_2[trial] == -1:
            p_choice_2[trial] = 1.0
            continue

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updating ---
        
        # Determine current learning rate based on reward valence
        # Note: We use the final reward to determine the rate for BOTH stages 
        # in this specific hypothesis of valence-dependent learning.
        current_alpha = alpha_plus if reward[trial] == 1 else alpha_minus

        # Prediction Error 1: Difference between Stage 2 Value and Stage 1 Value
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += current_alpha * delta_stage1
        
        # Prediction Error 2: Difference between Reward and Stage 2 Value
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += current_alpha * delta_stage2
        
        # Eligibility Trace Update: Apply stage 2 error to stage 1 choice
        q_stage1_mf[action_1[trial]] += current_alpha * lambda_trace * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Model-Based with Perseveration (Stickiness) and Inverse-Temperature Modulation
This model posits that the participant is purely Model-Based (calculating values via transitions) but is influenced by simple motor perseveration (`stickiness` on the previous Stage 1 choice). Additionally, it introduces `beta_stickiness`, which allows the inverse temperature (randomness) to fluctuate based on whether the previous trial was rewarded. This tests if participants become more exploratory (lower beta) after a loss or more exploitative (higher beta) after a win.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Based with Choice Stickiness and Reward-Modulated Beta.
    
    This model assumes a Pure Model-Based strategy. It adds a 'stickiness' 
    parameter to capture repetition bias. Uniquely, it modulates the 
    softmax beta based on the previous trial's outcome: a baseline beta 
    is boosted by 'beta_boost' if the previous trial was rewarded.

    Parameters:
    learning_rate: [0, 1] - Update rate for Stage 2 values.
    beta_base: [0, 10] - Baseline inverse temperature.
    beta_boost: [0, 5] - Additional beta added if previous trial was rewarded.
    stickiness: [0, 5] - Bonus added to the Q-value of the previously chosen action.
    """
    learning_rate, beta_base, beta_boost, stickiness = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Only Stage 2 MF values are learned; Stage 1 is derived via MB
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    last_reward = 0

    for trial in range(n_trials):
        if action_1[trial] == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            last_action_1 = -1
            last_reward = 0
            continue

        # --- Stage 1 Policy (Pure MB) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Add Stickiness
        if last_action_1 != -1:
            q_stage1_mb[last_action_1] += stickiness
            
        # Modulate Beta
        current_beta = beta_base + (beta_boost if last_reward == 1 else 0.0)

        exp_q1 = np.exp(current_beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_action_1 = action_1[trial]
        state_idx = state[trial]
        
        if state_idx == -1 or action_2[trial] == -1:
            p_choice_2[trial] = 1.0
            last_reward = 0
            continue

        # --- Stage 2 Policy ---
        # Note: We use base beta for stage 2 to isolate the effect to stage 1 planning
        exp_q2 = np.exp(beta_base * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updating ---
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        last_reward = reward[trial]
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```