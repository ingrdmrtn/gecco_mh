participant,fitted_parameters,parsed_params,num_params,extraction_mismatch,shared_params,unique_params,shared_mechanisms,unique_mechanisms,shared,model_type,baseline_params,baseline_bic,best_model_bic
14,"[np.float64(1.0), np.float64(0.4369022699370622), np.float64(1.7550241259325678), np.float64(0.0844102204817049), np.float64(0.4099863788444596), np.float64(0.5528743673574067)]",['lr_pos' 'lr_neg' 'beta' 'w' 'lam' 'stickiness'],6,False,"['beta', 'w', 'lambd', 'perseveration']","['lr_pos', 'lr_neg']","['model-based-learning', 'model-free-learning', 'eligibility-traces', 'choice-perseveration', 'hybrid-valuation']",['asymmetric-learning-rates'],partial,Hybrid MB/MF model with Asymmetric Learning Rates and Stickiness,"['learning_rate', 'learning_rate_2', 'beta', 'beta_2', 'w', 'lambd', 'perseveration']",516.9225775710403,493.1994821906962
15,"[np.float64(0.4912235515923661), np.float64(0.1444018744694392), np.float64(0.0995089100420503), np.float64(6.478020088102445), np.float64(8.480786438247552), np.float64(2.372309010885032)]",['alpha_pos' 'alpha_neg' 'forget' 'beta_mb' 'beta_2' 'stickiness'],6,False,"['beta', 'beta_2', 'perseveration']","['alpha_pos', 'alpha_neg', 'forget']","['model-based-planning', 'choice-perseveration']","['asymmetric-learning', 'passive-forgetting']",partial,Model-Based RL with Asymmetric Learning and Forgetting,"['learning_rate', 'learning_rate_2', 'beta', 'beta_2', 'w', 'lambd', 'perseveration']",350.1519827210901,306.21126194157426
16,"[np.float64(1.0), np.float64(0.6722901515906621), np.float64(3.4440839537074885), np.float64(1.278059828460354)]",['alpha_pos' 'alpha_neg' 'beta_1' 'beta_2'],4,False,"['beta', 'beta_2']","['alpha_pos', 'alpha_neg']","['model-free-learning', 'softmax-action-selection']","['asymmetric-learning-rates', 'valence-dependent-learning']",partial,Model-Free RL with Asymmetric Learning Rates and Separate Betas,"['learning_rate', 'learning_rate_2', 'beta', 'beta_2', 'w', 'lambd', 'perseveration']",448.29091589139017,428.4671315472904
17,"[np.float64(0.5884331063913804), np.float64(3.7483455720940833), np.float64(0.4633854614370358), np.float64(0.3484297759381269)]",['learning_rate' 'beta' 'w' 'perseverance'],4,False,"['learning_rate', 'beta', 'w', 'perseveration']",[],"['model-based-learning', 'model-free-learning', 'choice-perseveration', 'eligibility-traces']",[],partial,Hybrid learner combining Model-Based and Model-Free values with perseverance.,"['learning_rate', 'learning_rate_2', 'beta', 'beta_2', 'w', 'lambd', 'perseveration']",358.76648668733304,347.081182135626
17,"[np.float64(0.5884331063913804), np.float64(3.7483455720940833), np.float64(0.4633854614370358), np.float64(0.3484297759381269)]",['learning_rate' 'beta' 'w' 'perseverance'],4,False,"['learning_rate', 'beta', 'w', 'perseveration']",[],"['model-based-learning', 'model-free-learning', 'choice-perseveration', 'eligibility-traces']",[],partial,Hybrid learner combining Model-Based and Model-Free values with perseverance.,"['learning_rate', 'learning_rate_2', 'beta', 'beta_2', 'w', 'lambd', 'perseveration']",358.76648668733304,347.081182135626
18,"[np.float64(1.0), np.float64(2.58654634389334), np.float64(0.2462211142366992), np.float64(0.8044691744944518), np.float64(0.616257686779161)]",['lr' 'beta' 'w' 'decay_unc' 'pers'],5,False,"['learning_rate', 'beta', 'w', 'perseveration']",['decay_unc'],"['model-based-weighting', 'perseveration-bonus', 'eligibility-traces']",['selective-decay'],partial,MB/MF model with Selective Decay for Unchosen Options,"['learning_rate', 'learning_rate_2', 'beta', 'beta_2', 'w', 'lambd', 'perseveration']",465.0871657985018,420.3998720905437
19,"[np.float64(0.6673092813592224), np.float64(0.9627549297611236), np.float64(2.7074131632582588), np.float64(0.7144459884547214)]",['alpha_pos' 'alpha_neg' 'beta' 'forget_rate'],4,False,['beta'],"['alpha_pos', 'alpha_neg', 'forget_rate']","['model-free-learning', 'softmax-action-selection']","['asymmetric-learning-rates', 'value-forgetting']",partial,Asymmetric Learning Q-Learning with Forgetting,"['learning_rate', 'learning_rate_2', 'beta', 'beta_2', 'w', 'lambd', 'perseveration']",454.2144083137601,430.67106781925537
20,"[np.float64(0.7336479011980928), np.float64(10.0), np.float64(5.561620669887917), np.float64(0.874482890669427), np.float64(0.96141373127779), np.float64(0.9062119608426528)]",['learning_rate' 'beta_1' 'beta_2' 'w' 'stick_1' 'stick_2'],6,False,"['learning_rate', 'beta', 'beta_2', 'w', 'perseveration']",['stick_2'],"['model-based-learning', 'model-free-learning', 'stage-one-perseveration']",['stage-two-stickiness'],partial,Dual Stage Stickiness Model,"['learning_rate', 'learning_rate_2', 'beta', 'beta_2', 'w', 'lambd', 'perseveration']",362.1901046298327,268.6412491329822
21,"[np.float64(0.5861490377876086), np.float64(0.420751700542508), np.float64(7.81395723105584), np.float64(3.877890834950181), np.float64(0.9697236717625932), np.float64(0.3530405377028137)]",['lr_reward' 'lr_trans' 'beta_1' 'beta_2' 'w' 'stickiness'],6,False,"['learning_rate', 'beta', 'beta_2', 'w', 'perseveration']",['lr_trans'],"['model-free-learning', 'model-based-learning', 'hybrid-value-calculation', 'choice-perseveration', 'softmax-action-selection']",['dynamic-transition-learning'],partial,Dynamic Transition Learning Model,"['learning_rate', 'learning_rate_2', 'beta', 'beta_2', 'w', 'lambd', 'perseveration']",288.53968578957466,271.92596051250644
22,"[np.float64(0.6435450649554438), np.float64(5.364072458026897), np.float64(4.219354367168003), np.float64(1.0), np.float64(1.919237633822), np.float64(0.6705937884342438), np.float64(0.2249351330999199)]",['lr' 'beta_1' 'beta_2' 'w' 'stick' 'decay' 'q_bias'],7,False,"['learning_rate', 'beta', 'beta_2', 'w', 'perseveration']","['decay', 'q_bias']","['model-based-weighting', 'temporal-difference-learning', 'softmax-action-selection', 'choice-perseveration']","['unchosen-value-decay', 'equilibrium-bias']",partial,Hybrid Model with Decay to Equilibrium Bias,"['learning_rate', 'learning_rate_2', 'beta', 'beta_2', 'w', 'lambd', 'perseveration']",373.1977620978524,331.5134420006443
23,"[np.float64(0.2413198334266888), np.float64(2.180082275653913), np.float64(2.3764830540782094), np.float64(1.285993314551738)]",['learning_rate' 'beta' 'stick_s1' 'stick_s2'],4,False,"['learning_rate', 'beta', 'perseveration']",['stick_s2'],"['model-free-learning', 'stage-one-perseveration']",['stage-two-perseveration'],partial,Stage-Specific Stickiness Model-Free Q-Learning,"['learning_rate', 'learning_rate_2', 'beta', 'beta_2', 'w', 'lambd', 'perseveration']",374.5861601736647,316.9258674696726
24,"[np.float64(0.7152648165396842), np.float64(0.0203173940457018), np.float64(0.8245650589970216), np.float64(0.559729328465656)]",['lr_val' 'lr_common' 'beta' 'pers'],4,False,"['learning_rate_2', 'beta', 'perseveration']",['lr_common'],"['model-based valuation', 'stage-2 q-learning', 'choice perseveration']",['coupled transition-learning'],partial,Model-Based with Coupled Transition Learning,"['learning_rate', 'learning_rate_2', 'beta', 'beta_2', 'w', 'lambd', 'perseveration']",572.5972640584621,519.7700033314477
25,"[np.float64(0.8285417148280306), np.float64(0.2710936999633623), np.float64(3.3541381844419287), np.float64(1.0), np.float64(0.1559633922155959)]",['lr_pos' 'lr_neg' 'beta' 'lam' 'stickiness'],5,False,"['beta', 'lambd', 'perseveration']","['lr_pos', 'lr_neg']","['model-free-learning', 'eligibility-traces', 'choice-stickiness']",['valence-dependent-learning'],partial,Asymmetric TD(lambda) with Choice Stickiness,"['learning_rate', 'learning_rate_2', 'beta', 'beta_2', 'w', 'lambd', 'perseveration']",468.7032695600562,462.21988858750876
26,"[np.float64(0.6533704498089139), np.float64(3.295063537471302), np.float64(0.5994713329134045), np.float64(-0.2355979165570963), np.float64(0.0922819979861643)]",['learning_rate' 'beta_1' 'beta_2' 'stick_win' 'stick_loss'],5,False,"['learning_rate_2', 'beta', 'beta_2']","['stick_win', 'stick_loss']","['model-based-valuation', 'stage-two-learning']",['outcome-dependent-stickiness'],partial,Pure Model-Based Learner with Outcome-Dependent Stickiness,"['learning_rate', 'learning_rate_2', 'beta', 'beta_2', 'w', 'lambd', 'perseveration']",581.4052370598846,540.1588473886261
27,"[np.float64(0.4559442285441008), np.float64(7.2277327550332915), np.float64(4.000585214797519), np.float64(0.2112883165122476), np.float64(0.2570752648249013), np.float64(0.1906638795305679)]",['learning_rate' 'beta_1' 'beta_2' 'w' 'stickiness' 'decay_unchosen'],6,False,"['learning_rate', 'beta_1', 'beta_2', 'w', 'stickiness']",['decay_unchosen'],"['model-based-learning', 'model-free-learning', 'choice-perseveration', 'softmax-action-selection']",['unchosen-option-decay'],partial,Unchosen Decay Model,"['learning_rate', 'learning_rate_2', 'beta', 'beta_2', 'w', 'lambd', 'perseveration']",369.7799113976207,305.1619033711422
28,"[np.float64(1.0), np.float64(0.3110260915799533), np.float64(0.8066633301671182), np.float64(4.62084027554272), np.float64(4.159281802880896), np.float64(0.1549465197813559), np.float64(1.0)]",['lr' 'lr_cf' 'decay' 'beta_1' 'beta_2' 'w' 'lam'],7,False,"['learning_rate', 'beta', 'beta_2', 'w', 'lambd']","['lr_cf', 'decay']","['model-based-learning', 'model-free-learning', 'eligibility-traces']","['counterfactual-updating', 'value-decay']",partial,"Hybrid Model with Counterfactual Updating, Decay, and Eligibility Traces","['learning_rate', 'learning_rate_2', 'beta', 'beta_2', 'w', 'lambd', 'perseveration']",418.2194663249507,369.7660698640346
29,"[np.float64(0.7926355674662221), np.float64(0.241407397078654), np.float64(3.046681127230476), np.float64(0.957500976141337)]",['learning_rate' 'beta_mb' 'beta_mf' 'forget_rate'],4,False,['learning_rate'],"['beta_mb', 'beta_mf', 'forget_rate']","['model-based learning', 'model-free learning', 'hybrid value-integration']",['q-value forgetting'],partial,Independent Beta Hybrid Model with Forgetting,"['learning_rate', 'learning_rate_2', 'beta', 'beta_2', 'w', 'lambd', 'perseveration']",404.9113655797641,363.57619180024795
30,"[np.float64(1.0), np.float64(10.0), np.float64(1.0), np.float64(0.7409448476285232), np.float64(0.0175511585949121), np.float64(0.0508949656924569)]",['learning_rate' 'beta' 'lambda_eligibility' 'decay' 'pers_s1' 'pers_s2'],6,False,"['learning_rate', 'beta', 'lambd', 'perseveration']","['decay', 'pers_s2']","['model-free learning', 'eligibility traces', 'stage-one perseveration']","['q-value decay', 'stage-two perseveration']",partial,Model-Free Q-Learning with Stage-Specific Perseveration,"['learning_rate', 'learning_rate_2', 'beta', 'beta_2', 'w', 'lambd', 'perseveration']",549.5326707817405,451.93927899617165
31,"[np.float64(0.1957200496698137), np.float64(2.846393987543654), np.float64(1.059034261571883), np.float64(0.0), np.float64(-0.6318453745681211)]",['lr' 'beta_win' 'beta_loss' 'w' 'stick'],5,False,"['learning_rate', 'w', 'perseveration']","['beta_win', 'beta_loss']","['model-based-weighting', 'choice-perseveration', 'temporal-difference-learning']",['outcome-dependent-exploration'],partial,Outcome-Dependent Exploration (Beta-Win/Loss),"['learning_rate', 'learning_rate_2', 'beta', 'beta_2', 'w', 'lambd', 'perseveration']",579.9471739252648,502.779540824582
32,"[np.float64(0.3012275606146802), np.float64(0.8691325005122977), np.float64(10.0), np.float64(6.044727145525668), np.float64(0.3185588900394618), np.float64(1.399108639234307), np.float64(0.7391853966341636)]",['lr_trans' 'lr_val' 'beta1' 'beta2' 'decay' 'pers_win' 'pers_loss'],7,False,"['learning_rate_2', 'beta', 'beta_2']","['lr_trans', 'decay', 'pers_win', 'pers_loss']","['model-based valuation', 'stage-two q-learning', 'softmax action-selection']","['adaptive transition-learning', 'passive value-decay', 'outcome-dependent perseveration']",partial,"Adaptive Model-Based with Transition Learning, Value Decay, and Perseveration","['learning_rate', 'learning_rate_2', 'beta', 'beta_2', 'w', 'lambd', 'perseveration']",400.24736890606937,337.79413522271244
33,"[np.float64(0.0304750258050673), np.float64(0.2225849972616322), np.float64(6.819254849354541), np.float64(1.0), np.float64(0.6625092144370494), np.float64(0.5228251373845327)]",['learning_rate' 'decay_rate' 'beta' 'w' 'stick_1' 'stick_2'],6,False,"['learning_rate', 'beta', 'w', 'perseveration']","['decay_rate', 'stick_2']","['hybrid-value-estimation', 'stage-one-perseveration']","['unchosen-value-decay', 'stage-two-perseveration']",partial,Dual Stickiness Hybrid Model with Decay,"['learning_rate', 'learning_rate_2', 'beta', 'beta_2', 'w', 'lambd', 'perseveration']",495.5818873885879,483.28564789276385
34,"[np.float64(0.0244293201806271), np.float64(9.723547147007336), np.float64(10.0), np.float64(0.4261556729298051), np.float64(0.2854928758893574), np.float64(0.073197964395893)]",['learning_rate' 'beta_1' 'beta_2' 'w' 'stick_win' 'stick_loss'],6,False,"['learning_rate', 'beta', 'beta_2', 'w']","['stick_win', 'stick_loss']","['model-based-learning', 'model-free-learning', 'hybrid-value-estimation']",['reward-dependent-stickiness'],partial,Hybrid Model with Reward-Dependent Stickiness,"['learning_rate', 'learning_rate_2', 'beta', 'beta_2', 'w', 'lambd', 'perseveration']",261.75895325333863,254.98474577044965
35,"[np.float64(0.6426404981788192), np.float64(2.0399809624491776), np.float64(1.0)]",['learning_rate' 'beta' 'eligibility_trace'],3,False,"['learning_rate', 'beta', 'beta_2', 'lambd', 'perseveration']",['decay'],"['model-free-learning', 'eligibility-traces', 'choice-perseveration']",['value-decay'],partial,Model-Free RL with Perseverance and Value Decay,"['learning_rate', 'learning_rate_2', 'beta', 'beta_2', 'w', 'lambd', 'perseveration']",491.3304954895365,462.73854917209115
36,"[np.float64(0.5197917424916639), np.float64(0.2494719357801693), np.float64(10.0), np.float64(10.0), np.float64(0.0), np.float64(0.1469627215203881), np.float64(1.0), np.float64(0.2563461021300984)]",['lr_pos' 'lr_neg' 'beta_1' 'beta_2' 'w' 'p' 'lam' 'decay'],8,False,"['beta', 'beta_2', 'w', 'perseveration', 'lambd']","['lr_pos', 'lr_neg', 'decay']","['model-based-learning', 'model-free-learning', 'eligibility-traces', 'choice-perseveration']","['asymmetric-learning-rates', 'value-decay']",partial,Asymmetric Learning Decay Model,"['learning_rate', 'learning_rate_2', 'beta', 'beta_2', 'w', 'lambd', 'perseveration']",386.4734622975251,349.63731763891235
37,"[np.float64(0.7402350279675785), np.float64(9.260129453790196), np.float64(3.524947067971168), np.float64(0.0), np.float64(0.8623735585217513), np.float64(0.0), np.float64(1.0)]",['learning_rate' 'beta_1' 'beta_2' 'w' 'lam' 'stickiness' 'decay'],7,False,"['learning_rate', 'beta', 'beta_2', 'w', 'lambd', 'perseveration']",['decay'],"['hybrid-learning', 'eligibility-traces', 'choice-perseveration', 'separate-stage-temperatures']",['value-decay'],partial,"Separate Beta (Stage 1 vs Stage 2) model with Eligibility Trace, Stickiness, and Decay","['learning_rate', 'learning_rate_2', 'beta', 'beta_2', 'w', 'lambd', 'perseveration']",322.92733134044107,270.1613746613308
38,"[np.float64(0.9969516923905692), np.float64(1.0), np.float64(10.0), np.float64(2.006035885183829), np.float64(0.1926540910156987), np.float64(0.6476027858380976)]",['lr_val' 'lr_trans' 'beta_1' 'beta_2' 'w' 'decay'],6,False,"['learning_rate', 'beta', 'beta_2', 'w']","['lr_trans', 'decay']","['model-free-learning', 'hybrid-value-estimation', 'softmax-action-selection']","['dynamic-transition-learning', 'passive-value-decay']",partial,Dynamic Transition Learning Hybrid Model with Decay,"['learning_rate', 'learning_rate_2', 'beta', 'beta_2', 'w', 'lambd', 'perseveration']",419.61136829159153,378.5963621777179
39,"[np.float64(0.732148465342431), np.float64(10.0), np.float64(1.0583933007508877), np.float64(0.2271295235692873), np.float64(0.1770488659878618), np.float64(0.0152958541884436)]",['lr' 'beta_1' 'beta_2' 'w' 'stick' 'phi'],6,False,"['learning_rate', 'beta', 'beta_2', 'w', 'perseveration']",['phi'],"['hybrid-learning', 'choice-perseveration']",['uncertainty-based-exploration'],partial,Hybrid Model-Based / Model-Free learner with Uncertainty-Based Exploration,"['learning_rate', 'learning_rate_2', 'beta', 'beta_2', 'w', 'lambd', 'perseveration']",397.4487043827186,378.9969165207103
40,"[np.float64(1.0), np.float64(0.7612291665262352), np.float64(1.0), np.float64(2.9505681726131865)]",['learning_rate' 'beta' 'decay' 'perseverance'],4,False,"['learning_rate', 'beta', 'perseveration']",['decay'],"['model-free-learning', 'choice-perseveration']",['memory-decay'],partial,MF Q-Learning with Forgetting (Decay) and Perseverance,"['learning_rate', 'learning_rate_2', 'beta', 'beta_2', 'w', 'lambd', 'perseveration']",370.2672505628743,351.76699021340744
41,"[np.float64(0.9725065670908182), np.float64(1.0), np.float64(0.8762708884373211), np.float64(1.8406573226308007), np.float64(3.221241852190159), np.float64(0.0)]",['alpha_pos' 'alpha_neg' 'beta_1' 'beta_2' 'sticky_win' 'sticky_lose'],6,False,"['beta', 'beta_2']","['alpha_pos', 'alpha_neg', 'sticky_win', 'sticky_lose']",['model-free-learning'],"['risk-sensitive-learning', 'outcome-dependent-stickiness']",partial,Risk-Sensitive Q-Learning with Outcome-Dependent Stickiness,"['learning_rate', 'learning_rate_2', 'beta', 'beta_2', 'w', 'lambd', 'perseveration']",409.3410998263276,386.4355019320361
42,"[np.float64(0.9223684642604988), np.float64(10.0), np.float64(0.1037103228226197), np.float64(1.0), np.float64(0.0), np.float64(0.9449628462832524)]",['learning_rate' 'beta' 'w' 'lambda_coef' 'stickiness' 'coupling_factor'],6,False,"['learning_rate', 'beta', 'w', 'lambd', 'perseveration']",['coupling_factor'],"['model-based-weighting', 'eligibility-traces', 'choice-perseveration']",['coupled-value-updating'],partial,Coupled Alien Learning Model,"['learning_rate', 'learning_rate_2', 'beta', 'beta_2', 'w', 'lambd', 'perseveration']",389.6452395409176,286.75223145221906
43,"[np.float64(0.2596954887551749), np.float64(0.60916234550815), np.float64(0.5976965953918832), np.float64(10.0), np.float64(2.124522437193306), np.float64(0.1580044648038094), np.float64(0.2704653227471779)]",['lr_pos' 'lr_neg' 'decay' 'beta_1' 'beta_2' 'w' 'pers'],7,False,"['beta', 'beta_2', 'w', 'perseveration']","['lr_pos', 'lr_neg', 'decay']","['model-based-learning', 'choice-perseveration']","['asymmetric-learning-rates', 'value-decay']",partial,Asymmetric Learning Rates with Decay and MB/MF,"['learning_rate', 'learning_rate_2', 'beta', 'beta_2', 'w', 'lambd', 'perseveration']",441.0912385821735,423.2489278175843
44,"[np.float64(0.1909122618424745), np.float64(2.0272549506258715), np.float64(1.0), np.float64(0.1878711452807768)]",['lr' 'beta' 'w' 'bias'],4,False,"['learning_rate', 'beta', 'w']",['bias'],"['model-based-learning', 'model-free-learning', 'hybrid-value-weighting']",['static-spatial-bias'],partial,Hybrid learner with a static Spatial Bias (Side Bias),"['learning_rate', 'learning_rate_2', 'beta', 'beta_2', 'w', 'lambd', 'perseveration']",524.198844100239,513.0287924283452
