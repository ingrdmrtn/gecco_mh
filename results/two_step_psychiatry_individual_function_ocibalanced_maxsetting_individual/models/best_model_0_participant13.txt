def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model 3: Outcome-Dependent Beta Model-Free Controller.
    
    A Model-Free agent where the inverse temperature (beta) changes based on 
    the previous reward. This allows the agent to be more deterministic after 
    wins (high confidence) and more exploratory after losses (low confidence).
    
    Parameters:
    learning_rate: [0, 1] - Learning rate for value updates.
    beta_win: [0, 10] - Inverse temperature applied after a rewarded trial.
    beta_loss: [0, 10] - Inverse temperature applied after an unrewarded trial.
    perseveration: [-3, 3] - Stickiness to the previous Stage 1 choice.
    """
    learning_rate, beta_win, beta_loss, perseveration = model_parameters
    n_trials = len(action_1)
    
    q_stage1 = np.zeros(2) + 0.5
    q_stage2 = np.zeros((2, 2)) + 0.5
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1
    prev_reward = 0 # Assume neutral/loss start or could be parameter
    
    for t in range(n_trials):

        current_beta = beta_win if prev_reward == 1 else beta_loss

        logits_1 = current_beta * q_stage1
        if prev_a1 != -1:
            logits_1[prev_a1] += perseveration
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[t] = probs_1[action_1[t]]
        
        a1 = action_1[t]
        s_idx = state[t]


        logits_2 = current_beta * q_stage2[s_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[t] = probs_2[action_2[t]]
        
        a2 = action_2[t]
        r = reward[t]


        delta_2 = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += learning_rate * delta_2

        delta_1 = q_stage2[s_idx, a2] - q_stage1[a1]
        q_stage1[a1] += learning_rate * delta_1
        
        prev_a1 = a1
        prev_reward = r

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss