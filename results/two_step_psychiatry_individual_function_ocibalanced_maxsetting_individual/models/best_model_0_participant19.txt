def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Model 1: Asymmetric Learning Q-Learning with Forgetting.
    
    A Model-Free learner that distinguishes between positive and negative 
    reward prediction errors (RPE). It also decays the Q-values of unchosen 
    actions on every trial (forgetting), which helps in drifting environments.

    Parameters:
    - alpha_pos: [0, 1] Learning rate for positive RPE (better than expected).
    - alpha_neg: [0, 1] Learning rate for negative RPE (worse than expected).
    - beta: [0, 10] Inverse temperature (exploration/exploitation balance).
    - forget_rate: [0, 1] Decay rate for unchosen actions (Q_unchosen -> 0).
    """
    alpha_pos, alpha_neg, beta, forget_rate = model_parameters
    n_trials = len(action_1)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for t in range(n_trials):

        logits_1 = beta * q_stage1
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        s_idx = state[t]
        logits_2 = beta * q_stage2[s_idx]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]
        
        r = reward[t]

        pe2 = r - q_stage2[s_idx, a2]
        if pe2 >= 0:
            q_stage2[s_idx, a2] += alpha_pos * pe2
        else:
            q_stage2[s_idx, a2] += alpha_neg * pe2

        unchosen_a2 = 1 - a2
        q_stage2[s_idx, unchosen_a2] *= (1.0 - forget_rate)


        target_val = q_stage2[s_idx, a2]
        pe1 = target_val - q_stage1[a1]
        if pe1 >= 0:
            q_stage1[a1] += alpha_pos * pe1
        else:
            q_stage1[a1] += alpha_neg * pe1

        unchosen_a1 = 1 - a1
        q_stage1[unchosen_a1] *= (1.0 - forget_rate)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss