def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Unchosen Decay Model.
    
    This model implements a 'use-it-or-lose-it' forgetting mechanism. Unlike global decay 
    where all values deteriorate, here only the Q-values of unchosen options (both unchosen 
    aliens on the current planet and aliens on the unvisited planet) decay towards 0. 
    The chosen option is updated via standard reinforcement learning without decay interference.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for chosen options.
    - beta_1: [0, 10] Inverse temperature for Stage 1 (Spaceships).
    - beta_2: [0, 10] Inverse temperature for Stage 2 (Aliens).
    - w: [0, 1] Weighting parameter (0 = Model-Free, 1 = Model-Based).
    - stickiness: [0, 5] Perseveration bonus for repeating the Stage 1 choice.
    - decay_unchosen: [0, 1] Decay rate applied only to unchosen Stage 2 options.
    """
    learning_rate, beta_1, beta_2, w, stickiness, decay_unchosen = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1 or reward[trial] == -1:
            last_action_1 = -1
            continue

        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        state_idx = state[trial]
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]



        mask = np.ones((2, 2), dtype=bool)

        mask[state_idx, action_2[trial]] = False

        q_stage2_mf[mask] *= (1 - decay_unchosen)

        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        last_action_1 = action_1[trial]

    eps = 1e-10
    mask = (p_choice_1 > 0) & (p_choice_2 > 0)
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss