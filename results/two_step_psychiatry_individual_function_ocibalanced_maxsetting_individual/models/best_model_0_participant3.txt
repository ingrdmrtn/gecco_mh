def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Dual-Stage Stickiness Model.
    
    This model incorporates choice perseveration (stickiness) in both the first stage 
    (spaceship choice) and the second stage (alien choice). This accounts for 
    motor repetition or habit formation at both decision points.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate for action values.
    beta: [0, 10] - Inverse temperature for softmax choice.
    w: [0, 1] - Weight for Model-Based values.
    stick_1: [0, 10] - Stickiness bonus for repeating the previous Stage 1 choice.
    stick_2: [0, 10] - Stickiness bonus for repeating the previous Stage 2 choice.
    """
    learning_rate, beta, w, stick_1, stick_2 = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_a1 = -1
    prev_a2 = -1 

    for trial in range(n_trials):
        if action_1[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf

        logits_1 = beta * q_net
        if prev_a1 != -1:
            logits_1[prev_a1] += stick_1
            
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]


        logits_2 = beta * q_stage2_mf[state_idx]
        if prev_a2 != -1:
            logits_2[prev_a2] += stick_2
            
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        chosen_a1 = action_1[trial]
        chosen_a2 = action_2[trial]
        
        delta_stage1 = q_stage2_mf[state_idx, chosen_a2] - q_stage1_mf[chosen_a1]
        q_stage1_mf[chosen_a1] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, chosen_a2]
        q_stage2_mf[state_idx, chosen_a2] += learning_rate * delta_stage2
        
        prev_a1 = chosen_a1
        prev_a2 = chosen_a2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss