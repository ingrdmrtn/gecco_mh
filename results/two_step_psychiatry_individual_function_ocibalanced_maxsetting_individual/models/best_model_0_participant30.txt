def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Q-Learning with Stage-Specific Perseveration.
    Includes separate perseveration parameters for the first stage (Spaceship choice)
    and the second stage (Alien choice within a planet), capturing different 
    stickiness behaviors at each level.
    
    Parameters:
    learning_rate: Learning rate [0,1]
    beta: Inverse temperature [0,10]
    lambda_eligibility: Eligibility trace decay [0,1]
    decay: Decay rate for Q-values [0,1]
    pers_s1: Perseveration for Stage 1 [0,5]
    pers_s2: Perseveration for Stage 2 [0,5]
    """
    learning_rate, beta, lambda_eligibility, decay, pers_s1, pers_s2 = model_parameters
    n_trials = len(action_1)
    
    q_s1 = np.full(2, 0.5)
    q_s2 = np.full((2, 2), 0.5)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1
    last_action_2 = np.full(2, -1) # Track last action for each state (planet) separately
    
    for trial in range(n_trials):
        if action_1[trial] == -1:
            last_action_1 = -1
            last_action_2 = np.full(2, -1)
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        q_s1 = q_s1 * (1 - decay) + 0.5 * decay
        q_s2 = q_s2 * (1 - decay) + 0.5 * decay

        q_net_s1 = q_s1.copy()
        if last_action_1 != -1:
            q_net_s1[last_action_1] += pers_s1
            
        exp_q1 = np.exp(beta * q_net_s1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        s_idx = state[trial]
        q_net_s2 = q_s2[s_idx].copy()

        if last_action_2[s_idx] != -1:
            q_net_s2[last_action_2[s_idx]] += pers_s2
            
        exp_q2 = np.exp(beta * q_net_s2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        delta_1 = q_s2[s_idx, action_2[trial]] - q_s1[action_1[trial]]
        q_s1[action_1[trial]] += learning_rate * delta_1
        
        delta_2 = reward[trial] - q_s2[s_idx, action_2[trial]]
        q_s2[s_idx, action_2[trial]] += learning_rate * delta_2
        
        q_s1[action_1[trial]] += learning_rate * lambda_eligibility * delta_2
        
        last_action_1 = action_1[trial]
        last_action_2[s_idx] = action_2[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss