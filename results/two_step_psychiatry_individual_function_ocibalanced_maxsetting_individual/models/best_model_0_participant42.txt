def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Coupled Alien Learning Model.
    
    This model assumes that learning about an alien in one state (planet) 
    partially transfers to the same alien in the other state. This is controlled 
    by the 'coupling_factor'.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for Q-value updates.
    - beta: [0, 10] Inverse temperature for softmax choice policy.
    - w: [0, 1] Weighting parameter for Model-Based (1) vs Model-Free (0) control.
    - lambda_coef: [0, 1] Eligibility trace decay parameter.
    - stickiness: [0, 10] Choice perseverance bonus for Stage 1.
    - coupling_factor: [0, 1] Degree to which update in one state transfers to the other state.
    """
    learning_rate, beta, w, lambda_coef, stickiness, coupling_factor = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action (Alien)

    last_action_1 = -1

    for trial in range(n_trials):

        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_s1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net_s1
        if last_action_1 != -1:
            logits_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_action_1 = action_1[trial]
        state_idx = state[trial]

        q_s2 = q_stage2_mf[state_idx]
        logits_2 = beta * q_s2
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        chosen_a2 = action_2[trial]

        delta_stage1 = q_stage2_mf[state_idx, chosen_a2] - q_stage1_mf[action_1[trial]]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, chosen_a2]
        
        q_stage1_mf[action_1[trial]] += learning_rate * (delta_stage1 + lambda_coef * delta_stage2)

        q_stage2_mf[state_idx, chosen_a2] += learning_rate * delta_stage2



        other_state = 1 - state_idx
        delta_coupled = reward[trial] - q_stage2_mf[other_state, chosen_a2]
        q_stage2_mf[other_state, chosen_a2] += learning_rate * coupling_factor * delta_coupled

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss