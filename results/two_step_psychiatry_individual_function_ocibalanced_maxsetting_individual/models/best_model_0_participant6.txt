def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Learner with Separate Stage Learning Rates and Value Decay.
    
    Differentiates plasticity between the two stages (spaceship choice vs alien choice)
    and incorporates value decay for unchosen options to handle non-stationarity.
    
    Parameters:
    - alpha_1: [0, 1] Learning rate for Stage 1.
    - alpha_2: [0, 1] Learning rate for Stage 2.
    - decay_rate: [0, 1] Decay rate for unchosen Q-values.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - stickiness: [0, 5] Bonus for repeating the last Stage 1 choice.
    """
    alpha_1, alpha_2, decay_rate, beta_1, beta_2, stickiness = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        q_net_1 = q_stage1_mf.copy()
        if last_action_1 != -1:
            q_net_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta_1 * (q_net_1 - np.max(q_net_1)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_action_1 = action_1[trial]
        state_idx = state[trial]

        exp_q2 = np.exp(beta_2 * (q_stage2_mf[state_idx] - np.max(q_stage2_mf[state_idx])))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]


        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += alpha_1 * delta_stage1

        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += alpha_2 * delta_stage2


        unchosen_alien = 1 - action_2[trial]
        q_stage2_mf[state_idx, unchosen_alien] *= (1 - decay_rate)

        unchosen_planet = 1 - state_idx
        q_stage2_mf[unchosen_planet, 0] *= (1 - decay_rate)
        q_stage2_mf[unchosen_planet, 1] *= (1 - decay_rate)

        unchosen_spaceship = 1 - action_1[trial]
        q_stage1_mf[unchosen_spaceship] *= (1 - decay_rate)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss