Here are three cognitive models implemented as Python functions, designed to capture the decision-making patterns observed in the participant data.

### Model 1: Hybrid Model-Based / Model-Free RL
This model implements the classic "two-step" theory (Daw et al., 2011), hypothesizing that the participant combines a Model-Based (MB) strategy (planning using the transition matrix) and a Model-Free (MF) strategy (learning from direct reward history). The `w` parameter controls the balance between these systems.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Reinforcement Learning Model.
    
    This model assumes the participant uses a weighted combination of:
    1. A Model-Based system that uses the known transition matrix and Stage 2 values to plan.
    2. A Model-Free system that learns Stage 1 values via TD-learning with eligibility traces.
    
    Parameters:
    - learning_rate: [0, 1] Rate at which Q-values are updated.
    - beta_1: [0, 10] Inverse temperature (softness) for Stage 1 softmax choice.
    - beta_2: [0, 10] Inverse temperature (softness) for Stage 2 softmax choice.
    - w_mb: [0, 1] Weight of the Model-Based system (0 = Pure MF, 1 = Pure MB).
    - lambda_eligibility: [0, 1] Eligibility trace decay, linking Stage 2 reward to Stage 1 choice.
    """
    learning_rate, beta_1, beta_2, w_mb, lambda_eligibility = model_parameters
    n_trials = len(action_1)
    
    # Transition matrix: Row 0 (Act 0) -> [0.7 to S0, 0.3 to S1], Row 1 (Act 1) -> [0.3 to S0, 0.7 to S1]
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Initialize Q-values
    # Stage 1 MF values: [Value(Act0), Value(Act1)]
    q_stage1_mf = np.zeros(2)
    # Stage 2 values: [Planet][Alien] -> [State][Action]
    q_stage2 = np.zeros((2, 2)) # 0.5 initialization can be used, but 0 is standard for this template
    
    log_likelihood = 0.0
    eps = 1e-10

    for t in range(n_trials):
        a1 = int(action_1[t])
        s2 = int(state[t])
        a2 = int(action_2[t])
        r = int(reward[t])

        # --- Stage 1 Decision ---
        # Model-Based Value: Expected max value of next stage
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Value: Weighted sum of MB and MF
        q_net_s1 = w_mb * q_stage1_mb + (1 - w_mb) * q_stage1_mf
        
        # Softmax probability for Stage 1
        probs_1 = np.exp(beta_1 * q_net_s1)
        probs_1 /= np.sum(probs_1)
        
        # Accumulate likelihood
        log_likelihood += np.log(probs_1[a1] + eps)

        # --- Stage 2 Decision ---
        # Standard Q-learning (Bandit) for Stage 2
        probs_2 = np.exp(beta_2 * q_stage2[s2])
        probs_2 /= np.sum(probs_2)
        
        log_likelihood += np.log(probs_2[a2] + eps)

        # --- Learning / Updates ---
        
        # Prediction Error 1: Value of state reached - Value of action chosen
        # Note: In this task, Stage 1 has no immediate reward, so reward is 0.
        # We use the value of the chosen Stage 2 action as the target (SARSA-like) or max (Q-learning).
        # Standard 2-step implementations often use max_q_stage2[s2] or q_stage2[s2, a2].
        # Using q_stage2[s2, a2] (SARSA style) is common for the trace.
        delta_1 = q_stage2[s2, a2] - q_stage1_mf[a1]
        
        # Prediction Error 2: Reward received - Value of Stage 2 action
        delta_2 = r - q_stage2[s2, a2]
        
        # Update Stage 1 MF Q-values (TD(lambda))
        q_stage1_mf[a1] += learning_rate * (delta_1 + lambda_eligibility * delta_2)
        
        # Update Stage 2 Q-values
        q_stage2[s2, a2] += learning_rate * delta_2

    return -log_likelihood
```

### Model 2: Model-Free with Perseveration
This model hypothesizes that the participant relies primarily on Model-Free learning (ignoring the transition structure) but exhibits significant "stickiness" or perseveration (a tendency to repeat the previous choice regardless of reward). This addresses the long streaks of identical choices observed in the data (e.g., trials 82-119).

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Reinforcement Learning with Perseveration (Stickiness).
    
    This model assumes the participant does not use the transition matrix (Pure MF).
    It includes a 'perseveration' parameter to model the tendency to repeat
    Stage 1 choices, which is prominent in the participant's data streaks.
    
    Parameters:
    - learning_rate: [0, 1] Rate at which Q-values are updated.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - lambda_eligibility: [0, 1] Eligibility trace decay.
    - perseveration: [0, 10] Bonus added to the logit of the previously chosen action.
    """
    learning_rate, beta_1, beta_2, lambda_eligibility, perseveration = model_parameters
    n_trials = len(action_1)
    
    q_stage1_mf = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    log_likelihood = 0.0
    eps = 1e-10
    
    last_action_1 = -1 # No previous action for the first trial

    for t in range(n_trials):
        a1 = int(action_1[t])
        s2 = int(state[t])
        a2 = int(action_2[t])
        r = int(reward[t])

        # --- Stage 1 Decision ---
        # Base logits from MF value
        logits_1 = beta_1 * q_stage1_mf
        
        # Add perseveration bonus if applicable
        if last_action_1 != -1:
            logits_1[last_action_1] += perseveration
            
        probs_1 = np.exp(logits_1 - np.max(logits_1)) # Subtract max for numerical stability
        probs_1 /= np.sum(probs_1)
        
        log_likelihood += np.log(probs_1[a1] + eps)
        
        # Update last action
        last_action_1 = a1

        # --- Stage 2 Decision ---
        probs_2 = np.exp(beta_2 * q_stage2[s2])
        probs_2 /= np.sum(probs_2)
        log_likelihood += np.log(probs_2[a2] + eps)

        # --- Learning ---
        delta_1 = q_stage2[s2, a2] - q_stage1_mf[a1]
        delta_2 = r - q_stage2[s2, a2]
        
        q_stage1_mf[a1] += learning_rate * (delta_1 + lambda_eligibility * delta_2)
        q_stage2[s2, a2] += learning_rate * delta_2

    return -log_likelihood
```

### Model 3: Model-Based with Perseveration
This model tests the hypothesis that the participant is actually Model-Based (planning using the transition structure) but their behavior is masked by strong habit/perseveration. It removes the Model-Free component entirely to contrast with Model 2.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Based Reinforcement Learning with Perseveration.
    
    This model assumes the participant plans using the transition matrix (Pure MB)
    but also exhibits choice stickiness. It does not maintain separate MF values
    for Stage 1 choices.
    
    Parameters:
    - learning_rate: [0, 1] Rate at which Stage 2 Q-values are updated.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - perseveration: [0, 10] Bonus added to the logit of the previously chosen action.
    """
    learning_rate, beta_1, beta_2, perseveration = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage2 = np.zeros((2, 2))
    
    log_likelihood = 0.0
    eps = 1e-10
    
    last_action_1 = -1

    for t in range(n_trials):
        a1 = int(action_1[t])
        s2 = int(state[t])
        a2 = int(action_2[t])
        r = int(reward[t])

        # --- Stage 1 Decision ---
        # Calculate Model-Based values
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        logits_1 = beta_1 * q_stage1_mb
        
        # Add perseveration bonus
        if last_action_1 != -1:
            logits_1[last_action_1] += perseveration
            
        probs_1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 /= np.sum(probs_1)
        
        log_likelihood += np.log(probs_1[a1] + eps)
        last_action_1 = a1

        # --- Stage 2 Decision ---
        probs_2 = np.exp(beta_2 * q_stage2[s2])
        probs_2 /= np.sum(probs_2)
        log_likelihood += np.log(probs_2[a2] + eps)

        # --- Learning ---
        # Only Stage 2 values need updating in a Pure MB model
        delta_2 = r - q_stage2[s2, a2]
        q_stage2[s2, a2] += learning_rate * delta_2

    return -log_likelihood
```