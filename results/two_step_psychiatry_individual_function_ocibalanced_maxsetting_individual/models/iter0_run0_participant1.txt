Here are three cognitive models based on the participant data and the two-step task structure.

### Model 1: Hybrid Model-Based / Model-Free Learner
This model (often called the "Daw" model) assumes the participant uses a mixture of two strategies:
1.  **Model-Based (MB):** Uses the known transition probabilities (0.7/0.3) to calculate the value of spaceships based on the values of the planets.
2.  **Model-Free (MF):** Learns the value of spaceships directly from reward history, ignoring the transition structure.

This model is chosen because while the participant shows signs of habit (repeating choices after rare-transition rewards), a pure model-free algorithm rarely captures human variance fully. The mixing weight `w` allows the model to find the optimal balance for this specific participant.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based and Model-Free learner.
    
    Parameters:
    learning_rate: [0, 1] - Rate at which Q-values are updated.
    beta_1: [0, 10] - Inverse temperature for Stage 1 (Spaceship) choice.
    beta_2: [0, 10] - Inverse temperature for Stage 2 (Alien) choice.
    w: [0, 1] - Weighting parameter (0 = Pure MF, 1 = Pure MB).
    """
    learning_rate, beta_1, beta_2, w = model_parameters
    n_trials = len(action_1)
    
    # Transition matrix: [0->0, 0->1], [1->0, 1->1]
    # Spaceship 0 goes to Planet 0 (0.7), Spaceship 1 goes to Planet 1 (0.7)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Q-values
    q_stage1_mf = np.zeros(2)      # Model-Free values for Spaceships
    q_stage2 = np.zeros((2, 2))    # Values for Aliens (Planet x Alien)
    
    log_loss = 0.0
    eps = 1e-10

    for trial in range(n_trials):
        a1 = action_1[trial]
        s_idx = state[trial]
        a2 = action_2[trial]
        r = reward[trial]

        # Handle missing/timeout data (e.g., trial 102)
        if a2 == -1:
            continue

        # --- STAGE 1 CHOICE ---
        # 1. Calculate Model-Based values
        # Max value available on each planet
        max_q_stage2 = np.max(q_stage2, axis=1) 
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Calculate Net Value (Hybrid)
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # 3. Probability of Choice 1
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + eps)

        # --- STAGE 2 CHOICE ---
        # Probability of Choice 2 given the state (planet)
        exp_q2 = np.exp(beta_2 * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[a2] + eps)

        # --- LEARNING ---
        # Prediction Error 1 (SARSA-style for MF path)
        delta_1 = q_stage2[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_1
        
        # Prediction Error 2 (Reward prediction error)
        delta_2 = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += learning_rate * delta_2
        
        # Note: In the standard hybrid model, the stage 2 update 
        # also drives a second-stage update to stage 1 MF values (lambda=1 implied)
        q_stage1_mf[a1] += learning_rate * delta_2

    return log_loss
```

### Model 2: Model-Free with Eligibility Traces (TD-Lambda)
This model hypothesizes that the participant is a pure "habitual" learner who does not use the transition matrix (Model-Free). However, it includes an eligibility trace parameter `lam` (lambda). This parameter controls how much the final reward reinforces the *first* choice (Spaceship).
*   If `lam` is high, a reward from an alien strongly reinforces the spaceship chosen, even if the transition was rare.
*   This fits the data where the participant often repeats the spaceship choice (Win-Stay) even after a rare transition (e.g., Trial 5->6), suggesting they credit the spaceship for the coin, not the transition.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free learner with Eligibility Traces (TD-Lambda).
    
    Parameters:
    learning_rate: [0, 1] - Learning rate.
    beta: [0, 10] - Inverse temperature (shared for both stages for parsimony).
    lam: [0, 1] - Eligibility trace parameter (credit assignment to stage 1).
    """
    learning_rate, beta, lam = model_parameters
    n_trials = len(action_1)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2)) # State x Action
    
    log_loss = 0.0
    eps = 1e-10

    for trial in range(n_trials):
        a1 = action_1[trial]
        s_idx = state[trial]
        a2 = action_2[trial]
        r = reward[trial]

        if a2 == -1:
            continue

        # --- STAGE 1 CHOICE ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + eps)

        # --- STAGE 2 CHOICE ---
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[a2] + eps)

        # --- LEARNING ---
        # 1. Stage 1 Prediction Error (TD error based on Stage 2 Q-value)
        delta_1 = q_stage2[s_idx, a2] - q_stage1[a1]
        q_stage1[a1] += learning_rate * delta_1
        
        # 2. Stage 2 Prediction Error (TD error based on Reward)
        delta_2 = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += learning_rate * delta_2
        
        # 3. Eligibility Trace Update
        # The Stage 2 error (delta_2) propagates back to Stage 1 
        # scaled by lambda.
        q_stage1[a1] += learning_rate * lam * delta_2

    return log_loss
```

### Model 3: Model-Free with Choice Perseveration
The participant data shows significant "streaks" where they choose the same spaceship (Spaceship 0) for many trials in a row (e.g., trials 60-113), and then switch and stick to the other (trials 114-160). This suggests a "stickiness" or perseveration bias: they prefer to repeat their last motor action regardless of the reward value.
This model adds a `stickiness` parameter to the Model-Free Q-values.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free learner with Choice Perseveration (Stickiness).
    
    Parameters:
    learning_rate: [0, 1] - Learning rate.
    beta_1: [0, 10] - Inverse temperature for Stage 1.
    beta_2: [0, 10] - Inverse temperature for Stage 2.
    stickiness: [0, 5] - Bonus added to the Q-value of the previously chosen action.
    """
    learning_rate, beta_1, beta_2, stickiness = model_parameters
    n_trials = len(action_1)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    last_action_1 = -1 # Initialize with no previous action
    
    log_loss = 0.0
    eps = 1e-10

    for trial in range(n_trials):
        a1 = action_1[trial]
        s_idx = state[trial]
        a2 = action_2[trial]
        r = reward[trial]
        
        if a2 == -1:
            continue

        # --- STAGE 1 CHOICE ---
        # Add stickiness bonus to the previously chosen action
        q_net_1 = q_stage1.copy()
        if last_action_1 != -1:
            q_net_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta_1 * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + eps)
        
        # Update tracker
        last_action_1 = a1

        # --- STAGE 2 CHOICE ---
        exp_q2 = np.exp(beta_2 * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[a2] + eps)

        # --- LEARNING ---
        # Standard SARSA-style updates
        delta_1 = q_stage2[s_idx, a2] - q_stage1[a1]
        q_stage1[a1] += learning_rate * delta_1
        
        delta_2 = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += learning_rate * delta_2
        
        # Direct reinforcement of stage 1 from reward (simple MF assumption)
        q_stage1[a1] += learning_rate * delta_2

    return log_loss
```