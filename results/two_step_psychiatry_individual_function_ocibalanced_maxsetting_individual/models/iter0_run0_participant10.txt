Here are the three cognitive models based on the participant's data and the two-step task structure.

### Model 1: Hybrid Model-Based/Model-Free with Perseveration
**rationale:** The participant shows significant "stickiness" to specific spaceships (e.g., sticking with Spaceship 1 for 40+ trials, then Spaceship 0 for 20 trials), even when rewards are sparse. This suggests a "perseveration" bonus where repeating the previous choice is intrinsically rewarding or habitual. This model combines the standard trade-off between Model-Based (planning) and Model-Free (habit) learning, plus a parameter to account for this choice repetition.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based/Model-Free learner with Choice Perseveration.
    
    Parameters:
    lr: [0, 1] Learning rate for value updates.
    beta: [0, 10] Inverse temperature (softmax sensitivity).
    w: [0, 1] Weighting parameter (0 = Pure MF, 1 = Pure MB).
    perseveration: [0, 5] Bonus added to the Q-value of the previously chosen action.
    """
    lr, beta, w, perseveration = model_parameters
    n_trials = len(action_1)
    
    # Fixed transition matrix for MB planning (70% common, 30% rare)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)      # Model-free values for Stage 1 (Spaceships)
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for Stage 2 (Aliens)
    
    prev_action_1 = -1 # Track previous choice for perseveration

    for trial in range(n_trials):
        # Handle missing data
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 0.5 # Dummy value
            p_choice_2[trial] = 0.5 # Dummy value
            continue

        # --- Stage 1 Policy ---
        # 1. Calculate MB values: Transition probability * Max Stage 2 Value
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Combine MB and MF values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # 3. Add Perseveration Bonus
        if prev_action_1 != -1:
            q_net[prev_action_1] += perseveration
            
        # 4. Softmax probability
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial] # 0 or 1 (Planet X or Y)

        # --- Stage 2 Policy ---
        # Standard Softmax on Stage 2 Q-values
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        # Prediction errors
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Update Stage 1 MF values (TD(0))
        q_stage1_mf[action_1[trial]] += lr * delta_stage1
        
        # Update Stage 2 MF values
        q_stage2_mf[state_idx, action_2[trial]] += lr * delta_stage2
        
        prev_action_1 = action_1[trial]

    eps = 1e-10
    # Mask out trials where probability is 0.5 due to missing data (optional, but good practice is to ignore them in sum)
    valid_trials = (action_1 != -1) & (state != -1) & (action_2 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```

### Model 2: Model-Free with Eligibility Traces (TD-Lambda)
**rationale:** The participant might not be using the transition structure (MB) at all. Instead, their behavior might be driven by direct reinforcement of the first choice by the final reward. In standard TD(0), Stage 1 is only updated by the value of Stage 2. With an eligibility trace ($\lambda$), the Stage 1 choice is updated directly by the Stage 2 reward outcome. This helps explain why the participant persists with a spaceship if they eventually get gold, regardless of the intermediate planet transition.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free learner with Eligibility Traces (TD-Lambda).
    
    Parameters:
    lr: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    lambda_param: [0, 1] Eligibility trace decay. 
                  0 = TD(0) (Stage 1 learned from Stage 2 value).
                  1 = Monte Carlo (Stage 1 learned from Reward).
    """
    lr, beta, lambda_param = model_parameters
    n_trials = len(action_1)
    
    # No transition matrix needed for pure MF
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
            continue

        # --- Stage 1 Policy (Pure MF) ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # Prediction Error 1: Difference between Stage 2 Value and Stage 1 Value
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        # Prediction Error 2: Difference between Reward and Stage 2 Value
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Update Stage 1: Uses delta_1 AND a portion of delta_2 (eligibility trace)
        # If lambda=1, Stage 1 is effectively updated by (Reward - Q_stage1)
        q_stage1_mf[action_1[trial]] += lr * (delta_stage1 + lambda_param * delta_stage2)
        
        # Update Stage 2: Standard TD update
        q_stage2_mf[state_idx, action_2[trial]] += lr * delta_stage2
        

    eps = 1e-10
    valid_trials = (action_1 != -1) & (state != -1) & (action_2 != -1)
    # Using boolean indexing to only sum valid trials
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```

### Model 3: Asymmetric Learning Rate (Confirmation Bias)
**rationale:** The participant persists with choices (e.g., Spaceship 1) even after receiving multiple 0 outcomes (trials 1-10, 39-49). This suggests they might be learning differently from positive outcomes (rewards) versus negative outcomes (omissions). This model implements asymmetric learning rates: `alpha_pos` for positive prediction errors (better than expected) and `alpha_neg` for negative prediction errors (worse than expected). A low `alpha_neg` would explain why they don't "unlearn" a choice quickly after failure.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free learner with Asymmetric Learning Rates (Confirmation Bias).
    
    Parameters:
    alpha_pos: [0, 1] Learning rate for positive prediction errors (wins).
    alpha_neg: [0, 1] Learning rate for negative prediction errors (losses).
    beta: [0, 10] Inverse temperature.
    """
    alpha_pos, alpha_neg, beta = model_parameters
    n_trials = len(action_1)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
            continue

        # --- Stage 1 Policy ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        # Select learning rate based on sign of prediction error
        if delta_stage1 >= 0:
            eff_lr_1 = alpha_pos
        else:
            eff_lr_1 = alpha_neg
            
        q_stage1_mf[action_1[trial]] += eff_lr_1 * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Select learning rate for second stage
        if delta_stage2 >= 0:
            eff_lr_2 = alpha_pos
        else:
            eff_lr_2 = alpha_neg

        q_stage2_mf[state_idx, action_2[trial]] += eff_lr_2 * delta_stage2
        

    eps = 1e-10
    valid_trials = (action_1 != -1) & (state != -1) & (action_2 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```