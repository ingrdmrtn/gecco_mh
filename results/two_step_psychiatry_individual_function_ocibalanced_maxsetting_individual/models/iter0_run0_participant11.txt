Here are three cognitive models designed to explain the participant's behavior in the two-step task.

### Model 1: Hybrid Model with Choice Stickiness (Perseveration)
**Rationale:** The participant data shows long "streaks" of choosing the same spaceship (e.g., Trials 17-28, 45-63), even when outcomes are occasionally negative. This suggests a "stickiness" or perseveration bias, where the participant prefers to repeat their previous Stage 1 choice regardless of the reward value. This model combines Model-Based (planning) and Model-Free (habit) learning, weighted by `w`, and adds a `stickiness` parameter to account for the repetition bias.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Reinforcement Learning with Choice Stickiness.
    
    This model assumes the agent combines a model-based evaluation (using the transition matrix)
    and a model-free evaluation (TD learning) to make decisions. It includes a 'stickiness'
    parameter to capture the participant's tendency to repeat Stage 1 choices.

    Parameters:
    - learning_rate: [0, 1] Rate at which Q-values are updated.
    - beta_1: [0, 10] Inverse temperature for Stage 1 softmax (exploration vs exploitation).
    - beta_2: [0, 10] Inverse temperature for Stage 2 softmax.
    - w: [0, 1] Weighting parameter. 1 = Pure Model-Based, 0 = Pure Model-Free.
    - stickiness: [0, 5] Bonus added to the Q-value of the previously chosen spaceship.
    """
    learning_rate, beta_1, beta_2, w, stickiness = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)      # Model-free values for spaceships
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for aliens (state, action)
    
    last_action_1 = -1 # Initialize previous action placeholder

    for trial in range(n_trials):
        # --- STAGE 1 POLICY ---
        # 1. Calculate Model-Based values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Combine MB and MF values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # 3. Add Stickiness (Perseveration)
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        # 4. Softmax Choice 1
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Record choice for next trial stickiness
        last_action_1 = action_1[trial]
        
        state_idx = state[trial] # Planet arrived at

        # --- STAGE 2 POLICY ---
        # Standard Softmax on Stage 2 Q-values
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- UPDATING ---
        # Prediction errors
        # Delta 1: Difference between Stage 2 value and Stage 1 expectation
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        # Delta 2: Difference between Reward and Stage 2 expectation
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Update Stage 1 MF Q-values
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Update Stage 2 MF Q-values
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Pure Model-Free with Eligibility Trace ($\lambda$)
**Rationale:** The participant often ignores the transition structure (e.g., switching spaceships after a rare transition failure, or sticking to a spaceship despite rare transitions). This suggests they might not be using a Model-Based planner at all. Instead, they might be using a pure Model-Free strategy where the reward at the end of the trial reinforces the *first* choice directly. The eligibility trace parameter ($\lambda$) controls how much the Stage 2 reward outcome updates the Stage 1 choice value directly.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free TD(lambda) Learning.
    
    This model assumes the participant does not use the transition matrix (Model-Based)
    knowledge. Instead, they use Temporal Difference learning with an eligibility trace.
    If lambda > 0, the reward received at Stage 2 directly updates the value of the 
    Spaceship chosen in Stage 1.

    Parameters:
    - learning_rate: [0, 1] Rate at which Q-values are updated.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - lambda_param: [0, 1] Eligibility trace decay. Controls how much Stage 2 reward updates Stage 1.
    - stickiness: [0, 5] Perseveration bonus for Stage 1.
    """
    learning_rate, beta_1, beta_2, lambda_param, stickiness = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):

        # --- STAGE 1 POLICY (Pure MF) ---
        q_net = q_stage1_mf.copy()
        
        # Add Stickiness
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness

        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_action_1 = action_1[trial]
        state_idx = state[trial]

        # --- STAGE 2 POLICY ---
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- UPDATING ---
        # TD(0) error for Stage 1
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        # TD(0) error for Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Update Stage 1: Uses direct error + scaled Stage 2 error (eligibility trace)
        # Q1(t+1) = Q1(t) + lr * (delta1 + lambda * delta2)
        q_stage1_mf[action_1[trial]] += learning_rate * (delta_stage1 + lambda_param * delta_stage2)
        
        # Update Stage 2
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Asymmetric Learning Rate Model (Hybrid)
**Rationale:** The participant shows distinct behavior where they stick to a choice for a long time (ignoring zeros) but learn quickly when rewards change. This suggests they might weigh positive prediction errors (learning from gains) differently than negative prediction errors (learning from losses). This "Confirmation Bias" style model helps explain why they might persist on a spaceship despite getting 0 coins (low negative learning rate) but lock in quickly when they get 1 coin (high positive learning rate).

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Asymmetric Learning Rates (Positive vs Negative).
    
    This model allows for different learning rates depending on whether the prediction
    error is positive (better than expected) or negative (worse than expected).
    This can account for 'stubbornness' (ignoring losses) or 'optimism'.

    Parameters:
    - lr_pos: [0, 1] Learning rate for positive prediction errors.
    - lr_neg: [0, 1] Learning rate for negative prediction errors.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - w: [0, 1] Weighting parameter (1 = MB, 0 = MF).
    """
    lr_pos, lr_neg, beta_1, beta_2, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- STAGE 1 POLICY ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid mix
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # --- STAGE 2 POLICY ---
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- UPDATING ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Apply Asymmetric Learning Rate for Stage 1
        current_lr_1 = lr_pos if delta_stage1 > 0 else lr_neg
        q_stage1_mf[action_1[trial]] += current_lr_1 * delta_stage1
        
        # Apply Asymmetric Learning Rate for Stage 2
        current_lr_2 = lr_pos if delta_stage2 > 0 else lr_neg
        q_stage2_mf[state_idx, action_2[trial]] += current_lr_2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```