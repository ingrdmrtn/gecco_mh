Here are three cognitive models based on the participant data and the two-step task structure.

The participant data shows significant "stickiness" (repeating the same spaceship choice for long blocks, e.g., trials 107-119) and sensitivity to rewards (learning which alien pays out). The models below range from a standard hybrid reinforcement learning model to variations incorporating perseveration (stickiness) and asymmetric learning from positive vs. negative outcomes.

### Model 1: Standard Hybrid MB/MF Model
This model implements the classic "Daw et al. (2011)" algorithm, combining Model-Based (MB) planning with Model-Free (MF) temporal difference learning. It assumes the participant weighs both strategies to make decisions.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Standard Hybrid Model-Based / Model-Free Reinforcement Learning.
    
    Combines a Model-Based value (derived from the transition matrix) and a 
    Model-Free value (derived from TD learning) for the first stage choice.
    
    Parameters:
    lr (float): Learning rate for value updates [0, 1].
    beta_1 (float): Inverse temperature for Stage 1 softmax [0, 10].
    beta_2 (float): Inverse temperature for Stage 2 softmax [0, 10].
    w (float): Mixing weight. 0 = Pure MF, 1 = Pure MB [0, 1].
    lam (float): Eligibility trace parameter (lambda). Controls how much Stage 2 
                 outcome affects Stage 1 MF value [0, 1].
    """
    lr, beta_1, beta_2, w, lam = model_parameters
    n_trials = len(action_1)
  
    # Transition matrix: 0->[0,1] (70/30), 1->[1,0] (70/30)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)      # Model-free values for spaceships
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for aliens (State x Alien)

    log_loss = 0
    eps = 1e-10

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        
        # 1. Calculate Model-Based values for Stage 1
        # V_MB(s1) = Transition_Matrix * max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Combine MB and MF values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # 3. Probability of Choice 1 (Softmax)
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        # Accumulate Loss
        chosen_a1 = action_1[trial]
        p_choice_1[trial] = probs_1[chosen_a1]
        log_loss -= np.log(p_choice_1[trial] + eps)
        
        # Handle missing data in Stage 2 (e.g., Trial 76)
        if action_2[trial] == -1:
            continue
            
        state_idx = state[trial]
        chosen_a2 = action_2[trial]
        r = reward[trial]

        # --- Stage 2 Decision ---
        
        # Probability of Choice 2 (Softmax on Q_stage2)
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        # Accumulate Loss
        p_choice_2[trial] = probs_2[chosen_a2]
        log_loss -= np.log(p_choice_2[trial] + eps)
        
        # --- Value Updates (TD Learning) ---
        
        # Prediction error stage 1 (TD(0))
        delta_stage1 = q_stage2_mf[state_idx, chosen_a2] - q_stage1_mf[chosen_a1]
        
        # Prediction error stage 2
        delta_stage2 = r - q_stage2_mf[state_idx, chosen_a2]
        
        # Update Stage 1 MF values
        # Note: We include the eligibility trace (lam) to pass Stage 2 error back to Stage 1
        q_stage1_mf[chosen_a1] += lr * delta_stage1 + lr * lam * delta_stage2
        
        # Update Stage 2 MF values
        q_stage2_mf[state_idx, chosen_a2] += lr * delta_stage2
        
    return log_loss
```

### Model 2: Hybrid MB/MF with Choice Stickiness
The participant data shows long streaks of repeating the same spaceship choice (e.g., trials 50-66, 107-119). This model adds a "stickiness" parameter to the Stage 1 decision, increasing the probability of repeating the previous action regardless of reward history. This accounts for habit formation or motor perseveration.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF Model with Choice Perseveration (Stickiness).
    
    Adds a 'stickiness' bonus to the softmax of the previously chosen action
    in Stage 1, capturing the participant's tendency to repeat choices.
    
    Parameters:
    lr (float): Learning rate [0, 1].
    beta_1 (float): Inverse temp Stage 1 [0, 10].
    beta_2 (float): Inverse temp Stage 2 [0, 10].
    w (float): MB/MF mixing weight [0, 1].
    lam (float): Eligibility trace [0, 1].
    stick (float): Stickiness parameter. Positive values encourage repetition [0, 5].
    """
    lr, beta_1, beta_2, w, lam, stick = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    log_loss = 0
    eps = 1e-10
    
    last_action_1 = -1 # Initialize with no previous action

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Calculate logits (Value + Stickiness)
        logits = beta_1 * q_net
        if last_action_1 != -1:
            logits[last_action_1] += stick
            
        exp_q1 = np.exp(logits)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        chosen_a1 = action_1[trial]
        log_loss -= np.log(probs_1[chosen_a1] + eps)
        
        # Update tracker for next trial
        last_action_1 = chosen_a1
        
        # Handle missing data
        if action_2[trial] == -1:
            continue
            
        state_idx = state[trial]
        chosen_a2 = action_2[trial]
        r = reward[trial]

        # --- Stage 2 Decision ---
        
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[chosen_a2] + eps)
        
        # --- Value Updates ---
        
        delta_stage1 = q_stage2_mf[state_idx, chosen_a2] - q_stage1_mf[chosen_a1]
        delta_stage2 = r - q_stage2_mf[state_idx, chosen_a2]
        
        q_stage1_mf[chosen_a1] += lr * delta_stage1 + lr * lam * delta_stage2
        q_stage2_mf[state_idx, chosen_a2] += lr * delta_stage2
        
    return log_loss
```

### Model 3: Hybrid Model with Asymmetric Learning Rates
Participants often update their beliefs differently following positive rewards (1) versus lack of rewards (0). This model splits the learning rate (`lr`) into `lr_pos` (for positive prediction errors) and `lr_neg` (for negative prediction errors). This captures potential optimism or pessimism biases in the participant's learning process.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF Model with Asymmetric Learning Rates.
    
    Uses different learning rates for positive and negative prediction errors,
    allowing the model to fit participants who learn differently from wins vs losses.
    
    Parameters:
    lr_pos (float): Learning rate for positive prediction errors [0, 1].
    lr_neg (float): Learning rate for negative prediction errors [0, 1].
    beta_1 (float): Inverse temp Stage 1 [0, 10].
    beta_2 (float): Inverse temp Stage 2 [0, 10].
    w (float): MB/MF mixing weight [0, 1].
    lam (float): Eligibility trace [0, 1].
    """
    lr_pos, lr_neg, beta_1, beta_2, w, lam = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    log_loss = 0
    eps = 1e-10

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        chosen_a1 = action_1[trial]
        log_loss -= np.log(probs_1[chosen_a1] + eps)
        
        if action_2[trial] == -1:
            continue
            
        state_idx = state[trial]
        chosen_a2 = action_2[trial]
        r = reward[trial]

        # --- Stage 2 Decision ---
        
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[chosen_a2] + eps)
        
        # --- Value Updates with Asymmetry ---
        
        delta_stage1 = q_stage2_mf[state_idx, chosen_a2] - q_stage1_mf[chosen_a1]
        delta_stage2 = r - q_stage2_mf[state_idx, chosen_a2]
        
        # Determine effective learning rate for Stage 1 update
        # Using the sign of the combined error term
        combined_error = delta_stage1 + lam * delta_stage2
        lr_eff_1 = lr_pos if combined_error > 0 else lr_neg
        
        q_stage1_mf[chosen_a1] += lr_eff_1 * combined_error
        
        # Determine effective learning rate for Stage 2 update
        lr_eff_2 = lr_pos if delta_stage2 > 0 else lr_neg
        
        q_stage2_mf[state_idx, chosen_a2] += lr_eff_2 * delta_stage2
        
    return log_loss
```