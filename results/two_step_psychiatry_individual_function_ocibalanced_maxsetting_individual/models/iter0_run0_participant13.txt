Here are the three cognitive models based on the participant data and the two-step task structure.

### Model 1: Model-Free Q-Learning with Perseveration
This model assumes the participant relies on a Model-Free (Habitual) strategy, learning the value of spaceships and aliens directly from reward prediction errors (TD learning). It includes a **perseveration** parameter, as the data shows significant "stickiness" (repeating the same Stage 1 choice regardless of outcome).

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Model 1: Model-Free Q-Learning with Perseveration.
    Assumes the agent learns values via TD prediction errors (SARSA-style) and 
    has a tendency to repeat Stage 1 choices (perseveration).
    
    Parameters:
    learning_rate: [0, 1] - Rate at which Q-values are updated.
    beta: [0, 10] - Inverse temperature controlling choice stochasticity.
    perseveration: [-3, 3] - Additional bias added to the previously chosen action's value.
    """
    learning_rate, beta, perseveration = model_parameters
    n_trials = len(action_1)
  
    # Initialize Q-values to 0.5 (neutral expectation for 0/1 rewards)
    q_stage1_mf = np.zeros(2) + 0.5
    q_stage2_mf = np.zeros((2, 2)) + 0.5
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        # Calculate logits: Beta * Q_MF + Stickiness
        logits_1 = beta * q_stage1_mf
        if prev_a1 != -1:
            logits_1[prev_a1] += perseveration
            
        # Softmax probability calculation
        exp_q1 = np.exp(logits_1 - np.max(logits_1)) # Subtract max for numerical stability
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        logits_2 = beta * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # Stage 1 Update (SARSA: using value of the action actually taken in Stage 2)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Stage 2 Update (Reward Prediction Error)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        prev_a1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid (Model-Based + Model-Free) with Perseveration
This model implements the classic "Hybrid" architecture (Daw et al., 2011). It calculates a Model-Based value for Stage 1 (using the known transition matrix and max Stage 2 values) and mixes it with the Model-Free value. This tests if the participant uses the task structure (MB) or just reward history (MF), while still accounting for stickiness.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model 2: Hybrid Model (MB + MF) with Perseveration.
    Combines Model-Based planning (using transition probabilities) and 
    Model-Free learning, plus a stickiness bias.
    
    Parameters:
    learning_rate: [0, 1] - Rate of value updating for MF values.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - Mixing weight (0 = Pure MF, 1 = Pure MB).
    perseveration: [-3, 3] - Stickiness to previous Stage 1 choice.
    """
    learning_rate, beta, w, perseveration = model_parameters
    n_trials = len(action_1)
  
    # Transition matrix: A(0)->X(0) is 0.7, A(0)->Y(1) is 0.3, etc.
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2) + 0.5
    q_stage2_mf = np.zeros((2, 2)) + 0.5
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        # 1. Calculate Model-Based values: T * max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Mix MB and MF values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # 3. Add stickiness and Softmax
        logits_1 = beta * q_net
        if prev_a1 != -1:
            logits_1[prev_a1] += perseveration
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        logits_2 = beta * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates (MF only) ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        prev_a1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Model-Free with Asymmetric Learning Rates
This model is a Model-Free learner that updates values differently depending on whether the prediction error is positive (better than expected) or negative (worse than expected). This captures potential biases in processing rewards versus omissions, which is common in psychiatric populations or specific strategies (like "maxsetting").

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model 3: Model-Free with Asymmetric Learning Rates and Perseveration.
    Uses different learning rates for positive and negative prediction errors.
    
    Parameters:
    alpha_pos: [0, 1] - Learning rate for positive prediction errors (Reward).
    alpha_neg: [0, 1] - Learning rate for negative prediction errors (Omission).
    beta: [0, 10] - Inverse temperature.
    perseveration: [-3, 3] - Stickiness bias.
    """
    alpha_pos, alpha_neg, beta, perseveration = model_parameters
    n_trials = len(action_1)
  
    q_stage1_mf = np.zeros(2) + 0.5
    q_stage2_mf = np.zeros((2, 2)) + 0.5
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        logits_1 = beta * q_stage1_mf
        if prev_a1 != -1:
            logits_1[prev_a1] += perseveration
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        logits_2 = beta * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates with Asymmetric Learning Rates ---
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        lr_1 = alpha_pos if delta_stage1 > 0 else alpha_neg
        q_stage1_mf[action_1[trial]] += lr_1 * delta_stage1
        
        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        lr_2 = alpha_pos if delta_stage2 > 0 else alpha_neg
        q_stage2_mf[state_idx, action_2[trial]] += lr_2 * delta_stage2
        
        prev_a1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```