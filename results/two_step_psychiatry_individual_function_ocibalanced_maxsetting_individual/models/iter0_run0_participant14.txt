Here are three cognitive models implemented as Python functions, designed to capture different strategies in the two-step task (Model-Based vs. Model-Free, Asymmetric Learning, and Perseveration).

### Model 1: Hybrid Model-Based / Model-Free RL
This model assumes the participant uses a mixture of two strategies: a Model-Based (MB) strategy that plans using the transition matrix, and a Model-Free (MF) strategy that learns from direct experience (TD learning). A weighting parameter `w` controls the balance between them. It also includes an eligibility trace `lambda` to allow the Stage 2 outcome to reinforce the Stage 1 choice directly.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Reinforcement Learning Model.
    
    The agent calculates Stage 1 values as a weighted sum of:
    1. Model-Based (MB) values: Derived from the transition matrix and max Stage 2 values.
    2. Model-Free (MF) values: Learned via Temporal Difference (TD) errors.
    
    Parameters:
    - learning_rate: [0, 1] Rate at which Q-values are updated.
    - beta: [0, 10] Inverse temperature for softmax choice (exploration/exploitation).
    - w: [0, 1] Weighting parameter. 0 = Pure Model-Free, 1 = Pure Model-Based.
    - lam: [0, 1] Eligibility trace (lambda). Controls how much Stage 2 reward affects Stage 1 MF values.
    """
    learning_rate, beta, w, lam = model_parameters
    n_trials = len(action_1)
    
    # Fixed transition matrix: T[0] = [0.7, 0.3], T[1] = [0.3, 0.7]
    # Representing P(State|Action)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Initialize Q-values
    q_mf = np.zeros(2)         # Stage 1 Model-Free values
    q2 = np.zeros((2, 2))      # Stage 2 values (State x Alien)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    eps = 1e-10

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])

        # Skip missing data
        if a1 == -1 or s == -1 or a2 == -1:
            p_choice_1[trial] = 0.5 # Neutral guess for log loss calculation
            p_choice_2[trial] = 0.5
            continue

        # --- Stage 1 Decision ---
        # 1. Calculate Model-Based values: T * max(Q2)
        max_q2 = np.max(q2, axis=1) # Max value for each state
        q_mb = transition_matrix @ max_q2
        
        # 2. Combine MB and MF values
        q_net = w * q_mb + (1 - w) * q_mf
        
        # 3. Softmax for Stage 1
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Decision ---
        # Standard Softmax on Q2 values for the current state
        exp_q2 = np.exp(beta * q2[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning / Updates ---
        
        # 1. Prediction Error Stage 1 (TD error between Stage 1 and Stage 2)
        # Value of the state we arrived at (Q2 of chosen action)
        delta_1 = q2[s, a2] - q_mf[a1]
        q_mf[a1] += learning_rate * delta_1
        
        # 2. Prediction Error Stage 2 (Reward Prediction Error)
        delta_2 = r - q2[s, a2]
        q2[s, a2] += learning_rate * delta_2
        
        # 3. Eligibility Trace Update for Stage 1
        # The Stage 2 RPE also updates the Stage 1 choice, scaled by lambda
        q_mf[a1] += learning_rate * lam * delta_2

    # Calculate Negative Log Likelihood
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Asymmetric Model-Free RL with Perseveration
This model hypothesizes that the participant is primarily Model-Free (relying on reward history rather than transition structure) but learns differently from positive outcomes (wins) versus negative outcomes (losses/omissions). It also includes a "perseveration" parameter, as the data shows runs of repeating the same spaceship choice.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Asymmetric Model-Free RL with Perseveration.
    
    The agent uses separate learning rates for positive and negative prediction errors.
    It also includes a "stickiness" parameter to model the tendency to repeat the previous Stage 1 choice.
    This model ignores the transition matrix (Pure Model-Free).
    
    Parameters:
    - lr_pos: [0, 1] Learning rate for positive prediction errors (better than expected).
    - lr_neg: [0, 1] Learning rate for negative prediction errors (worse than expected).
    - beta: [0, 10] Inverse temperature.
    - stickiness: [0, 5] Bonus added to the Q-value of the previously chosen spaceship.
    """
    lr_pos, lr_neg, beta, stickiness = model_parameters
    n_trials = len(action_1)
    
    q1 = np.zeros(2)           # Stage 1 values
    q2 = np.zeros((2, 2))      # Stage 2 values
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1
    eps = 1e-10

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])

        if a1 == -1 or s == -1 or a2 == -1:
            continue

        # --- Stage 1 Decision ---
        # Add stickiness bonus to the previously chosen action
        q1_modified = q1.copy()
        if last_action_1 != -1:
            q1_modified[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q1_modified)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q2[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        
        # Stage 2 RPE
        delta_2 = r - q2[s, a2]
        
        # Update Stage 2 Q-values (Asymmetric)
        if delta_2 >= 0:
            q2[s, a2] += lr_pos * delta_2
        else:
            q2[s, a2] += lr_neg * delta_2
            
        # Stage 1 Update (Direct Reinforcement from Stage 2 outcome)
        # In pure MF (TD-lambda=1 logic for simplicity here), we update Q1 based on the final reward
        # or we can update based on Q2. Let's use simple TD: Q1 updates towards Q2.
        delta_1 = q2[s, a2] - q1[a1]
        
        if delta_1 >= 0:
            q1[a1] += lr_pos * delta_1
        else:
            q1[a1] += lr_neg * delta_1
            
        last_action_1 = a1

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Pure Model-Based RL with Decay
This model assumes the participant is fully aware of the transition structure (Model-Based) but has imperfect memory. The values of the aliens (Stage 2) decay back to a neutral point if they are not visited. This explains erratic switching behavior, as the agent might "forget" that a specific alien was good or bad if they haven't visited that planet recently.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Based RL with Memory Decay.
    
    The agent calculates Stage 1 values purely from the transition matrix and Stage 2 values.
    Crucially, unchosen Stage 2 options decay towards a neutral value (0.5) on every trial,
    simulating forgetting.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for chosen options.
    - decay_rate: [0, 1] Rate at which unchosen Q-values decay to 0.5.
    - beta: [0, 10] Inverse temperature.
    """
    learning_rate, decay_rate, beta = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Initialize Q2 at 0.5 (neutral expectation)
    q2 = np.full((2, 2), 0.5)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    eps = 1e-10

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])

        if a1 == -1 or s == -1 or a2 == -1:
            continue

        # --- Stage 1 Decision (Pure Model-Based) ---
        # Calculate expected value of each spaceship based on current Alien estimates
        max_q2 = np.max(q2, axis=1)
        q_mb = transition_matrix @ max_q2
        
        exp_q1 = np.exp(beta * q_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q2[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning and Decay ---
        
        # Update chosen alien
        q2[s, a2] += learning_rate * (r - q2[s, a2])
        
        # Decay unchosen aliens towards 0.5
        for state_i in range(2):
            for action_j in range(2):
                if not (state_i == s and action_j == a2):
                    q2[state_i, action_j] += decay_rate * (0.5 - q2[state_i, action_j])

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```