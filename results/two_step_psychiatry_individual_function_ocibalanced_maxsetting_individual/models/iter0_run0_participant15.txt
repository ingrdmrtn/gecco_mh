Here are three cognitive models developed based on the participant's data and the two-step task structure.

### Model 1: Hybrid Model-Based/Model-Free with Stickiness
This model posits that the participant makes decisions using a weighted combination of Model-Based (planning using the transition matrix) and Model-Free (habitual) reinforcement learning. The participant data shows long streaks of repeating the same spaceship (e.g., trials 15-51), suggesting a "stickiness" or perseveration parameter is necessary to explain the inertia in their choices.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Reinforcement Learning with Choice Stickiness.
    
    The agent combines a Model-Based value (calculated via the transition matrix)
    and a Model-Free value (learned via TD errors) to make the Stage 1 choice.
    A stickiness parameter accounts for the tendency to repeat the previous choice.
    
    Parameters:
    - learning_rate: [0, 1] Rate at which Q-values are updated.
    - beta_1: [0, 10] Inverse temperature for Stage 1 softmax (exploration/exploitation).
    - beta_2: [0, 10] Inverse temperature for Stage 2 softmax.
    - w: [0, 1] Mixing weight. 1 = Pure Model-Based, 0 = Pure Model-Free.
    - stickiness: [0, 5] Bonus added to the value of the previously chosen action.
    """
    learning_rate, beta_1, beta_2, w, stickiness = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values initialization
    q_stage1_mf = np.zeros(2)      # Model-free values for spaceships
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for aliens (State x Action)
    
    prev_action_1 = -1

    for trial in range(n_trials):
        # Handle missing data
        if action_1[trial] == -1:
            p_choice_1[trial] = 1.0 # arbitrary placeholder, log(1)=0, no loss
            p_choice_2[trial] = 1.0
            continue

        # --- Policy Stage 1 ---
        # 1. Model-Based Value: V(S2) * Transition
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Hybrid Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # 3. Add Stickiness
        stick_vec = np.zeros(2)
        if prev_action_1 != -1:
            stick_vec[prev_action_1] = stickiness
            
        # Softmax
        exp_q1 = np.exp(beta_1 * (q_net + stick_vec))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Policy Stage 2 ---
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # Stage 1 Update (SARSA-like for MF)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Stage 2 Update (Reward Prediction Error)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        prev_action_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Asymmetric Learning Model-Based Agent
The participant data exhibits "Win-Stay" behavior but also periods of sticking to a choice despite lack of reward (e.g., trials 11-12). This model suggests the participant is primarily Model-Based (aware of the transition structure) but updates their expectations of the aliens asymmetricallyâ€”learning more from positive outcomes (optimism) or negative outcomes (pessimism).

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Based RL with Asymmetric Learning Rates.
    
    The agent makes decisions based on the transition structure (Model-Based).
    However, the values of the aliens (Stage 2) are updated with different 
    learning rates depending on whether the prediction error was positive 
    (better than expected) or negative (worse than expected).
    
    Parameters:
    - alpha_pos: [0, 1] Learning rate for positive prediction errors.
    - alpha_neg: [0, 1] Learning rate for negative prediction errors.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - stickiness: [0, 5] Tendency to repeat the previous Stage 1 choice.
    """
    alpha_pos, alpha_neg, beta_1, beta_2, stickiness = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # We only track Stage 2 values; Stage 1 values are derived purely from MB
    q_stage2_mf = np.zeros((2, 2))
    
    # Placeholder for MF variable required by template structure, though unused for choice
    q_stage1_mf = np.zeros(2) 

    prev_action_1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # --- Policy Stage 1 (Pure Model-Based + Stickiness) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        stick_vec = np.zeros(2)
        if prev_action_1 != -1:
            stick_vec[prev_action_1] = stickiness

        exp_q1 = np.exp(beta_1 * (q_stage1_mb + stick_vec))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Policy Stage 2 ---
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # Stage 1 Update: Not used for decision, but kept for consistency
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += alpha_pos * delta_stage1 # arbitrary LR
        
        # Stage 2 Update: Asymmetric
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        if delta_stage2 >= 0:
            q_stage2_mf[state_idx, action_2[trial]] += alpha_pos * delta_stage2
        else:
            q_stage2_mf[state_idx, action_2[trial]] += alpha_neg * delta_stage2
            
        prev_action_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Model-Free with Eligibility Traces (TD-Lambda)
This model assumes the participant is purely Model-Free (does not calculate transitions) but uses an "eligibility trace" (lambda). This allows the reward received at the second stage to immediately influence the value of the spaceship chosen in the first stage. This mechanism can mimic Model-Based behavior (by connecting rewards to actions across time) without explicit planning, explaining how the participant learns to favor specific spaceships based on alien rewards.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Q-Learning with Eligibility Traces (TD-Lambda).
    
    The agent does not use the transition matrix (w=0). Instead, the outcome
    of Stage 2 (reward) directly updates the Stage 1 choice via an eligibility
    trace parameter (lambda). 
    
    Parameters:
    - learning_rate: [0, 1] Rate at which Q-values are updated.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - lambda_decay: [0, 1] Eligibility trace decay. 0 = TD(0), 1 = Monte Carlo.
      Controls how much the Stage 2 reward updates Stage 1 value directly.
    - stickiness: [0, 5] Tendency to repeat the previous Stage 1 choice.
    """
    learning_rate, beta_1, beta_2, lambda_decay, stickiness = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]]) # Unused in decision
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # --- Policy Stage 1 (Pure MF + Stickiness) ---
        # Note: q_stage1_mb is calculated but NOT used in the choice (w=0 implied)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2 
        
        stick_vec = np.zeros(2)
        if prev_action_1 != -1:
            stick_vec[prev_action_1] = stickiness

        exp_q1 = np.exp(beta_1 * (q_stage1_mf + stick_vec))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Policy Stage 2 ---
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # Calculate Stage 2 RPE first to use in Stage 1 update (Eligibility Trace)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Stage 1 Update: Includes both the transition error AND the reward error scaled by lambda
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        q_stage1_mf[action_1[trial]] += learning_rate * (delta_stage1 + lambda_decay * delta_stage2)
        
        # Stage 2 Update
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        prev_action_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```