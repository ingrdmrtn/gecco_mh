Here are three cognitive models based on the provided data and template.

### Model 1: Hybrid Model-Based/Model-Free with Eligibility Traces
This model assumes the participant uses a combination of Model-Based (planning based on transition probabilities) and Model-Free (trial-and-error) strategies. It includes an eligibility trace (`lambda_param`) allowing the outcome at the second stage (reward) to directly reinforce the first stage choice, bridging the gap between the two steps.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF model with eligibility traces.
    
    Parameters:
    lr (float): Learning rate for value updates [0,1].
    beta (float): Inverse temperature for softmax choice consistency [0,10].
    w (float): Weighting parameter between Model-Based (1) and Model-Free (0) [0,1].
    lambda_param (float): Eligibility trace decay, linking Stage 2 reward to Stage 1 choice [0,1].
    """
    lr, beta, w, lambda_param = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action

    for trial in range(n_trials):
        # Skip invalid trials
        if action_1[trial] == -1:
            continue

        # --- Policy for the first choice ---
        # Calculate Model-Based value: Transition probability * Max Stage 2 Value
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combine MB and MF values
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax selection
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Policy for the second choice ---
        # Standard Softmax on Stage 2 Q-values for the current state
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # TD Error 1: Difference between Stage 2 value and Stage 1 expectation
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        # Update Stage 1 MF value
        q_stage1_mf[action_1[trial]] += lr * delta_stage1

        # TD Error 2: Difference between Reward and Stage 2 expectation
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Update Stage 2 MF value
        q_stage2_mf[state_idx, action_2[trial]] += lr * delta_stage2
        
        # Eligibility Trace: Propagate Stage 2 error back to Stage 1 choice
        q_stage1_mf[action_1[trial]] += lr * lambda_param * delta_stage2

    eps = 1e-10
    # Sum logs only for valid trials (where p > 0)
    valid_mask = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss
```

### Model 2: Hybrid Model with Choice Perseveration
This model extends the hybrid approach by adding a "stickiness" or perseveration parameter. Participants often repeat their previous Stage 1 choice regardless of reward (motor perseveration) or due to cognitive inertia. This model captures that behavioral tendency, which is distinct from reward-based learning.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF model with choice perseveration (stickiness).
    
    Parameters:
    lr (float): Learning rate [0,1].
    beta (float): Inverse temperature [0,10].
    w (float): MB/MF Weighting [0,1].
    perseveration (float): "Stickiness" bonus for repeating the previous Stage 1 choice [0,5].
    """
    lr, beta, w, perseveration = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue

        # --- Policy for the first choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Calculate perseveration bonus
        pers_vec = np.zeros(2)
        if last_action_1 != -1:
            pers_vec[last_action_1] = 1.0
            
        # Net value = Weighted Q + Perseveration
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf + perseveration * pers_vec
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Policy for the second choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr * delta_stage2
        
        # Assuming lambda=1 implicitly for the MF part in this variant to focus on perseveration
        q_stage1_mf[action_1[trial]] += lr * delta_stage2
        
        last_action_1 = action_1[trial]

    eps = 1e-10
    valid_mask = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss
```

### Model 3: Asymmetric Learning Rates (Positive vs Negative)
This model hypothesizes that the participant learns differently from positive prediction errors (unexpected rewards) versus negative prediction errors (unexpected omissions). This is biologically plausible (dopamine signaling) and often explains behavior in volatile tasks better than a single learning rate.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid model with asymmetric learning rates for positive and negative prediction errors.
    
    Parameters:
    lr_pos (float): Learning rate for positive prediction errors (delta > 0) [0,1].
    lr_neg (float): Learning rate for negative prediction errors (delta < 0) [0,1].
    beta (float): Inverse temperature [0,10].
    w (float): MB/MF Weighting [0,1].
    """
    lr_pos, lr_neg, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue

        # --- Policy for the first choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Policy for the second choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        # Apply asymmetric learning rate for Stage 1
        lr_1 = lr_pos if delta_stage1 > 0 else lr_neg
        q_stage1_mf[action_1[trial]] += lr_1 * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Apply asymmetric learning rate for Stage 2
        lr_2 = lr_pos if delta_stage2 > 0 else lr_neg
        q_stage2_mf[state_idx, action_2[trial]] += lr_2 * delta_stage2
        
        # Apply asymmetric learning to the eligibility trace update (using Stage 2 error)
        q_stage1_mf[action_1[trial]] += lr_2 * delta_stage2

    eps = 1e-10
    valid_mask = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss
```