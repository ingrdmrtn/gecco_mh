Here are three cognitive models implemented as Python functions based on the provided template and participant data analysis.

### Model 1: Pure Model-Free with Asymmetric Learning
**Hypothesis:** The participant's behavior (e.g., repeating choices after rare transitions leading to rewards) suggests a reliance on **Model-Free** reinforcement learning, which reinforces actions that lead to rewards regardless of the transition structure. Additionally, the participant may weigh positive outcomes (gains) differently than negative outcomes (omissions), captured by asymmetric learning rates.

```python
def cognitive_model1_mf_asym(action_1, state, action_2, reward, model_parameters):
    """
    Model 1: Pure Model-Free with Asymmetric Learning Rates.
    
    This model assumes the participant learns purely from experience (Model-Free),
    ignoring the transition probabilities between spaceships and planets.
    It updates value estimates using separate learning rates for positive 
    (reward > expectation) and negative (reward < expectation) prediction errors.
    
    Parameters:
    - learning_rate_pos: [0, 1] Learning rate for positive prediction errors.
    - learning_rate_neg: [0, 1] Learning rate for negative prediction errors.
    - beta: [0, 10] Inverse temperature for softmax choice (exploration/exploitation).
    """
    learning_rate_pos, learning_rate_neg, beta = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Initialize Q-values (Model-Free)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # -- Stage 1 Choice (Pure Model-Free) --
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # -- Stage 2 Choice --
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # -- Updates --
        
        # Stage 1 Update: SARSA-like update (Q(s1,a1) driven by Q(s2,a2))
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        lr1 = learning_rate_pos if delta_stage1 > 0 else learning_rate_neg
        q_stage1_mf[action_1[trial]] += lr1 * delta_stage1
        
        # Stage 2 Update: Reward prediction error
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        lr2 = learning_rate_pos if delta_stage2 > 0 else learning_rate_neg
        q_stage2_mf[state_idx, action_2[trial]] += lr2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Model-Based with Choice Stickiness
**Hypothesis:** The participant displays long streaks of repeating the same spaceship (e.g., Trials 11-26), suggesting a **Model-Based** strategy (planning based on transition structure) augmented by **Choice Stickiness** (a tendency to repeat the previous action regardless of reward). This stickiness explains the stability in their choices even when rewards fluctuate slightly.

```python
def cognitive_model2_mb_sticky(action_1, state, action_2, reward, model_parameters):
    """
    Model 2: Model-Based with Choice Stickiness.
    
    This model assumes the participant uses the known transition structure (Model-Based)
    to calculate the value of Stage 1 actions. It includes a 'stickiness' parameter
    that adds a bonus to the previously chosen action, capturing perseveration.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for updating Stage 2 values.
    - beta: [0, 10] Inverse temperature for softmax choice.
    - stickiness: [0, 5] Bonus added to the logits of the previously chosen action.
    """
    learning_rate, beta, stickiness = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Stage 2 MF values are needed to compute MB values
    q_stage2_mf = np.zeros((2, 2)) 

    for trial in range(n_trials):
        # -- Stage 1 Choice (Model-Based + Stickiness) --
        
        # Calculate Model-Based values: Transition * Max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Calculate Logits
        logits = beta * q_stage1_mb
        
        # Add stickiness bonus to the action taken in the previous trial
        if trial > 0:
            logits[action_1[trial-1]] += stickiness
            
        exp_q1 = np.exp(logits)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # -- Stage 2 Choice --
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # -- Updates --
        # Update Stage 2 values based on reward
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model-Based / Model-Free
**Hypothesis:** The standard "Two-Step" task model. The participant likely employs a mixture of both strategies: a **Model-Based** system that plans using transitions and a **Model-Free** system that reinforces successful actions. A mixing weight `w` determines the balance between these two systems.

```python
def cognitive_model3_hybrid(action_1, state, action_2, reward, model_parameters):
    """
    Model 3: Hybrid Model-Based / Model-Free Reinforcement Learning.
    
    This model combines Model-Based (planning) and Model-Free (habit) systems.
    The Stage 1 choice is driven by a weighted average of values from both systems.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for value updates.
    - beta: [0, 10] Inverse temperature for softmax choice.
    - w: [0, 1] Weight parameter (0 = Pure MF, 1 = Pure MB).
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # -- Stage 1 Choice (Hybrid) --
        
        # 1. Model-Based Value
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Hybrid Net Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # -- Stage 2 Choice --
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # -- Updates --
        
        # Update Model-Free Stage 1 value
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Update Stage 2 value (used by both systems)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```