Here are three cognitive models based on the participant data and the two-step task structure.

### Model 1: Hybrid Model-Based/Model-Free Reinforcement Learning
This model hypothesizes that the participant makes decisions by combining two distinct value systems:
1.  **Model-Based (MB):** Calculates the value of a spaceship by planning forward, using the known transition probabilities (70%/30%) and the values of the aliens on the destination planets.
2.  **Model-Free (MF):** Learns the value of spaceships solely based on past reward history (Temporal Difference learning), ignoring the transition structure.
A weighting parameter `w` determines the balance between these systems.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based and Model-Free Reinforcement Learning Model.
    
    This model assumes the participant combines a Model-Based (planning) strategy 
    and a Model-Free (habitual) strategy.
    
    Parameters:
    learning_rate: [0, 1] Rate at which Q-values are updated.
    beta: [0, 10] Inverse temperature (exploration/exploitation trade-off).
    w: [0, 1] Weighting parameter. 1 = Pure Model-Based, 0 = Pure Model-Free.
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)
    
    # Transition matrix: Row 0=Spaceship A, Row 1=Spaceship U. Col 0=Planet X, Col 1=Planet Y.
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Q-values
    # Stage 1 MF: Values for Spaceships [A, U]
    q_stage1_mf = np.zeros(2) 
    # Stage 2 MF: Values for Aliens [Planet, Alien]
    q_stage2_mf = np.zeros((2, 2)) 
    
    log_likelihood = 0.0
    eps = 1e-10

    for trial in range(n_trials):
        a1 = action_1[trial]
        s2 = state[trial] # Planet arrived at
        a2 = action_2[trial]
        r = reward[trial]

        # --- Stage 1 Decision ---
        # 1. Model-Based Value calculation:
        # Value of spaceship = P(Planet X|Spaceship)*Max(Val_Aliens_X) + P(Planet Y|Spaceship)*Max(Val_Aliens_Y)
        max_q_stage2 = np.max(q_stage2_mf, axis=1) # Max value available on each planet
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Integrated Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # 3. Probability of choice (Softmax)
        exp_q = np.exp(beta * q_net)
        probs_1 = exp_q / np.sum(exp_q)
        
        # Accumulate Likelihood
        log_likelihood += np.log(probs_1[a1] + eps)
        
        # Handle missing data (timeouts/invalid actions)
        if a2 == -1:
            continue

        # --- Stage 2 Decision ---
        # Standard Softmax on Stage 2 Q-values
        exp_q2 = np.exp(beta * q_stage2_mf[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_likelihood += np.log(probs_2[a2] + eps)
        
        # --- Learning / Updates ---
        # Stage 2 RPE (Prediction Error)
        delta_stage2 = r - q_stage2_mf[s2, a2]
        
        # Update Stage 2 Q-values
        q_stage2_mf[s2, a2] += learning_rate * delta_stage2
        
        # Update Stage 1 MF Q-values
        # We use TD(1) logic here: The reward at stage 2 reinforces the stage 1 choice directly.
        # This is common in analysis of this task to capture strong MF habits.
        q_stage1_mf[a1] += learning_rate * (r - q_stage1_mf[a1])

    return -log_likelihood
```

### Model 2: Model-Free with Choice Perseverance
The participant data shows "chunks" of repeated choices (e.g., choosing Spaceship 1 for 10 trials, then Spaceship 0 for 10 trials). This suggests a "stickiness" or perseverance bias, where the participant prefers to repeat their last action regardless of the reward. This model ignores the transition structure (Pure Model-Free) but adds a `stickiness` parameter to the decision policy.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Q-Learning with Choice Perseverance (Stickiness).
    
    Assumes the participant ignores transition probabilities (Pure MF) but has a 
    tendency to repeat the previous Stage 1 choice.
    
    Parameters:
    learning_rate: [0, 1] Rate at which Q-values are updated.
    beta: [0, 10] Inverse temperature.
    stickiness: [0, 5] Bonus added to the Q-value of the previously chosen spaceship.
    """
    learning_rate, beta, stickiness = model_parameters
    n_trials = len(action_1)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    log_likelihood = 0.0
    eps = 1e-10
    
    last_action_1 = -1 # No previous action for first trial

    for trial in range(n_trials):
        a1 = action_1[trial]
        s2 = state[trial]
        a2 = action_2[trial]
        r = reward[trial]

        # --- Stage 1 Decision ---
        # Calculate effective values including stickiness
        q_effective = q_stage1.copy()
        if last_action_1 != -1:
            q_effective[last_action_1] += stickiness
            
        exp_q = np.exp(beta * q_effective)
        probs_1 = exp_q / np.sum(exp_q)
        
        log_likelihood += np.log(probs_1[a1] + eps)
        
        # Update history
        last_action_1 = a1

        # Handle missing data
        if a2 == -1:
            continue

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_likelihood += np.log(probs_2[a2] + eps)
        
        # --- Learning ---
        # Update Stage 2
        delta_2 = r - q_stage2[s2, a2]
        q_stage2[s2, a2] += learning_rate * delta_2
        
        # Update Stage 1 (TD(1) / Direct Reinforcement)
        # The Stage 1 value is updated toward the actual reward received
        q_stage1[a1] += learning_rate * (r - q_stage1[a1])

    return -log_likelihood
```

### Model 3: Asymmetric Learning Rate Model
This model tests the hypothesis that the participant learns differently from positive outcomes (finding gold) versus negative outcomes (no gold). Given the participant sometimes persists despite failures, or switches rapidly after success, they may have distinct sensitivities to Prediction Errors (RPE) depending on whether the RPE is positive or negative.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Asymmetric Learning Rate Model (Model-Free).
    
    This model uses different learning rates for positive prediction errors (getting 
    more than expected) and negative prediction errors (getting less than expected).
    
    Parameters:
    alpha_pos: [0, 1] Learning rate for positive prediction errors (RPE > 0).
    alpha_neg: [0, 1] Learning rate for negative prediction errors (RPE < 0).
    beta: [0, 10] Inverse temperature.
    """
    alpha_pos, alpha_neg, beta = model_parameters
    n_trials = len(action_1)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    log_likelihood = 0.0
    eps = 1e-10

    for trial in range(n_trials):
        a1 = action_1[trial]
        s2 = state[trial]
        a2 = action_2[trial]
        r = reward[trial]

        # --- Stage 1 Decision ---
        exp_q = np.exp(beta * q_stage1)
        probs_1 = exp_q / np.sum(exp_q)
        log_likelihood += np.log(probs_1[a1] + eps)

        if a2 == -1:
            continue

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_likelihood += np.log(probs_2[a2] + eps)
        
        # --- Learning ---
        # Update Stage 2
        delta_2 = r - q_stage2[s2, a2]
        lr_2 = alpha_pos if delta_2 > 0 else alpha_neg
        q_stage2[s2, a2] += lr_2 * delta_2
        
        # Update Stage 1 (Direct Reinforcement)
        delta_1 = r - q_stage1[a1]
        lr_1 = alpha_pos if delta_1 > 0 else alpha_neg
        q_stage1[a1] += lr_1 * delta_1

    return -log_likelihood
```