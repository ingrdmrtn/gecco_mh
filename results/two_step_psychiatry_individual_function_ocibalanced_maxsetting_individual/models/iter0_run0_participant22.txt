Here are the three cognitive models based on the participant data and the two-step task structure.

### Cognitive Model 1: Hybrid Model with Choice Stickiness
This model implements the classic "Daw et al. (2011)" hybrid reinforcement learning hypothesis. It posits that the participant uses a weighted combination of Model-Based (planning using the transition matrix) and Model-Free (habitual) value estimation. Given the participant's data shows long streaks of repeating the same spaceship choice (e.g., trials 55-84), a "stickiness" parameter is included to account for choice perseverance.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Reinforcement Learning with Choice Stickiness.
    
    This model assumes the participant acts based on a weighted average of 
    Model-Based (planning) and Model-Free (TD learning) values. It includes a 
    stickiness parameter to explain the strong perseverance observed in the data.

    Parameters:
    learning_rate: [0,1] - Rate at which Q-values are updated.
    beta_1: [0,10] - Inverse temperature for Stage 1 (exploration/exploitation).
    beta_2: [0,10] - Inverse temperature for Stage 2.
    w: [0,1] - Weighting parameter (0 = Pure Model-Free, 1 = Pure Model-Based).
    stick: [0,5] - Choice stickiness (tendency to repeat previous Stage 1 action).
    """
    learning_rate, beta_1, beta_2, w, stick = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition probabilities as described in the task
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)      # Model-free values for spaceships (0, 1)
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for aliens (State 0/1, Alien 0/1)
    
    prev_action_1 = -1

    for trial in range(n_trials):
        # Handle missing/timeout trials
        if action_1[trial] == -1 or action_2[trial] == -1:
            continue

        # --- Stage 1 Policy ---
        # Model-Based Value: Transition * Max(Stage 2 Values)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Value: Weighted sum of MB and MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness bonus to the previously chosen action
        logits_1 = beta_1 * q_net
        if prev_action_1 != -1:
            logits_1[prev_action_1] += stick
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1)) # Subtract max for numerical stability
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Policy ---
        state_idx = state[trial]
        # Standard softmax on Stage 2 Q-values
        logits_2 = beta_2 * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        # Prediction Error Stage 1 (SARSA-style for MF)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Prediction Error Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Update eligibility trace for MB (Step 1 MF values are updated by Step 2 PE * lambda, 
        # but in the standard Daw model, this is often implicitly handled or lambda=0. 
        # Here we stick to the standard Hybrid logic where MF Q1 is updated by delta1).
        
        prev_action_1 = action_1[trial]

    eps = 1e-10
    # Sum logs, ignoring trials where probability was not calculated (0 due to initialization)
    valid_trials = (p_choice_1 > 0) & (p_choice_2 > 0)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```

### Cognitive Model 2: Model-Free TD($\lambda$) Model
This model hypothesizes that the participant does *not* use the transition map (Model-Based) at all, effectively ignoring the "common/rare" transition structure. Instead, they rely purely on temporal difference learning. The parameter $\lambda$ (lambda) connects the reward received at the second stage directly back to the first stage choice. This tests if the participant is simply chaining actions based on reward history without planning.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free TD(lambda) Learning with Stickiness.
    
    This model assumes the participant ignores the transition structure (w=0)
    and learns purely via reward prediction errors. It uses an eligibility 
    trace (lambda) to allow the outcome at Stage 2 to update the value of the 
    Stage 1 choice.

    Parameters:
    learning_rate: [0,1] - Update rate.
    beta_1: [0,10] - Inverse temp Stage 1.
    beta_2: [0,10] - Inverse temp Stage 2.
    lam: [0,1] - Eligibility trace parameter (0 = TD(0), 1 = Monte Carlo).
    stick: [0,5] - Choice stickiness.
    """
    learning_rate, beta_1, beta_2, lam, stick = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    prev_action_1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1 or action_2[trial] == -1:
            continue

        # --- Stage 1 Policy (Pure MF) ---
        logits_1 = beta_1 * q_stage1
        if prev_action_1 != -1:
            logits_1[prev_action_1] += stick
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        logits_2 = beta_2 * q_stage2[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        # Stage 1 PE
        delta_stage1 = q_stage2[state_idx, action_2[trial]] - q_stage1[action_1[trial]]
        
        # Stage 2 PE
        delta_stage2 = reward[trial] - q_stage2[state_idx, action_2[trial]]
        
        # Update Stage 2
        q_stage2[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Update Stage 1: Includes immediate PE (delta1) AND discounted Stage 2 PE (lambda * delta2)
        # This allows the final reward to reinforce the spaceship choice directly.
        q_stage1[action_1[trial]] += learning_rate * (delta_stage1 + lam * delta_stage2)
        
        prev_action_1 = action_1[trial]

    eps = 1e-10
    valid_trials = (p_choice_1 > 0) & (p_choice_2 > 0)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```

### Cognitive Model 3: Asymmetric Learning Rate Hybrid Model
The participant data shows instances where they stay with a choice despite failure (e.g., Trial 31-32) and others where they switch. This model modifies the standard Hybrid model by introducing asymmetric learning rates for positive and negative prediction errors. This captures "optimism bias" (ignoring bad news) or "pessimism bias" (overreacting to bad news), which is common in psychiatric populations.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Asymmetric Learning Rates (Positive/Negative).
    
    This model extends the Hybrid model by allowing different learning rates 
    for positive prediction errors (better than expected) and negative 
    prediction errors (worse than expected).

    Parameters:
    lr_pos: [0,1] - Learning rate for positive prediction errors (delta > 0).
    lr_neg: [0,1] - Learning rate for negative prediction errors (delta < 0).
    beta_1: [0,10] - Inverse temp Stage 1.
    beta_2: [0,10] - Inverse temp Stage 2.
    w: [0,1] - MB/MF weighting.
    stick: [0,5] - Choice stickiness.
    """
    lr_pos, lr_neg, beta_1, beta_2, w, stick = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1 or action_2[trial] == -1:
            continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta_1 * q_net
        if prev_action_1 != -1:
            logits_1[prev_action_1] += stick
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        logits_2 = beta_2 * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates with Asymmetric Learning Rates ---
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        lr_1 = lr_pos if delta_stage1 > 0 else lr_neg
        q_stage1_mf[action_1[trial]] += lr_1 * delta_stage1
        
        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        lr_2 = lr_pos if delta_stage2 > 0 else lr_neg
        q_stage2_mf[state_idx, action_2[trial]] += lr_2 * delta_stage2
        
        prev_action_1 = action_1[trial]

    eps = 1e-10
    valid_trials = (p_choice_1 > 0) & (p_choice_2 > 0)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```