Here are the three cognitive models based on the participant data and the two-step task structure.

### Cognitive Model 1: Asymmetric Model-Free Learner
**Rationale:** The participant data shows blocks of persistence (e.g., Trials 9-12) where they continue choosing a spaceship despite receiving 0 coins, yet they also maintain choices when rewarded. This suggests they may update their value estimates differently for positive outcomes (rewards) versus negative outcomes (omissions). This model implements "Asymmetric Learning Rates," allowing the participant to learn more quickly from rewards than punishments (or vice versa). It relies purely on Model-Free (TD) learning, ignoring the transition structure, which fits the observation that they repeat choices after "Rare" transitions that lead to reward (e.g., Trial 8, Trial 73).

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Asymmetric Model-Free Learner.
    
    This model assumes the participant is Model-Free (ignores transition probabilities)
    but has different learning rates for positive prediction errors (doing better than expected)
    and negative prediction errors (doing worse than expected).
    
    Parameters:
    lr_pos: [0, 1] Learning rate for positive prediction errors (Reward > Expectation).
    lr_neg: [0, 1] Learning rate for negative prediction errors (Reward < Expectation).
    beta:   [0, 10] Inverse temperature for softmax choice consistency.
    """
    lr_pos, lr_neg, beta = model_parameters
    n_trials = len(action_1)
  
    # Q-values initialization
    # Stage 1: 2 actions (Spaceships)
    q_stage1_mf = np.zeros(2) + 0.5 # Initialize at 0.5 (neutral)
    # Stage 2: 2 states (Planets) x 2 actions (Aliens)
    q_stage2_mf = np.zeros((2, 2)) + 0.5

    log_loss = 0
    eps = 1e-10

    for trial in range(n_trials):
        a1 = action_1[trial]
        s_idx = state[trial]
        a2 = action_2[trial]
        r = reward[trial]

        # Handle missing data
        if a1 == -1 or s_idx == -1 or a2 == -1:
            continue

        # --- Step 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        # Accumulate loss for action 1
        log_loss -= np.log(probs_1[a1] + eps)

        # --- Step 2 Choice ---
        # Select action based on the specific state (planet) arrived at
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        # Accumulate loss for action 2
        log_loss -= np.log(probs_2[a2] + eps)

        # --- Learning / Updating ---
        
        # Stage 1 Update (TD-0): Update Q1 based on value of State 2
        # Prediction Error: Value of chosen Q2 - Value of chosen Q1
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        
        # Select learning rate based on sign of error
        alpha_1 = lr_pos if delta_stage1 > 0 else lr_neg
        q_stage1_mf[a1] += alpha_1 * delta_stage1

        # Stage 2 Update: Update Q2 based on Reward
        # Prediction Error: Reward - Value of chosen Q2
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        # Select learning rate based on sign of error
        alpha_2 = lr_pos if delta_stage2 > 0 else lr_neg
        q_stage2_mf[s_idx, a2] += alpha_2 * delta_stage2

    return log_loss
```

### Cognitive Model 2: Sticky Model-Free Learner
**Rationale:** The participant exhibits significant "stickiness" or perseveration. For example, in trials 1-7 and 98-112, they repeat the same spaceship choice for many trials. Even after a loss (e.g., Trial 5), they often repeat the choice (Trial 6). This model adds a `stickiness` parameter to a standard Model-Free Q-learner. This parameter biases the softmax decision at Step 1 to repeat the previously chosen action, independent of the reward history.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Sticky Model-Free Learner.
    
    This model assumes the participant learns values via Model-Free TD learning
    but has a 'stickiness' bias, making them prone to repeating their previous 
    Stage 1 choice regardless of the outcome.
    
    Parameters:
    learning_rate: [0, 1] Rate of updating Q-values.
    beta:          [0, 10] Inverse temperature (exploitation vs exploration).
    stickiness:    [0, 5]  Bonus added to the logit of the previously chosen action.
    """
    learning_rate, beta, stickiness = model_parameters
    n_trials = len(action_1)
  
    q_stage1_mf = np.zeros(2) + 0.5
    q_stage2_mf = np.zeros((2, 2)) + 0.5

    log_loss = 0
    eps = 1e-10
    
    last_action_1 = -1

    for trial in range(n_trials):
        a1 = action_1[trial]
        s_idx = state[trial]
        a2 = action_2[trial]
        r = reward[trial]

        if a1 == -1 or s_idx == -1 or a2 == -1:
            last_action_1 = -1 # Reset memory on error/missing
            continue

        # --- Step 1 Choice ---
        # Calculate logits (values * beta)
        logits_1 = beta * q_stage1_mf
        
        # Add stickiness bonus to the previous action if it exists
        if last_action_1 != -1:
            logits_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1)) # Subtract max for stability
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        log_loss -= np.log(probs_1[a1] + eps)
        last_action_1 = a1

        # --- Step 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        log_loss -= np.log(probs_2[a2] + eps)

        # --- Learning / Updating ---
        
        # Stage 1 Update (TD-0)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

    return log_loss
```

### Cognitive Model 3: Hybrid Learner (Model-Based + Model-Free)
**Rationale:** While the participant often looks Model-Free, the "Gold Standard" for analyzing this task is a Hybrid model that mixes Model-Based (MB) and Model-Free (MF) planning. The MB system calculates values using the known transition matrix (0.7/0.3), allowing the agent to infer that a reward on a "Rare" planet should increase the value of the *other* spaceship. The `w` parameter quantifies the balance. This model tests if the participant uses any structural knowledge, even if heavily weighted towards MF.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Learner.
    
    Combines a Model-Free TD learner with a Model-Based planner.
    MB values are calculated by multiplying the transition matrix by the 
    best values available at Stage 2.
    
    Parameters:
    learning_rate: [0, 1] Rate of updating MF Q-values.
    beta:          [0, 10] Inverse temperature.
    w:             [0, 1]  Mixing weight. 0 = Pure MF, 1 = Pure MB.
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)
  
    # Transition matrix: Row 0 -> [0.7 to Pl 0, 0.3 to Pl 1], Row 1 -> [0.3 to Pl 0, 0.7 to Pl 1]
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2) + 0.5
    q_stage2_mf = np.zeros((2, 2)) + 0.5

    log_loss = 0
    eps = 1e-10

    for trial in range(n_trials):
        a1 = action_1[trial]
        s_idx = state[trial]
        a2 = action_2[trial]
        r = reward[trial]

        if a1 == -1 or s_idx == -1 or a2 == -1:
            continue

        # --- Step 1 Choice ---
        # 1. Calculate Model-Based values
        # Max value achievable in each state at stage 2
        max_q_stage2 = np.max(q_stage2_mf, axis=1) 
        # Bellman equation using known transition matrix
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Mix MB and MF values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        log_loss -= np.log(probs_1[a1] + eps)

        # --- Step 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        log_loss -= np.log(probs_2[a2] + eps)

        # --- Learning / Updating ---
        
        # Stage 1 MF Update (TD-0)
        # Note: MB values are not updated via error, they are computed on the fly.
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

        # Stage 2 MF Update
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

    return log_loss
```