Here are the three cognitive models based on the participant data and the two-step task structure.

### Cognitive Model 1: Model-Free with Eligibility Traces and Perseverance
**Concept:** This model assumes the participant relies primarily on "Model-Free" reinforcement learning. They do not calculate transition probabilities (Model-Based) but instead learn the value of spaceships based on the eventual reward obtained, linked via an eligibility trace parameter (`lam`). It also includes a `pers` (perseverance) parameter to account for the "stickiness" observed in the data (repeating choices regardless of outcome).

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Model 1: Pure Model-Free TD(lambda) with Perseverance.
    
    This model ignores the transition structure (Model-Based) and relies on 
    temporal difference learning. It connects the Stage 2 reward back to the 
    Stage 1 choice via an eligibility trace (lambda). It also includes 
    perseverance (stickiness) for the Stage 1 choice.
    
    Parameters:
    learning_rate: [0, 1] - Rate of Q-value updating.
    beta: [0, 10] - Inverse temperature (exploration/exploitation balance).
    lam: [0, 1] - Eligibility trace (0 = TD(0), 1 = Monte Carlo/Direct reinforcement).
    pers: [0, 5] - Perseverance bonus for repeating the previous Stage 1 choice.
    """
    learning_rate, beta, lam, pers = model_parameters
    n_trials = len(action_1)
    
    # Q-values initialization
    # Stage 1: 2 spaceships
    q_stage1_mf = np.zeros(2) 
    # Stage 2: 2 planets x 2 aliens
    q_stage2_mf = np.zeros((2, 2)) 
    
    last_action_1 = -1
    log_loss = 0.0
    eps = 1e-10

    for trial in range(n_trials):
        # Data cleaning: skip trials with missing data
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1 or reward[trial] == -1:
            last_action_1 = -1
            continue

        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = float(reward[trial])

        # --- Stage 1 Decision ---
        # Calculate net values (MF only + Perseverance)
        q_net_1 = q_stage1_mf.copy()
        
        # Add perseverance bonus to the previously chosen action
        if last_action_1 != -1:
            q_net_1[last_action_1] += pers
            
        # Softmax probability for Stage 1
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        # Accumulate likelihood
        log_loss -= np.log(probs_1[a1] + eps)
        
        # --- Stage 2 Decision ---
        # Standard Softmax on Stage 2 Q-values
        exp_q2 = np.exp(beta * q_stage2_mf[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        # Accumulate likelihood
        log_loss -= np.log(probs_2[a2] + eps)
        
        # --- Learning (TD Updates) ---
        
        # 1. Prediction Error Stage 1 (TD(0)): Q2 - Q1
        # Value of the state we arrived at (max Q or actual choice Q? Standard is Max for Q-learning, Choice for SARSA)
        # We use SARSA-like update here (value of state chosen)
        delta_1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_1
        
        # 2. Prediction Error Stage 2: Reward - Q2
        delta_2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += learning_rate * delta_2
        
        # 3. Eligibility Trace Update for Stage 1
        # Propagate the Stage 2 RPE back to Stage 1 scaled by lambda
        q_stage1_mf[a1] += learning_rate * lam * delta_2

        # Update history
        last_action_1 = a1

    return log_loss
```

### Cognitive Model 2: Hybrid Model-Based / Model-Free
**Concept:** This is the standard "Daw" model used in two-step tasks. It assumes the participant uses a mixture of Model-Based (MB) planning (using the known transition matrix) and Model-Free (MF) learning. A weight parameter `w` determines the balance. If `w` is high, the agent realizes that a rare transition to a good planet makes the *other* spaceship valuable. If `w` is low, they just reinforce the spaceship they just chose.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model 2: Hybrid Model-Based / Model-Free Reinforcement Learning.
    
    Computes a weighted sum of Model-Based values (derived from the transition matrix)
    and Model-Free values (derived from experience).
    
    Parameters:
    learning_rate: [0, 1] - Update rate for MF values.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - Weighting parameter (0 = Pure MF, 1 = Pure MB).
    lam: [0, 1] - Eligibility trace for the MF component.
    pers: [0, 5] - Perseverance bonus.
    """
    learning_rate, beta, w, lam, pers = model_parameters
    n_trials = len(action_1)
    
    # Transition matrix: Row=Action, Col=State (Planet)
    # Action 0 (A) -> Planet 0 (70%), Planet 1 (30%)
    # Action 1 (U) -> Planet 0 (30%), Planet 1 (70%)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    log_loss = 0.0
    eps = 1e-10

    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1 or reward[trial] == -1:
            last_action_1 = -1
            continue
            
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = float(reward[trial])

        # --- Stage 1 Decision ---
        
        # 1. Calculate Model-Based Values
        # Q_MB(a1) = Sum(P(s|a1) * Max(Q_stage2(s)))
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Mix MB and MF values
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # 3. Add Perseverance
        if last_action_1 != -1:
            q_net_1[last_action_1] += pers

        # Softmax
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + eps)

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[a2] + eps)

        # --- Learning ---
        
        # Stage 1 MF Update (TD(0))
        delta_1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_1
        
        # Stage 2 Update
        delta_2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += learning_rate * delta_2
        
        # Eligibility Trace (MF component only)
        q_stage1_mf[a1] += learning_rate * lam * delta_2
        
        last_action_1 = a1

    return log_loss
```

### Cognitive Model 3: Asymmetric Learning Hybrid Model
**Concept:** In psychiatric contexts, participants often show different sensitivities to positive rewards versus negative outcomes (omissions). This model extends the Hybrid architecture by splitting the `learning_rate` into `alpha_pos` (used when Prediction Error > 0) and `alpha_neg` (used when Prediction Error < 0). This allows the model to capture if the participant learns quickly from gold coins but slowly from empty boxes (or vice versa).

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model 3: Hybrid Model with Asymmetric Learning Rates.
    
    This model distinguishes between learning from positive prediction errors 
    (better than expected) and negative prediction errors (worse than expected).
    This is often relevant for analyzing reward/punishment sensitivity.
    
    Parameters:
    alpha_pos: [0, 1] - Learning rate for positive prediction errors.
    alpha_neg: [0, 1] - Learning rate for negative prediction errors.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - MB/MF weighting.
    lam: [0, 1] - Eligibility trace.
    pers: [0, 5] - Perseverance.
    """
    alpha_pos, alpha_neg, beta, w, lam, pers = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    log_loss = 0.0
    eps = 1e-10

    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1 or reward[trial] == -1:
            last_action_1 = -1
            continue
            
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = float(reward[trial])

        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_action_1 != -1:
            q_net_1[last_action_1] += pers

        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + eps)

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[a2] + eps)

        # --- Learning with Asymmetry ---
        
        # 1. Stage 1 MF Update
        delta_1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        lr_1 = alpha_pos if delta_1 > 0 else alpha_neg
        q_stage1_mf[a1] += lr_1 * delta_1
        
        # 2. Stage 2 Update
        delta_2 = r - q_stage2_mf[s, a2]
        lr_2 = alpha_pos if delta_2 > 0 else alpha_neg
        q_stage2_mf[s, a2] += lr_2 * delta_2
        
        # 3. Eligibility Trace
        # Note: We use the learning rate associated with the Stage 2 outcome
        q_stage1_mf[a1] += lr_2 * lam * delta_2
        
        last_action_1 = a1

    return log_loss
```