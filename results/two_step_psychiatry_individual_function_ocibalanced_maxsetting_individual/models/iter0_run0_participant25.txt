Here are the 3 cognitive models based on the participant data and the two-step task structure.

### Model 1: Model-Free TD Learning with Eligibility Traces
**Rationale:** The participant data shows a tendency to repeat Stage 1 choices that result in rewards, even if those rewards were obtained via "rare" transitions (e.g., Trials 4, 8, 11). This behavior—reinforcing the action taken rather than the optimal plan—is characteristic of **Model-Free** reinforcement learning. This model uses an eligibility trace (`lambda`) to allow the reward at Stage 2 to directly update the value of the Stage 1 choice, bypassing the need for a model of the transition structure.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free TD Learning with Eligibility Traces (TD(lambda)).
    
    This model relies purely on reward history to value actions. It does not use 
    the transition matrix. It updates Stage 1 values based on Stage 2 outcomes 
    via an eligibility trace, capturing the participant's tendency to repeat 
    rewarded actions regardless of transition type.
    
    Parameters:
    - learning_rate: [0, 1] Rate at which Q-values are updated.
    - beta: [0, 10] Inverse temperature for Softmax choice rule (exploration/exploitation).
    - lam: [0, 1] Eligibility trace decay. Controls how much the Stage 2 reward 
           updates the Stage 1 choice directly.
    """
    learning_rate, beta, lam = model_parameters
    n_trials = len(action_1)
    
    # Initialize Q-values
    # Stage 1: 2 actions (Spaceship 0, Spaceship 1)
    q_stage1_mf = np.zeros(2) + 0.5
    # Stage 2: 2 states (Planet 0, Planet 1) x 2 actions (Alien 0, Alien 1)
    q_stage2_mf = np.zeros((2, 2)) + 0.5
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        # --- Stage 1 Selection ---
        # Calculate probabilities using Softmax on Model-Free values
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        # Store probability of the chosen action
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Selection ---
        s_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        # Store probability of the chosen action
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning / Updating ---
        r = reward[trial]
        
        # Prediction Error 1: Difference between Stage 2 value and Stage 1 value
        # Standard SARSA-like update for first step
        pe_1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * pe_1
        
        # Prediction Error 2: Difference between Reward and Stage 2 value
        pe_2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * pe_2
        
        # Eligibility Trace: Allow Stage 2 PE to further update Stage 1 choice
        q_stage1_mf[a1] += learning_rate * lam * pe_2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model (MB/MF) with Choice Stickiness
**Rationale:** While the participant exhibits Model-Free characteristics, human behavior in this task is often best described by a hybrid of Model-Based (planning) and Model-Free systems. Additionally, the participant data shows significant "streaks" (e.g., trials 39-54, 120-151) where they repeat the same spaceship choice. This model includes a "stickiness" parameter to account for choice perseveration (repeating the previous motor action) which is distinct from learning.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Learner with Choice Stickiness.
    
    Combines a Model-Based planner (using the known transition structure) and 
    a Model-Free learner. Also includes a 'stickiness' parameter to account 
    for the participant's tendency to repeat Stage 1 choices (perseveration) 
    independent of reward history.
    
    Parameters:
    - learning_rate: [0, 1] Update rate for MF Q-values.
    - beta: [0, 10] Inverse temperature for Softmax.
    - w: [0, 1] Weighting parameter. 1 = Pure Model-Based, 0 = Pure Model-Free.
    - stickiness: [0, 10] Bonus added to the Q-value of the previously chosen spaceship.
    """
    learning_rate, beta, w, stickiness = model_parameters
    n_trials = len(action_1)

    # Assumed fixed transition matrix (A->X common, U->Y common)
    # Row 0 (Spaceship 0) -> [Prob Planet 0, Prob Planet 1]
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2) + 0.5
    q_stage2_mf = np.zeros((2, 2)) + 0.5
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1 # No previous action on trial 0

    for trial in range(n_trials):
        # --- Stage 1 Selection ---
        # 1. Model-Based Value: Expected max value of next stage weighted by transitions
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Hybrid Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # 3. Add Stickiness (Perseveration)
        if prev_a1 != -1:
            q_net[prev_a1] += stickiness
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        prev_a1 = a1 # Update previous action
        
        # --- Stage 2 Selection ---
        s_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        r = reward[trial]
        
        # Update Stage 1 MF (Standard TD)
        # Note: In hybrid models, MF usually updates via TD(1) or TD(0). 
        # Using TD(1) logic here (updating Q1 based on reward directly implies lambda=1)
        # or standard SARSA. Let's use simple TD(0) chain.
        
        pe_1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * pe_1
        
        pe_2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * pe_2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Asymmetric Learning Rate Model (Model-Free)
**Rationale:** The participant sometimes switches immediately after a loss (e.g., Trial 26-27) but stays for long periods during wins. Cognitive mechanisms often process positive prediction errors (doing better than expected) and negative prediction errors (doing worse than expected) with different sensitivities. This model splits the learning rate into `alpha_pos` and `alpha_neg` to capture this asymmetry within a Model-Free framework.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Learner with Asymmetric Learning Rates.
    
    This model assumes the participant updates their value estimates differently 
    depending on whether the outcome was better (positive prediction error) or 
    worse (negative prediction error) than expected. This helps explain 
    differential sensitivity to gains versus losses.
    
    Parameters:
    - lr_pos: [0, 1] Learning rate for positive prediction errors (Reward > Expectation).
    - lr_neg: [0, 1] Learning rate for negative prediction errors (Reward < Expectation).
    - beta: [0, 10] Inverse temperature for Softmax.
    """
    lr_pos, lr_neg, beta = model_parameters
    n_trials = len(action_1)
    
    q_stage1_mf = np.zeros(2) + 0.5
    q_stage2_mf = np.zeros((2, 2)) + 0.5
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        # --- Stage 1 Selection ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Selection ---
        s_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning with Asymmetry ---
        r = reward[trial]
        
        # Update Stage 2
        pe_2 = r - q_stage2_mf[s_idx, a2]
        if pe_2 >= 0:
            q_stage2_mf[s_idx, a2] += lr_pos * pe_2
        else:
            q_stage2_mf[s_idx, a2] += lr_neg * pe_2
            
        # Update Stage 1 
        # Using the updated Stage 2 value for the Stage 1 update (common in Q-learning)
        # or using the value *before* update (SARSA). Using value before update here
        # effectively, but since we just updated Q2, let's use the Q2 value that
        # drove the choice (temporal difference).
        
        # We recalculate PE1 based on the value of the state we landed in
        pe_1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        
        if pe_1 >= 0:
            q_stage1_mf[a1] += lr_pos * pe_1
        else:
            q_stage1_mf[a1] += lr_neg * pe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```