Here are the three cognitive models based on the participant's behavior and the two-step task structure.

### Model 1: Hybrid Model with Perseveration
This model combines Model-Based (MB) and Model-Free (MF) reinforcement learning strategies. It assumes the participant calculates values using the transition matrix (MB) while also learning from direct experience (MF). A mixing weight `w` controls the balance. Additionally, a perseveration parameter `p` accounts for the participant's tendency to repeat the same spaceship choice (stickiness).

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model combining Model-Based and Model-Free learning with Perseveration.
    
    Parameters:
    - learning_rate (alpha): [0, 1] Rate of updating Q-values.
    - beta: [0, 10] Inverse temperature for softmax choice (exploration/exploitation).
    - w: [0, 1] Weight mixing Model-Based (1) and Model-Free (0) values.
    - p: [0, 5] Perseveration bonus for repeating the previous stage 1 choice.
    """
    learning_rate, beta, w, p = model_parameters
    n_trials = len(action_1)
    
    # Fixed transition matrix: P(Planet|Spaceship)
    # Row 0: Spaceship 0 (A) -> [0.7 to Planet 0, 0.3 to Planet 1]
    # Row 1: Spaceship 1 (U) -> [0.3 to Planet 0, 0.7 to Planet 1]
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Initialize Q-values
    q_stage1_mf = np.zeros(2)          # MF values for Spaceships 0, 1
    q_stage2_mf = np.zeros((2, 2))     # MF values for (Planet 0/1, Alien 0/1)
    
    log_loss = 0.0
    prev_choice_1 = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based Value: Expected max stage 2 value given transitions
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Value: Weighted sum of MB and MF values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add Perseveration bonus to the previously chosen spaceship
        if prev_choice_1 != -1:
            q_net[prev_choice_1] += p
            
        # Softmax probability for Stage 1
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        # Accumulate negative log-likelihood
        log_loss -= np.log(probs_1[action_1[trial]] + 1e-10)
        
        # --- Stage 2 Policy ---
        state_idx = state[trial] # Planet 0 or 1
        q_s2 = q_stage2_mf[state_idx]
        
        # Softmax probability for Stage 2
        exp_q2 = np.exp(beta * q_s2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        log_loss -= np.log(probs_2[action_2[trial]] + 1e-10)
        
        # --- Updates ---
        # Prediction errors
        # Stage 1 PE: Difference between Stage 2 value (of chosen action) and Stage 1 value
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Stage 2 PE: Difference between Reward and Stage 2 value
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        prev_choice_1 = action_1[trial]

    return log_loss
```

### Model 2: Model-Free with Eligibility Traces (TD-Lambda)
This model assumes a purely Model-Free strategy but incorporates an eligibility trace parameter `lambda`. This allows the reward received at the second stage to directly influence the value of the first-stage choice (Spaceship), bridging the gap between the action and the outcome. This fits the participant's tendency to reinforce the spaceship choice based on the final coin reward. It also includes perseveration.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free TD(lambda) model with Perseveration.
    Uses eligibility traces to update Stage 1 values based on Stage 2 rewards.
    
    Parameters:
    - learning_rate (alpha): [0, 1] Rate of updating Q-values.
    - beta: [0, 10] Inverse temperature for softmax choice.
    - lam (lambda): [0, 1] Eligibility trace decay. 0 = TD(0), 1 = Monte Carlo/Direct Reinforcement.
    - p: [0, 5] Perseveration bonus.
    """
    learning_rate, beta, lam, p = model_parameters
    n_trials = len(action_1)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    log_loss = 0.0
    prev_choice_1 = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        q_net = q_stage1_mf.copy()
        
        if prev_choice_1 != -1:
            q_net[prev_choice_1] += p
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        log_loss -= np.log(probs_1[action_1[trial]] + 1e-10)
        
        # --- Stage 2 Policy ---
        state_idx = state[trial]
        q_s2 = q_stage2_mf[state_idx]
        
        exp_q2 = np.exp(beta * q_s2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        log_loss -= np.log(probs_2[action_2[trial]] + 1e-10)
        
        # --- Updates ---
        # TD(0) error for Stage 1
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        # TD error for Stage 2 (Reward prediction error)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Update Stage 2
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Update Stage 1 with Eligibility Trace
        # Q1 changes by alpha * (delta1 + lambda * delta2)
        # If lambda=1, Q1 is effectively updated by (Reward - Q1)
        q_stage1_mf[action_1[trial]] += learning_rate * (delta_stage1 + lam * delta_stage2)
        
        prev_choice_1 = action_1[trial]

    return log_loss
```

### Model 3: Asymmetric Model-Free with Perseveration
The participant data shows long streaks of sticking to a choice even when receiving 0 coins (negative outcomes), but responding well to 1 coin. This model uses separate learning rates for positive prediction errors (`lr_pos`) and negative prediction errors (`lr_neg`). This allows the model to capture behavior where the participant might learn quickly from rewards but slowly (or not at all) from lack of rewards.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Asymmetric Model-Free model with Perseveration.
    Uses different learning rates for positive and negative prediction errors.
    
    Parameters:
    - lr_pos: [0, 1] Learning rate for positive prediction errors (better than expected).
    - lr_neg: [0, 1] Learning rate for negative prediction errors (worse than expected).
    - beta: [0, 10] Inverse temperature for softmax choice.
    - p: [0, 5] Perseveration bonus.
    """
    lr_pos, lr_neg, beta, p = model_parameters
    n_trials = len(action_1)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    log_loss = 0.0
    prev_choice_1 = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        q_net = q_stage1_mf.copy()
        
        if prev_choice_1 != -1:
            q_net[prev_choice_1] += p
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        log_loss -= np.log(probs_1[action_1[trial]] + 1e-10)
        
        # --- Stage 2 Policy ---
        state_idx = state[trial]
        q_s2 = q_stage2_mf[state_idx]
        
        exp_q2 = np.exp(beta * q_s2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        log_loss -= np.log(probs_2[action_2[trial]] + 1e-10)
        
        # --- Updates ---
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        lr_1 = lr_pos if delta_stage1 > 0 else lr_neg
        q_stage1_mf[action_1[trial]] += lr_1 * delta_stage1
        
        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        lr_2 = lr_pos if delta_stage2 > 0 else lr_neg
        q_stage2_mf[state_idx, action_2[trial]] += lr_2 * delta_stage2
        
        prev_choice_1 = action_1[trial]

    return log_loss
```