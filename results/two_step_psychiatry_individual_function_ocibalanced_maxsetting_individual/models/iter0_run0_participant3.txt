Here are three cognitive models based on the provided template and participant data.

### Model 1: Hybrid Model-Based/Model-Free Learner
This model hypothesizes that the participant combines two strategies: a **Model-Free (MF)** strategy that learns purely from reward history, and a **Model-Based (MB)** strategy that uses the known transition probabilities (70%/30%) to plan ahead. A mixing parameter `w` determines the balance between these strategies. This explains how the participant might persist with a spaceship (MF) but switch when the underlying structure suggests a better option (MB).

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Reinforcement Learning Model.
    
    This model assumes the participant's Stage 1 choice is a weighted combination
    of Model-Free (MF) values (learned via SARSA) and Model-Based (MB) values 
    (computed via the transition matrix and max Stage 2 values).
    
    Parameters:
    learning_rate: [0, 1] - Rate at which Q-values are updated.
    beta: [0, 10] - Inverse temperature (exploration/exploitation trade-off).
    w: [0, 1] - Weighting parameter. 0 = Pure MF, 1 = Pure MB.
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values initialization
    q_stage1_mf = np.zeros(2)      # Model-free values for Stage 1 (Spaceships)
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for Stage 2 (Aliens per Planet)

    for trial in range(n_trials):
        # Skip invalid trials (e.g., missing data indicated by -1)
        if action_1[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 1.0 # No loss contribution
            p_choice_2[trial] = 1.0
            continue

        # --- Policy for the first choice ---
        # 1. Calculate Model-Based values: Transition prob * Best value in next stage
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Calculate Net values: Weighted sum of MB and MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # 3. Softmax probability for Stage 1
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial] # Planet reached (0 or 1)

        # --- Policy for the second choice ---
        # Standard Softmax on Stage 2 MF values for the current state (Planet)
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # Stage 1 Update (SARSA-style): Update S1 value based on value of S2 action taken
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Stage 2 Update (Prediction Error): Update S2 value based on received reward
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model with Choice Perseveration
The participant data shows "sticky" behavior (long runs of choosing Spaceship 0, then long runs of Spaceship 1). A standard RL model might switch too frequently. This model adds a **Perseveration** parameter (`stickiness`) that gives a "bonus" to the option chosen in the previous trial, explaining the inertia observed in the data.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Choice Perseveration (Stickiness).
    
    Extends the hybrid model by adding a 'stickiness' bonus to the previously 
    chosen action in Stage 1. This accounts for the participant's tendency to 
    repeat choices (inertia) regardless of reward outcomes.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - MB/MF mixing weight.
    stickiness: [0, 5] - Bonus added to the logits of the previously chosen action.
    """
    learning_rate, beta, w, stickiness = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1 # Initialize with no previous action

    for trial in range(n_trials):
        if action_1[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # --- Policy for the first choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Calculate Net Q-values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Apply Stickiness: Add bonus to the Q-value of the previous choice
        # We work in 'logit' space effectively by adding to Q before Softmax
        # (Note: Some models add this to probability, here we add to value for stability)
        q_net_with_stick = q_net.copy()
        if last_action_1 != -1:
            q_net_with_stick[last_action_1] += stickiness

        exp_q1 = np.exp(beta * q_net_with_stick)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Record choice for next trial
        last_action_1 = action_1[trial]
        state_idx = state[trial]

        # --- Policy for the second choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Asymmetric Learning Rate Model
This model posits that the participant learns differently from positive outcomes (finding gold) versus negative/neutral outcomes (no gold or losing gold). By splitting the learning rate into `alpha_pos` and `alpha_neg`, the model can capture biases where the participant might be quick to learn from success but slow to unlearn from failure (or vice versa).

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Asymmetric Learning Rates.
    
    This model separates the learning rate for positive prediction errors (better than expected)
    and negative prediction errors (worse than expected). This helps explain behavior where
    gains and losses influence future decisions with different magnitudes.
    
    Parameters:
    lr_pos: [0, 1] - Learning rate when prediction error is positive.
    lr_neg: [0, 1] - Learning rate when prediction error is negative.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - MB/MF mixing weight.
    """
    lr_pos, lr_neg, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        if action_1[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # --- Policy for the first choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Policy for the second choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        # Select learning rate based on sign of delta
        current_lr_1 = lr_pos if delta_stage1 >= 0 else lr_neg
        q_stage1_mf[action_1[trial]] += current_lr_1 * delta_stage1
        
        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        # Select learning rate based on sign of delta
        current_lr_2 = lr_pos if delta_stage2 >= 0 else lr_neg
        q_stage2_mf[state_idx, action_2[trial]] += current_lr_2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```