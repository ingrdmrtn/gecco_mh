Here are the three cognitive models based on the participant data and the two-step task structure.

### Cognitive Model 1: Hybrid Model-Based/Model-Free with Eligibility Trace
This model assumes the participant uses a combination of Model-Based (MB) planning (using the transition structure) and Model-Free (MF) learning (simple reinforcement). Based on the data showing switches after rewarded rare transitions (e.g., Trial 36-37), the participant likely employs MB reasoning. The eligibility trace (`lam`) allows the reward at the second stage to directly influence the first stage choice, a crucial component for MF learning in multi-step tasks.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free agent with Eligibility Trace.
    
    This model blends Model-Based (planning using transition matrix) and 
    Model-Free (TD learning) values. It uses an eligibility trace (lambda)
    to allow the Stage 2 reward to update Stage 1 values directly.
    
    Parameters:
    - learning_rate: [0, 1] Rate of updating Q-values.
    - beta_1: [0, 10] Inverse temperature for Stage 1 softmax (exploration/exploitation).
    - beta_2: [0, 10] Inverse temperature for Stage 2 softmax.
    - w: [0, 1] Mixing weight. 1 = Pure Model-Based, 0 = Pure Model-Free.
    - lam: [0, 1] Eligibility trace (lambda). How much Stage 2 reward affects Stage 1.
    """
    learning_rate, beta_1, beta_2, w, lam = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition matrix for the task (Common: 0->0, 1->1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)      # Model-free values for Stage 1 (Spaceships)
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for Stage 2 (Aliens)

    for trial in range(n_trials):
        # --- STAGE 1 POLICY ---
        # 1. Calculate Model-Based values: Transition * Max(Stage 2 Values)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Mix MB and MF values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # 3. Softmax Choice 1
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial] # The planet arrived at

        # --- STAGE 2 POLICY ---
        # Standard Softmax on Stage 2 Q-values
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- UPDATES ---
        # Prediction Error 1: Difference between Stage 2 value and Stage 1 value
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        # Update Stage 1 MF value (TD(0) part)
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Prediction Error 2: Reward minus expectation
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Update Stage 2 MF value
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Eligibility Trace Update: Propagate Stage 2 error back to Stage 1
        q_stage1_mf[action_1[trial]] += learning_rate * lam * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2: Hybrid Model with Choice Perseveration
This model extends the hybrid approach by accounting for "stickiness" or perseveration. In the participant data, there are sequences where the participant repeats the same spaceship choice (e.g., Trials 158-163). This model adds a parameter `stickiness` that boosts the probability of repeating the previous action, independent of the reward history. This helps separate genuine learning from motor repetition.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Agent with Choice Perseveration (Stickiness).
    
    Adds a 'stickiness' parameter to the Stage 1 choice policy. This accounts
    for the tendency to repeat the same motor action regardless of value.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature (shared for both stages).
    - w: [0, 1] Mixing weight (MB vs MF).
    - stickiness: [-5, 5] Bonus added to the Q-value of the previously chosen action.
    """
    learning_rate, beta, w, stickiness = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track previous choice for stickiness (init with -1 or random)
    last_action_1 = -1

    for trial in range(n_trials):

        # --- STAGE 1 POLICY ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Mix values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add Stickiness Bonus
        # Create a copy to calculate probabilities so we don't corrupt the actual Q-values
        logits = q_net.copy()
        if last_action_1 != -1:
            logits[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * logits)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- STAGE 2 POLICY ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- UPDATES ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Update history
        last_action_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3: Asymmetric Learning Rates (Positive vs Negative)
The participant data shows distinct reactions to rewards (1) versus omissions (0). For example, after receiving 0 coins (e.g., Trial 2, 5, 10), the participant often switches strategies, but not always. This model hypothesizes that the participant updates their value estimates differently when they are pleasantly surprised (positive prediction error) versus unpleasantly surprised (negative prediction error).

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Agent with Asymmetric Learning Rates.
    
    Uses two different learning rates: one for positive prediction errors (doing better
    than expected) and one for negative prediction errors (doing worse).
    
    Parameters:
    - alpha_pos: [0, 1] Learning rate for positive prediction errors.
    - alpha_neg: [0, 1] Learning rate for negative prediction errors.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight (MB vs MF).
    """
    alpha_pos, alpha_neg, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- STAGE 1 POLICY ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # --- STAGE 2 POLICY ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- UPDATES ---
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        lr_1 = alpha_pos if delta_stage1 > 0 else alpha_neg
        q_stage1_mf[action_1[trial]] += lr_1 * delta_stage1
        
        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        lr_2 = alpha_pos if delta_stage2 > 0 else alpha_neg
        q_stage2_mf[state_idx, action_2[trial]] += lr_2 * delta_stage2
        
        # Note: No eligibility trace in this specific variant to isolate the effect of asymmetry,
        # but Stage 1 is updated by the value of the state reached (TD(0)).

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```