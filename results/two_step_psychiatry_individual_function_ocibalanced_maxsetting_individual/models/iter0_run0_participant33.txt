Here are the 3 cognitive models based on the participant data and the two-step task structure.

### Cognitive Model 1: Hybrid Model with Perseverance (Stickiness)
This model combines Model-Based (MB) and Model-Free (MF) learning strategies. It assumes the participant plans using the transition matrix but also learns from direct experience. Crucially, it adds a "perseverance" (stickiness) parameter, as the participant data shows significant streaks of repeating the same spaceship choice (e.g., trials 12-23, 132-146) regardless of outcomes.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based/Model-Free learner with Perseverance (Stickiness).
    
    The agent computes a net value for Stage 1 choices as a weighted sum of 
    Model-Based values (planning) and Model-Free values (experience).
    It also adds a 'stickiness' bonus to the previously chosen action.
    
    Parameters:
    - learning_rate: [0,1] Rate at which Q-values are updated.
    - beta: [0,10] Inverse temperature (softness) of the softmax choice policy.
    - w: [0,1] Weight of Model-Based system (1=Pure MB, 0=Pure MF).
    - stickiness: [0,5] Bonus added to the log-probability of repeating the previous choice.
    """
    learning_rate, beta, w, stickiness = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)     # Model-free values for spaceships
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for aliens (State x Action)
    
    last_action_1 = -1

    for trial in range(n_trials):
        # Handle missing data
        if action_1[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # --- Stage 1 Policy ---
        # 1. Calculate Model-Based values: Transition * Max(Stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Net Value: Weighted sum of MB and MF
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # 3. Add Stickiness (Perseverance)
        # We add the bonus directly to the Q-value equivalent before softmax
        # effectively shifting the probability mass.
        q_net_with_stickiness = q_net_stage1.copy()
        if last_action_1 != -1:
            q_net_with_stickiness[last_action_1] += stickiness

        # 4. Softmax
        exp_q1 = np.exp(beta * q_net_with_stickiness)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]

        # --- Stage 2 Policy ---
        # Standard Softmax on Stage 2 Q-values
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # --- Updates ---
        # Prediction Errors
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        
        # Update Stage 1 MF values (TD(0))
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Update Stage 2 MF values
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2: Hybrid Model with Asymmetric Learning Rates
This model hypothesizes that the participant learns differently from positive outcomes (finding gold) versus negative outcomes (no gold). This "optimism" or "pessimism" bias is captured by splitting the learning rate into `alpha_pos` and `alpha_neg`.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Asymmetric Learning Rates (Positive/Negative Prediction Errors).
    
    The agent updates values differently depending on whether the outcome was 
    better (positive PE) or worse (negative PE) than expected.
    
    Parameters:
    - lr_pos: [0,1] Learning rate for positive prediction errors.
    - lr_neg: [0,1] Learning rate for negative prediction errors.
    - beta: [0,10] Inverse temperature.
    - w: [0,1] Weight of Model-Based system (1=Pure MB, 0=Pure MF).
    """
    lr_pos, lr_neg, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        if action_1[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # --- Updates ---
        # Stage 1 Update (TD(0))
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        
        current_lr_1 = lr_pos if delta_stage1 > 0 else lr_neg
        q_stage1_mf[a1] += current_lr_1 * delta_stage1
        
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        
        current_lr_2 = lr_pos if delta_stage2 > 0 else lr_neg
        q_stage2_mf[state_idx, a2] += current_lr_2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3: Hybrid Model with Eligibility Traces (Lambda)
This model introduces an eligibility trace parameter ($\lambda$). In standard TD learning (TD(0)), the first choice is only updated based on the value of the state it leads to. With eligibility traces (TD($\lambda$)), the outcome of the second stage (the actual reward) also directly influences the update of the first stage choice, bridging the temporal gap.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Eligibility Traces (Lambda).
    
    Allows the reward at the end of the trial to directly influence the 
    value of the first-stage choice, modulated by lambda.
    
    Parameters:
    - learning_rate: [0,1] Learning rate.
    - beta: [0,10] Inverse temperature.
    - w: [0,1] Weight of Model-Based system.
    - lam: [0,1] Eligibility trace parameter. 0 = TD(0), 1 = Monte Carlo/TD(1).
    """
    learning_rate, beta, w, lam = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        if action_1[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        
        # Update Stage 1: Combined update using lambda
        # If lambda=0, update is purely based on Stage 2 value (TD(0))
        # If lambda=1, update is based on final reward (Monte Carlo)
        q_stage1_mf[a1] += learning_rate * (delta_stage1 + lam * delta_stage2)
        
        # Update Stage 2
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```