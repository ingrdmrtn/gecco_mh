Here are the three cognitive models based on the participant data and the two-step task structure.

### Model 1: Hybrid Model with Perseveration and Eligibility Traces
This model represents the standard hypothesis for this task. It assumes the participant uses a mixture of Model-Based (planning using the transition matrix) and Model-Free (learning from reward history) strategies. Additionally, it includes a "perseveration" parameter (`p`), as the participant data shows significant "stickiness" (repeating the same spaceship choice in blocks, e.g., trials 57-79), and an eligibility trace (`lam`) to allow the Model-Free system to learn from the final outcome.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based/Model-Free agent with Perseveration and Eligibility Traces.
    
    Explains behavior by mixing planning (MB) and trial-and-error (MF).
    Includes a 'stickiness' parameter to explain the participant's tendency 
    to repeat spaceship choices.
    
    Parameters:
    lr (alpha): Learning rate [0,1]
    beta: Inverse temperature (randomness) [0,10]
    w: Weight mixing MB and MF (0=Pure MF, 1=Pure MB) [0,1]
    p: Perseveration (stickiness) to previous choice [0,1]
    lam: Eligibility trace parameter (0=TD(0), 1=TD(1)) [0,1]
    """
    lr, beta, w, p, lam = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition probabilities (A->X, U->Y are common)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2) # MF values for spaceships
    q_stage2_mf = np.zeros((2, 2)) # MF values for aliens (State x Action)
    
    prev_action_1 = -1 # For perseveration

    for trial in range(n_trials):
        # --- STAGE 1 POLICY ---
        # 1. Calculate Model-Based values: Transition * Max(Stage2 Values)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Combine MF and MB values
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # 3. Add Perseveration Bonus
        if prev_action_1 != -1:
            q_net_stage1[prev_action_1] += p
            
        # 4. Softmax Choice 1
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Store state and action for updates
        s1_choice = action_1[trial]
        state_idx = state[trial] # 0 or 1
        prev_action_1 = s1_choice

        # --- STAGE 2 POLICY ---
        # Standard Softmax on Stage 2 Q-values
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        s2_choice = action_2[trial]
        r = reward[trial]

        # --- UPDATES ---
        # 1. Prediction Error Stage 1 (TD(0)): Value of State 2 - Value of Action 1
        # Note: We use the value of the chosen action in stage 2 as the target
        delta_stage1 = q_stage2_mf[state_idx, s2_choice] - q_stage1_mf[s1_choice]
        
        # Update Stage 1 MF (TD(0) part)
        q_stage1_mf[s1_choice] += lr * delta_stage1
        
        # 2. Prediction Error Stage 2: Reward - Value of Action 2
        delta_stage2 = r - q_stage2_mf[state_idx, s2_choice]
        
        # Update Stage 2 MF
        q_stage2_mf[state_idx, s2_choice] += lr * delta_stage2
        
        # 3. Eligibility Trace Update for Stage 1 (TD(lambda))
        # Pass the stage 2 RPE back to stage 1, scaled by lambda
        q_stage1_mf[s1_choice] += lr * lam * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Asymmetric Learning Rate Model
This model tests the hypothesis that the participant learns differently from positive outcomes (rewards) versus negative outcomes (omissions). The data shows the participant sometimes persists after failure (trials 19-20) but switches quickly after other failures, or stays for long periods during success. Separating `lr_pos` and `lr_neg` allows the model to capture "risk-seeking" or "risk-averse" updating dynamics.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Asymmetric Learning Rates (Positive vs Negative RPE).
    
    Explains behavior where the participant updates values differently 
    when receiving a reward (1) vs no reward (0).
    
    Parameters:
    lr_pos: Learning rate for positive prediction errors [0,1]
    lr_neg: Learning rate for negative prediction errors [0,1]
    beta: Inverse temperature [0,10]
    w: Weight mixing MB and MF [0,1]
    lam: Eligibility trace parameter [0,1]
    """
    lr_pos, lr_neg, beta, w, lam = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- STAGE 1 POLICY ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        s1_choice = action_1[trial]
        state_idx = state[trial]

        # --- STAGE 2 POLICY ---
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        s2_choice = action_2[trial]
        r = reward[trial]
  
        # --- UPDATES ---
        # Determine effective learning rate based on sign of prediction error
        
        # 1. Stage 1 Update (TD0)
        delta_stage1 = q_stage2_mf[state_idx, s2_choice] - q_stage1_mf[s1_choice]
        curr_lr1 = lr_pos if delta_stage1 > 0 else lr_neg
        q_stage1_mf[s1_choice] += curr_lr1 * delta_stage1
        
        # 2. Stage 2 Update
        delta_stage2 = r - q_stage2_mf[state_idx, s2_choice]
        curr_lr2 = lr_pos if delta_stage2 > 0 else lr_neg
        q_stage2_mf[state_idx, s2_choice] += curr_lr2 * delta_stage2
        
        # 3. Stage 1 Eligibility Trace Update (TD1)
        # Use the learning rate associated with the stage 2 error
        q_stage1_mf[s1_choice] += curr_lr2 * lam * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Split Temperature Model (Noise Separation)
This model posits that the participant exhibits different levels of randomness (exploration/exploitation balance) in the two stages. For example, they might be very precise when choosing Aliens (Stage 2) based on immediate reward history, but more exploratory or noisy when choosing Spaceships (Stage 1) due to the complexity of the probabilistic transition.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Split Inverse Temperatures (Beta).
    
    Explains behavior where decision noise differs between the Spaceship choice (Stage 1)
    and the Alien choice (Stage 2).
    
    Parameters:
    lr: Learning rate [0,1]
    beta_1: Inverse temperature for Stage 1 (Spaceships) [0,10]
    beta_2: Inverse temperature for Stage 2 (Aliens) [0,10]
    w: Weight mixing MB and MF [0,1]
    lam: Eligibility trace parameter [0,1]
    """
    lr, beta_1, beta_2, w, lam = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- STAGE 1 POLICY ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Use beta_1 for Stage 1
        exp_q1 = np.exp(beta_1 * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        s1_choice = action_1[trial]
        state_idx = state[trial]

        # --- STAGE 2 POLICY ---
        qs_current_state = q_stage2_mf[state_idx]
        
        # Use beta_2 for Stage 2
        exp_q2 = np.exp(beta_2 * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        s2_choice = action_2[trial]
        r = reward[trial]
  
        # --- UPDATES ---
        delta_stage1 = q_stage2_mf[state_idx, s2_choice] - q_stage1_mf[s1_choice]
        q_stage1_mf[s1_choice] += lr * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[state_idx, s2_choice]
        q_stage2_mf[state_idx, s2_choice] += lr * delta_stage2
        
        # Eligibility trace
        q_stage1_mf[s1_choice] += lr * lam * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```