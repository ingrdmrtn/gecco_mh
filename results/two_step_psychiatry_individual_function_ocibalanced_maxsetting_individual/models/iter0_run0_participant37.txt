Here are the three cognitive models based on the participant data and the provided template.

### Model 1: Hybrid Model with Perseveration
**Hypothesis:** The participant data shows long streaks of selecting the same spaceship (e.g., Trials 17-30, 151-158). This suggests a "stickiness" or perseveration bias where the participant tends to repeat their previous Stage 1 choice regardless of the reward outcome, in addition to standard Model-Based (MB) and Model-Free (MF) learning.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Reinforcement Learning model with Perseveration.
    Mixes Model-Based and Model-Free values, with an added 'stickiness' parameter.
    
    Parameters:
    learning_rate: [0,1] - Rate of value updating.
    beta: [0,10] - Inverse temperature (randomness) for choice.
    w: [0,1] - Weight of Model-Based system (0=Pure MF, 1=Pure MB).
    p: [0,5] - Perseveration bonus (stickiness to previous choice).
    """
    learning_rate, beta, w, p = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track previous choice for perseveration
    last_action_1 = -1

    for trial in range(n_trials):
        # Skip invalid data (from -1 entries in description)
        if action_1[trial] == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combine MF and MB values
        q_hybrid = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add perseveration bonus to the previously chosen action
        if last_action_1 != -1:
            q_hybrid[last_action_1] += p
            
        exp_q1 = np.exp(beta * q_hybrid)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        last_action_1 = action_1[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Prediction Errors and Updates
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        # Update Stage 1 MF values
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Update Stage 2 MF values
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model with Separate Learning Rates
**Hypothesis:** The participant switches aliens (Stage 2) more frequently than spaceships (Stage 1). This suggests they may learn the value of specific aliens (immediate reward probability) at a different rate than they learn the value of the spaceships. This model separates the learning rates for the two stages.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid model with distinct learning rates for Stage 1 and Stage 2.
    
    Parameters:
    lr_1: [0,1] - Learning rate for Stage 1 (Spaceships).
    lr_2: [0,1] - Learning rate for Stage 2 (Aliens).
    beta: [0,10] - Inverse temperature.
    w: [0,1] - Weight of Model-Based system.
    """
    lr_1, lr_2, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        if action_1[trial] == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_hybrid = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_hybrid)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        # Update Stage 1 using lr_1
        q_stage1_mf[action_1[trial]] += lr_1 * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Update Stage 2 using lr_2
        q_stage2_mf[state_idx, action_2[trial]] += lr_2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model with Eligibility Trace (TD-Lambda)
**Hypothesis:** Standard Model-Free learning only updates the first stage based on the value of the second stage state (TD(0)). However, the participant might credit the final reward directly to the spaceship choice. This model uses an eligibility trace parameter (`lambda`) to allow the Stage 2 reward prediction error to directly influence the Stage 1 update.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid model with Eligibility Trace (Lambda).
    Allows the Stage 2 outcome to directly update Stage 1 values via eligibility traces.
    
    Parameters:
    learning_rate: [0,1] - Learning rate.
    beta: [0,10] - Inverse temperature.
    w: [0,1] - Weight of Model-Based system.
    lam: [0,1] - Eligibility trace parameter (0=TD(0), 1=Monte Carlo).
    """
    learning_rate, beta, w, lam = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        if action_1[trial] == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_hybrid = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_hybrid)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Update Stage 1 including the eligibility trace from Stage 2
        q_stage1_mf[action_1[trial]] += learning_rate * (delta_stage1 + lam * delta_stage2)
        
        # Update Stage 2 normally
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```