Here are three cognitive models based on the two-step task structure and the participant's behavior.

### Cognitive Model 1: Hybrid Model with Perseveration
This model hypothesizes that the participant combines Model-Based (planning via transition matrix) and Model-Free (habitual) learning. It also includes a "perseveration" parameter, as the participant data shows significant "stickiness" (e.g., repeating choices in trials 20-24 and 177-180). This model assumes the participant is influenced by both the structure of the task and a tendency to repeat previous motor actions.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free agent with Perseveration.
    
    The agent computes values using both a Model-Based approach (using the 
    known transition matrix) and a Model-Free approach (TD learning).
    The final choice is a weighted mixture of these values, plus a 
    perseveration bonus for repeating the last choice.

    Parameters:
    - learning_rate: [0, 1] Rate at which Q-values update.
    - beta_1: [0, 10] Inverse temperature for Stage 1 softmax (exploration/exploitation).
    - beta_2: [0, 10] Inverse temperature for Stage 2 softmax.
    - w: [0, 1] Weighting parameter. 0 = Pure Model-Free, 1 = Pure Model-Based.
    - p: [0, 5] Perseveration bonus. Added to the value of the previously chosen action.
    """
    learning_rate, beta_1, beta_2, w, p = model_parameters
    n_trials = len(action_1)
  
    # Transition probabilities: A->X (0.7), A->Y (0.3); U->Y (0.7), U->X (0.3)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)      # Model-free values for Stage 1 (Spaceships)
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for Stage 2 (Aliens per Planet)
    
    prev_action_1 = -1 # To track perseveration

    for trial in range(n_trials):
        # --- STAGE 1 CHOICE ---
        # 1. Calculate Model-Based values: Transition Matrix * max(Stage 2 Values)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Calculate Net Values: Mix MB and MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # 3. Add Perseveration Bonus
        if prev_action_1 != -1:
            q_net[prev_action_1] += p
        
        # 4. Softmax probability
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Record state and actions
        state_idx = state[trial]
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]

        # --- STAGE 2 CHOICE ---
        # Standard Softmax on Stage 2 Q-values
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- UPDATES ---
        # Prediction Error 1: Difference between Stage 2 value and Stage 1 MF value
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] = q_stage1_mf[a1] + learning_rate * delta_stage1
        
        # Prediction Error 2: Difference between Reward and Stage 2 value
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] = q_stage2_mf[state_idx, a2] + learning_rate * delta_stage2
        
        # Update history
        prev_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2: Model-Free TD(lambda)
This model assumes the participant does *not* use the transition structure (Model-Based) but instead relies on a pure reinforcement learning strategy with an eligibility trace (`lambda`). This allows the reward received at the end of the trial to directly reinforce the Stage 1 choice (Spaceship), bridging the gap between the first action and the final outcome. This explains the rapid switching behavior seen in the data (e.g., Trial 25-26) without requiring complex planning.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free Agent with Eligibility Traces (TD-Lambda).
    
    This agent does not use the transition matrix. Instead, it uses an 
    eligibility trace (lambda) to allow the final reward to update the 
    Stage 1 choice directly.
    
    Parameters:
    - learning_rate: [0, 1] Rate at which Q-values update.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - lam: [0, 1] Eligibility trace parameter (lambda). 
           0 = standard TD (Stage 1 updates only from Stage 2 value).
           1 = Monte Carlo (Stage 1 updates directly from Reward).
    """
    learning_rate, beta_1, beta_2, lam = model_parameters
    n_trials = len(action_1)
  
    # Note: Transition matrix is unused in pure MF, but defined for consistency
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- STAGE 1 CHOICE (Pure Model-Free) ---
        exp_q1 = np.exp(beta_1 * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]

        # --- STAGE 2 CHOICE ---
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # --- UPDATES ---
        # Stage 1 PE: Driven by the value of the state arrived at
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        
        # Stage 2 PE: Driven by the reward
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        
        # Update Stage 2 (Standard Q-learning)
        q_stage2_mf[state_idx, a2] = q_stage2_mf[state_idx, a2] + learning_rate * delta_stage2
        
        # Update Stage 1 (TD-Lambda)
        # The update includes the immediate TD error (delta_stage1) plus 
        # a portion of the Stage 2 error (delta_stage2) scaled by lambda.
        q_stage1_mf[a1] = q_stage1_mf[a1] + learning_rate * (delta_stage1 + lam * delta_stage2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3: Asymmetric Learning Rate Model
This model posits that the participant learns differently from positive outcomes (finding gold) versus negative outcomes (no gold). The participant data shows "Win-Stay" streaks (Trials 20-24) followed by immediate "Lose-Shift" (Trial 25->26). Separate learning rates for positive and negative prediction errors allow the model to capture this specific sensitivity to losses versus gains.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Agent with Asymmetric Learning Rates.
    
    This agent updates its values differently depending on whether the 
    outcome was better than expected (positive prediction error) or 
    worse than expected (negative prediction error).
    
    Parameters:
    - lr_pos: [0, 1] Learning rate for positive prediction errors (Reward > Expectation).
    - lr_neg: [0, 1] Learning rate for negative prediction errors (Reward < Expectation).
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    """
    lr_pos, lr_neg, beta_1, beta_2 = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- STAGE 1 CHOICE (Pure MF) ---
        exp_q1 = np.exp(beta_1 * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]

        # --- STAGE 2 CHOICE ---
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # --- UPDATES ---
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        if delta_stage1 >= 0:
            q_stage1_mf[a1] += lr_pos * delta_stage1
        else:
            q_stage1_mf[a1] += lr_neg * delta_stage1

        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        if delta_stage2 >= 0:
            q_stage2_mf[state_idx, a2] += lr_pos * delta_stage2
        else:
            q_stage2_mf[state_idx, a2] += lr_neg * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```