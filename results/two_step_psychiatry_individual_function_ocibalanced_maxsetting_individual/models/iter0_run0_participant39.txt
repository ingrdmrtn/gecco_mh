Here are three cognitive models formulated as Python functions.

### Model 1: Hybrid Model-Based / Model-Free Learner
This model represents the standard hypothesis for the two-step task. It assumes the participant's choices are a weighted combination of a **Model-Based (MB)** system, which plans using the transition matrix, and a **Model-Free (MF)** system, which learns from prediction errors (TD(0)). It includes a **stickiness** parameter to account for the participant's tendency to repeat choices.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free learner with Choice Stickiness.
    
    Parameters:
    - lr: Learning rate for value updates [0,1]
    - beta_1: Inverse temperature for stage 1 choice [0,10]
    - beta_2: Inverse temperature for stage 2 choice [0,10]
    - w: Mixing weight (0 = pure MF, 1 = pure MB) [0,1]
    - stick: Choice stickiness for stage 1 [0,5]
    """
    lr, beta_1, beta_2, w, stick = model_parameters
    n_trials = len(action_1)
    
    # Fixed transition matrix: Row=Action, Col=State
    # Spaceship 0 -> 0.7 Planet 0, 0.3 Planet 1
    # Spaceship 1 -> 0.3 Planet 0, 0.7 Planet 1
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Initialize Q-values
    q_stage1_mf = np.zeros(2) # Model-Free values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Model-Free values for stage 2 (2 planets x 2 aliens)
    
    log_loss = 0.0
    eps = 1e-10
    prev_action_1 = -1
    
    for trial in range(n_trials):
        # --- Stage 1: Choice ---
        
        # 1. Model-Based Value: Expected max value of next stage
        # max_q_stage2[s] = max over aliens of Q(s, alien)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Hybrid Net Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # 3. Add Stickiness (Perseveration)
        q_net_stick = q_net.copy()
        if prev_action_1 != -1:
            q_net_stick[prev_action_1] += stick
            
        # 4. Probability of Action 1
        exp_q1 = np.exp(beta_1 * q_net_stick)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = action_1[trial]
        log_loss -= np.log(probs_1[a1] + eps)
        
        # --- Handle Missing Data ---
        a2 = action_2[trial]
        if a2 == -1:
            # Skip updates for missing/timeout trials
            prev_action_1 = a1
            continue
            
        state_idx = state[trial]
        
        # --- Stage 2: Choice ---
        qs_2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta_2 * qs_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        log_loss -= np.log(probs_2[a2] + eps)
        
        # --- Learning (Updates) ---
        r = reward[trial]
        
        # Stage 2 Update (TD Error)
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += lr * delta_stage2
        
        # Stage 1 MF Update (TD(0) Error using Stage 2 value)
        # We use the value of the chosen option in stage 2 (SARSA-like)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        prev_action_1 = a1
        
    return log_loss
```

### Model 2: Asymmetric Model-Based Learner
This model hypothesizes that the participant relies primarily on the **Model-Based** structure (knowing the transitions) but learns from rewards asymmetrically. It uses separate learning rates for positive and negative prediction errors (`lr_pos`, `lr_neg`), allowing it to be "optimistic" or "pessimistic" about the aliens' coin probabilities.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Based learner with Asymmetric Learning Rates.
    
    Parameters:
    - lr_pos: Learning rate for positive prediction errors [0,1]
    - lr_neg: Learning rate for negative prediction errors [0,1]
    - beta: Inverse temperature for both stages [0,10]
    - stick: Choice stickiness for stage 1 [0,5]
    """
    lr_pos, lr_neg, beta, stick = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # We only track Stage 2 values; Stage 1 is derived purely from planning
    q_stage2 = np.zeros((2, 2))
    
    log_loss = 0.0
    eps = 1e-10
    prev_action_1 = -1
    
    for trial in range(n_trials):
        # --- Stage 1: Planning ---
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Add Stickiness
        q_net = q_stage1_mb.copy()
        if prev_action_1 != -1:
            q_net[prev_action_1] += stick
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = action_1[trial]
        log_loss -= np.log(probs_1[a1] + eps)
        
        # --- Handle Missing Data ---
        a2 = action_2[trial]
        if a2 == -1:
            prev_action_1 = a1
            continue
            
        state_idx = state[trial]
        
        # --- Stage 2: Choice ---
        qs_2 = q_stage2[state_idx]
        exp_q2 = np.exp(beta * qs_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        log_loss -= np.log(probs_2[a2] + eps)
        
        # --- Learning with Asymmetry ---
        r = reward[trial]
        delta = r - q_stage2[state_idx, a2]
        
        if delta >= 0:
            q_stage2[state_idx, a2] += lr_pos * delta
        else:
            q_stage2[state_idx, a2] += lr_neg * delta
            
        prev_action_1 = a1
        
    return log_loss
```

### Model 3: Model-Free TD(1) Learner
This model represents a simpler heuristic strategy. It ignores the transition structure and the intermediate state values for the first-stage update. Instead, it updates the spaceship values (Stage 1) directly based on the final reward received (TD(1)), effectively collapsing the two steps into one. This is often called "direct reinforcement."

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free TD(1) learner (Direct Reinforcement).
    
    Updates stage 1 values directly from the final reward, ignoring
    the intermediate state value (Planet).
    
    Parameters:
    - lr: Learning rate [0,1]
    - beta: Inverse temperature for both stages [0,10]
    - stick: Choice stickiness for stage 1 [0,5]
    """
    lr, beta, stick = model_parameters
    n_trials = len(action_1)
    
    # Independent Q-values for both stages
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    log_loss = 0.0
    eps = 1e-10
    prev_action_1 = -1
    
    for trial in range(n_trials):
        # --- Stage 1: Choice ---
        q_net = q_stage1.copy()
        if prev_action_1 != -1:
            q_net[prev_action_1] += stick
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = action_1[trial]
        log_loss -= np.log(probs_1[a1] + eps)
        
        # --- Handle Missing Data ---
        a2 = action_2[trial]
        if a2 == -1:
            prev_action_1 = a1
            continue
            
        state_idx = state[trial]
        
        # --- Stage 2: Choice ---
        qs_2 = q_stage2[state_idx]
        exp_q2 = np.exp(beta * qs_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        log_loss -= np.log(probs_2[a2] + eps)
        
        # --- Learning ---
        r = reward[trial]
        
        # Stage 2 Update: Standard Q-learning
        q_stage2[state_idx, a2] += lr * (r - q_stage2[state_idx, a2])
        
        # Stage 1 Update: Direct Reinforcement (TD(1))
        # Update spaceship value based directly on the coins received
        q_stage1[a1] += lr * (r - q_stage1[a1])
        
        prev_action_1 = a1
        
    return log_loss
```