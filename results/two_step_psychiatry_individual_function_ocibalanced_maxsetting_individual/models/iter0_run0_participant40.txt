Here are three cognitive models based on the two-step task structure and the participant's behavioral data.

### Analysis of Participant Data
The participant exhibits **strong perseverance (stickiness)**.
1.  **Stage 1:** They choose Spaceship 0 for the first ~150 trials almost exclusively, despite frequent losses. Then, they switch to Spaceship 1 and stick with it for the remaining 50 trials. This suggests a high "perseverance" parameter or very asymmetric learning (ignoring losses).
2.  **Stage 2:** Once on a planet, they seem to learn the value of aliens reasonably well, but the primary driver of behavior appears to be a strong habit or "stay" bias in the first stage.

The following models attempt to capture these dynamics using Hybrid RL, Eligibility Traces, and Asymmetric Learning rates.

### Model 1: Hybrid Model-Based/Model-Free with Perseverance
This model assumes the participant uses a weighted combination of Model-Based (planning using the transition matrix) and Model-Free (habitual) values. Crucially, it adds a **perseverance (stickiness)** parameter `p`. This parameter adds a bonus to the action chosen in the previous trial, which helps explain the long streaks of selecting the same spaceship.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Reinforcement Learning with Perseverance.
    
    This model combines a Model-Based (MB) planner, which uses the transition matrix,
    and a Model-Free (MF) learner. It also includes a perseverance bonus to capture
    the participant's tendency to repeat the previous Stage 1 choice ("stickiness").

    Parameters:
    - learning_rate: [0, 1] Rate at which Q-values are updated.
    - beta: [0, 10] Inverse temperature (softness of choice) for both stages.
    - w: [0, 1] Weighting parameter. 1 = Pure MB, 0 = Pure MF.
    - p: [0, 5] Perseverance bonus added to the previously chosen action's value.
    """
    learning_rate, beta, w, p = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition matrix as per template assumption (70% common, 30% rare)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2) # Model-Free values for Stage 1
    q_stage2_mf = np.zeros((2, 2)) # Values for Stage 2 (State x Action)
    
    prev_action_1 = -1 # Track previous action for perseverance

    for trial in range(n_trials):
        # --- STAGE 1 POLICY ---
        # 1. Calculate Model-Based values: Transition * max(Q_Stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Combine MB and MF values using weight 'w'
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # 3. Add Perseverance Bonus to the previously chosen action
        if prev_action_1 != -1:
            q_net[prev_action_1] += p
            
        # 4. Softmax Choice
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial] # Planet arrived at

        # --- STAGE 2 POLICY ---
        # Simple Softmax on Stage 2 Q-values
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- UPDATES ---
        # Prediction Error 1 (SARSA-style): Q_Stage2_Selected - Q_Stage1_MF
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Prediction Error 2: Reward - Q_Stage2_Selected
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Update history
        prev_action_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: TD($\lambda$) Model-Free Learning
This model removes the Model-Based component entirely. Instead, it uses an eligibility trace parameter $\lambda$. This allows the reward received at the end of the trial (Stage 2) to directly influence the value of the spaceship chosen at the beginning (Stage 1). Given the participant's strong habits, a Model-Free learner that efficiently connects outcomes to initial choices is a strong candidate.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free TD(lambda) Learning.
    
    This model ignores the transition structure (Pure Model-Free). 
    It uses an eligibility trace (lambda) to pass the second-stage reward prediction error
    back to the first-stage choice. This allows the agent to update the spaceship value
    based on the final coin outcome.

    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - lam: [0, 1] Eligibility trace parameter (lambda). 
           0 = TD(0) (Stage 1 only learns from Stage 2 value).
           1 = Monte Carlo (Stage 1 learns directly from Reward).
    """
    learning_rate, beta, lam = model_parameters
    n_trials = len(action_1)
  
    # No transition matrix usage here (Pure MF)
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- STAGE 1 POLICY ---
        # Pure Model-Free choice
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- STAGE 2 POLICY ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- UPDATES ---
        # RPE 1: Difference between value of state 2 and value of action 1
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        # RPE 2: Difference between reward and value of action 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Update Stage 2 Value
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Update Stage 1 Value:
        # Includes direct learning from State 2 (delta_stage1)
        # AND eligibility trace learning from Reward (lam * delta_stage2)
        q_stage1_mf[action_1[trial]] += learning_rate * (delta_stage1 + lam * delta_stage2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Asymmetric Learning Rate (Pos/Neg) Model-Free
The participant stays with Spaceship 0 for a very long time despite receiving many 0 rewards. This suggests they might be "ignoring" negative feedback (learning slowly from 0s) while reinforcing strongly on positive feedback (learning quickly from 1s). This model splits the learning rate into $\alpha_{pos}$ and $\alpha_{neg}$.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Asymmetric Learning Rate Model-Free RL.
    
    This model allows for different learning rates depending on whether the 
    prediction error is positive (better than expected) or negative (worse than expected).
    This can explain the participant's persistence if alpha_neg is very low 
    (ignoring losses).

    Parameters:
    - alpha_pos: [0, 1] Learning rate for positive prediction errors.
    - alpha_neg: [0, 1] Learning rate for negative prediction errors.
    - beta: [0, 10] Inverse temperature.
    """
    alpha_pos, alpha_neg, beta = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- STAGE 1 POLICY ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- STAGE 2 POLICY ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- UPDATES ---
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        if delta_stage1 > 0:
            q_stage1_mf[action_1[trial]] += alpha_pos * delta_stage1
        else:
            q_stage1_mf[action_1[trial]] += alpha_neg * delta_stage1
            
        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        if delta_stage2 > 0:
            q_stage2_mf[state_idx, action_2[trial]] += alpha_pos * delta_stage2
        else:
            q_stage2_mf[state_idx, action_2[trial]] += alpha_neg * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```