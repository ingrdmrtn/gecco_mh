Here are three cognitive models implemented as Python functions based on the participant data and the two-step task structure.

### Model 1: Hybrid Model-Based/Model-Free with Stickiness
This model posits that the participant combines Model-Based (planning based on transition probabilities) and Model-Free (habitual) value estimation. Given the participant's strong tendency to repeat choices (e.g., trials 151-165), a `stickiness` parameter is added to the Stage 1 choice policy to capture choice perseveration independent of reward history.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Reinforcement Learning agent with Choice Stickiness.
    
    This model assumes the agent computes values using a weighted mixture of 
    Model-Based (transition-aware) and Model-Free (TD-learning) systems.
    It includes a stickiness parameter to account for the participant's observed 
    tendency to repeat Stage 1 choices.

    Parameters:
    - learning_rate: [0, 1] Rate at which Q-values are updated.
    - beta: [0, 10] Inverse temperature (softness) of the softmax choice rule.
    - w: [0, 1] Weighting parameter (0 = Pure MF, 1 = Pure MB).
    - lambda_coef: [0, 1] Eligibility trace parameter connecting Stage 2 outcome to Stage 1.
    - stickiness: [0, 10] Bonus added to the logits of the previously chosen action.
    """
    learning_rate, beta, w, lambda_coef, stickiness = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)      # Model-free values for Stage 1 (Spaceships)
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for Stage 2 (Planets, Aliens)
    
    last_action_1 = -1 # To track stickiness

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # 1. Calculate Model-Based values: Transition Matrix * Max Stage 2 Values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Calculate Net Q-values: Weighted sum of MB and MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # 3. Add stickiness bonus to the previously chosen action
        logits = beta * q_net
        if last_action_1 != -1:
            logits[last_action_1] += stickiness
            
        # 4. Softmax probability
        exp_q1 = np.exp(logits - np.max(logits)) # Subtract max for numerical stability
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Record action for next trial's stickiness
        last_action_1 = action_1[trial]
        
        # --- Stage 2 Policy ---
        state_idx = state[trial] # 0 or 1
        q_s2 = q_stage2_mf[state_idx]
        
        # Standard Softmax for Stage 2
        exp_q2 = np.exp(beta * q_s2 - np.max(beta * q_s2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning Updates ---
        # Prediction Errors
        # Stage 1 PE: Difference between Stage 2 value and Stage 1 MF value
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        # Stage 2 PE: Difference between Reward and Stage 2 value
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Update Stage 1 MF values (TD-Lambda update)
        # The Stage 1 value is updated by its own error + discounted Stage 2 error
        q_stage1_mf[action_1[trial]] += learning_rate * (delta_stage1 + lambda_coef * delta_stage2)
        
        # Update Stage 2 MF values
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Pure Model-Free with Asymmetric Learning Rates
This model assumes the participant does *not* use a mental map of the transitions (Pure Model-Free, $w=0$) but learns differently from positive outcomes versus negative outcomes. The data shows long streaks of choices (e.g., trials 8-13) that persist until multiple failures occur, suggesting different sensitivities to reward (reinforcement) and punishment (extinction).

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free Agent with Asymmetric Learning Rates for Positive/Negative Prediction Errors.
    
    This model ignores the transition structure (w=0) and relies solely on temporal difference learning.
    It distinguishes between learning from 'good news' (positive prediction error) and 
    'bad news' (negative prediction error). It also includes stickiness.

    Parameters:
    - lr_pos: [0, 1] Learning rate when prediction error is positive.
    - lr_neg: [0, 1] Learning rate when prediction error is negative.
    - beta: [0, 10] Inverse temperature (softness) of the softmax choice rule.
    - lambda_coef: [0, 1] Eligibility trace parameter.
    - stickiness: [0, 10] Choice perseveration bonus.
    """
    lr_pos, lr_neg, beta, lambda_coef, stickiness = model_parameters
    n_trials = len(action_1)
  
    # No transition matrix usage in policy, purely MF
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):

        # --- Stage 1 Policy (Pure MF + Stickiness) ---
        logits = beta * q_stage1_mf
        if last_action_1 != -1:
            logits[last_action_1] += stickiness
            
        exp_q1 = np.exp(logits - np.max(logits))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_action_1 = action_1[trial]
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        q_s2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * q_s2 - np.max(beta * q_s2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates with Asymmetric Learning Rates ---
        
        # Calculate Prediction Errors
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Combined error for Stage 1 (TD-Lambda)
        combined_error_1 = delta_stage1 + lambda_coef * delta_stage2
        
        # Apply Learning Rate based on sign of error
        # Stage 1 Update
        eff_lr_1 = lr_pos if combined_error_1 > 0 else lr_neg
        q_stage1_mf[action_1[trial]] += eff_lr_1 * combined_error_1
        
        # Stage 2 Update
        eff_lr_2 = lr_pos if delta_stage2 > 0 else lr_neg
        q_stage2_mf[state_idx, action_2[trial]] += eff_lr_2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid MB/MF with Separate Stage Exploration
This model builds on the Hybrid MB/MF framework but separates the exploration parameter (`beta`) for Stage 1 (Spaceships) and Stage 2 (Aliens). The participant shows very different behavior in the two stages: highly repetitive/sticky behavior in Stage 1, but fairly efficient bandit-learning behavior in Stage 2. Using separate temperatures allows the model to capture high noise/exploration in one stage and deterministic exploitation in the other.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF Agent with Separate Inverse Temperatures for Stage 1 and Stage 2.
    
    This model acknowledges that the decision dynamics (and noise levels) might differ 
    between the strategic first stage and the bandit-like second stage.

    Parameters:
    - learning_rate: [0, 1] Rate at which Q-values are updated.
    - beta_1: [0, 10] Inverse temperature for Stage 1 (Spaceship choice).
    - beta_2: [0, 10] Inverse temperature for Stage 2 (Alien choice).
    - w: [0, 1] Weighting parameter (0 = Pure MF, 1 = Pure MB).
    - lambda_coef: [0, 1] Eligibility trace parameter.
    - stickiness: [0, 10] Choice perseveration bonus for Stage 1.
    """
    learning_rate, beta_1, beta_2, w, lambda_coef, stickiness = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):

        # --- Stage 1 Policy (Uses beta_1) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits = beta_1 * q_net
        if last_action_1 != -1:
            logits[last_action_1] += stickiness
            
        exp_q1 = np.exp(logits - np.max(logits))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_action_1 = action_1[trial]
        state_idx = state[trial]

        # --- Stage 2 Policy (Uses beta_2) ---
        # Note: No stickiness applied to stage 2, as per standard models
        q_s2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta_2 * q_s2 - np.max(beta_2 * q_s2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Update Stage 1
        q_stage1_mf[action_1[trial]] += learning_rate * (delta_stage1 + lambda_coef * delta_stage2)
        
        # Update Stage 2
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```