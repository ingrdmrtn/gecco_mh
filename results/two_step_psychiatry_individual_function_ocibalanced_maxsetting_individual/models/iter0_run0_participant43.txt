Here are three cognitive models developed based on the participant's data and the two-step task structure.

### Analysis of Participant Data
The participant shows a strong preference for **Spaceship 0** (Action 1 = 0), maintaining this choice for very long blocks (e.g., trials 45-120 and 141-200), even when rewards are intermittent. This suggests:
1.  **High Perseveration:** A tendency to repeat the previous Stage 1 choice regardless of the outcome.
2.  **Model-Based vs. Model-Free dynamics:** The participant navigates the transition structure but seems to rely heavily on habit or a specific strategy (sticking to one side) rather than frequent switching based on transition probabilities.
3.  **Reward Sensitivity:** They do switch (e.g., early trials), but they are slow to abandon a chosen spaceship when it stops paying out, suggesting potentially asymmetric learning rates (learning more from wins than losses) or a strong eligibility trace linking rewards back to the spaceship choice.

### Model 1: Hybrid Model-Based/Model-Free with Perseveration
This model posits that the participant combines a Model-Based (planning) strategy with a Model-Free (habitual) strategy. Crucially, it includes a **perseveration** parameter to account for the participant's "stickiness" to Spaceship 0.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Reinforcement Learning with Perseveration.
    
    This model assumes the participant's Stage 1 choice is a weighted combination
    of a Model-Based value (computed via the transition matrix) and a Model-Free
    value (learned via TD errors), plus a 'stickiness' bonus for repeating the
    previous action.
    
    Parameters:
    - lr (learning_rate): [0, 1] Rate at which Q-values are updated.
    - beta_1: [0, 10] Inverse temperature for Stage 1 (Spaceship) choice.
    - beta_2: [0, 10] Inverse temperature for Stage 2 (Alien) choice.
    - w: [0, 1] Weighting parameter (0 = Pure MF, 1 = Pure MB).
    - pers: [0, 1] Perseveration bonus added to the Q-value of the previously chosen action.
    """
    lr, beta_1, beta_2, w, pers = model_parameters
    n_trials = len(action_1)
    
    # Fixed transition matrix as per task description (0.7 common, 0.3 rare)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)      # Model-free values for Spaceships
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for Aliens (State x Action)
    
    prev_action_1 = -1 # Initialize previous action
    
    for trial in range(n_trials):
        # --- STAGE 1 CHOICE ---
        # 1. Calculate Model-Based values: Transition * Max(Stage2 Values)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Calculate Net Value: w * MB + (1-w) * MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # 3. Add Perseveration Bonus
        if prev_action_1 != -1:
            q_net[prev_action_1] += pers
            
        # 4. Softmax Probability
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Record choice for next trial's perseveration
        prev_action_1 = action_1[trial]
        
        # --- STAGE 2 CHOICE ---
        state_idx = state[trial]
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- UPDATES ---
        # Prediction error at Stage 1 (transition to state)
        # Note: In standard Hybrid models, MF Q1 is updated by the value of the state reached.
        # Here we use the max Q-value of the state reached as the target (TD(0)).
        delta_stage1 = np.max(q_stage2_mf[state_idx]) - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr * delta_stage1
        
        # Prediction error at Stage 2 (reward receipt)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr * delta_stage2
        
        # Note: Some hybrid models allow delta_stage2 to update Stage 1 (eligibility).
        # This implementation keeps them separate to distinguish from Model 2.

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Model-Free TD($\lambda$) with Eligibility Traces
The participant is very successful at sticking to the "good" spaceship. This might not require a map of the world (Model-Based), but rather an efficient way to credit the reward received at Stage 2 back to the choice made at Stage 1. This model uses an eligibility trace parameter ($\lambda$) to pass the Stage 2 reward prediction error back to update the Stage 1 choice.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free TD(Lambda) Learning.
    
    This model assumes no knowledge of the transition structure (w=0). Instead,
    it uses an eligibility trace (lambda) to allow the reward prediction error
    experienced at Stage 2 to update the value of the Stage 1 choice.
    
    Parameters:
    - lr (learning_rate): [0, 1] Rate at which Q-values are updated.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - lam (lambda): [0, 1] Eligibility trace parameter. Determines how much the 
      Stage 2 outcome updates the Stage 1 value.
    """
    lr, beta_1, beta_2, lam = model_parameters
    n_trials = len(action_1)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- STAGE 1 CHOICE ---
        # Pure Model-Free choice
        exp_q1 = np.exp(beta_1 * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- STAGE 2 CHOICE ---
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- UPDATES ---
        # 1. Stage 1 PE: Difference between Value of State reached and Value of Action 1
        # We use the Q-value of the chosen 2nd stage action as the estimate of State Value (SARSA-like)
        v_state_reached = q_stage2_mf[state_idx, action_2[trial]]
        delta_stage1 = v_state_reached - q_stage1_mf[action_1[trial]]
        
        # Update Stage 1 based on immediate transition surprise
        q_stage1_mf[action_1[trial]] += lr * delta_stage1
        
        # 2. Stage 2 PE: Difference between Reward and Value of Action 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Update Stage 2 Action Value
        q_stage2_mf[state_idx, action_2[trial]] += lr * delta_stage2
        
        # 3. Eligibility Trace Update
        # The Stage 2 error (delta_stage2) is passed back to Stage 1, scaled by lambda
        q_stage1_mf[action_1[trial]] += lr * lam * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Asymmetric Learning Rate Model
The participant's data shows long streaks of choosing Spaceship 0 even when rewards are sparse (e.g., trials 116-119). This suggests they might update their values differently when they receive a reward (Positive Prediction Error) versus when they get nothing (Negative Prediction Error). If the learning rate for negative outcomes is low, they will be slow to abandon a previously rewarding option.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Asymmetric Learning Rate Model (Model-Free).
    
    This model separates the learning rate into two components: one for positive
    prediction errors (lr_pos) and one for negative prediction errors (lr_neg).
    This helps explain behavior where a participant learns quickly from wins
    but ignores losses (or vice versa).
    
    Parameters:
    - lr_pos: [0, 1] Learning rate when prediction error is positive (Reward > Expectation).
    - lr_neg: [0, 1] Learning rate when prediction error is negative (Reward < Expectation).
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    """
    lr_pos, lr_neg, beta_1, beta_2 = model_parameters
    n_trials = len(action_1)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- STAGE 1 CHOICE ---
        exp_q1 = np.exp(beta_1 * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- STAGE 2 CHOICE ---
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- UPDATES ---
        
        # Stage 1 Update (SARSA style: Q1 -> Q2)
        target_stage1 = q_stage2_mf[state_idx, action_2[trial]]
        delta_stage1 = target_stage1 - q_stage1_mf[action_1[trial]]
        
        if delta_stage1 > 0:
            q_stage1_mf[action_1[trial]] += lr_pos * delta_stage1
        else:
            q_stage1_mf[action_1[trial]] += lr_neg * delta_stage1
            
        # Stage 2 Update (Q2 -> Reward)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        if delta_stage2 > 0:
            q_stage2_mf[state_idx, action_2[trial]] += lr_pos * delta_stage2
        else:
            q_stage2_mf[state_idx, action_2[trial]] += lr_neg * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```