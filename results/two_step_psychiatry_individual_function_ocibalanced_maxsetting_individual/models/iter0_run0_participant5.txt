Here are three cognitive models based on the participant data and the two-step task structure.

### Model 1: Hybrid Model-Based / Model-Free Learning
This model assumes the participant utilizes both a Model-Based (planning) strategy and a Model-Free (habitual) strategy. The Model-Based system calculates values using the transition matrix, while the Model-Free system learns from direct experience. A mixing parameter `w` determines the balance between these two systems. This addresses the data where the participant sometimes follows the transition structure and sometimes repeats rewarded actions regardless of transitions.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Reinforcement Learning Model.
    
    This model combines Model-Based (MB) planning, which uses the transition matrix,
    with Model-Free (MF) temporal difference learning.
    
    Parameters:
    - learning_rate: [0, 1] Rate at which Q-values are updated.
    - beta_1: [0, 10] Inverse temperature for Stage 1 softmax (exploration/exploitation).
    - beta_2: [0, 10] Inverse temperature for Stage 2 softmax.
    - w: [0, 1] Weighting parameter. 1 = Pure MB, 0 = Pure MF.
    """
    learning_rate, beta_1, beta_2, w = model_parameters
    n_trials = len(action_1)
  
    # Transition matrix: T[action, state] (approximate based on task description)
    # Action 0 -> commonly Planet 0 (X), Action 1 -> commonly Planet 1 (Y)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)      # Model-free values for stage 1 (Actions 0, 1)
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for stage 2 (States 0, 1; Actions 0, 1)

    for trial in range(n_trials):
        # Handle missing data
        if action_1[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 0.5 # Default for missing
            p_choice_2[trial] = 0.5
            continue

        # --- Stage 1 Policy ---
        # 1. Calculate Model-Based values
        # V_MB(state) = max_a Q_MF(state, a)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Combine MB and MF values
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # 3. Softmax choice probability
        exp_q1 = np.exp(beta_1 * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # --- Updates ---
        # Prediction error for Stage 1 (SARSA-style based on Stage 2 Q-value)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] = q_stage1_mf[a1] + learning_rate * delta_stage1
        
        # Prediction error for Stage 2
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] = q_stage2_mf[state_idx, a2] + learning_rate * delta_stage2

    eps = 1e-10
    # Sum logs only for valid trials (where p > 0)
    valid_mask = (p_choice_1 > 0) & (p_choice_2 > 0)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss
```

### Model 2: Model-Free with Eligibility Trace (TD-Lambda)
This model hypothesizes that the participant relies on temporal difference learning but allows the reward at the second stage to directly reinforce the first stage choice. The `lambda` parameter controls this "eligibility trace," bridging the gap between the final outcome and the initial decision. This explains the "Win-Stay" behavior observed even after rare transitions (e.g., Trial 29-30), where a pure Model-Based agent would switch.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Reinforcement Learning with Eligibility Traces (TD-Lambda).
    
    This model assumes the participant learns purely from reward prediction errors,
    but the reward at Stage 2 can directly update the value of the Stage 1 choice
    scaled by the eligibility trace parameter lambda.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - lambd: [0, 1] Eligibility trace decay (0 = TD(0), 1 = Monte Carlo).
    """
    learning_rate, beta_1, beta_2, lambd = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Only MF values used here
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        if action_1[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        # --- Stage 1 Policy ---
        exp_q1 = np.exp(beta_1 * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # --- Updates ---
        # 1. Stage 1 Update (TD(0) part)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] = q_stage1_mf[a1] + learning_rate * delta_stage1
        
        # 2. Stage 2 Update
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] = q_stage2_mf[state_idx, a2] + learning_rate * delta_stage2
        
        # 3. Eligibility Trace Update
        # The Stage 2 RPE (delta_stage2) reinforces Stage 1 choice scaled by lambda
        q_stage1_mf[a1] = q_stage1_mf[a1] + learning_rate * lambd * delta_stage2
        

    eps = 1e-10
    valid_mask = (p_choice_1 > 0) & (p_choice_2 > 0)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss
```

### Model 3: Model-Free with Choice Stickiness
The participant data shows significant streaks of repeating the same spaceship choice (e.g., trials 16-28), even when rewards vary. This model implements a pure Model-Free learner augmented with a "stickiness" parameter. This parameter adds a bias to the softmax function, increasing the probability of repeating the immediately preceding Stage 1 choice, independent of the learned value.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Reinforcement Learning with Choice Stickiness (Perseveration).
    
    This model captures the tendency to repeat the previous Stage 1 action
    regardless of reward history, combined with standard Q-learning.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - stickiness: [0, 5] Bonus added to the logit of the previously chosen action.
    """
    learning_rate, beta_1, beta_2, stickiness = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1 or action_2[trial] == -1:
            # If data is missing, we lose the 'last action' history for stickiness
            last_action_1 = -1 
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        # --- Stage 1 Policy ---
        # Calculate logits: Beta * Q + Stickiness
        logits_1 = beta_1 * q_stage1_mf
        
        # Add stickiness bonus if we have a valid previous action
        if last_action_1 != -1:
            logits_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] = q_stage1_mf[a1] + learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] = q_stage2_mf[state_idx, a2] + learning_rate * delta_stage2
        
        # Update history
        last_action_1 = a1

    eps = 1e-10
    valid_mask = (p_choice_1 > 0) & (p_choice_2 > 0)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss
```