Here are three cognitive models implemented as Python functions.

### Model 1: Hybrid Model with Stickiness
This model combines Model-Based (MB) and Model-Free (MF) reinforcement learning strategies. The MB component computes values using the task's transition structure, while the MF component learns from direct experience (TD(1)). A mixing weight `w` controls the balance between these strategies. Additionally, a `stickiness` parameter accounts for the participant's tendency to repeat choices (perseveration), which is prominent in the data.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model (Model-Based + Model-Free) with Choice Stickiness.
    
    Combines a Model-Based learner (using the known transition matrix) and a 
    Model-Free learner (TD(1)). Includes a stickiness parameter to account for 
    choice perseveration.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate for Q-value updates.
    beta: [0, 10] - Inverse temperature for softmax choice.
    w: [0, 1] - Mixing weight (0 = pure MF, 1 = pure MB).
    stickiness: [0, 5] - Bonus added to the Q-value of the previously chosen action.
    """
    learning_rate, beta, w, stickiness = model_parameters
    n_trials = len(action_1)
    
    # Fixed transition matrix: Row=Action(S0, S1), Col=State(P0, P1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_mf_stage1 = np.zeros(2)
    q_mf_stage2 = np.zeros((2, 2)) # State x Action
    
    last_action_1 = -1
    log_loss = 0.0
    eps = 1e-10
    
    for trial in range(n_trials):
        a1 = action_1[trial]
        s_idx = state[trial]
        a2 = action_2[trial]
        r = reward[trial]
        
        # --- Stage 1 Choice ---
        # Model-Based Value: Q_MB(s1, a) = sum(T(s1, a, s2) * max(Q_MF(s2, :)))
        max_q_stage2 = np.max(q_mf_stage2, axis=1)
        q_mb_stage1 = transition_matrix @ max_q_stage2
        
        # Net Value: Weighted sum of MB and MF
        q_net_stage1 = w * q_mb_stage1 + (1 - w) * q_mf_stage1
        
        # Add Stickiness
        if last_action_1 != -1:
            q_net_stage1[last_action_1] += stickiness
            
        # Softmax Stage 1
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + eps)
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_mf_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        if a2 != -1:
            log_loss -= np.log(probs_2[a2] + eps)
            
            # --- Updates ---
            # Update Stage 2 Q-values (Standard Q-learning)
            delta_2 = r - q_mf_stage2[s_idx, a2]
            q_mf_stage2[s_idx, a2] += learning_rate * delta_2
            
            # Update Stage 1 MF Q-values (TD(1) logic: update towards Reward)
            # This captures the direct reinforcement of the first stage choice
            q_mf_stage1[a1] += learning_rate * (r - q_mf_stage1[a1])
            
        last_action_1 = a1

    return log_loss
```

### Model 2: Asymmetric Learning Rate Model
This model is a Model-Free learner that updates Q-values differently depending on whether the outcome was better than expected (positive prediction error) or worse than expected (negative prediction error). This accounts for potential differences in how the participant processes rewards (coins) versus omissions (no coins). It also includes stickiness to capture the baseline perseveration.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Asymmetric Learning Rate Model (Model-Free) with Stickiness.
    
    Distinguishes between learning from positive prediction errors (better than expected)
    and negative prediction errors (worse than expected).
    
    Parameters:
    alpha_pos: [0, 1] - Learning rate for positive prediction errors.
    alpha_neg: [0, 1] - Learning rate for negative prediction errors.
    beta: [0, 10] - Inverse temperature.
    stickiness: [0, 5] - Choice perseveration bonus.
    """
    alpha_pos, alpha_neg, beta, stickiness = model_parameters
    n_trials = len(action_1)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    last_action_1 = -1
    log_loss = 0.0
    eps = 1e-10

    for trial in range(n_trials):
        a1 = action_1[trial]
        s_idx = state[trial]
        a2 = action_2[trial]
        r = reward[trial]
        
        # --- Stage 1 Choice ---
        q_net_1 = q_stage1.copy()
        if last_action_1 != -1:
            q_net_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + eps)
        
        # --- Stage 2 Choice ---
        if a2 != -1:
            exp_q2 = np.exp(beta * q_stage2[s_idx])
            probs_2 = exp_q2 / np.sum(exp_q2)
            log_loss -= np.log(probs_2[a2] + eps)
            
            # --- Updates ---
            # Stage 2 Prediction Error
            delta_2 = r - q_stage2[s_idx, a2]
            if delta_2 > 0:
                q_stage2[s_idx, a2] += alpha_pos * delta_2
            else:
                q_stage2[s_idx, a2] += alpha_neg * delta_2
                
            # Stage 1 Prediction Error (TD(1): update towards Reward)
            delta_1 = r - q_stage1[a1]
            if delta_1 > 0:
                q_stage1[a1] += alpha_pos * delta_1
            else:
                q_stage1[a1] += alpha_neg * delta_1
                
        last_action_1 = a1

    return log_loss
```

### Model 3: Decay-Based Choice Kernel (Momentum) Model
The participant shows long streaks of choosing the same spaceship (e.g., Spaceship 0). Instead of a simple "last choice" stickiness, this model uses a **Choice Kernel** that accumulates over time. This creates a "momentum" effect: the more often an action is chosen, the more likely it is to be chosen again, decaying slowly if not chosen.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Learner with Decay-Based Choice Kernel (Momentum).
    
    Instead of simple 1-step stickiness, uses a decaying trace of past choices (Choice Kernel)
    to capture longer-term perseveration or momentum.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate for Q-values.
    beta: [0, 10] - Inverse temperature.
    decay: [0, 1] - Decay rate of the choice kernel (0 = reset every trial, 1 = no decay).
    kernel_weight: [0, 5] - Weight of the choice kernel in the decision.
    """
    learning_rate, beta, decay, kernel_weight = model_parameters
    n_trials = len(action_1)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    choice_kernel = np.zeros(2)
    
    log_loss = 0.0
    eps = 1e-10

    for trial in range(n_trials):
        a1 = action_1[trial]
        s_idx = state[trial]
        a2 = action_2[trial]
        r = reward[trial]
        
        # --- Stage 1 Choice ---
        # Decision variable combines Q-value and Choice Kernel
        logits = beta * q_stage1 + kernel_weight * choice_kernel
        
        # Numerical stability for softmax
        logits = logits - np.max(logits)
        exp_q1 = np.exp(logits)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + eps)
        
        # --- Update Choice Kernel ---
        # Convex update: CK decays towards 0, then boosts the chosen action
        # This keeps the kernel bounded between [0, 1]
        choice_kernel = choice_kernel * decay
        choice_kernel[a1] += (1.0 - decay)
        
        # --- Stage 2 Choice ---
        if a2 != -1:
            exp_q2 = np.exp(beta * q_stage2[s_idx])
            probs_2 = exp_q2 / np.sum(exp_q2)
            log_loss -= np.log(probs_2[a2] + eps)
            
            # --- Updates (Standard TD(1)) ---
            delta_2 = r - q_stage2[s_idx, a2]
            q_stage2[s_idx, a2] += learning_rate * delta_2
            
            q_stage1[a1] += learning_rate * (r - q_stage1[a1])

    return log_loss
```