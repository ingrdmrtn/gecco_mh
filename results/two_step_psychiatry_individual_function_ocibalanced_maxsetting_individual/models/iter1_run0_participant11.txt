Here are the three proposed cognitive models.

### Model 1: TD($\lambda$) with Separate Stage Learning Rates
This model extends the standard TD($\lambda$) reinforcement learning by assigning distinct learning rates to the first stage (Spaceship choice) and the second stage (Alien choice). This allows the model to capture the participant's behavior where they might update their preferences for spaceships (which have stable transition probabilities) at a different speed than their valuation of aliens (whose reward probabilities drift over time).

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    TD(lambda) Learning with Separate Stage Learning Rates.
    
    This model assumes the participant uses Model-Free Temporal Difference learning
    but updates Stage 1 (Spaceship) and Stage 2 (Alien) values with different learning rates.
    This separates the learning of transitions/spaceship preferences from the learning
    of drifting alien reward probabilities.

    Parameters:
    - lr_1: [0, 1] Learning rate for Stage 1 (Spaceship) updates.
    - lr_2: [0, 1] Learning rate for Stage 2 (Alien) updates.
    - beta_1: [0, 10] Inverse temperature for Stage 1 choices.
    - beta_2: [0, 10] Inverse temperature for Stage 2 choices.
    - lambda_param: [0, 1] Eligibility trace decay (credits Stage 2 reward to Stage 1 choice).
    - stickiness: [0, 5] Perseveration bonus for repeating the last Stage 1 choice.
    """
    lr_1, lr_2, beta_1, beta_2, lambda_param, stickiness = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    
    last_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        q_net_1 = q_stage1_mf.copy()
        if last_action_1 != -1:
            q_net_1[last_action_1] += stickiness

        exp_q1 = np.exp(beta_1 * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_action_1 = action_1[trial]
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        # Prediction error for Stage 1 (driven by Stage 2 value)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        # Prediction error for Stage 2 (driven by Reward)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]

        # Update Stage 1 Q-values using lr_1
        # The eligibility trace (lambda * delta_stage2) is also scaled by lr_1
        q_stage1_mf[action_1[trial]] += lr_1 * (delta_stage1 + lambda_param * delta_stage2)

        # Update Stage 2 Q-values using lr_2
        q_stage2_mf[state_idx, action_2[trial]] += lr_2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: TD($\lambda$) with Asymmetric Learning Rates
This model posits that the participant learns differently from "good" news (positive prediction errors) versus "bad" news (negative prediction errors). In a task with probabilistic rewards, this asymmetry can help explain risk-averse or optimistic behavior. It applies to both the direct updates at Stage 2 and the eligibility-trace updates at Stage 1.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    TD(lambda) Learning with Asymmetric Learning Rates.
    
    This model distinguishes between learning from positive prediction errors (better than expected)
    and negative prediction errors (worse than expected). This asymmetry applies to both
    stages of the task.

    Parameters:
    - lr_pos: [0, 1] Learning rate for positive prediction errors.
    - lr_neg: [0, 1] Learning rate for negative prediction errors.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - lambda_param: [0, 1] Eligibility trace decay.
    - stickiness: [0, 5] Perseveration bonus for Stage 1.
    """
    lr_pos, lr_neg, beta_1, beta_2, lambda_param, stickiness = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        q_net_1 = q_stage1_mf.copy()
        if last_action_1 != -1:
            q_net_1[last_action_1] += stickiness

        exp_q1 = np.exp(beta_1 * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_action_1 = action_1[trial]
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]

        # Stage 1 Update: Combined error signal
        # The total error signal for Stage 1 is the immediate TD error + eligibility trace
        total_error_1 = delta_stage1 + lambda_param * delta_stage2
        effective_lr_1 = lr_pos if total_error_1 > 0 else lr_neg
        q_stage1_mf[action_1[trial]] += effective_lr_1 * total_error_1

        # Stage 2 Update: Immediate reward error
        effective_lr_2 = lr_pos if delta_stage2 > 0 else lr_neg
        q_stage2_mf[state_idx, action_2[trial]] += effective_lr_2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: TD($\lambda$) with Forgetting (Value Decay)
Since the reward probabilities of the aliens drift slowly over trials, information about unchosen aliens becomes stale. This model incorporates a "forgetting" or "decay" parameter. On each trial, the Q-values of unchosen aliens in the visited state decay towards a neutral prior (0.5), reflecting the participant's uncertainty about options they haven't observed recently.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    TD(lambda) Learning with Forgetting (Value Decay).
    
    This model includes a passive decay mechanism for unchosen options at Stage 2.
    Because alien reward probabilities drift, the model assumes that the value of 
    an unvisited alien reverts towards a baseline (0.5) over time.

    Parameters:
    - learning_rate: [0, 1] Standard learning rate for updates.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - lambda_param: [0, 1] Eligibility trace decay.
    - stickiness: [0, 5] Perseveration bonus for Stage 1.
    - forgetting_rate: [0, 1] Rate at which unchosen Q-values decay to 0.5.
    """
    learning_rate, beta_1, beta_2, lambda_param, stickiness, forgetting_rate = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    # Initialize Q-values at 0.5 (chance) since decay target is 0.5
    q_stage2_mf.fill(0.5)
    
    last_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        q_net_1 = q_stage1_mf.copy()
        if last_action_1 != -1:
            q_net_1[last_action_1] += stickiness

        exp_q1 = np.exp(beta_1 * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_action_1 = action_1[trial]
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]

        q_stage1_mf[action_1[trial]] += learning_rate * (delta_stage1 + lambda_param * delta_stage2)
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # --- Forgetting / Decay ---
        # Apply decay to the unchosen action in the current state
        # (and potentially unvisited states, but here we focus on the active context)
        # We decay all Q-values in the current state except the chosen one.
        unchosen_action = 1 - action_2[trial]
        q_stage2_mf[state_idx, unchosen_action] = (1 - forgetting_rate) * q_stage2_mf[state_idx, unchosen_action] + forgetting_rate * 0.5
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```