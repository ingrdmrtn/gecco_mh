Here are the 3 proposed cognitive models. They explore different mechanisms: separate learning rates for different stages, memory decay for unchosen options, and dynamic learning of the transition structure.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF Model with Separate Learning Rates.
    
    Distinguishes between the learning rate for the spaceship selection (Stage 1)
    and the alien selection (Stage 2). This allows the model to capture if the 
    participant updates their spaceship preferences at a different speed than 
    their alien preferences.
    
    Parameters:
    lr_1 (float): Learning rate for Stage 1 (Spaceships) [0, 1].
    lr_2 (float): Learning rate for Stage 2 (Aliens) [0, 1].
    beta_1 (float): Inverse temperature for Stage 1 softmax [0, 10].
    beta_2 (float): Inverse temperature for Stage 2 softmax [0, 10].
    w (float): Mixing weight for MB vs MF (0=MF, 1=MB) [0, 1].
    lam (float): Eligibility trace parameter [0, 1].
    """
    lr_1, lr_2, beta_1, beta_2, w, lam = model_parameters
    n_trials = len(action_1)
    
    # Fixed transition matrix as per task structure
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    
    log_loss = 0
    eps = 1e-10

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # Model-Based Value: Expected max value of next stage
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Value: Weighted sum of MB and MF
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax Policy
        exp_q1 = np.exp(beta_1 * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        # Accumulate Loss
        chosen_a1 = action_1[trial]
        log_loss -= np.log(probs_1[chosen_a1] + eps)
        
        # Handle missing data
        if action_2[trial] == -1:
            continue
            
        state_idx = state[trial]
        chosen_a2 = action_2[trial]
        r = reward[trial]

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        log_loss -= np.log(probs_2[chosen_a2] + eps)
        
        # --- Updates ---
        # Stage 1 PE (SARSA-style)
        delta_stage1 = q_stage2_mf[state_idx, chosen_a2] - q_stage1_mf[chosen_a1]
        
        # Stage 2 PE
        delta_stage2 = r - q_stage2_mf[state_idx, chosen_a2]
        
        # Update Stage 1 MF values (using lr_1)
        q_stage1_mf[chosen_a1] += lr_1 * delta_stage1 + lr_1 * lam * delta_stage2
        
        # Update Stage 2 MF values (using lr_2)
        q_stage2_mf[state_idx, chosen_a2] += lr_2 * delta_stage2

    return log_loss

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF Model with Passive Value Decay.
    
    Incorporates a decay parameter that causes the Q-values of unchosen aliens
    to slowly revert towards zero. This simulates forgetting or the assumption 
    that unobserved rewards might diminish over time, encouraging re-exploration 
    or reflecting memory limits.
    
    Parameters:
    lr (float): Learning rate [0, 1].
    beta_1 (float): Inverse temperature Stage 1 [0, 10].
    beta_2 (float): Inverse temperature Stage 2 [0, 10].
    w (float): MB/MF weight [0, 1].
    lam (float): Eligibility trace [0, 1].
    decay (float): Decay rate for unchosen Stage 2 actions [0, 1].
    """
    lr, beta_1, beta_2, w, lam, decay = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) 
    
    log_loss = 0
    eps = 1e-10

    for trial in range(n_trials):
        
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_1 * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        chosen_a1 = action_1[trial]
        log_loss -= np.log(probs_1[chosen_a1] + eps)
        
        if action_2[trial] == -1:
            continue
            
        state_idx = state[trial]
        chosen_a2 = action_2[trial]
        r = reward[trial]

        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        log_loss -= np.log(probs_2[chosen_a2] + eps)
        
        delta_stage1 = q_stage2_mf[state_idx, chosen_a2] - q_stage1_mf[chosen_a1]
        delta_stage2 = r - q_stage2_mf[state_idx, chosen_a2]
        
        q_stage1_mf[chosen_a1] += lr * delta_stage1 + lr * lam * delta_stage2
        q_stage2_mf[state_idx, chosen_a2] += lr * delta_stage2
        
        # Apply decay to the unchosen action in the current state
        unchosen_a2 = 1 - chosen_a2
        q_stage2_mf[state_idx, unchosen_a2] *= (1 - decay)

    return log_loss

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF Model with Dynamic Transition Learning.
    
    The agent learns the transition matrix online rather than using a fixed one.
    This allows the model-based system to adapt if the participant perceives
    fluctuations in spaceship reliability or does not fully trust the 
    instructed probabilities.
    
    Parameters:
    lr (float): Learning rate for reward values [0, 1].
    lr_trans (float): Learning rate for transition probabilities [0, 1].
    beta_1 (float): Inverse temperature Stage 1 [0, 10].
    beta_2 (float): Inverse temperature Stage 2 [0, 10].
    w (float): MB/MF weight [0, 1].
    lam (float): Eligibility trace [0, 1].
    """
    lr, lr_trans, beta_1, beta_2, w, lam = model_parameters
    n_trials = len(action_1)
    
    # Initialize belief about transitions (started at standard task truth)
    # Rows: Action 1 (Spaceships), Cols: State (Planets)
    trans_probs = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    log_loss = 0
    eps = 1e-10

    for trial in range(n_trials):
        
        # MB Step uses current belief of transitions
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = trans_probs @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_1 * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        chosen_a1 = action_1[trial]
        log_loss -= np.log(probs_1[chosen_a1] + eps)
        
        if action_2[trial] == -1:
            continue
            
        state_idx = state[trial]
        chosen_a2 = action_2[trial]
        r = reward[trial]

        # Update Transition Belief based on observed state
        # Delta rule: P_new = P_old + lr * (1 - P_old) for the observed transition
        trans_probs[chosen_a1, state_idx] += lr_trans * (1 - trans_probs[chosen_a1, state_idx])
        trans_probs[chosen_a1, 1 - state_idx] = 1.0 - trans_probs[chosen_a1, state_idx]

        # Stage 2 Choice
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[chosen_a2] + eps)
        
        # Reward Learning
        delta_stage1 = q_stage2_mf[state_idx, chosen_a2] - q_stage1_mf[chosen_a1]
        delta_stage2 = r - q_stage2_mf[state_idx, chosen_a2]
        
        q_stage1_mf[chosen_a1] += lr * delta_stage1 + lr * lam * delta_stage2
        q_stage2_mf[state_idx, chosen_a2] += lr * delta_stage2

    return log_loss
```