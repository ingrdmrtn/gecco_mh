Here are three new cognitive models for the two-step decision task, designed to capture different potential learning mechanisms such as memory decay, stage-specific learning speeds, and credit assignment via eligibility traces.

### Cognitive Model 1: Model-Free Q-Learning with Forgetting
This model hypothesizes that the participant's value estimates decay over time if an option is not chosen. This "forgetting" mechanism allows the agent to remain flexible in an environment where reward probabilities change slowly, as stated in the task description.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Model 1: Model-Free Q-Learning with Forgetting and Perseveration.
    
    This model implements a Q-learning agent that updates the value of chosen actions
    based on prediction errors. Crucially, it also decays the value of UNCHOSEN actions
    towards a neutral baseline (0.5). This allows the agent to 'forget' old information 
    and potentially re-explore options if reward contingencies change.
    
    Parameters:
    learning_rate: [0, 1] - Rate at which chosen action values are updated.
    decay_rate: [0, 1] - Rate at which unchosen action values decay toward 0.5.
    beta: [0, 10] - Inverse temperature controlling choice stochasticity.
    perseveration: [-3, 3] - Bias added to the previously chosen Stage 1 action.
    """
    learning_rate, decay_rate, beta, perseveration = model_parameters
    n_trials = len(action_1)
  
    # Initialize Q-values to 0.5 (neutral, as rewards are 0 or 1)
    q_stage1_mf = np.zeros(2) + 0.5
    q_stage2_mf = np.zeros((2, 2)) + 0.5
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1

    for trial in range(n_trials):

        # --- Policy for Choice 1 ---
        logits_1 = beta * q_stage1_mf
        if prev_a1 != -1:
            logits_1[prev_a1] += perseveration
        
        # Softmax with numerical stability
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Policy for Choice 2 ---
        logits_2 = beta * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Update Choice 1 ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        # Update chosen action
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        # Decay unchosen action
        unchosen_a1 = 1 - action_1[trial]
        q_stage1_mf[unchosen_a1] = (1 - decay_rate) * q_stage1_mf[unchosen_a1] + decay_rate * 0.5

        # --- Update Choice 2 ---
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Update chosen action
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        # Decay unchosen action
        unchosen_a2 = 1 - action_2[trial]
        q_stage2_mf[state_idx, unchosen_a2] = (1 - decay_rate) * q_stage2_mf[state_idx, unchosen_a2] + decay_rate * 0.5
        
        prev_a1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2: Dual-Learning-Rate Q-Learning
This model posits that the participant learns the values of spaceships (Stage 1) and aliens (Stage 2) at different rates. The structural choice (Stage 1) might be updated more slowly or quickly than the bandit-like choice (Stage 2), allowing for distinct learning dynamics across the two steps.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model 2: Dual-Learning-Rate Q-Learning with Perseveration.
    
    This model assumes the agent has separate learning rates for the first stage
    (spaceship choice) and the second stage (alien choice). This separation allows
    the model to capture differences in volatility or learning difficulty between
    navigating the transition structure and maximizing immediate reward from aliens.
    
    Parameters:
    lr_stage1: [0, 1] - Learning rate for Stage 1 (Spaceships).
    lr_stage2: [0, 1] - Learning rate for Stage 2 (Aliens).
    beta: [0, 10] - Inverse temperature.
    perseveration: [-3, 3] - Bias for repeating the previous Stage 1 choice.
    """
    lr_stage1, lr_stage2, beta, perseveration = model_parameters
    n_trials = len(action_1)
  
    q_stage1_mf = np.zeros(2) + 0.5
    q_stage2_mf = np.zeros((2, 2)) + 0.5
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1

    for trial in range(n_trials):

        # --- Policy for Choice 1 ---
        logits_1 = beta * q_stage1_mf
        if prev_a1 != -1:
            logits_1[prev_a1] += perseveration
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Policy for Choice 2 ---
        logits_2 = beta * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Update Choice 1 ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr_stage1 * delta_stage1

        # --- Update Choice 2 ---
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr_stage2 * delta_stage2
        
        prev_a1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3: TD(lambda) Q-Learning
This model introduces an eligibility trace parameter ($\lambda$). In standard Q-learning (TD-0), the first choice is updated based on the value of the second choice. In this model, the first choice is updated based on a mixture of the second choice's value *and* the final reward outcome. This effectively allows the reward to propagate back to the start state more efficiently.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model 3: TD(lambda) Q-Learning with Perseveration.
    
    This model uses eligibility traces to update Stage 1 values.
    Instead of updating Stage 1 solely based on the Stage 2 value (TD-0), 
    it mixes in the final reward prediction error (TD-1/Monte Carlo).
    The parameter lambda controls this mix: 0 is pure TD-0, 1 is pure Monte Carlo.
    
    Parameters:
    learning_rate: [0, 1] - Base learning rate.
    lambda_eligibility: [0, 1] - Eligibility trace decay parameter.
    beta: [0, 10] - Inverse temperature.
    perseveration: [-3, 3] - Bias for repeating the previous Stage 1 choice.
    """
    learning_rate, lambda_eligibility, beta, perseveration = model_parameters
    n_trials = len(action_1)
  
    q_stage1_mf = np.zeros(2) + 0.5
    q_stage2_mf = np.zeros((2, 2)) + 0.5
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1

    for trial in range(n_trials):

        # --- Policy for Choice 1 ---
        logits_1 = beta * q_stage1_mf
        if prev_a1 != -1:
            logits_1[prev_a1] += perseveration
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Policy for Choice 2 ---
        logits_2 = beta * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Update Choice 1 ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        # Calculate Stage 2 prediction error locally to use for eligibility trace update
        # (Note: q_stage2_mf is not yet updated here, which is correct for TD-lambda)
        current_delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Update Q1 with both the transition error and the weighted final reward error
        q_stage1_mf[action_1[trial]] += learning_rate * (delta_stage1 + lambda_eligibility * current_delta_stage2)

        # --- Update Choice 2 ---
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        prev_a1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```