Here are three cognitive models that analyze the participant's decision-making process using different reinforcement learning hypotheses.

### Cognitive Model 1: Hybrid Model with Perseverance
This model combines Model-Based (MB) and Model-Free (MF) reinforcement learning. It hypothesizes that the participant makes decisions by mixing a planning strategy (MB, using knowledge of transition probabilities) and a caching strategy (MF, learning from direct experience), while also exhibiting a "stickiness" or perseverance bias to repeat the previous Stage 1 choice.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid learner combining Model-Based and Model-Free values with perseverance.
    
    Parameters:
    - learning_rate: [0, 1] Update rate for MF values and Stage 2 values.
    - beta: [0, 10] Inverse temperature for softmax choice.
    - w: [0, 1] Weighting parameter (0 = Pure MF, 1 = Pure MB).
    - perseverance: [0, 5] Bonus added to the previously chosen spaceship.
    """
    learning_rate, beta, w, perseverance = model_parameters
    n_trials = len(action_1)
    
    # Hardcoded transition matrix: A(0)->X(0) is 0.7, U(1)->Y(1) is 0.7
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_mf = np.zeros(2)       # Stage 1 Model-Free values
    q_stage2 = np.zeros((2, 2)) # Stage 2 values (Aliens)
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # Model-Based Value: Expected value based on transition matrix and Stage 2 max values
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value: Weighted sum of MB and MF
        q_net = w * q_mb + (1 - w) * q_mf
        
        # Perseverance Bonus
        if last_action_1 != -1:
            q_net[last_action_1] += perseverance
            
        # Softmax Policy Stage 1
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_action_1 = action_1[trial]
        s_idx = state[trial]
        
        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning Updates ---
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]
        
        # 1. Update Stage 1 MF value (TD(1)-like logic assuming lambda=1 implicitly)
        # Update towards the value of the state reached (Stage 2 Q-value)
        delta1 = q_stage2[s_idx, a2] - q_mf[a1]
        q_mf[a1] += learning_rate * delta1
        
        # 2. Update Stage 2 value based on Reward
        delta2 = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += learning_rate * delta2
        
        # 3. Propagate Reward prediction error back to Stage 1 MF (Eligibility Trace)
        q_mf[a1] += learning_rate * delta2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2: Asymmetric Model-Based with Perseverance
This model modifies the "best so far" Model-Based approach by splitting the learning rate into positive and negative components. It hypothesizes that the participant learns differently from rewards (coins) versus omissions (no coins), while still relying on the transition structure of the task and exhibiting perseverance.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Based learner with asymmetric learning rates for positive/negative errors
    and a perseverance bonus.
    
    Parameters:
    - lr_pos: [0, 1] Learning rate for positive prediction errors (Reward > Expectation).
    - lr_neg: [0, 1] Learning rate for negative prediction errors (Reward < Expectation).
    - beta: [0, 10] Inverse temperature.
    - perseverance: [0, 5] Bonus added to the previously chosen spaceship.
    """
    lr_pos, lr_neg, beta, perseverance = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage2 = np.zeros((2, 2)) # Values for aliens
    last_action_1 = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Decision (Model-Based) ---
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_stage1 = transition_matrix @ max_q_stage2
        
        # Apply Perseverance
        q_net = q_stage1.copy()
        if last_action_1 != -1:
            q_net[last_action_1] += perseverance
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_action_1 = action_1[trial]
        s_idx = state[trial]
        
        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning Updates ---
        a2 = action_2[trial]
        r = reward[trial]
        
        delta = r - q_stage2[s_idx, a2]
        
        # Asymmetric Update
        if delta >= 0:
            q_stage2[s_idx, a2] += lr_pos * delta
        else:
            q_stage2[s_idx, a2] += lr_neg * delta

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3: Model-Free with Eligibility Trace and Perseverance
This model hypothesizes that the participant is not Model-Based at all, but purely Model-Free. To account for the complexity of the two-step task, it uses an eligibility trace parameter (`lam`) to modulate how much the Stage 2 outcome updates the Stage 1 choice. It also includes perseverance to explain the streaks in the data.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free learner with eligibility traces (TD-lambda) and perseverance.
    
    Parameters:
    - learning_rate: [0, 1] Rate of updating Q-values.
    - beta: [0, 10] Inverse temperature.
    - lam: [0, 1] Eligibility trace decay (lambda). Controls how much Stage 2 reward updates Stage 1.
    - perseverance: [0, 5] Bonus added to the previously chosen spaceship.
    """
    learning_rate, beta, lam, perseverance = model_parameters
    n_trials = len(action_1)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)       # Stage 1 Q-values (MF)
    q_stage2 = np.zeros((2, 2))  # Stage 2 Q-values
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        q_net = q_stage1.copy()
        if last_action_1 != -1:
            q_net[last_action_1] += perseverance
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_action_1 = action_1[trial]
        s_idx = state[trial]
        
        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning Updates ---
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]
        
        # TD Error 1: Difference between Stage 2 Value and Stage 1 Expectation
        delta1 = q_stage2[s_idx, a2] - q_stage1[a1]
        q_stage1[a1] += learning_rate * delta1
        
        # TD Error 2: Difference between Reward and Stage 2 Expectation
        delta2 = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += learning_rate * delta2
        
        # Eligibility Trace: Propagate Stage 2 error back to Stage 1 scaled by lambda
        q_stage1[a1] += learning_rate * lam * delta2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```