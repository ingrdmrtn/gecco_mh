Here are the three cognitive models based on the two-step task structure and the participant data provided.

### Model 1: Hybrid MB/MF with Separate Stage Learning Rates
This model hypothesizes that the participant learns the value of the spaceship choices (Stage 1) and the alien choices (Stage 2) at different rates. This captures potential differences in how credit is assigned to immediate outcomes versus distal choices.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF model with separate learning rates for Stage 1 and Stage 2.
    
    Parameters:
    lr1 (float): Learning rate for Stage 1 updates [0,1].
    lr2 (float): Learning rate for Stage 2 updates [0,1].
    beta (float): Inverse temperature [0,10].
    w (float): MB/MF Weighting [0,1].
    """
    lr1, lr2, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue

        # Policy for the first choice (Stage 1)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # Policy for the second choice (Stage 2)
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Prediction errors
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Action value updating for choice 1 (using lr1)
        # Includes immediate Step 1 error and propagated Step 2 error (eligibility trace = 1)
        q_stage1_mf[action_1[trial]] += lr1 * delta_stage1
        q_stage1_mf[action_1[trial]] += lr1 * delta_stage2
        
        # Action value updating for choice 2 (using lr2)
        q_stage2_mf[state_idx, action_2[trial]] += lr2 * delta_stage2

    eps = 1e-10
    valid_mask = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss
```

### Model 2: Hybrid MB/MF with Memory Decay
This model introduces a forgetting mechanism. Since the probabilities of the aliens providing gold change slowly, older values may become obsolete. A decay parameter reduces the Q-values of all options on each trial, effectively modeling a "leak" in memory or a prior for uncertainty.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF model with value decay (forgetting).
    
    Parameters:
    lr (float): Learning rate [0,1].
    beta (float): Inverse temperature [0,10].
    w (float): MB/MF Weighting [0,1].
    decay (float): Decay rate for Q-values per trial [0,1].
    """
    lr, beta, w, decay = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue

        # Apply decay to all Q-values before choice
        q_stage1_mf *= (1 - decay)
        q_stage2_mf *= (1 - decay)

        # Policy for the first choice (Stage 1)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # Policy for the second choice (Stage 2)
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Prediction errors
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Action value updating for choice 1
        q_stage1_mf[action_1[trial]] += lr * delta_stage1
        q_stage1_mf[action_1[trial]] += lr * delta_stage2
        
        # Action value updating for choice 2
        q_stage2_mf[state_idx, action_2[trial]] += lr * delta_stage2

    eps = 1e-10
    valid_mask = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss
```

### Model 3: Hybrid MB/MF with Asymmetric Learning and Perseveration
This model combines asymmetric learning rates (bias towards positive or negative prediction errors) with choice perseveration (stickiness). This configuration tests if the participant learns differently from gains vs. losses while simultaneously accounting for the tendency to repeat previous motor actions.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF model with asymmetric learning rates and choice perseveration.
    
    Parameters:
    lr_pos (float): Learning rate for positive prediction errors [0,1].
    lr_neg (float): Learning rate for negative prediction errors [0,1].
    beta (float): Inverse temperature [0,10].
    w (float): MB/MF Weighting [0,1].
    perseveration (float): Stickiness bonus for repeating Stage 1 choice [0,5].
    """
    lr_pos, lr_neg, beta, w, perseveration = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue

        # Policy for the first choice (Stage 1)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Add perseveration bonus
        pers_vec = np.zeros(2)
        if last_action_1 != -1:
            pers_vec[last_action_1] = 1.0
            
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf + perseveration * pers_vec
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # Policy for the second choice (Stage 2)
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Prediction errors
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Action value updating for choice 1 (Asymmetric LR)
        lr_s1 = lr_pos if delta_stage1 > 0 else lr_neg
        q_stage1_mf[action_1[trial]] += lr_s1 * delta_stage1
        
        # Propagate Stage 2 error to Stage 1 (Asymmetric LR based on outcome)
        lr_s2 = lr_pos if delta_stage2 > 0 else lr_neg
        q_stage1_mf[action_1[trial]] += lr_s2 * delta_stage2
        
        # Action value updating for choice 2 (Asymmetric LR)
        q_stage2_mf[state_idx, action_2[trial]] += lr_s2 * delta_stage2
        
        last_action_1 = action_1[trial]

    eps = 1e-10
    valid_mask = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss
```