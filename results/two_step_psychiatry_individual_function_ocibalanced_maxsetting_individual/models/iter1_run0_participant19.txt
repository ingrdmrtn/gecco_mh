Here are three new cognitive models for the two-step task, formulated as Python functions.

### Model 1: Model-Free with Eligibility Traces
This model introduces an eligibility trace parameter ($\lambda$). In a standard Model-Free Temporal Difference (TD) learner, the first-stage value is updated only based on the second-stage value. With eligibility traces, the reward received at the second stage also directly updates the first-stage choice, weighted by $\lambda$. This mechanism helps bridge the delay between the first choice and the final reward, which may explain the participant's ability to maintain streaks of successful choices.

```python
def cognitive_model1_mf_eligibility(action_1, state, action_2, reward, model_parameters):
    """
    Model 1: Model-Free with Eligibility Traces (TD-lambda).
    
    This model assumes the participant learns purely from experience but uses 
    eligibility traces to assign credit for the second-stage reward back to the 
    first-stage choice. This bridges the gap between the first action and the 
    final outcome more effectively than simple TD(0).
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for value updates.
    - lambda_param: [0, 1] Decay parameter for eligibility traces (0=TD(0), 1=Monte Carlo).
    - beta: [0, 10] Inverse temperature for softmax choice.
    """
    learning_rate, lambda_param, beta = model_parameters
    n_trials = len(action_1)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values initialized to 0
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2)) # State (Planet) x Action (Alien)
    
    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        s2 = state[trial] # Planet arrived at
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]
        
        # --- Learning ---
        # Prediction Error 1: Value of state 2 - Value of action 1
        delta1 = q_stage2[s2, a2] - q_stage1[a1]
        
        # Prediction Error 2: Reward - Value of state 2
        delta2 = r - q_stage2[s2, a2]
        
        # Update Stage 2 Q-values (Standard TD)
        q_stage2[s2, a2] += learning_rate * delta2
        
        # Update Stage 1 Q-values (TD-lambda)
        # The update includes the immediate error (delta1) and the propagated error (lambda * delta2)
        q_stage1[a1] += learning_rate * (delta1 + lambda_param * delta2)
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model-Based / Model-Free
This model posits that the participant uses a mixture of Model-Based (MB) planning and Model-Free (MF) learning. The MB component calculates the expected value of the first stage by using the known transition matrix and the values of the second stage. The MF component learns values directly from experience. A mixing parameter $w$ determines the relative reliance on each system.

```python
def cognitive_model2_hybrid(action_1, state, action_2, reward, model_parameters):
    """
    Model 2: Hybrid Model-Based / Model-Free.
    
    This model assumes the participant uses a combination of Model-Based (MB) 
    planning (using knowledge of transition probabilities) and Model-Free (MF) 
    learning (using direct experience). The influence of each is weighted by 'w'.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for MF value updates.
    - w: [0, 1] Weight parameter (0 = Pure MF, 1 = Pure MB).
    - beta: [0, 10] Inverse temperature for softmax choice.
    """
    learning_rate, w, beta = model_parameters
    n_trials = len(action_1)
    
    # Transition matrix: Row 0 (Space A) -> [0.7 Planet X, 0.3 Planet Y]
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based Value: Expected value of best option in next state
        max_q2 = np.max(q_stage2, axis=1) # [Max(Planet0), Max(Planet1)]
        q_stage1_mb = transition_matrix @ max_q2
        
        # Net Value: Weighted sum
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        s2 = state[trial]
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]
        
        # --- Learning ---
        # Update MF values
        delta1 = q_stage2[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta1
        
        delta2 = r - q_stage2[s2, a2]
        q_stage2[s2, a2] += learning_rate * delta2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Model-Free with Choice Stickiness
The participant data shows significant "streaks" where the same spaceship is chosen repeatedly. This model adds a "stickiness" or "perseveration" parameter to a standard Model-Free learner. This parameter adds a bonus to the logit of the previously chosen action, biasing the participant to repeat their last choice regardless of the reward outcome, which captures habit-like behavior.

```python
def cognitive_model3_mf_stickiness(action_1, state, action_2, reward, model_parameters):
    """
    Model 3: Model-Free with Choice Stickiness (Perseveration).
    
    This model extends the standard Model-Free learner by adding a 'stickiness' 
    parameter. This parameter adds a bonus to the choice made in the previous 
    trial, capturing the tendency to repeat actions regardless of reward.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for value updates.
    - stickiness: [0, 5] Bonus added to the logit of the previously chosen spaceship.
    - beta: [0, 10] Inverse temperature for softmax choice.
    """
    learning_rate, stickiness, beta = model_parameters
    n_trials = len(action_1)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        logits = beta * q_stage1
        # Add stickiness bonus to the previously chosen action
        if last_action_1 != -1:
            logits[last_action_1] += stickiness
            
        # Softmax with potentially modified logits
        exp_q1 = np.exp(logits) 
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_action_1 = action_1[trial]
        a1 = action_1[trial]
        s2 = state[trial]
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]
        
        # --- Learning ---
        # Standard MF updates
        delta1 = q_stage2[s2, a2] - q_stage1[a1]
        q_stage1[a1] += learning_rate * delta1
        
        delta2 = r - q_stage2[s2, a2]
        q_stage2[s2, a2] += learning_rate * delta2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```