Here are three cognitive models based on the participant data and the two-step task structure.

### Cognitive Model 1: Hybrid Model with Choice Stickiness
This model extends the baseline Hybrid (MB/MF) model by adding a **stickiness** parameter. The participant data shows repeated selection of the same spaceship for long blocks (e.g., trials 1-6, 18-24). Stickiness accounts for the tendency to repeat the previous choice regardless of reward history (perseveration).

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based and Model-Free agent with Choice Stickiness.
    
    Incorporates a 'stickiness' parameter that adds a bonus to the value of the 
    previously chosen spaceship, capturing the participant's tendency to repeat actions.

    Parameters:
    - learning_rate: [0,1] Rate at which Q-values are updated.
    - beta_1: [0,10] Inverse temperature for Stage 1 softmax.
    - beta_2: [0,10] Inverse temperature for Stage 2 softmax.
    - w: [0,1] Weighting parameter. 1 = Pure Model-Based, 0 = Pure Model-Free.
    - stickiness: [0,10] Bonus added to the logit of the previously chosen action.
    """
    learning_rate, beta_1, beta_2, w, stickiness = model_parameters
    n_trials = len(action_1)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)      
    q_stage2_mf = np.zeros((2, 2)) 
    
    last_action_1 = -1

    for trial in range(n_trials):
        # Skip missing data
        if action_1[trial] == -1:
            continue

        # --- Stage 1 Policy ---
        # Model-Based Value: Expected max value of next stage based on transition probs
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2

        # Hybrid Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Calculate Logits (Values * Beta)
        logits = beta_1 * q_net
        
        # Add Stickiness: Bonus to the action taken in the previous trial
        if last_action_1 != -1:
            logits[last_action_1] += stickiness

        # Softmax
        exp_q1 = np.exp(logits - np.max(logits)) # Subtract max for numerical stability
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Store current action for next trial's stickiness
        last_action_1 = action_1[trial]

        # --- Stage 2 Policy ---
        state_idx = state[trial] # Current Planet
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        # Stage 1 MF Update (TD(1) logic effectively, linking Stage 2 val to Stage 1)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1

        # Stage 2 MF Update (Reward Prediction Error)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
    eps = 1e-10
    # Calculate NLL only for valid trials
    valid_indices = p_choice_1 > 0
    log_loss = -(np.sum(np.log(p_choice_1[valid_indices] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_indices] + eps)))
    return log_loss
```

### Cognitive Model 2: Hybrid Model with Asymmetric Learning Rates
This model investigates if the participant learns differently from positive prediction errors (better than expected) versus negative prediction errors (worse than expected). This is common in reinforcement learning where agents may be risk-averse (learning more from punishment) or optimistic (learning more from reward).

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Asymmetric Learning Rates for Positive/Negative Prediction Errors.
    
    Splits the learning rate into alpha_pos and alpha_neg to capture biases in 
    updating values based on whether the outcome was better or worse than expected.

    Parameters:
    - alpha_pos: [0,1] Learning rate when prediction error is positive.
    - alpha_neg: [0,1] Learning rate when prediction error is negative.
    - beta_1: [0,10] Inverse temperature for Stage 1.
    - beta_2: [0,10] Inverse temperature for Stage 2.
    - w: [0,1] Weighting parameter (MB vs MF).
    """
    alpha_pos, alpha_neg, beta_1, beta_2, w = model_parameters
    n_trials = len(action_1)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)      
    q_stage2_mf = np.zeros((2, 2)) 

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf

        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        lr_1 = alpha_pos if delta_stage1 > 0 else alpha_neg
        q_stage1_mf[action_1[trial]] += lr_1 * delta_stage1

        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        lr_2 = alpha_pos if delta_stage2 > 0 else alpha_neg
        q_stage2_mf[state_idx, action_2[trial]] += lr_2 * delta_stage2
        
    eps = 1e-10
    valid_indices = p_choice_1 > 0
    log_loss = -(np.sum(np.log(p_choice_1[valid_indices] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_indices] + eps)))
    return log_loss
```

### Cognitive Model 3: Hybrid Model with Stage-Specific Learning Rates
This model posits that learning plasticity differs between the two stages. Stage 1 involves abstract choices about spaceships (distal predictors), while Stage 2 involves concrete choices about aliens (proximal reward predictors). The participant might update expectations about aliens (`alpha_2`) at a different rate than expectations about spaceships (`alpha_1`).

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Stage-Specific Learning Rates.
    
    Uses separate learning rates for Stage 1 (Spaceship choice) and Stage 2 (Alien choice).
    This accounts for potentially different plasticity in learning distal vs proximal values.

    Parameters:
    - alpha_1: [0,1] Learning rate for Stage 1 MF values.
    - alpha_2: [0,1] Learning rate for Stage 2 values.
    - beta_1: [0,10] Inverse temperature for Stage 1.
    - beta_2: [0,10] Inverse temperature for Stage 2.
    - w: [0,1] Weighting parameter (MB vs MF).
    """
    alpha_1, alpha_2, beta_1, beta_2, w = model_parameters
    n_trials = len(action_1)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)      
    q_stage2_mf = np.zeros((2, 2)) 

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf

        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        
        # Stage 1 Update uses alpha_1
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += alpha_1 * delta_stage1

        # Stage 2 Update uses alpha_2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += alpha_2 * delta_stage2
        
    eps = 1e-10
    valid_indices = p_choice_1 > 0
    log_loss = -(np.sum(np.log(p_choice_1[valid_indices] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_indices] + eps)))
    return log_loss
```