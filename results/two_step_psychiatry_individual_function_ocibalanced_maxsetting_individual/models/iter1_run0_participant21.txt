Here are three new cognitive models based on the participant data and the two-step task structure.

### Model 1: Hybrid MB/MF with Asymmetric Learning and Stickiness
This model extends the previous best approach by acknowledging that participants often update their value estimates differently following positive outcomes (wins) versus negative outcomes (omissions). This "positivity bias" or "negativity bias" is common in reinforcement learning. It includes stickiness to account for the participant's blocky choice behavior.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF agent with Asymmetric Learning Rates and Stickiness.
    
    Distinguishes between learning from positive prediction errors (doing better than expected)
    and negative prediction errors (doing worse than expected).
    
    Parameters:
    alpha_pos: [0, 1] Learning rate for positive prediction errors.
    alpha_neg: [0, 1] Learning rate for negative prediction errors.
    beta_1: [0, 10] Inverse temperature for Stage 1.
    beta_2: [0, 10] Inverse temperature for Stage 2.
    w: [0, 1] Weighting between MB (1) and MF (0).
    stickiness: [0, 5] Choice perseverance bonus for Stage 1.
    """
    alpha_pos, alpha_neg, beta_1, beta_2, w, stickiness = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    
    last_action_1 = -1

    for trial in range(n_trials):

        # --- Policy for Choice 1 ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Policy for Choice 2 ---
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta_2 * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        # Stage 1 MF Update (TD(0))
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        eff_alpha_1 = alpha_pos if delta_stage1 > 0 else alpha_neg
        q_stage1_mf[action_1[trial]] += eff_alpha_1 * delta_stage1
        
        # Stage 2 MF Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        eff_alpha_2 = alpha_pos if delta_stage2 > 0 else alpha_neg
        q_stage2_mf[state_idx, action_2[trial]] += eff_alpha_2 * delta_stage2
        
        last_action_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid MB/MF with Forgetful Decay and Stickiness
In dynamic environments where reward probabilities drift (as described in the task), old information becomes less reliable. This model implements "forgetting" or value decay, where Q-values for all options slowly decay toward a neutral point (0.5) on every trial. This helps the agent adapt to changing probabilities better than standard RL.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF agent with Value Decay (Forgetting) and Stickiness.
    
    Q-values decay toward 0.5 on every trial to handle drifting reward probabilities.
    
    Parameters:
    learning_rate: [0, 1] Rate of updating chosen options.
    decay_rate: [0, 1] Rate at which values decay toward 0.5 (forgetting).
    beta_1: [0, 10] Inverse temperature for Stage 1.
    beta_2: [0, 10] Inverse temperature for Stage 2.
    w: [0, 1] Weighting between MB (1) and MF (0).
    stickiness: [0, 5] Choice perseverance bonus for Stage 1.
    """
    learning_rate, decay_rate, beta_1, beta_2, w, stickiness = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Initialize at 0.5 (neutral) since we decay towards this
    q_stage1_mf = np.full(2, 0.5) 
    q_stage2_mf = np.full((2, 2), 0.5)

    last_action_1 = -1

    for trial in range(n_trials):
        
        # Decay all values toward 0.5 before decision/update
        q_stage1_mf = q_stage1_mf * (1 - decay_rate) + 0.5 * decay_rate
        q_stage2_mf = q_stage2_mf * (1 - decay_rate) + 0.5 * decay_rate

        # --- Policy for Choice 1 ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # --- Policy for Choice 2 ---
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta_2 * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        last_action_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid MB/MF with Separate Stage Learning Rates
This model posits that the participant learns the value of Spaceships (Stage 1) and Aliens (Stage 2) at different speeds. For instance, they might update their beliefs about aliens rapidly (high volatility) but their beliefs about spaceships slowly (low volatility). This separation can capture complex dynamics where the agent is confident about the transition structure but uncertain about the rewards, or vice versa.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF agent with Separate Stage Learning Rates and Stickiness.
    
    Uses distinct learning rates for the Stage 1 (Spaceship) and Stage 2 (Alien) 
    updates, allowing for different timescales of learning at each level.
    
    Parameters:
    alpha_1: [0, 1] Learning rate for Stage 1 (Spaceship choice).
    alpha_2: [0, 1] Learning rate for Stage 2 (Alien choice).
    beta_1: [0, 10] Inverse temperature for Stage 1.
    beta_2: [0, 10] Inverse temperature for Stage 2.
    w: [0, 1] Weighting between MB (1) and MF (0).
    stickiness: [0, 5] Choice perseverance bonus for Stage 1.
    """
    alpha_1, alpha_2, beta_1, beta_2, w, stickiness = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    last_action_1 = -1

    for trial in range(n_trials):

        # --- Policy for Choice 1 ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # --- Policy for Choice 2 ---
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta_2 * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        # Stage 1 updated with alpha_1
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += alpha_1 * delta_stage1
        
        # Stage 2 updated with alpha_2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += alpha_2 * delta_stage2
        
        last_action_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```