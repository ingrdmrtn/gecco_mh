Here are three new cognitive models for the two-step decision task, designed to capture different potential mechanisms underlying the participant's behavior.

### Model 1: Hybrid Model with Separate Stage Learning Rates
This model hypothesizes that the participant updates their valuation of the spaceships (Stage 1) at a different rate than their valuation of the aliens (Stage 2). This distinction is useful if the participant perceives the stability of the two stages differently (e.g., stable spaceship transitions vs. drifting alien rewards).

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Separate Learning Rates for Stage 1 and Stage 2.
    
    This model allows for different update speeds for the first-stage (spaceship)
    predictions and the second-stage (alien) reward predictions.
    
    Parameters:
    lr_1: [0,1] - Learning rate for Stage 1 (Spaceship values).
    lr_2: [0,1] - Learning rate for Stage 2 (Alien values).
    beta_1: [0,10] - Inverse temperature for Stage 1 choices.
    beta_2: [0,10] - Inverse temperature for Stage 2 choices.
    w: [0,1] - Weighting between Model-Based and Model-Free values (1=Pure MB, 0=Pure MF).
    stick: [0,5] - Choice stickiness for Stage 1 (tendency to repeat spaceship choice).
    """
    lr_1, lr_2, beta_1, beta_2, w, stick = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    
    prev_action_1 = -1

    for trial in range(n_trials):
        # Handle missing/invalid data
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
            continue

        # --- Stage 1 Policy ---
        # Model-Based Value: Expected max value of next stage weighted by transition probs
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax with stickiness
        logits_1 = beta_1 * q_net
        if prev_action_1 != -1:
            logits_1[prev_action_1] += stick
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Policy ---
        state_idx = state[trial]
        logits_2 = beta_2 * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        # Stage 1 Update (TD-0) using lr_1
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr_1 * delta_stage1

        # Stage 2 Update using lr_2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr_2 * delta_stage2
        
        prev_action_1 = action_1[trial]

    eps = 1e-10
    valid_trials = (p_choice_1 > 0) & (p_choice_2 > 0)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```

### Model 2: Hybrid Model with Passive Decay
This model incorporates a "forgetting" mechanism. Since the reward probabilities of the aliens change slowly over time, older Q-values become unreliable. This model decays the Q-values of unchosen actions, allowing the agent to adapt to non-stationarity and potentially encouraging re-exploration of neglected options.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Passive Decay (Forgetting).
    
    Incorporates a decay parameter that reduces the value of unchosen actions 
    on each trial, helping the model adapt to the drifting reward probabilities.
    
    Parameters:
    lr: [0,1] - Learning rate for prediction errors.
    beta_1: [0,10] - Inverse temperature for Stage 1.
    beta_2: [0,10] - Inverse temperature for Stage 2.
    w: [0,1] - MB/MF weighting.
    stick: [0,5] - Choice stickiness for Stage 1.
    decay: [0,1] - Decay rate for unchosen Q-values (0=no decay, 1=instant forgetting).
    """
    lr, beta_1, beta_2, w, stick, decay = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
            continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta_1 * q_net
        if prev_action_1 != -1:
            logits_1[prev_action_1] += stick
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Policy ---
        state_idx = state[trial]
        logits_2 = beta_2 * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        # Update Chosen Stage 1
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr * delta_stage1
        # Decay Unchosen Stage 1
        q_stage1_mf[1 - action_1[trial]] *= (1 - decay)

        # Update Chosen Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr * delta_stage2
        # Decay Unchosen Stage 2 (in the current state only)
        q_stage2_mf[state_idx, 1 - action_2[trial]] *= (1 - decay)
        
        prev_action_1 = action_1[trial]

    eps = 1e-10
    valid_trials = (p_choice_1 > 0) & (p_choice_2 > 0)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```

### Model 3: Hybrid Model with Dynamic Transition Learning
Standard models assume the participant believes the transition probabilities are fixed (70%/30%). This model allows the participant to learn the transition matrix from experience. If the participant experiences a streak of "rare" transitions, they may update their belief about which spaceship leads to which planet, affecting their Model-Based value calculation.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Dynamic Transition Learning.
    
    The participant does not assume fixed transition probabilities (0.7/0.3) but 
    updates their belief about the spaceship-planet transitions based on experience.
    
    Parameters:
    lr_rew: [0,1] - Learning rate for reward values (Q-values).
    lr_trans: [0,1] - Learning rate for transition probabilities.
    beta_1: [0,10] - Inverse temperature for Stage 1.
    beta_2: [0,10] - Inverse temperature for Stage 2.
    w: [0,1] - MB/MF weighting.
    stick: [0,5] - Choice stickiness for Stage 1.
    """
    lr_rew, lr_trans, beta_1, beta_2, w, stick = model_parameters
    n_trials = len(action_1)
  
    # Initialize with instructed probabilities
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
            continue

        # --- Stage 1 Policy ---
        # Use CURRENT transition_matrix for MB calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta_1 * q_net
        if prev_action_1 != -1:
            logits_1[prev_action_1] += stick
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Policy ---
        state_idx = state[trial]
        logits_2 = beta_2 * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Value Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr_rew * delta_stage1

        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr_rew * delta_stage2
        
        # --- Transition Matrix Update ---
        # Update the row corresponding to the chosen spaceship
        # Increase prob of the observed state, decrease prob of the other state
        chosen_spaceship = action_1[trial]
        observed_planet = state[trial]
        
        # Apply delta rule to transitions: T_new = T_old + lr * (Outcome - T_old)
        # Outcome is 1 for the observed transition, 0 for the unobserved
        transition_matrix[chosen_spaceship, observed_planet] += lr_trans * (1 - transition_matrix[chosen_spaceship, observed_planet])
        transition_matrix[chosen_spaceship, 1 - observed_planet] += lr_trans * (0 - transition_matrix[chosen_spaceship, 1 - observed_planet])
        
        prev_action_1 = action_1[trial]

    eps = 1e-10
    valid_trials = (p_choice_1 > 0) & (p_choice_2 > 0)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```