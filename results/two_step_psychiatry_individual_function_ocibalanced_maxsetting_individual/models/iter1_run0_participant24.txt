Here are three new cognitive models for the two-step decision task.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Model 1: Model-Free TD(lambda) with Separate Stage Learning Rates.
    
    This model distinguishes between the learning rate for the first stage 
    (spaceship choice) and the second stage (alien choice). This captures 
    potential differences in plasticity between the distal prediction (stage 1) 
    and proximal reward association (stage 2).
    
    Parameters:
    alpha1: [0, 1] - Learning rate for Stage 1 Q-values.
    alpha2: [0, 1] - Learning rate for Stage 2 Q-values.
    beta: [0, 10] - Inverse temperature (shared across stages).
    lam: [0, 1] - Eligibility trace parameter (0=TD(0), 1=Monte Carlo).
    pers: [0, 5] - Perseverance bonus for repeating the previous Stage 1 choice.
    """
    alpha1, alpha2, beta, lam, pers = model_parameters
    n_trials = len(action_1)

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    
    last_action_1 = -1
    log_loss = 0.0
    eps = 1e-10

    for trial in range(n_trials):
        # Skip missing data
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1 or reward[trial] == -1:
            last_action_1 = -1
            continue

        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = float(reward[trial])

        # Stage 1 Choice Likelihood
        q_net_1 = q_stage1_mf.copy()
        if last_action_1 != -1:
            q_net_1[last_action_1] += pers
        
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + eps)

        # Stage 2 Choice Likelihood
        exp_q2 = np.exp(beta * q_stage2_mf[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[a2] + eps)

        # Value Updates
        # Stage 1 update (TD error from Stage 2 value)
        delta_1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha1 * delta_1

        # Stage 2 update (TD error from Reward)
        delta_2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha2 * delta_2

        # Eligibility Trace update for Stage 1 (propagate Reward error)
        # We use alpha1 here as it updates the Stage 1 value
        q_stage1_mf[a1] += alpha1 * lam * delta_2

        last_action_1 = a1

    return log_loss

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model 2: Hybrid Model-Based / Model-Free with Separate Inverse Temperatures.
    
    This model combines Model-Based (transition-matrix dependent) and Model-Free 
    value estimation. Crucially, it uses different inverse temperature (beta) 
    parameters for Stage 1 and Stage 2 choices, allowing for different levels 
    of exploration/exploitation or decision noise at each step.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate for Q-values (MF).
    beta1: [0, 10] - Inverse temperature for Stage 1 choice.
    beta2: [0, 10] - Inverse temperature for Stage 2 choice.
    w: [0, 1] - Weight of Model-Based control (0 = Pure MF, 1 = Pure MB).
    lam: [0, 1] - Eligibility trace.
    pers: [0, 5] - Perseverance bonus for Stage 1.
    """
    learning_rate, beta1, beta2, w, lam, pers = model_parameters
    n_trials = len(action_1)

    # Transition matrix: Row 0 (Action A) -> [0.7 to X, 0.3 to Y], Row 1 (Action U) -> [0.3 to X, 0.7 to Y]
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    log_loss = 0.0
    eps = 1e-10

    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1 or reward[trial] == -1:
            last_action_1 = -1
            continue

        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = float(reward[trial])

        # Calculate Model-Based Values for Stage 1
        # V_MB(s1, a1) = Sum(P(s'|s1,a1) * max_a' Q_MF(s', a'))
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2

        # Integrated Net Value
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add perseverance
        if last_action_1 != -1:
            q_net_1[last_action_1] += pers

        # Stage 1 Choice (using beta1)
        exp_q1 = np.exp(beta1 * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + eps)

        # Stage 2 Choice (using beta2)
        exp_q2 = np.exp(beta2 * q_stage2_mf[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[a2] + eps)

        # Updates (MF)
        delta_1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_1

        delta_2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += learning_rate * delta_2

        q_stage1_mf[a1] += learning_rate * lam * delta_2

        last_action_1 = a1

    return log_loss

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model 3: Model-Free TD(lambda) with Reward-Modulated Perseverance.
    
    Extends the standard perseverance model by separating 'stickiness' into 
    general repetition (pers) and reward-dependent repetition (pers_rew).
    This captures heuristic 'Win-Stay' behavior explicitly, independent of 
    value learning.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate.
    beta: [0, 10] - Inverse temperature.
    lam: [0, 1] - Eligibility trace.
    pers: [0, 5] - General perseverance (stickiness).
    pers_rew: [0, 5] - Additional perseverance bonus if the previous trial was rewarded.
    """
    learning_rate, beta, lam, pers, pers_rew = model_parameters
    n_trials = len(action_1)

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    last_reward = 0.0
    log_loss = 0.0
    eps = 1e-10

    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1 or reward[trial] == -1:
            last_action_1 = -1
            last_reward = 0.0
            continue

        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = float(reward[trial])

        # Stage 1 Choice with Reward-Modulated Perseverance
        q_net_1 = q_stage1_mf.copy()
        if last_action_1 != -1:
            q_net_1[last_action_1] += pers
            if last_reward == 1.0:
                 q_net_1[last_action_1] += pers_rew
        
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + eps)

        # Stage 2 Choice
        exp_q2 = np.exp(beta * q_stage2_mf[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[a2] + eps)

        # Updates
        delta_1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_1

        delta_2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += learning_rate * delta_2

        q_stage1_mf[a1] += learning_rate * lam * delta_2

        last_action_1 = a1
        last_reward = r

    return log_loss
```