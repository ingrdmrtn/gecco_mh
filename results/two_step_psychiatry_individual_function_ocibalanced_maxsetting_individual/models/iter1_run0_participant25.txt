Here are 3 new cognitive models based on the participant's behavior and the feedback provided.

The participant data shows strong persistence (stickiness) in Stage 1 choices (e.g., choosing Spaceship 0 for trials 1-21 despite mixed rewards) and effective learning in Stage 2. The best model so far was TD($\lambda$), which links Stage 2 outcomes to Stage 1 choices. The proposed models extend this by exploring mechanisms for persistence (stickiness), valence-dependent learning (asymmetry), and hierarchical differences in learning speeds.

### Model 1: TD($\lambda$) with Choice Stickiness
This model adds a "stickiness" or perseveration parameter to the TD($\lambda$) framework. The participant exhibits long streaks of repeating the same spaceship choice. This model posits that the participant has a bias to repeat their previous Stage 1 choice, independent of the reward history, which helps explain the inertia observed in the data.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    TD(lambda) with Choice Stickiness.
    
    Extends the TD(lambda) model by adding a stickiness bonus to the previously 
    chosen Stage 1 action. This captures the participant's tendency to repeat 
    choices (inertia) regardless of the reward outcome.

    Parameters:
    - learning_rate: [0, 1] Rate at which Q-values are updated.
    - beta: [0, 10] Inverse temperature for Softmax.
    - lam: [0, 1] Eligibility trace decay (credit assignment to Stage 1).
    - stickiness: [0, 5] Bonus added to the Q-value of the previously chosen action.
    """
    learning_rate, beta, lam, stickiness = model_parameters
    n_trials = len(action_1)

    q_stage1 = np.zeros(2) + 0.5
    q_stage2 = np.zeros((2, 2)) + 0.5
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Add stickiness bonus to the Q-value of the previous choice
        q_1_effective = q_stage1.copy()
        if prev_a1 != -1:
            q_1_effective[prev_a1] += stickiness
        
        exp_q1 = np.exp(beta * q_1_effective)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Choice ---
        s_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2[s_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        r = reward[trial]
        
        # Stage 1 Prediction Error (TD(0))
        # driven by the value of the state arrived at (s_idx)
        pe_1 = q_stage2[s_idx, a2] - q_stage1[a1]
        q_stage1[a1] += learning_rate * pe_1
        
        # Stage 2 Prediction Error
        pe_2 = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += learning_rate * pe_2
        
        # Eligibility Trace Update
        # Propagate Stage 2 PE back to Stage 1 choice
        q_stage1[a1] += learning_rate * lam * pe_2
        
        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Asymmetric TD($\lambda$) Learning
This model hypothesizes that the participant learns differently from positive outcomes (rewards) versus negative outcomes (omissions). Given the participant stays with a choice even after failures (e.g., Trials 3, 6, 18), they may have a lower learning rate for negative prediction errors, effectively ignoring losses while reinforcing wins.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Asymmetric TD(lambda) Learning.
    
    Uses separate learning rates for positive and negative prediction errors.
    This allows the model to capture biases such as "optimism" (ignoring losses)
    or "pessimism" (overweighting losses), combined with eligibility traces.

    Parameters:
    - lr_pos: [0, 1] Learning rate for positive prediction errors (Better than expected).
    - lr_neg: [0, 1] Learning rate for negative prediction errors (Worse than expected).
    - beta: [0, 10] Inverse temperature.
    - lam: [0, 1] Eligibility trace decay.
    """
    lr_pos, lr_neg, beta, lam = model_parameters
    n_trials = len(action_1)

    q_stage1 = np.zeros(2) + 0.5
    q_stage2 = np.zeros((2, 2)) + 0.5
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Choice ---
        s_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2[s_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        r = reward[trial]
        
        # Stage 1 Update
        pe_1 = q_stage2[s_idx, a2] - q_stage1[a1]
        lr_1 = lr_pos if pe_1 > 0 else lr_neg
        q_stage1[a1] += lr_1 * pe_1
        
        # Stage 2 Update
        pe_2 = r - q_stage2[s_idx, a2]
        lr_2 = lr_pos if pe_2 > 0 else lr_neg
        q_stage2[s_idx, a2] += lr_2 * pe_2
        
        # Trace Update
        # We use the learning rate associated with the Stage 2 error
        q_stage1[a1] += lr_2 * lam * pe_2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Dual-Stage Learning Rate TD($\lambda$)
This model separates the learning rates for Stage 1 (Spaceships) and Stage 2 (Aliens). The participant might update their preferences for the spaceships (navigation) slowly to maintain stability, while updating preferences for aliens (immediate reward sources) quickly. This hierarchical difference can explain the stability in spaceship choice despite fluctuations in alien rewards.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Dual-Stage Learning Rate TD(lambda).
    
    Applies different learning rates to Stage 1 (Spaceship) and Stage 2 (Alien) 
    updates. This captures the possibility that the participant learns the 
    structure/navigation values at a different speed than the direct reward values.

    Parameters:
    - lr_stage1: [0, 1] Learning rate for Stage 1 (Spaceship) values.
    - lr_stage2: [0, 1] Learning rate for Stage 2 (Alien) values.
    - beta: [0, 10] Inverse temperature.
    - lam: [0, 1] Eligibility trace decay.
    """
    lr_stage1, lr_stage2, beta, lam = model_parameters
    n_trials = len(action_1)

    q_stage1 = np.zeros(2) + 0.5
    q_stage2 = np.zeros((2, 2)) + 0.5
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Choice ---
        s_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2[s_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        r = reward[trial]
        
        # Stage 1 Update (using lr_stage1)
        pe_1 = q_stage2[s_idx, a2] - q_stage1[a1]
        q_stage1[a1] += lr_stage1 * pe_1
        
        # Stage 2 Update (using lr_stage2)
        pe_2 = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += lr_stage2 * pe_2
        
        # Trace Update
        # Updates Stage 1 value based on Stage 2 outcome. 
        # Since this modifies Q1, we scale by lr_stage1.
        q_stage1[a1] += lr_stage1 * lam * pe_2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```