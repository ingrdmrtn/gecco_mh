Here are three new cognitive models for the two-step task.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner with Fixed Lambda=1 (Monte Carlo Stage 1).
    
    This model represents a hybrid learner combining Model-Based (MB) and 
    Model-Free (MF) strategies. Unlike the standard hybrid model which learns 
    an eligibility trace parameter (lambda), this model fixes lambda=1. 
    This implies the MF component updates Stage 1 values based purely on the 
    final outcome (reward), effectively ignoring the Stage 2 value estimate 
    (Monte Carlo update), while the MB component uses the transition structure.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for value updates.
    - beta_1: [0, 10] Inverse temperature for Stage 1 choices.
    - beta_2: [0, 10] Inverse temperature for Stage 2 choices.
    - w: [0, 1] Weighting parameter (0 = Pure MF, 1 = Pure MB).
    - stickiness: [0, 5] Tendency to repeat the previous Stage 1 choice.
    """
    learning_rate, beta_1, beta_2, w, stickiness = model_parameters
    n_trials = len(action_1)
    
    # Transition probabilities: 0->0 (0.7), 0->1 (0.3), etc.
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Initialize Q-values to 0.5 (neutral expectation)
    q_stage1_mf = np.zeros(2) + 0.5
    q_stage2 = np.zeros((2, 2)) + 0.5
    
    last_action_1 = -1
    log_loss = 0.0
    eps = 1e-10
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        # Skip missing trials
        if a1 == -1:
            continue
        
        # --- Stage 1 Policy ---
        # Model-Based Value: Expected max stage 2 value given transitions
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Net Value
        q_net_s1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Apply Stickiness
        if last_action_1 != -1:
            q_net_s1[last_action_1] += stickiness
            
        # Softmax for Stage 1
        exp_q1 = np.exp(beta_1 * q_net_s1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + eps)
        
        # --- Stage 2 Policy ---
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        
        # Softmax for Stage 2
        exp_q2 = np.exp(beta_2 * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[a2] + eps)
        
        # --- Updates ---
        r = reward[trial]
        
        # Stage 2 Update (Standard TD)
        q_stage2[s_idx, a2] += learning_rate * (r - q_stage2[s_idx, a2])
        
        # Stage 1 MF Update (Lambda=1 / Monte Carlo)
        # Update based directly on reward, bypassing Stage 2 value
        q_stage1_mf[a1] += learning_rate * (r - q_stage1_mf[a1])
        
        last_action_1 = a1

    return log_loss

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Based Learner with Passive Decay.
    
    This model assumes a Model-Based strategy where the participant calculates
    Stage 1 values using the transition matrix and Stage 2 alien values.
    It introduces a 'decay' mechanism: while the chosen alien's value is 
    updated with the reward, the unchosen alien's value on the current planet 
    decays towards 0.5. This accounts for the participant's belief that 
    reward probabilities are volatile and information becomes stale.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for the chosen alien.
    - decay_rate: [0, 1] Rate at which unchosen alien values decay to 0.5.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - stickiness: [0, 5] Tendency to repeat the previous Stage 1 choice.
    """
    learning_rate, decay_rate, beta_1, beta_2, stickiness = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage2 = np.zeros((2, 2)) + 0.5
    
    last_action_1 = -1
    log_loss = 0.0
    eps = 1e-10
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        if a1 == -1:
            continue
            
        # Stage 1: MB calculation
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = q_stage1_mb.copy()
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + eps)
        
        # Stage 2
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        
        exp_q2 = np.exp(beta_2 * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[a2] + eps)
        
        # Update
        r = reward[trial]
        
        # Update chosen alien
        q_stage2[s_idx, a2] += learning_rate * (r - q_stage2[s_idx, a2])
        
        # Decay unchosen alien in the current state
        unchosen_a2 = 1 - a2
        q_stage2[s_idx, unchosen_a2] += decay_rate * (0.5 - q_stage2[s_idx, unchosen_a2])
        
        last_action_1 = a1

    return log_loss

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Based Learner with Multi-Stage Stickiness.
    
    This model extends the Model-Based approach by including stickiness 
    (perseveration) in both Stage 1 (Spaceship choice) and Stage 2 (Alien choice).
    It tests the hypothesis that the participant exhibits habitual repetition 
    of responses at both levels of the decision tree, independent of value learning.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for Stage 2 values.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - stick_1: [0, 5] Stickiness for Stage 1 (Spaceship).
    - stick_2: [0, 5] Stickiness for Stage 2 (Alien).
    """
    learning_rate, beta_1, beta_2, stick_1, stick_2 = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage2 = np.zeros((2, 2)) + 0.5
    
    last_action_1 = -1
    # Track last action for each state in stage 2: [last_a2_in_state0, last_a2_in_state1]
    last_action_2 = [-1, -1] 
    
    log_loss = 0.0
    eps = 1e-10
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        if a1 == -1:
            continue
            
        # Stage 1
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_s1 = q_stage1_mb.copy()
        if last_action_1 != -1:
            q_net_s1[last_action_1] += stick_1
            
        exp_q1 = np.exp(beta_1 * q_net_s1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + eps)
        
        # Stage 2
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        
        q_net_s2 = q_stage2[s_idx].copy()
        # Apply stickiness if we have made a choice in this state before
        if last_action_2[s_idx] != -1:
            q_net_s2[last_action_2[s_idx]] += stick_2
            
        exp_q2 = np.exp(beta_2 * q_net_s2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[a2] + eps)
        
        # Update
        r = reward[trial]
        q_stage2[s_idx, a2] += learning_rate * (r - q_stage2[s_idx, a2])
        
        last_action_1 = a1
        last_action_2[s_idx] = a2

    return log_loss
```