Here are three new cognitive models based on the participant's behavior.

The participant displays **extreme perseveration** (stickiness), often choosing the same spaceship for 50+ trials regardless of outcomes, but then switching and sticking to the other. This suggests mechanisms deeper than simple "last-choice" repetition.

1.  **Cognitive Model 1: Choice Kernel Hybrid Model.**
    *   **Rationale:** Instead of simple stickiness (did I choose A last time?), this model uses a "Choice Kernel" (how frequently have I chosen A recently?). This allows "habits" to build up strength over time (explaining the long runs) and decay slowly (explaining why they don't switch immediately after one failure).
2.  **Cognitive Model 2: Asymmetric Learning Hybrid Model.**
    *   **Rationale:** The participant often ignores 0-coin outcomes (failures) while staying with a spaceship. This model separates learning rates for positive prediction errors (rewards) and negative prediction errors (omissions). If $\alpha_{neg} \approx 0$, the agent won't unlearn the value of a spaceship despite repeated failures, explaining the persistence.
3.  **Cognitive Model 3: TD($\lambda$) Hybrid Model.**
    *   **Rationale:** This model adds an eligibility trace ($\lambda$). This allows the reward at the very end (Stage 2) to directly update the value of the spaceship chosen in Stage 1, bypassing the Model-Based transition calculation. This represents a more robust "Model-Free" pathway that links the spaceship directly to the gold, which might drive the behavior when the Model-Based system is uncertain.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Choice Kernel Hybrid Model.
    
    Incorporates a 'Choice Kernel' that tracks the frequency of past choices 
    rather than just the most recent choice. This models habit formation 
    where actions become 'stickier' the more they are repeated.

    Parameters:
    - learning_rate: [0, 1] Value update rate.
    - beta_1: [0, 10] Inverse temp for Stage 1.
    - beta_2: [0, 10] Inverse temp for Stage 2.
    - w: [0, 1] Weight for Model-Based control (0=MF, 1=MB).
    - ck_decay: [0, 1] Decay rate of the choice kernel (0=no memory, 1=only last choice).
    - ck_weight: [0, 5] Weight of the choice kernel in decision making.
    """
    learning_rate, beta_1, beta_2, w, ck_decay, ck_weight = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    
    # Choice Kernel (frequency tracker)
    choice_kernel = np.zeros(2)

    for trial in range(n_trials):
        # Handle missing data
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1 or reward[trial] == -1:
            continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combine MB, MF, and Choice Kernel
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf + ck_weight * choice_kernel
        
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # 1. Update Choice Kernel
        # The chosen action kernel increases, unchosen decays (or both decay towards new state)
        # Standard implementation: CK_{t+1}(a) = (1-decay)*CK_t(a) + decay*Ind(a_t==a)
        choice_kernel = (1 - ck_decay) * choice_kernel
        choice_kernel[action_1[trial]] += ck_decay * 1.0

        # 2. Value Updates
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    # Filter out trials where probabilities were not calculated (missing data)
    mask = (p_choice_1 > 0) & (p_choice_2 > 0)
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Asymmetric Learning Hybrid Model.
    
    This model assumes the participant learns differently from positive outcomes (rewards)
    versus negative outcomes (omission of reward). A low `lr_neg` explains why the 
    participant might persist with a choice despite receiving 0 coins.

    Parameters:
    - lr_pos: [0, 1] Learning rate when prediction error is positive.
    - lr_neg: [0, 1] Learning rate when prediction error is negative.
    - beta_1: [0, 10] Inverse temp for Stage 1.
    - beta_2: [0, 10] Inverse temp for Stage 2.
    - w: [0, 1] Weight for Model-Based control.
    - stickiness: [0, 5] Simple repetition bonus for Stage 1.
    """
    lr_pos, lr_neg, beta_1, beta_2, w, stickiness = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1 or reward[trial] == -1:
            continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        eff_lr_1 = lr_pos if delta_stage1 > 0 else lr_neg
        q_stage1_mf[action_1[trial]] += eff_lr_1 * delta_stage1
        
        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        eff_lr_2 = lr_pos if delta_stage2 > 0 else lr_neg
        q_stage2_mf[state_idx, action_2[trial]] += eff_lr_2 * delta_stage2
        
        last_action_1 = action_1[trial]

    eps = 1e-10
    mask = (p_choice_1 > 0) & (p_choice_2 > 0)
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    TD(Lambda) Hybrid Model.
    
    Incorporates an eligibility trace (lambda). This allows the Stage 1 choice 
    to be updated directly by the final reward (Stage 2 RPE), bridging the gap 
    between the spaceship choice and the coin receipt.

    Parameters:
    - learning_rate: [0, 1] Base learning rate.
    - beta_1: [0, 10] Inverse temp for Stage 1.
    - beta_2: [0, 10] Inverse temp for Stage 2.
    - w: [0, 1] Weight for Model-Based control.
    - lambda_param: [0, 1] Eligibility trace decay (0 = TD(0), 1 = Monte Carlo).
    - stickiness: [0, 5] Simple repetition bonus.
    """
    learning_rate, beta_1, beta_2, w, lambda_param, stickiness = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1 or reward[trial] == -1:
            continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness

        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # 1. Prediction error at Stage 1 (Transition)
        # delta1 = V(s') - Q(s,a)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # 2. Prediction error at Stage 2 (Reward)
        # delta2 = r - Q(s',a')
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # 3. Eligibility Trace Update for Stage 1
        # The Stage 1 action is also updated by the Stage 2 prediction error, scaled by lambda
        q_stage1_mf[action_1[trial]] += learning_rate * lambda_param * delta_stage2
        
        last_action_1 = action_1[trial]

    eps = 1e-10
    mask = (p_choice_1 > 0) & (p_choice_2 > 0)
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss
```