Here are three new cognitive models based on the participant data and the two-step task structure. These models explore different mechanisms of value updating (asymmetry, stage-specificity, and memory decay) while maintaining the hybrid Model-Based/Model-Free architecture.

### Model 1: Asymmetric Learning Rates (Positive/Negative) + Hybrid MB/MF
This model posits that the participant learns differently from positive prediction errors (getting a coin/better than expected) versus negative prediction errors (no coin/worse than expected). It combines this with the standard weighting (`w`) between Model-Based and Model-Free systems.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF model with Asymmetric Learning Rates.
    
    Distinguishes between learning from positive outcomes (alpha_pos) and 
    negative outcomes (alpha_neg). This captures potential optimism or 
    pessimism biases in how the participant updates value estimates.
    
    Parameters:
    alpha_pos: [0, 1] Learning rate for positive prediction errors (delta > 0).
    alpha_neg: [0, 1] Learning rate for negative prediction errors (delta <= 0).
    beta_1: [0, 10] Inverse temperature for Stage 1 choice.
    beta_2: [0, 10] Inverse temperature for Stage 2 choice.
    w: [0, 1] Weighting parameter. 0 = Pure Model-Free, 1 = Pure Model-Based.
    """
    alpha_pos, alpha_neg, beta_1, beta_2, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # 2 states (planets), 2 actions (aliens)

    for trial in range(n_trials):

        # --- Policy for Stage 1 ---
        # Model-Based value: expected value of best option in next stage * transition probs
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Weighted integrated value
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_1 * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Policy for Stage 2 ---
        # Standard Model-Free Softmax on the aliens of the current planet
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updating ---
        # TD(0) Prediction Error for Stage 1
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        # Determine Learning Rate for Stage 1 based on sign of error
        lr_1 = alpha_pos if delta_stage1 > 0 else alpha_neg
        q_stage1_mf[action_1[trial]] += lr_1 * delta_stage1
        
        # Prediction Error for Stage 2 (Reward - Expectation)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Determine Learning Rate for Stage 2 based on sign of error
        lr_2 = alpha_pos if delta_stage2 > 0 else alpha_neg
        q_stage2_mf[state_idx, action_2[trial]] += lr_2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Separate Stage Learning Rates + Hybrid MB/MF
This model assumes the participant has different plasticity for the two stages of the task. Stage 1 (Spaceship choice) is more abstract and strategic, while Stage 2 (Alien choice) is a direct bandit task. The participant may update their preferences for spaceships (`lr_stage1`) at a different speed than their preferences for aliens (`lr_stage2`).

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF model with Separate Stage Learning Rates.
    
    Allows for different learning speeds for the first-stage (spaceship)
    and second-stage (alien) choices.
    
    Parameters:
    lr_stage1: [0, 1] Learning rate for Stage 1 Model-Free values.
    lr_stage2: [0, 1] Learning rate for Stage 2 Q-values.
    beta_1: [0, 10] Inverse temperature for Stage 1 choice.
    beta_2: [0, 10] Inverse temperature for Stage 2 choice.
    w: [0, 1] Weighting parameter. 0 = Pure Model-Free, 1 = Pure Model-Based.
    """
    lr_stage1, lr_stage2, beta_1, beta_2, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Policy for Stage 1 ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_1 * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Policy for Stage 2 ---
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updating ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr_stage1 * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr_stage2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Value Decay (Forgetting) + Hybrid MB/MF
This model introduces a `decay` parameter. In standard Q-learning, unchosen options retain their values indefinitely. Here, the value of unchosen spaceships and aliens decays towards zero over time. This accounts for memory limitations or the participant's assumption that the environment is volatile and old information becomes less relevant.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF model with Value Decay (Forgetting).
    
    In addition to learning from experience, the values of unchosen options 
    decay over time. This models memory loss or a 'return to baseline' 
    for options not recently explored.
    
    Parameters:
    lr: [0, 1] Learning rate for chosen options.
    decay: [0, 1] Decay rate for unchosen options (0 = no decay, 1 = instant forgetting).
    beta_1: [0, 10] Inverse temperature for Stage 1 choice.
    beta_2: [0, 10] Inverse temperature for Stage 2 choice.
    w: [0, 1] Weighting parameter. 0 = Pure Model-Free, 1 = Pure Model-Based.
    """
    lr, decay, beta_1, beta_2, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Policy for Stage 1 ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_1 * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Policy for Stage 2 ---
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updating ---
        # 1. Update chosen Stage 1 value
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr * delta_stage1
        
        # 2. Decay unchosen Stage 1 value
        unchosen_s1 = 1 - action_1[trial]
        q_stage1_mf[unchosen_s1] *= (1.0 - decay)
        
        # 3. Update chosen Stage 2 value
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr * delta_stage2
        
        # 4. Decay unchosen Stage 2 value (on the current planet)
        unchosen_s2 = 1 - action_2[trial]
        q_stage2_mf[state_idx, unchosen_s2] *= (1.0 - decay)
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```