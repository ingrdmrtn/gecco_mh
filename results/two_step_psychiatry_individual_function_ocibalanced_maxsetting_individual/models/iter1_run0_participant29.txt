Here are 3 new cognitive models based on the participant data and the two-step task structure.

### Model 1: Hybrid Model with Separate Stage Learning Rates
This model hypothesizes that the participant learns the value of the spaceships (Stage 1) and the aliens (Stage 2) at different speeds. For example, they might be quick to learn which alien gives gold (`alpha_stage2`) but slow to update their preference for the spaceship (`alpha_stage1`). It combines Model-Based (planning) and Model-Free (habit) influences using a weighting parameter `w`.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model (MB/MF) with distinct learning rates for Stage 1 and Stage 2.
    
    Parameters:
    - alpha_s1: [0, 1] Learning rate for Stage 1 (Spaceship choice).
    - alpha_s2: [0, 1] Learning rate for Stage 2 (Alien choice).
    - beta: [0, 10] Inverse temperature (softmax sensitivity).
    - w: [0, 1] Mixing weight. 0 = Pure Model-Free, 1 = Pure Model-Based.
    """
    alpha_s1, alpha_s2, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2) # Model-Free Stage 1 values
    q_stage2_mf = np.zeros((2, 2)) # Stage 2 values (used by both MF and MB)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based valuation: Transition * Max(Stage 2 values)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated value (Hybrid)
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        q_s2_current = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * q_s2_current)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # Prediction Error Stage 1 (TD-0)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += alpha_s1 * delta_stage1
        
        # Prediction Error Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += alpha_s2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Pure Model-Based with Perseveration
This model tests the hypothesis that the participant is purely goal-directed (Model-Based) regarding the value calculation but suffers from motor perseveration (stickiness). It removes the Model-Free Stage 1 learning component entirely (`w` is effectively 1) and replaces it with a perseveration parameter `p`. This checks if the behavior is explained by knowing the map + repeating keys, rather than slowly learning spaceship values.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Based Reinforcement Learning with Perseveration.
    Assumes no Model-Free learning at Stage 1, only planning + habit.
    
    Parameters:
    - learning_rate: [0, 1] Rate of updating Stage 2 (Alien) values.
    - beta: [0, 10] Inverse temperature.
    - p: [0, 5] Perseveration bonus (stickiness to previous Stage 1 choice).
    """
    learning_rate, beta, p = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage2_mf = np.zeros((2, 2)) # Stage 2 values
    prev_choice_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based: Transition * Max(Stage 2 values)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = q_stage1_mb.copy()
        
        # Add perseveration bonus to the previously chosen spaceship
        if prev_choice_1 != -1:
            q_net[prev_choice_1] += p
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        q_s2_current = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * q_s2_current)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # Only update Stage 2 values (Stage 1 is derived purely from model + p)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        prev_choice_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model with Eligibility Traces (MB + TD($\lambda$))
This model combines the Hybrid architecture (Model-Based planning) with Eligibility Traces (TD-Lambda) for the Model-Free component. While standard Hybrid models often use TD(0) or TD(1) for the Model-Free part, this allows the "credit assignment" strategy to vary via `lam`. It captures how much the reward at the very end (Stage 2) directly updates the value of the spaceship chosen at the start (Stage 1), mixed with the planned value from the map.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model (MB/MF) with Eligibility Traces (TD-lambda).
    The Model-Free component uses an eligibility trace to update Stage 1.
    
    Parameters:
    - learning_rate: [0, 1] Rate of updating Q-values.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight. 0 = Pure MF, 1 = Pure MB.
    - lam: [0, 1] Eligibility trace decay parameter (Lambda).
    """
    learning_rate, beta, w, lam = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Mix MB and MF values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        q_s2_current = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * q_s2_current)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # 1. Prediction error at Stage 1 (Transition)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        # 2. Prediction error at Stage 2 (Reward)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Update Stage 2 value
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Update Stage 1 MF value using Eligibility Trace (combining delta1 and delta2)
        # If lam=0, this is TD(0). If lam=1, this is effectively Monte Carlo.
        q_stage1_mf[action_1[trial]] += learning_rate * (delta_stage1 + lam * delta_stage2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```