Here are three new cognitive models for the two-step task, designed to explore different mechanisms of learning and decision-making while adhering to the parameter constraints.

### Cognitive Model 1: Hybrid Model with Separate Learning Rates
This model hypothesizes that the participant updates their valuation of the spaceship (Stage 1) and the aliens (Stage 2) at different rates. This can explain the behavior where the spaceship choice remains stable (low learning rate) while the alien choice adapts quickly to drifting reward probabilities (high learning rate).

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Separate Learning Rates for Stage 1 and Stage 2.
    
    Distinguishes between the learning rate for the spaceship preference (Stage 1)
    and the alien preference (Stage 2). This allows the model to capture different 
    timescales of adaptation for the two stages (e.g., stable spaceship choice 
    but dynamic alien tracking).
    
    Parameters:
    lr_1: [0, 1] - Learning rate for Stage 1 (Spaceship choice).
    lr_2: [0, 1] - Learning rate for Stage 2 (Alien choice).
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - MB/MF mixing weight.
    """
    lr_1, lr_2, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # Skip trials with missing data
        if action_1[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # --- Stage 1 Policy ---
        # Model-Based Value: Expected max value of next stage
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Value: Mixture of MB and MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax choice
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # Stage 1 Update (uses lr_1)
        # Driven by the value of the state actually reached (TD(0))
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr_1 * delta_stage1
        
        # Stage 2 Update (uses lr_2)
        # Driven by the reward received
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr_2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2: Hybrid Model with Eligibility Trace (Lambda)
This model introduces an eligibility trace parameter $\lambda$. Standard Model-Free learning can update Stage 1 values based on the value of the next state (TD(0)) or the final reward (TD(1)). This model allows for a continuous interpolation between these strategies, capturing how strongly the final outcome reinforces the initial choice directly.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Eligibility Trace (Lambda).
    
    Introduces an eligibility trace parameter (lambda) that controls how much the 
    Stage 1 Model-Free values are updated by the Stage 2 outcome (Reward) versus 
    the Stage 2 value estimate.
    lambda = 1: TD(1) update (Stage 1 driven by Reward).
    lambda = 0: TD(0) update (Stage 1 driven by Stage 2 Value).
    
    Parameters:
    learning_rate: [0, 1] - Learning rate.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - MB/MF mixing weight.
    lambda_param: [0, 1] - Eligibility trace weight.
    """
    learning_rate, beta, w, lambda_param = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        if action_1[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # Calculate prediction errors
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Stage 1 Update: Includes lambda-weighted eligibility trace of stage 2 error
        # If lambda=1, this effectively updates Q1 based on Reward.
        q_stage1_mf[action_1[trial]] += learning_rate * (delta_stage1 + lambda_param * delta_stage2)
        
        # Stage 2 Update
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3: Hybrid Model with Separate Inverse Temperatures
This model acknowledges that the decision noise (exploration vs. exploitation) might differ between the two stages. For instance, the participant might be very consistent (high beta) in choosing a spaceship but more exploratory (low beta) when choosing an alien, or vice versa.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Separate Inverse Temperatures (Betas).
    
    Uses different inverse temperature (beta) parameters for Stage 1 and Stage 2.
    This accounts for potentially different levels of exploration/exploitation noise 
    in the spaceship choice versus the alien choice.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate.
    beta_1: [0, 10] - Inverse temperature for Stage 1.
    beta_2: [0, 10] - Inverse temperature for Stage 2.
    w: [0, 1] - MB/MF mixing weight.
    """
    learning_rate, beta_1, beta_2, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        if action_1[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Use beta_1 for Stage 1
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        # Use beta_2 for Stage 2
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```