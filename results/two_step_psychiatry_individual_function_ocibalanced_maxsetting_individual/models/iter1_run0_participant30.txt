Here are three new cognitive models based on the participant data and previous feedback.

### Model 1: Asymmetric Pure Model-Free Learning (TD-Lambda)
**Rationale:** The previous best model was a Pure Model-Free algorithm (TD-Lambda). A common cognitive bias is responding differently to positive outcomes (rewards) versus negative outcomes (omissions). This model splits the learning rate into `lr_pos` and `lr_neg` within the Model-Free framework, while retaining the eligibility trace (`lambda`) to connect Stage 2 outcomes to Stage 1 choices. It ignores the transition matrix (`w=0`).

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Asymmetric Pure Model-Free Q-Learning (TD-Lambda).
    Differentiates learning from positive vs negative prediction errors.
    
    Parameters:
    lr_pos: learning rate for positive prediction errors [0,1]
    lr_neg: learning rate for negative prediction errors [0,1]
    beta: inverse temperature [0,10]
    lambda_eligibility: eligibility trace decay [0,1]
    perseveration: stickiness to previous choice [0,5]
    """
    lr_pos, lr_neg, beta, lambda_eligibility, perseveration = model_parameters
    n_trials = len(action_1)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        # Handle missing data
        if action_1[trial] == -1:
            last_action_1 = -1
            p_choice_1[trial] = 1.0 # Avoid log(0)
            p_choice_2[trial] = 1.0
            continue

        # --- Stage 1 Choice ---
        q_net_1 = q_stage1_mf.copy()
        if last_action_1 != -1:
            q_net_1[last_action_1] += perseveration
            
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning ---
        # Stage 1 PE
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        # Stage 2 PE
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]

        # Determine LRs based on sign of error
        lr_1 = lr_pos if delta_stage1 > 0 else lr_neg
        lr_2 = lr_pos if delta_stage2 > 0 else lr_neg
        
        # Update Stage 1 (TD-Lambda)
        # We apply the specific LR to the specific delta component
        q_stage1_mf[action_1[trial]] += (lr_1 * delta_stage1) + (lr_2 * lambda_eligibility * delta_stage2)

        # Update Stage 2
        q_stage2_mf[state_idx, action_2[trial]] += lr_2 * delta_stage2
        
        last_action_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Pure Model-Free Learning with Passive Decay
**Rationale:** The task description states that reward probabilities "changed slowly over trials." Standard Q-learning assumes values remain constant until revisited. This model adds a `decay` parameter to the Pure MF framework. On every trial, all Q-values slightly decay toward a neutral value (0.5). This allows the model to "forget" old information that may no longer be relevant due to the drifting probabilities, potentially fitting the participant's adaptation better than fixed Q-values.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free Q-Learning with Passive Decay.
    Q-values decay toward 0.5 (neutral) on every trial to handle drifting probabilities.
    
    Parameters:
    learning_rate: rate of value updating [0,1]
    beta: inverse temperature [0,10]
    lambda_eligibility: eligibility trace decay [0,1]
    decay: rate at which values decay to 0.5 [0,1]
    perseveration: stickiness to previous choice [0,5]
    """
    learning_rate, beta, lambda_eligibility, decay, perseveration = model_parameters
    n_trials = len(action_1)
    
    # Initialize at 0.5 (neutral expectation for 0/1 rewards)
    q_stage1_mf = np.full(2, 0.5)
    q_stage2_mf = np.full((2, 2), 0.5)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    last_action_1 = -1
    
    for trial in range(n_trials):
        # Handle missing data
        if action_1[trial] == -1:
            last_action_1 = -1
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # --- Decay Process ---
        # All Q-values drift toward 0.5 before choice/update
        q_stage1_mf = q_stage1_mf * (1 - decay) + 0.5 * decay
        q_stage2_mf = q_stage2_mf * (1 - decay) + 0.5 * decay

        # --- Stage 1 Choice ---
        q_net_1 = q_stage1_mf.copy()
        if last_action_1 != -1:
            q_net_1[last_action_1] += perseveration
            
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]

        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1 + learning_rate * lambda_eligibility * delta_stage2
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        last_action_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid with Stage-Specific Learning Rates
**Rationale:** The previous feedback indicated that a hybrid model with a single learning rate was not the best fit. However, the decision process in Stage 1 (strategic planning) is cognitively distinct from Stage 2 (simple bandit). This model proposes a Hybrid architecture (mixing MB and MF) but assigns separate learning rates for Stage 1 (`lr_1`) and Stage 2 (`lr_2`). This allows the participant to update second-stage valuations quickly (or slowly) while updating first-stage preferences at a different speed.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Stage-Specific Learning Rates.
    Separates learning speeds for the first-step choice vs second-step bandit.
    
    Parameters:
    lr_1: learning rate for stage 1 MF values [0,1]
    lr_2: learning rate for stage 2 MF values [0,1]
    beta: inverse temperature [0,10]
    w: weight of model-based system (0=Pure MF, 1=Pure MB) [0,1]
    perseveration: stickiness to previous choice [0,5]
    """
    lr_1, lr_2, beta, w, perseveration = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    last_action_1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1:
            last_action_1 = -1
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # --- Stage 1 Choice (Hybrid) ---
        # Model-Based Value: Expected max value of next stage
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Value: Weighted sum of MB and MF
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_action_1 != -1:
            q_net_1[last_action_1] += perseveration
            
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Choice (Model-Free only) ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning ---
        # Note: No lambda here, relying on MB for structure, but separate LRs
        
        # Update Stage 1 MF using TD(0) logic (SARSA-like for first stage)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr_1 * delta_stage1
        
        # Update Stage 2 MF
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr_2 * delta_stage2
        
        last_action_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```