Here are three new cognitive models based on the participant data and the two-step task structure.

### Cognitive Model 1: Hybrid MB/MF with Eligibility Trace
This model extends the standard hybrid learner by adding an **eligibility trace parameter (`lam`)**. The participant data shows rapid locking onto a successful strategy (Spaceship 1 -> Planet 1 -> Alien 0) and switching when it fails. An eligibility trace allows the outcome at the second stage (reward) to directly reinforce the first stage choice (spaceship), bridging the gap between action and reward in the Model-Free system. This helps explain how the participant might develop a strong preference for a spaceship based on the rewards received from the aliens, distinct from the model-based calculation.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Model with Eligibility Trace.
    
    Combines Model-Based (MB) planning and Model-Free (MF) learning.
    Includes an eligibility trace (lambda) that allows the Stage 2 reward prediction error 
    to update the Stage 1 Model-Free values directly.
    
    Parameters:
    - learning_rate: [0, 1] Rate at which MF values are updated.
    - beta: [0, 10] Inverse temperature for softmax choice policy.
    - w: [0, 1] Weighting parameter. 1 = Pure MB, 0 = Pure MF.
    - lam: [0, 1] Eligibility trace decay. 0 = TD(0), 1 = Monte Carlo.
    """
    learning_rate, beta, w, lam = model_parameters
    n_trials = len(action_1)
    
    # Transition probabilities: Row 0 -> [Planet 0, Planet 1], Row 1 -> [Planet 0, Planet 1]
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Initialize Q-values
    q_stage1_mf = np.zeros(2)      # MF values for Stage 1 (Spaceships)
    q_stage2_mf = np.zeros((2, 2)) # MF values for Stage 2 (Aliens per Planet)
    
    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # Model-Based: Expected value based on transitions and max stage 2 values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Value: Weighted sum of MB and MF
        q_integrated = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax for Action 1
        logits_1 = beta * q_integrated
        logits_1 = logits_1 - np.max(logits_1) # Prevent overflow
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Decision ---
        state_idx = state[trial]
        
        # Softmax for Action 2 (based on Stage 2 MF values)
        logits_2 = beta * q_stage2_mf[state_idx]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        # 1. Stage 1 PE (SARSA-style): Value of chosen Stage 2 state/action - Value of Stage 1 choice
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # 2. Stage 2 PE: Reward - Value of chosen Stage 2 state/action
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # 3. Eligibility Trace Update
        # The Stage 2 PE also updates the Stage 1 choice, scaled by lambda
        q_stage1_mf[action_1[trial]] += learning_rate * lam * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2: Hybrid MB/MF with Perseveration
This model tests the hypothesis that the participant uses a mix of Model-Based and Model-Free strategies (`w`) but is also subject to **perseveration** (stickiness). The participant data shows long streaks of repeating the same spaceship choice (e.g., trials 13-20). By adding a perseveration bonus to the *Hybrid* value (rather than just Pure MB), this model captures the inertia in decision-making while still allowing for reinforcement learning updates from both MB and MF systems.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Model with Perseveration.
    
    Combines weighted MB and MF values for decision making.
    Adds a 'perseveration' parameter that increases the probability of repeating 
    the previous Stage 1 choice, independent of value.
    
    Parameters:
    - learning_rate: [0, 1] Update rate for MF values.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weight for Model-Based values (1=MB, 0=MF).
    - perseveration: [0, 5] Bonus added to the logit of the previously chosen spaceship.
    """
    learning_rate, beta, w, perseveration = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1 # Initialize as invalid
    
    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_integrated = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_integrated
        
        # Add perseveration bonus
        if last_action_1 != -1:
            logits_1[last_action_1] += perseveration
            
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_action_1 = action_1[trial]
        
        # --- Stage 2 Decision ---
        state_idx = state[trial]
        logits_2 = beta * q_stage2_mf[state_idx]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3: Pure MB with Asymmetric Learning and Perseveration
Building on the insight that the participant may be largely Model-Based (relying on structure) with stickiness, this model refines the learning mechanism. It assumes **asymmetric learning rates** (`lr_pos`, `lr_neg`) for the second stage values. The data shows the participant holds onto a winning strategy for a long time but abandons it after a few failures. Asymmetric learning allows the model to capture different sensitivities to rewards (gold coins) versus omissions (no gold), combined with the cognitive inertia of perseveration.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Based with Asymmetric Learning and Perseveration.
    
    Assumes Stage 1 choices are purely Model-Based (derived from transition matrix).
    Stage 2 values are updated with separate learning rates for positive and negative
    prediction errors. Includes perseveration on Stage 1 choice.
    
    Parameters:
    - lr_pos: [0, 1] Learning rate for positive prediction errors (Reward > Expectation).
    - lr_neg: [0, 1] Learning rate for negative prediction errors (Reward < Expectation).
    - beta: [0, 10] Inverse temperature.
    - perseveration: [0, 5] Stickiness bonus for Stage 1.
    """
    lr_pos, lr_neg, beta, perseveration = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Only Stage 2 values are learned (Aliens)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # Pure MB: Calculate value directly from transition matrix and Stage 2 values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        logits_1 = beta * q_stage1_mb
        
        # Add perseveration
        if last_action_1 != -1:
            logits_1[last_action_1] += perseveration
            
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_action_1 = action_1[trial]
        
        # --- Stage 2 Decision ---
        state_idx = state[trial]
        logits_2 = beta * q_stage2_mf[state_idx]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        # Only update Stage 2 values. No Stage 1 MF values to update.
        pe = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        if pe >= 0:
            q_stage2_mf[state_idx, action_2[trial]] += lr_pos * pe
        else:
            q_stage2_mf[state_idx, action_2[trial]] += lr_neg * pe
            
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```