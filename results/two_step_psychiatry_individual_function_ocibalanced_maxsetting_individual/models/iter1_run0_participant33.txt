Here are three cognitive models based on the participant data and the two-step task structure.

### Model 1: Hybrid Model with Separate Stage Learning Rates
This model hypothesizes that the participant learns at different speeds for the two distinct stages of the task. Stage 1 involves learning the value of a spaceship (which is an abstract transition to a state), while Stage 2 involves learning the value of an alien (a direct reward probability). The participant may update their beliefs about the "harvesting" stage (aliens) differently than the "navigation" stage (spaceships).

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Separate Learning Rates for Stage 1 and Stage 2.
    
    The agent has distinct plasticity (learning rates) for the navigation choice (Stage 1)
    and the harvesting choice (Stage 2).
    
    Parameters:
    - lr_stage1: [0,1] Learning rate for Stage 1 (Spaceship) Q-values.
    - lr_stage2: [0,1] Learning rate for Stage 2 (Alien) Q-values.
    - beta: [0,10] Inverse temperature (softness of choice).
    - w: [0,1] Weight of Model-Based system (1=Pure MB, 0=Pure MF).
    """
    lr_stage1, lr_stage2, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # Handle missing/invalid trials
        if action_1[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # --- Stage 1 Policy ---
        # Model-Based Value: Expected value of next state (max of stage 2) weighted by transitions
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combined Value
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax Choice 1
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]

        # --- Stage 2 Policy ---
        # Standard Softmax on Stage 2 Q-values
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        
        # Stage 1 Update (TD-0) using lr_stage1
        # Prediction Error: Value of chosen Stage 2 option minus expected Stage 1 value
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr_stage1 * delta_stage1

        # Stage 2 Update (Reward Prediction Error) using lr_stage2
        # Prediction Error: Actual reward minus expected Stage 2 value
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += lr_stage2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model with Asymmetric Learning and Choice Stickiness
This model combines two successful mechanisms: asymmetric learning rates (learning more from positive vs. negative outcomes) and choice stickiness (perseveration). The participant data shows long streaks of choosing the same spaceship. This model posits that while the agent learns asymmetrically, they also have a primitive tendency to repeat their previous Stage 1 action regardless of the calculated value.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Asymmetric Learning and Choice Stickiness.
    
    Combines differential learning from positive/negative prediction errors with 
    a tendency to repeat the previous Stage 1 choice (stickiness).
    
    Parameters:
    - lr_pos: [0,1] Learning rate for positive prediction errors.
    - lr_neg: [0,1] Learning rate for negative prediction errors.
    - beta: [0,10] Inverse temperature.
    - w: [0,1] Weight of Model-Based system.
    - stickiness: [0,5] Bonus added to the Q-value of the previously chosen Stage 1 action.
    """
    lr_pos, lr_neg, beta, w, stickiness = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track previous action for stickiness (initialize to -1 or None)
    prev_a1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            prev_a1 = -1 # Reset stickiness on invalid trial
            continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add Stickiness Bonus
        if prev_a1 != -1:
            q_net_stage1[prev_a1] += stickiness
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates (Asymmetric) ---
        
        # Stage 1
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        current_lr_1 = lr_pos if delta_stage1 > 0 else lr_neg
        q_stage1_mf[a1] += current_lr_1 * delta_stage1

        # Stage 2
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        current_lr_2 = lr_pos if delta_stage2 > 0 else lr_neg
        q_stage2_mf[state_idx, a2] += current_lr_2 * delta_stage2
        
        # Update previous action for next trial
        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model with Stage-Specific Inverse Temperatures
This model proposes that the participant exhibits different levels of exploration/exploitation (randomness) in the two stages. For example, they might be very decisive about which spaceship to take (Stage 1, high beta) but more exploratory regarding the aliens (Stage 2, low beta), or vice versa. This decouples the decision noise of the planning stage from the harvesting stage.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Stage-Specific Inverse Temperatures (Betas).
    
    Allows for different levels of exploration/exploitation in the spaceship choice (Stage 1)
    versus the alien choice (Stage 2).
    
    Parameters:
    - learning_rate: [0,1] Standard learning rate.
    - beta_1: [0,10] Inverse temperature for Stage 1 (Spaceship choice).
    - beta_2: [0,10] Inverse temperature for Stage 2 (Alien choice).
    - w: [0,1] Weight of Model-Based system.
    """
    learning_rate, beta_1, beta_2, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        if action_1[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # --- Stage 1 Policy (using beta_1) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_1 * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]

        # --- Stage 2 Policy (using beta_2) ---
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        
        # Stage 1
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

        # Stage 2
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```