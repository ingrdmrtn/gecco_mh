Here are the three proposed cognitive models.

### Model 1: Asymmetric Learning Hybrid Model
This model extends the hybrid learner by distinguishing between positive and negative prediction errors. This allows the agent to learn differently from "better than expected" outcomes (rewards) versus "worse than expected" outcomes (omissions), capturing potential optimism or pessimism biases. It retains the choice stickiness mechanism.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Asymmetric Learning Rates and Stickiness.
    
    Distinguishes between learning from positive prediction errors (better than expected)
    and negative prediction errors (worse than expected). This applies to both the
    Stage 1 (MF) and Stage 2 updates.
    
    Parameters:
    - lr_pos: [0, 1] Learning rate for positive prediction errors (delta > 0).
    - lr_neg: [0, 1] Learning rate for negative prediction errors (delta <= 0).
    - beta_1: [0, 10] Inverse temperature for Stage 1 softmax.
    - beta_2: [0, 10] Inverse temperature for Stage 2 softmax.
    - w: [0, 1] Weighting parameter (1 = Pure MB, 0 = Pure MF).
    - stickiness: [0, 5] Bonus added to the previously chosen Stage 1 action.
    """
    lr_pos, lr_neg, beta_1, beta_2, w, stickiness = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_mf1 = np.zeros(2)        # Stage 1 MF values
    q_s2 = np.zeros((2, 2))    # Stage 2 values (State x Action)
    
    last_action_1 = -1
    
    for t in range(n_trials):
        # --- Stage 1 Decision ---
        # Model-Based Value: Expected value of next state (using max of stage 2)
        max_q_s2 = np.max(q_s2, axis=1)
        q_mb1 = transition_matrix @ max_q_s2
        
        # Net Value mixing MB and MF
        q_net = w * q_mb1 + (1 - w) * q_mf1
        
        # Add stickiness to previously chosen spaceship
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        # Softmax Stage 1 (with numerical stability)
        q_net_stable = q_net - np.max(q_net)
        exp_q1 = np.exp(beta_1 * q_net_stable)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[t] = probs_1[action_1[t]]
        
        a1 = action_1[t]
        s_next = state[t]
        last_action_1 = a1
        
        # --- Stage 2 Decision ---
        q_s2_curr = q_s2[s_next]
        q_s2_stable = q_s2_curr - np.max(q_s2_curr)
        exp_q2 = np.exp(beta_2 * q_s2_stable)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[t] = probs_2[action_2[t]]
        
        a2 = action_2[t]
        r = reward[t]
        
        # --- Updates ---
        # Stage 1 MF Update (SARSA-style: using value of chosen action in stage 2)
        pe1 = q_s2[s_next, a2] - q_mf1[a1]
        alpha1 = lr_pos if pe1 > 0 else lr_neg
        q_mf1[a1] += alpha1 * pe1
        
        # Stage 2 Update
        pe2 = r - q_s2[s_next, a2]
        alpha2 = lr_pos if pe2 > 0 else lr_neg
        q_s2[s_next, a2] += alpha2 * pe2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model with Unchosen Option Decay
Since the reward probabilities in the task drift over time, this model implements a decay mechanism for unchosen options. While chosen options are updated via prediction error, unchosen options "forget" their value and decay towards zero. This helps the agent adapt to the non-stationary environment.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Unchosen Option Decay and Stickiness.
    
    Updates chosen actions via standard reinforcement learning, but applies
    a decay rate to unchosen actions. This allows the agent to 'forget' 
    values of options not recently sampled, appropriate for drifting probabilities.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for chosen actions.
    - decay_rate: [0, 1] Decay rate for unchosen actions (0 = no decay).
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - w: [0, 1] Weight for Model-Based control.
    - stickiness: [0, 5] Choice stickiness for Stage 1.
    """
    learning_rate, decay_rate, beta_1, beta_2, w, stickiness = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_mf1 = np.zeros(2)
    q_s2 = np.zeros((2, 2))
    
    last_action_1 = -1
    
    for t in range(n_trials):
        # --- Stage 1 ---
        max_q_s2 = np.max(q_s2, axis=1)
        q_mb1 = transition_matrix @ max_q_s2
        
        q_net = w * q_mb1 + (1 - w) * q_mf1
        
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        q_net_stable = q_net - np.max(q_net)
        exp_q1 = np.exp(beta_1 * q_net_stable)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[t] = probs_1[action_1[t]]
        
        a1 = action_1[t]
        s_next = state[t]
        last_action_1 = a1
        
        # --- Stage 2 ---
        q_s2_curr = q_s2[s_next]
        q_s2_stable = q_s2_curr - np.max(q_s2_curr)
        exp_q2 = np.exp(beta_2 * q_s2_stable)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[t] = probs_2[action_2[t]]
        
        a2 = action_2[t]
        r = reward[t]
        
        # --- Updates ---
        # Stage 1 MF Update
        pe1 = q_s2[s_next, a2] - q_mf1[a1]
        q_mf1[a1] += learning_rate * pe1
        # Decay unchosen Stage 1
        q_mf1[1-a1] *= (1 - decay_rate)
        
        # Stage 2 Update
        pe2 = r - q_s2[s_next, a2]
        q_s2[s_next, a2] += learning_rate * pe2
        # Decay unchosen Stage 2 (on the current planet)
        q_s2[s_next, 1-a2] *= (1 - decay_rate)
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Dual Learning Rate Hybrid Model
This model separates the learning rates for Stage 1 (Spaceship choice) and Stage 2 (Alien choice). This reflects the hypothesis that the participant may learn the immediate bandit task (Stage 2) at a different speed than the cached Model-Free values for the first stage.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Separate Stage Learning Rates and Stickiness.
    
    Uses distinct learning rates for the Model-Free Stage 1 values and the 
    Stage 2 values (which feed into the Model-Based system). This allows 
    the model to capture different rates of adaptation for the transition 
    policy vs the reward estimation.
    
    Parameters:
    - lr_stage1: [0, 1] Learning rate for Stage 1 MF values.
    - lr_stage2: [0, 1] Learning rate for Stage 2 values (used in MB).
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - w: [0, 1] Weight for Model-Based control.
    - stickiness: [0, 5] Choice stickiness for Stage 1.
    """
    lr_stage1, lr_stage2, beta_1, beta_2, w, stickiness = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_mf1 = np.zeros(2)
    q_s2 = np.zeros((2, 2))
    
    last_action_1 = -1
    
    for t in range(n_trials):
        # --- Stage 1 ---
        max_q_s2 = np.max(q_s2, axis=1)
        q_mb1 = transition_matrix @ max_q_s2
        
        q_net = w * q_mb1 + (1 - w) * q_mf1
        
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        q_net_stable = q_net - np.max(q_net)
        exp_q1 = np.exp(beta_1 * q_net_stable)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[t] = probs_1[action_1[t]]
        
        a1 = action_1[t]
        s_next = state[t]
        last_action_1 = a1
        
        # --- Stage 2 ---
        q_s2_curr = q_s2[s_next]
        q_s2_stable = q_s2_curr - np.max(q_s2_curr)
        exp_q2 = np.exp(beta_2 * q_s2_stable)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[t] = probs_2[action_2[t]]
        
        a2 = action_2[t]
        r = reward[t]
        
        # --- Updates ---
        # Stage 1 MF Update uses lr_stage1
        pe1 = q_s2[s_next, a2] - q_mf1[a1]
        q_mf1[a1] += lr_stage1 * pe1
        
        # Stage 2 Update uses lr_stage2
        pe2 = r - q_s2[s_next, a2]
        q_s2[s_next, a2] += lr_stage2 * pe2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```