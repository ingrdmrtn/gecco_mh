Here are three new cognitive models that explore different mechanisms for learning and decision-making in the two-step task.

### Model 1: Split Learning Rates (Stage 1 vs Stage 2)
This model hypothesizes that the participant learns the value of spaceships (Stage 1) at a different rate than they learn the value of aliens (Stage 2). This separates the "habitual" choice of spaceship from the "goal-directed" value estimation of the aliens.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Split Learning Rate Model (Stage 1 vs Stage 2).
    
    Distinguishes between the learning rate for the first decision (spaceships)
    and the second decision (aliens). This allows the model to capture 
    fast adaptation to alien rewards but slow habit formation for spaceships 
    (or vice versa).
    
    Parameters:
    lr_1: Learning rate for Stage 1 (Spaceships) [0,1]
    lr_2: Learning rate for Stage 2 (Aliens) [0,1]
    beta: Inverse temperature [0,10]
    w: Model-based weight (0=MF, 1=MB) [0,1]
    p: Perseveration (stickiness) [0,1]
    lam: Eligibility trace parameter [0,1]
    """
    lr_1, lr_2, beta, w, p, lam = model_parameters
    n_trials = len(action_1)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    
    prev_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Apply perseveration
        if prev_action_1 != -1:
            q_net_stage1[prev_action_1] += p
            
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        s1_choice = action_1[trial]
        state_idx = state[trial]
        prev_action_1 = s1_choice

        # --- Stage 2 Policy ---
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        s2_choice = action_2[trial]
        r = reward[trial]

        # --- Updates ---
        # TD Error Stage 1
        delta_stage1 = q_stage2_mf[state_idx, s2_choice] - q_stage1_mf[s1_choice]
        # Update Stage 1 MF values using lr_1
        q_stage1_mf[s1_choice] += lr_1 * delta_stage1
        
        # TD Error Stage 2
        delta_stage2 = r - q_stage2_mf[state_idx, s2_choice]
        # Update Stage 2 MF values using lr_2
        q_stage2_mf[state_idx, s2_choice] += lr_2 * delta_stage2
        
        # Eligibility Trace: Update Stage 1 based on Stage 2 error
        # Uses lr_1 because we are updating Stage 1 weights
        q_stage1_mf[s1_choice] += lr_1 * lam * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Value Decay (Forgetting)
This model introduces a memory decay mechanism. If an alien (Stage 2 option) is not chosen, its value decays towards 0. This mechanism explains behavior where participants might re-explore options they haven't visited in a while, or simply forget old reward probabilities.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Value Decay on Unchosen Options.
    
    Incorporates a forgetting mechanism where the Q-values of unchosen 
    aliens (Stage 2) decay on every trial. This promotes exploration 
    or regression to a baseline.
    
    Parameters:
    lr: Learning rate [0,1]
    beta: Inverse temperature [0,10]
    w: Model-based weight [0,1]
    lam: Eligibility trace [0,1]
    decay: Decay rate for unchosen Stage 2 actions [0,1]
    """
    lr, beta, w, lam, decay = model_parameters
    n_trials = len(action_1)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        s1_choice = action_1[trial]
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        s2_choice = action_2[trial]
        r = reward[trial]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, s2_choice] - q_stage1_mf[s1_choice]
        q_stage1_mf[s1_choice] += lr * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[state_idx, s2_choice]
        q_stage2_mf[state_idx, s2_choice] += lr * delta_stage2
        
        # Eligibility trace
        q_stage1_mf[s1_choice] += lr * lam * delta_stage2
        
        # --- Decay Mechanism ---
        # Decay the value of the unchosen alien in the current state
        unchosen_s2 = 1 - s2_choice
        q_stage2_mf[state_idx, unchosen_s2] *= (1.0 - decay)
        
        # Optionally, one could decay the aliens in the *other* planet too, 
        # but standard decay usually applies to the current context or all.
        # Here we decay the unchosen option in the visited state to drive exploration within that state.

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Outcome-Dependent Perseveration (Win-Stay, Lose-Shift Stickiness)
Instead of a single "stickiness" parameter, this model differentiates between repeating a choice after a reward (Win-Stay) versus repeating a choice after no reward. This captures more granular heuristic behaviors often seen in reinforcement learning tasks.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Outcome-Dependent Perseveration Model.
    
    Replaces the single perseveration parameter with two separate parameters:
    one for sticking to the previous choice after a reward (stick_win), 
    and one for sticking after a non-reward (stick_loss).
    
    Parameters:
    lr: Learning rate [0,1]
    beta: Inverse temperature [0,10]
    w: Model-based weight [0,1]
    lam: Eligibility trace [0,1]
    stick_win: Perseveration bonus after a reward [0,5]
    stick_loss: Perseveration bonus after a loss [0,5]
    """
    lr, beta, w, lam, stick_win, stick_loss = model_parameters
    n_trials = len(action_1)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1
    prev_reward = 0

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Apply outcome-dependent perseveration
        if prev_action_1 != -1:
            if prev_reward == 1:
                q_net_stage1[prev_action_1] += stick_win
            else:
                q_net_stage1[prev_action_1] += stick_loss
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        s1_choice = action_1[trial]
        state_idx = state[trial]
        prev_action_1 = s1_choice

        # --- Stage 2 Policy ---
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        s2_choice = action_2[trial]
        r = reward[trial]
        prev_reward = r # Store for next trial's perseveration logic

        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, s2_choice] - q_stage1_mf[s1_choice]
        q_stage1_mf[s1_choice] += lr * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[state_idx, s2_choice]
        q_stage2_mf[state_idx, s2_choice] += lr * delta_stage2
        
        q_stage1_mf[s1_choice] += lr * lam * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```