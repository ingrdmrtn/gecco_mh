Here are three cognitive models based on the participant data and the two-step task structure.

### Model 1: Hybrid Model with Choice Stickiness (Perseveration)
**Rationale:** The participant data shows long streaks of choosing the same spaceship (e.g., Spaceship 1 from trial 12 to 40, and 63+), even when rewards are intermittent or absent. This suggests a "stickiness" or perseveration bias where the participant tends to repeat their previous Stage 1 choice regardless of the value calculated by the MB or MF systems. This model adds a parameter to quantify this behavioral inertia.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid model with Choice Stickiness (Perseveration).
    Adds a bonus to the value of the previously chosen action in Stage 1,
    capturing the tendency to repeat choices.

    Parameters:
    learning_rate: [0,1] - Rate at which Q-values are updated.
    beta: [0,10] - Inverse temperature (exploration/exploitation trade-off).
    w: [0,1] - Weight of Model-Based system (0=Pure MF, 1=Pure MB).
    stickiness: [0,10] - Additional value added to the previously chosen action (perseveration bonus).
    """
    learning_rate, beta, w, stickiness = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values initialization
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    
    last_action_1 = -1

    for trial in range(n_trials):
        # Handle missing data
        if action_1[trial] == -1:
            last_action_1 = -1
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        # --- Stage 1 Policy ---
        # Model-Based Value Calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add Stickiness to the previously chosen action
        # We create a temporary Q-value vector for decision making
        q_decision = q_net.copy()
        if last_action_1 != -1:
            q_decision[last_action_1] += stickiness
        
        # Softmax Stage 1
        exp_q1 = np.exp(beta * q_decision)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        # Standard Softmax on Stage 2 MF values
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning (Update) ---
        # Stage 1 Prediction Error (TD(0))
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Stage 2 Prediction Error
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Update history
        last_action_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model with Asymmetric Learning Rates
**Rationale:** The participant continues to choose Spaceship 1 despite receiving 0 coins frequently (e.g., trials 27, 28, 31, 33). This suggests they might be updating their value estimates more strongly after rewards (positive prediction errors) than after omissions (negative prediction errors), or vice versa. This model splits the learning rate into $\alpha_{pos}$ and $\alpha_{neg}$.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid model with Asymmetric Learning Rates.
    Uses different learning rates for positive vs negative prediction errors,
    allowing for optimism or pessimism bias in value updating.

    Parameters:
    alpha_pos: [0,1] - Learning rate for positive prediction errors (better than expected).
    alpha_neg: [0,1] - Learning rate for negative prediction errors (worse than expected).
    beta: [0,10] - Inverse temperature.
    w: [0,1] - Weight of Model-Based system.
    """
    alpha_pos, alpha_neg, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        if action_1[trial] == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        if delta_stage1 > 0:
            q_stage1_mf[action_1[trial]] += alpha_pos * delta_stage1
        else:
            q_stage1_mf[action_1[trial]] += alpha_neg * delta_stage1
            
        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        if delta_stage2 > 0:
            q_stage2_mf[state_idx, action_2[trial]] += alpha_pos * delta_stage2
        else:
            q_stage2_mf[state_idx, action_2[trial]] += alpha_neg * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model with Stage-Specific Inverse Temperatures
**Rationale:** The decision dynamics in Stage 1 (planning/spaceship choice) and Stage 2 (bandit/alien choice) are distinct. The participant might be very decisive in Stage 2 (exploiting the best alien) but more exploratory or noisy in Stage 1, or vice versa. Using separate `beta` parameters allows the model to capture different levels of decision noise (randomness) for the two distinct types of choices.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid model with Stage-Specific Inverse Temperatures.
    Decouples the exploration/exploitation trade-off for the two stages.

    Parameters:
    learning_rate: [0,1] - Learning rate.
    beta_1: [0,10] - Inverse temperature for Stage 1 (Spaceship choice).
    beta_2: [0,10] - Inverse temperature for Stage 2 (Alien choice).
    w: [0,1] - Weight of Model-Based system.
    """
    learning_rate, beta_1, beta_2, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        if action_1[trial] == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        # --- Stage 1 Policy (uses beta_1) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy (uses beta_2) ---
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```