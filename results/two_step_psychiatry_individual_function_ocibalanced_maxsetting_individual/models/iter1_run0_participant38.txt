Here are three new cognitive models for the two-step decision task.

### Model 1: Hybrid Model-Based / Model-Free Agent
This model implements the canonical hybrid theory (Daw et al., 2011), hypothesizing that the participant combines a Model-Based (MB) planning system with a Model-Free (MF) reinforcement learning system.
*   **Mechanism**: The MB system calculates Stage 1 values by multiplying the known transition matrix with the learned Stage 2 values. The MF system learns Stage 1 values via prediction errors. The final Stage 1 choice is a weighted mixture of both.
*   **Parameters**: `learning_rate`, `beta_1`, `beta_2`, `w` (mixing weight).

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Agent.
    
    Combines a Model-Based planner (using the transition matrix) and a Model-Free
    learner (using TD errors). The 'w' parameter controls the balance between
    these strategies for the Stage 1 decision.
    
    Parameters:
    - learning_rate: [0, 1] Rate at which MF Q-values update.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - w: [0, 1] Mixing weight. 0 = Pure Model-Free, 1 = Pure Model-Based.
    """
    learning_rate, beta_1, beta_2, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based Value: V_MB(S1) = T * max(Q_S2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Value: weighted sum of MB and MF values
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_1 * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Policy ---
        state_idx = state[trial]
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]
        
        # Calculate Prediction Errors
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        
        # Update Stage 2 MF values
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Update Stage 1 MF values (Standard TD(0))
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Model-Free Agent with Choice Stickiness
This model builds on the Model-Free eligibility trace model (the "best so far") by adding a **stickiness** (perseverance) parameter.
*   **Mechanism**: Participants often repeat their previous Stage 1 choice regardless of reward (behavioral inertia). This model adds a "stickiness bonus" to the Q-value of the action chosen in the previous trial.
*   **Parameters**: `learning_rate`, `beta_1`, `beta_2`, `lam` (eligibility trace), `stickiness`.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Agent with Eligibility Traces and Choice Stickiness.
    
    This agent relies on Model-Free RL (TD-Lambda) but includes a 'stickiness'
    bias, increasing the probability of repeating the previous Stage 1 action.
    
    Parameters:
    - learning_rate: [0, 1] Update rate.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - lam: [0, 1] Eligibility trace decay (0=TD, 1=Monte Carlo).
    - stickiness: [0, 5] Bonus added to the Q-value of the previously chosen action.
    """
    learning_rate, beta_1, beta_2, lam, stickiness = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_a1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        q_net_stage1 = q_stage1_mf.copy()
        
        # Add stickiness bonus to the previous action
        if trial > 0 and prev_a1 != -1:
            q_net_stage1[prev_a1] += stickiness
            
        exp_q1 = np.exp(beta_1 * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Record action for next trial
        prev_a1 = action_1[trial]
        
        # --- Stage 2 Policy ---
        state_idx = state[trial]
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Update Stage 1 with eligibility trace
        q_stage1_mf[a1] += learning_rate * (delta_stage1 + lam * delta_stage2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Asymmetric Learning Rate Agent (Risk Sensitive)
This model tests if the participant learns differently from positive outcomes (Reward=1) versus negative outcomes (Reward=0).
*   **Mechanism**: It uses two distinct learning rates, `lr_pos` and `lr_neg`, applied based on the sign of the prediction error. This captures optimism (ignoring failures) or pessimism (ignoring successes). It maintains the eligibility trace (`lam`) to propagate these asymmetric updates to Stage 1.
*   **Parameters**: `lr_pos`, `lr_neg`, `beta` (shared), `lam`.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Asymmetric Learning Rate Agent with Eligibility Traces.
    
    Uses separate learning rates for positive and negative prediction errors.
    This allows the agent to update values differently when outcomes are better
    than expected (positive PE) vs worse than expected (negative PE).
    
    Parameters:
    - lr_pos: [0, 1] Learning rate for positive prediction errors.
    - lr_neg: [0, 1] Learning rate for negative prediction errors.
    - beta: [0, 10] Inverse temperature (shared for both stages).
    - lam: [0, 1] Eligibility trace parameter.
    """
    lr_pos, lr_neg, beta, lam = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Policy ---
        state_idx = state[trial]
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]
        
        # Calculate raw errors
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        
        # Update Stage 2 with asymmetric rates
        eff_lr_2 = lr_pos if delta_stage2 > 0 else lr_neg
        q_stage2_mf[state_idx, a2] += eff_lr_2 * delta_stage2
        
        # Update Stage 1 with asymmetric rates on the combined error
        total_error_s1 = delta_stage1 + lam * delta_stage2
        eff_lr_1 = lr_pos if total_error_s1 > 0 else lr_neg
        q_stage1_mf[a1] += eff_lr_1 * total_error_s1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```