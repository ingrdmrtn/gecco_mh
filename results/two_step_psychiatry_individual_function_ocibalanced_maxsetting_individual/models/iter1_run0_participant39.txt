Here are three new cognitive models for the two-step task, formulated as Python functions. They explore different mechanisms: eligibility traces, valence-dependent learning (asymmetry), and value decay.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free learner with Eligibility Traces (Lambda) and Stickiness.
    
    This model extends the standard hybrid model by allowing the stage 2 prediction error 
    to directly influence stage 1 values via an eligibility trace parameter (lambda). 
    This bridges the gap between pure TD(0) learning and Monte Carlo updates.

    Parameters:
    - lr: Learning rate [0,1]
    - beta_1: Inverse temperature for stage 1 choice [0,10]
    - beta_2: Inverse temperature for stage 2 choice [0,10]
    - w: Mixing weight (0 = pure MF, 1 = pure MB) [0,1]
    - stick: Choice stickiness for stage 1 [0,5]
    - lambda_coeff: Eligibility trace decay parameter [0,1]
    """
    lr, beta_1, beta_2, w, stick, lambda_coeff = model_parameters
    n_trials = len(action_1)
    
    # Fixed transition matrix: 70% probability of common transition
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    q_stage1_mf = np.zeros(2) 
    q_stage2_mf = np.zeros((2, 2)) # (planet, alien)
    
    log_loss = 0.0
    eps = 1e-10
    prev_action_1 = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based: Expected value of next state (using max of stage 2 values)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2

        # Net Value: Weighted sum of MB and MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf

        # Add Stickiness to the net value
        q_net_stick = q_net.copy()
        if prev_action_1 != -1:
            q_net_stick[prev_action_1] += stick

        # Softmax Choice 1
        exp_q1 = np.exp(beta_1 * q_net_stick)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = action_1[trial]
        log_loss -= np.log(probs_1[a1] + eps)

        # --- Stage 2 Policy ---
        a2 = action_2[trial]
        if a2 == -1:
            # Handle missing data or timeout
            prev_action_1 = a1
            continue
            
        state_idx = state[trial]
        qs_2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta_2 * qs_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        log_loss -= np.log(probs_2[a2] + eps)

        # --- Updates ---
        r = reward[trial]

        # Stage 2 Prediction Error
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        
        # Update Stage 2
        q_stage2_mf[state_idx, a2] += lr * delta_stage2

        # Stage 1 Prediction Error
        # Calculated using the updated stage 2 value to propagate latest info
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]

        # Update Stage 1
        # Includes direct TD error + eligibility trace from stage 2 error
        q_stage1_mf[a1] += lr * (delta_stage1 + lambda_coeff * delta_stage2)
        
        prev_action_1 = a1
        
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free learner with Asymmetric Learning Rates and Stickiness.
    
    This model posits that the participant learns differently from positive prediction errors 
    (better than expected) versus negative prediction errors (worse than expected).
    
    Parameters:
    - lr_pos: Learning rate for positive prediction errors [0,1]
    - lr_neg: Learning rate for negative prediction errors [0,1]
    - beta_1: Inverse temperature for stage 1 choice [0,10]
    - beta_2: Inverse temperature for stage 2 choice [0,10]
    - w: Mixing weight (0 = pure MF, 1 = pure MB) [0,1]
    - stick: Choice stickiness for stage 1 [0,5]
    """
    lr_pos, lr_neg, beta_1, beta_2, w, stick = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    log_loss = 0.0
    eps = 1e-10
    prev_action_1 = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        q_net_stick = q_net.copy()
        if prev_action_1 != -1:
            q_net_stick[prev_action_1] += stick
            
        exp_q1 = np.exp(beta_1 * q_net_stick)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = action_1[trial]
        log_loss -= np.log(probs_1[a1] + eps)
        
        # --- Stage 2 Policy ---
        a2 = action_2[trial]
        if a2 == -1:
            prev_action_1 = a1
            continue
            
        state_idx = state[trial]
        qs_2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta_2 * qs_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        log_loss -= np.log(probs_2[a2] + eps)
        
        # --- Updates ---
        r = reward[trial]
        
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        lr_2 = lr_pos if delta_stage2 > 0 else lr_neg
        q_stage2_mf[state_idx, a2] += lr_2 * delta_stage2
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        lr_1 = lr_pos if delta_stage1 > 0 else lr_neg
        q_stage1_mf[a1] += lr_1 * delta_stage1
        
        prev_action_1 = a1
        
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free learner with Value Decay and Stickiness.
    
    This model incorporates a forgetting mechanism (decay) for unchosen options. 
    This accounts for the environment's slowly changing probabilities, as old estimates 
    become less reliable over time.
    
    Parameters:
    - lr: Learning rate [0,1]
    - beta_1: Inverse temperature for stage 1 choice [0,10]
    - beta_2: Inverse temperature for stage 2 choice [0,10]
    - w: Mixing weight (0 = pure MF, 1 = pure MB) [0,1]
    - stick: Choice stickiness for stage 1 [0,5]
    - decay: Decay rate for unchosen options [0,1]
    """
    lr, beta_1, beta_2, w, stick, decay = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    log_loss = 0.0
    eps = 1e-10
    prev_action_1 = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        q_net_stick = q_net.copy()
        if prev_action_1 != -1:
            q_net_stick[prev_action_1] += stick
            
        exp_q1 = np.exp(beta_1 * q_net_stick)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = action_1[trial]
        log_loss -= np.log(probs_1[a1] + eps)
        
        # --- Stage 2 Policy ---
        a2 = action_2[trial]
        if a2 == -1:
            prev_action_1 = a1
            continue
            
        state_idx = state[trial]
        qs_2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta_2 * qs_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        log_loss -= np.log(probs_2[a2] + eps)
        
        # --- Updates ---
        r = reward[trial]
        
        # Update Chosen Stage 2
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += lr * delta_stage2
        
        # Decay Unchosen Stage 2 (in the current planet)
        unchosen_a2 = 1 - a2
        q_stage2_mf[state_idx, unchosen_a2] *= (1 - decay)
        
        # Update Chosen Stage 1
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        # Decay Unchosen Stage 1
        unchosen_a1 = 1 - a1
        q_stage1_mf[unchosen_a1] *= (1 - decay)
        
        prev_action_1 = a1
        
    return log_loss
```