Here are three new cognitive models for the two-step task, designed to capture the participant's behavior, particularly the strong persistence (stickiness) and the response to rewards.

### Model 1: Model-Based with Asymmetric Learning and Stickiness
This model posits that the participant uses a mental model of the spaceship-planet transitions (Model-Based) but updates their valuation of aliens differently depending on whether the outcome was positive or negative (Asymmetric Learning). This helps explain why they might persist in choices despite losses (low negative learning rate) or lock in quickly on wins. It also includes simple choice stickiness.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Model-Based Learner with Asymmetric Learning Rates and Stickiness.
    
    The agent uses a model-based strategy for Stage 1 (planning using transition matrix).
    For Stage 2 value updates, it uses different learning rates for positive (reward > 0)
    and negative (reward <= 0) outcomes. This captures potential risk sensitivity or 
    bias in processing wins versus losses.
    Includes choice stickiness on Stage 1 to account for perseveration.

    Parameters:
    alpha_pos: [0, 1] Learning rate for positive outcomes (reward > 0).
    alpha_neg: [0, 1] Learning rate for negative outcomes (reward <= 0).
    beta_1: [0, 10] Inverse temperature for Stage 1 choices.
    beta_2: [0, 10] Inverse temperature for Stage 2 choices.
    stickiness: [0, 5] Bonus added to the logit of the previously chosen action.
    """
    alpha_pos, alpha_neg, beta_1, beta_2, stickiness = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values for Stage 2 (aliens): 2 planets x 2 aliens
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1

    for trial in range(n_trials):
        # -- Stage 1 Policy (Model-Based + Stickiness) --
        # Calculate MB values: V(State) = max(Q_Stage2(State))
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        # Q_MB(Action) = T * V(State)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        logits_1 = beta_1 * q_stage1_mb
        
        # Add stickiness to previously chosen action
        if prev_action_1 != -1:
            logits_1[prev_action_1] += stickiness
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1)) # Subtract max for stability
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Update previous action
        prev_action_1 = action_1[trial]
        
        # -- Stage 2 Policy --
        state_idx = state[trial]
        a2 = action_2[trial]
        
        # If valid choice made in Stage 2 (ignoring -1 data)
        if a2 != -1:
            qs_current_state = q_stage2_mf[state_idx]
            logits_2 = beta_2 * qs_current_state
            exp_q2 = np.exp(logits_2 - np.max(logits_2))
            probs_2 = exp_q2 / np.sum(exp_q2)
            p_choice_2[trial] = probs_2[a2]
            
            # -- Learning --
            # Prediction error
            pe = reward[trial] - q_stage2_mf[state_idx, a2]
            
            # Asymmetric update
            if reward[trial] > 0:
                q_stage2_mf[state_idx, a2] += alpha_pos * pe
            else:
                q_stage2_mf[state_idx, a2] += alpha_neg * pe
        else:
            # Handle missing data: assume probability 1 so no penalty
            p_choice_2[trial] = 1.0 

    eps = 1e-10
    # Sum logs
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Model-Based with Choice Kernel (Momentum)
The participant shows long streaks of repeating the same spaceship. This model replaces simple 1-step stickiness with a "Choice Kernel" that accumulates over time. This creates a "momentum" effect: the more they choose an option, the more likely they are to choose it again, decaying slowly.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Based Learner with Choice Kernel (Momentum).
    
    Uses a model-based planner for Stage 1. 
    Instead of simple 1-step stickiness, this model maintains a 'Choice Kernel'
    that accumulates history of choices, decaying over time. This captures 
    longer-term perseveration or momentum observed in the data.
    
    Parameters:
    learning_rate: [0, 1] Rate at which Stage 2 Q-values are updated.
    beta_1: [0, 10] Inverse temperature for Stage 1 (Q-values).
    beta_2: [0, 10] Inverse temperature for Stage 2.
    k_decay: [0, 1] Decay rate of the choice kernel (0 = no decay, 1 = instant reset).
    k_weight: [0, 5] Weight of the choice kernel in the Stage 1 decision.
    """
    learning_rate, beta_1, beta_2, k_decay, k_weight = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage2_mf = np.zeros((2, 2))
    choice_kernel = np.zeros(2) # For actions 0 and 1 in Stage 1

    for trial in range(n_trials):
        # -- Stage 1 Policy --
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combine Model-Based Value and Choice Momentum
        logits_1 = (beta_1 * q_stage1_mb) + (k_weight * choice_kernel)
        
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Update Choice Kernel
        # Decay all traces
        choice_kernel = choice_kernel * (1 - k_decay)
        # Reinforce chosen action
        choice_kernel[action_1[trial]] += 1.0
        
        # -- Stage 2 Policy --
        state_idx = state[trial]
        a2 = action_2[trial]
        
        if a2 != -1:
            qs_current_state = q_stage2_mf[state_idx]
            logits_2 = beta_2 * qs_current_state
            exp_q2 = np.exp(logits_2 - np.max(logits_2))
            probs_2 = exp_q2 / np.sum(exp_q2)
            p_choice_2[trial] = probs_2[a2]
            
            # -- Learning --
            pe = reward[trial] - q_stage2_mf[state_idx, a2]
            q_stage2_mf[state_idx, a2] += learning_rate * pe
        else:
            p_choice_2[trial] = 1.0

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Model-Based with Forgetting and Stickiness
This model assumes the participant treats the environment as volatile. While they learn from observed outcomes, they also "forget" the values of unchosen aliens, causing those values to decay towards zero over time. This helps explain behavior where values don't remain static in the absence of feedback.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Based Learner with Forgetting and Stickiness.
    
    The agent learns Stage 2 values but 'forgets' the values of unchosen aliens,
    decaying them towards zero. This assumes the environment is volatile and 
    old information becomes less reliable.
    Includes simple stickiness for Stage 1.
    
    Parameters:
    learning_rate: [0, 1] Update rate for chosen options.
    beta_1: [0, 10] Inverse temperature for Stage 1.
    beta_2: [0, 10] Inverse temperature for Stage 2.
    forget_rate: [0, 1] Rate at which unchosen Q-values decay to 0.
    stickiness: [0, 5] Perseveration bonus for Stage 1.
    """
    learning_rate, beta_1, beta_2, forget_rate, stickiness = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage2_mf = np.zeros((2, 2))
    prev_action_1 = -1

    for trial in range(n_trials):
        # -- Stage 1 Policy --
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        logits_1 = beta_1 * q_stage1_mb
        if prev_action_1 != -1:
            logits_1[prev_action_1] += stickiness
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        prev_action_1 = action_1[trial]
        
        # -- Stage 2 Policy --
        state_idx = state[trial]
        a2 = action_2[trial]
        
        if a2 != -1:
            qs_current_state = q_stage2_mf[state_idx]
            logits_2 = beta_2 * qs_current_state
            exp_q2 = np.exp(logits_2 - np.max(logits_2))
            probs_2 = exp_q2 / np.sum(exp_q2)
            p_choice_2[trial] = probs_2[a2]
            
            # -- Learning and Forgetting --
            # Decay ALL values first (passage of time)
            q_stage2_mf *= (1 - forget_rate)
            
            # Update the chosen one (adding the learning signal)
            # Note: We must be careful not to double-penalize or mathematically conflict.
            # Standard approach: Q_new = Q_old * (1-forget) for unchosen.
            # For chosen: Q_new = Q_old + lr * (R - Q_old). 
            # To implement this cleanly without overwriting the chosen update with decay:
            
            # Revert decay for the chosen one to apply standard update, 
            # OR apply decay to everything and then apply update to chosen based on decayed value?
            # Standard implementations usually separate them. Let's separate.
            
            # Restore the chosen one to pre-decay state for the standard update formula, 
            # then apply standard update? No, that implies chosen doesn't forget.
            # Let's use the explicit loop for clarity and correctness.
            
            # Re-fetch the value before decay
            old_val = q_stage2_mf[state_idx, a2] / (1 - forget_rate) if (1-forget_rate) > 0 else 0
            
            # Actually, simplest interpretation: Unchosen decay, Chosen updates.
            # Reset the chosen value to what it was before the bulk decay line above
            # (This is slightly inefficient but clear logic)
            # Better:
            # 1. Store old Q
            # 2. Apply decay to everything
            # 3. Overwrite chosen with standard update Q_old + lr*(R - Q_old)
            
            # Undo decay for chosen to calculate PE based on Q_t (not Q_t+1_decayed)
            q_stage2_mf[state_idx, a2] = q_stage2_mf[state_idx, a2] / (1 - forget_rate) if (1-forget_rate) > 1e-9 else 0
            
            pe = reward[trial] - q_stage2_mf[state_idx, a2]
            q_stage2_mf[state_idx, a2] += learning_rate * pe
            
        else:
            p_choice_2[trial] = 1.0

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```