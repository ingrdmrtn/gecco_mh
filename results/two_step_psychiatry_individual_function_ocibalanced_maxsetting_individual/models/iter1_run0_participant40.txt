Here are the three proposed cognitive models.

### Model 1: Hybrid Model with Separate Learning Rates
This model hypothesizes that the participant learns the value of the spaceships (Stage 1) and the aliens (Stage 2) at different rates. For instance, they might be slow to change their spaceship preference (`alpha1`) to account for the stable transition structure, but quick to update their alien preference (`alpha2`) to track the drifting reward probabilities.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Separate Learning Rates for Stage 1 and Stage 2.
    
    This model distinguishes between the learning rate for the first stage (spaceships)
    and the second stage (aliens/planets). It assumes the participant updates values
    at different speeds for the two hierarchical levels of the task.
    
    Parameters:
    - alpha1: [0, 1] Learning rate for Stage 1 (spaceships).
    - alpha2: [0, 1] Learning rate for Stage 2 (aliens).
    - beta: [0, 10] Inverse temperature (choice consistency).
    - w: [0, 1] Weighting between Model-Based (1) and Model-Free (0).
    - p: [0, 5] Perseverance bonus for repeating the previous Stage 1 choice.
    """
    alpha1, alpha2, beta, w, p = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if prev_action_1 != -1:
            q_net[prev_action_1] += p
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        # Action value updating for choice 1 using alpha1
        q_stage1_mf[action_1[trial]] += alpha1 * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        # Action value updating for choice 2 using alpha2
        q_stage2_mf[state_idx, action_2[trial]] += alpha2 * delta_stage2
        
        prev_action_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model with Asymmetric Learning Rates
This model captures the tendency to learn differently from positive prediction errors (better than expected) versus negative prediction errors (worse than expected). This asymmetry applies to both stages of the task, potentially explaining why the participant might stick to a choice despite occasional losses (low `alpha_neg`) or switch quickly after gains (high `alpha_pos`).

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Asymmetric Learning Rates (Positive vs Negative Prediction Errors).
    
    This model uses different learning rates depending on whether the prediction error
    (delta) is positive or negative. This allows for differential sensitivity to
    rewards (gains) versus omissions (losses).
    
    Parameters:
    - alpha_pos: [0, 1] Learning rate for positive prediction errors.
    - alpha_neg: [0, 1] Learning rate for negative prediction errors.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weighting between Model-Based (1) and Model-Free (0).
    - p: [0, 5] Perseverance bonus.
    """
    alpha_pos, alpha_neg, beta, w, p = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if prev_action_1 != -1:
            q_net[prev_action_1] += p
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        # Action value updating for choice 1 using asymmetric learning rates
        current_alpha1 = alpha_pos if delta_stage1 > 0 else alpha_neg
        q_stage1_mf[action_1[trial]] += current_alpha1 * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        # Action value updating for choice 2 using asymmetric learning rates
        current_alpha2 = alpha_pos if delta_stage2 > 0 else alpha_neg
        q_stage2_mf[state_idx, action_2[trial]] += current_alpha2 * delta_stage2
        
        prev_action_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model with Eligibility Traces (Lambda)
This model incorporates eligibility traces (`lam`), which allow the outcome of Stage 2 (the reward) to directly influence the value of the Stage 1 choice. This bridges the gap between pure Model-Free (TD(0)) and Monte Carlo (TD(1)) updates within the hybrid framework, allowing for more efficient credit assignment across the two steps.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Eligibility Traces (Lambda).
    
    This model includes an eligibility trace parameter (lambda) for the Model-Free component.
    This allows the reward prediction error from Stage 2 to update the value of the 
    Stage 1 choice, weighted by lambda.
    
    Parameters:
    - learning_rate: [0, 1] Global learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weighting between Model-Based (1) and Model-Free (0).
    - p: [0, 5] Perseverance bonus.
    - lam: [0, 1] Eligibility trace decay parameter (0 = TD(0), 1 = Monte Carlo).
    """
    learning_rate, beta, w, p, lam = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if prev_action_1 != -1:
            q_net[prev_action_1] += p
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        # Action value updating for choice 1 (TD(0) part)
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        # Action value updating for choice 2
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Additional update for choice 1 using eligibility trace from stage 2 error
        q_stage1_mf[action_1[trial]] += learning_rate * lam * delta_stage2
        
        prev_action_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```