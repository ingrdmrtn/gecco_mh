Here are three new cognitive models for the two-step task, designed based on the participant's behavior and the feedback provided.

### Model 1: Hybrid Model-Based / Model-Free with Eligibility Traces
This model integrates the structural knowledge of the Model-Based (MB) system with the temporal credit assignment of the Model-Free (MF) system using eligibility traces. While the "Hybrid" model (using `w`) and "MF-TD(lambda)" (using `lambda`) have been tried separately, combining them allows the model to arbitrate between a planning system and a reward-history system that accounts for the two-stage delay. This captures the possibility that the participant uses transition knowledge (MB) but relies on trace-decay (MF) for value updates, a common finding in this task.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Q-Learning with Eligibility Traces.
    
    Combines a Model-Based planner (using the known transition matrix) and a 
    Model-Free learner (using TD(lambda)). A weight parameter 'w' controls 
    the balance between the two systems during Stage 1 choice.
    
    Parameters:
    learning_rate: [0,1] - Update rate for Q-values.
    beta_1: [0,10] - Inverse temperature for Stage 1 choice.
    beta_2: [0,10] - Inverse temperature for Stage 2 choice.
    w: [0,1] - Weighting parameter (0 = Pure MF, 1 = Pure MB).
    lambda_param: [0,1] - Eligibility trace decay for MF system.
    """
    learning_rate, beta_1, beta_2, w, lambda_param = model_parameters
    n_trials = len(action_1)
    
    # Fixed transition probabilities for MB system
    # Spaceship 0 (A) -> Planet 0 (70%), Planet 1 (30%)
    # Spaceship 1 (U) -> Planet 1 (70%), Planet 0 (30%)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Model-Free Q-values
    q_mf_stage1 = np.zeros(2)
    q_mf_stage2 = np.zeros((2, 2)) # [state, action]

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based Value Calculation: V_MB = Transition * max(Q_MF_stage2)
        # We use the MF Q-values of stage 2 as the terminal values for the MB planner
        max_q_stage2 = np.max(q_mf_stage2, axis=1)
        q_mb_stage1 = transition_matrix @ max_q_stage2
        
        # Net Q-value: Weighted sum of MB and MF
        q_net_stage1 = w * q_mb_stage1 + (1 - w) * q_mf_stage1
        
        # Softmax Policy
        exp_q1 = np.exp(beta_1 * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        if a1 < 0 or a1 > 1: # Handle missing data
            p_choice_1[trial] = 1.0 / 2.0
            continue
        else:
            p_choice_1[trial] = probs_1[a1]
        
        state_idx = int(state[trial])
        
        # --- Stage 2 Choice ---
        # Pure Model-Free choice based on current state
        exp_q2 = np.exp(beta_2 * q_mf_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        if a2 < 0 or a2 > 1: # Handle missing data
            p_choice_2[trial] = 1.0 / 2.0
            continue
        else:
            p_choice_2[trial] = probs_2[a2]
        
        r = reward[trial]

        # --- Updates (Model-Free System) ---
        # Stage 1 TD Error
        delta_1 = q_mf_stage2[state_idx, a2] - q_mf_stage1[a1]
        q_mf_stage1[a1] += learning_rate * delta_1
        
        # Stage 2 TD Error
        delta_2 = r - q_mf_stage2[state_idx, a2]
        q_mf_stage2[state_idx, a2] += learning_rate * delta_2
        
        # Eligibility Trace: Propagate Stage 2 error to Stage 1 value
        q_mf_stage1[a1] += learning_rate * lambda_param * delta_2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Asymmetric Model-Free Q-Learning with Eligibility Traces
This model builds on the successful "MF with eligibility traces" approach but introduces asymmetric learning rates for positive and negative prediction errors. Participants often exhibit a "positivity bias" (learning more from gains) or "negativity bias" (learning more from omissions). This model captures such nuances in how the participant updates their valuation of spaceships and aliens, which a single learning rate cannot distinguish.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Asymmetric Model-Free Q-Learning with Eligibility Traces.
    
    Uses separate learning rates for positive and negative prediction errors,
    allowing for differential sensitivity to rewards (gains) versus omissions (losses).
    Includes an eligibility trace (lambda) to connect stage 2 outcomes to stage 1 choices.
    
    Parameters:
    alpha_pos: [0,1] - Learning rate for positive prediction errors.
    alpha_neg: [0,1] - Learning rate for negative prediction errors.
    beta_1: [0,10] - Inverse temperature for Stage 1 choice.
    beta_2: [0,10] - Inverse temperature for Stage 2 choice.
    lambda_param: [0,1] - Eligibility trace decay.
    """
    alpha_pos, alpha_neg, beta_1, beta_2, lambda_param = model_parameters
    n_trials = len(action_1)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_mf_stage1 = np.zeros(2)
    q_mf_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta_1 * q_mf_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        if a1 < 0 or a1 > 1:
            p_choice_1[trial] = 1.0 / 2.0
            continue
        p_choice_1[trial] = probs_1[a1]
        
        state_idx = int(state[trial])
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta_2 * q_mf_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        if a2 < 0 or a2 > 1:
            p_choice_2[trial] = 1.0 / 2.0
            continue
        p_choice_2[trial] = probs_2[a2]
        
        r = reward[trial]

        # --- Updates ---
        # Stage 1 Update
        delta_1 = q_mf_stage2[state_idx, a2] - q_mf_stage1[a1]
        lr_1 = alpha_pos if delta_1 > 0 else alpha_neg
        q_mf_stage1[a1] += lr_1 * delta_1
        
        # Stage 2 Update
        delta_2 = r - q_mf_stage2[state_idx, a2]
        lr_2 = alpha_pos if delta_2 > 0 else alpha_neg
        q_mf_stage2[state_idx, a2] += lr_2 * delta_2
        
        # Eligibility Trace Update
        # We apply the learning rate associated with the Stage 2 error
        q_mf_stage1[a1] += lr_2 * lambda_param * delta_2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Model-Free Q-Learning with Eligibility Traces and Stickiness
The participant data shows distinct streaks of repeating the same spaceship choice (e.g., trials 97-109), suggesting a tendency to persevere with a choice independent of reward history. This model adds a "stickiness" parameter to the best-performing MF-TD(lambda) chassis. Unlike the previously tried hybrid-stickiness model, this isolates whether the perseveration is a simple motor/choice repetition bias added to a purely model-free learning process.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Q-Learning with Eligibility Traces and Choice Stickiness.
    
    Augments the standard TD(lambda) model with a 'stickiness' parameter that
    biases the Stage 1 choice towards the action taken in the previous trial,
    capturing choice perseveration or repetition bias.
    
    Parameters:
    learning_rate: [0,1] - Update rate for Q-values.
    beta_1: [0,10] - Inverse temperature for Stage 1 choice.
    beta_2: [0,10] - Inverse temperature for Stage 2 choice.
    lambda_param: [0,1] - Eligibility trace decay.
    stickiness: [0,10] - Bonus added to the logits of the previously chosen action.
    """
    learning_rate, beta_1, beta_2, lambda_param, stickiness = model_parameters
    n_trials = len(action_1)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_mf_stage1 = np.zeros(2)
    q_mf_stage2 = np.zeros((2, 2))
    
    prev_a1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Calculate logits (Q * beta)
        logits_1 = beta_1 * q_mf_stage1
        
        # Add stickiness bonus to the previous action
        if prev_a1 != -1:
            logits_1[prev_a1] += stickiness
            
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        if a1 < 0 or a1 > 1:
            p_choice_1[trial] = 1.0 / 2.0
            prev_a1 = -1 # Reset stickiness if invalid trial
            continue
        
        p_choice_1[trial] = probs_1[a1]
        prev_a1 = a1 # Update previous action
        
        state_idx = int(state[trial])
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta_2 * q_mf_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        if a2 < 0 or a2 > 1:
            p_choice_2[trial] = 1.0 / 2.0
            continue
        p_choice_2[trial] = probs_2[a2]
        
        r = reward[trial]

        # --- Updates ---
        # Stage 1 TD(0) update
        delta_1 = q_mf_stage2[state_idx, a2] - q_mf_stage1[a1]
        q_mf_stage1[a1] += learning_rate * delta_1
        
        # Stage 2 TD(0) update
        delta_2 = r - q_mf_stage2[state_idx, a2]
        q_mf_stage2[state_idx, a2] += learning_rate * delta_2
        
        # Eligibility Trace update
        q_mf_stage1[a1] += learning_rate * lambda_param * delta_2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```