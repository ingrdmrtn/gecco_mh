Here are three new cognitive models based on the participant data and task structure.

### Model 1: Hybrid Agent with Asymmetric Learning Rates (Reward vs. No-Reward)
This model hypothesizes that the participant updates their value estimates differently depending on whether they receive a reward (Gold Coin) or not. This asymmetry captures "optimism" or "pessimism" and is particularly relevant given the drifting probabilities and the mix of successes and failures in the data.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF Agent with Asymmetric Learning Rates for Reward and No-Reward.
    
    This model allows for different learning rates depending on the outcome (Reward=1 vs Reward=0).
    This captures potential biases in how the participant processes positive vs negative feedback
    across both stages of the task.

    Parameters:
    - lr_pos: [0, 1] Learning rate when reward is received (1).
    - lr_neg: [0, 1] Learning rate when no reward is received (0).
    - beta: [0, 10] Inverse temperature (softness of choice).
    - w: [0, 1] Weighting parameter (0 = Pure MF, 1 = Pure MB).
    - lambda_coef: [0, 1] Eligibility trace parameter connecting Stage 2 outcome to Stage 1.
    - stickiness: [0, 10] Choice perseveration bonus for Stage 1.
    """
    lr_pos, lr_neg, beta, w, lambda_coef, stickiness = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    
    last_action_1 = -1

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        # Model-Based Value: Expected value of next stage states
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Value: Weighted sum of MB and MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax with Stickiness
        logits_1 = beta * q_net
        if last_action_1 != -1:
            logits_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_action_1 = action_1[trial]
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        q_s2 = q_stage2_mf[state_idx]
        logits_2 = beta * q_s2
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        # Select learning rate based on outcome
        current_lr = lr_pos if reward[trial] == 1 else lr_neg

        # Prediction errors
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]

        # Update Stage 1 MF values (TD(1) / Eligibility Trace)
        q_stage1_mf[action_1[trial]] += current_lr * (delta_stage1 + lambda_coef * delta_stage2)

        # Update Stage 2 MF values
        q_stage2_mf[state_idx, action_2[trial]] += current_lr * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Agent with Dual Learning Rates (Stage 1 vs. Stage 2)
This model acknowledges that the two stages of the task are fundamentally different: Stage 1 is a structural choice leading to a state, while Stage 2 is a bandit task leading to reward. The participant may learn the value of aliens (Stage 2) at a different speed than they update the value of spaceships (Stage 1).

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF Agent with Separate Learning Rates for Stage 1 and Stage 2.
    
    This model differentiates between the learning speed for the structural choice (Stage 1)
    and the bandit choice (Stage 2). This allows the agent to update alien values quickly
    while maintaining stable spaceship preferences, or vice versa.

    Parameters:
    - lr_stage1: [0, 1] Learning rate for Stage 1 MF values.
    - lr_stage2: [0, 1] Learning rate for Stage 2 Q-values.
    - beta: [0, 10] Inverse temperature (global choice randomness).
    - w: [0, 1] Weighting parameter (0 = Pure MF, 1 = Pure MB).
    - lambda_coef: [0, 1] Eligibility trace parameter.
    - stickiness: [0, 10] Choice perseveration bonus for Stage 1.
    """
    lr_stage1, lr_stage2, beta, w, lambda_coef, stickiness = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net
        if last_action_1 != -1:
            logits_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_action_1 = action_1[trial]
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        q_s2 = q_stage2_mf[state_idx]
        logits_2 = beta * q_s2
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]

        # Update Stage 1 MF using lr_stage1
        q_stage1_mf[action_1[trial]] += lr_stage1 * (delta_stage1 + lambda_coef * delta_stage2)

        # Update Stage 2 values using lr_stage2
        q_stage2_mf[state_idx, action_2[trial]] += lr_stage2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Agent with Stage 2 Stickiness
The participant data shows runs of choosing the same alien on the same planet, even after failures. This model includes a specific stickiness parameter for Stage 2 (aliens), allowing the agent to model perseveration at the bandit stage independently of the spaceship stage.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF Agent with Separate Stickiness for Stage 1 and Stage 2.
    
    This model adds a stickiness parameter to the second stage decisions.
    The agent tracks the last choice made in each state (planet) and applies a bonus
    to that choice in the next visit, capturing perseveration on specific aliens.

    Parameters:
    - learning_rate: [0, 1] Global learning rate.
    - beta: [0, 10] Global inverse temperature.
    - w: [0, 1] Weighting parameter (0 = Pure MF, 1 = Pure MB).
    - lambda_coef: [0, 1] Eligibility trace parameter.
    - stickiness_1: [0, 10] Choice perseveration bonus for Stage 1 (Spaceships).
    - stickiness_2: [0, 10] Choice perseveration bonus for Stage 2 (Aliens).
    """
    learning_rate, beta, w, lambda_coef, stickiness_1, stickiness_2 = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    # Track last action taken in each state (Planet 0 and Planet 1)
    last_action_2_per_state = np.array([-1, -1])

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net
        if last_action_1 != -1:
            logits_1[last_action_1] += stickiness_1
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_action_1 = action_1[trial]
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        q_s2 = q_stage2_mf[state_idx]
        logits_2 = beta * q_s2
        
        # Apply stickiness if we have visited this state before
        prev_action = last_action_2_per_state[state_idx]
        if prev_action != -1:
            logits_2[prev_action] += stickiness_2
            
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # Record action for this state
        last_action_2_per_state[state_idx] = action_2[trial]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]

        q_stage1_mf[action_1[trial]] += learning_rate * (delta_stage1 + lambda_coef * delta_stage2)
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```