Here are three cognitive models designed to explain the participant's behavior in the two-step task.

### Model 1: Dual Learning Rate Model
This model separates the learning rates for Stage 1 (Spaceship choice) and Stage 2 (Alien choice). The participant data shows very different dynamics between the two stages: extreme stickiness/stability in spaceship choice (long blocks of one spaceship) but potentially faster adaptation or noise in alien choices within those blocks. By decoupling `lr_1` and `lr_2`, the model can capture a slow-evolving preference for spaceships alongside a faster-updating valuation of aliens.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Dual Learning Rate Model with MB/MF arbitration and Perseveration.
    
    This model allows for different plasticity (learning rates) in the two stages of the task.
    It posits that the participant updates their valuation of the navigation steps (Spaceships)
    at a different rate than the bandit steps (Aliens).
    
    Parameters:
    - lr_1: [0, 1] Learning rate for Stage 1 (Spaceship) Q-values.
    - lr_2: [0, 1] Learning rate for Stage 2 (Alien) Q-values.
    - beta_1: [0, 10] Inverse temperature for Stage 1 choice.
    - beta_2: [0, 10] Inverse temperature for Stage 2 choice.
    - w: [0, 1] Weighting parameter (0 = Pure MF, 1 = Pure MB).
    - pers: [0, 1] Perseveration bonus for repeating the previous Stage 1 action.
    """
    lr_1, lr_2, beta_1, beta_2, w, pers = model_parameters
    n_trials = len(action_1)
    
    # Transition probabilities: A(0)->X(0) is 0.7, U(1)->Y(1) is 0.7
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)      # Model-free values for Spaceships
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for Aliens (State x Action)
    
    prev_action_1 = -1
    
    for trial in range(n_trials):
        
        # --- Stage 1 Choice ---
        # Model-Based Value: Expected max value of next stage
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Net Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Perseveration Bonus
        if prev_action_1 != -1:
            q_net[prev_action_1] += pers
            
        # Softmax Policy Stage 1
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        prev_action_1 = action_1[trial]
        state_idx = state[trial]
        
        # --- Stage 2 Choice ---
        # Softmax Policy Stage 2
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning Updates ---
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]
        
        # Stage 1 Update (TD(0) using Stage 2 Q-value)
        # Using separate learning rate lr_1
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr_1 * delta_stage1
        
        # Stage 2 Update (Reward Prediction Error)
        # Using separate learning rate lr_2
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += lr_2 * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Asymmetric Learning Rate Model
This model differentiates between learning from positive prediction errors (better than expected outcomes) and negative prediction errors (worse than expected). The participant demonstrates significant persistence, often staying with a choice despite receiving 0 coins (negative feedback). A low `lr_neg` would explain this insensitivity to loss, while `lr_pos` captures the reinforcement from wins.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Asymmetric Learning Rate Model (Pos/Neg) with MB/MF and Perseveration.
    
    Splits the learning rate based on the sign of the prediction error. 
    This allows the model to capture biases such as 'positivity bias' (learning more from wins)
    or 'confirmation bias' (ignoring losses), which aligns with the participant's sticky behavior.
    
    Parameters:
    - lr_pos: [0, 1] Learning rate for positive prediction errors (delta > 0).
    - lr_neg: [0, 1] Learning rate for negative prediction errors (delta < 0).
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - w: [0, 1] Weighting parameter (0 = Pure MF, 1 = Pure MB).
    - pers: [0, 1] Perseveration bonus.
    """
    lr_pos, lr_neg, beta_1, beta_2, w, pers = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1
    
    for trial in range(n_trials):
        
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if prev_action_1 != -1:
            q_net[prev_action_1] += pers
            
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        prev_action_1 = action_1[trial]
        state_idx = state[trial]
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning Updates ---
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        alpha_1 = lr_pos if delta_stage1 > 0 else lr_neg
        q_stage1_mf[a1] += alpha_1 * delta_stage1
        
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        alpha_2 = lr_pos if delta_stage2 > 0 else lr_neg
        q_stage2_mf[state_idx, a2] += alpha_2 * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Value Decay Model
This model incorporates a forgetting mechanism where the values of unchosen actions decay towards a neutral prior (0.5) over time. In the participant's data, they stick to one spaceship for very long periods. A decay model suggests that during these blocks, the value of the *other* spaceship slowly drifts. If it drifts to 0.5 (neutral) while the current spaceship performs poorly (below 0.5), a switch may occur. This models memory limitations or uncertainty estimation.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Value Decay Model with MB/MF and Perseveration.
    
    Incorporates a decay parameter that causes the Q-values of unchosen options 
    to drift towards a neutral value (0.5) on each trial. This represents 
    forgetting or increasing uncertainty about options not recently explored.
    
    Parameters:
    - lr: [0, 1] Learning rate for chosen options.
    - decay: [0, 1] Rate at which unchosen option values decay to 0.5.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - w: [0, 1] Weighting parameter (0 = Pure MF, 1 = Pure MB).
    - pers: [0, 1] Perseveration bonus.
    """
    lr, decay, beta_1, beta_2, w, pers = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1
    
    for trial in range(n_trials):
        
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if prev_action_1 != -1:
            q_net[prev_action_1] += pers
            
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        prev_action_1 = action_1[trial]
        state_idx = state[trial]
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning Updates ---
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]
        
        # Update Stage 1 Chosen
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        # Decay Stage 1 Unchosen
        unchosen_a1 = 1 - a1
        q_stage1_mf[unchosen_a1] = (1 - decay) * q_stage1_mf[unchosen_a1] + decay * 0.5
        
        # Update Stage 2 Chosen
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += lr * delta_stage2
        
        # Decay Stage 2 Unchosen (only for the current planet)
        unchosen_a2 = 1 - a2
        q_stage2_mf[state_idx, unchosen_a2] = (1 - decay) * q_stage2_mf[state_idx, unchosen_a2] + decay * 0.5

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```