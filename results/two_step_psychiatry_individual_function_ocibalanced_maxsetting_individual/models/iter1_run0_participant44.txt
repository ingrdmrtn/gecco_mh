Here are the 3 proposed cognitive models based on the participant data and the template structure.

### Model 1: Hybrid Learner with Separate Stage Learning Rates
This model hypothesizes that the participant learns the values of the "Spaceships" (Stage 1, abstract transitions) and the "Aliens" (Stage 2, concrete rewards) at different speeds. The data shows consistent behavior at Stage 2 (strong preference for aliens) but switching behavior at Stage 1, suggesting different sensitivities to prediction errors at each stage.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid learner with distinct learning rates for Stage 1 and Stage 2.
    
    Captures the possibility that the participant updates their valuation of 
    spaceships (Stage 1) at a different rate than their valuation of aliens (Stage 2).
    
    Parameters:
    - alpha1: [0, 1] Learning rate for Stage 1 (Spaceship choice).
    - alpha2: [0, 1] Learning rate for Stage 2 (Alien choice).
    - beta: [0, 10] Inverse temperature (choice consistency).
    - w: [0, 1] Mixing weight (0 = Model-Free, 1 = Model-Based).
    """
    alpha1, alpha2, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Mix MB and MF values
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Update Stage 1 values using alpha1
        # TD(0) update: value of chosen spaceship moves toward value of chosen alien
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += alpha1 * delta_stage1
        
        # Update Stage 2 values using alpha2
        # Prediction error based on actual reward received
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += alpha2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Learner with Choice Perseverance
The participant data shows significant "stickiness" or streaks (e.g., choosing Spaceship 1 for trials 1-12, or Spaceship 0 for trials 22-23) that might not be fully explained by reward feedback alone. This model adds a `perseverance` parameter that biases the participant to repeat their previous Stage 1 choice, regardless of the outcome.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid learner with Choice Perseverance (Stickiness).
    
    Adds a bias to repeat the previous Stage 1 action, capturing the 
    participant's tendency to form streaks of choices (motor perseveration).
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for value updates.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight (MB vs MF).
    - perseverance: [0, 5] Bonus added to the Q-value of the previously chosen action.
    """
    learning_rate, beta, w, perseverance = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1 # Initialize with no previous action

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add perseverance bonus to the logits (before softmax)
        logits = beta * q_net_stage1
        if prev_action_1 != -1:
            logits[prev_action_1] += perseverance
            
        exp_q1 = np.exp(logits)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Store current action for next trial's perseverance
        prev_action_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Learner with Stage-Specific Exploration (Separate Betas)
The participant appears to have very high confidence in Stage 2 (consistently picking the 'better' alien per planet) but higher uncertainty in Stage 1 (switching spaceships more often). This model allows for different levels of "exploration noise" (inverse temperature) for the spaceship choice (`beta1`) versus the alien choice (`beta2`).

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid learner with Stage-Specific Exploration levels.
    
    Uses two distinct inverse temperature parameters to capture different 
    levels of decision noise (randomness) at Stage 1 vs Stage 2.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta1: [0, 10] Inverse temperature for Stage 1 (Spaceship choice).
    - beta2: [0, 10] Inverse temperature for Stage 2 (Alien choice).
    - w: [0, 1] Mixing weight (MB vs MF).
    """
    learning_rate, beta1, beta2, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Use beta1 for the first stage choice
        exp_q1 = np.exp(beta1 * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        # Use beta2 for the second stage choice
        exp_q2 = np.exp(beta2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```