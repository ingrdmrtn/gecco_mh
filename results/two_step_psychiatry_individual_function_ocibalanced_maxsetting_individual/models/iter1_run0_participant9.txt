Here are the 3 new cognitive models based on the participant data and the two-step task structure.

### Model 1: Hybrid MB/MF with Separate Learning Rates
This model hypothesizes that the participant updates their value estimates for the two stages at different rates. For instance, they might learn the value of specific aliens (Stage 2) quickly while updating their preference for spaceships (Stage 1) more slowly, or vice versa. It builds on the successful "stickiness" mechanism identified in previous iterations.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based/Model-Free learner with Separate Learning Rates for Stages.
    
    This model allows for different plasticity (learning rates) in the high-level 
    planning stage (Stage 1) versus the low-level bandit stage (Stage 2).
    
    Parameters:
    lr_stage1: [0,1] - Learning rate for Stage 1 Q-values (Spaceships).
    lr_stage2: [0,1] - Learning rate for Stage 2 Q-values (Aliens).
    beta: [0,10] - Inverse temperature (exploration/exploitation balance).
    w: [0,1] - Weighting parameter (0 = Pure MF, 1 = Pure MB).
    stickiness: [0,5] - Bonus added to the Q-value of the previously chosen action.
    """
    lr_stage1, lr_stage2, beta, w, stickiness = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if prev_action_1 != -1:
            q_net_1[prev_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        prev_action_1 = action_1[trial]
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        # ACTION VALUE UPDATING FOR CHOICE 1 (Using lr_stage1)
        q_stage1_mf[action_1[trial]] += lr_stage1 * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        # ACTION VALUE UPDATING FOR CHOICE 2 (Using lr_stage2)
        q_stage2_mf[state_idx, action_2[trial]] += lr_stage2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid MB/MF with Separate Inverse Temperatures
This model captures the possibility that the participant exhibits different levels of randomness (exploration noise) in the two stages. For example, they might be very precise and exploitative when choosing aliens (Stage 2) but more exploratory or noisy when choosing spaceships (Stage 1).

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based/Model-Free learner with Separate Inverse Temperatures (Betas).
    
    This model assumes different levels of choice stochasticity for Stage 1 
    and Stage 2.
    
    Parameters:
    learning_rate: [0,1] - Learning rate for Q-values.
    beta_stage1: [0,10] - Inverse temperature for Stage 1.
    beta_stage2: [0,10] - Inverse temperature for Stage 2.
    w: [0,1] - Weighting parameter (0 = Pure MF, 1 = Pure MB).
    stickiness: [0,5] - Choice stickiness for Stage 1.
    """
    learning_rate, beta_stage1, beta_stage2, w, stickiness = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if prev_action_1 != -1:
            q_net_1[prev_action_1] += stickiness
            
        exp_q1 = np.exp(beta_stage1 * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        prev_action_1 = action_1[trial]
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta_stage2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        # ACTION VALUE UPDATING FOR CHOICE 1
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        # ACTION VALUE UPDATING FOR CHOICE 2
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid MB/MF with Decay for Unchosen Options
This model implements a "forgetting" mechanism. In standard Q-learning, the value of unchosen options remains static. Here, the value of the unchosen alien in the current state decays towards zero. This can explain behavior where a participant revisits a previously poor option because they have "forgotten" how bad it was, or conversely, stops choosing a good option if they haven't visited it in a while.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based/Model-Free learner with Decay for Unchosen Options.
    
    This model adds a passive decay to the Q-values of unchosen Stage 2 options,
    representing memory loss or a "return to baseline" heuristic.
    
    Parameters:
    learning_rate: [0,1] - Learning rate for chosen options.
    beta: [0,10] - Inverse temperature.
    w: [0,1] - Weighting parameter.
    stickiness: [0,5] - Choice stickiness for Stage 1.
    decay: [0,1] - Decay rate for unchosen Stage 2 options (1 = full forget, 0 = no forget).
    """
    learning_rate, beta, w, stickiness, decay = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if prev_action_1 != -1:
            q_net_1[prev_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        prev_action_1 = action_1[trial]
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        # ACTION VALUE UPDATING FOR CHOICE 1
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        # ACTION VALUE UPDATING FOR CHOICE 2
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Decay unchosen option in the current state
        unchosen_action_2 = 1 - action_2[trial]
        q_stage2_mf[state_idx, unchosen_action_2] *= (1 - decay)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```