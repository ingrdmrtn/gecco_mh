Here are three cognitive models formulated as Python functions. These models explore different mechanisms for credit assignment, learning asymmetry, and stage-specific learning rates, designed to capture the stickiness and switching behavior observed in the participant data.

### Model 1: TD($\lambda$) with Stickiness (Pure Model-Free)
This model hypothesizes that the participant does not explicitly use a model of the transition probabilities (Model-Based) but instead uses an eligibility trace (TD-$\lambda$) to assign credit from the final reward back to the first-stage choice. It includes a stickiness parameter to account for the observed perseveration.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    TD(Lambda) Model-Free Learner with Choice Stickiness.
    
    This model assumes the participant is a pure model-free learner but uses 
    eligibility traces (lambda) to modulate how much the Stage 2 reward 
    updates the Stage 1 value directly.
    
    Parameters:
    learning_rate: [0, 1] - Rate of value updating.
    beta: [0, 10] - Inverse temperature for softmax choice.
    lam: [0, 1] - Eligibility trace decay parameter (0 = TD(0), 1 = Monte Carlo).
    stickiness: [0, 5] - Bonus added to the previously chosen Stage 1 action.
    """
    learning_rate, beta, lam, stickiness = model_parameters
    n_trials = len(action_1)
    
    # Initialize Q-values
    # Stage 1: 2 actions
    q_stage1 = np.zeros(2) # Model-free values for stage 1
    # Stage 2: 2 states (planets) x 2 actions (aliens)
    q_stage2 = np.zeros((2, 2)) 
    
    last_action_1 = -1
    log_loss = 0.0
    eps = 1e-10
    
    for trial in range(n_trials):
        a1 = action_1[trial]
        s_idx = state[trial]
        a2 = action_2[trial]
        r = reward[trial]
        
        # --- Stage 1 Choice ---
        # Add stickiness to the Q-value of the previous action
        q_stage1_biased = q_stage1.copy()
        if last_action_1 != -1:
            q_stage1_biased[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_stage1_biased)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        # Accumulate loss for Stage 1
        if a1 != -1:
            log_loss -= np.log(probs_1[a1] + eps)
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        if a2 != -1:
            log_loss -= np.log(probs_2[a2] + eps)
            
            # --- Learning Updates ---
            # Prediction Error 1: Difference between Stage 2 value and Stage 1 value
            # Standard SARSA/Q-learning update for Stage 1 based on Stage 2 state-action value
            delta_1 = q_stage2[s_idx, a2] - q_stage1[a1]
            q_stage1[a1] += learning_rate * delta_1
            
            # Prediction Error 2: Difference between Reward and Stage 2 value
            delta_2 = r - q_stage2[s_idx, a2]
            
            # Update Stage 2 value
            q_stage2[s_idx, a2] += learning_rate * delta_2
            
            # Eligibility Trace Update for Stage 1:
            # Stage 1 value is further updated by the Stage 2 RPE, scaled by lambda.
            # If lambda=1, Stage 1 learns directly from Reward. If lambda=0, only from Stage 2 Value.
            q_stage1[a1] += learning_rate * lam * delta_2
            
        last_action_1 = a1

    return log_loss
```

### Model 2: Hybrid MB/MF with Dual Learning Rates
This model posits that the participant learns the values of the aliens (Stage 2) and the values of the spaceships (Stage 1) at different rates. Stage 2 requires tracking drifting probabilities, while Stage 1 involves structural decisions. This model combines Model-Based and Model-Free systems.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Dual Learning Rates.
    
    Differentiates learning speeds for the two stages. Stage 2 (aliens) is a 
    drifting bandit task, potentially requiring a different learning rate than 
    the Stage 1 (spaceship) Model-Free caching.
    
    Parameters:
    lr_stage1: [0, 1] - Learning rate for Stage 1 Model-Free values.
    lr_stage2: [0, 1] - Learning rate for Stage 2 values (used by both MF and MB).
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - Mixing weight (0 = pure MF, 1 = pure MB).
    """
    lr_stage1, lr_stage2, beta, w = model_parameters
    n_trials = len(action_1)
    
    # Fixed transition matrix: [P(State 0|Action 0), P(State 1|Action 0)]
    #                          [P(State 0|Action 1), P(State 1|Action 1)]
    # Action 0 (A) -> Planet 0 (X) commonly (0.7)
    # Action 1 (U) -> Planet 1 (Y) commonly (0.7)
    # Note: State indices in data: Planet 0, Planet 1.
    # Transition matrix based on task description:
    # A(0) -> X(0) 0.7, Y(1) 0.3
    # U(1) -> X(0) 0.3, Y(1) 0.7
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_mf_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2)) # State x Action
    
    log_loss = 0.0
    eps = 1e-10
    
    for trial in range(n_trials):
        a1 = action_1[trial]
        s_idx = state[trial]
        a2 = action_2[trial]
        r = reward[trial]
        
        # --- Stage 1 Choice ---
        # Model-Based Value: Expected max value of next stage
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_mb_stage1 = transition_matrix @ max_q_stage2
        
        # Integrated Value
        q_net_stage1 = w * q_mb_stage1 + (1 - w) * q_mf_stage1
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        if a1 != -1:
            log_loss -= np.log(probs_1[a1] + eps)
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        if a2 != -1:
            log_loss -= np.log(probs_2[a2] + eps)
            
            # --- Learning Updates ---
            # Update Stage 2 Q-values (Bandit task)
            delta_2 = r - q_stage2[s_idx, a2]
            q_stage2[s_idx, a2] += lr_stage2 * delta_2
            
            # Update Stage 1 MF Q-values (TD(0))
            # Note: We use the *updated* Stage 2 value or the pre-update? 
            # Standard TD uses the value of the state landed in (before outcome known), 
            # but in 2-step tasks, it's often modeled as updating Q1 towards Q2.
            delta_1 = q_stage2[s_idx, a2] - q_mf_stage1[a1]
            q_mf_stage1[a1] += lr_stage1 * delta_1
            
    return log_loss
```

### Model 3: Hybrid MB/MF with Asymmetric Learning
This model investigates whether the participant updates their beliefs differently following positive prediction errors (rewards/better-than-expected) versus negative prediction errors (omissions/worse-than-expected). This asymmetry applies to both the Stage 2 values and the Stage 1 Model-Free values.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Asymmetric Learning Rates (Confirmation Bias/Risk).
    
    Updates Q-values using different learning rates depending on whether the 
    prediction error is positive or negative. This captures potential biases 
    where the participant might ignore losses or over-weight wins.
    
    Parameters:
    alpha_pos: [0, 1] - Learning rate for positive prediction errors.
    alpha_neg: [0, 1] - Learning rate for negative prediction errors.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - Mixing weight (0 = pure MF, 1 = pure MB).
    """
    alpha_pos, alpha_neg, beta, w = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_mf_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    log_loss = 0.0
    eps = 1e-10
    
    for trial in range(n_trials):
        a1 = action_1[trial]
        s_idx = state[trial]
        a2 = action_2[trial]
        r = reward[trial]
        
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_mb_stage1 = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_mb_stage1 + (1 - w) * q_mf_stage1
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        if a1 != -1:
            log_loss -= np.log(probs_1[a1] + eps)
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        if a2 != -1:
            log_loss -= np.log(probs_2[a2] + eps)
            
            # --- Learning Updates ---
            
            # Stage 2 Update (Bandit)
            delta_2 = r - q_stage2[s_idx, a2]
            lr_2 = alpha_pos if delta_2 > 0 else alpha_neg
            q_stage2[s_idx, a2] += lr_2 * delta_2
            
            # Stage 1 MF Update (TD(0))
            # We use the updated q_stage2 to drive the MF update
            delta_1 = q_stage2[s_idx, a2] - q_mf_stage1[a1]
            lr_1 = alpha_pos if delta_1 > 0 else alpha_neg
            q_mf_stage1[a1] += lr_1 * delta_1
            
    return log_loss
```