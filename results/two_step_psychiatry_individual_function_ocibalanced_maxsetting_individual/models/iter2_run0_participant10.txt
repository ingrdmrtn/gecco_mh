Here are three cognitive models based on the participant data and the two-step task structure.

### Model 1: Hybrid Model with Eligibility Traces (TD-$\lambda$)
This model integrates Model-Based (MB) and Model-Free (MF) value estimation. Unlike the baseline, it uses an eligibility trace ($\lambda$) to allow the Stage 2 reward prediction error to directly update Stage 1 MF values. This tests if the participant combines structural knowledge (MB) with temporal difference learning that bridges the two stages.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based/Model-Free learner with Eligibility Traces (TD-lambda).
    
    Combines a weighted mixture of Model-Based transition planning and 
    Model-Free reinforcement learning. The Model-Free component uses 
    eligibility traces to propagate Stage 2 reward errors back to Stage 1.

    Parameters:
    lr:           [0, 1] Learning rate for Model-Free updates.
    beta:         [0, 10] Inverse temperature (softmax sensitivity).
    w:            [0, 1] Weight of Model-Based system (0=Pure MF, 1=Pure MB).
    lambda_param: [0, 1] Eligibility trace decay (0=TD(0), 1=Monte Carlo).
    """
    lr, beta, w, lambda_param = model_parameters
    n_trials = len(action_1)
    
    # Fixed transition matrix for MB calculation: [0.7, 0.3; 0.3, 0.7]
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # [State, Action]

    for trial in range(n_trials):
        # Handle missing data
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        # --- Stage 1 Policy ---
        # Model-Based Value: Transition prob * max(Stage 2 values)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax Choice 1
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        
        # --- Stage 2 Policy ---
        # Standard Model-Free Q-values for Stage 2
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        # Prediction Error 1: Difference between Stage 2 Value and Stage 1 Value
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        # Prediction Error 2: Difference between Reward and Stage 2 Value
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Update Stage 2 Q-value
        q_stage2_mf[state_idx, action_2[trial]] += lr * delta_stage2
        
        # Update Stage 1 Q-value using Eligibility Trace
        # Combines the immediate transition error and the subsequent reward error
        q_stage1_mf[action_1[trial]] += lr * (delta_stage1 + lambda_param * delta_stage2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Model-Free with Split Learning Rates and Perseveration
This model hypothesizes that the participant learns at different speeds for the high-level navigation (Stage 1) versus the specific alien selection (Stage 2). It removes the Model-Based component but includes Choice Perseveration ("stickiness") to account for the participant's tendency to repeat Stage 1 choices.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free learner with Separate Stage Learning Rates and Perseveration.
    
    Assumes the participant learns Stage 1 (Spaceships) and Stage 2 (Aliens)
    values at different rates. Includes a sticky choice bonus for Stage 1.
    
    Parameters:
    lr_1:          [0, 1] Learning rate for Stage 1 (Spaceships).
    lr_2:          [0, 1] Learning rate for Stage 2 (Aliens).
    beta:          [0, 10] Inverse temperature.
    perseveration: [0, 5] Stickiness bonus added to the previously chosen Stage 1 action.
    """
    lr_1, lr_2, beta, perseveration = model_parameters
    n_trials = len(action_1)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    prev_action_1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        # --- Stage 1 Policy ---
        q_stage1_eff = q_stage1.copy()
        if prev_action_1 != -1:
            q_stage1_eff[prev_action_1] += perseveration
            
        exp_q1 = np.exp(beta * q_stage1_eff)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        # Update Stage 2 (Direct Reward)
        delta_stage2 = reward[trial] - q_stage2[state_idx, action_2[trial]]
        q_stage2[state_idx, action_2[trial]] += lr_2 * delta_stage2
        
        # Update Stage 1 (TD-1 style: update based on reward directly)
        # We assume a direct reinforcement from the final outcome to the first choice
        # This effectively sets lambda=1 implicitly for the connection, but controlled by lr_1
        delta_total = reward[trial] - q_stage1[action_1[trial]]
        q_stage1[action_1[trial]] += lr_1 * delta_total
        
        prev_action_1 = action_1[trial]

    eps = 1e-10
    valid_trials = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```

### Model 3: Hybrid Model with Memory Decay
This model assumes the participant uses a Hybrid (MB+MF) strategy but suffers from passive forgetting. Unchosen options in both Stage 1 and Stage 2 decay toward zero. This mechanism can explain why the participant might switch strategies after a long period of not visiting a specific state or choosing a specific action.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Passive Memory Decay.
    
    A Hybrid learner where Q-values for unchosen actions decay over time.
    This accounts for forgetting or uncertainty growth for unvisited options.
    
    Parameters:
    lr:    [0, 1] Learning rate.
    beta:  [0, 10] Inverse temperature.
    w:     [0, 1] Weight of Model-Based system.
    decay: [0, 1] Decay rate for unchosen actions (0=no decay, 1=instant forgetting).
    """
    lr, beta, w, decay = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        # Standard SARSA/Q-Learning updates
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr * delta_stage2
        
        # --- Decay Unchosen ---
        # Decay Stage 1 unchosen
        unchosen_1 = 1 - action_1[trial]
        q_stage1_mf[unchosen_1] *= (1 - decay)
        
        # Decay Stage 2 unchosen (only for the visited state)
        unchosen_2 = 1 - action_2[trial]
        q_stage2_mf[state_idx, unchosen_2] *= (1 - decay)
        
        # Optional: Decay Stage 2 unvisited state? 
        # For simplicity, we only decay the unchosen option in the *visited* state
        # to mimic "forgetting the alternative" in the current context.

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```