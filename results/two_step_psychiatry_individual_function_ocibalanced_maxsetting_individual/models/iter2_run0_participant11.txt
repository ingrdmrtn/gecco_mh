Here are three new cognitive models for the two-step task, designed to capture different hypotheses about the participant's behavior, particularly focusing on how they integrate model-based information, learn transition structures, and form habits.

### Model 1: Hybrid Model-Based / Model-Free with TD(lambda)
This model extends the "Best Model" (TD(lambda)) by re-introducing the Model-Based component. It tests the hypothesis that the participant is not purely Model-Free but mixes a Model-Based planning strategy with a flexible Model-Free system that uses eligibility traces (allowing it to vary between TD(0) and Monte Carlo updating).

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free with TD(lambda).
    
    Combines the weighted hybrid architecture (Daw et al., 2011) with eligibility traces
    in the Model-Free component. This allows the model to span between pure Model-Based,
    pure TD(0) Model-Free, and pure TD(1) Model-Free behavior.

    Parameters:
    - learning_rate: [0, 1] Update rate for Q-values.
    - beta_1: [0, 10] Inverse temperature for Stage 1 choices.
    - beta_2: [0, 10] Inverse temperature for Stage 2 choices.
    - w: [0, 1] Weighting parameter (0 = Pure MF, 1 = Pure MB).
    - lambda_param: [0, 1] Eligibility trace decay for MF updates.
    - stickiness: [0, 5] Perseveration bonus for repeated Stage 1 choices.
    """
    learning_rate, beta_1, beta_2, w, lambda_param, stickiness = model_parameters
    n_trials = len(action_1)
    
    # Q-values
    q_mf_1 = np.zeros(2)
    q_mf_2 = np.zeros((2, 2)) # State x Action
    
    # Fixed transition matrix for MB calculation
    # Row 0: Space 0 -> [Planet 0 (0.7), Planet 1 (0.3)]
    # Row 1: Space 1 -> [Planet 0 (0.3), Planet 1 (0.7)]
    trans_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1
    
    for t in range(n_trials):
        # Stage 1 Policy
        
        # Model-Based Values: Expected value of next state given transition matrix
        v_stage2 = np.max(q_mf_2, axis=1) # [V(State0), V(State1)]
        q_mb_1 = trans_matrix @ v_stage2
        
        # Net Q-values: Weighted sum of MB and MF
        q_net_1 = w * q_mb_1 + (1 - w) * q_mf_1
        
        # Add stickiness
        if last_action_1 != -1:
            q_net_1[last_action_1] += stickiness
            
        # Softmax Stage 1
        exp_q1 = np.exp(beta_1 * q_net_1)
        if np.sum(exp_q1) == 0: exp_q1 = np.ones(2) # Prevent numerical error
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[t] = probs_1[action_1[t]]
        
        # Stage 2 Policy
        state_idx = state[t]
        exp_q2 = np.exp(beta_2 * q_mf_2[state_idx])
        if np.sum(exp_q2) == 0: exp_q2 = np.ones(2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[t] = probs_2[action_2[t]]
        
        # Updates
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]
        
        # Prediction Errors
        delta_1 = q_mf_2[state_idx, a2] - q_mf_1[a1]
        delta_2 = r - q_mf_2[state_idx, a2]
        
        # Update Stage 1 (with eligibility trace lambda)
        q_mf_1[a1] += learning_rate * (delta_1 + lambda_param * delta_2)
        
        # Update Stage 2
        q_mf_2[state_idx, a2] += learning_rate * delta_2
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Dynamic Transition Learning Hybrid
This model assumes the participant does not trust the fixed 70/30 transition probabilities but learns them from experience. The Model-Based system updates its belief about the transition matrix ($T$) after every trial. This captures behavior where the participant might react to "rare" transitions by updating their internal model of the spaceship-planet mapping.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Dynamic Transition Learning Hybrid Model.
    
    Assumes the participant learns the transition structure between spaceships and planets
    over time, rather than using fixed probabilities. The Model-Based component uses
    these evolving transition probabilities to compute values.

    Parameters:
    - lr_reward: [0, 1] Learning rate for reward values.
    - lr_trans: [0, 1] Learning rate for transition probabilities.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - w: [0, 1] Weight for Model-Based control.
    - stickiness: [0, 5] Perseveration bonus.
    """
    lr_reward, lr_trans, beta_1, beta_2, w, stickiness = model_parameters
    n_trials = len(action_1)
    
    q_mf_1 = np.zeros(2)
    q_mf_2 = np.zeros((2, 2))
    
    # Initialize transition beliefs with 0.7/0.3 priors
    trans_probs = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1
    
    for t in range(n_trials):
        # Stage 1 Policy
        v_stage2 = np.max(q_mf_2, axis=1)
        q_mb_1 = trans_probs @ v_stage2
        
        q_net_1 = w * q_mb_1 + (1 - w) * q_mf_1
        if last_action_1 != -1:
            q_net_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta_1 * q_net_1)
        if np.sum(exp_q1) == 0: exp_q1 = np.ones(2)
        p_choice_1[t] = exp_q1[action_1[t]] / np.sum(exp_q1)
        
        # Stage 2 Policy
        s_idx = state[t]
        exp_q2 = np.exp(beta_2 * q_mf_2[s_idx])
        if np.sum(exp_q2) == 0: exp_q2 = np.ones(2)
        p_choice_2[t] = exp_q2[action_2[t]] / np.sum(exp_q2)
        
        # Updates
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]
        
        # Update Transition Beliefs
        # Observed transition: a1 -> s_idx
        # We update the probability P(Planet 0 | a1) towards 1 (if s_idx==0) or 0 (if s_idx==1)
        is_planet_0 = 1.0 if s_idx == 0 else 0.0
        
        # Linear delta rule for transition probability
        trans_probs[a1, 0] += lr_trans * (is_planet_0 - trans_probs[a1, 0])
        trans_probs[a1, 1] = 1.0 - trans_probs[a1, 0]
        
        # Update Reward Values (MF TD(0))
        delta_1 = q_mf_2[s_idx, a2] - q_mf_1[a1]
        delta_2 = r - q_mf_2[s_idx, a2]
        
        q_mf_1[a1] += lr_reward * delta_1
        q_mf_2[s_idx, a2] += lr_reward * delta_2
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Model-Free TD(lambda) with Decaying Choice Kernel
This model replaces the simple "stickiness" parameter with a "Choice Kernel". While simple stickiness only reinforces the immediately previous action, a choice kernel accumulates a trace of past actions that decays over time. This better captures habit formation and the long streaks of repeated choices seen in the participant's data.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free TD(lambda) with Decaying Choice Kernel.
    
    Replaces the simple 1-step stickiness with a Choice Kernel (habit trace) that
    accumulates and decays over time. This accounts for the strong "streaks" of behavior
    observed in the participant data (habit formation).

    Parameters:
    - learning_rate: [0, 1] Value learning rate.
    - beta_1: [0, 10] Stage 1 inverse temperature.
    - beta_2: [0, 10] Stage 2 inverse temperature.
    - lambda_param: [0, 1] Eligibility trace for value update.
    - ck_decay: [0, 1] Decay rate for the choice kernel (0 = instant forgetting, 1 = no forgetting).
    - ck_weight: [0, 5] Weight of the choice kernel in decision making.
    """
    learning_rate, beta_1, beta_2, lambda_param, ck_decay, ck_weight = model_parameters
    n_trials = len(action_1)
    
    q_mf_1 = np.zeros(2)
    q_mf_2 = np.zeros((2, 2))
    
    # Choice kernel: tracks frequency/recency of choices
    choice_kernel = np.zeros(2)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for t in range(n_trials):
        # Stage 1 Policy
        # Combine Value (Q) and Habit (Choice Kernel)
        logits_1 = beta_1 * q_mf_1 + ck_weight * choice_kernel
        
        exp_q1 = np.exp(logits_1 - np.max(logits_1)) # Numerical stability
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[t] = probs_1[action_1[t]]
        
        # Update Choice Kernel
        # Decay past trace and increment current choice
        choice_kernel *= ck_decay
        choice_kernel[action_1[t]] += 1.0
        
        # Stage 2 Policy
        s_idx = state[t]
        logits_2 = beta_2 * q_mf_2[s_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[t] = probs_2[action_2[t]]
        
        # Updates
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]
        
        delta_1 = q_mf_2[s_idx, a2] - q_mf_1[a1]
        delta_2 = r - q_mf_2[s_idx, a2]
        
        q_mf_1[a1] += learning_rate * (delta_1 + lambda_param * delta_2)
        q_mf_2[s_idx, a2] += learning_rate * delta_2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```