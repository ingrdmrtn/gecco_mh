Here are the three proposed cognitive models.

### Model 1: Hybrid Model with Separate Learning Rates
This model hypothesizes that the participant learns the values of the specific aliens (Stage 2) at a different rate than they update the habitual values of the spaceships (Stage 1). This separation allows the model to capture differences in plasticity between the goal-directed (alien value) and habitual (spaceship preference) systems.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid model with distinct learning rates for Stage 1 (MF) and Stage 2.
    
    Allows the agent to update second-stage values (aliens) and first-stage 
    habitual values (spaceships) at different speeds.
    
    Parameters:
    - lr_1: [0, 1] Learning rate for Stage 1 Model-Free values.
    - lr_2: [0, 1] Learning rate for Stage 2 values.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weight of Model-Based system (0=MF, 1=MB).
    - lam: [0, 1] Eligibility trace parameter.
    """
    lr_1, lr_2, beta, w, lam = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])
        
        if a1 == -1 or s == -1 or a2 == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        state_idx = s

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        # ACTION VALUE UPDATING FOR CHOICE 1
        q_stage1_mf[a1] += lr_1 * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        # ACTION VALUE UPDATING FOR CHOICE 2
        q_stage2_mf[state_idx, a2] += lr_2 * delta_stage2
        
        # Eligibility trace update using lr_1
        q_stage1_mf[a1] += lr_1 * lam * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model with Value Decay
This model introduces a decay parameter for unchosen actions in the second stage. Since the reward probabilities of aliens drift over time, old value estimates become unreliable. This model assumes the participant actively "forgets" or decays the value of aliens they haven't visited recently.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid model with value decay for unchosen Stage 2 actions.
    
    Captures forgetting of value estimates for unvisited aliens, which helps
    adapt to the drifting reward probabilities in the task.
    
    Parameters:
    - learning_rate: [0, 1] Update rate for chosen actions.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weight of Model-Based system.
    - lam: [0, 1] Eligibility trace.
    - decay: [0, 1] Decay rate for unchosen Stage 2 actions (0=no decay).
    """
    learning_rate, beta, w, lam, decay = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])
        
        if a1 == -1 or s == -1 or a2 == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        state_idx = s

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        # ACTION VALUE UPDATING FOR CHOICE 1
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        # ACTION VALUE UPDATING FOR CHOICE 2
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Eligibility trace
        q_stage1_mf[a1] += learning_rate * lam * delta_stage2
        
        # Decay unchosen option in Stage 2
        unchosen_a2 = 1 - a2
        q_stage2_mf[state_idx, unchosen_a2] *= (1 - decay)
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model with Dual Stickiness
This model separates "stickiness" (perseveration) into two components: one for the spaceship choice (Stage 1) and one for the alien choice (Stage 2). This accounts for the possibility that the participant has different tendencies to repeat their previous motor actions or choices depending on the stage of the task.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid model with distinct stickiness parameters for Stage 1 and Stage 2.
    
    Separates the tendency to repeat spaceship choices (Stage 1) from the 
    tendency to repeat alien choices within a planet (Stage 2).
    
    Parameters:
    - learning_rate: [0, 1]
    - beta: [0, 10]
    - w: [0, 1]
    - lam: [0, 1]
    - stick_1: [0, 5] Stickiness bonus for Stage 1 (Spaceship).
    - stick_2: [0, 5] Stickiness bonus for Stage 2 (Alien).
    """
    learning_rate, beta, w, lam, stick_1, stick_2 = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_a1 = -1
    last_a2_per_state = np.array([-1, -1])

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])
        
        if a1 == -1 or s == -1 or a2 == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            last_a1 = -1
            continue

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Apply Stage 1 Stickiness
        if last_a1 != -1:
            q_net_1[last_a1] += stick_1
            
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        state_idx = s

        # policy for the second choice
        q_net_2 = q_stage2_mf[state_idx].copy()
        
        # Apply Stage 2 Stickiness (per state)
        if last_a2_per_state[state_idx] != -1:
            q_net_2[last_a2_per_state[state_idx]] += stick_2
            
        exp_q2 = np.exp(beta * q_net_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        # ACTION VALUE UPDATING FOR CHOICE 1
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        # ACTION VALUE UPDATING FOR CHOICE 2
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Eligibility trace
        q_stage1_mf[a1] += learning_rate * lam * delta_stage2
        
        # Update history
        last_a1 = a1
        last_a2_per_state[state_idx] = a2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```