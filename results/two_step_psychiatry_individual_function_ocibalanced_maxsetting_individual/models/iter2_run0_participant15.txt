Here are three new cognitive models for the two-step decision task, designed to capture the participant's behavior, particularly the strong perseveration (streaks) and the specific learning dynamics.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Model-Based RL with Reward-Sensitive Stickiness.
    
    This model assumes the participant uses a Model-Based strategy to evaluate 
    spaceships based on alien values, but their "stickiness" (tendency to repeat 
    choices) depends on the previous outcome. This distinguishes between 
    "Win-Stay" (perseveration after reward) and "Lose-Stay/Shift" dynamics, 
    which may explain the long streaks better than a single stickiness parameter.
    
    Parameters:
    - alpha: [0, 1] Learning rate for Stage 2 alien values.
    - beta_mb: [0, 10] Inverse temperature for Model-Based Stage 1 values.
    - beta_2: [0, 10] Inverse temperature for Stage 2 choices.
    - stick_win: [0, 10] Stickiness bonus applied to the previous choice if it was rewarded.
    - stick_loss: [0, 10] Stickiness bonus applied to the previous choice if it was not rewarded.
    """
    alpha, beta_mb, beta_2, stick_win, stick_loss = model_parameters
    n_trials = len(action_1)
  
    # Transition matrix: A(0)->X(0) 0.7, U(1)->Y(1) 0.7
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values for Stage 2 (2 planets x 2 aliens)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_choice = -1
    prev_reward = -1

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue

        # --- Stage 1 Policy ---
        # Model-Based Value Calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Calculate utilities
        utilities = beta_mb * q_stage1_mb
        
        # Add Reward-Sensitive Stickiness
        if prev_choice != -1:
            if prev_reward == 1:
                utilities[prev_choice] += stick_win
            else:
                utilities[prev_choice] += stick_loss
        
        # Softmax for Stage 1
        exp_u1 = np.exp(utilities)
        probs_1 = exp_u1 / np.sum(exp_u1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Policy ---
        state_idx = state[trial]
        
        # Softmax for Stage 2
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        # Update Stage 2 Q-values (Standard Q-Learning)
        curr_reward = reward[trial]
        delta_stage2 = curr_reward - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += alpha * delta_stage2
        
        # Track history
        prev_choice = action_1[trial]
        prev_reward = curr_reward

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    TD(lambda) Model-Free Reinforcement Learning.
    
    This model assumes the participant is purely Model-Free but uses an eligibility 
    trace (lambda) to connect the Stage 2 outcome directly to the Stage 1 choice.
    This allows for learning the value of spaceships based on final rewards without 
    explicitly using the transition matrix, offering a simpler explanation for 
    behavior than Model-Based control.
    
    Parameters:
    - alpha: [0, 1] Learning rate for both stages.
    - lambda_param: [0, 1] Eligibility trace parameter (0 = TD(0), 1 = Monte Carlo).
    - beta_1: [0, 10] Inverse temperature for Stage 1 choices.
    - beta_2: [0, 10] Inverse temperature for Stage 2 choices.
    - stickiness: [0, 10] General stickiness to the previous Stage 1 choice.
    """
    alpha, lambda_param, beta_1, beta_2, stickiness = model_parameters
    n_trials = len(action_1)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1 = np.zeros(2)      # Values for Spaceships
    q_stage2 = np.zeros((2, 2)) # Values for Aliens
    
    prev_choice = -1

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue

        # --- Stage 1 Policy ---
        utilities = beta_1 * q_stage1.copy()
        
        if prev_choice != -1:
            utilities[prev_choice] += stickiness
            
        exp_u1 = np.exp(utilities)
        probs_1 = exp_u1 / np.sum(exp_u1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Policy ---
        state_idx = state[trial]
        exp_q2 = np.exp(beta_2 * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates (TD Lambda) ---
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]
        
        # Prediction error at Stage 1 (SARSA-style for consistency with path)
        # The value of the state arrived at is Q_stage2[state, a2]
        delta1 = q_stage2[state_idx, a2] - q_stage1[a1]
        
        # Prediction error at Stage 2
        delta2 = r - q_stage2[state_idx, a2]
        
        # Update Stage 2
        q_stage2[state_idx, a2] += alpha * delta2
        
        # Update Stage 1
        # 1. Update from Stage 1 transition (TD(0) part)
        q_stage1[a1] += alpha * delta1
        # 2. Update from Stage 2 outcome via eligibility trace (lambda part)
        q_stage1[a1] += alpha * lambda_param * delta2
        
        prev_choice = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Based RL with Value Decay (Forgetting).
    
    This model incorporates a forgetting mechanism where Stage 2 alien values 
    decay towards a neutral prior (0.5) on every trial. This captures the 
    notion that the participant's knowledge of the environment is volatile, 
    which might drive them to switch spaceships (re-explore) after long streaks 
    when the current option's value degrades or alternative options reset.
    
    Parameters:
    - alpha: [0, 1] Learning rate for Stage 2 updates.
    - decay: [0, 1] Decay rate of all Q-values towards 0.5 per trial.
    - beta_mb: [0, 10] Inverse temperature for Model-Based Stage 1 values.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - stickiness: [0, 10] Stickiness to previous Stage 1 choice.
    """
    alpha, decay, beta_mb, beta_2, stickiness = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Initialize Q-values at 0.5 (neutral)
    q_stage2_mf = np.ones((2, 2)) * 0.5
    
    prev_choice = -1

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue

        # --- Decay Step ---
        # All values decay toward 0.5 before decision/update
        q_stage2_mf = q_stage2_mf * (1 - decay) + 0.5 * decay

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        utilities = beta_mb * q_stage1_mb
        if prev_choice != -1:
            utilities[prev_choice] += stickiness
            
        exp_u1 = np.exp(utilities)
        probs_1 = exp_u1 / np.sum(exp_u1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Policy ---
        state_idx = state[trial]
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Update ---
        delta = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += alpha * delta
        
        prev_choice = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```