Here are three new cognitive models for the two-step task, designed to capture different potential mechanisms underlying the participant's behavior.

### Model 1: Hybrid Learner with Dual Learning Rates
This model hypothesizes that the participant learns at different speeds for the two stages of the task. For instance, they might update their preferences for spaceships (Stage 1, navigation) slowly while rapidly updating their estimates of alien generosity (Stage 2, harvesting).

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid learner with separate learning rates for Stage 1 (Spaceships) and Stage 2 (Aliens).
    This allows the model to capture different volatilities or learning speeds for 
    navigation choices versus harvesting choices.

    Parameters:
    - lr_s1: [0, 1] Learning rate for Stage 1 Model-Free values (Spaceships).
    - lr_s2: [0, 1] Learning rate for Stage 2 values (Aliens).
    - beta: [0, 10] Inverse temperature for softmax choice (exploration/exploitation).
    - w: [0, 1] Weighting parameter (0 = Pure Model-Free, 1 = Pure Model-Based).
    """
    lr_s1, lr_s2, beta, w = model_parameters
    n_trials = len(action_1)

    # Transition matrix: Space A(0)->Planet X(0) is 0.7, etc.
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_mf = np.zeros(2)          # Stage 1 Model-Free values
    q_stage2 = np.zeros((2, 2)) # Stage 2 values (Rows: Planet, Cols: Alien)
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based Value calculation
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_mb = transition_matrix @ max_q_stage2

        # Integrated Value
        q_net = w * q_mb + (1 - w) * q_mf

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Policy ---
        s_idx = state[trial]
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]

        # Stage 1 Prediction Error (TD(0)-like part)
        # Difference between expected value of state arrived at and value of action taken
        delta1 = q_stage2[s_idx, a2] - q_mf[a1]
        q_mf[a1] += lr_s1 * delta1

        # Stage 2 Prediction Error
        delta2 = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += lr_s2 * delta2
        
        # Eligibility Trace: Propagate Stage 2 reward error back to Stage 1
        # We use lr_s1 here as it updates the Stage 1 value
        q_mf[a1] += lr_s1 * delta2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Learner with Value Decay
Since the reward probabilities of the aliens change slowly over time, older information becomes less reliable. This model incorporates a `decay` parameter. On each trial, the values of unchosen aliens decay slightly towards zero, simulating forgetting or an assumption of environmental volatility.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid learner with decay (forgetting) applied to Stage 2 values.
    Unchosen aliens' values decay on every trial, helping the agent adapt 
    to changing reward probabilities by reducing the weight of old experiences.
    
    Parameters:
    - learning_rate: [0, 1] Update rate for chosen values.
    - beta: [0, 10] Inverse temperature for softmax choice.
    - w: [0, 1] Weighting parameter (0 = Pure MF, 1 = Pure MB).
    - decay: [0, 1] Decay rate for unchosen Stage 2 values (0 = no decay, 1 = instant forgetting).
    """
    learning_rate, beta, w, decay = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_mf = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_mb = transition_matrix @ max_q_stage2
        q_net = w * q_mb + (1 - w) * q_mf

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Policy ---
        s_idx = state[trial]
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]

        # Apply decay to unchosen options in Stage 2
        # We iterate over all planet/alien combinations
        for s in range(2):
            for a in range(2):
                # If this option was NOT the one just chosen/observed
                if not (s == s_idx and a == a2):
                    q_stage2[s, a] *= (1 - decay)

        # Standard Updates for Chosen Options
        delta1 = q_stage2[s_idx, a2] - q_mf[a1]
        q_mf[a1] += learning_rate * delta1

        delta2 = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += learning_rate * delta2
        
        q_mf[a1] += learning_rate * delta2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Learner with Stage 2 Stickiness
While "perseverance" is often modeled for the first choice (spaceship), participants may also develop habits regarding which alien to ask on a specific planet (e.g., always asking 'W' when on Planet X). This model adds a "stickiness" bonus to the previously chosen alien within the current context (planet), independent of the reward outcome.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid learner with Stickiness (Perseverance) specifically on Stage 2 choices.
    The agent receives a 'bonus' for repeating the alien choice made previously 
    in the same planet, capturing context-specific habit formation.
    
    Parameters:
    - learning_rate: [0, 1] Update rate for values.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weighting parameter (0 = Pure MF, 1 = Pure MB).
    - stick_s2: [0, 5] Perseverance bonus added to the previously chosen alien on the current planet.
    """
    learning_rate, beta, w, stick_s2 = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_mf = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    # Track last choice made for each planet (0 or 1). Initialize with -1 (none).
    last_action_2 = np.array([-1, -1]) 
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_mb = transition_matrix @ max_q_stage2
        q_net = w * q_mb + (1 - w) * q_mf

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Policy ---
        s_idx = state[trial]
        
        # Calculate effective values including stickiness bonus
        q_s2_effective = q_stage2[s_idx].copy()
        if last_action_2[s_idx] != -1:
            q_s2_effective[last_action_2[s_idx]] += stick_s2
            
        exp_q2 = np.exp(beta * q_s2_effective)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]
        
        # Update history for this state
        last_action_2[s_idx] = a2

        delta1 = q_stage2[s_idx, a2] - q_mf[a1]
        q_mf[a1] += learning_rate * delta1

        delta2 = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += learning_rate * delta2
        
        q_mf[a1] += learning_rate * delta2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```