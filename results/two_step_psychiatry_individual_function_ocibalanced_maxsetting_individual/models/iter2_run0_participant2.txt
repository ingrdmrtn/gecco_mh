Here are three new cognitive models for the two-step task, designed based on the participant's behavior and distinct from the previously tried parameter combinations.

### Model 1: Independent Stage Learning Rates (MF + Stickiness)
This model assumes the participant updates their valuation of the spaceships (Stage 1) and aliens (Stage 2) at different rates. Given the participant's "blocky" choice behavior, they may have a slow-changing preference for spaceships (low learning rate) while rapidly tracking the noisy alien rewards (high learning rate).

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Independent Stage Learning Rates Model (MF + Stickiness).
    
    Distinguishes between the learning rate for the distal choice (Spaceship, Stage 1)
    and the proximal choice (Alien, Stage 2). This allows the model to capture 
    different timescales of learning for the two stages (e.g., slow habit formation 
    vs fast bandit learning).
    
    Parameters:
    lr_s1: [0, 1] Learning rate for Stage 1 (Spaceships).
    lr_s2: [0, 1] Learning rate for Stage 2 (Aliens).
    beta: [0, 10] Inverse temperature.
    stickiness: [0, 5] Choice perseverance for Stage 1.
    """
    lr_s1, lr_s2, beta, stickiness = model_parameters
    n_trials = len(action_1)
    
    q_stage1 = np.zeros(2) # Spaceship values
    q_stage2 = np.zeros((2, 2)) # Alien values: [Planet, Alien]
    
    log_likelihood = 0.0
    eps = 1e-10
    
    last_action_1 = -1 

    for trial in range(n_trials):
        a1 = action_1[trial]
        s2 = state[trial]
        a2 = action_2[trial]
        r = reward[trial]

        # --- Stage 1 Decision ---
        q_eff_1 = q_stage1.copy()
        if last_action_1 != -1:
            q_eff_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_eff_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_likelihood += np.log(probs_1[a1] + eps)
        
        last_action_1 = a1

        # Handle missing second stage
        if a2 == -1:
            continue

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_likelihood += np.log(probs_2[a2] + eps)

        # --- Updates ---
        # Stage 2: Standard Q-learning
        q_stage2[s2, a2] += lr_s2 * (r - q_stage2[s2, a2])
        
        # Stage 1: Direct Reinforcement (MF)
        q_stage1[a1] += lr_s1 * (r - q_stage1[a1])

    return -log_likelihood
```

### Model 2: Q-Learning with Forgetting (Decay)
This model posits that the participant's memory of unvisited options (both spaceships and aliens) decays over time. This "forgetting" mechanism causes values to revert to zero, which can explain switching behavior when the current option's value degrades, even if the alternative hasn't been sampled recently.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Q-Learning with Forgetting (Decay).
    
    Incorporates a decay mechanism where unchosen options (both spaceships and aliens)
    slowly revert to 0. This models the participant's potential to forget the value 
    of options that haven't been visited recently.
    
    Parameters:
    learning_rate: [0, 1] Update rate for chosen options.
    decay_rate: [0, 1] Decay rate for unchosen options (Q = Q * (1-decay)).
    beta: [0, 10] Inverse temperature.
    stickiness: [0, 5] Choice perseverance for Stage 1.
    """
    learning_rate, decay_rate, beta, stickiness = model_parameters
    n_trials = len(action_1)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    log_likelihood = 0.0
    eps = 1e-10
    
    last_action_1 = -1 

    for trial in range(n_trials):
        a1 = action_1[trial]
        s2 = state[trial]
        a2 = action_2[trial]
        r = reward[trial]

        # --- Stage 1 Decision ---
        q_eff_1 = q_stage1.copy()
        if last_action_1 != -1:
            q_eff_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_eff_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_likelihood += np.log(probs_1[a1] + eps)
        
        last_action_1 = a1

        if a2 == -1:
            continue

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_likelihood += np.log(probs_2[a2] + eps)

        # --- Updates with Forgetting ---
        # Stage 2: Update chosen
        q_stage2[s2, a2] += learning_rate * (r - q_stage2[s2, a2])
        
        # Decay unchosen alien in current planet
        q_stage2[s2, 1-a2] *= (1.0 - decay_rate)
        # Decay aliens in the other planet (time passes)
        q_stage2[1-s2, :] *= (1.0 - decay_rate)

        # Stage 1: Update chosen
        q_stage1[a1] += learning_rate * (r - q_stage1[a1])
        # Decay unchosen spaceship
        q_stage1[1-a1] *= (1.0 - decay_rate)

    return -log_likelihood
```

### Model 3: Outcome-Dependent Stickiness (Win-Stay, Lose-Shift Hybrid)
This model refines the stickiness parameter by splitting it based on the previous trial's outcome. It tests the hypothesis that the participant's tendency to repeat a choice is stronger after a reward ("Win-Stay") and weaker (or different) after a non-reward ("Lose-Shift"), providing a more granular explanation for the observed perseverance blocks.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Outcome-Dependent Stickiness (Win-Stay, Lose-Shift Hybrid).
    
    Differentiates choice perseverance based on the outcome of the previous trial.
    A 'win' (reward > 0) induces a different level of stickiness (usually higher) 
    than a 'loss' (reward <= 0).
    
    Parameters:
    learning_rate: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    stick_win: [0, 5] Stickiness bonus after a rewarded trial.
    stick_loss: [0, 5] Stickiness bonus after an unrewarded trial.
    """
    learning_rate, beta, stick_win, stick_loss = model_parameters
    n_trials = len(action_1)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    log_likelihood = 0.0
    eps = 1e-10
    
    last_action_1 = -1
    last_reward = 0 # Placeholder, effect applies only after first trial

    for trial in range(n_trials):
        a1 = action_1[trial]
        s2 = state[trial]
        a2 = action_2[trial]
        r = reward[trial]

        # --- Stage 1 Decision ---
        q_eff_1 = q_stage1.copy()
        
        if last_action_1 != -1:
            if last_reward > 0:
                q_eff_1[last_action_1] += stick_win
            else:
                q_eff_1[last_action_1] += stick_loss
            
        exp_q1 = np.exp(beta * q_eff_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_likelihood += np.log(probs_1[a1] + eps)
        
        last_action_1 = a1
        last_reward = r 

        if a2 == -1:
            continue

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_likelihood += np.log(probs_2[a2] + eps)

        # --- Updates ---
        q_stage2[s2, a2] += learning_rate * (r - q_stage2[s2, a2])
        q_stage1[a1] += learning_rate * (r - q_stage1[a1])

    return -log_likelihood
```