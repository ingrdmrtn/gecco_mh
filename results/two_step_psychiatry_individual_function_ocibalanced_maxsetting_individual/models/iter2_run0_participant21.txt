Here are the three proposed cognitive models.

### Cognitive Model 1: Hybrid Model with Choice Trace (Perseveration)
This model extends the standard hybrid agent by replacing simple one-step stickiness with an exponentially decaying **Choice Trace** (also known as a perseveration kernel). This allows the model to account for the participant's tendency to persist with a specific spaceship choice over long blocks of trials (inertia), accumulating a bias that requires significant counter-evidence to overcome.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Choice Trace (Perseveration).

    This model extends the standard hybrid MB/MF agent by replacing simple 
    one-step stickiness with an exponentially decaying choice trace (perseveration kernel).
    This allows the model to capture the participant's tendency to persist with 
    choices over long blocks (inertia), accumulating bias for frequently chosen options.

    Parameters:
    learning_rate: [0, 1] - Rate at which Q-values are updated via TD error.
    beta_1: [0, 10] - Inverse temperature for Stage 1 softmax.
    beta_2: [0, 10] - Inverse temperature for Stage 2 softmax.
    w: [0, 1] - Weighting parameter (0 = Pure MF, 1 = Pure MB).
    trace_decay: [0, 1] - Decay rate of the choice trace (0 = only last choice, 1 = no decay).
    trace_str: [0, 10] - Weight of the choice trace bias added to Stage 1 Q-values.
    """
    learning_rate, beta_1, beta_2, w, trace_decay, trace_str = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # 2 states, 2 actions
    
    # Choice trace for Stage 1 actions
    choice_trace = np.zeros(2)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Value: Weighted Average + Choice Trace
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf + trace_str * choice_trace
        
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Policy ---
        state_idx = state[trial]
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta_2 * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        # Update Choice Trace (Exponential Moving Average)
        choice_trace *= trace_decay
        choice_trace[action_1[trial]] += (1.0 - trace_decay)
        
        # TD Updates
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2: Hybrid Model with Dual-Stage Stickiness
The participant data shows repetitive behavior not just in spaceship choice but also in alien choice (Stage 2). This model introduces a second stickiness parameter specifically for Stage 2, allowing the agent to exhibit perseverance in alien selection within each planet, independent of the Stage 1 strategy.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Dual-Stage Stickiness.

    This model assumes the participant exhibits perseverance (stickiness) in both 
    Stage 1 (Spaceship choice) and Stage 2 (Alien choice). Stage 2 stickiness 
    is tracked separately for each planet (state).

    Parameters:
    learning_rate: [0, 1] - Rate at which Q-values are updated.
    beta_1: [0, 10] - Inverse temperature for Stage 1.
    beta_2: [0, 10] - Inverse temperature for Stage 2.
    w: [0, 1] - Weighting parameter (0 = Pure MF, 1 = Pure MB).
    stickiness_1: [0, 5] - Stickiness bonus for repeating the previous Stage 1 choice.
    stickiness_2: [0, 5] - Stickiness bonus for repeating the previous Stage 2 choice (in that state).
    """
    learning_rate, beta_1, beta_2, w, stickiness_1, stickiness_2 = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    last_action_2 = np.array([-1, -1]) # Stores last action for state 0 and state 1

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness_1
            
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Policy ---
        state_idx = state[trial]
        qs_current_state = q_stage2_mf[state_idx].copy()
        
        if last_action_2[state_idx] != -1:
            qs_current_state[last_action_2[state_idx]] += stickiness_2
            
        exp_q2 = np.exp(beta_2 * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        last_action_1 = action_1[trial]
        last_action_2[state_idx] = action_2[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3: Hybrid Model with Outcome-Dependent Stickiness
This model differentiates between "Win-Stay" and "Lose-Stay" tendencies by applying different stickiness bonuses depending on whether the previous trial was rewarded. This allows the model to capture if the participant is more prone to repetition after a success (reinforcement-like heuristic) versus general inertia after a failure.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Outcome-Dependent Stickiness.

    This model applies different stickiness bonuses depending on whether the 
    previous trial resulted in a reward (Win) or not (Loss). This allows for 
    asymmetric perseverance (e.g., Win-Stay vs. Lose-Stay).

    Parameters:
    learning_rate: [0, 1] - Rate at which Q-values are updated.
    beta_1: [0, 10] - Inverse temperature for Stage 1.
    beta_2: [0, 10] - Inverse temperature for Stage 2.
    w: [0, 1] - Weighting parameter (0 = Pure MF, 1 = Pure MB).
    stick_win: [0, 5] - Stickiness bonus if the previous trial was rewarded.
    stick_loss: [0, 5] - Stickiness bonus if the previous trial was unrewarded.
    """
    learning_rate, beta_1, beta_2, w, stick_win, stick_loss = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    last_reward = 0

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_action_1 != -1:
            if last_reward == 1:
                q_net[last_action_1] += stick_win
            else:
                q_net[last_action_1] += stick_loss
            
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Policy ---
        state_idx = state[trial]
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta_2 * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        last_action_1 = action_1[trial]
        last_reward = reward[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```