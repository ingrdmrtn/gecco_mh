Here are three new cognitive models for the two-step task, designed to capture the specific behaviors observed in the participant data (e.g., strong perseveration, potential asymmetry in learning).

### Model 1: Hybrid Learner with Decaying Habit
This model extends the standard Hybrid (Model-Based + Model-Free) approach by replacing simple 1-step stickiness with a "Habit" mechanism. The habit strength accumulates when an action is chosen and decays over time. This accounts for the participant's tendency to stick with a spaceship for long blocks of trials (inertia).

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model (MB+MF) with Decaying Habit.
    
    Uses a weighted mixture of Model-Based and Model-Free values for Stage 1.
    Instead of simple stickiness, it maintains a habit strength variable that
    decays over time and increments upon choice, capturing longer-term perseveration.
    
    Parameters:
    learning_rate: [0,1] Rate of updating Q-values.
    beta:          [0,10] Inverse temperature.
    w:             [0,1] Weight for MB (1=Pure MB, 0=Pure MF).
    habit_decay:   [0,1] Rate at which habit strength decays (0=instant, 1=no decay).
    habit_amp:     [0,5] Strength of the habit influence on choice.
    """
    learning_rate, beta, w, habit_decay, habit_amp = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2) + 0.5
    q_stage2_mf = np.zeros((2, 2)) + 0.5
    habit = np.zeros(2)

    eps = 1e-10

    for trial in range(n_trials):
        a1 = action_1[trial]
        s_idx = state[trial]
        a2 = action_2[trial]
        r = reward[trial]

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combine MB, MF, and Habit
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf + habit_amp * habit
        
        exp_q1 = np.exp(beta * q_net - np.max(beta * q_net)) # subtract max for numerical stability
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        # Update habit (decay always, increment if choice made)
        habit *= habit_decay
        
        if a1 != -1:
            p_choice_1[trial] = probs_1[a1]
            habit[a1] += 1
        else:
            p_choice_1[trial] = 1.0 # Ignore missing data in likelihood
            
        # policy for the second choice
        if s_idx != -1:
            exp_q2 = np.exp(beta * q_stage2_mf[s_idx] - np.max(beta * q_stage2_mf[s_idx]))
            probs_2 = exp_q2 / np.sum(exp_q2)
            if a2 != -1:
                p_choice_2[trial] = probs_2[a2]
            else:
                p_choice_2[trial] = 1.0
        else:
            p_choice_2[trial] = 1.0
        
        # Skip updates if data is missing
        if a1 == -1 or s_idx == -1 or a2 == -1:
            continue
            
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Asymmetric TD($\lambda$) Learner
This model is a Model-Free learner that uses eligibility traces (TD($\lambda$)) to credit Stage 2 outcomes to Stage 1 choices. Crucially, it uses **asymmetric learning rates** for positive and negative prediction errors. This tests the hypothesis that the participant updates their beliefs differently after receiving gold (gain) versus no gold (loss/omission).

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Asymmetric TD(Lambda) Learner.
    
    A Model-Free learner that updates values using eligibility traces (lambda),
    but uses different learning rates for positive and negative prediction errors.
    This captures potential risk-sensitivity alongside direct reinforcement.
    
    Parameters:
    lr_pos:       [0,1] Learning rate for positive prediction errors.
    lr_neg:       [0,1] Learning rate for negative prediction errors.
    beta:         [0,10] Inverse temperature.
    lambda_param: [0,1] Eligibility trace decay (0=TD(0), 1=Monte Carlo).
    stickiness:   [0,5] Choice stickiness bonus for Stage 1.
    """
    lr_pos, lr_neg, beta, lambda_param, stickiness = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]]) # Unused in pure MF but part of template
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2) + 0.5
    q_stage2_mf = np.zeros((2, 2)) + 0.5
    
    eps = 1e-10
    last_action_1 = -1

    for trial in range(n_trials):
        a1 = action_1[trial]
        s_idx = state[trial]
        a2 = action_2[trial]
        r = reward[trial]

        # policy for the first choice
        # Template calculates MB, we ignore it for this MF model
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        logits_1 = beta * q_stage1_mf
        if last_action_1 != -1:
            logits_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        if a1 != -1:
            p_choice_1[trial] = probs_1[a1]
            last_action_1 = a1
        else:
            p_choice_1[trial] = 1.0
            last_action_1 = -1

        # policy for the second choice
        if s_idx != -1:
            exp_q2 = np.exp(beta * q_stage2_mf[s_idx] - np.max(beta * q_stage2_mf[s_idx]))
            probs_2 = exp_q2 / np.sum(exp_q2)
            if a2 != -1:
                p_choice_2[trial] = probs_2[a2]
            else:
                p_choice_2[trial] = 1.0
        else:
            p_choice_2[trial] = 1.0
  
        # Skip updates if data is missing
        if a1 == -1 or s_idx == -1 or a2 == -1:
            continue
            
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        alpha_1 = lr_pos if delta_stage1 > 0 else lr_neg
        q_stage1_mf[a1] += alpha_1 * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        alpha_2 = lr_pos if delta_stage2 > 0 else lr_neg
        q_stage2_mf[s_idx, a2] += alpha_2 * delta_stage2
        
        # Eligibility trace update for Stage 1 based on Stage 2 outcome
        # Uses the alpha associated with the Stage 2 error
        q_stage1_mf[a1] += alpha_2 * lambda_param * delta_stage2
        

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Dual-Learning Rate Hybrid Learner
This model posits that the participant learns at different speeds for different stages. It combines Model-Based planning with Model-Free learning but assigns a separate learning rate to Stage 1 (spaceships) and Stage 2 (aliens). This helps explain scenarios where the participant might adapt quickly to changing alien rewards but is slow to switch spaceship preferences (or vice versa).

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Separate Stage Learning Rates.
    
    Combines Model-Based planning with Model-Free learning, but allows for
    different learning rates for Stage 1 (spaceships) and Stage 2 (aliens).
    This accounts for different plasticity in high-level vs low-level choices.
    
    Parameters:
    lr_stage1:  [0,1] Learning rate for Stage 1 Q-values.
    lr_stage2:  [0,1] Learning rate for Stage 2 Q-values.
    beta:       [0,10] Inverse temperature.
    w:          [0,1] Weight for MB (1=Pure MB, 0=Pure MF).
    stickiness: [0,5] Choice stickiness bonus for Stage 1.
    """
    lr_stage1, lr_stage2, beta, w, stickiness = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2) + 0.5
    q_stage2_mf = np.zeros((2, 2)) + 0.5
    
    eps = 1e-10
    last_action_1 = -1

    for trial in range(n_trials):
        a1 = action_1[trial]
        s_idx = state[trial]
        a2 = action_2[trial]
        r = reward[trial]

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net
        if last_action_1 != -1:
            logits_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        if a1 != -1:
            p_choice_1[trial] = probs_1[a1]
            last_action_1 = a1
        else:
            p_choice_1[trial] = 1.0
            last_action_1 = -1
            
        # policy for the second choice
        if s_idx != -1:
            exp_q2 = np.exp(beta * q_stage2_mf[s_idx] - np.max(beta * q_stage2_mf[s_idx]))
            probs_2 = exp_q2 / np.sum(exp_q2)
            if a2 != -1:
                p_choice_2[trial] = probs_2[a2]
            else:
                p_choice_2[trial] = 1.0
        else:
            p_choice_2[trial] = 1.0
  
        # Skip updates if data is missing
        if a1 == -1 or s_idx == -1 or a2 == -1:
            continue
            
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr_stage1 * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr_stage2 * delta_stage2
        

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```