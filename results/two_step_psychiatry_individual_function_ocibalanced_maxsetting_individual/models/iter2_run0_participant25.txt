Here are three new cognitive models for the two-step decision task, designed to capture different potential mechanisms underlying the participant's behavior, such as asymmetric learning, habit formation (choice kernel), and memory decay (forgetting).

### Model 1: Hybrid Asymmetric Learning
This model combines Model-Based (planning) and Model-Free (trial-and-error) systems. Crucially, it incorporates **asymmetric learning rates** for positive and negative prediction errors. This allows the model to weigh "better than expected" and "worse than expected" outcomes differently, capturing potential optimism or pessimism biases while maintaining the structural knowledge of the task.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based/Model-Free with Asymmetric Learning Rates.
    
    Combines a weighted mix of Model-Based (MB) and Model-Free (MF) values.
    Uses separate learning rates for positive and negative prediction errors
    to capture valence-dependent learning biases (e.g., optimism/pessimism).

    Parameters:
    - lr_pos: [0, 1] Learning rate for positive prediction errors (PE > 0).
    - lr_neg: [0, 1] Learning rate for negative prediction errors (PE < 0).
    - beta:   [0, 10] Inverse temperature (softmax sensitivity).
    - w:      [0, 1] Weight of Model-Based system (1 = Pure MB, 0 = Pure MF).
    """
    lr_pos, lr_neg, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Initialize values at 0.5 (neutral expectation for 0/1 rewards)
    q_stage1_mf = np.zeros(2) + 0.5
    q_stage2_mf = np.zeros((2, 2)) + 0.5

    for trial in range(n_trials):
        
        # --- Stage 1 Decision ---
        # Model-Based Value: Expected value of next stage's best option
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        r = reward[trial]

        # --- Updates with Asymmetric Learning Rates ---
        
        # Stage 1 Update (TD-error using Stage 2 value)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        lr_1 = lr_pos if delta_stage1 > 0 else lr_neg
        q_stage1_mf[a1] += lr_1 * delta_stage1
        
        # Stage 2 Update (Prediction error using Reward)
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        lr_2 = lr_pos if delta_stage2 > 0 else lr_neg
        q_stage2_mf[state_idx, a2] += lr_2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid with Choice Kernel
This model adds a **Choice Kernel** to the standard Hybrid framework. Unlike simple "stickiness" (which repeats the very last action), a choice kernel integrates the history of past choices to form a "habit" strength that decays slowly over time. This captures the participant's tendency to perseverate on specific spaceships (e.g., Spaceship 0) independently of reward feedback.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Choice Kernel (Habit).
    
    Augments the Hybrid MB/MF model with a Choice Kernel that tracks 
    choice history. This captures perseveration/habit formation 
    (tendency to repeat choices) that decays over time.

    Parameters:
    - lr:       [0, 1] Learning rate for Q-values.
    - beta:     [0, 10] Inverse temperature.
    - w:        [0, 1] Weight of Model-Based system.
    - k_decay:  [0, 1] Decay rate of the choice kernel (1 = instant forgetting).
    - k_weight: [0, 10] Weight of the choice kernel in decision making.
    """
    lr, beta, w, k_decay, k_weight = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2) + 0.5
    q_stage2_mf = np.zeros((2, 2)) + 0.5
    
    # Choice Kernel for Stage 1 choices
    choice_kernel = np.zeros(2)

    for trial in range(n_trials):

        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combine MB, MF, and Choice Kernel
        # The kernel adds a bias based on choice history
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        logits = beta * q_net + k_weight * choice_kernel
        
        # Softmax with kernel
        exp_q1 = np.exp(logits - np.max(logits)) # sub max for stability
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        r = reward[trial]
        
        # --- Updates ---
        
        # Update Choice Kernel (Decaying trace of past choices)
        # CK(t+1) = (1-lambda)*CK(t) + lambda*Indicator(a)
        choice_kernel *= (1 - k_decay)
        choice_kernel[a1] += k_decay * 1.0

        # Standard Q-learning updates
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += lr * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid with Value Forgetting
This model addresses the slowly drifting reward probabilities of the aliens. It assumes that the agent "forgets" the value of unchosen aliens, causing their value estimates to decay back towards a neutral prior (0.5). This helps the agent remain flexible in a changing environment by not holding onto old reward information forever.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Passive Forgetting.
    
    Includes a forgetting mechanism for unchosen options in Stage 2.
    Unchosen alien values decay towards a neutral prior (0.5), helping
    adapt to the drifting reward probabilities in the task.

    Parameters:
    - lr:          [0, 1] Learning rate for chosen options.
    - beta:        [0, 10] Inverse temperature.
    - w:           [0, 1] Weight of Model-Based system.
    - forget_rate: [0, 1] Decay rate for unchosen options (towards 0.5).
    """
    lr, beta, w, forget_rate = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2) + 0.5
    q_stage2_mf = np.zeros((2, 2)) + 0.5

    for trial in range(n_trials):

        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        r = reward[trial]

        # --- Updates ---
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        # Stage 2 Update (Chosen)
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += lr * delta_stage2
        
        # Stage 2 Update (Unchosen) - Forgetting
        # Decays the unchosen alien's value towards 0.5
        unchosen_a2 = 1 - a2
        q_stage2_mf[state_idx, unchosen_a2] = (1 - forget_rate) * q_stage2_mf[state_idx, unchosen_a2] + forget_rate * 0.5
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```