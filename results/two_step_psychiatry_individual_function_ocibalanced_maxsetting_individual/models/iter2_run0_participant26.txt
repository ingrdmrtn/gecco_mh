Here are three new cognitive models for the two-step task, formulated as Python functions.

### Model 1: Model-Free Learner with Asymmetric Learning Rates
This model hypothesizes that the participant learns differently from positive outcomes (rewards) versus negative outcomes (omissions). It is a pure Model-Free learner (TD(0)) that does not use the transition matrix. Instead, it updates values based on prediction errors, using separate learning rates (`lr_pos`, `lr_neg`) depending on the sign of the error. This asymmetry applies to both the spaceship choice (Stage 1) and the alien choice (Stage 2).

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Learner with Asymmetric Learning Rates.
    
    This model assumes a Model-Free strategy (TD(0)) where prediction errors 
    are weighted differently depending on whether they are positive (better than expected) 
    or negative (worse than expected). This applies to both Stage 1 and Stage 2 updates.
    
    Parameters:
    - lr_pos: [0, 1] Learning rate for positive prediction errors.
    - lr_neg: [0, 1] Learning rate for negative prediction errors.
    - beta_1: [0, 10] Inverse temperature for Stage 1 choices.
    - beta_2: [0, 10] Inverse temperature for Stage 2 choices.
    """
    lr_pos, lr_neg, beta_1, beta_2 = model_parameters
    n_trials = len(action_1)
    
    # Initialize Q-values
    # Stage 1: 2 choices (Spaceships)
    q_stage1 = np.zeros(2) 
    # Stage 2: 2 states (Planets) x 2 choices (Aliens)
    q_stage2 = np.zeros((2, 2))
    
    log_loss = 0.0
    eps = 1e-10
    
    for trial in range(n_trials):
        # Handle missing data
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Stage 1 Choice (Softmax)
        exp_q1 = np.exp(beta_1 * q_stage1)
        prob_a1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(prob_a1[a1] + eps)
        
        # Stage 2 Choice (Softmax)
        exp_q2 = np.exp(beta_2 * q_stage2[s_idx])
        prob_a2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(prob_a2[a2] + eps)
        
        # Update Stage 2
        # RPE = Reward - Q_stage2
        delta_2 = r - q_stage2[s_idx, a2]
        eff_lr_2 = lr_pos if delta_2 > 0 else lr_neg
        q_stage2[s_idx, a2] += eff_lr_2 * delta_2
        
        # Update Stage 1
        # TD(0): Update towards value of state reached (Q_stage2 of chosen action)
        target_val = q_stage2[s_idx, a2]
        delta_1 = target_val - q_stage1[a1]
        eff_lr_1 = lr_pos if delta_1 > 0 else lr_neg
        q_stage1[a1] += eff_lr_1 * delta_1
        
    return log_loss
```

### Model 2: Hybrid MB/MF with Separate Stage Learning Rates
This model combines Model-Based (planning) and Model-Free (habitual) strategies. It introduces specific flexibility by assigning different learning rates to Stage 1 (`lr_stage1`) and Stage 2 (`lr_stage2`). This accounts for the possibility that the participant updates their high-level preferences (spaceships) at a different speed than their low-level bandit estimates (aliens), which is common if one task level is perceived as more volatile than the other.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Learner with Separate Stage Learning Rates.
    
    This model combines Model-Based (planning using transition matrix) and 
    Model-Free (direct reinforcement) strategies. Crucially, it allows for 
    different learning rates for the high-level choice (Stage 1) and the 
    low-level bandit task (Stage 2), reflecting potentially different 
    plasticity or learning speeds for abstract vs concrete associations.
    
    Parameters:
    - lr_stage1: [0, 1] Learning rate for Stage 1 Model-Free values.
    - lr_stage2: [0, 1] Learning rate for Stage 2 Q-values (used by both MB and MF).
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - w: [0, 1] Weight of Model-Based control (1 = Pure MB, 0 = Pure MF).
    - stickiness: [0, 5] Choice stickiness for Stage 1 (tendency to repeat last choice).
    """
    lr_stage1, lr_stage2, beta_1, beta_2, w, stickiness = model_parameters
    n_trials = len(action_1)
    
    # Transition matrix: A(0)->X(0) 0.7, A(0)->Y(1) 0.3; U(1)->X(0) 0.3, U(1)->Y(1) 0.7
    trans_mat = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_mf = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    last_a1 = -1
    log_loss = 0.0
    eps = 1e-10
    
    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Calculate Model-Based values
        # V(state) = max(Q_stage2(state))
        v_stage2 = np.max(q_stage2, axis=1)
        q_mb = trans_mat @ v_stage2
        
        # Net Stage 1 values
        q_net = w * q_mb + (1 - w) * q_mf
        
        # Add stickiness
        if last_a1 != -1:
            q_net[last_a1] += stickiness
            
        # Stage 1 Choice
        exp_q1 = np.exp(beta_1 * q_net)
        prob_a1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(prob_a1[a1] + eps)
        
        # Stage 2 Choice
        exp_q2 = np.exp(beta_2 * q_stage2[s_idx])
        prob_a2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(prob_a2[a2] + eps)
        
        # Update Stage 2
        delta_2 = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += lr_stage2 * delta_2
        
        # Update Stage 1 MF
        # TD(0): update towards Q_stage2[s_idx, a2]
        delta_1 = q_stage2[s_idx, a2] - q_mf[a1]
        q_mf[a1] += lr_stage1 * delta_1
        
        last_a1 = a1
        
    return log_loss
```

### Model 3: Model-Based Learner with Dynamic Habit Learning
This model is a Model-Based learner that includes a dynamic "habit" mechanism rather than simple static stickiness. A habit strength trace accumulates for chosen actions and decays for unchosen ones over time. This captures a more persistent form of perseverance (momentum) that builds up over a run of identical choices, which differs from the immediate repetition bias of standard stickiness.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Based Learner with Dynamic Habit Learning.
    
    Instead of a static stickiness parameter, this model implements a dynamic 
    'habit' strength that accumulates when an action is chosen and decays 
    when it is not. This habit strength is added to the Model-Based evaluation.
    This captures 'perseverance' that builds up over repeated choices.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for Stage 2 values.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - habit_lr: [0, 1] Rate at which habit strength updates (builds/decays).
    - habit_w: [0, 10] Weight of the habit strength in the decision.
    """
    learning_rate, beta_1, beta_2, habit_lr, habit_w = model_parameters
    n_trials = len(action_1)
    
    trans_mat = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage2 = np.zeros((2, 2))
    habit_strength = np.zeros(2) # Habits for Stage 1 choices
    
    log_loss = 0.0
    eps = 1e-10
    
    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Model-Based Values
        v_stage2 = np.max(q_stage2, axis=1)
        q_mb = trans_mat @ v_stage2
        
        # Combine with Habit
        q_net = q_mb + habit_w * habit_strength
        
        # Stage 1 Choice
        exp_q1 = np.exp(beta_1 * q_net)
        prob_a1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(prob_a1[a1] + eps)
        
        # Stage 2 Choice
        exp_q2 = np.exp(beta_2 * q_stage2[s_idx])
        prob_a2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(prob_a2[a2] + eps)
        
        # Update Stage 2
        delta_2 = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += learning_rate * delta_2
        
        # Update Habit
        # H[a] <- H[a] + alpha * (1 - H[a]) if chosen
        # H[a] <- H[a] + alpha * (0 - H[a]) if not chosen (decay)
        one_hot = np.zeros(2)
        one_hot[a1] = 1.0
        habit_strength += habit_lr * (one_hot - habit_strength)
        
    return log_loss
```