Here are three cognitive models based on the participant data and the two-step task structure.

### Model 1: Hybrid MB/MF with Eligibility Trace and Choice Perseveration (Stickiness)
This model extends the previous "best" model by adding a **stickiness** parameter. The participant data shows distinct "streaks" of choosing the same spaceship (e.g., trials 48-58, 133-139). This parameter captures the tendency to repeat the previous Stage 1 choice regardless of reward, a common phenomenon in this task that is distinct from reinforcement learning.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF model with Eligibility Trace and Choice Stickiness.
    
    Adds a 'stickiness' parameter to the Stage 1 choice, capturing the 
    tendency to repeat the previous action regardless of value.
    
    Parameters:
    lr: [0, 1] Learning rate.
    beta_1: [0, 10] Inverse temp for Stage 1.
    beta_2: [0, 10] Inverse temp for Stage 2.
    w: [0, 1] MB/MF weighting (0=MF, 1=MB).
    lam: [0, 1] Eligibility trace (lambda).
    stick: [0, 10] Choice stickiness (perseveration) bonus.
    """
    lr, beta_1, beta_2, w, lam, stick = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Q-value is weighted sum of MB and MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Calculate logits: beta * Q + stickiness
        logits_1 = beta_1 * q_net
        if trial > 0:
            logits_1[action_1[trial-1]] += stick
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1)) # subtract max for numerical stability
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        qs_2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta_2 * qs_2 - np.max(beta_2 * qs_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Action value updating
        # Stage 1 PE: Value of state reached - Value of spaceship chosen
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        # Stage 2 PE: Reward received - Value of alien chosen
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Update Stage 1 MF value (with eligibility trace from Stage 2)
        q_stage1_mf[action_1[trial]] += lr * (delta_stage1 + lam * delta_stage2)
        
        # Update Stage 2 MF value
        q_stage2_mf[state_idx, action_2[trial]] += lr * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid MB/MF with Separate Learning Rates for Stages
The participant might learn about the immediate rewards (aliens) at a different rate than they learn about the spaceships. This model decouples the learning rate into `lr_1` (Stage 1 spaceship preference) and `lr_2` (Stage 2 alien preference), while retaining the hybrid MB/MF structure and eligibility trace.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF model with Separate Learning Rates for Stage 1 and Stage 2.
    
    Allows the participant to update spaceship values (Stage 1) and 
    alien values (Stage 2) at different speeds.
    
    Parameters:
    lr_1: [0, 1] Learning rate for Stage 1 (spaceships).
    lr_2: [0, 1] Learning rate for Stage 2 (aliens).
    beta_1: [0, 10] Inverse temp for Stage 1.
    beta_2: [0, 10] Inverse temp for Stage 2.
    w: [0, 1] MB/MF weighting.
    lam: [0, 1] Eligibility trace.
    """
    lr_1, lr_2, beta_1, beta_2, w, lam = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_1 * q_net - np.max(beta_1 * q_net))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        qs_2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta_2 * qs_2 - np.max(beta_2 * qs_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Action value updating
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Update Stage 1 using lr_1
        q_stage1_mf[action_1[trial]] += lr_1 * (delta_stage1 + lam * delta_stage2)
        
        # Update Stage 2 using lr_2
        q_stage2_mf[state_idx, action_2[trial]] += lr_2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid MB/MF with Asymmetric Learning Rates (+/-) and Eligibility Trace
This model investigates if the participant learns differently from positive prediction errors (getting more than expected) versus negative prediction errors (getting less). This "optimism/pessimism" bias is common in reinforcement learning. It uses `lr_pos` when the error is positive and `lr_neg` when it is negative.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF model with Asymmetric Learning Rates (Positive vs Negative).
    
    Differentiates learning speed based on whether the outcome was better 
    (positive PE) or worse (negative PE) than expected.
    
    Parameters:
    lr_pos: [0, 1] Learning rate for positive prediction errors.
    lr_neg: [0, 1] Learning rate for negative prediction errors.
    beta_1: [0, 10] Inverse temp for Stage 1.
    beta_2: [0, 10] Inverse temp for Stage 2.
    w: [0, 1] MB/MF weighting.
    lam: [0, 1] Eligibility trace.
    """
    lr_pos, lr_neg, beta_1, beta_2, w, lam = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_1 * q_net - np.max(beta_1 * q_net))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        qs_2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta_2 * qs_2 - np.max(beta_2 * qs_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Action value updating
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Combined error for Stage 1
        total_error_1 = delta_stage1 + lam * delta_stage2
        lr_1 = lr_pos if total_error_1 > 0 else lr_neg
        q_stage1_mf[action_1[trial]] += lr_1 * total_error_1
        
        # Error for Stage 2
        lr_2 = lr_pos if delta_stage2 > 0 else lr_neg
        q_stage2_mf[state_idx, action_2[trial]] += lr_2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```