Here are the 3 new cognitive models based on the participant data and task structure.

### Model 1: Hybrid Model with Unchosen Action Decay
This model modifies the standard learning rule. In addition to updating the value of the chosen action, the values of **unchosen** actions decay over time. This mechanism helps the agent handle the non-stationary reward probabilities (the "slowly changing" probabilities mentioned in the description) by gradually "forgetting" old estimates for options not recently explored.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Unchosen Action Decay.
    
    In this model, the Q-values of unchosen actions are decayed towards zero on each trial.
    This promotes re-exploration and adaptation in non-stationary environments where 
    reward probabilities change over time.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate for the chosen action.
    beta: [0, 10] - Inverse temperature for softmax choice.
    w: [0, 1] - Weight for Model-Based values (0 = pure MF, 1 = pure MB).
    decay: [0, 1] - Decay rate for unchosen action values (0 = no decay, 1 = full reset).
    """
    learning_rate, beta, w, decay = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # state x action

    for trial in range(n_trials):
        if action_1[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # Stage 1 MF Update
        chosen_a1 = action_1[trial]
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[chosen_a1]
        q_stage1_mf[chosen_a1] += learning_rate * delta_stage1
        
        # Decay unchosen Stage 1 action
        unchosen_a1 = 1 - chosen_a1
        q_stage1_mf[unchosen_a1] *= (1 - decay)

        # Stage 2 MF Update
        chosen_a2 = action_2[trial]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, chosen_a2]
        q_stage2_mf[state_idx, chosen_a2] += learning_rate * delta_stage2
        
        # Decay unchosen Stage 2 action (in the current state)
        unchosen_a2 = 1 - chosen_a2
        q_stage2_mf[state_idx, unchosen_a2] *= (1 - decay)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model with Choice Kernel (Accumulated Stickiness)
The participant data shows long "blocks" of repeated choices (inertia). Standard stickiness only considers the immediately previous choice. This model uses a **Choice Kernel** (or trace) that accumulates over repeated choices, creating a stronger, persistent habit that takes longer to break.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Choice Kernel (Accumulated Stickiness).
    
    Uses a 'choice trace' that accumulates when an action is repeated and decays when 
    it is not. This models strong inertia or habit formation (momentum) observed in 
    the participant's long streaks of identical Stage 1 choices.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - MB/MF mixing weight.
    stick_decay: [0, 1] - Decay rate of the choice trace (higher = shorter memory).
    stick_weight: [0, 5] - Weight of the choice trace added to the decision value.
    """
    learning_rate, beta, w, stick_decay, stick_weight = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Choice trace for Stage 1 actions (initialized to 0)
    choice_trace = np.zeros(2)

    for trial in range(n_trials):
        if action_1[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add accumulated stickiness (choice trace) to logits
        logits = q_net + stick_weight * choice_trace
        
        exp_q1 = np.exp(beta * logits)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Update Choice Trace: Decay all, then increment chosen
        choice_trace *= (1 - stick_decay)
        choice_trace[action_1[trial]] += 1
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model with Outcome-Dependent Stickiness
This model differentiates between "Win-Stay" and "Lose-Shift" tendencies by applying different stickiness bonuses depending on the outcome of the previous trial. This captures the participant's tendency to switch specifically after losses (or streaks of losses) while persevering after wins.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Outcome-Dependent Stickiness.
    
    Applies a different stickiness bonus to the previously chosen Stage 1 action
    depending on whether the previous trial resulted in a reward (Win) or not (Loss).
    This allows the model to capture 'Win-Stay' and 'Lose-Stay' (or shift) behaviors separately.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - MB/MF mixing weight.
    stick_win: [0, 5] - Stickiness bonus applied if the previous trial was rewarded.
    stick_loss: [0, 5] - Stickiness bonus applied if the previous trial was unrewarded.
    """
    learning_rate, beta, w, stick_win, stick_loss = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    last_reward = 0

    for trial in range(n_trials):
        if action_1[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Apply outcome-dependent stickiness
        logits = q_net.copy()
        if last_action_1 != -1:
            if last_reward > 0: # Previous Win
                logits[last_action_1] += stick_win
            else: # Previous Loss (0 or -1)
                logits[last_action_1] += stick_loss
        
        exp_q1 = np.exp(beta * logits)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Store history
        last_action_1 = action_1[trial]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        last_reward = reward[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```