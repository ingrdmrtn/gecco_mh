Here are three new cognitive models for the two-step task, designed to capture different potential mechanisms underlying the participant's behavior, specifically addressing the learning rates across stages, the nature of perseveration (habits), and the combination of model-based planning with memory decay.

### Model 1: Dual Learning Rate Model-Free Agent
This model hypothesizes that the participant learns the value of spaceships (Stage 1) and aliens (Stage 2) at different speeds. Given the stable but probabilistic transition structure of Stage 1 versus the drifting reward probabilities of Stage 2, decoupling these learning rates allows the agent to adapt to the alien rewards quickly (`lr_2`) while potentially maintaining more stable estimates for the spaceships (`lr_1`), or vice versa.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Model 1: Model-Free Q-Learning with Separate Stage Learning Rates.
    
    This model separates the learning rates for the first stage (spaceships) and 
    the second stage (aliens). This allows the agent to track the drifting 
    probabilities of the aliens with one rate, while updating the value of 
    spaceships with another, potentially stabilizing choice behavior.
    
    Parameters:
    lr_1: Learning rate for stage 1 (spaceships) [0,1]
    lr_2: Learning rate for stage 2 (aliens) [0,1]
    beta: Inverse temperature (exploration/exploitation) [0,10]
    lambda_eligibility: Eligibility trace decay connecting stage 2 outcome to stage 1 [0,1]
    perseveration: Stickiness to previous choice [0,5]
    """
    lr_1, lr_2, beta, lambda_eligibility, perseveration = model_parameters
    n_trials = len(action_1)
    
    # Initialize Q-values to neutral 0.5
    q_stage1 = np.full(2, 0.5)
    q_stage2 = np.full((2, 2), 0.5)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        a1 = action_1[trial]
        s_idx = state[trial]
        a2 = action_2[trial]
        r = reward[trial]
        
        # Handle missing data
        if a1 == -1:
            last_action_1 = -1
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue
            
        # --- Stage 1 Policy ---
        q_net_1 = q_stage1.copy()
        if last_action_1 != -1:
            q_net_1[last_action_1] += perseveration
            
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        # TD Error Stage 1: Difference between Stage 2 value and Stage 1 value
        delta_1 = q_stage2[s_idx, a2] - q_stage1[a1]
        
        # TD Error Stage 2: Difference between Reward and Stage 2 value
        delta_2 = r - q_stage2[s_idx, a2]
        
        # Update Stage 1: Uses lr_1. Includes eligibility trace from Stage 2 error.
        q_stage1[a1] += lr_1 * delta_1 + lr_1 * lambda_eligibility * delta_2
        
        # Update Stage 2: Uses lr_2.
        q_stage2[s_idx, a2] += lr_2 * delta_2
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Choice Kernel Reinforcement Learning
This model addresses the participant's "streaky" behavior by implementing a "Choice Kernel" instead of simple one-step perseveration. The Choice Kernel accumulates a history of past choices which decays over time. This captures habitual behavior that builds up over repeated choices and persists longer than a single trial, potentially explaining the long blocks of spaceship choices observed in the data.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model 2: Model-Free Q-Learning with Choice Kernel (Habit Trace).
    
    Replaces simple 1-step perseveration with a decaying Choice Kernel (CK).
    The CK tracks the frequency of recent choices, allowing the model to 
    capture stronger, accumulated habits (streaks) and gradual un-learning 
    of habits.
    
    Parameters:
    learning_rate: Value learning rate [0,1]
    beta: Inverse temperature [0,10]
    lambda_eligibility: Eligibility trace decay [0,1]
    ck_decay: Decay rate of the choice kernel history [0,1]
    ck_weight: Weight of the choice kernel in the decision [0,10]
    """
    learning_rate, beta, lambda_eligibility, ck_decay, ck_weight = model_parameters
    n_trials = len(action_1)
    
    q_stage1 = np.full(2, 0.5)
    q_stage2 = np.full((2, 2), 0.5)
    
    # Choice Kernel: Tracks habit strength for each spaceship
    choice_kernel = np.zeros(2)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        a1 = action_1[trial]
        s_idx = state[trial]
        a2 = action_2[trial]
        r = reward[trial]
        
        if a1 == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue
            
        # --- Stage 1 Policy ---
        # Decision is a mix of Value (Q) and Habit (CK)
        logits = beta * q_stage1 + ck_weight * choice_kernel
        
        # Softmax with stability subtraction
        logits = logits - np.max(logits)
        exp_vals = np.exp(logits)
        probs_1 = exp_vals / np.sum(exp_vals)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Update Choice Kernel ---
        # Decay existing traces and reinforce current choice
        choice_kernel = choice_kernel * (1 - ck_decay)
        choice_kernel[a1] += 1.0
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        delta_1 = q_stage2[s_idx, a2] - q_stage1[a1]
        delta_2 = r - q_stage2[s_idx, a2]
        
        q_stage1[a1] += learning_rate * delta_1 + learning_rate * lambda_eligibility * delta_2
        q_stage2[s_idx, a2] += learning_rate * delta_2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model with Passive Value Decay
This model builds on the "best model so far" (Pure MF with Decay) by re-introducing the Model-Based (MB) component, but maintaining the decay mechanism. Since the environment's reward probabilities drift, this model applies passive decay to *all* learned values (both the MF values and the Stage 2 values used for MB planning). This tests if the participant uses a hybrid strategy where both habitual and planned values are subject to forgetting/decay due to the changing environment.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model 3: Hybrid Model with Passive Value Decay.
    
    A Hybrid (Model-Based + Model-Free) agent where all Q-values decay 
    towards neutral (0.5) on every trial. This allows both the model-based 
    estimates of the aliens and the model-free cached values to remain 
    plastic in the face of drifting probabilities.
    
    Parameters:
    learning_rate: Learning rate for values [0,1]
    beta: Inverse temperature [0,10]
    w: Mixing weight (0=Pure MF, 1=Pure MB) [0,1]
    lambda_eligibility: Eligibility trace for MF component [0,1]
    decay: Decay rate of Q-values to neutral (0.5) [0,1]
    perseveration: Stickiness to previous choice [0,5]
    """
    learning_rate, beta, w, lambda_eligibility, decay, perseveration = model_parameters
    n_trials = len(action_1)
    
    # Transition matrix: A(0)->X(0) 70%, U(1)->Y(1) 70%
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_mf1 = np.full(2, 0.5)
    q_mf2 = np.full((2, 2), 0.5) 
    # Note: q_mf2 also serves as the state-values for the MB calculation
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        a1 = action_1[trial]
        s_idx = state[trial]
        a2 = action_2[trial]
        r = reward[trial]
        
        if a1 == -1:
            last_action_1 = -1
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue
            
        # --- Decay ---
        # All values decay toward 0.5 to handle drift
        q_mf1 = q_mf1 * (1 - decay) + 0.5 * decay
        q_mf2 = q_mf2 * (1 - decay) + 0.5 * decay
        
        # --- Model-Based Valuation ---
        # V_MB(s) = max_a Q(s,a)
        # Q_MB(a) = sum P(s|a) * V_MB(s)
        max_q2 = np.max(q_mf2, axis=1) 
        q_mb1 = transition_matrix @ max_q2
        
        # --- Net Q-value (Hybrid) ---
        q_net = w * q_mb1 + (1 - w) * q_mf1
        
        # Perseveration
        if last_action_1 != -1:
            q_net[last_action_1] += perseveration
            
        # --- Stage 1 Policy ---
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_mf2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        # MF Stage 1 Error
        delta_1 = q_mf2[s_idx, a2] - q_mf1[a1]
        # Stage 2 Error
        delta_2 = r - q_mf2[s_idx, a2]
        
        # Update MF Stage 1
        q_mf1[a1] += learning_rate * delta_1 + learning_rate * lambda_eligibility * delta_2
        # Update MF Stage 2 (and MB basis)
        q_mf2[s_idx, a2] += learning_rate * delta_2
        
        last_action_1 = a1
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```