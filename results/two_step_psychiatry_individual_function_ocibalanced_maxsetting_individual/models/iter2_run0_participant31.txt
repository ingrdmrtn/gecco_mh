Here are three new cognitive models for the two-step task, designed based on the participant data and the history of parameter combinations.

### Model 1: Asymmetric Learning Rates with Stickiness
This model posits that the participant updates their value estimates differently depending on whether the outcome was better or worse than expected (positive vs. negative prediction errors). This is combined with a "stickiness" parameter to account for the participant's tendency to repeat or switch choices independent of value (motor perseveration).

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Asymmetric Learning Rates (Pos/Neg) with Stickiness.
    
    Distinguishes learning from positive vs negative prediction errors.
    Includes choice stickiness to capture perseveration.
    
    Parameters:
    - alpha_pos: [0, 1] Learning rate for positive prediction errors.
    - alpha_neg: [0, 1] Learning rate for negative prediction errors.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Model-based weight.
    - stickiness: [-5, 5] Choice perseveration bonus.
    """
    alpha_pos, alpha_neg, beta, w, stickiness = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits = q_net.copy()
        if last_action_1 != -1:
            logits[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * logits)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        # Action value updating for choice 1
        lr_1 = alpha_pos if delta_stage1 > 0 else alpha_neg
        q_stage1_mf[action_1[trial]] += lr_1 * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Action value updating for choice 2
        lr_2 = alpha_pos if delta_stage2 > 0 else alpha_neg
        q_stage2_mf[state_idx, action_2[trial]] += lr_2 * delta_stage2
        
        last_action_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Separate Stage Learning Rates with Stickiness
This model recognizes that the two stages of the task (Spaceship choice vs. Alien choice) are structurally different. Stage 1 involves navigation and planning, while Stage 2 is a bandit task. It assigns distinct learning rates to each stage, combined with stickiness to handle the switching behavior.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Separate Stage Learning Rates with Stickiness.
    
    Uses different learning rates for the first stage (spaceship choice)
    and second stage (alien choice), acknowledging the structural differences.
    Includes choice stickiness.
    
    Parameters:
    - lr_1: [0, 1] Learning rate for stage 1.
    - lr_2: [0, 1] Learning rate for stage 2.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Model-based weight.
    - stickiness: [-5, 5] Choice perseveration bonus.
    """
    lr_1, lr_2, beta, w, stickiness = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits = q_net.copy()
        if last_action_1 != -1:
            logits[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * logits)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        # Action value updating for choice 1
        q_stage1_mf[action_1[trial]] += lr_1 * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Action value updating for choice 2
        q_stage2_mf[state_idx, action_2[trial]] += lr_2 * delta_stage2
        
        last_action_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Agent with Decay and Stickiness
This model incorporates a "forgetting" or decay mechanism for the unchosen options in the second stage. Since the alien reward probabilities change slowly, older information becomes less reliable. This model decays the value of the unchosen alien towards a neutral value (0.5), encouraging re-exploration or forgetting of old outcomes, alongside choice stickiness.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Agent with Decay (Forgetting) and Stickiness.
    
    Unchosen second-stage options decay towards a neutral value (0.5),
    simulating forgetting or return to prior.
    Includes choice stickiness.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Model-based weight.
    - decay: [0, 1] Decay rate for unchosen stage 2 options towards 0.5.
    - stickiness: [-5, 5] Choice perseveration bonus.
    """
    learning_rate, beta, w, decay, stickiness = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits = q_net.copy()
        if last_action_1 != -1:
            logits[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * logits)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        # Action value updating for choice 1
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Action value updating for choice 2
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Decay unchosen option in the visited state towards 0.5
        unchosen_a2 = 1 - action_2[trial]
        q_stage2_mf[state_idx, unchosen_a2] += decay * (0.5 - q_stage2_mf[state_idx, unchosen_a2])
        
        last_action_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```