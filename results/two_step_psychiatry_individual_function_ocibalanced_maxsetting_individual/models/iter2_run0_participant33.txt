Here are three cognitive models based on the participant data and the two-step task structure.

### Model 1: Hybrid Model with Passive Decay
This model introduces a **decay rate** for unchosen actions. In volatile environments (or when the participant suspects volatility), values of options not recently sampled may decay toward a baseline (neutral) value. This mechanism explains why a participant might eventually switch away from a preferred option even if they haven't received explicit negative feedback recently, or why they might revisit a previously poor option after a long time.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Passive Decay for Unchosen Options.
    
    Chosen options are updated via Q-learning. Unchosen options decay towards 0.
    This helps the agent explore options that haven't been visited in a while.
    
    Parameters:
    - learning_rate: [0,1] Rate of updating value based on reward.
    - decay_rate: [0,1] Rate at which unchosen action values decay to 0.
    - beta: [0,10] Inverse temperature (softness of choice).
    - w: [0,1] Weight of Model-Based system (1=Pure MB, 0=Pure MF).
    """
    learning_rate, decay_rate, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # Handle missing data
        if action_1[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # --- Stage 1 Decision ---
        # Model-Based Value: Expected max value of next stage
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        
        # 1. Prediction Errors for Chosen Actions
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # 2. Decay for Unchosen Actions
        # Stage 1 Unchosen
        unchosen_a1 = 1 - a1
        q_stage1_mf[unchosen_a1] *= (1.0 - decay_rate)
        
        # Stage 2 Unchosen (in the visited state)
        unchosen_a2 = 1 - a2
        q_stage2_mf[state_idx, unchosen_a2] *= (1.0 - decay_rate)
        
        # Note: We do not typically decay the Q-values of the unvisited state 
        # in this paradigm, but one could. Here we stick to unchosen actions in visited states.

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model with Separate Stage Stickiness
The participant data shows significant repetition not just in the choice of spaceship (Stage 1), but also in the choice of aliens (Stage 2). Standard models often only apply a "choice kernel" or stickiness bonus to the first stage. This model applies distinct stickiness parameters to both stages, allowing the model to capture habit formation at specific levels of the task hierarchy.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Separate Stickiness for Stage 1 and Stage 2.
    
    Captures perseveration (repetition bias) independently for spaceship choice
    and alien choice.
    
    Parameters:
    - learning_rate: [0,1] Learning rate.
    - beta: [0,10] Inverse temperature.
    - w: [0,1] Model-Based weight.
    - stick_1: [0,5] Stickiness bonus for repeating the previous Stage 1 choice.
    - stick_2: [0,5] Stickiness bonus for repeating the previous Stage 2 choice (context-dependent).
    """
    learning_rate, beta, w, stick_1, stick_2 = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track previous choices for stickiness
    prev_a1 = -1
    prev_a2_by_state = [-1, -1] # Track last choice made in each state (planet)

    for trial in range(n_trials):
        if action_1[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add Stickiness to Stage 1
        logits_1 = beta * q_net_stage1
        if prev_a1 != -1:
            logits_1[prev_a1] += stick_1
            
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]

        # --- Stage 2 Choice ---
        # Add Stickiness to Stage 2 (specific to the planet)
        logits_2 = beta * q_stage2_mf[state_idx]
        if prev_a2_by_state[state_idx] != -1:
            logits_2[prev_a2_by_state[state_idx]] += stick_2

        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Update history
        prev_a1 = a1
        prev_a2_by_state[state_idx] = a2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model with Outcome-Dependent Stickiness
This model posits that the participant's tendency to repeat a choice depends on the previous outcome (Win-Stay, Lose-Shift), but implemented as a bias within the reinforcement learning framework. Instead of a single "stickiness" parameter, there is a `stick_win` (bonus for repeating after a reward) and a `stick_lose` (bonus/penalty for repeating after no reward). This separates the *value* learning from the *heuristic* response to outcomes.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Outcome-Dependent Stickiness (Win-Stay / Lose-Shift bias).
    
    The tendency to repeat the previous Stage 1 choice is modulated by whether 
    the previous trial resulted in a reward.
    
    Parameters:
    - learning_rate: [0,1] Learning rate.
    - beta: [0,10] Inverse temperature.
    - w: [0,1] Model-Based weight.
    - stick_win: [0,5] Stickiness bonus if previous trial was a Win (Reward=1).
    - stick_lose: [0,5] Stickiness bonus if previous trial was a Loss (Reward=0).
    """
    learning_rate, beta, w, stick_win, stick_lose = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_a1 = -1
    prev_reward = -1

    for trial in range(n_trials):
        if action_1[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Apply Outcome-Dependent Stickiness
        logits_1 = beta * q_net_stage1
        if prev_a1 != -1:
            if prev_reward == 1:
                logits_1[prev_a1] += stick_win
            else:
                logits_1[prev_a1] += stick_lose
        
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]

        # --- Stage 2 Choice ---
        # Standard Softmax for Stage 2
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        prev_a1 = a1
        prev_reward = r

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```