Here are three new cognitive models designed to explain the participant's behavior, specifically addressing the rapid switching in the second stage and the strong persistence (stickiness) in the first stage.

### Model 1: Hybrid Model with Counterfactual Updating
This model extends the standard Hybrid MB/MF model by adding a **counterfactual update** mechanism in Stage 2. The participant often switches aliens immediately after a loss. This model assumes that when the agent observes the outcome of the chosen alien, they also update the value of the *unchosen* alien, assuming it would have yielded the opposite outcome (e.g., if chosen gave 0, unchosen would have given 1).

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Counterfactual Updating.
    
    In Stage 2, the agent updates the chosen alien based on the received reward.
    Crucially, it also updates the UNCHOSEN alien based on a counterfactual 
    assumption (assuming the unchosen alien would have yielded 1 - Reward).
    This allows for faster switching between options in Stage 2.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for value updates.
    - cf_weight: [0, 1] Weight of counterfactual update (0=none, 1=full inverse update).
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - w: [0, 1] Weight for Model-Based control (0=MF, 1=MB).
    - stickiness: [0, 5] Choice stickiness for Stage 1.
    """
    learning_rate, cf_weight, beta_1, beta_2, w, stickiness = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_mf1 = np.zeros(2)
    q_s2 = np.zeros((2, 2)) # State x Action
    
    last_action_1 = -1
    
    for t in range(n_trials):
        # --- Stage 1 Decision ---
        max_q_s2 = np.max(q_s2, axis=1)
        q_mb1 = transition_matrix @ max_q_s2
        
        q_net = w * q_mb1 + (1 - w) * q_mf1
        
        # Apply stickiness to previously chosen spaceship
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        q_net_stable = q_net - np.max(q_net)
        exp_q1 = np.exp(beta_1 * q_net_stable)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[t] = probs_1[action_1[t]]
        
        a1 = action_1[t]
        s_next = state[t]
        last_action_1 = a1
        
        # --- Stage 2 Decision ---
        q_s2_curr = q_s2[s_next]
        q_s2_stable = q_s2_curr - np.max(q_s2_curr)
        exp_q2 = np.exp(beta_2 * q_s2_stable)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[t] = probs_2[action_2[t]]
        
        a2 = action_2[t]
        r = reward[t]
        
        # --- Updates ---
        # Stage 1 MF Update (TD(0))
        pe1 = q_s2[s_next, a2] - q_mf1[a1]
        q_mf1[a1] += learning_rate * pe1
        
        # Stage 2 Update (Chosen Option)
        pe2 = r - q_s2[s_next, a2]
        q_s2[s_next, a2] += learning_rate * pe2
        
        # Counterfactual Update (Unchosen Option)
        # Assume the other alien would have given the opposite reward (1.0 - r)
        # This is weighted by cf_weight.
        r_cf = 1.0 - r
        pe_cf = r_cf - q_s2[s_next, 1-a2]
        q_s2[s_next, 1-a2] += learning_rate * cf_weight * pe_cf
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model with Dynamic Habit Learning
The participant shows long streaks of choices (inertia). Instead of a simple one-step stickiness parameter, this model implements a **dynamic habit** mechanism. The 'habit' strength for an action increases when chosen and decays when not chosen (Exponential Moving Average). This allows the model to capture the buildup of perseveration over time, separate from reward-based value learning.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Dynamic Habit Learning.
    
    Instead of a static stickiness bonus added based on the previous trial, 
    this model learns a 'habit' value that strengthens with repetition and 
    decays when not chosen. This separates value-based learning from 
    perseveration dynamics.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for Q-values.
    - habit_lr: [0, 1] Learning rate for habit strength accumulation/decay.
    - habit_w: [0, 10] Weight of the habit term in decision making.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - w: [0, 1] Weight for Model-Based control.
    """
    learning_rate, habit_lr, habit_w, beta_1, beta_2, w = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_mf1 = np.zeros(2)
    q_s2 = np.zeros((2, 2))
    habit = np.zeros(2) # Habit values for Stage 1 actions
    
    for t in range(n_trials):
        # --- Stage 1 Decision ---
        max_q_s2 = np.max(q_s2, axis=1)
        q_mb1 = transition_matrix @ max_q_s2
        
        # Net value combines MB, MF, and Habit
        q_net = w * q_mb1 + (1 - w) * q_mf1 + habit_w * habit
            
        q_net_stable = q_net - np.max(q_net)
        exp_q1 = np.exp(beta_1 * q_net_stable)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[t] = probs_1[action_1[t]]
        
        a1 = action_1[t]
        s_next = state[t]
        
        # --- Update Habit ---
        # Chosen action habit increases towards 1, unchosen decays towards 0
        habit[a1] += habit_lr * (1.0 - habit[a1])
        habit[1-a1] += habit_lr * (0.0 - habit[1-a1])
        
        # --- Stage 2 Decision ---
        q_s2_curr = q_s2[s_next]
        q_s2_stable = q_s2_curr - np.max(q_s2_curr)
        exp_q2 = np.exp(beta_2 * q_s2_stable)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[t] = probs_2[action_2[t]]
        
        a2 = action_2[t]
        r = reward[t]
        
        # --- Value Updates ---
        pe1 = q_s2[s_next, a2] - q_mf1[a1]
        q_mf1[a1] += learning_rate * pe1
        
        pe2 = r - q_s2[s_next, a2]
        q_s2[s_next, a2] += learning_rate * pe2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Sarsa(Î») with Stickiness
This is a pure Model-Free model that uses **eligibility traces (Sarsa-Lambda)** to link Stage 2 rewards directly to Stage 1 choices. Unlike previous attempts that might have tested lambda or stickiness separately, this model combines them. It tests the hypothesis that the participant is purely Model-Free but uses eligibility traces to solve the credit assignment problem, with stickiness accounting for the choice autocorrelation.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Sarsa(Lambda) Model with Stickiness.
    
    A pure Model-Free agent using eligibility traces (lambda) to connect
    Stage 2 outcomes directly to Stage 1 choices. It includes choice stickiness
    to account for the participant's tendency to repeat spaceship choices.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - lambda_param: [0, 1] Eligibility trace decay parameter (0=TD, 1=Monte Carlo).
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - stickiness: [0, 5] Choice stickiness for Stage 1.
    """
    learning_rate, lambda_param, beta_1, beta_2, stickiness = model_parameters
    n_trials = len(action_1)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values: q1 for stage 1 (2 actions), q2 for stage 2 (2 states x 2 actions)
    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))
    
    last_action_1 = -1
    
    for t in range(n_trials):
        # --- Stage 1 Decision ---
        q1_net = q1.copy()
        if last_action_1 != -1:
            q1_net[last_action_1] += stickiness
            
        q1_stable = q1_net - np.max(q1_net)
        exp_q1 = np.exp(beta_1 * q1_stable)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[t] = probs_1[action_1[t]]
        
        a1 = action_1[t]
        s_next = state[t]
        last_action_1 = a1
        
        # --- Stage 2 Decision ---
        q2_curr = q2[s_next]
        q2_stable = q2_curr - np.max(q2_curr)
        exp_q2 = np.exp(beta_2 * q2_stable)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[t] = probs_2[action_2[t]]
        
        a2 = action_2[t]
        r = reward[t]
        
        # --- Sarsa(Lambda) Updates ---
        # Delta 1: Prediction Error from Stage 1 -> Stage 2 transition
        delta1 = q2[s_next, a2] - q1[a1]
        
        # Delta 2: Prediction Error from Stage 2 -> Reward
        delta2 = r - q2[s_next, a2]
        
        # Update Q1: Includes direct TD(0) part (delta1) and 
        # the eligibility trace part from Stage 2 (delta2 weighted by lambda)
        q1[a1] += learning_rate * delta1 + learning_rate * lambda_param * delta2
        
        # Update Q2: Standard TD update
        q2[s_next, a2] += learning_rate * delta2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```