Here are three new cognitive models based on the participant data and the feedback provided.

### Model 1: Hybrid Model with Memory Decay
**Rationale:** The participant shows distinct blocks of behavior (switching preferences after long periods). This suggests a "forgetting" or decay mechanism where the value of options (especially unchosen ones or old habits) degrades over time, promoting re-exploration or strategy switching. This model adds a `decay` parameter to the standard hybrid architecture.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid model with Q-value Decay (Forgetting).
    Q-values decay toward zero on every trial, simulating memory loss.
    
    Parameters:
    learning_rate: [0,1] - Learning rate for MF updates.
    beta: [0,10] - Inverse temperature.
    w: [0,1] - Weight of Model-Based system (1=MB, 0=MF).
    decay: [0,1] - Rate at which Q-values decay (0=no decay, 1=instant forgetting).
    """
    learning_rate, beta, w, decay = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # Handle missing data
        if action_1[trial] == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_hybrid = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_hybrid)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # --- Decay Step ---
        # Apply decay to all model-free Q-values
        q_stage1_mf *= (1 - decay)
        q_stage2_mf *= (1 - decay)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Independent System Weights (Dual Beta)
**Rationale:** Standard models use a weighting parameter `w` to trade off between Model-Based (MB) and Model-Free (MF) systems, summing to a total `beta`. This model proposes that the two systems are independent drivers of behavior with their own signal strengths (`beta_mb` and `beta_mf`). This allows for cases where both systems are weak (randomness) or both are strong (deterministic), which the `w` parameter constrains. This is distinct from previous "Dual Beta" attempts that usually separate Stage 1 vs Stage 2.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Independent Model-Based and Model-Free Betas.
    Removes 'w' and assigns independent inverse temperatures to MB and MF systems.
    
    Parameters:
    learning_rate: [0,1] - Learning rate.
    beta_mb: [0,10] - Strength/Confidence of Model-Based system.
    beta_mf: [0,10] - Strength/Confidence of Model-Free system.
    """
    learning_rate, beta_mb, beta_mf = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        if action_1[trial] == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combine independently weighted signals
        # Note: No 'w' parameter. The betas determine the weight.
        q_net = (beta_mb * q_stage1_mb) + (beta_mf * q_stage1_mf)
        
        # Softmax on the net Q-value (implicit beta=1 for the combination)
        exp_q1 = np.exp(q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        # Use beta_mf for stage 2 as it is a pure model-free decision
        exp_q2 = np.exp(beta_mf * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid with Eligibility Trace and Choice Stickiness
**Rationale:** The feedback indicated that the "Hybrid with Eligibility Trace (Lambda)" was the best model so far. However, the participant data shows significant "stickiness" (repeating the same spaceship choice for long blocks, e.g., trials 14-50). This model combines the successful eligibility trace mechanism (`lam`) with a choice perseveration parameter (`stickiness`) to account for this behavioral inertia.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid model with Eligibility Trace (Lambda) AND Choice Stickiness.
    Combines direct stage-2 feedback (Lambda) with behavioral inertia (Stickiness).
    
    Parameters:
    learning_rate: [0,1] - Learning rate.
    beta: [0,10] - Inverse temperature.
    w: [0,1] - Weight of Model-Based system.
    lam: [0,1] - Eligibility trace (credit assignment to Stage 1 from Stage 2 outcome).
    stickiness: [0,5] - Bonus added to the previously chosen action (perseveration).
    """
    learning_rate, beta, w, lam, stickiness = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track previous action for stickiness
    last_action = -1

    for trial in range(n_trials):
        if action_1[trial] == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_hybrid = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness bonus to the Q-values used for choice (not learning)
        q_choice = q_hybrid.copy()
        if last_action != -1:
            q_choice[last_action] += stickiness
        
        exp_q1 = np.exp(beta * q_choice)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]

        # Update Stage 1 MF with eligibility trace (lambda)
        q_stage1_mf[action_1[trial]] += learning_rate * (delta_stage1 + lam * delta_stage2)

        # Update Stage 2 MF
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Update history
        last_action = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```