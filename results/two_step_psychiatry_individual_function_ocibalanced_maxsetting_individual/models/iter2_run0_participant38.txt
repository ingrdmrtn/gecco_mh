Here are three cognitive models implemented as Python functions, based on the participant's behavior and the two-step task structure.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Agent with Forgetting (Decay).
    
    This model assumes the participant is a model-free learner (uses eligibility traces)
    but also forgets the value of unchosen aliens over time. This accounts for the 
    drifting reward probabilities of the aliens.

    Parameters:
    - learning_rate: [0, 1] Rate at which chosen Q-values update.
    - beta_1: [0, 10] Inverse temperature for Stage 1 choice.
    - beta_2: [0, 10] Inverse temperature for Stage 2 choice.
    - lam: [0, 1] Eligibility trace parameter (0=TD, 1=Monte Carlo).
    - decay: [0, 1] Rate at which unchosen Stage 2 Q-values decay toward 0.
    """
    learning_rate, beta_1, beta_2, lam, decay = model_parameters
    n_trials = len(action_1)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Initialize Q-values
    q_stage1_mf = np.zeros(2)      # Values for Spaceships A(0) and U(1)
    q_stage2_mf = np.zeros((2, 2)) # Values for Aliens on Planet X(0) and Y(1)

    for trial in range(n_trials):
        # Skip invalid trials
        if action_1[trial] < 0 or action_2[trial] < 0:
            continue

        # --- Stage 1: Choice ---
        exp_q1 = np.exp(beta_1 * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Observe state
        state_idx = state[trial]
        a1 = action_1[trial]
        
        # --- Stage 2: Choice ---
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]

        # --- Updates ---
        # Prediction errors
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[state_idx, a2]

        # Update Stage 2 (Chosen)
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Decay Stage 2 (Unchosen)
        unchosen_a2 = 1 - a2
        q_stage2_mf[state_idx, unchosen_a2] *= (1 - decay)

        # Update Stage 1 (Eligibility Trace)
        q_stage1_mf[a1] += learning_rate * (delta_stage1 + lam * delta_stage2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Agent with Separate Stage Learning Rates.
    
    This model posits that learning dynamics differ between the two stages.
    Stage 2 (Bandit task) might be learned at a different rate than Stage 1 
    (Structural/Temporal credit assignment).

    Parameters:
    - lr_1: [0, 1] Learning rate for Stage 1 updates.
    - lr_2: [0, 1] Learning rate for Stage 2 updates.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - lam: [0, 1] Eligibility trace parameter.
    """
    lr_1, lr_2, beta_1, beta_2, lam = model_parameters
    n_trials = len(action_1)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        if action_1[trial] < 0 or action_2[trial] < 0:
            continue

        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta_1 * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        a1 = action_1[trial]
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[state_idx, a2]

        # Update Stage 2 with its own learning rate
        q_stage2_mf[state_idx, a2] += lr_2 * delta_stage2

        # Update Stage 1 with its own learning rate and trace
        q_stage1_mf[a1] += lr_1 * (delta_stage1 + lam * delta_stage2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Agent with Eligibility Traces.
    
    This is the canonical 'Daw' model which combines a Model-Based planner 
    (using the transition matrix) and a Model-Free learner (using TD-lambda).
    The Stage 1 value is a weighted mix of both.

    Parameters:
    - learning_rate: [0, 1] Learning rate for MF updates.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - w: [0, 1] Weighting of Model-Based system (0=Pure MF, 1=Pure MB).
    - lam: [0, 1] Eligibility trace parameter for the MF system.
    """
    learning_rate, beta_1, beta_2, w, lam = model_parameters
    n_trials = len(action_1)
    
    # Transition matrix: A(0)->[X(0):0.7, Y(1):0.3], U(1)->[X(0):0.3, Y(1):0.7]
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        if action_1[trial] < 0 or action_2[trial] < 0:
            continue

        # --- Stage 1 Choice (Hybrid) ---
        # Model-Based Value: Expected max value of next stage
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        a1 = action_1[trial]
        
        # --- Stage 2 Choice (Model-Free) ---
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]

        # --- Updates (MF System) ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[state_idx, a2]

        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        q_stage1_mf[a1] += learning_rate * (delta_stage1 + lam * delta_stage2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```