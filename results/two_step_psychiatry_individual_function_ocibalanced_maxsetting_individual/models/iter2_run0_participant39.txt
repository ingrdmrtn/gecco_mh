Here are three new cognitive models for the two-step decision task.

### Model 1: Dynamic Transition Learning
This model extends the hybrid learner by allowing the participant to update their internal model of the spaceship-planet transition probabilities. While the standard model assumes fixed probabilities (e.g., 0.7/0.3), this model assumes the participant learns the transition matrix $T(s'|s,a)$ online based on observed transitions.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF model with Dynamic Transition Learning.
    The participant updates their belief about transition probabilities 
    based on observed transitions, rather than assuming them fixed.
    
    Parameters:
    - lr: Learning rate for Q-values [0,1]
    - beta_1: Inverse temperature for stage 1 choice [0,10]
    - beta_2: Inverse temperature for stage 2 choice [0,10]
    - w: Weight for MB (1) vs MF (0) [0,1]
    - stick: Choice stickiness for stage 1 [0,5]
    - lr_trans: Learning rate for transition matrix updates [0,1]
    """
    lr, beta_1, beta_2, w, stick, lr_trans = model_parameters
    n_trials = len(action_1)
    
    # Initialize transition matrix (belief)
    # Rows: Choice 0 (A), Choice 1 (U). Cols: Planet 0 (X), Planet 1 (Y).
    # Initialize with the common prior (0.7 probability of common transition)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    log_loss = 0.0
    eps = 1e-10
    prev_a1 = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based Value: T * max(Q2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net value: weighted sum of MB and MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add Stickiness
        if prev_a1 != -1:
            q_net[prev_a1] += stick
            
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = action_1[trial]
        log_loss -= np.log(probs_1[a1] + eps)
        
        # --- Stage 2 Choice ---
        state_idx = state[trial]
        a2 = action_2[trial]
        
        # Handle missing data if any (though typically clean)
        if a2 == -1:
            prev_a1 = a1
            continue
            
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[a2] + eps)
        
        r = reward[trial]
        
        # --- Updates ---
        
        # 1. Update Transition Matrix Belief
        # P(state|a1) moves towards 1 if state observed, 0 otherwise
        # Update the observed transition probability
        transition_matrix[a1, state_idx] += lr_trans * (1.0 - transition_matrix[a1, state_idx])
        # Ensure row sums to 1
        transition_matrix[a1, 1 - state_idx] = 1.0 - transition_matrix[a1, state_idx]
        
        # 2. Stage 2 MF Update (TD Error)
        delta_2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += lr * delta_2
        
        # 3. Stage 1 MF Update (SARSA-style)
        delta_1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_1
        
        prev_a1 = a1
        
    return log_loss
```

### Model 2: Dual Learning Rates (Stage Separation)
This model hypothesizes that the participant learns at different rates for the two stages of the task. `lr_1` governs how quickly stage 1 values are updated (transfer from stage 2), while `lr_2` governs how quickly stage 2 values are updated based on rewards. This captures potential differences in plasticity between the spaceship choice and the alien choice.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF model with separate learning rates for Stage 1 and Stage 2.
    
    Parameters:
    - lr_1: Learning rate for stage 1 value updates [0,1]
    - lr_2: Learning rate for stage 2 value updates [0,1]
    - beta_1: Inverse temperature for stage 1 choice [0,10]
    - beta_2: Inverse temperature for stage 2 choice [0,10]
    - w: Weight for MB (1) vs MF (0) [0,1]
    - stick: Choice stickiness for stage 1 [0,5]
    """
    lr_1, lr_2, beta_1, beta_2, w, stick = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    log_loss = 0.0
    eps = 1e-10
    prev_a1 = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if prev_a1 != -1:
            q_net[prev_a1] += stick
            
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = action_1[trial]
        log_loss -= np.log(probs_1[a1] + eps)
        
        # --- Stage 2 Choice ---
        state_idx = state[trial]
        a2 = action_2[trial]
        
        if a2 == -1:
            prev_a1 = a1
            continue
            
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[a2] + eps)
        
        r = reward[trial]
        
        # --- Updates ---
        # Stage 2 update uses lr_2
        delta_2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += lr_2 * delta_2
        
        # Stage 1 update uses lr_1
        delta_1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr_1 * delta_1
        
        prev_a1 = a1

    return log_loss
```

### Model 3: Dual Stickiness (Stage 2 Perseveration)
This model adds a stickiness parameter to the second stage (`stick_2`). This tests if the participant has a tendency to repeat the same alien choice within a planet, independent of the reward history. This is distinct from value-based learning and captures habitual perseveration at the second level.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF model with stickiness in both Stage 1 and Stage 2 choices.
    Captures perseveration at the alien-choice level.
    
    Parameters:
    - lr: Learning rate [0,1]
    - beta_1: Inverse temperature for stage 1 [0,10]
    - beta_2: Inverse temperature for stage 2 [0,10]
    - w: Weight for MB (1) vs MF (0) [0,1]
    - stick_1: Stickiness for stage 1 [0,5]
    - stick_2: Stickiness for stage 2 [0,5]
    """
    lr, beta_1, beta_2, w, stick_1, stick_2 = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track the last action taken in each state (planet)
    # Initialize to -1 (no previous choice)
    last_choice_stage2 = np.array([-1, -1]) 
    
    log_loss = 0.0
    eps = 1e-10
    prev_a1 = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if prev_a1 != -1:
            q_net[prev_a1] += stick_1
            
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = action_1[trial]
        log_loss -= np.log(probs_1[a1] + eps)
        
        # --- Stage 2 Choice ---
        state_idx = state[trial]
        a2 = action_2[trial]
        
        if a2 == -1:
            prev_a1 = a1
            continue
            
        # Calculate Stage 2 values with stickiness bonus
        qs_2 = q_stage2_mf[state_idx].copy()
        last_c = last_choice_stage2[state_idx]
        if last_c != -1:
            qs_2[last_c] += stick_2
            
        exp_q2 = np.exp(beta_2 * qs_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[a2] + eps)
        
        r = reward[trial]
        
        # --- Updates ---
        delta_2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += lr * delta_2
        
        delta_1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_1
        
        prev_a1 = a1
        last_choice_stage2[state_idx] = a2
        
    return log_loss
```