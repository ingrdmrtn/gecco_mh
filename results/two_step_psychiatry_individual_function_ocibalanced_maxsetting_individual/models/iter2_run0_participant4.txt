Here are three cognitive models for the two-step task, designed to capture different hypotheses about the participant's learning and decision-making processes.

### Model 1: Hybrid Model with Stickiness
This model combines Model-Based (MB) and Model-Free (MF) reinforcement learning, a standard approach for this task. It adds a "stickiness" parameter to account for the participant's tendency to repeat the same spaceship choice (perseveration), which is evident in the data's long streaks of identical Stage 1 choices.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Stickiness.
    
    Combines Model-Based (MB) and Model-Free (MF) value estimation.
    Includes a choice stickiness parameter to account for perseveration
    independent of reward history.
    
    Parameters:
    learning_rate: [0, 1] Learning rate for value updates.
    beta_1: [0, 10] Inverse temperature for Stage 1.
    beta_2: [0, 10] Inverse temperature for Stage 2.
    w: [0, 1] Weighting parameter (0=MF, 1=MB).
    stickiness: [0, 5] Perseveration bonus for Stage 1 choice.
    """
    learning_rate, beta_1, beta_2, w, stickiness = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1

    for trial in range(n_trials):
        # Stage 1 Policy: Hybrid of MB and MF
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta_1 * q_net
        if prev_action_1 != -1:
            logits_1[prev_action_1] += stickiness
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        prev_action_1 = action_1[trial]
        state_idx = state[trial]
        a2 = action_2[trial]

        # Stage 2 Policy
        if a2 != -1:
            logits_2 = beta_2 * q_stage2_mf[state_idx]
            exp_q2 = np.exp(logits_2 - np.max(logits_2))
            probs_2 = exp_q2 / np.sum(exp_q2)
            p_choice_2[trial] = probs_2[a2]
            
            # Updates
            # MF Stage 1 update (TD(0)-like using Stage 2 value)
            delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[action_1[trial]]
            q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
            
            # MF Stage 2 update (Reward Prediction Error)
            delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
            q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        else:
            p_choice_2[trial] = 1.0

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Model-Based with Win-Stay Lose-Shift Stickiness
This model assumes the participant primarily uses Model-Based planning (calculating values based on the transition structure) but their choice persistence is modulated by the previous outcome. It distinguishes between "Win-Stay" (sticking after a reward) and "Lose-Shift" (switching after no reward), parameterized separately.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Based Learner with Win-Stay Lose-Shift Stickiness.
    
    Pure Model-Based valuation for Stage 1, but choice is biased by 
    outcome-dependent stickiness. This allows the model to capture
    different reactions to wins versus losses/omissions.
    
    Parameters:
    learning_rate: [0, 1] Learning rate for Stage 2 values.
    beta_1: [0, 10] Inverse temperature for Stage 1.
    beta_2: [0, 10] Inverse temperature for Stage 2.
    stick_win: [0, 5] Stickiness bonus after a reward (Win-Stay).
    stick_loss: [-5, 5] Stickiness bonus/penalty after no reward (Lose-Stay/Shift).
    """
    learning_rate, beta_1, beta_2, stick_win, stick_loss = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage2_mf = np.zeros((2, 2)) # Only learning stage 2 values
    
    prev_action_1 = -1
    prev_reward = 0

    for trial in range(n_trials):
        # Stage 1 Policy: MB + Outcome-Dependent Stickiness
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        logits_1 = beta_1 * q_stage1_mb
        
        if prev_action_1 != -1:
            if prev_reward > 0: # Win
                logits_1[prev_action_1] += stick_win
            else: # Loss or Omission
                logits_1[prev_action_1] += stick_loss
        
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        prev_action_1 = action_1[trial]
        
        state_idx = state[trial]
        a2 = action_2[trial]
        r = reward[trial]

        # Stage 2 Policy
        if a2 != -1:
            logits_2 = beta_2 * q_stage2_mf[state_idx]
            exp_q2 = np.exp(logits_2 - np.max(logits_2))
            probs_2 = exp_q2 / np.sum(exp_q2)
            p_choice_2[trial] = probs_2[a2]
            
            # Update Stage 2
            delta_stage2 = r - q_stage2_mf[state_idx, a2]
            q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
            
            prev_reward = r 
        else:
            p_choice_2[trial] = 1.0
            prev_reward = 0 

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model with Separate Stage Learning Rates
This model introduces separate learning rates for Stage 1 (spaceships) and Stage 2 (aliens). This allows the agent to update their preferences for spaceships at a different speed than they learn the fluctuating values of the aliens, which can explain behavior where the participant is slow to switch spaceships even when alien values change (habitual Stage 1 control).

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Separate Stage Learning Rates.
    
    Allows different learning speeds for the spaceship values (Stage 1 MF)
    and the alien values (Stage 2). This decouples the habit strength
    of the first choice from the value learning of the second choice.
    
    Parameters:
    alpha_1: [0, 1] Learning rate for Stage 1 (Spaceships).
    alpha_2: [0, 1] Learning rate for Stage 2 (Aliens).
    beta_1: [0, 10] Inverse temperature for Stage 1.
    beta_2: [0, 10] Inverse temperature for Stage 2.
    w: [0, 1] Weighting parameter (0=MF, 1=MB).
    stickiness: [0, 5] Perseveration bonus for Stage 1.
    """
    alpha_1, alpha_2, beta_1, beta_2, w, stickiness = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1

    for trial in range(n_trials):
        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta_1 * q_net
        if prev_action_1 != -1:
            logits_1[prev_action_1] += stickiness
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        prev_action_1 = action_1[trial]
        state_idx = state[trial]
        a2 = action_2[trial]

        # Stage 2 Policy
        if a2 != -1:
            logits_2 = beta_2 * q_stage2_mf[state_idx]
            exp_q2 = np.exp(logits_2 - np.max(logits_2))
            probs_2 = exp_q2 / np.sum(exp_q2)
            p_choice_2[trial] = probs_2[a2]
            
            # Updates
            # MF Stage 1 update with alpha_1
            delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[action_1[trial]]
            q_stage1_mf[action_1[trial]] += alpha_1 * delta_stage1
            
            # MF Stage 2 update with alpha_2
            delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
            q_stage2_mf[state_idx, a2] += alpha_2 * delta_stage2
        else:
            p_choice_2[trial] = 1.0

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```