def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Agent with Memory Leak (Forgetting).
    
    This model incorporates a 'forgetting_rate' that decays all Q-values 
    towards 0 at the beginning of each trial. This mechanism allows the agent 
    to adapt to the non-stationary (drifting) reward probabilities of the aliens 
    by suppressing outdated information more aggressively than standard learning.

    Parameters:
    - learning_rate: [0, 1] Rate at which Q-values are updated based on prediction errors.
    - beta: [0, 10] Inverse temperature for softmax choice (higher = more deterministic).
    - w: [0, 1] Weighting parameter (0 = Pure Model-Free, 1 = Pure Model-Based).
    - lambda_coef: [0, 1] Eligibility trace parameter for Stage 1 updates.
    - stickiness: [0, 10] Choice perseveration bonus for Stage 1.
    - forgetting_rate: [0, 1] Rate at which stored Q-values decay toward 0 each trial.
    """
    learning_rate, beta, w, lambda_coef, stickiness, forgetting_rate = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    
    last_action_1 = -1

    for trial in range(n_trials):
        # Decay all Q-values (Memory Leak) before making choices
        q_stage1_mf *= (1 - forgetting_rate)
        q_stage2_mf *= (1 - forgetting_rate)

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits = beta * q_net
        if last_action_1 != -1:
            logits[last_action_1] += stickiness
            
        exp_q1 = np.exp(logits - np.max(logits))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_action_1 = action_1[trial]
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        q_s2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * q_s2 - np.max(beta * q_s2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updating ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]

        q_stage1_mf[action_1[trial]] += learning_rate * (delta_stage1 + lambda_coef * delta_stage2)
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Agent with Subjective Transition Belief.
    
    This model assumes the participant may not believe or strictly use the true 
    transition probabilities (0.7/0.3). Instead, they use a subjective 
    transition probability 'trans_prob' for Model-Based planning. This captures 
    potential misconceptions about the task structure (e.g., treating it as random 
    or deterministic).

    Parameters:
    - learning_rate: [0, 1] Rate at which Q-values are updated.
    - beta: [0, 10] Inverse temperature for softmax choice.
    - w: [0, 1] Weighting parameter for Model-Based influence.
    - lambda_coef: [0, 1] Eligibility trace parameter.
    - stickiness: [0, 10] Choice perseveration bonus.
    - trans_prob: [0, 1] Subjective probability of the common transition (Spaceship A->X, U->Y).
    """
    learning_rate, beta, w, lambda_coef, stickiness, trans_prob = model_parameters
    n_trials = len(action_1)
  
    # Construct subjective transition matrix based on the parameter
    transition_matrix = np.array([[trans_prob, 1-trans_prob], [1-trans_prob, trans_prob]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits = beta * q_net
        if last_action_1 != -1:
            logits[last_action_1] += stickiness
            
        exp_q1 = np.exp(logits - np.max(logits))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_action_1 = action_1[trial]
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        q_s2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * q_s2 - np.max(beta * q_s2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updating ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]

        q_stage1_mf[action_1[trial]] += learning_rate * (delta_stage1 + lambda_coef * delta_stage2)
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Agent with Outcome-Dependent Stickiness.
    
    This model differentiates between perseveration after a reward (Win-Stay) and 
    perseveration after no reward (Lose-Stay). It replaces the single stickiness 
    parameter with 'stickiness_win' and 'stickiness_lose', allowing the model 
    to capture asymmetric behavioral inertia based on the previous trial's outcome.

    Parameters:
    - learning_rate: [0, 1] Rate at which Q-values are updated.
    - beta: [0, 10] Inverse temperature for softmax choice.
    - w: [0, 1] Weighting parameter for Model-Based influence.
    - lambda_coef: [0, 1] Eligibility trace parameter.
    - stickiness_win: [0, 10] Bonus added to repeating the previous choice if it was rewarded.
    - stickiness_lose: [0, 10] Bonus added to repeating the previous choice if it was unrewarded.
    """
    learning_rate, beta, w, lambda_coef, stickiness_win, stickiness_lose = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    last_reward = 0

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits = beta * q_net
        if last_action_1 != -1:
            if last_reward == 1:
                logits[last_action_1] += stickiness_win
            else:
                logits[last_action_1] += stickiness_lose
            
        exp_q1 = np.exp(logits - np.max(logits))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_action_1 = action_1[trial]
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        q_s2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * q_s2 - np.max(beta * q_s2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updating ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]

        q_stage1_mf[action_1[trial]] += learning_rate * (delta_stage1 + lambda_coef * delta_stage2)
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        last_reward = reward[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss