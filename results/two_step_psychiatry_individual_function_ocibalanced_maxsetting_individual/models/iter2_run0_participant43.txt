Here are the 3 proposed cognitive models.

### Model 1: Hybrid MB/MF Model with Reward-Dependent Perseveration
This model hypothesizes that the participant's tendency to repeat a choice (perseveration) differs depending on whether the previous choice was rewarded or not ("Win-Stay, Lose-Shift" dynamics), integrated into the hybrid Reinforcement Learning framework.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF Model with Reward-Dependent Perseveration.
    
    Distinguishes between perseveration (stickiness) after a rewarded trial 
    versus an unrewarded trial. This captures the intuition that participants
    may be more likely to repeat a choice if it led to a reward.
    
    Parameters:
    - lr: [0, 1] Learning rate.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - w: [0, 1] Weighting parameter (0 = Pure MF, 1 = Pure MB).
    - pers_rew: [0, 1] Perseveration bonus after a rewarded trial.
    - pers_unrew: [0, 1] Perseveration bonus after an unrewarded trial.
    """
    lr, beta_1, beta_2, w, pers_rew, pers_unrew = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track previous action and reward for perseveration
    prev_action_1 = -1
    prev_reward = 0
    
    for trial in range(n_trials):
        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Apply reward-dependent perseveration
        if prev_action_1 != -1:
            if prev_reward == 1:
                q_net[prev_action_1] += pers_rew
            else:
                q_net[prev_action_1] += pers_unrew
            
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Record Stage 1 choice
        a1 = action_1[trial]
        prev_action_1 = a1
        
        # Stage 2 Policy
        state_idx = state[trial]
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # Updates
        a2 = action_2[trial]
        r = reward[trial]
        prev_reward = r
        
        # Stage 1 MF Update (TD(0))
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        # Stage 2 MF Update
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += lr * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid MB/MF Model with Choice Habit (Habit Kernel)
This model replaces the simple one-step perseveration with a "habit" trace that accumulates over time. This accounts for the participant's long streaks of choosing the same spaceship, as habits take time to build up and time to decay, unlike simple repetition.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF Model with Choice Habit (Habit Kernel).
    
    Instead of simple 1-step perseveration, this model maintains a 'habit' trace
    for Stage 1 actions that integrates past choices over time. This explains 
    long streaks of behavior better than simple perseveration.
    
    Parameters:
    - lr: [0, 1] Learning rate for Q-values.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - w: [0, 1] Weighting parameter (0 = Pure MF, 1 = Pure MB).
    - habit_lr: [0, 1] Learning rate (decay) for the habit trace.
    - habit_w: [0, 10] Weight of the habit strength in the decision.
    """
    lr, beta_1, beta_2, w, habit_lr, habit_w = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Habit trace for Stage 1 actions (initialized to 0)
    habit_trace = np.zeros(2)
    
    for trial in range(n_trials):
        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add habit bonus
        q_net += habit_w * habit_trace
            
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Stage 2 Policy
        state_idx = state[trial]
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # Updates
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]
        
        # Update Habit Trace
        # H(t+1) = H(t) + alpha_h * (Choice_vector - H(t))
        choice_vec = np.zeros(2)
        choice_vec[a1] = 1.0
        habit_trace += habit_lr * (choice_vec - habit_trace)
        
        # Stage 1 MF Update
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        # Stage 2 MF Update
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += lr * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid MB/MF Model with Eligibility Traces (Lambda)
This model introduces an eligibility trace parameter ($\lambda$) to the hybrid framework. This modulates the Model-Free update: $\lambda=0$ corresponds to pure TD learning (updating Stage 1 value based on Stage 2 value), while $\lambda=1$ corresponds to Monte Carlo updating (updating Stage 1 value based on the final reward).

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF Model with Eligibility Traces (Lambda) and Perseveration.
    
    Incorporates an eligibility trace parameter (lambda) that bridges the gap 
    between TD(0) (update from next state value) and Monte Carlo (update from reward)
    in the Model-Free component.
    
    Parameters:
    - lr: [0, 1] Learning rate.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - w: [0, 1] Weighting parameter (0 = Pure MF, 1 = Pure MB).
    - lam: [0, 1] Eligibility trace parameter (0 = TD(0), 1 = TD(1)).
    - pers: [0, 1] Perseveration bonus.
    """
    lr, beta_1, beta_2, w, lam, pers = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1
    
    for trial in range(n_trials):
        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if prev_action_1 != -1:
            q_net[prev_action_1] += pers
            
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Stage 2 Policy
        state_idx = state[trial]
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # Updates
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]
        prev_action_1 = a1
        
        # Stage 1 MF Update with Lambda
        # delta_1 = Q2 - Q1
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        
        # delta_2 = r - Q2
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        
        # Q1 += lr * (delta_1 + lambda * delta_2)
        q_stage1_mf[a1] += lr * (delta_stage1 + lam * delta_stage2)
        
        # Stage 2 MF Update
        q_stage2_mf[state_idx, a2] += lr * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```