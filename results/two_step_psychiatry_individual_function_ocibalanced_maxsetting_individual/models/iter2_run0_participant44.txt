Here are the three proposed cognitive models.

### Model 1: Hybrid Learner with Passive Decay
This model introduces a **decay parameter** for the second-stage Q-values. Given that the task description states probabilities change slowly over trials, this model assumes the participant forgets or discounts the value of unchosen aliens over time, causing their values to decay towards zero. This promotes re-exploration of neglected options or simply reflects memory limitations.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid learner with Passive Decay for unchosen Stage 2 options.
    
    Captures the participant's potential forgetting of unchosen alien values 
    or adaptation to the non-stationary nature of the reward probabilities.
    Unchosen Stage 2 Q-values decay towards 0 on each trial.

    Parameters:
    - learning_rate: [0,1] Learning rate for value updates.
    - beta: [0,10] Inverse temperature for softmax choice.
    - w: [0,1] Mixing weight for Model-Based (1) vs Model-Free (0).
    - decay: [0,1] Decay rate for unchosen Stage 2 values (Value *= (1-decay)).
    """
    learning_rate, beta, w, decay = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_s1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_s1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Value Updating ---
        # Stage 1 Update (SARSA-like)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Decay unchosen Stage 2 option
        unchosen_action = 1 - action_2[trial]
        q_stage2_mf[state_idx, unchosen_action] *= (1.0 - decay)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Learner with Dynamic Transition Learning
This model relaxes the assumption that the participant believes the transition matrix is fixed (0.7/0.3). Instead, the participant **learns the transition probabilities** ($P(State|Action)$) online based on observed transitions. This dynamic belief is then used for the Model-Based value calculation.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid learner with Dynamic Transition Learning.
    
    The participant updates their internal model of the spaceship-planet 
    transition probabilities based on experience, rather than using a fixed matrix.
    
    Parameters:
    - lr_value: [0,1] Learning rate for Q-values (Model-Free).
    - lr_transition: [0,1] Learning rate for updating the transition matrix.
    - beta: [0,10] Inverse temperature.
    - w: [0,1] Mixing weight for Model-Based (1) vs Model-Free (0).
    """
    lr_value, lr_transition, beta, w = model_parameters
    n_trials = len(action_1)
  
    # Initialize estimated transition matrix (start with prior or flat)
    # Using 0.7/0.3 as a starting prior similar to instructions
    est_transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        # Use the DYNAMIC estimated transition matrix for MB calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = est_transition_matrix @ max_q_stage2
        
        q_net_s1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_s1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Value Updating ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr_value * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr_value * delta_stage2
        
        # --- Transition Matrix Updating ---
        # Update P(State | Action_1)
        a1 = action_1[trial]
        observed_s = state[trial]
        
        # Move probability towards the observed transition
        est_transition_matrix[a1, observed_s] += lr_transition * (1 - est_transition_matrix[a1, observed_s])
        # Ensure probabilities sum to 1 (update the other state)
        est_transition_matrix[a1, 1 - observed_s] = 1.0 - est_transition_matrix[a1, observed_s]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Learner with Rare Transition Damping
This model hypothesizes that the participant might gate their Model-Free learning based on the "rareness" of the transition. When a **rare transition** occurs (e.g., Spaceship A going to Planet Y), the participant may suppress the Model-Free update of the first-stage spaceship value, preventing the "bad luck" of a rare transition from incorrectly devaluing a good spaceship.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid learner with Rare Transition Damping.
    
    Modulates the Model-Free update of Stage 1 values when a rare transition occurs.
    If a transition is rare, the learning rate for the Stage 1 update is scaled by 
    'rare_damping', allowing the agent to ignore or discount updates from 'unlucky' transitions.
    
    Parameters:
    - learning_rate: [0,1] Base learning rate.
    - beta: [0,10] Inverse temperature.
    - w: [0,1] Mixing weight (MB vs MF).
    - rare_damping: [0,1] Multiplier for learning rate on rare transitions (0=ignore, 1=full).
    """
    learning_rate, beta, w, rare_damping = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_s1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_s1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Value Updating ---
        
        # Determine if transition was common or rare
        # Common: 0->0 or 1->1 (based on fixed 0.7 probability diagonal)
        is_common = (action_1[trial] == state[trial])
        
        # Modulate learning rate for Stage 1 update
        current_lr_s1 = learning_rate if is_common else (learning_rate * rare_damping)
        
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += current_lr_s1 * delta_stage1
        
        # Stage 2 update uses standard learning rate
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```