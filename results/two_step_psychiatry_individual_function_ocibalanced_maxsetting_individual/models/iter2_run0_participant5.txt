Here are three cognitive models designed to explain the participant's behavior in the two-step task.

### Model 1: TD-Lambda with Separate Stage Learning Rates
This model extends the best-performing TD-Lambda approach by introducing separate learning rates for Stage 1 (Spaceship choice) and Stage 2 (Alien choice). The participant may learn the value of specific aliens (which drift over time) at a different rate than they learn the value of spaceships (which are more stable but depend on transitions).

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    TD-Lambda with Separate Learning Rates for Stage 1 and Stage 2.
    
    Parameters:
    - alpha1: [0, 1] Learning rate for Stage 1 (Spaceships).
    - alpha2: [0, 1] Learning rate for Stage 2 (Aliens).
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - lambd: [0, 1] Eligibility trace decay (0 = TD(0), 1 = Monte Carlo).
    """
    alpha1, alpha2, beta_1, beta_2, lambd = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = action_1[trial]
        s = state[trial]
        a2 = action_2[trial]
        r = reward[trial]
        
        if a1 == -1 or s == -1 or a2 == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        # Stage 1 Choice
        exp_q1 = np.exp(beta_1 * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2 Choice
        exp_q2 = np.exp(beta_2 * q_stage2[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Learning
        # Stage 1 update based on Stage 2 Q-value (SARSA-style)
        delta_stage1 = q_stage2[s, a2] - q_stage1[a1]
        q_stage1[a1] = q_stage1[a1] + alpha1 * delta_stage1
        
        # Stage 2 update based on reward
        delta_stage2 = r - q_stage2[s, a2]
        q_stage2[s, a2] = q_stage2[s, a2] + alpha2 * delta_stage2
        
        # Eligibility trace: Propagate Stage 2 RPE to Stage 1 value
        q_stage1[a1] = q_stage1[a1] + alpha1 * lambd * delta_stage2

    eps = 1e-10
    valid_mask = (p_choice_1 > 0) & (p_choice_2 > 0)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss
```

### Model 2: Dynamic Model-Based RL
Standard Model-Based RL assumes the participant knows the fixed transition probabilities (e.g., 0.7/0.3). This model assumes the participant **learns** the transition structure online. It maintains a dynamic transition matrix that updates after every trial, allowing the model-based values to evolve as the participant experiences common or rare transitions.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free RL with Dynamic Transition Learning.
    
    The agent learns the transition matrix between Stage 1 (Spaceships) and 
    Stage 2 (Planets) online using a delta rule, rather than assuming a fixed structure.
    
    Parameters:
    - alpha: [0, 1] Learning rate for Stage 2 (Alien) values and MF Stage 1 values.
    - eta: [0, 1] Learning rate for transition probabilities.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - w: [0, 1] Mixing weight for Stage 1 (0 = Pure MF, 1 = Pure MB).
    """
    alpha, eta, beta_1, beta_2, w = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Learned Transition Matrix: T[spaceship, planet]
    # Initialize as uniform (0.5) representing max entropy / no prior knowledge
    transition_matrix = np.ones((2, 2)) * 0.5

    for trial in range(n_trials):
        a1 = action_1[trial]
        s = state[trial]
        a2 = action_2[trial]
        r = reward[trial]

        if a1 == -1 or s == -1 or a2 == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        # Model-Based Value Calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Q-value (Hybrid)
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Stage 1 Choice
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2 Choice
        exp_q2 = np.exp(beta_2 * q_stage2_mf[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Updates
        
        # 1. Update Transition Matrix (Dynamic MB)
        # Move probability mass towards the observed transition planet s
        transition_matrix[a1, s] += eta * (1 - transition_matrix[a1, s])
        # Decrease probability for the unobserved planet 1-s
        transition_matrix[a1, 1-s] += eta * (0 - transition_matrix[a1, 1-s])
        
        # 2. MF Updates
        delta_stage1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha * delta_stage2

    eps = 1e-10
    valid_mask = (p_choice_1 > 0) & (p_choice_2 > 0)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss
```

### Model 3: TD-Lambda with Spaceship Bias
The participant data shows a strong, persistent preference for Spaceship 0 (A), even after failures. This model incorporates a static **bias** parameter into the TD-Lambda framework. This bias acts as a baseline preference for Spaceship 0 that is independent of the learned Q-values, helping to explain the high repetition rate of that choice.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    TD-Lambda with Spaceship Bias.
    
    Incorporates a static bias parameter for Spaceship 0 (A), capturing the 
    participant's baseline preference that persists regardless of value fluctuations.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - lambd: [0, 1] Eligibility trace decay.
    - bias: [-10, 10] Bias added to the logit of Spaceship 0. Positive values favor Spaceship 0.
    """
    learning_rate, beta_1, beta_2, lambd, bias = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = action_1[trial]
        s = state[trial]
        a2 = action_2[trial]
        r = reward[trial]

        if a1 == -1 or s == -1 or a2 == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        # Stage 1 Choice with Bias
        logits_1 = beta_1 * q_stage1
        logits_1[0] += bias  # Add bias specifically to Spaceship 0
        
        # Softmax with stability fix
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2 Choice
        exp_q2 = np.exp(beta_2 * q_stage2[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Learning (TD-Lambda)
        delta_stage1 = q_stage2[s, a2] - q_stage1[a1]
        q_stage1[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += learning_rate * delta_stage2
        
        q_stage1[a1] += learning_rate * lambd * delta_stage2

    eps = 1e-10
    valid_mask = (p_choice_1 > 0) & (p_choice_2 > 0)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss
```