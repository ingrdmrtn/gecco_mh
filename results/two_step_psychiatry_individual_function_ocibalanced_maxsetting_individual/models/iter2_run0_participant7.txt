Here are three new cognitive models for the two-step decision task.

### Cognitive Model 1
This model introduces **separate stickiness parameters** for Stage 1 (spaceship choice) and Stage 2 (alien choice). The participant data shows long "runs" of choosing the same spaceship, while Stage 2 choices seem more sensitive to recent rewards. Separating these perseveration tendencies allows the model to capture the strong habit formation in the first stage without compromising the flexibility required for learning in the second stage.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with separate stickiness parameters for Stage 1 and Stage 2.
    Allows the model to capture different levels of perseveration for spaceship choice
    versus alien choice.
    
    Parameters:
    - learning_rate: Rate at which Q-values are updated [0, 1]
    - beta_1: Inverse temperature for Stage 1 softmax [0, 10]
    - beta_2: Inverse temperature for Stage 2 softmax [0, 10]
    - w: Mixing weight between Model-Based and Model-Free values [0, 1]
    - lam: Eligibility trace parameter [0, 1]
    - stick_1: Perseverance bonus for repeating Stage 1 choice [0, 10]
    - stick_2: Perseverance bonus for repeating Stage 2 choice (within state) [0, 10]
    """
    learning_rate, beta_1, beta_2, w, lam, stick_1, stick_2 = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Initialize previous actions for stickiness (-1 indicates no previous action)
    last_a1 = -1
    last_a2 = np.array([-1, -1]) # Track last choice for each state (planet)

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add Stage 1 stickiness bonus
        stick_bonus_1 = np.zeros(2)
        if last_a1 != -1:
            stick_bonus_1[last_a1] = stick_1
            
        exp_q1 = np.exp(beta_1 * (q_net_1 + stick_bonus_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Update last Stage 1 action
        last_a1 = action_1[trial]
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        # Add Stage 2 stickiness (specific to the current state/planet)
        stick_bonus_2 = np.zeros(2)
        if last_a2[state_idx] != -1:
            stick_bonus_2[last_a2[state_idx]] = stick_2

        exp_q2 = np.exp(beta_2 * (q_stage2_mf[state_idx] + stick_bonus_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # Update last Stage 2 action for this state
        last_a2[state_idx] = action_2[trial]

        # --- Learning ---
        # Stage 1 PE
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Stage 2 PE
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Eligibility trace update for Stage 1
        q_stage1_mf[action_1[trial]] += learning_rate * lam * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2
This model implements a **Choice Kernel** mechanism for Stage 1. Instead of simple 1-back stickiness, the model maintains a decaying trace of past choices. This accounts for the "momentum" observed in the participant's behavior, where they stick to one spaceship for many trials. The `ck_decay` parameter controls how fast this habit forms and fades, while `ck_weight` determines its influence relative to value-based learning.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Choice Kernel for Stage 1.
    Uses a decaying trace of past choices to model habit formation (stickiness),
    rather than just the immediately preceding choice.
    
    Parameters:
    - learning_rate: Rate at which Q-values are updated [0, 1]
    - beta_1: Inverse temperature for Stage 1 softmax [0, 10]
    - beta_2: Inverse temperature for Stage 2 softmax [0, 10]
    - w: Mixing weight between Model-Based and Model-Free values [0, 1]
    - lam: Eligibility trace parameter [0, 1]
    - ck_decay: Decay rate of the choice kernel trace [0, 1]
    - ck_weight: Weight of the choice kernel in Stage 1 decision [0, 10]
    """
    learning_rate, beta_1, beta_2, w, lam, ck_decay, ck_weight = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Choice kernel trace for Stage 1
    ck_trace = np.zeros(2)

    for trial in range(n_trials):

        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add Choice Kernel to Stage 1 logits
        logits_1 = beta_1 * q_net_1 + ck_weight * ck_trace
        
        # Softmax with stability fix
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Update Choice Kernel (Exponential Moving Average)
        ck_trace = (1 - ck_decay) * ck_trace
        ck_trace[action_1[trial]] += ck_decay * 1.0

        state_idx = state[trial]

        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # Learning
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        q_stage1_mf[action_1[trial]] += learning_rate * lam * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3
This model combines **Asymmetric Learning Rates** with **Passive Decay**. It builds on the success of the previous "Decay" model but refines the learning process by allowing different learning rates for positive (reward) and negative (no reward) prediction errors. This is motivated by the observation that participants may learn differently from gains versus losses, while the decay mechanism handles the forgetting of unchosen options.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Asymmetric Learning Rates and Passive Decay.
    Updates Q-values differently for positive vs negative prediction errors,
    and decays unchosen Stage 2 values to model forgetting.
    
    Parameters:
    - alpha_pos: Learning rate for positive prediction errors [0, 1]
    - alpha_neg: Learning rate for negative prediction errors [0, 1]
    - beta_1: Inverse temperature for Stage 1 softmax [0, 10]
    - beta_2: Inverse temperature for Stage 2 softmax [0, 10]
    - w: Mixing weight between Model-Based and Model-Free values [0, 1]
    - lam: Eligibility trace parameter [0, 1]
    - decay: Decay rate for unchosen Q-values in Stage 2 [0, 1]
    """
    alpha_pos, alpha_neg, beta_1, beta_2, w, lam, decay = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_1 * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        lr_1 = alpha_pos if delta_stage1 > 0 else alpha_neg
        q_stage1_mf[action_1[trial]] += lr_1 * delta_stage1
        
        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        lr_2 = alpha_pos if delta_stage2 > 0 else alpha_neg
        q_stage2_mf[state_idx, action_2[trial]] += lr_2 * delta_stage2
        
        # Lambda Update (Stage 1 driven by Stage 2 error)
        # We use the learning rate corresponding to the sign of the Stage 2 error
        q_stage1_mf[action_1[trial]] += lr_2 * lam * delta_stage2
        
        # Decay unchosen options in Stage 2
        for s in range(2):
            for a in range(2):
                if not (s == state_idx and a == action_2[trial]):
                    q_stage2_mf[s, a] = (1 - decay) * q_stage2_mf[s, a] + decay * 0.5

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```