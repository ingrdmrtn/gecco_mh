Here are three new cognitive models for the two-step decision task.

### Model 1: Asymmetric Hybrid Model with Eligibility Traces
This model integrates the **asymmetric learning rates** (which performed well in previous iterations) with **eligibility traces** ($\lambda$). This combination allows the model to capture both valence-dependent learning (optimism/pessimism bias) and the direct reinforcement of Stage 1 choices by Stage 2 outcomes (a hallmark of Model-Free learning in multi-step tasks).

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Asymmetric Hybrid Model with Eligibility Traces.
    
    Combines asymmetric learning rates (valence-dependent learning) with an 
    eligibility trace (lambda). This allows Stage 2 rewards to directly influence 
    Stage 1 Model-Free values, scaled by lambda, while also accounting for 
    different sensitivities to positive vs negative prediction errors.
    
    Parameters:
    lr_pos: [0,1] - Learning rate for positive prediction errors (better than expected).
    lr_neg: [0,1] - Learning rate for negative prediction errors (worse than expected).
    beta: [0,10] - Inverse temperature (randomness) for both stages.
    w: [0,1] - Weighting parameter (0 = Pure MF, 1 = Pure MB).
    lam: [0,1] - Eligibility trace parameter (0 = TD(0), 1 = Monte Carlo).
    stickiness: [0,5] - Choice perseveration bonus for Stage 1.
    """
    lr_pos, lr_neg, beta, w, lam, stickiness = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based Value: Expected value of next stage
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Value mixing MB and MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Stickiness
        if prev_a1 != -1:
            q_net[prev_a1] += stickiness
            
        # Softmax Stage 1
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        prev_a1 = action_1[trial]
        state_idx = state[trial]
        
        # --- Stage 2 Choice ---
        # Softmax Stage 2
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        # Prediction Errors
        # Stage 1 PE: difference between Stage 2 value and Stage 1 expectation
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        # Stage 2 PE: difference between Reward and Stage 2 expectation
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Determine Learning Rates based on sign of PE
        lr_1 = lr_pos if delta_stage1 >= 0 else lr_neg
        lr_2 = lr_pos if delta_stage2 >= 0 else lr_neg
        
        # Update Stage 1 MF (TD(lambda))
        # Q1 is updated by its own error AND the error from stage 2 (scaled by lambda)
        q_stage1_mf[action_1[trial]] += lr_1 * delta_stage1 + lam * lr_2 * delta_stage2
        
        # Update Stage 2 MF
        q_stage2_mf[state_idx, action_2[trial]] += lr_2 * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model with Decay (Forgetting)
This model addresses the non-stationary nature of the task (probabilities changing slowly) by implementing a **decay mechanism**. Unchosen actions in both stages slowly decay towards zero, allowing the agent to "forget" outdated information and preventing them from getting stuck on old estimates.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Decay (Forgetting).
    
    Incorporates a decay parameter that slowly reduces the value of unchosen actions 
    towards zero. This is particularly effective for environments where reward 
    probabilities drift slowly over time (non-stationary), preventing the agent 
    from relying on outdated estimates.
    
    Parameters:
    lr: [0,1] - Learning rate for chosen options.
    decay: [0,1] - Decay rate for unchosen options (0 = no decay).
    beta: [0,10] - Inverse temperature.
    w: [0,1] - Weighting parameter (0 = Pure MF, 1 = Pure MB).
    stickiness: [0,5] - Choice perseveration bonus.
    """
    lr, decay, beta, w, stickiness = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if prev_a1 != -1:
            q_net[prev_a1] += stickiness
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        prev_a1 = action_1[trial]
        state_idx = state[trial]
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        chosen_a1 = action_1[trial]
        chosen_a2 = action_2[trial]
        
        # Calculate PEs using current values
        delta_stage1 = q_stage2_mf[state_idx, chosen_a2] - q_stage1_mf[chosen_a1]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, chosen_a2]
        
        # Update Chosen Options
        q_stage1_mf[chosen_a1] += lr * delta_stage1
        q_stage2_mf[state_idx, chosen_a2] += lr * delta_stage2
        
        # Decay Unchosen Options
        # Stage 1
        q_stage1_mf[1 - chosen_a1] *= (1 - decay)
        
        # Stage 2: Decay all options that were NOT chosen in the current state
        # (Global decay for unvisited states to handle drifting probabilities)
        for s in range(2):
            for a in range(2):
                if not (s == state_idx and a == chosen_a2):
                    q_stage2_mf[s, a] *= (1 - decay)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Dynamic Model-Based Hybrid
This model relaxes the assumption that the participant uses a fixed transition matrix. Instead, it **learns the transition probabilities** (Structural Learning) trial-by-trial. This allows the Model-Based system to adapt if the participant's internal model of the spaceship-planet transitions evolves or fluctuates.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Dynamic Model-Based Hybrid.
    
    Unlike standard models that assume a fixed transition matrix, this model 
    dynamically learns the transition probabilities between Stage 1 and Stage 2.
    It combines this evolving Model-Based value (Structural Learning) with a 
    standard Model-Free system.
    
    Parameters:
    lr_rew: [0,1] - Learning rate for reward prediction (MF values).
    lr_trans: [0,1] - Learning rate for transition probability estimation.
    beta: [0,10] - Inverse temperature.
    w: [0,1] - Weighting parameter (0 = Pure MF, 1 = Pure MB).
    stickiness: [0,5] - Choice perseveration bonus.
    """
    lr_rew, lr_trans, beta, w, stickiness = model_parameters
    n_trials = len(action_1)
    
    # Initialize transitions (Start with prior knowledge 0.7/0.3)
    # Row 0: A -> [X, Y], Row 1: U -> [X, Y]
    trans_probs = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # MB Calculation using DYNAMIC transition matrix
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = trans_probs @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if prev_a1 != -1:
            q_net[prev_a1] += stickiness
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        prev_a1 = action_1[trial]
        state_idx = state[trial]
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        # 1. Transition Learning (Structural Learning)
        # Observed transition: action_1[trial] -> state_idx
        # Update P(observed state | action) towards 1
        curr_prob = trans_probs[action_1[trial], state_idx]
        trans_probs[action_1[trial], state_idx] += lr_trans * (1 - curr_prob)
        # Ensure row sums to 1 by updating the complement
        trans_probs[action_1[trial], 1 - state_idx] = 1.0 - trans_probs[action_1[trial], state_idx]
        
        # 2. Reward Learning (MF)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        q_stage1_mf[action_1[trial]] += lr_rew * delta_stage1
        q_stage2_mf[state_idx, action_2[trial]] += lr_rew * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```