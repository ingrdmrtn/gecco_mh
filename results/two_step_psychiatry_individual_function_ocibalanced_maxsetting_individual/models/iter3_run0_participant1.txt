def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Separate Learning Rates and Stickiness.
    
    This model distinguishes between the learning speed for the first stage (spaceships) 
    and the second stage (aliens). It posits that the participant might update 
    high-level preferences (spaceships) at a different rate than low-level associations (aliens),
    while maintaining hybrid Model-Based/Model-Free arbitration and choice stickiness.
    
    Parameters:
    lr_stage1: [0, 1] - Learning rate for Stage 1 Q-values (Spaceships).
    lr_stage2: [0, 1] - Learning rate for Stage 2 Q-values (Aliens).
    beta: [0, 10] - Inverse temperature for softmax choice (exploration/exploitation).
    w: [0, 1] - Mixing weight (0 = pure Model-Free, 1 = pure Model-Based).
    stickiness: [0, 5] - Choice perseverance bonus added to the previously chosen spaceship.
    """
    lr_stage1, lr_stage2, beta, w, stickiness = model_parameters
    n_trials = len(action_1)
    
    # Transition matrix: Row 0 -> [P(Planet0|Space0), P(Planet1|Space0)]
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_mf_stage1 = np.zeros(2)
    q_mf_stage2 = np.zeros((2, 2)) # State (0,1) x Action (0,1)
    
    last_action_1 = -1
    log_loss = 0.0
    eps = 1e-10
    
    for trial in range(n_trials):
        a1 = action_1[trial]
        s_idx = state[trial]
        a2 = action_2[trial]
        r = reward[trial]

        # --- Stage 1 Decision ---
        # Model-Based Value: Expected max value of next stage
        max_q_stage2 = np.max(q_mf_stage2, axis=1)
        q_mb_stage1 = transition_matrix @ max_q_stage2
        
        # Hybrid Value Integration
        q_net_stage1 = w * q_mb_stage1 + (1 - w) * q_mf_stage1
        
        # Apply Stickiness
        if last_action_1 != -1:
            q_net_stage1[last_action_1] += stickiness
            
        # Softmax Probability for Stage 1
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + eps)
        
        # --- Stage 2 Decision ---
        # Softmax Probability for Stage 2
        exp_q2 = np.exp(beta * q_mf_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        # Only update if a valid second stage choice was made
        if a2 != -1:
            log_loss -= np.log(probs_2[a2] + eps)
            
            # --- Learning ---
            # Stage 2 Update (TD(0)) using lr_stage2
            delta_2 = r - q_mf_stage2[s_idx, a2]
            q_mf_stage2[s_idx, a2] += lr_stage2 * delta_2
            
            # Stage 1 Update (TD(1)) using lr_stage1
            # Updates spaceship value based on the final reward obtained
            q_mf_stage1[a1] += lr_stage1 * (r - q_mf_stage1[a1])
            
        last_action_1 = a1
        
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Asymmetric Learning Rates and Stickiness.
    
    This model implements different learning rates for positive prediction errors (rewards) 
    and negative prediction errors (omissions). This captures potential biases where 
    the participant might learn more from winning than losing (or vice versa), 
    combined with hybrid planning and stickiness.
    
    Parameters:
    alpha_pos: [0, 1] - Learning rate for positive prediction errors (Reward > Expectation).
    alpha_neg: [0, 1] - Learning rate for negative prediction errors (Reward < Expectation).
    beta: [0, 10] - Inverse temperature for softmax choice.
    w: [0, 1] - Mixing weight (0 = pure Model-Free, 1 = pure Model-Based).
    stickiness: [0, 5] - Choice perseverance bonus for Stage 1.
    """
    alpha_pos, alpha_neg, beta, w, stickiness = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_mf_stage1 = np.zeros(2)
    q_mf_stage2 = np.zeros((2, 2))
    
    last_action_1 = -1
    log_loss = 0.0
    eps = 1e-10
    
    for trial in range(n_trials):
        a1 = action_1[trial]
        s_idx = state[trial]
        a2 = action_2[trial]
        r = reward[trial]

        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_mf_stage2, axis=1)
        q_mb_stage1 = transition_matrix @ max_q_stage2
        q_net_stage1 = w * q_mb_stage1 + (1 - w) * q_mf_stage1
        
        if last_action_1 != -1:
            q_net_stage1[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + eps)
        
        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_mf_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        if a2 != -1:
            log_loss -= np.log(probs_2[a2] + eps)
            
            # --- Learning ---
            # Stage 2 Prediction Error and Update
            pe_2 = r - q_mf_stage2[s_idx, a2]
            lr_2 = alpha_pos if pe_2 > 0 else alpha_neg
            q_mf_stage2[s_idx, a2] += lr_2 * pe_2
            
            # Stage 1 Prediction Error and Update (TD(1))
            pe_1 = r - q_mf_stage1[a1]
            lr_1 = alpha_pos if pe_1 > 0 else alpha_neg
            q_mf_stage1[a1] += lr_1 * pe_1
            
        last_action_1 = a1
        
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Separate Betas and Stickiness.
    
    This model allows for different levels of decision noise (exploration/exploitation) 
    at Stage 1 (spaceships) versus Stage 2 (aliens). It accounts for the possibility 
    that the participant is more precise/consistent in one stage than the other.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate for Q-values.
    beta_1: [0, 10] - Inverse temperature for Stage 1 choice.
    beta_2: [0, 10] - Inverse temperature for Stage 2 choice.
    w: [0, 1] - Mixing weight (0 = pure Model-Free, 1 = pure Model-Based).
    stickiness: [0, 5] - Choice perseverance bonus for Stage 1.
    """
    learning_rate, beta_1, beta_2, w, stickiness = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_mf_stage1 = np.zeros(2)
    q_mf_stage2 = np.zeros((2, 2))
    
    last_action_1 = -1
    log_loss = 0.0
    eps = 1e-10
    
    for trial in range(n_trials):
        a1 = action_1[trial]
        s_idx = state[trial]
        a2 = action_2[trial]
        r = reward[trial]

        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_mf_stage2, axis=1)
        q_mb_stage1 = transition_matrix @ max_q_stage2
        q_net_stage1 = w * q_mb_stage1 + (1 - w) * q_mf_stage1
        
        if last_action_1 != -1:
            q_net_stage1[last_action_1] += stickiness
            
        # Use beta_1 for Stage 1
        exp_q1 = np.exp(beta_1 * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + eps)
        
        # --- Stage 2 Decision ---
        # Use beta_2 for Stage 2
        exp_q2 = np.exp(beta_2 * q_mf_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        if a2 != -1:
            log_loss -= np.log(probs_2[a2] + eps)
            
            # --- Learning ---
            # Standard updates with single learning rate
            q_mf_stage2[s_idx, a2] += learning_rate * (r - q_mf_stage2[s_idx, a2])
            q_mf_stage1[a1] += learning_rate * (r - q_mf_stage1[a1])
            
        last_action_1 = a1
        
    return log_loss