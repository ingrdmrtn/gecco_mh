Here are three new cognitive models for the two-step task, designed to capture the specific behaviors observed in the participant data (e.g., choice stickiness and potential asymmetric learning).

### Model 1: Asymmetric TD($\lambda$) Learner with Perseveration
This model extends the successful TD($\lambda$) approach by introducing **asymmetric learning rates** for positive and negative prediction errors. This allows the model to weigh "wins" (better-than-expected outcomes) differently from "losses" (worse-than-expected outcomes), a mechanism often relevant in clinical populations.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Asymmetric TD(lambda) Learner with Perseveration.
    
    Distinguishes between positive and negative prediction errors with separate learning rates,
    allowing for different sensitivity to better-than-expected vs worse-than-expected outcomes.
    
    Parameters:
    lr_pos:        [0, 1] Learning rate for positive prediction errors.
    lr_neg:        [0, 1] Learning rate for negative prediction errors.
    beta:          [0, 10] Inverse temperature (softness of choice).
    lambda_param:  [0, 1] Eligibility trace decay parameter.
    perseveration: [0, 5] Stickiness bonus for repeating the previous Stage 1 choice.
    """
    lr_pos, lr_neg, beta, lambda_param, perseveration = model_parameters
    n_trials = len(action_1)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)      
    q_stage2 = np.zeros((2, 2)) 
    
    prev_action_1 = -1
    
    for trial in range(n_trials):
        # Handle missing data
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue
            
        # Stage 1 Choice (Spaceship)
        q_stage1_eff = q_stage1.copy()
        if prev_action_1 != -1:
            q_stage1_eff[prev_action_1] += perseveration
            
        exp_q1 = np.exp(beta * q_stage1_eff)
        # Numerical stability
        if np.any(np.isinf(exp_q1)):
             max_val = np.max(beta * q_stage1_eff)
             exp_q1 = np.exp(beta * q_stage1_eff - max_val)
        
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial] # Planet 0 or 1
        
        # Stage 2 Choice (Alien)
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        if np.any(np.isinf(exp_q2)):
             max_val = np.max(beta * q_stage2[state_idx])
             exp_q2 = np.exp(beta * q_stage2[state_idx] - max_val)
             
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # Updates
        # TD(0) error for Stage 1: (Value of State 2) - (Value of Action 1)
        delta_stage1 = q_stage2[state_idx, action_2[trial]] - q_stage1[action_1[trial]]
        
        # TD(0) error for Stage 2: Reward - (Value of Action 2)
        delta_stage2 = reward[trial] - q_stage2[state_idx, action_2[trial]]
        
        # Update Stage 2 Q-values using asymmetric learning rates
        lr_s2 = lr_pos if delta_stage2 > 0 else lr_neg
        q_stage2[state_idx, action_2[trial]] += lr_s2 * delta_stage2
        
        # Update Stage 1 Q-values (TD(lambda))
        # The effective error propagated back is delta1 + lambda * delta2
        combined_delta = delta_stage1 + lambda_param * delta_stage2
        lr_s1 = lr_pos if combined_delta > 0 else lr_neg
        q_stage1[action_1[trial]] += lr_s1 * combined_delta
        
        prev_action_1 = action_1[trial]
        
    eps = 1e-10
    valid_trials = (action_1 != -1) & (state != -1) & (action_2 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```

### Model 2: Hybrid Model with Separate Learning Rates and Perseveration
This model posits that the participant learns at different speeds for the two stages. For example, they might rapidly update their estimates of alien rewards (Stage 2) while being slow to change their spaceship preference (Stage 1). It combines this with a Model-Based (planning) component and choice perseveration.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model (MB/MF) with Separate Learning Rates and Perseveration.
    
    Combines Model-Based planning with Model-Free learning, using distinct learning rates
    for Stage 1 (Spaceships) and Stage 2 (Aliens) to capture different adaptation speeds.
    
    Parameters:
    lr_stage1:     [0, 1] Learning rate for Stage 1 Model-Free values.
    lr_stage2:     [0, 1] Learning rate for Stage 2 Model-Free values.
    beta:          [0, 10] Inverse temperature.
    w:             [0, 1] Weight of Model-Based values (0 = Pure MF, 1 = Pure MB).
    perseveration: [0, 5] Stickiness bonus for Stage 1.
    """
    lr_stage1, lr_stage2, beta, w, perseveration = model_parameters
    n_trials = len(action_1)
    
    # Transition matrix: A(0)->X(0) 0.7, U(1)->Y(1) 0.7
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1
    
    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue
            
        # Model-Based Values for Stage 1: T * max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1) # Max value of each planet
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combined Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add perseveration
        if prev_action_1 != -1:
            q_net[prev_action_1] += perseveration
            
        exp_q1 = np.exp(beta * q_net)
        if np.any(np.isinf(exp_q1)):
             max_val = np.max(beta * q_net)
             exp_q1 = np.exp(beta * q_net - max_val)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        
        # Stage 2 Choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        if np.any(np.isinf(exp_q2)):
             max_val = np.max(beta * q_stage2_mf[state_idx])
             exp_q2 = np.exp(beta * q_stage2_mf[state_idx] - max_val)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # Updates
        # Stage 2 MF Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr_stage2 * delta_stage2
        
        # Stage 1 MF Update (TD(0))
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr_stage1 * delta_stage1
        
        prev_action_1 = action_1[trial]
        
    eps = 1e-10
    valid_trials = (action_1 != -1) & (state != -1) & (action_2 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```

### Model 3: Model-Free Learner with Forgetting (Decay) and Perseveration
This model tests the hypothesis that the participant "forgets" the value of options they haven't chosen recently. Unchosen actions decay towards a neutral value (0.5), which can induce switching or staying depending on whether the current option is performing above or below neutral.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Learner with Forgetting (Decay) and Perseveration.
    
    Standard Q-learning where unchosen action values decay towards a neutral prior (0.5),
    simulating memory loss or uncertainty-driven regression to the mean.
    
    Parameters:
    lr:            [0, 1] Learning rate for chosen actions.
    beta:          [0, 10] Inverse temperature.
    decay:         [0, 1] Decay rate for unchosen actions (0 = no decay, 1 = instant reset).
    perseveration: [0, 5] Stickiness bonus for Stage 1.
    """
    lr, beta, decay, perseveration = model_parameters
    n_trials = len(action_1)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Initialize to neutral (0.5) since rewards are 0 or 1
    q_stage1 = np.zeros(2) + 0.5 
    q_stage2 = np.zeros((2, 2)) + 0.5
    
    prev_action_1 = -1
    neutral_val = 0.5
    
    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue
            
        # Stage 1 Choice
        q_stage1_eff = q_stage1.copy()
        if prev_action_1 != -1:
            q_stage1_eff[prev_action_1] += perseveration
            
        exp_q1 = np.exp(beta * q_stage1_eff)
        if np.any(np.isinf(exp_q1)):
             max_val = np.max(beta * q_stage1_eff)
             exp_q1 = np.exp(beta * q_stage1_eff - max_val)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        
        # Stage 2 Choice
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        if np.any(np.isinf(exp_q2)):
             max_val = np.max(beta * q_stage2[state_idx])
             exp_q2 = np.exp(beta * q_stage2[state_idx] - max_val)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # Updates
        # Stage 1 Update: Moves towards the value of the state reached (SARSA-like)
        target_s1 = q_stage2[state_idx, action_2[trial]]
        q_stage1[action_1[trial]] += lr * (target_s1 - q_stage1[action_1[trial]])
        
        # Decay unchosen Stage 1
        unchosen_s1 = 1 - action_1[trial]
        q_stage1[unchosen_s1] = (1 - decay) * q_stage1[unchosen_s1] + decay * neutral_val
        
        # Stage 2 Update
        q_stage2[state_idx, action_2[trial]] += lr * (reward[trial] - q_stage2[state_idx, action_2[trial]])
        
        # Decay unchosen Stage 2 (in the current state only)
        unchosen_s2 = 1 - action_2[trial]
        q_stage2[state_idx, unchosen_s2] = (1 - decay) * q_stage2[state_idx, unchosen_s2] + decay * neutral_val
        
        prev_action_1 = action_1[trial]
        
    eps = 1e-10
    valid_trials = (action_1 != -1) & (state != -1) & (action_2 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```