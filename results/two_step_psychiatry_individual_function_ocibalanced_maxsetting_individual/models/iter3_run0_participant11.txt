Here are three new cognitive models for the two-step decision task.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Q-Learning with Reward-Dependent Eligibility Traces.
    
    This model modifies the standard TD(lambda) algorithm by allowing the eligibility trace (lambda)
    to vary based on the outcome of the trial. The hypothesis is that participants may credit
    their first-stage choice differently depending on whether it resulted in a reward or an omission.
    For instance, a reward might strongly reinforce the path taken (high lambda), whereas a lack 
    of reward might lead to a weaker association (low lambda).

    Parameters:
    - learning_rate: [0, 1] Rate at which Q-values are updated.
    - beta_1: [0, 10] Inverse temperature for Stage 1 choice stochasticity.
    - beta_2: [0, 10] Inverse temperature for Stage 2 choice stochasticity.
    - lambda_pos: [0, 1] Eligibility trace decay when the outcome is a Reward (1).
    - lambda_neg: [0, 1] Eligibility trace decay when the outcome is No Reward (0).
    - stickiness: [0, 5] Perseveration bonus for Stage 1 choices.
    """
    learning_rate, beta_1, beta_2, lambda_pos, lambda_neg, stickiness = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2)) # state x action
    
    last_action_1 = -1

    for trial in range(n_trials):
        # Stage 1 Policy
        q_net_1 = q_stage1.copy()
        if last_action_1 != -1:
            q_net_1[last_action_1] += stickiness
        
        exp_q1 = np.exp(beta_1 * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_action_1 = action_1[trial]
        state_idx = state[trial]
        
        # Stage 2 Policy
        exp_q2 = np.exp(beta_2 * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # TD Updates
        r = reward[trial]
        delta_1 = q_stage2[state_idx, action_2[trial]] - q_stage1[action_1[trial]]
        delta_2 = r - q_stage2[state_idx, action_2[trial]]
        
        # Select lambda based on reward
        current_lambda = lambda_pos if r == 1 else lambda_neg
        
        q_stage1[action_1[trial]] += learning_rate * (delta_1 + current_lambda * delta_2)
        q_stage2[state_idx, action_2[trial]] += learning_rate * delta_2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Q-Learning with Transition-Dependent Learning Rates.
    
    This model posits that the participant modulates their learning update for the first stage
    based on the type of transition experienced (Common vs. Rare). 
    If the transition was Common (expected), the update to the spaceship's value might be 
    standard. If the transition was Rare (unexpected), the participant might discount the 
    update (lower learning rate) or treat it differently, effectively filtering noise 
    caused by the probabilistic transition structure.
    
    Parameters:
    - lr_common: [0, 1] Learning rate for Stage 1 updates after a Common transition.
    - lr_rare: [0, 1] Learning rate for Stage 1 updates after a Rare transition.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - lambda_param: [0, 1] Eligibility trace decay.
    - stickiness: [0, 5] Perseveration bonus for Stage 1.
    """
    lr_common, lr_rare, beta_1, beta_2, lambda_param, stickiness = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        # Stage 1 Policy
        q_net_1 = q_stage1.copy()
        if last_action_1 != -1:
            q_net_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta_1 * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_action_1 = action_1[trial]
        state_idx = state[trial]
        
        # Stage 2 Policy
        exp_q2 = np.exp(beta_2 * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # TD Updates
        delta_1 = q_stage2[state_idx, action_2[trial]] - q_stage1[action_1[trial]]
        delta_2 = reward[trial] - q_stage2[state_idx, action_2[trial]]
        
        # Determine transition type (Assuming 0->0 and 1->1 are Common based on task structure)
        is_common = (action_1[trial] == state_idx)
        current_lr = lr_common if is_common else lr_rare
        
        # Update Stage 1 with transition-specific learning rate
        q_stage1[action_1[trial]] += current_lr * (delta_1 + lambda_param * delta_2)
        
        # Update Stage 2 with standard learning rate (using lr_common as baseline)
        q_stage2[state_idx, action_2[trial]] += lr_common * delta_2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Q-Learning with Dual-Stage Stickiness.
    
    Previous best models incorporated stickiness (perseveration) only at the first stage.
    This model adds a stickiness parameter for the second stage as well, allowing for 
    perseveration in choices of aliens within each planet. This captures the tendency 
    to repeat the same alien choice when revisiting a planet, independent of reward.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for Q-values.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - lambda_param: [0, 1] Eligibility trace decay.
    - stick_1: [0, 5] Perseveration bonus for Stage 1 (Spaceship).
    - stick_2: [0, 5] Perseveration bonus for Stage 2 (Alien).
    """
    learning_rate, beta_1, beta_2, lambda_param, stick_1, stick_2 = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    last_action_1 = -1
    # Track last action for each state (planet) to apply stickiness correctly at Stage 2
    last_action_2 = np.array([-1, -1]) 

    for trial in range(n_trials):
        # Stage 1 Policy
        q_net_1 = q_stage1.copy()
        if last_action_1 != -1:
            q_net_1[last_action_1] += stick_1
            
        exp_q1 = np.exp(beta_1 * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_action_1 = action_1[trial]
        state_idx = state[trial]
        
        # Stage 2 Policy
        q_net_2 = q_stage2[state_idx].copy()
        if last_action_2[state_idx] != -1:
            q_net_2[last_action_2[state_idx]] += stick_2
            
        exp_q2 = np.exp(beta_2 * q_net_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        last_action_2[state_idx] = action_2[trial]
        
        # TD Updates
        delta_1 = q_stage2[state_idx, action_2[trial]] - q_stage1[action_1[trial]]
        delta_2 = reward[trial] - q_stage2[state_idx, action_2[trial]]
        
        q_stage1[action_1[trial]] += learning_rate * (delta_1 + lambda_param * delta_2)
        q_stage2[state_idx, action_2[trial]] += learning_rate * delta_2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```