Here are three cognitive models implemented as Python functions.

### Model 1: Hybrid Model with Subjective Transition Belief
This model hypothesizes that the participant may not believe the transition probabilities are exactly 0.7/0.3, or they may treat the transitions as more deterministic (or more random) than they really are. A `trans_prob` parameter replaces the fixed 0.7 in the Model-Based calculation, allowing the model to capture the participant's subjective understanding of the spaceship-planet structure (e.g., a "max-prob" heuristic where they assume the common transition is guaranteed).

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF Model with Subjective Transition Belief.
    
    This model allows the participant's internal model of the transition matrix 
    to differ from the true probabilities (0.7/0.3).
    
    Parameters:
    lr (float): Learning rate for Q-values [0, 1].
    beta_1 (float): Inverse temperature for Stage 1 choices [0, 10].
    beta_2 (float): Inverse temperature for Stage 2 choices [0, 10].
    w (float): Weighting of Model-Based values (0=MF, 1=MB) [0, 1].
    lam (float): Eligibility trace parameter connecting Stage 2 to Stage 1 [0, 1].
    trans_prob (float): Subjective probability of the common transition [0, 1].
                        (e.g., 1.0 implies belief in deterministic transitions).
    """
    lr, beta_1, beta_2, w, lam, trans_prob = model_parameters
    n_trials = len(action_1)
    
    # Subjective transition matrix based on parameter
    # Row 0: Space 0 -> [Planet 0, Planet 1]
    # Row 1: Space 1 -> [Planet 0, Planet 1]
    # Assuming symmetry in structure belief
    transition_matrix = np.array([
        [trans_prob, 1.0 - trans_prob], 
        [1.0 - trans_prob, trans_prob]
    ])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    log_loss = 0
    eps = 1e-10
    
    for trial in range(n_trials):
        # -- Stage 1 Decision --
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = action_1[trial]
        if a1 != -1:
            log_loss -= np.log(probs_1[a1] + eps)
        else:
            # Missing data, skip update
            continue
            
        if action_2[trial] == -1:
            continue

        # -- Stage 2 Decision --
        state_idx = state[trial]
        a2 = action_2[trial]
        
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[a2] + eps)

        # -- Learning --
        r = reward[trial]
        if r == -1: r = 0 # Handle missing reward if necessary
        
        # Prediction errors
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        
        # Update Stage 1 MF value (with eligibility trace)
        q_stage1_mf[a1] += lr * delta_stage1 + lr * lam * delta_stage2
        
        # Update Stage 2 MF value (also serves as V for MB)
        q_stage2_mf[state_idx, a2] += lr * delta_stage2
        
    return log_loss
```

### Model 2: Hybrid Model with Dual-Stage Choice Trace
Building on the "Choice Trace" model which was identified as the best so far, this model extends the perseveration mechanism to the second stage (Aliens). The participant shows significant repetition in both spaceship choice and alien choice. This model maintains separate decaying memory traces for Stage 1 and Stage 2 choices, allowing for distinct stickiness weights (`trace_w_1`, `trace_w_2`) while sharing a decay rate (`trace_decay`) to keep the parameter count efficient.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF Model with Dual-Stage Choice Trace.
    
    Extends the choice trace mechanism to both Stage 1 (Spaceships) and 
    Stage 2 (Aliens), capturing perseveration or habits in both steps.
    
    Parameters:
    lr (float): Learning rate [0, 1].
    beta_1 (float): Inverse temperature Stage 1 [0, 10].
    beta_2 (float): Inverse temperature Stage 2 [0, 10].
    w (float): MB/MF mixing weight [0, 1].
    lam (float): Eligibility trace parameter [0, 1].
    trace_decay (float): Decay rate of choice traces [0, 1].
    trace_w_1 (float): Weight of choice trace in Stage 1 [0, 5].
    trace_w_2 (float): Weight of choice trace in Stage 2 [0, 5].
    """
    lr, beta_1, beta_2, w, lam, trace_decay, trace_w_1, trace_w_2 = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Traces: 
    # trace_1 for spaceships (size 2)
    # trace_2 for aliens (size 2x2: 2 planets, 2 aliens each)
    choice_trace_1 = np.zeros(2)
    choice_trace_2 = np.zeros((2, 2))
    
    log_loss = 0
    eps = 1e-10
    
    for trial in range(n_trials):
        # -- Stage 1 Decision --
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add trace stickiness to logits
        logits_1 = beta_1 * q_net + trace_w_1 * choice_trace_1
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = action_1[trial]
        if a1 != -1:
            log_loss -= np.log(probs_1[a1] + eps)
            
            # Update Trace 1
            choice_trace_1 *= trace_decay
            choice_trace_1[a1] += 1.0
        else:
            choice_trace_1 *= trace_decay
            continue
            
        if action_2[trial] == -1:
            # Even if stage 2 is missing, decay stage 2 trace over time
            choice_trace_2 *= trace_decay
            continue

        # -- Stage 2 Decision --
        state_idx = state[trial]
        a2 = action_2[trial]
        
        # Add trace stickiness to logits (specific to current state)
        logits_2 = beta_2 * q_stage2_mf[state_idx] + trace_w_2 * choice_trace_2[state_idx]
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[a2] + eps)

        # Update Trace 2
        # We decay the entire trace matrix (time passes)
        choice_trace_2 *= trace_decay
        # Strengthen the chosen alien in the specific planet
        choice_trace_2[state_idx, a2] += 1.0

        # -- Learning --
        r = reward[trial]
        if r == -1: r = 0
        
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        
        q_stage1_mf[a1] += lr * delta_stage1 + lr * lam * delta_stage2
        q_stage2_mf[state_idx, a2] += lr * delta_stage2
        
    return log_loss
```

### Model 3: Hybrid Model with Outcome-Dependent Eligibility Traces
This model proposes that the participant's credit assignment from the final outcome (reward) to the initial choice (spaceship) depends on whether the outcome was positive or negative. The parameter `lam` (lambda) is split into `lam_win` and `lam_loss`. This allows the model to capture "defensive attribution" (ignoring losses) or "reinforcement asymmetry" (learning more about the first step from wins than losses) without changing the core learning rates.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF Model with Outcome-Dependent Eligibility Traces.
    
    The strength of the connection between the final outcome and the Stage 1 
    update (lambda) varies depending on whether the trial was a Win or a Loss.
    
    Parameters:
    lr (float): Learning rate [0, 1].
    beta_1 (float): Inverse temperature Stage 1 [0, 10].
    beta_2 (float): Inverse temperature Stage 2 [0, 10].
    w (float): MB/MF mixing weight [0, 1].
    lam_win (float): Eligibility trace strength for rewarded trials [0, 1].
    lam_loss (float): Eligibility trace strength for unrewarded trials [0, 1].
    """
    lr, beta_1, beta_2, w, lam_win, lam_loss = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    log_loss = 0
    eps = 1e-10
    
    for trial in range(n_trials):
        # -- Stage 1 Decision --
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = action_1[trial]
        if a1 != -1:
            log_loss -= np.log(probs_1[a1] + eps)
        else:
            continue
            
        if action_2[trial] == -1:
            continue

        # -- Stage 2 Decision --
        state_idx = state[trial]
        a2 = action_2[trial]
        
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[a2] + eps)

        # -- Learning --
        r = reward[trial]
        if r == -1: r = 0
        
        # Select lambda based on outcome
        current_lam = lam_win if r == 1 else lam_loss
        
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        
        # Apply outcome-specific lambda
        q_stage1_mf[a1] += lr * delta_stage1 + lr * current_lam * delta_stage2
        q_stage2_mf[state_idx, a2] += lr * delta_stage2
        
    return log_loss
```