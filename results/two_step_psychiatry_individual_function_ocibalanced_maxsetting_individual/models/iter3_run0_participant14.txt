Here are three new cognitive models based on the participant data and the provided template.

### Model 1: Dynamic Transition Learning with Stickiness
This model extends the standard hybrid learner by assuming the participant does not trust the transition probabilities (70/30) to be fixed. Instead, they update their internal model of the spaceship-planet transitions based on observation (`lr_trans`). This addresses the "rare transitions" mentioned in the task description. It also includes choice stickiness, which was successful in previous iterations.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF model with Dynamic Transition Learning and Stickiness.
    
    The agent learns the transition matrix (Spaceship -> Planet) from experience
    rather than assuming fixed probabilities.
    
    Parameters:
    - learning_rate: [0, 1] Update rate for Q-values.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weight of Model-Based system.
    - lam: [0, 1] Eligibility trace.
    - stickiness: [0, 5] Choice perseveration bonus.
    - lr_trans: [0, 1] Learning rate for transition probabilities.
    """
    learning_rate, beta, w, lam, stickiness, lr_trans = model_parameters
    n_trials = len(action_1)

    # Initialize transition matrix estimate (start with prior knowledge or neutral)
    # Rows: Action (0 or 1), Cols: State (0 or 1)
    trans_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    q_mf = np.zeros(2)         
    q2 = np.zeros((2, 2))      
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1
    eps = 1e-10

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])

        # Handle missing data
        if a1 == -1:
            continue

        # --- Stage 1 Policy ---
        max_q2 = np.max(q2, axis=1)
        q_mb = trans_matrix @ max_q2 # Use dynamic transition matrix

        q_net = w * q_mb + (1 - w) * q_mf

        if last_action_1 != -1:
            q_net[last_action_1] += stickiness

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q2[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        
        # 1. Update Action Values (MB/MF)
        delta_1 = q2[s, a2] - q_mf[a1]
        q_mf[a1] += learning_rate * delta_1

        delta_2 = r - q2[s, a2]
        q2[s, a2] += learning_rate * delta_2
        q_mf[a1] += learning_rate * lam * delta_2

        # 2. Update Transition Matrix
        # If we chose a1 and arrived at s, update probability T[a1, s] towards 1
        # and T[a1, other_s] towards 0.
        trans_matrix[a1, s] += lr_trans * (1 - trans_matrix[a1, s])
        trans_matrix[a1, 1-s] = 1.0 - trans_matrix[a1, s]

        last_action_1 = a1

    # Filter out 0 probabilities from missing trials before log
    mask = p_choice_1 > 0
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss
```

### Model 2: Asymmetric Learning with Stickiness
This model investigates if the participant learns differently from positive outcomes (coins) versus negative outcomes (no coins). It combines "Asymmetric Learning Rates" (which was tried separately) with "Stickiness" (which was successful). This combination is crucial because high stickiness can sometimes mask specific learning asymmetries.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF model with Asymmetric Learning Rates and Stickiness.
    
    Differentiates between learning from positive prediction errors (wins/better than expected)
    and negative prediction errors (losses/worse than expected).

    Parameters:
    - lr_pos: [0, 1] Learning rate for positive prediction errors.
    - lr_neg: [0, 1] Learning rate for negative prediction errors.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weight of Model-Based system.
    - lam: [0, 1] Eligibility trace.
    - stickiness: [0, 5] Choice perseveration bonus.
    """
    lr_pos, lr_neg, beta, w, lam, stickiness = model_parameters
    n_trials = len(action_1)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    q_mf = np.zeros(2)         
    q2 = np.zeros((2, 2))      
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1
    eps = 1e-10

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])

        if a1 == -1:
            continue

        # --- Stage 1 Policy ---
        max_q2 = np.max(q2, axis=1)
        q_mb = transition_matrix @ max_q2
        q_net = w * q_mb + (1 - w) * q_mf

        if last_action_1 != -1:
            q_net[last_action_1] += stickiness

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q2[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates with Asymmetry ---
        
        # Stage 1 MF Update
        delta_1 = q2[s, a2] - q_mf[a1]
        lr_1 = lr_pos if delta_1 > 0 else lr_neg
        q_mf[a1] += lr_1 * delta_1

        # Stage 2 Update
        delta_2 = r - q2[s, a2]
        lr_2 = lr_pos if delta_2 > 0 else lr_neg
        q2[s, a2] += lr_2 * delta_2

        # Eligibility Trace Update (Stage 1 driven by Stage 2 RPE)
        # Note: We use the lr associated with delta_2 for the transfer
        q_mf[a1] += lr_2 * lam * delta_2

        last_action_1 = a1

    mask = p_choice_1 > 0
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss
```

### Model 3: Hybrid Model with Stickiness and Random Lapses (Epsilon-Greedy Mixture)
This model posits that the participant mostly follows a softmax strategy (Hybrid MB/MF + Stickiness) but occasionally makes random errors or "lapses" due to inattention. This is modeled by mixing the softmax distribution with a uniform distribution, controlled by parameter `epsilon`.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF model with Stickiness and Random Lapses (Mixture Model).
    
    Assumes the participant behaves according to the softmax policy with probability (1-epsilon),
    and chooses randomly (uniform distribution) with probability epsilon.
    
    Parameters:
    - learning_rate: [0, 1] Update rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weight of Model-Based system.
    - lam: [0, 1] Eligibility trace.
    - stickiness: [0, 5] Choice perseveration bonus.
    - epsilon: [0, 1] Lapse rate (probability of random choice).
    """
    learning_rate, beta, w, lam, stickiness, epsilon = model_parameters
    n_trials = len(action_1)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    q_mf = np.zeros(2)         
    q2 = np.zeros((2, 2))      
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1
    eps = 1e-10

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])

        if a1 == -1:
            continue

        # --- Stage 1 Policy ---
        max_q2 = np.max(q2, axis=1)
        q_mb = transition_matrix @ max_q2
        q_net = w * q_mb + (1 - w) * q_mf

        if last_action_1 != -1:
            q_net[last_action_1] += stickiness

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        # Apply Mixture (Lapse)
        probs_1 = (1 - epsilon) * probs_1 + epsilon * 0.5
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q2[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        # Apply Mixture (Lapse)
        probs_2 = (1 - epsilon) * probs_2 + epsilon * 0.5
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        delta_1 = q2[s, a2] - q_mf[a1]
        q_mf[a1] += learning_rate * delta_1

        delta_2 = r - q2[s, a2]
        q2[s, a2] += learning_rate * delta_2
        q_mf[a1] += learning_rate * lam * delta_2

        last_action_1 = a1

    mask = p_choice_1 > 0
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss
```