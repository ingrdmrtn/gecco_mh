Here are the 3 cognitive models as requested.

### Model 1: Asymmetric Model-Based RL with Choice Kernel
This model extends the Model-Based strategy by distinguishing between positive and negative prediction errors (learning rates `alpha_pos` and `alpha_neg`). This allows the agent to learn differently from rewards versus omissions, capturing potential biases like "optimism" or "pessimism". It retains the Choice Kernel (habit) mechanism to account for the participant's strong perseveration.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Asymmetric Model-Based RL with Choice Kernel.
    
    Combines Model-Based value estimation with asymmetric learning rates for 
    positive and negative prediction errors, and a Choice Kernel for habituation.
    
    Parameters:
    - alpha_pos: [0, 1] Learning rate for positive prediction errors (Reward=1).
    - alpha_neg: [0, 1] Learning rate for negative prediction errors (Reward=0).
    - beta_mb: [0, 10] Inverse temperature for Model-Based values in Stage 1.
    - beta_habit: [0, 10] Weight for the Choice Kernel (Habit).
    - decay_k: [0, 1] Decay rate for the Choice Kernel.
    - beta_2: [0, 10] Inverse temperature for Stage 2 choices.
    """
    alpha_pos, alpha_neg, beta_mb, beta_habit, decay_k, beta_2 = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage2_mf = np.zeros((2, 2))
    choice_kernel = np.zeros(2) 

    for trial in range(n_trials):
        if action_1[trial] == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        utilities = beta_mb * q_stage1_mb + beta_habit * choice_kernel
        exp_u1 = np.exp(utilities)
        probs_1 = exp_u1 / np.sum(exp_u1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        # Stage 2 Policy
        state_idx = state[trial]
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # Stage 2 Learning (Asymmetric)
        delta = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        if delta >= 0:
            q_stage2_mf[state_idx, action_2[trial]] += alpha_pos * delta
        else:
            q_stage2_mf[state_idx, action_2[trial]] += alpha_neg * delta

        # Choice Kernel Update
        mask = np.zeros(2)
        mask[action_1[trial]] = 1
        choice_kernel = (1 - decay_k) * choice_kernel + decay_k * mask

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid (MB/MF) RL with Choice Kernel
This model assumes the participant uses a mixture of Model-Based and Model-Free strategies (controlled by weight `w`). This allows the agent to sometimes rely on the transition structure and other times on simple cached values. It includes the Choice Kernel to capture the observed stickiness.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid (Model-Based + Model-Free) RL with Choice Kernel.
    
    A hybrid model that mixes Model-Based and Model-Free value estimates for 
    Stage 1 choices. It also includes a Choice Kernel for perseveration.
    
    Parameters:
    - alpha: [0, 1] Learning rate for MF values (Stage 1 and Stage 2).
    - w: [0, 1] Mixing weight (1 = Pure MB, 0 = Pure MF).
    - beta_1: [0, 10] Inverse temperature for Stage 1 choices.
    - beta_habit: [0, 10] Weight for the Choice Kernel.
    - decay_k: [0, 1] Decay rate for the Choice Kernel.
    - beta_2: [0, 10] Inverse temperature for Stage 2 choices.
    """
    alpha, w, beta_1, beta_habit, decay_k, beta_2 = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    choice_kernel = np.zeros(2)

    for trial in range(n_trials):
        if action_1[trial] == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # Stage 1 Policy (Hybrid)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        utilities = beta_1 * q_net + beta_habit * choice_kernel
        
        exp_u1 = np.exp(utilities)
        probs_1 = exp_u1 / np.sum(exp_u1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        # Stage 2 Policy
        state_idx = state[trial]
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # Learning
        # Stage 1 MF Update (TD(1) logic: updates with stage 2 reward)
        # Note: We approximate TD(1) by updating Q_MF_1 towards Reward directly
        q_stage1_mf[action_1[trial]] += alpha * (reward[trial] - q_stage1_mf[action_1[trial]])
        
        # Stage 2 MF Update
        delta_s2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += alpha * delta_s2

        # Choice Kernel Update
        mask = np.zeros(2)
        mask[action_1[trial]] = 1
        choice_kernel = (1 - decay_k) * choice_kernel + decay_k * mask

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Model-Based RL with Forgetting and Choice Kernel
Since the alien reward probabilities change slowly (drift), this model introduces a `forget` parameter. Values for aliens decay toward a neutral probability (0.5) over time, allowing the model to "unlearn" old values and adapt to the non-stationary environment better than a standard RL model. It retains the Choice Kernel.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Based RL with Forgetting and Choice Kernel.
    
    Includes a forgetting mechanism where Stage 2 Q-values decay towards 0.5 
    (chance) on every trial, helping adaptation to drifting reward probabilities.
    
    Parameters:
    - alpha: [0, 1] Learning rate for Stage 2 values.
    - forget: [0, 1] Forgetting rate (decay towards 0.5).
    - beta_mb: [0, 10] Inverse temperature for Model-Based values in Stage 1.
    - beta_habit: [0, 10] Weight for the Choice Kernel.
    - decay_k: [0, 1] Decay rate for the Choice Kernel.
    - beta_2: [0, 10] Inverse temperature for Stage 2 choices.
    """
    alpha, forget, beta_mb, beta_habit, decay_k, beta_2 = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage2_mf = np.zeros((2, 2))
    choice_kernel = np.zeros(2)

    for trial in range(n_trials):
        if action_1[trial] == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # Passive Forgetting (Decay Q-values towards 0.5)
        q_stage2_mf = (1 - forget) * q_stage2_mf + forget * 0.5

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        utilities = beta_mb * q_stage1_mb + beta_habit * choice_kernel
        exp_u1 = np.exp(utilities)
        probs_1 = exp_u1 / np.sum(exp_u1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        # Stage 2 Policy
        state_idx = state[trial]
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # Active Learning
        delta = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += alpha * delta

        # Choice Kernel Update
        mask = np.zeros(2)
        mask[action_1[trial]] = 1
        choice_kernel = (1 - decay_k) * choice_kernel + decay_k * mask

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```