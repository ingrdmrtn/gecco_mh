Here are three new cognitive models based on the participant data and the feedback provided.

### Model 1: Hybrid Model-Based/Model-Free with Separate Learning Rates
**Hypothesis:** The participant updates their valuation of the immediate context (Aliens/Stage 2) at a different rate than their valuation of the distal context (Spaceships/Stage 1). This model combines the classic "Daw" Hybrid architecture with separate learning rates for each stage to capture different timescales of adaptation.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free RL with Separate Learning Rates.
    
    Distinguishes between the learning rate for the Spaceship choice (Stage 1)
    and the Alien choice (Stage 2), allowing for different speeds of adaptation
    at different hierarchical levels.
    
    Parameters:
    - alpha_1: [0, 1] Learning rate for Stage 1 (Spaceship) MF values.
    - alpha_2: [0, 1] Learning rate for Stage 2 (Alien) values.
    - beta: [0, 10] Inverse temperature (exploration/exploitation).
    - w: [0, 1] Weighting parameter (0 = Pure MF, 1 = Pure MB).
    """
    alpha_1, alpha_2, beta, w = model_parameters
    n_trials = len(action_1)
    
    # Fixed transition matrix: Row 0 -> [Planet 0, Planet 1], Row 1 -> [Planet 0, Planet 1]
    # Spaceship A (0) -> 0.7 to Planet 0, 0.3 to Planet 1
    # Spaceship B (1) -> 0.3 to Planet 0, 0.7 to Planet 1
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # [State, Action]

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        
        # Model-Based Value Calculation
        # V_MB(s1) = Sum(P(s2|s1) * Max(Q(s2, :)))
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Weighted Hybrid Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Decision ---
        state_idx = state[trial]
        
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning ---
        
        # Stage 2 RPE (Standard Q-learning)
        # Note: We use alpha_2 here
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += alpha_2 * delta_stage2
        
        # Stage 1 RPE (SARSA-style for MF path)
        # Note: We use alpha_1 here
        # MF update uses the value of the state actually reached (state_idx)
        # and the max value of that state (or value of chosen option).
        # Standard Daw implementation uses Q(s2, a2) as the target for Stage 1.
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += alpha_1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Model-Free RL with Eligibility Trace and Memory Decay
**Hypothesis:** Since the reward probabilities drift slowly over time, older learned values become obsolete. This model introduces a `decay` parameter. Unlike the previous best model which only had an eligibility trace, this model actively forgets unchosen option values, pulling them towards 0 (or a neutral point), helping the agent stay flexible in the changing environment.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free RL with Eligibility Trace and Value Decay.
    
    Includes an eligibility trace to connect Stage 2 outcomes to Stage 1 choices,
    plus a decay parameter that causes Q-values of unchosen Stage 2 options
    to slowly revert to 0, representing forgetting in a drifting environment.
    
    Parameters:
    - learning_rate: [0, 1] Weight for prediction error updates.
    - beta: [0, 10] Inverse temperature.
    - lam: [0, 1] Eligibility trace parameter.
    - decay: [0, 1] Rate at which unchosen Q-values decay to 0.
    """
    learning_rate, beta, lam, decay = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Decision ---
        state_idx = state[trial]
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning ---
        
        # Stage 1 Update (TD(0) part)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Eligibility Trace Update (Stage 1 updated by Stage 2 RPE)
        q_stage1_mf[action_1[trial]] += learning_rate * lam * delta_stage2
        
        # --- Decay / Forgetting ---
        # Decay the unchosen alien on the current planet
        unchosen_a2 = 1 - action_2[trial]
        q_stage2_mf[state_idx, unchosen_a2] *= (1 - decay)
        
        # Also decay the aliens on the other planet (since we didn't visit it)
        other_state = 1 - state_idx
        q_stage2_mf[other_state, :] *= (1 - decay)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model-Based/Model-Free with Separate Betas
**Hypothesis:** The "best model" so far utilized separate betas (noise levels) for Stage 1 and Stage 2. This model integrates that successful feature into the Hybrid (MB/MF) framework. It posits that the participant uses Model-Based planning for the spaceship choice (weighted by `w`), but the level of stochasticity/exploration differs significantly between the spaceship choice and the alien choice.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free RL with Separate Stage Betas.
    
    Combines the structural learning of the Hybrid model with the 
    observation that exploration noise differs between the two stages.
    
    Parameters:
    - learning_rate: [0, 1] Weight for prediction error updates.
    - beta_1: [0, 10] Inverse temperature for Stage 1 (Spaceship).
    - beta_2: [0, 10] Inverse temperature for Stage 2 (Alien).
    - w: [0, 1] Weighting parameter (0 = Pure MF, 1 = Pure MB).
    """
    learning_rate, beta_1, beta_2, w = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Decision (Hybrid) ---
        
        # MB Component
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Mix
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Selection using beta_1
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Decision (Model-Free) ---
        state_idx = state[trial]
        
        # Selection using beta_2
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning ---
        
        # Stage 1 MF Update
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Stage 2 MF Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```