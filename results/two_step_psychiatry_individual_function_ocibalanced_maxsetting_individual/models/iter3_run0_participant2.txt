Here are the three cognitive models based on the participant data and the two-step task structure.

### Model 1: Hybrid MB/MF with Asymmetric Learning Rates
This model hypothesizes that the participant learns differently from positive prediction errors (better-than-expected outcomes) versus negative prediction errors (worse-than-expected outcomes). This asymmetry (often related to risk sensitivity or optimism/pessimism bias) applies to both the Model-Free updates and the values used by the Model-Based system.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF Model with Asymmetric Learning Rates.
    
    This model distinguishes between learning from positive and negative 
    prediction errors. It updates Q-values with different rates depending on 
    whether the outcome was better or worse than expected. This captures 
    potential valence-induced biases in learning within a hybrid architecture.
    
    Parameters:
    alpha_pos: [0, 1] Learning rate for positive prediction errors (Delta > 0).
    alpha_neg: [0, 1] Learning rate for negative prediction errors (Delta < 0).
    beta: [0, 10] Inverse temperature (softmax sensitivity).
    w: [0, 1] Mixing weight (0 = Pure Model-Free, 1 = Pure Model-Based).
    """
    alpha_pos, alpha_neg, beta, w = model_parameters
    n_trials = len(action_1)
  
    # Transition matrix: Row 0 -> Space A (Common Planet X/0), Row 1 -> Space U (Common Planet Y/1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    log_likelihood = 0.0
    eps = 1e-10

    for trial in range(n_trials):
        a1 = action_1[trial]
        s2 = state[trial]
        a2 = action_2[trial]
        r = reward[trial]

        # --- Stage 1 Policy ---
        # Model-Based Value: Expected value of Stage 2 states weighted by transition probs
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value: Mixture of MB and MF
        q_net_s1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_s1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_likelihood += np.log(probs_1[a1] + eps)

        # Handle missing/invalid Stage 2 data
        if a2 == -1:
            continue

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_likelihood += np.log(probs_2[a2] + eps)
        
        # --- Updates with Asymmetric Learning Rates ---
        
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[s2, a2]
        alpha_s2 = alpha_pos if delta_stage2 > 0 else alpha_neg
        q_stage2_mf[s2, a2] += alpha_s2 * delta_stage2
        
        # Stage 1 MF Update (TD(0))
        # Update S1 MF value based on the value of the state actually reached (S2)
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        alpha_s1 = alpha_pos if delta_stage1 > 0 else alpha_neg
        q_stage1_mf[a1] += alpha_s1 * delta_stage1

    return -log_likelihood
```

### Model 2: Hybrid MB/MF with Forgetting (Decay)
This model extends the successful "forgetting" mechanism to the hybrid Model-Based/Model-Free framework. It assumes that while the participant integrates model-based information, the value representations for both specific actions (MF) and the underlying states (used by MB) decay over time if not visited. This accounts for memory limitations during the task.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF Model with Forgetting (Decay).
    
    Combines Model-Based planning with Model-Free learning, where unchosen
    options decay toward zero. This models the participant's potential to 
    forget the value of options (both spaceships and aliens) that haven't 
    been visited recently, affecting both the direct MF cache and the 
    MB planning values.
    
    Parameters:
    learning_rate: [0, 1] Update rate for chosen options.
    decay_rate: [0, 1] Decay rate for unchosen options (Q = Q * (1-decay)).
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Mixing weight (0 = Pure Model-Free, 1 = Pure Model-Based).
    """
    learning_rate, decay_rate, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    log_likelihood = 0.0
    eps = 1e-10

    for trial in range(n_trials):
        a1 = action_1[trial]
        s2 = state[trial]
        a2 = action_2[trial]
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_s1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_s1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_likelihood += np.log(probs_1[a1] + eps)

        if a2 == -1:
            continue

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_likelihood += np.log(probs_2[a2] + eps)
        
        # --- Updates ---
        
        # Stage 2 Update
        q_stage2_mf[s2, a2] += learning_rate * (r - q_stage2_mf[s2, a2])
        
        # Stage 1 MF Update
        q_stage1_mf[a1] += learning_rate * (q_stage2_mf[s2, a2] - q_stage1_mf[a1])
        
        # --- Decay (Forgetting) ---
        # Decay unchosen alien on current planet
        q_stage2_mf[s2, 1-a2] *= (1.0 - decay_rate)
        # Decay all aliens on the unvisited planet
        q_stage2_mf[1-s2, :] *= (1.0 - decay_rate)
        # Decay unchosen spaceship
        q_stage1_mf[1-a1] *= (1.0 - decay_rate)

    return -log_likelihood
```

### Model 3: Hybrid MB/MF with Separate Stage Betas
This model posits that the participant's level of exploration or stochasticity differs between the high-level planning stage (choosing a spaceship) and the low-level bandit stage (choosing an alien). By using separate inverse temperature parameters (`beta_1` and `beta_2`), the model can capture different degrees of decision noise in the two distinct phases of the task.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF Model with Separate Stage Betas.
    
    Uses distinct inverse temperature parameters for Stage 1 and Stage 2.
    This allows the model to capture different levels of exploration/exploitation
    trade-offs (or decision noise) in the planning phase (Stage 1) versus 
    the harvesting phase (Stage 2).
    
    Parameters:
    learning_rate: [0, 1] Learning rate for value updates.
    beta_1: [0, 10] Inverse temperature for Stage 1 (Spaceship choice).
    beta_2: [0, 10] Inverse temperature for Stage 2 (Alien choice).
    w: [0, 1] Mixing weight (0 = Pure Model-Free, 1 = Pure Model-Based).
    """
    learning_rate, beta_1, beta_2, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    log_likelihood = 0.0
    eps = 1e-10

    for trial in range(n_trials):
        a1 = action_1[trial]
        s2 = state[trial]
        a2 = action_2[trial]
        r = reward[trial]

        # --- Stage 1 Policy (uses beta_1) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_s1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_1 * q_net_s1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_likelihood += np.log(probs_1[a1] + eps)

        if a2 == -1:
            continue

        # --- Stage 2 Policy (uses beta_2) ---
        exp_q2 = np.exp(beta_2 * q_stage2_mf[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_likelihood += np.log(probs_2[a2] + eps)
        
        # --- Updates ---
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += learning_rate * delta_stage2
        
        # Stage 1 MF Update
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    return -log_likelihood
```