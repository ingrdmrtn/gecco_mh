Here are three new cognitive models for the two-step task.

### Model 1: Hybrid Model with Dual Stickiness
This model posits that the participant exhibits perseveration (stickiness) not just at the first stage (choosing a spaceship) but also at the second stage (choosing an alien). It tracks the previous choice made in each state (planet) and adds a bonus to that choice in the next visit, capturing motor or decision inertia at both levels of the hierarchy.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Dual Stickiness.
    
    Incorporates choice perseveration (stickiness) for both the first stage (Spaceship)
    and the second stage (Alien). This assumes the participant has a tendency to 
    repeat choices at both levels of the task.
    
    Parameters:
    lr: [0,1] - Learning rate.
    beta_1: [0,10] - Inverse temperature for Stage 1.
    beta_2: [0,10] - Inverse temperature for Stage 2.
    w: [0,1] - Weighting parameter (0=MF, 1=MB).
    stick_1: [0,5] - Stickiness for Stage 1 choice.
    stick_2: [0,5] - Stickiness for Stage 2 choice (state-dependent).
    """
    lr, beta_1, beta_2, w, stick_1, stick_2 = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1
    # Track previous action for each state (planet 0 and planet 1)
    prev_action_2 = np.array([-1, -1]) 

    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
            continue

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        logits_1 = beta_1 * q_net_1
        
        if prev_action_1 != -1:
            logits_1[prev_action_1] += stick_1
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # policy for the second choice
        logits_2 = beta_2 * q_stage2_mf[state_idx]
        
        if prev_action_2[state_idx] != -1:
            logits_2[prev_action_2[state_idx]] += stick_2
            
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Update Stage 1
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr * delta_stage1
        
        # Update Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr * delta_stage2
        
        prev_action_1 = action_1[trial]
        prev_action_2[state_idx] = action_2[trial]

    eps = 1e-10
    valid_indices = (p_choice_1 > 0) & (p_choice_2 > 0)
    log_loss = -(np.sum(np.log(p_choice_1[valid_indices] + eps)) + np.sum(np.log(p_choice_2[valid_indices] + eps)))
    return log_loss
```

### Model 2: Hybrid Model with Asymmetric Learning and Decay
This model combines two powerful mechanisms: asymmetric learning rates (learning differently from better-than-expected vs. worse-than-expected outcomes) and passive decay of unchosen options. This allows the model to capture biases in how rewards are processed while ensuring that old values fade over time, facilitating adaptation to the drifting reward probabilities.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Asymmetric Learning and Decay.
    
    Distinguishes between positive and negative prediction errors, 
    combined with passive decay of unchosen action values.
    
    Parameters:
    lr_pos: [0,1] - Learning rate for positive prediction errors.
    lr_neg: [0,1] - Learning rate for negative prediction errors.
    beta_1: [0,10] - Inverse temperature for Stage 1.
    beta_2: [0,10] - Inverse temperature for Stage 2.
    w: [0,1] - Weighting parameter.
    stick: [0,5] - Stickiness for Stage 1.
    decay: [0,1] - Decay rate for unchosen Q-values.
    """
    lr_pos, lr_neg, beta_1, beta_2, w, stick, decay = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
            continue

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        logits_1 = beta_1 * q_net_1
        
        if prev_action_1 != -1:
            logits_1[prev_action_1] += stick
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # policy for the second choice
        logits_2 = beta_2 * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Update Stage 1
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        current_lr_1 = lr_pos if delta_stage1 > 0 else lr_neg
        q_stage1_mf[action_1[trial]] += current_lr_1 * delta_stage1
        q_stage1_mf[1 - action_1[trial]] *= (1 - decay) # Decay unchosen

        # Update Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        current_lr_2 = lr_pos if delta_stage2 > 0 else lr_neg
        q_stage2_mf[state_idx, action_2[trial]] += current_lr_2 * delta_stage2
        q_stage2_mf[state_idx, 1 - action_2[trial]] *= (1 - decay) # Decay unchosen
        
        prev_action_1 = action_1[trial]

    eps = 1e-10
    valid_indices = (p_choice_1 > 0) & (p_choice_2 > 0)
    log_loss = -(np.sum(np.log(p_choice_1[valid_indices] + eps)) + np.sum(np.log(p_choice_2[valid_indices] + eps)))
    return log_loss
```

### Model 3: Hybrid Model with Separate Stage Learning Rates and Decay
This model acknowledges that the two stages of the task (choosing a spaceship vs. choosing an alien) might be learned at different rates. It assigns a distinct learning rate to each stage's model-free update, while retaining the decay mechanism that helps the agent forget outdated information.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Separate Stage Learning Rates and Decay.
    
    Uses distinct learning rates for Stage 1 (Spaceship) and Stage 2 (Alien) updates,
    allowing the model to adapt to the two levels of the task hierarchy at different speeds.
    Includes passive decay for unchosen options.
    
    Parameters:
    lr_1: [0,1] - Learning rate for Stage 1 (MF).
    lr_2: [0,1] - Learning rate for Stage 2.
    beta_1: [0,10] - Inverse temperature for Stage 1.
    beta_2: [0,10] - Inverse temperature for Stage 2.
    w: [0,1] - Weighting parameter.
    stick: [0,5] - Stickiness for Stage 1.
    decay: [0,1] - Decay rate for unchosen Q-values.
    """
    lr_1, lr_2, beta_1, beta_2, w, stick, decay = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
            continue

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        logits_1 = beta_1 * q_net_1
        
        if prev_action_1 != -1:
            logits_1[prev_action_1] += stick
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # policy for the second choice
        logits_2 = beta_2 * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Update Stage 1
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr_1 * delta_stage1
        q_stage1_mf[1 - action_1[trial]] *= (1 - decay)

        # Update Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr_2 * delta_stage2
        q_stage2_mf[state_idx, 1 - action_2[trial]] *= (1 - decay)
        
        prev_action_1 = action_1[trial]

    eps = 1e-10
    valid_indices = (p_choice_1 > 0) & (p_choice_2 > 0)
    log_loss = -(np.sum(np.log(p_choice_1[valid_indices] + eps)) + np.sum(np.log(p_choice_2[valid_indices] + eps)))
    return log_loss
```