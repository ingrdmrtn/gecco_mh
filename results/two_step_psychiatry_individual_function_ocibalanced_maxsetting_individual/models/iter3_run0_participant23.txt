Here are three new cognitive models for the two-step task, designed to capture the specific behaviors observed in the participant data (e.g., strong persistence/stickiness, potential asymmetric learning from wins/losses, and varying exploration levels).

### Model 1: Hybrid Learner with Asymmetric Learning Rates
This model hypothesizes that the participant integrates Model-Based (planning) and Model-Free (habit) systems but updates their value estimates differently depending on whether the outcome was better or worse than expected (Positive vs. Negative Prediction Error). This "optimism" or "pessimism" bias can explain the long streaks of staying with a choice despite losses (by ignoring or under-weighting negative errors).

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner with Asymmetric Learning Rates.
    
    Combines Model-Based (MB) and Model-Free (MF) value estimation.
    Uses different learning rates for positive and negative prediction errors,
    allowing the model to capture biases in processing wins versus losses
    (e.g., ignoring losses to maintain persistence).
    
    Parameters:
    lr_pos: [0, 1] Learning rate for positive prediction errors.
    lr_neg: [0, 1] Learning rate for negative prediction errors.
    beta:   [0, 10] Inverse temperature for softmax choice.
    w:      [0, 1] Weight of Model-Based system (0 = Pure MF, 1 = Pure MB).
    """
    lr_pos, lr_neg, beta, w = model_parameters
    n_trials = len(action_1)
    
    # Initialize Q-values
    # Stage 1 MF values (2 spaceships)
    q_mf = np.zeros(2) + 0.5
    # Stage 2 values (2 planets x 2 aliens)
    q_s2 = np.zeros((2, 2)) + 0.5
    
    # Transition matrix (fixed): A=0 -> 70% P=0; A=1 -> 70% P=1
    trans_probs = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    log_loss = 0.0
    eps = 1e-10
    
    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = reward[t]
        
        # Handle missing data
        if a1 == -1 or s == -1 or a2 == -1:
            continue
        
        # --- Stage 1 Decision ---
        # Model-Based Value: Expected max value of next stage
        max_q_s2 = np.max(q_s2, axis=1) # Max value for each planet
        v_mb = trans_probs @ max_q_s2
        
        # Net Value
        q_net = w * v_mb + (1 - w) * q_mf
        
        # Softmax Stage 1
        logits_1 = beta * q_net
        exp_1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_1 / np.sum(exp_1)
        
        log_loss -= np.log(probs_1[a1] + eps)
        
        # --- Stage 2 Decision ---
        # Softmax Stage 2 (based on current planet)
        logits_2 = beta * q_s2[s, :]
        exp_2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_2 / np.sum(exp_2)
        
        log_loss -= np.log(probs_2[a2] + eps)
        
        # --- Learning ---
        # Stage 2 Update (RPE)
        pe_2 = r - q_s2[s, a2]
        eff_lr_2 = lr_pos if pe_2 >= 0 else lr_neg
        q_s2[s, a2] += eff_lr_2 * pe_2
        
        # Stage 1 MF Update (TD(0) style)
        # Driven by the value of the chosen action in Stage 2
        pe_1 = q_s2[s, a2] - q_mf[a1]
        eff_lr_1 = lr_pos if pe_1 >= 0 else lr_neg
        q_mf[a1] += eff_lr_1 * pe_1

    return log_loss
```

### Model 2: Model-Free Learner with Separate Stage Temperatures
This model posits that the participant's exploration-exploitation balance differs between the two stages. For example, they may be very consistent/greedy in choosing a spaceship (Stage 1) but more exploratory or noisy when choosing an alien (Stage 2). This structure allows for the observed high persistence in Stage 1 without enforcing it in Stage 2.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Learner with Separate Stage Temperatures.
    
    Assumes that the exploration-exploitation balance differs between
    choosing a spaceship (Stage 1) and choosing an alien (Stage 2).
    Includes stickiness for Stage 1 to capture choice autocorrelation.
    
    Parameters:
    learning_rate: [0, 1] Learning rate for Q-values.
    beta_1:        [0, 10] Inverse temperature for Stage 1 (Spaceship).
    beta_2:        [0, 10] Inverse temperature for Stage 2 (Alien).
    stickiness:    [0, 5] Choice stickiness bonus for Stage 1.
    """
    learning_rate, beta_1, beta_2, stickiness = model_parameters
    n_trials = len(action_1)
    
    q_mf = np.zeros(2) + 0.5
    q_s2 = np.zeros((2, 2)) + 0.5
    
    log_loss = 0.0
    eps = 1e-10
    
    last_a1 = -1
    
    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = reward[t]
        
        if a1 == -1 or s == -1 or a2 == -1:
            last_a1 = -1
            continue
        
        # Stage 1 Choice
        logits_1 = beta_1 * q_mf
        if last_a1 != -1:
            logits_1[last_a1] += stickiness
        
        exp_1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_1 / np.sum(exp_1)
        log_loss -= np.log(probs_1[a1] + eps)
        
        last_a1 = a1
        
        # Stage 2 Choice
        logits_2 = beta_2 * q_s2[s, :]
        exp_2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_2 / np.sum(exp_2)
        log_loss -= np.log(probs_2[a2] + eps)
        
        # Learning
        # Update Stage 2
        pe_2 = r - q_s2[s, a2]
        q_s2[s, a2] += learning_rate * pe_2
        
        # Update Stage 1 (TD(0))
        # Update based on the value of the subsequent state-action pair
        pe_1 = q_s2[s, a2] - q_mf[a1]
        q_mf[a1] += learning_rate * pe_1
        
    return log_loss
```

### Model 3: Model-Free Learner with Value Decay
This model introduces a memory decay mechanism. Values of *unchosen* actions decay toward a neutral point (0.5) over time. This mimics forgetting or a "return to baseline." This mechanism can explain why the participant eventually switches after a long block of one choice: if the current choice's value degrades due to losses, and the unchosen option has decayed back to neutral (0.5), a switch becomes favorable.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Learner with Value Decay.
    
    Q-values for unchosen actions decay toward a neutral value (0.5) on each trial.
    This simulates forgetting or a return to baseline, encouraging switching
    after long periods of neglecting an option if the current option performs poorly.
    
    Parameters:
    learning_rate: [0, 1] Learning rate for chosen actions.
    beta:          [0, 10] Inverse temperature.
    stickiness:    [0, 5] Choice stickiness bonus for Stage 1.
    decay_rate:    [0, 1] Rate at which unchosen values decay to 0.5.
    """
    learning_rate, beta, stickiness, decay_rate = model_parameters
    n_trials = len(action_1)
    
    q_mf = np.zeros(2) + 0.5
    q_s2 = np.zeros((2, 2)) + 0.5
    
    log_loss = 0.0
    eps = 1e-10
    
    last_a1 = -1
    
    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = reward[t]
        
        if a1 == -1 or s == -1 or a2 == -1:
            last_a1 = -1
            continue
        
        # Stage 1 Choice
        logits_1 = beta * q_mf
        if last_a1 != -1:
            logits_1[last_a1] += stickiness
            
        exp_1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_1 / np.sum(exp_1)
        log_loss -= np.log(probs_1[a1] + eps)
        
        last_a1 = a1
        
        # Stage 2 Choice
        logits_2 = beta * q_s2[s, :]
        exp_2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_2 / np.sum(exp_2)
        log_loss -= np.log(probs_2[a2] + eps)
        
        # --- Learning & Decay ---
        
        # Stage 2 Update
        # Update chosen alien
        pe_2 = r - q_s2[s, a2]
        q_s2[s, a2] += learning_rate * pe_2
        
        # Decay unchosen alien on current planet
        unchosen_a2 = 1 - a2
        q_s2[s, unchosen_a2] = (1 - decay_rate) * q_s2[s, unchosen_a2] + decay_rate * 0.5
        
        # Decay aliens on the other planet (simulating global passage of time)
        other_s = 1 - s
        q_s2[other_s, :] = (1 - decay_rate) * q_s2[other_s, :] + decay_rate * 0.5
        
        # Stage 1 Update
        # Update chosen spaceship
        pe_1 = q_s2[s, a2] - q_mf[a1]
        q_mf[a1] += learning_rate * pe_1
        
        # Decay unchosen spaceship
        unchosen_a1 = 1 - a1
        q_mf[unchosen_a1] = (1 - decay_rate) * q_mf[unchosen_a1] + decay_rate * 0.5
        
    return log_loss
```