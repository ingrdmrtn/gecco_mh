Here are three new cognitive models for the two-step decision task, designed to capture different potential mechanisms underlying the participant's behavior, such as asymmetric update sensitivity, hybrid planning with eligibility traces, and stage-specific learning dynamics.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Asymmetric Learning Rates with Choice Stickiness (MF).
    
    This model extends the asymmetric learning hypothesis by incorporating choice 
    stickiness (perseveration). It assumes the participant updates value estimates 
    differently depending on whether the prediction error is positive (better than 
    expected) or negative (worse than expected), while also exhibiting a tendency 
    to repeat the previous Stage 1 choice regardless of reward.

    Parameters:
    - lr_pos: [0, 1] Learning rate for positive prediction errors.
    - lr_neg: [0, 1] Learning rate for negative prediction errors.
    - beta: [0, 10] Inverse temperature (softmax sensitivity).
    - stickiness: [0, 10] Bonus added to the Q-value of the previously chosen spaceship.
    """
    lr_pos, lr_neg, beta, stickiness = model_parameters
    n_trials = len(action_1)

    # Initialize Q-values: 2 spaceships (Stage 1), 2 planets x 2 aliens (Stage 2)
    q_stage1 = np.zeros(2) + 0.5
    q_stage2 = np.zeros((2, 2)) + 0.5
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_a1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        q_net_1 = q_stage1.copy()
        # Add stickiness bonus to the previously chosen spaceship
        if last_a1 != -1:
            q_net_1[last_a1] += stickiness
            
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Choice ---
        s_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2[s_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        r = reward[trial]
        
        # Stage 2 Update (Reward Prediction Error)
        pe_2 = r - q_stage2[s_idx, a2]
        # Use asymmetric learning rates
        lr_2 = lr_pos if pe_2 > 0 else lr_neg
        q_stage2[s_idx, a2] += lr_2 * pe_2
        
        # Stage 1 Update (TD Prediction Error)
        # Update Q1 based on the value of the chosen option in Stage 2 (SARSA-like)
        pe_1 = q_stage2[s_idx, a2] - q_stage1[a1]
        lr_1 = lr_pos if pe_1 > 0 else lr_neg
        q_stage1[a1] += lr_1 * pe_1
        
        last_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free with Eligibility Traces.
    
    This model combines a Model-Based (MB) planner with a Model-Free (MF) learner.
    Unlike standard hybrid models that often use TD(0) or TD(1) for the MF component,
    this model uses TD(lambda) eligibility traces. This allows the MF system to 
    assign credit to the first stage action based on the second stage outcome directly,
    blended with the MB system's transition-matrix-based evaluation.
    
    Parameters:
    - lr: [0, 1] Learning rate for MF updates.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight (0 = Pure MF, 1 = Pure MB).
    - lam: [0, 1] Eligibility trace decay parameter (lambda).
    """
    lr, beta, w, lam = model_parameters
    n_trials = len(action_1)

    # Fixed transition matrix: A(0)->X(0) 0.7, A(0)->Y(1) 0.3; U(1)->Y(1) 0.7...
    # Rows are actions (spaceships), cols are states (planets)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_mf_stage1 = np.zeros(2) + 0.5
    q_mf_stage2 = np.zeros((2, 2)) + 0.5
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based Value Calculation: V_MB(s1) = T * max(Q_MF_s2)
        max_q_stage2 = np.max(q_mf_stage2, axis=1)
        q_mb_stage1 = transition_matrix @ max_q_stage2
        
        # Weighted mixture of MB and MF values
        q_net_stage1 = w * q_mb_stage1 + (1 - w) * q_mf_stage1
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        s_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_mf_stage2[s_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        r = reward[trial]
        
        # 1. Stage 1 MF Update (TD error from transition)
        pe_1 = q_mf_stage2[s_idx, a2] - q_mf_stage1[a1]
        q_mf_stage1[a1] += lr * pe_1
        
        # 2. Stage 2 MF Update (Reward error)
        pe_2 = r - q_mf_stage2[s_idx, a2]
        q_mf_stage2[s_idx, a2] += lr * pe_2
        
        # 3. Eligibility Trace Update for Stage 1 (Direct credit from Reward)
        # The trace decays by lambda and updates Q1 based on the Stage 2 PE
        q_mf_stage1[a1] += lr * lam * pe_2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free with Stage-Specific Learning Rates and Stickiness.
    
    This model posits that learning dynamics differ between the abstract choice
    of a spaceship (Stage 1) and the concrete choice of an alien (Stage 2).
    It uses separate learning rates for each stage and includes a stickiness
    parameter to capture the participant's tendency to repeat spaceship choices.
    
    Parameters:
    - lr_1: [0, 1] Learning rate for Stage 1 (Spaceship) updates.
    - lr_2: [0, 1] Learning rate for Stage 2 (Alien) updates.
    - beta: [0, 10] Inverse temperature.
    - stickiness: [0, 10] Bonus for repeating the previous Stage 1 choice.
    """
    lr_1, lr_2, beta, stickiness = model_parameters
    n_trials = len(action_1)
    
    q_stage1 = np.zeros(2) + 0.5
    q_stage2 = np.zeros((2, 2)) + 0.5
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_a1 = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        q_net_1 = q_stage1.copy()
        if last_a1 != -1:
            q_net_1[last_a1] += stickiness
            
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Choice ---
        s_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2[s_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        r = reward[trial]
        
        # Update Stage 2 (driven by reward)
        pe_2 = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += lr_2 * pe_2
        
        # Update Stage 1 (driven by value of Stage 2 choice)
        pe_1 = q_stage2[s_idx, a2] - q_stage1[a1]
        q_stage1[a1] += lr_1 * pe_1
        
        last_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```