Here are three new cognitive models designed to capture the specific patterns in the participant's data, particularly the high switching rate and potential reactions to transition dynamics.

### Model 1: Model-Based Learner with Alternation Bias
This model hypothesizes that the participant uses a Model-Based strategy (calculating values based on the transition structure) but has a specific bias towards **alternating** their Stage 1 choice, rather than repeating it (stickiness). This directly addresses the high frequency of switching observed in the data, even after rewards.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Model-Based Learner with Alternation Bias.
    
    This model assumes the participant uses a model-based strategy (calculating
    Stage 1 values from the fixed transition matrix and Stage 2 values) but has 
    a bias to switch their Stage 1 choice (alternation). This is implemented 
    as a bonus added to the *unchosen* option from the previous trial.
    
    Parameters:
    - learning_rate: [0, 1] Update rate for Stage 2 alien values.
    - beta_1: [0, 10] Inverse temperature for Stage 1 choice.
    - beta_2: [0, 10] Inverse temperature for Stage 2 choice.
    - alternation_bias: [0, 5] Bonus value added to the unchosen Stage 1 action.
    """
    learning_rate, beta_1, beta_2, alternation_bias = model_parameters
    n_trials = len(action_1)
    
    # Fixed transition matrix (A->X mostly, U->Y mostly)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Q-values for Stage 2 (2 planets x 2 aliens)
    q_stage2 = np.zeros((2, 2))
    
    last_action_1 = -1
    log_loss = 0.0
    eps = 1e-10
    
    for trial in range(n_trials):
        # Handle missing data
        if action_1[trial] == -1:
            continue
            
        # --- Stage 1 Decision ---
        # Model-Based Value Calculation: V(S) = max_a Q(S, a)
        max_q_stage2 = np.max(q_stage2, axis=1)
        # Q_MB(A) = T(A, S) * V(S)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Apply Alternation Bias
        q_net = q_stage1_mb.copy()
        if last_action_1 != -1:
            # Add bonus to the option NOT chosen last time (encouraging switching)
            q_net[1 - last_action_1] += alternation_bias
            
        # Softmax choice probability
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        chosen_a1 = int(action_1[trial])
        log_loss -= np.log(probs_1[chosen_a1] + eps)
        
        # --- Stage 2 Decision ---
        current_state = int(state[trial])
        exp_q2 = np.exp(beta_2 * q_stage2[current_state])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        chosen_a2 = int(action_2[trial])
        log_loss -= np.log(probs_2[chosen_a2] + eps)
        
        # --- Learning ---
        r = reward[trial]
        # Standard Q-learning update for Stage 2
        delta_stage2 = r - q_stage2[current_state, chosen_a2]
        q_stage2[current_state, chosen_a2] += learning_rate * delta_stage2
        
        last_action_1 = chosen_a1
        
    return log_loss
```

### Model 2: Model-Based with Counterfactual Updating
This model proposes that the participant learns efficiently by assuming a structure to the alien rewards. When they choose an alien and receive a reward (or not), they update the value of the *unchosen* alien as if it would have yielded the opposite outcome. This "fictitious play" or counterfactual updating can explain rapid shifts in preference.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Based Learner with Counterfactual Updating.
    
    This model assumes the participant learns Stage 2 values using counterfactual
    reasoning: updating the chosen alien with the received reward, and the 
    unchosen alien with the opposite reward (1-R). It uses separate learning 
    rates for chosen and unchosen updates to allow for asymmetric learning.
    
    Parameters:
    - lr_chosen: [0, 1] Learning rate for the chosen alien.
    - lr_counterfactual: [0, 1] Learning rate for the unchosen alien (assuming 1-R).
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - stickiness: [0, 5] Tendency to repeat Stage 1 choice.
    """
    lr_chosen, lr_counterfactual, beta_1, beta_2, stickiness = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage2 = np.zeros((2, 2))
    
    last_action_1 = -1
    log_loss = 0.0
    eps = 1e-10
    
    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        # --- Stage 1 ---
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = q_stage1_mb.copy()
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        chosen_a1 = int(action_1[trial])
        log_loss -= np.log(probs_1[chosen_a1] + eps)
        
        # --- Stage 2 ---
        current_state = int(state[trial])
        exp_q2 = np.exp(beta_2 * q_stage2[current_state])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        chosen_a2 = int(action_2[trial])
        log_loss -= np.log(probs_2[chosen_a2] + eps)
        
        # --- Learning ---
        r = reward[trial]
        
        # Update chosen alien
        q_stage2[current_state, chosen_a2] += lr_chosen * (r - q_stage2[current_state, chosen_a2])
        
        # Update unchosen alien (Counterfactual: assume reward was 1-r)
        unchosen_a2 = 1 - chosen_a2
        cf_reward = 1 - r
        q_stage2[current_state, unchosen_a2] += lr_counterfactual * (cf_reward - q_stage2[current_state, unchosen_a2])
        
        last_action_1 = chosen_a1
        
    return log_loss
```

### Model 3: Adaptive Model-Based Learner (Dynamic Transitions)
This model tests the hypothesis that the participant does not assume the spaceship transitions are fixed (70/30) but instead learns the transition matrix from observation. If they observe rare transitions, they may update their belief about which spaceship goes where, affecting their Stage 1 choices dynamically.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Adaptive Model-Based Learner (Dynamic Transitions).
    
    This model assumes the participant tracks the transition matrix dynamically 
    rather than assuming it is fixed. They update their belief about spaceship 
    transitions (A->X or A->Y) based on observed outcomes trial-by-trial.
    
    Parameters:
    - lr_value: [0, 1] Learning rate for Stage 2 alien values.
    - lr_transition: [0, 1] Learning rate for updating the transition matrix.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - stickiness: [0, 5] Tendency to repeat Stage 1 choice.
    """
    lr_value, lr_transition, beta_1, beta_2, stickiness = model_parameters
    n_trials = len(action_1)
    
    # Initialize transition matrix (start with standard assumption)
    # Rows: Actions (0, 1), Cols: States (0, 1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage2 = np.zeros((2, 2))
    
    last_action_1 = -1
    log_loss = 0.0
    eps = 1e-10
    
    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        # --- Stage 1 ---
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = q_stage1_mb.copy()
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        chosen_a1 = int(action_1[trial])
        log_loss -= np.log(probs_1[chosen_a1] + eps)
        
        # --- Stage 2 ---
        current_state = int(state[trial])
        exp_q2 = np.exp(beta_2 * q_stage2[current_state])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        chosen_a2 = int(action_2[trial])
        log_loss -= np.log(probs_2[chosen_a2] + eps)
        
        # --- Learning ---
        r = reward[trial]
        
        # Update Value
        q_stage2[current_state, chosen_a2] += lr_value * (r - q_stage2[current_state, chosen_a2])
        
        # Update Transition Matrix based on observed transition
        # T(a, s) moves towards 1, T(a, other_s) moves towards 0
        transition_matrix[chosen_a1, current_state] += lr_transition * (1 - transition_matrix[chosen_a1, current_state])
        transition_matrix[chosen_a1, 1 - current_state] = 1.0 - transition_matrix[chosen_a1, current_state]
        
        last_action_1 = chosen_a1
        
    return log_loss
```