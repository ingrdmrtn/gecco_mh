Here are three new cognitive models for the two-step decision task.

### Model 1: Dynamic Transition Learning
This model relaxes the assumption that the participant uses a fixed transition matrix (0.7/0.3). Instead, the participant learns the transition probabilities ($P(\text{Planet}|\text{Spaceship})$) online based on observed transitions. This allows the model-based system to adapt if the participant perceives the transition structure to be unstable or different from the instructions.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Dynamic Transition Learning.
    
    The agent learns the transition probabilities (Spaceship -> Planet) 
    online, rather than using fixed probabilities. 
    
    Parameters:
    - lr_rew: [0, 1] Learning rate for reward values.
    - lr_trans: [0, 1] Learning rate for transition probabilities.
    - beta_1: [0, 10] Stage 1 inverse temperature.
    - beta_2: [0, 10] Stage 2 inverse temperature.
    - w: [0, 1] Weight for model-based values (0=MF, 1=MB).
    - stickiness: [0, 5] Choice stickiness for Stage 1.
    """
    lr_rew, lr_trans, beta_1, beta_2, w, stickiness = model_parameters
    n_trials = len(action_1)
    
    # Initialize transition matrix: [Action, State]
    # Rows: Action 0 (A), Action 1 (U)
    # Cols: State 0 (X), State 1 (Y)
    # Start with instruction knowledge (0.7 common)
    trans_probs = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        # Handle missing data
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1 or reward[trial] == -1:
            last_action_1 = -1
            continue
            
        # --- Stage 1 Policy ---
        # Model-Based Value: Uses current belief of transition probabilities
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = trans_probs @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Policy ---
        state_idx = state[trial]
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        # 1. Transition Probability Update
        # Target is 1 for the observed state, 0 for the other
        target_trans = np.zeros(2)
        target_trans[state_idx] = 1.0
        # Linear update maintains sum=1 if started normalized
        trans_probs[action_1[trial]] += lr_trans * (target_trans - trans_probs[action_1[trial]])
        
        # 2. Value Updates
        # Stage 1 MF
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr_rew * delta_stage1
        
        # Stage 2 MF
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr_rew * delta_stage2
        
        last_action_1 = action_1[trial]

    eps = 1e-10
    mask = (p_choice_1 > 0) & (p_choice_2 > 0)
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss
```

### Model 2: Transition-Dependent MF Learning
This model posits that the participant's model-free (habitual) system learns differently depending on whether the transition was common or rare. For instance, participants might gate prediction error updates from rare transitions, treating them as "bad luck" or "noise" rather than actionable signals for the first-stage choice.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Transition-Dependent MF Learning.
    
    Splits the Stage 1 Model-Free learning rate into two parameters:
    one for common transitions and one for rare transitions. This allows
    the model to capture if the participant discounts prediction errors
    arising from rare transitions in their habitual system.
    
    Parameters:
    - lr_common: [0, 1] MF learning rate after common transitions.
    - lr_rare: [0, 1] MF learning rate after rare transitions.
    - lr_stage2: [0, 1] Learning rate for Stage 2 values.
    - beta_1: [0, 10] Stage 1 inverse temperature.
    - beta_2: [0, 10] Stage 2 inverse temperature.
    - w: [0, 1] Weight for model-based values.
    - stickiness: [0, 5] Choice stickiness.
    """
    lr_common, lr_rare, lr_stage2, beta_1, beta_2, w, stickiness = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1 or reward[trial] == -1:
            last_action_1 = -1
            continue
            
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Policy ---
        state_idx = state[trial]
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        # Determine if transition was common or rare
        # Common: 0->0, 1->1. Rare: 0->1, 1->0.
        is_common = (action_1[trial] == state_idx)
        current_lr_s1 = lr_common if is_common else lr_rare
        
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += current_lr_s1 * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr_stage2 * delta_stage2
        
        last_action_1 = action_1[trial]
        
    eps = 1e-10
    mask = (p_choice_1 > 0) & (p_choice_2 > 0)
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss
```

### Model 3: Reward-Modulated Exploration
This model introduces a mechanism where the participant's exploration-exploitation balance (inverse temperature $\beta$) is modulated by the recent average reward rate. A higher average reward leads to higher $\beta$ (more exploitation), while a lower average reward lowers $\beta$ (more exploration/noise).

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Reward-Modulated Exploration.
    
    The inverse temperature (beta) for Stage 1 varies based on the 
    recent average reward rate. This allows the model to capture 
    shifts between exploration (low beta) and exploitation (high beta)
    depending on performance context.
    
    Parameters:
    - learning_rate: [0, 1] Value learning rate.
    - lr_avg: [0, 1] Learning rate for the average reward tracker.
    - beta_base: [0, 10] Baseline inverse temperature for Stage 1.
    - beta_mod: [0, 10] Modulation strength. beta = beta_base + beta_mod * (avg_r - 0.5).
    - beta_2: [0, 10] Stage 2 inverse temperature.
    - w: [0, 1] MB/MF weight.
    - stickiness: [0, 5] Choice stickiness.
    """
    learning_rate, lr_avg, beta_base, beta_mod, beta_2, w, stickiness = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    avg_reward = 0.5 # Initialize at neutral (0.5)
    last_action_1 = -1
    
    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1 or reward[trial] == -1:
            last_action_1 = -1
            continue
            
        # --- Stage 1 Policy ---
        # Modulate Beta 1 based on average reward
        # Centered around 0.5. If avg_reward > 0.5, beta increases (exploit).
        beta_eff = beta_base + beta_mod * (avg_reward - 0.5)
        if beta_eff < 0:
            beta_eff = 0
            
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta_eff * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Policy ---
        state_idx = state[trial]
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Update Average Reward
        avg_reward += lr_avg * (reward[trial] - avg_reward)
        
        last_action_1 = action_1[trial]
        
    eps = 1e-10
    mask = (p_choice_1 > 0) & (p_choice_2 > 0)
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss
```