Here are three new cognitive models for the two-step task, designed to capture different potential mechanisms underlying the participant's behavior, such as stage-specific decision noise, adaptive learning of transition structures, and outcome-dependent perseveration.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free TD(1) with Stage-Specific Inverse Temperatures.
    This model assumes the participant has different levels of decision noise (exploration/exploitation)
    for the high-level planning stage (Spaceship choice) versus the low-level bandit stage (Alien choice).
    
    Parameters:
    - learning_rate: [0, 1] Rate of updating Q-values based on reward.
    - beta1: [0, 10] Inverse temperature for Stage 1 (Spaceship) choice.
    - beta2: [0, 10] Inverse temperature for Stage 2 (Alien) choice.
    - p: [0, 5] Perseveration bonus for Stage 1 choices (stickiness).
    """
    learning_rate, beta1, beta2, p = model_parameters
    n_trials = len(action_1)
    
    # Initialize Q-values
    # Stage 1: Spaceships 0 and 1
    q_stage1 = np.zeros(2) 
    # Stage 2: Planets 0 and 1, Aliens 0 and 1
    q_stage2 = np.zeros((2, 2)) 
    
    log_loss = 0.0
    prev_choice_1 = -1
    
    for trial in range(n_trials):
        # --- Stage 1: Spaceship Selection ---
        q_net_s1 = q_stage1.copy()
        
        # Apply perseveration bonus
        if prev_choice_1 != -1:
            q_net_s1[prev_choice_1] += p
            
        # Softmax with beta1
        exp_q1 = np.exp(beta1 * q_net_s1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[action_1[trial]] + 1e-10)
        
        # --- Stage 2: Alien Selection ---
        state_idx = state[trial]
        q_s2 = q_stage2[state_idx]
        
        # Softmax with beta2
        exp_q2 = np.exp(beta2 * q_s2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[action_2[trial]] + 1e-10)
        
        # --- Learning ---
        # Update Stage 2 Q-values based on reward
        pe_2 = reward[trial] - q_stage2[state_idx, action_2[trial]]
        q_stage2[state_idx, action_2[trial]] += learning_rate * pe_2
        
        # Update Stage 1 Q-values using TD(1) logic (Direct Reinforcement)
        # The Stage 1 choice is reinforced by the final reward obtained
        pe_1 = reward[trial] - q_stage1[action_1[trial]]
        q_stage1[action_1[trial]] += learning_rate * pe_1
        
        prev_choice_1 = action_1[trial]

    return log_loss

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Adaptive Model-Based Hybrid Model.
    Unlike standard models that assume fixed transition probabilities (0.7/0.3),
    this model learns the transition matrix (Spaceship -> Planet) from experience.
    
    Parameters:
    - lr_reward: [0, 1] Learning rate for value updates (Q-values).
    - lr_trans: [0, 1] Learning rate for transition probability updates.
    - beta: [0, 10] Inverse temperature for choices.
    - w: [0, 1] Weighting parameter (0 = Pure MF, 1 = Pure MB).
    """
    lr_reward, lr_trans, beta, w = model_parameters
    n_trials = len(action_1)
    
    # Initialize Q-values
    q_stage1_mf = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    # Initialize Transition Matrix Estimate
    # Rows: Spaceship 0, Spaceship 1. Cols: Planet 0, Planet 1.
    # Initializing to 0.7/0.3 assumes prior knowledge of structure but allows drift.
    trans_probs = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    log_loss = 0.0
    
    for trial in range(n_trials):
        # --- Stage 1: Spaceship Selection ---
        # Model-Based Value: Expected max value of next stage using learned transitions
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_stage1_mb = trans_probs @ max_q_stage2
        
        # Hybrid Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[action_1[trial]] + 1e-10)
        
        # --- Stage 2: Alien Selection ---
        state_idx = state[trial]
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[action_2[trial]] + 1e-10)
        
        # --- Learning ---
        # Update Stage 2 Values
        pe_2 = reward[trial] - q_stage2[state_idx, action_2[trial]]
        q_stage2[state_idx, action_2[trial]] += lr_reward * pe_2
        
        # Update Stage 1 MF Values (TD(1))
        pe_1 = reward[trial] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr_reward * pe_1
        
        # Update Transition Probabilities
        # Increase prob of observed transition, decrease prob of unobserved
        a1 = action_1[trial]
        s_observed = state[trial]
        s_unobserved = 1 - s_observed
        
        # Delta rule for probability estimation
        trans_probs[a1, s_observed] += lr_trans * (1 - trans_probs[a1, s_observed])
        trans_probs[a1, s_unobserved] += lr_trans * (0 - trans_probs[a1, s_unobserved])
        
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Outcome-Dependent Perseveration.
    Differentiates "stickiness" based on the previous trial's outcome (Win-Stay vs. Lose-Stay).
    Allows the model to capture different tendencies to repeat choices after rewards vs. omissions.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for Q-values.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weighting parameter (0 = Pure MF, 1 = Pure MB).
    - p_win: [0, 5] Perseveration bonus applied if previous trial was Rewarded.
    - p_loss: [0, 5] Perseveration bonus applied if previous trial was Unrewarded.
    """
    learning_rate, beta, w, p_win, p_loss = model_parameters
    n_trials = len(action_1)
    
    # Fixed transition matrix for MB calculation
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    log_loss = 0.0
    prev_choice_1 = -1
    prev_reward = 0
    
    for trial in range(n_trials):
        # --- Stage 1: Spaceship Selection ---
        # MB Value
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Apply Outcome-Dependent Perseveration
        if prev_choice_1 != -1:
            if prev_reward == 1:
                q_net[prev_choice_1] += p_win
            else:
                q_net[prev_choice_1] += p_loss
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[action_1[trial]] + 1e-10)
        
        # --- Stage 2: Alien Selection ---
        state_idx = state[trial]
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[action_2[trial]] + 1e-10)
        
        # --- Learning ---
        # Update Stage 2
        pe_2 = reward[trial] - q_stage2[state_idx, action_2[trial]]
        q_stage2[state_idx, action_2[trial]] += learning_rate * pe_2
        
        # Update Stage 1 MF (TD(1))
        pe_1 = reward[trial] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * pe_1
        
        prev_choice_1 = action_1[trial]
        prev_reward = reward[trial]
        
    return log_loss
```