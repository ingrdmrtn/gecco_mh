Here are the three proposed cognitive models.

### Model 1: Adaptive Transition Model
This model extends the standard hybrid model by allowing the agent to learn the transition probabilities (the "Model" in Model-Based) online, rather than using a fixed matrix. This tests the hypothesis that the participant actively updates their beliefs about the spaceship-planet structure (e.g., detecting non-stationarity or refining their knowledge), which influences the Model-Based value calculation.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Adaptive Transition Model.
    
    The agent actively learns the transition probabilities between spaceships 
    and planets using a delta rule, rather than relying on a fixed transition matrix.
    This learned transition matrix is used to compute Model-Based values.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate for action values (Stage 1 & 2).
    beta: [0, 10] - Inverse temperature for softmax choice.
    w: [0, 1] - Weight for Model-Based values (0 = pure MF, 1 = pure MB).
    lr_trans: [0, 1] - Learning rate for updating transition probabilities.
    """
    learning_rate, beta, w, lr_trans = model_parameters
    n_trials = len(action_1)
  
    # Initialize transition beliefs with the common structure (0.7/0.3)
    # but allow them to change over time.
    # Rows: Choice 0, Choice 1. Cols: Planet 0, Planet 1.
    trans_probs = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # state x action

    for trial in range(n_trials):
        # Handle missing data
        if action_1[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # --- Stage 1 Choice ---
        # Calculate MB values using the CURRENT learned transition matrix
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = trans_probs @ max_q_stage2
        
        # Hybrid value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Choice ---
        state_idx = state[trial]
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        chosen_a1 = action_1[trial]
        chosen_a2 = action_2[trial]
        
        # 1. Update MF Stage 1
        delta_stage1 = q_stage2_mf[state_idx, chosen_a2] - q_stage1_mf[chosen_a1]
        q_stage1_mf[chosen_a1] += learning_rate * delta_stage1
        
        # 2. Update MF Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, chosen_a2]
        q_stage2_mf[state_idx, chosen_a2] += learning_rate * delta_stage2
        
        # 3. Update Transition Probabilities (Model Learning)
        # T(a, s') <- T(a, s') + lr_trans * (1 - T(a, s'))
        # T(a, other) <- T(a, other) + lr_trans * (0 - T(a, other))
        observed_next_state = state[trial]
        unobserved_next_state = 1 - observed_next_state
        
        # Update the probability of the observed transition
        trans_probs[chosen_a1, observed_next_state] += lr_trans * (1.0 - trans_probs[chosen_a1, observed_next_state])
        # Update the probability of the unobserved transition (normalization)
        trans_probs[chosen_a1, unobserved_next_state] += lr_trans * (0.0 - trans_probs[chosen_a1, unobserved_next_state])

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Counterfactual Stage 2 Learning
This model posits that the participant updates the value of the *unchosen* alien on the current planet using a counterfactual reward assumption. Specifically, if the chosen alien yields 0 coins, the agent increases the value of the unchosen alien (assuming it would have yielded 1), and vice versa. This captures "the grass is greener" reasoning common in binary choice tasks.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Counterfactual Stage 2 Learning.
    
    The agent updates the unchosen alien's Q-value assuming an anti-correlated 
    reward structure (if chosen=0, unchosen=1; if chosen=1, unchosen=0).
    This mechanism helps the agent switch rapidly after losses.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate for chosen actions.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - MB/MF mixing weight.
    lr_cf: [0, 1] - Counterfactual learning rate for the unchosen Stage 2 action.
    """
    learning_rate, beta, w, lr_cf = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) 

    for trial in range(n_trials):
        if action_1[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Choice ---
        state_idx = state[trial]
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        chosen_a1 = action_1[trial]
        chosen_a2 = action_2[trial]
        unchosen_a2 = 1 - chosen_a2
        
        # Update MF Stage 1
        delta_stage1 = q_stage2_mf[state_idx, chosen_a2] - q_stage1_mf[chosen_a1]
        q_stage1_mf[chosen_a1] += learning_rate * delta_stage1
        
        # Update MF Stage 2 (Chosen)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, chosen_a2]
        q_stage2_mf[state_idx, chosen_a2] += learning_rate * delta_stage2
        
        # Update MF Stage 2 (Unchosen - Counterfactual)
        # Assume counterfactual reward is (1 - observed_reward)
        cf_reward = 1.0 - reward[trial]
        delta_cf = cf_reward - q_stage2_mf[state_idx, unchosen_a2]
        q_stage2_mf[state_idx, unchosen_a2] += lr_cf * delta_cf

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Outcome-Dependent Strategy Arbitration
This model proposes that the balance between Model-Based (planning) and Model-Free (habit/caching) strategies depends on the outcome of the previous trial. For instance, an agent might rely more on their model (`w_win`) after a success to maintain the strategy, but revert to (or away from) habits (`w_loss`) after a failure.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Outcome-Dependent Strategy Arbitration.
    
    The mixing weight 'w' (arbitration between MB and MF) varies depending on 
    whether the previous trial resulted in a reward or not. This tests if 
    strategy reliance is dynamic based on recent success.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate.
    beta: [0, 10] - Inverse temperature.
    w_win: [0, 1] - MB weight applied after a rewarded trial.
    w_loss: [0, 1] - MB weight applied after an unrewarded trial.
    """
    learning_rate, beta, w_win, w_loss = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track previous reward to decide which w to use. Start with 0 (loss assumption).
    prev_reward = 0.0

    for trial in range(n_trials):
        if action_1[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # --- Stage 1 Choice ---
        # Select w based on previous outcome
        current_w = w_win if prev_reward == 1.0 else w_loss
        
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = current_w * q_stage1_mb + (1 - current_w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Choice ---
        state_idx = state[trial]
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        chosen_a1 = action_1[trial]
        chosen_a2 = action_2[trial]
        
        delta_stage1 = q_stage2_mf[state_idx, chosen_a2] - q_stage1_mf[chosen_a1]
        q_stage1_mf[chosen_a1] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, chosen_a2]
        q_stage2_mf[state_idx, chosen_a2] += learning_rate * delta_stage2
        
        # Update history
        prev_reward = reward[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```