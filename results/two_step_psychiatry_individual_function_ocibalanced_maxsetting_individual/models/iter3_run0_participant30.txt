Here are the three proposed cognitive models. They build upon the success of the "Model-Free with Decay" mechanism by introducing flexibility in learning rates and exploration parameters to better capture the participant's behavior.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Q-Learning with Asymmetric Learning Rates and Passive Decay.
    Differentiates learning from positive vs negative prediction errors to capture 
    potential optimism or pessimism bias, while maintaining the decay mechanism 
    to handle drifting probabilities.
    
    Parameters:
    lr_pos: learning rate for positive prediction errors [0,1]
    lr_neg: learning rate for negative prediction errors [0,1]
    beta: inverse temperature [0,10]
    lambda_eligibility: eligibility trace decay [0,1]
    decay: rate at which values decay to 0.5 [0,1]
    perseveration: stickiness to previous choice [0,5]
    """
    lr_pos, lr_neg, beta, lambda_eligibility, decay, perseveration = model_parameters
    n_trials = len(action_1)

    q_stage1_mf = np.full(2, 0.5)
    q_stage2_mf = np.full((2, 2), 0.5)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    last_action_1 = -1
    
    for trial in range(n_trials):
        # Handle missing trials
        if action_1[trial] == -1:
            last_action_1 = -1
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # Passive Decay
        q_stage1_mf = q_stage1_mf * (1 - decay) + 0.5 * decay
        q_stage2_mf = q_stage2_mf * (1 - decay) + 0.5 * decay

        # Stage 1 Choice
        q_net_1 = q_stage1_mf.copy()
        if last_action_1 != -1:
            q_net_1[last_action_1] += perseveration
            
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # Stage 2 Choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # Prediction Errors
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]

        # Update Stage 1 (Immediate)
        lr_s1 = lr_pos if delta_stage1 > 0 else lr_neg
        q_stage1_mf[action_1[trial]] += lr_s1 * delta_stage1
        
        # Update Stage 1 (Eligibility Trace)
        lr_trace = lr_pos if delta_stage2 > 0 else lr_neg
        q_stage1_mf[action_1[trial]] += lr_trace * lambda_eligibility * delta_stage2
        
        # Update Stage 2
        lr_s2 = lr_pos if delta_stage2 > 0 else lr_neg
        q_stage2_mf[state_idx, action_2[trial]] += lr_s2 * delta_stage2
        
        last_action_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Q-Learning with Separate Stage Learning Rates and Passive Decay.
    Allows different learning speeds for the abstract spaceship choice (Stage 1) 
    and the concrete alien choice (Stage 2), combined with decay for non-stationarity.
    
    Parameters:
    lr_1: learning rate for stage 1 [0,1]
    lr_2: learning rate for stage 2 [0,1]
    beta: inverse temperature [0,10]
    lambda_eligibility: eligibility trace decay [0,1]
    decay: rate at which values decay to 0.5 [0,1]
    perseveration: stickiness to previous choice [0,5]
    """
    lr_1, lr_2, beta, lambda_eligibility, decay, perseveration = model_parameters
    n_trials = len(action_1)

    q_stage1_mf = np.full(2, 0.5)
    q_stage2_mf = np.full((2, 2), 0.5)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    last_action_1 = -1
    
    for trial in range(n_trials):
        if action_1[trial] == -1:
            last_action_1 = -1
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # Passive Decay
        q_stage1_mf = q_stage1_mf * (1 - decay) + 0.5 * decay
        q_stage2_mf = q_stage2_mf * (1 - decay) + 0.5 * decay

        # Stage 1 Choice
        q_net_1 = q_stage1_mf.copy()
        if last_action_1 != -1:
            q_net_1[last_action_1] += perseveration
            
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # Stage 2 Choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # Prediction Errors
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]

        # Update Q1 with lr_1
        q_stage1_mf[action_1[trial]] += lr_1 * delta_stage1 + lr_1 * lambda_eligibility * delta_stage2
        
        # Update Q2 with lr_2
        q_stage2_mf[state_idx, action_2[trial]] += lr_2 * delta_stage2
        
        last_action_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Q-Learning with Separate Stage Inverse Temperatures and Passive Decay.
    Allows different levels of exploration/noise for Stage 1 vs Stage 2 choices,
    acknowledging that the decision context (spaceship vs alien) differs.
    
    Parameters:
    learning_rate: rate of value updating [0,1]
    beta_1: inverse temperature for stage 1 [0,10]
    beta_2: inverse temperature for stage 2 [0,10]
    lambda_eligibility: eligibility trace decay [0,1]
    decay: rate at which values decay to 0.5 [0,1]
    perseveration: stickiness to previous choice [0,5]
    """
    learning_rate, beta_1, beta_2, lambda_eligibility, decay, perseveration = model_parameters
    n_trials = len(action_1)

    q_stage1_mf = np.full(2, 0.5)
    q_stage2_mf = np.full((2, 2), 0.5)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    last_action_1 = -1
    
    for trial in range(n_trials):
        if action_1[trial] == -1:
            last_action_1 = -1
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # Passive Decay
        q_stage1_mf = q_stage1_mf * (1 - decay) + 0.5 * decay
        q_stage2_mf = q_stage2_mf * (1 - decay) + 0.5 * decay

        # Stage 1 Choice (beta_1)
        q_net_1 = q_stage1_mf.copy()
        if last_action_1 != -1:
            q_net_1[last_action_1] += perseveration
            
        exp_q1 = np.exp(beta_1 * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # Stage 2 Choice (beta_2)
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # Prediction Errors
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]

        # Updates
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1 + learning_rate * lambda_eligibility * delta_stage2
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        last_action_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```