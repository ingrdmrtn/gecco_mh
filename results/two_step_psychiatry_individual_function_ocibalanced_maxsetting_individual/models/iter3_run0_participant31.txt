Here are three new cognitive models for the two-step task.

### Model 1: Hybrid Agent with Reward-Dependent Stickiness
This model extends the standard hybrid model by separating the choice perseveration (stickiness) parameter into two components: one for when the previous trial was rewarded (`stick_win`) and one for when it was unrewarded (`stick_loss`). This captures "Win-Stay, Lose-Shift" heuristics that may operate independently of value-based learning.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Agent with Reward-Dependent Stickiness.
    
    Distinguishes between stickiness after a reward (Win-Stay) and 
    stickiness after no reward (Lose-Shift or Lose-Stay).
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for Q-values.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight (1 = pure MB, 0 = pure MF).
    - stick_win: [-5, 5] Stickiness bonus after a rewarded trial.
    - stick_loss: [-5, 5] Stickiness bonus after an unrewarded trial.
    """
    learning_rate, beta, w, stick_win, stick_loss = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action

    last_action_1 = -1
    last_reward = -1

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2

        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits = q_net.copy()
        
        # Apply reward-dependent stickiness
        if last_action_1 != -1:
            if last_reward == 1:
                logits[last_action_1] += stick_win
            else:
                logits[last_action_1] += stick_loss
            
        exp_q1 = np.exp(beta * logits)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # Update Stage 1 MF Q-values (TD(0))
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Update Stage 2 MF Q-values
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        last_action_1 = action_1[trial]
        last_reward = reward[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Dual-Beta Hybrid Agent with Stickiness
This model posits that the level of decision noise (exploration) differs between the high-level planning stage (choosing a spaceship) and the low-level bandit stage (choosing an alien). It uses separate inverse temperature parameters (`beta_1` and `beta_2`) for the two stages, combined with choice perseveration.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Dual-Beta Hybrid Agent with Stickiness.
    
    Uses separate inverse temperature parameters for Stage 1 and Stage 2,
    allowing for different levels of exploration/exploitation at each step.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta_1: [0, 10] Inverse temperature for Stage 1 (Spaceship choice).
    - beta_2: [0, 10] Inverse temperature for Stage 2 (Alien choice).
    - w: [0, 1] Mixing weight (MB vs MF).
    - stickiness: [-5, 5] Bonus added to the previously chosen Stage 1 action.
    """
    learning_rate, beta_1, beta_2, w, stickiness = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    last_action_1 = -1

    for trial in range(n_trials):

        # --- Stage 1 Policy (uses beta_1) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2

        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf

        logits = q_net.copy()
        if last_action_1 != -1:
            logits[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta_1 * logits)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy (uses beta_2) ---
        # No stickiness applied to Stage 2 in this model
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        last_action_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Agent with Multi-Stage Stickiness
This model applies choice perseveration (stickiness) to *both* decision stages. In Stage 1, it biases the spaceship choice. In Stage 2, it biases the alien choice based on the last action taken *in that specific state* (planet). This accounts for habit formation at both levels of the task hierarchy.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Agent with Multi-Stage Stickiness.
    
    Applies stickiness to both the Stage 1 choice (Spaceship) and the 
    Stage 2 choice (Alien). Stage 2 stickiness is state-dependent (i.e., 
    repeating the same alien on the same planet).
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature (shared).
    - w: [0, 1] Mixing weight.
    - stick_1: [-5, 5] Stickiness for Stage 1 choice.
    - stick_2: [-5, 5] Stickiness for Stage 2 choice.
    """
    learning_rate, beta, w, stick_1, stick_2 = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    last_action_1 = -1
    # Track the last action taken in each state (planet 0 and planet 1)
    last_action_2_per_state = np.array([-1, -1])

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2

        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf

        logits_1 = q_net.copy()
        if last_action_1 != -1:
            logits_1[last_action_1] += stick_1
            
        exp_q1 = np.exp(beta * logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        logits_2 = q_stage2_mf[state_idx].copy()
        
        # Apply stickiness for the specific planet
        prev_alien = last_action_2_per_state[state_idx]
        if prev_alien != -1:
            logits_2[prev_alien] += stick_2

        exp_q2 = np.exp(beta * logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        last_action_1 = action_1[trial]
        last_action_2_per_state[state_idx] = action_2[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```