Here are the 3 proposed cognitive models.

### Model 1: Choice Trace (Habit) Model
This model extends the standard hybrid model by incorporating an "eligibility trace" of past choices (habit). Unlike simple stickiness (which only considers the previous trial), this model maintains a decaying trace of choice history. This accounts for the participant's tendency to persevere with a spaceship choice (e.g., Spaceship 0) over long runs, even when rewards are intermittent.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Choice Trace (Habit).
    
    Incorporates a decaying trace of past stage-1 choices to model habit formation.
    The trace acts as a bias added to the stage-1 decision values.
    
    Parameters:
    - learning_rate: [0,1] Rate of updating value based on reward.
    - beta: [0,10] Inverse temperature.
    - w: [0,1] Weight of Model-Based system (1=Pure MB, 0=Pure MF).
    - trace_decay: [0,1] Decay rate of the choice trace (0=instant forgetting, 1=perfect memory).
    - trace_weight: [0,5] Strength of the habit influence on choice.
    """
    learning_rate, beta, w, trace_decay, trace_weight = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Initialize choice trace (habit)
    choice_trace = np.zeros(2)

    for trial in range(n_trials):
        if action_1[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2

        # Combine MB, MF, and Habit Trace
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf + trace_weight * choice_trace
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Policy ---
        state_idx = state[trial]
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]

        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # 1. Update Choice Trace
        # Decay existing trace
        choice_trace *= trace_decay
        # Reinforce chosen action
        choice_trace[a1] += 1.0
        
        # 2. Update MF Values
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Transition-Dependent Learning Rates
This model hypothesizes that the participant learns differently from "Common" (expected) transitions versus "Rare" (unexpected) transitions. In the two-step task, rare transitions often cause "Model-Free" agents to erroneously reinforce the first-stage action. By splitting the Stage 1 learning rate, this model can capture if the participant suppresses learning from rare transitions (a sign of model-based reasoning filtering into the MF update) or over-reacts to them.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Transition-Dependent Learning Rates.
    
    Applies different learning rates for the Stage 1 update depending on whether
    the transition experienced was Common or Rare.
    
    Parameters:
    - lr_common: [0,1] Learning rate when transition was Common (0->0 or 1->1).
    - lr_rare: [0,1] Learning rate when transition was Rare (0->1 or 1->0).
    - beta: [0,10] Inverse temperature.
    - w: [0,1] Weight of Model-Based system.
    """
    lr_common, lr_rare, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        if action_1[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2

        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Policy ---
        state_idx = state[trial]
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]

        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        
        # Determine if transition was common or rare
        # Common: Action 0 -> State 0, Action 1 -> State 1
        is_common = (a1 == state_idx)
        current_lr = lr_common if is_common else lr_rare

        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += current_lr * delta_stage1
        
        # Stage 2 always uses lr_common (or could be a separate param, but using lr_common for parsimony)
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += lr_common * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid with Decay and Stickiness
This model combines the mechanism from the "best model so far" (decay of unchosen options) with simple perseveration (stickiness). The decay mechanism helps the agent re-explore options that haven't been chosen recently (counteracting the changing alien probabilities), while the stickiness parameter accounts for the immediate tendency to repeat the last action regardless of reward, which is a prominent feature in the provided data.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Unchosen Decay and Simple Stickiness.
    
    Combines Q-learning with decay for unchosen options (to encourage re-exploration)
    and a stickiness bonus for the previously chosen action (perseveration).
    
    Parameters:
    - learning_rate: [0,1] Rate of updating value based on reward.
    - decay_rate: [0,1] Rate at which unchosen action values decay to 0.
    - beta: [0,10] Inverse temperature.
    - w: [0,1] Weight of Model-Based system.
    - stickiness: [0,5] Bonus added to the previously chosen action's value.
    """
    learning_rate, decay_rate, beta, w, stickiness = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            last_action_1 = -1
            continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2

        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness bonus to logits
        logits_1 = beta * q_net_stage1
        if last_action_1 != -1:
            logits_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Policy ---
        state_idx = state[trial]
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]

        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

        # Decay unchosen options
        unchosen_a1 = 1 - a1
        q_stage1_mf[unchosen_a1] *= (1.0 - decay_rate)

        unchosen_a2 = 1 - a2
        q_stage2_mf[state_idx, unchosen_a2] *= (1.0 - decay_rate)
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```