Here are three new cognitive models for the two-step task.

### Model 1: Asymmetric Learning with Separate Betas
This model hypothesizes that the participant updates their value estimates differently depending on whether the prediction error is positive (better than expected) or negative (worse than expected). This "Win-Stay, Lose-Stay" (if `lr_neg` is low) or "Win-Stay, Lose-Shift" dynamic is combined with separate inverse temperature parameters for the spaceship and alien stages to capture stage-specific exploration levels.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Asymmetric Learning Rates Model with Separate Stage Betas.
    
    This model allows for different learning rates for positive and negative prediction errors
    (lr_pos, lr_neg). This can capture asymmetric sensitivity to rewards (e.g., ignoring losses
    if lr_neg is low). It also uses separate inverse temperatures for the two stages.
    
    Parameters:
    lr_pos: Learning rate for positive prediction errors [0,1]
    lr_neg: Learning rate for negative prediction errors [0,1]
    beta_1: Inverse temperature for Stage 1 (Spaceship) [0,10]
    beta_2: Inverse temperature for Stage 2 (Alien) [0,10]
    w: Weight mixing Model-Based and Model-Free values [0,1]
    lam: Eligibility trace parameter [0,1]
    """
    lr_pos, lr_neg, beta_1, beta_2, w, lam = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_1 * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        s1_choice = action_1[trial]
        state_idx = state[trial]
        
        # --- Stage 2 Decision ---
        qs_stage2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta_2 * qs_stage2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        s2_choice = action_2[trial]
        r = reward[trial]
        
        # --- Learning ---
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, s2_choice] - q_stage1_mf[s1_choice]
        lr_1 = lr_pos if delta_stage1 > 0 else lr_neg
        q_stage1_mf[s1_choice] += lr_1 * delta_stage1
        
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[state_idx, s2_choice]
        lr_2 = lr_pos if delta_stage2 > 0 else lr_neg
        q_stage2_mf[state_idx, s2_choice] += lr_2 * delta_stage2
        
        # Eligibility Trace (update Stage 1 based on Stage 2 outcome)
        # We use the learning rate associated with the Stage 2 error type
        q_stage1_mf[s1_choice] += lr_2 * lam * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Decay Model with Perseveration
This model introduces a `decay` parameter. Since the alien reward probabilities change slowly, older value estimates become unreliable. The decay mechanism pulls all Q-values toward a neutral baseline (0.5) on every trial, representing forgetting or active uncertainty management. This is combined with perseveration (`p`) to capture the participant's tendency to repeat spaceship choices.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Decay Model with Perseveration.
    
    Incorporates a decay parameter that pulls Q-values toward 0.5 (uncertainty/neutral) 
    on each trial, modeling forgetting or adaptation to non-stationary environments. 
    Includes perseveration and separate betas.
    
    Parameters:
    lr: Learning rate [0,1]
    beta_1: Inverse temperature for Stage 1 [0,10]
    beta_2: Inverse temperature for Stage 2 [0,10]
    w: Weight mixing Model-Based and Model-Free values [0,1]
    p: Perseveration (stickiness) to previous Stage 1 choice [0,1]
    lam: Eligibility trace parameter [0,1]
    decay: Decay rate of Q-values toward 0.5 [0,1]
    """
    lr, beta_1, beta_2, w, p, lam, decay = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if prev_action_1 != -1:
            q_net_stage1[prev_action_1] += p
            
        exp_q1 = np.exp(beta_1 * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        s1_choice = action_1[trial]
        state_idx = state[trial]
        prev_action_1 = s1_choice
        
        # --- Stage 2 Decision ---
        qs_stage2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta_2 * qs_stage2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        s2_choice = action_2[trial]
        r = reward[trial]
        
        # --- Learning ---
        delta_stage1 = q_stage2_mf[state_idx, s2_choice] - q_stage1_mf[s1_choice]
        q_stage1_mf[s1_choice] += lr * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[state_idx, s2_choice]
        q_stage2_mf[state_idx, s2_choice] += lr * delta_stage2
        
        q_stage1_mf[s1_choice] += lr * lam * delta_stage2
        
        # --- Decay ---
        # Decay all values toward 0.5 (center of 0-1 reward space)
        q_stage1_mf = q_stage1_mf * (1 - decay) + 0.5 * decay
        q_stage2_mf = q_stage2_mf * (1 - decay) + 0.5 * decay

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Stage-Specific Learning Rates
This model decouples the learning plasticity of the two stages. It proposes that the participant learns the value of spaceships (Stage 1) at a different rate than the value of aliens (Stage 2). This allows for stable preferences in the first stage even while rapidly updating estimates for the changing alien rewards in the second stage.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Stage-Specific Learning Rates with Perseveration.
    
    Uses separate learning rates for Stage 1 (Spaceship) and Stage 2 (Alien) value updates.
    This allows the model to learn the structure/habit of spaceships at a different speed
    than the reward probabilities of aliens.
    
    Parameters:
    lr_1: Learning rate for Stage 1 MF values [0,1]
    lr_2: Learning rate for Stage 2 values [0,1]
    beta_1: Inverse temperature for Stage 1 [0,10]
    beta_2: Inverse temperature for Stage 2 [0,10]
    w: Weight mixing Model-Based and Model-Free values [0,1]
    p: Perseveration (stickiness) to previous Stage 1 choice [0,1]
    lam: Eligibility trace parameter [0,1]
    """
    lr_1, lr_2, beta_1, beta_2, w, p, lam = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if prev_action_1 != -1:
            q_net_stage1[prev_action_1] += p
            
        exp_q1 = np.exp(beta_1 * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        s1_choice = action_1[trial]
        state_idx = state[trial]
        prev_action_1 = s1_choice
        
        # --- Stage 2 Decision ---
        qs_stage2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta_2 * qs_stage2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        s2_choice = action_2[trial]
        r = reward[trial]
        
        # --- Learning ---
        delta_stage1 = q_stage2_mf[state_idx, s2_choice] - q_stage1_mf[s1_choice]
        # Update Stage 1 MF value with lr_1
        q_stage1_mf[s1_choice] += lr_1 * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[state_idx, s2_choice]
        # Update Stage 2 value with lr_2
        q_stage2_mf[state_idx, s2_choice] += lr_2 * delta_stage2
        
        # Eligibility trace: connects Stage 2 RPE to Stage 1 Value
        # Uses lr_1 because it updates the Stage 1 weight
        q_stage1_mf[s1_choice] += lr_1 * lam * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```