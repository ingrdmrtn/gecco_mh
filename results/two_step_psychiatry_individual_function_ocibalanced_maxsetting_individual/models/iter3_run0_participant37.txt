Here are the three cognitive models implemented as Python functions.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Dual Learning Rate Model with Stickiness.
    This model assumes the participant learns the value of the first-stage choice (Spaceships)
    and the second-stage choice (Aliens) at different rates. It also includes a stickiness
    parameter to account for choice perseveration in the first stage.
    
    Parameters:
    lr_1: [0,1] - Learning rate for Stage 1 (Spaceship choice).
    lr_2: [0,1] - Learning rate for Stage 2 (Alien choice).
    beta: [0,10] - Inverse temperature (exploration/exploitation trade-off).
    w: [0,1] - Weight of Model-Based system (0=MF, 1=MB).
    stickiness: [0,5] - Bonus added to the previously chosen action in Stage 1.
    """
    lr_1, lr_2, beta, w, stickiness = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action

    last_action = -1

    for trial in range(n_trials):
        # --- Stage 1: Spaceship Selection ---
        if action_1[trial] == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue
            
        # Model-Based Value: Expected value of next state (max Q of stage 2) weighted by transition probs
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_hybrid = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add Stickiness to the previously chosen action
        q_choice_1 = q_hybrid.copy()
        if last_action != -1:
            q_choice_1[last_action] += stickiness
            
        # Softmax for Stage 1
        exp_q1 = np.exp(beta * q_choice_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Check for missing state or stage 2 action
        if state[trial] == -1 or action_2[trial] == -1:
            p_choice_2[trial] = 1.0
            last_action = action_1[trial]
            continue

        state_idx = state[trial]

        # --- Stage 2: Alien Selection ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Value Updates ---
        # Stage 1 PE: Difference between Value of chosen State-Action pair and Stage 1 Value
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        # Stage 2 PE: Difference between Reward and Stage 2 Value
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Update with separate learning rates
        q_stage1_mf[action_1[trial]] += lr_1 * delta_stage1
        q_stage2_mf[state_idx, action_2[trial]] += lr_2 * delta_stage2
        
        last_action = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Asymmetric Learning Rate Model with Stickiness.
    This model assumes the participant updates their value estimates differently depending on 
    whether the outcome was better (positive prediction error) or worse (negative prediction error) 
    than expected. It also includes choice stickiness.
    
    Parameters:
    alpha_pos: [0,1] - Learning rate for positive prediction errors.
    alpha_neg: [0,1] - Learning rate for negative prediction errors.
    beta: [0,10] - Inverse temperature.
    w: [0,1] - Weight of Model-Based system.
    stickiness: [0,5] - Bonus added to the previously chosen action.
    """
    alpha_pos, alpha_neg, beta, w, stickiness = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    last_action = -1

    for trial in range(n_trials):
        if action_1[trial] == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # Stage 1 Choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_hybrid = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        q_choice_1 = q_hybrid.copy()
        if last_action != -1:
            q_choice_1[last_action] += stickiness
            
        exp_q1 = np.exp(beta * q_choice_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        if state[trial] == -1 or action_2[trial] == -1:
            p_choice_2[trial] = 1.0
            last_action = action_1[trial]
            continue

        state_idx = state[trial]

        # Stage 2 Choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # Updates
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Asymmetric Update Stage 1
        lr_1 = alpha_pos if delta_stage1 > 0 else alpha_neg
        q_stage1_mf[action_1[trial]] += lr_1 * delta_stage1
        
        # Asymmetric Update Stage 2
        lr_2 = alpha_pos if delta_stage2 > 0 else alpha_neg
        q_stage2_mf[state_idx, action_2[trial]] += lr_2 * delta_stage2
        
        last_action = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Accumulating Stickiness (Choice Trace) Model.
    Instead of a simple "last action" bonus, this model maintains a decaying trace of past choices.
    This allows the model to capture stronger habit formation or inertia that builds up over 
    consecutive repetitions of the same choice.
    
    Parameters:
    learning_rate: [0,1] - Learning rate.
    beta: [0,10] - Inverse temperature.
    w: [0,1] - Weight of Model-Based system.
    trace_decay: [0,1] - Decay rate of the choice trace (0 = instant forgetting, 1 = perfect accumulation).
    stickiness_weight: [0,5] - Weight of the accumulated choice trace in decision making.
    """
    learning_rate, beta, w, trace_decay, stickiness_weight = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Choice trace for Stage 1 actions (initialized to 0)
    choice_trace = np.zeros(2)

    for trial in range(n_trials):
        if action_1[trial] == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # Stage 1 Choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_hybrid = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add Accumulating Stickiness (Trace)
        q_choice_1 = q_hybrid + stickiness_weight * choice_trace
            
        exp_q1 = np.exp(beta * q_choice_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Update Choice Trace (Decay then Increment)
        choice_trace *= trace_decay
        choice_trace[action_1[trial]] += 1.0
        
        if state[trial] == -1 or action_2[trial] == -1:
            p_choice_2[trial] = 1.0
            continue

        state_idx = state[trial]

        # Stage 2 Choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # Updates
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```