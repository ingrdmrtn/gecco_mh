Here are three new cognitive models for the two-step task, expressed as Python functions.

### Model 1: Dynamic Transition Hybrid Model
This model hypothesizes that the participant does not rely on a fixed transition matrix (e.g., 70/30) but learns the transition probabilities between spaceships and planets over time. It combines this dynamic Model-Based planning with Model-Free reinforcement learning.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Dynamic Transition Hybrid Model.
    
    The agent learns the transition matrix (Spaceship -> Planet) online using a
    Rescorla-Wagner rule, rather than assuming fixed probabilities. 
    It mixes Model-Based (using the learned transitions) and Model-Free values.

    Parameters:
    - lr_val: [0, 1] Learning rate for Stage 2 alien values (Q-values).
    - lr_trans: [0, 1] Learning rate for updating transition probabilities.
    - beta_1: [0, 10] Inverse temperature for Stage 1 choices.
    - beta_2: [0, 10] Inverse temperature for Stage 2 choices.
    - w: [0, 1] Mixing weight. 0 = Pure MF, 1 = Pure MB.
    """
    lr_val, lr_trans, beta_1, beta_2, w = model_parameters
    n_trials = len(action_1)

    # Initialize transition matrix (uniform prior: 0.5, 0.5)
    # Row: Spaceship (0, 1), Col: Planet (0, 1)
    transition_matrix = np.ones((2, 2)) * 0.5
    
    # Q-values
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # Skip invalid trials if any
        if action_1[trial] == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        # --- Stage 1 Decision ---
        # Model-Based Value: T * max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        # --- Transition ---
        s1 = action_1[trial]
        s2 = state[trial]
        
        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta_2 * q_stage2_mf[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning ---
        a2 = action_2[trial]
        r = reward[trial]

        # 1. Update Stage 2 Q-values (Model-Free)
        # Prediction error for Stage 2
        delta_stage2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += lr_val * delta_stage2

        # 2. Update Stage 1 Q-values (Model-Free TD(0))
        # Note: Standard Hybrid often uses TD(0) for the MF part of Stage 1
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[s1]
        q_stage1_mf[s1] += lr_val * delta_stage1

        # 3. Update Transition Matrix (Model-Based Learning)
        # Update the row corresponding to the chosen spaceship
        # Increase prob of observed planet, decrease others
        # T[s1, s2] <- T[s1, s2] + lr_trans * (1 - T[s1, s2])
        # T[s1, other] <- T[s1, other] + lr_trans * (0 - T[s1, other])
        
        # Vectorized update for the row s1
        target = np.zeros(2)
        target[s2] = 1.0
        transition_matrix[s1] = transition_matrix[s1] + lr_trans * (target - transition_matrix[s1])

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Model-Free with Outcome-Dependent Perseveration
This model extends the best-performing Model-Free with Eligibility Traces (TD-Lambda) by adding outcome-specific stickiness. It tests the hypothesis that the participant's tendency to repeat an action depends explicitly on whether the previous outcome was a reward (Win-Stay) or a lack of reward (Lose-Shift/Stay), separate from value learning.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Agent with Outcome-Dependent Perseveration (Win-Stay/Lose-Stick).
    
    Adds a "stickiness" bonus to the previously chosen Stage 1 action.
    The magnitude of this bonus depends on whether the previous trial was rewarded.
    
    Parameters:
    - learning_rate: [0, 1] Value update rate.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - lam: [0, 1] Eligibility trace (0=TD, 1=Monte Carlo).
    - stick_win: [0, 10] Stickiness bonus after a Reward=1 trial.
    - stick_loss: [0, 10] Stickiness bonus after a Reward=0 trial.
    """
    learning_rate, beta_1, beta_2, lam, stick_win, stick_loss = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track previous action and reward for stickiness
    prev_a1 = -1
    prev_reward = -1

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue

        # --- Stage 1 Decision ---
        # Add stickiness to Q-values (temporarily for choice probability)
        q_net_1 = q_stage1_mf.copy()
        
        if prev_a1 != -1:
            if prev_reward == 1:
                q_net_1[prev_a1] += stick_win
            else:
                q_net_1[prev_a1] += stick_loss

        exp_q1 = np.exp(beta_1 * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Decision ---
        s2 = state[trial]
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]

        exp_q2 = np.exp(beta_2 * q_stage2_mf[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += learning_rate * delta_stage2

        # Stage 1 Update (with Eligibility Trace)
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * (delta_stage1 + lam * delta_stage2)
        
        # Update history
        prev_a1 = a1
        prev_reward = r

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Asymmetric Hybrid Model
This model hypothesizes that the participant learns differently from positive prediction errors (better than expected) versus negative prediction errors (worse than expected). It applies this asymmetric learning to the value updates within a Hybrid (Model-Based + Model-Free) framework.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Asymmetric Hybrid Model.
    
    A Hybrid agent that uses different learning rates for positive and negative 
    prediction errors. This captures potential optimism or pessimism biases 
    in value updating.
    
    Parameters:
    - lr_pos: [0, 1] Learning rate for positive prediction errors (RPE > 0).
    - lr_neg: [0, 1] Learning rate for negative prediction errors (RPE < 0).
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - w: [0, 1] Mixing weight (0=MF, 1=MB).
    """
    lr_pos, lr_neg, beta_1, beta_2, w = model_parameters
    n_trials = len(action_1)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue

        # --- Stage 1 Decision ---
        # Model-Based
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Decision ---
        s2 = state[trial]
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]

        exp_q2 = np.exp(beta_2 * q_stage2_mf[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        # Stage 2 Update (Asymmetric)
        delta_stage2 = r - q_stage2_mf[s2, a2]
        if delta_stage2 >= 0:
            q_stage2_mf[s2, a2] += lr_pos * delta_stage2
        else:
            q_stage2_mf[s2, a2] += lr_neg * delta_stage2

        # Stage 1 Update (Asymmetric TD(0))
        # Note: We use the *updated* stage 2 value or the *pre-update*? 
        # Standard SARSA/Q uses bootstrapped value. Here we use the value of state arrived at.
        # We use q_stage2_mf[s2, a2] (which is the value of the state we landed in and action taken)
        
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        if delta_stage1 >= 0:
            q_stage1_mf[a1] += lr_pos * delta_stage1
        else:
            q_stage1_mf[a1] += lr_neg * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```