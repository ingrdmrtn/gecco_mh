Here are three new cognitive models for the two-step decision task, designed to capture different psychological mechanisms based on the participant's behavior.

### Model 1: Subjective Punishment Model
This model posits that the participant displays **loss aversion** or **subjective punishment** behavior. Instead of treating the absence of gold (0 coins) as a neutral outcome, the participant may perceive it as a loss. This is modeled by transforming the 0 reward into a negative value defined by `loss_mag`. This affects both the Model-Free value updates and the Model-Based value calculations (which rely on Stage 2 values).

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Subjective Punishment Model.
    Participants may perceive the absence of reward (0) as a loss (negative utility).
    This model introduces a parameter `loss_mag` which transforms the 0 reward into a negative value.
    
    Parameters:
    - lr: Learning rate [0,1]
    - beta_1: Inverse temperature for stage 1 [0,10]
    - beta_2: Inverse temperature for stage 2 [0,10]
    - w: Model-based weight [0,1]
    - stick: Choice stickiness for stage 1 [0,5]
    - loss_mag: Magnitude of subjective loss for 0 reward [0,5]
    """
    lr, beta_1, beta_2, w, stick, loss_mag = model_parameters
    n_trials = len(action_1)
    
    # Transition matrix: A(0)->X(0) 0.7, A(0)->Y(1) 0.3; U(1)->Y(1) 0.7, U(1)->X(0) 0.3
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    log_loss = 0.0
    eps = 1e-10
    prev_action_1 = -1
    
    for trial in range(n_trials):
        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Apply stickiness
        q_net_stick = q_net.copy()
        if prev_action_1 != -1:
            q_net_stick[prev_action_1] += stick
            
        exp_q1 = np.exp(beta_1 * q_net_stick)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = action_1[trial]
        log_loss -= np.log(probs_1[a1] + eps)
        
        # Stage 2
        a2 = action_2[trial]
        if a2 == -1: 
            prev_action_1 = a1
            continue
            
        state_idx = state[trial]
        
        # Stage 2 Policy
        qs_2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta_2 * qs_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        log_loss -= np.log(probs_2[a2] + eps)
        
        # Reward Transformation
        r = reward[trial]
        effective_r = r
        if r == 0:
            effective_r = -loss_mag
            
        # Updates
        # Stage 2 Update
        delta_stage2 = effective_r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += lr * delta_stage2
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        prev_action_1 = a1
        
    return log_loss
```

### Model 2: Habitual Persistence Model
This model replaces the simple 1-step "stickiness" parameter with a dynamic **Habit Trace**. The participant builds up a habit for chosen actions that decays over time. This allows the model to capture the extended "streaks" of behavior seen in the data (e.g., repeatedly choosing Spaceship 1 despite failures) more effectively than simple stickiness.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Habitual Persistence Model.
    Replaces 1-step stickiness with a decaying habit trace.
    Choices build up a 'habit' strength that decays over time, capturing longer-term persistence.
    
    Parameters:
    - lr: Learning rate [0,1]
    - beta_1: Inverse temperature for stage 1 [0,10]
    - beta_2: Inverse temperature for stage 2 [0,10]
    - w: Model-based weight [0,1]
    - habit_decay: Decay rate of the habit trace (0=instant decay, 1=no decay) [0,1]
    - habit_w: Weight of the habit trace in decision making [0,10]
    """
    lr, beta_1, beta_2, w, habit_decay, habit_w = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    habit_trace = np.zeros(2)
    
    log_loss = 0.0
    eps = 1e-10
    
    for trial in range(n_trials):
        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add habit influence
        logits = beta_1 * q_net + habit_w * habit_trace
        
        # Softmax with stability shift
        logits_shifted = logits - np.max(logits)
        exp_q1 = np.exp(logits_shifted)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = action_1[trial]
        log_loss -= np.log(probs_1[a1] + eps)
        
        # Update Habit Trace
        habit_trace *= habit_decay
        habit_trace[a1] += 1.0
        
        # Stage 2
        a2 = action_2[trial]
        if a2 == -1:
            continue
            
        state_idx = state[trial]
        
        qs_2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta_2 * qs_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        log_loss -= np.log(probs_2[a2] + eps)
        
        r = reward[trial]
        
        # Updates
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += lr * delta_stage2
        
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
    return log_loss
```

### Model 3: Epsilon-Greedy Lapse Model
This model incorporates a "lapse rate" or **Epsilon-Greedy** mechanism. It assumes that on a small fraction of trials (`epsilon`), the participant chooses randomly (uniformly) rather than based on learned values. This accounts for attentional lapses or undirected exploration, which is a standard mechanism in psychophysics to handle noise that the softmax temperature (`beta`) cannot fully capture.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Epsilon-Greedy Lapse Model.
    Assumes that with small probability 'epsilon', the participant makes a random choice
    due to attentional lapses or exploration, independent of value.
    
    Parameters:
    - lr: Learning rate [0,1]
    - beta_1: Inverse temperature for stage 1 [0,10]
    - beta_2: Inverse temperature for stage 2 [0,10]
    - w: Model-based weight [0,1]
    - stick: Choice stickiness for stage 1 [0,5]
    - epsilon: Probability of random choice (lapse rate) [0,1]
    """
    lr, beta_1, beta_2, w, stick, epsilon = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    log_loss = 0.0
    eps = 1e-10
    prev_action_1 = -1
    
    for trial in range(n_trials):
        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        q_net_stick = q_net.copy()
        if prev_action_1 != -1:
            q_net_stick[prev_action_1] += stick
            
        exp_q1 = np.exp(beta_1 * q_net_stick)
        softmax_probs_1 = exp_q1 / np.sum(exp_q1)
        
        # Epsilon-Greedy Mixture
        probs_1 = (1 - epsilon) * softmax_probs_1 + epsilon * 0.5
        
        a1 = action_1[trial]
        log_loss -= np.log(probs_1[a1] + eps)
        
        # Stage 2
        a2 = action_2[trial]
        if a2 == -1:
            prev_action_1 = a1
            continue
            
        state_idx = state[trial]
        
        qs_2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta_2 * qs_2)
        softmax_probs_2 = exp_q2 / np.sum(exp_q2)
        
        # Epsilon applied to Stage 2 as well
        probs_2 = (1 - epsilon) * softmax_probs_2 + epsilon * 0.5
        
        log_loss -= np.log(probs_2[a2] + eps)
        
        r = reward[trial]
        
        # Updates
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += lr * delta_stage2
        
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        prev_action_1 = a1
        
    return log_loss
```