Here are three new cognitive models for the two-step decision task, designed to capture the specific behaviors observed in the participant data (e.g., strong perseverance/stickiness, potential asymmetric learning, or habit formation).

### Model 1: Asymmetric Learning Rates + Perseverance
This model hypothesizes that the participant learns differently from positive outcomes (better than expected) versus negative outcomes (worse than expected). This "optimism" or "pessimism" bias is combined with a perseverance bonus to account for the observed stickiness in spaceship choice. It uses a pure Model-Free approach.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Q-Learning with Asymmetric Learning Rates and Perseverance.
    
    This model assumes the participant learns differently from positive and negative
    prediction errors (e.g., optimism/pessimism bias) and has a tendency to repeat
    their previous Stage 1 choice.
    
    Parameters:
    - alpha_pos: [0, 1] Learning rate for positive prediction errors.
    - alpha_neg: [0, 1] Learning rate for negative prediction errors.
    - beta: [0, 10] Inverse temperature for choice stochasticity.
    - p: [0, 5] Perseverance bonus for repeating Stage 1 choice.
    """
    alpha_pos, alpha_neg, beta, p = model_parameters
    n_trials = len(action_1)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2)) # State x Action
    
    prev_action_1 = -1
    
    for trial in range(n_trials):
        # Stage 1 Choice
        q_net = q_stage1.copy()
        if prev_action_1 != -1:
            q_net[prev_action_1] += p
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Stage 2 Choice
        s = state[trial]
        exp_q2 = np.exp(beta * q_stage2[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # Updates
        # Stage 1 update (TD-0 using SARSA target)
        target_stage1 = q_stage2[s, action_2[trial]]
        delta1 = target_stage1 - q_stage1[action_1[trial]]
        alpha1 = alpha_pos if delta1 > 0 else alpha_neg
        q_stage1[action_1[trial]] += alpha1 * delta1
        
        # Stage 2 update (Reward prediction error)
        delta2 = reward[trial] - q_stage2[s, action_2[trial]]
        alpha2 = alpha_pos if delta2 > 0 else alpha_neg
        q_stage2[s, action_2[trial]] += alpha2 * delta2
        
        prev_action_1 = action_1[trial]
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Separate Stage Alphas + Win-Stay Lose-Shift Perseverance
Building on the success of separate learning rates for stages, this model refines the perseverance mechanism. Instead of a constant "stickiness," the perseverance bonus depends on the outcome of the previous trial (Win-Stay, Lose-Shift). This allows the model to explain why the participant might stick to a choice for long blocks (high `p_win`) but eventually switch (impact of `p_loss`).

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Q-Learning with Separate Stage Learning Rates and Outcome-Dependent Perseverance.
    
    This model separates learning speeds for the two stages (Spaceship vs Alien)
    and implements a Win-Stay Lose-Shift type mechanism where perseverance strength
    depends on the previous trial's outcome.
    
    Parameters:
    - alpha1: [0, 1] Learning rate for Stage 1 (Spaceship).
    - alpha2: [0, 1] Learning rate for Stage 2 (Alien).
    - beta: [0, 10] Inverse temperature.
    - p_win: [0, 5] Perseverance bonus after a rewarded trial.
    - p_loss: [0, 5] Perseverance bonus after an unrewarded trial.
    """
    alpha1, alpha2, beta, p_win, p_loss = model_parameters
    n_trials = len(action_1)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    prev_action_1 = -1
    prev_reward = 0
    
    for trial in range(n_trials):
        # Stage 1 Choice
        q_net = q_stage1.copy()
        if prev_action_1 != -1:
            bonus = p_win if prev_reward == 1 else p_loss
            q_net[prev_action_1] += bonus
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Stage 2 Choice
        s = state[trial]
        exp_q2 = np.exp(beta * q_stage2[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # Updates
        target_stage1 = q_stage2[s, action_2[trial]]
        delta1 = target_stage1 - q_stage1[action_1[trial]]
        q_stage1[action_1[trial]] += alpha1 * delta1
        
        delta2 = reward[trial] - q_stage2[s, action_2[trial]]
        q_stage2[s, action_2[trial]] += alpha2 * delta2
        
        prev_action_1 = action_1[trial]
        prev_reward = reward[trial]
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Choice Kernel Learning
This model replaces the simple 1-step perseverance with a "Choice Kernel." This kernel tracks the history of choices as an exponentially moving average, allowing "habits" to build up over time and decay slowly. This is distinct from value learning (`q_stage1`) and captures the momentum observed in the participant's long blocks of identical choices.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Q-Learning with Decaying Choice Kernel.
    
    Instead of a simple 1-step perseverance, this model maintains a choice kernel
    (exponential moving average of past choices) to capture longer-term habit formation.
    
    Parameters:
    - alpha: [0, 1] Learning rate for Q-values.
    - beta: [0, 10] Inverse temperature for Q-values.
    - alpha_k: [0, 1] Decay rate for the choice kernel (higher = faster decay/shorter memory).
    - beta_k: [0, 10] Weight of the choice kernel in the decision (habit strength).
    """
    alpha, beta, alpha_k, beta_k = model_parameters
    n_trials = len(action_1)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    choice_kernel = np.zeros(2)
    
    for trial in range(n_trials):
        # Stage 1 Choice
        # Decision variable combines Value (Q) and Habit (Choice Kernel)
        logits = beta * q_stage1 + beta_k * choice_kernel
        exp_q1 = np.exp(logits)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Stage 2 Choice
        s = state[trial]
        exp_q2 = np.exp(beta * q_stage2[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # Updates
        # Q-value updates
        target_stage1 = q_stage2[s, action_2[trial]]
        delta1 = target_stage1 - q_stage1[action_1[trial]]
        q_stage1[action_1[trial]] += alpha * delta1
        
        delta2 = reward[trial] - q_stage2[s, action_2[trial]]
        q_stage2[s, action_2[trial]] += alpha * delta2
        
        # Choice Kernel update
        # Update chosen action towards 1, unchosen towards 0
        a1 = action_1[trial]
        choice_kernel[a1] = choice_kernel[a1] + alpha_k * (1.0 - choice_kernel[a1])
        choice_kernel[1-a1] = choice_kernel[1-a1] + alpha_k * (0.0 - choice_kernel[1-a1])
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```