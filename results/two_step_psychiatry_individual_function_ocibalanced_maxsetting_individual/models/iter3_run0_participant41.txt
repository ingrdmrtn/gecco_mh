Here are the 3 cognitive models.

### Model 1: Dual-Learning-Rate Model-Free RL with Stickiness
This model posits that the participant learns the value of the two stages (choosing a spaceship vs. choosing an alien) at different rates. The first stage is a structural choice, while the second is a direct reward bandit. It builds on the "best model so far" (MF + Lambda + Stickiness) by splitting the learning rate into `alpha_1` and `alpha_2`.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Dual-Learning-Rate Model-Free Q-Learning with Eligibility Traces and Stickiness.
    
    Distinguishes between the learning rate for Stage 1 (Spaceship choice) and 
    Stage 2 (Alien choice), acknowledging that learning dynamics may differ 
    between structural transitions and direct reward associations.
    
    Parameters:
    alpha_1: [0,1] - Learning rate for Stage 1 Q-values.
    alpha_2: [0,1] - Learning rate for Stage 2 Q-values.
    beta_1: [0,10] - Inverse temperature for Stage 1.
    beta_2: [0,10] - Inverse temperature for Stage 2.
    lambda_param: [0,1] - Eligibility trace decay parameter.
    stickiness: [0,10] - Choice perseveration bonus for Stage 1.
    """
    alpha_1, alpha_2, beta_1, beta_2, lambda_param, stickiness = model_parameters
    n_trials = len(action_1)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_mf_stage1 = np.zeros(2)
    q_mf_stage2 = np.zeros((2, 2)) # State x Action
    
    prev_a1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        logits_1 = beta_1 * q_mf_stage1
        if prev_a1 != -1:
            logits_1[prev_a1] += stickiness
            
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        prev_a1 = a1
        
        state_idx = int(state[trial])

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta_2 * q_mf_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        r = reward[trial]

        # --- Learning ---
        # Stage 1 Prediction Error (TD(0))
        delta_1 = q_mf_stage2[state_idx, a2] - q_mf_stage1[a1]
        q_mf_stage1[a1] += alpha_1 * delta_1

        # Stage 2 Prediction Error
        delta_2 = r - q_mf_stage2[state_idx, a2]
        q_mf_stage2[state_idx, a2] += alpha_2 * delta_2

        # Eligibility Trace Update for Stage 1 using Stage 2 error
        # Note: We use alpha_1 here as it updates a Stage 1 Q-value
        q_mf_stage1[a1] += alpha_1 * lambda_param * delta_2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Asymmetric Hybrid Model
This model combines Model-Based (MB) and Model-Free (MF) reinforcement learning, but introduces asymmetric learning rates for positive and negative prediction errors. This captures potential risk-averse or risk-seeking behavior where the participant updates their internal value map differently after wins versus losses.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Asymmetric Hybrid Learner (MB/MF) with Differential Positive/Negative Learning.
    
    Mixes Model-Based and Model-Free values. The learning of the Stage 2 values 
    (which feed into both the MB planning and MF caching) uses different rates 
    for positive and negative prediction errors.
    
    Parameters:
    alpha_pos: [0,1] - Learning rate when prediction error is positive.
    alpha_neg: [0,1] - Learning rate when prediction error is negative.
    beta_1: [0,10] - Inverse temperature for Stage 1.
    beta_2: [0,10] - Inverse temperature for Stage 2.
    w: [0,1] - Weighting parameter (0 = Pure MF, 1 = Pure MB).
    """
    alpha_pos, alpha_neg, beta_1, beta_2, w = model_parameters
    n_trials = len(action_1)
    
    # Fixed transition matrix as per task description
    # A(0) -> X(0) 70%, Y(1) 30%
    # U(1) -> X(0) 30%, Y(1) 70%
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_mf_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2)) # Used by both systems

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based Value Calculation
        # V_MB(s1) = Sum(P(s2|s1) * max(Q_stage2(s2)))
        v_mb_stage1 = np.zeros(2)
        max_q2 = np.max(q_stage2, axis=1)
        v_mb_stage1 = transition_matrix @ max_q2
        
        # Net Q-value
        q_net_stage1 = w * v_mb_stage1 + (1 - w) * q_mf_stage1
        
        exp_q1 = np.exp(beta_1 * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        state_idx = int(state[trial])

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta_2 * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        r = reward[trial]

        # --- Learning ---
        # Update Stage 1 MF
        delta_1 = q_stage2[state_idx, a2] - q_mf_stage1[a1]
        # Use asymmetric alpha for stage 1 as well
        eff_alpha_1 = alpha_pos if delta_1 > 0 else alpha_neg
        q_mf_stage1[a1] += eff_alpha_1 * delta_1
        
        # Update Stage 2 (Common to MB and MF)
        delta_2 = r - q_stage2[state_idx, a2]
        eff_alpha_2 = alpha_pos if delta_2 > 0 else alpha_neg
        q_stage2[state_idx, a2] += eff_alpha_2 * delta_2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Decaying Model-Free RL with Stickiness
This model addresses the non-stationarity of the environment (alien reward probabilities change slowly). It implements a `decay_rate` that discounts the values of *unchosen* actions, ensuring that outdated information fades over time. It combines this with eligibility traces and stickiness to match the participant's perseverative behavior.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Decaying Model-Free Q-Learning with Eligibility Traces and Stickiness.
    
    Adds a memory decay mechanism to the standard MF+Lambda+Stickiness model.
    Unchosen action values decay towards zero, allowing the agent to 'forget' 
    old values in the changing environment.
    
    Parameters:
    learning_rate: [0,1] - Update rate for chosen actions.
    beta_1: [0,10] - Inverse temperature for Stage 1.
    beta_2: [0,10] - Inverse temperature for Stage 2.
    lambda_param: [0,1] - Eligibility trace decay.
    stickiness: [0,10] - Perseveration bonus.
    decay_rate: [0,1] - Rate at which unchosen Q-values decay (1 = full reset, 0 = no decay).
    """
    learning_rate, beta_1, beta_2, lambda_param, stickiness, decay_rate = model_parameters
    n_trials = len(action_1)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_mf_stage1 = np.zeros(2)
    q_mf_stage2 = np.zeros((2, 2))
    
    prev_a1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        logits_1 = beta_1 * q_mf_stage1
        if prev_a1 != -1:
            logits_1[prev_a1] += stickiness
            
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        prev_a1 = a1
        
        state_idx = int(state[trial])

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta_2 * q_mf_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        r = reward[trial]

        # --- Learning ---
        delta_1 = q_mf_stage2[state_idx, a2] - q_mf_stage1[a1]
        q_mf_stage1[a1] += learning_rate * delta_1

        delta_2 = r - q_mf_stage2[state_idx, a2]
        q_mf_stage2[state_idx, a2] += learning_rate * delta_2
        
        # Eligibility Trace
        q_mf_stage1[a1] += learning_rate * lambda_param * delta_2
        
        # --- Decay Unchosen ---
        # Decay unchosen stage 1
        unchosen_a1 = 1 - a1
        q_mf_stage1[unchosen_a1] *= (1.0 - decay_rate)
        
        # Decay unchosen stage 2 (in the current state)
        unchosen_a2 = 1 - a2
        q_mf_stage2[state_idx, unchosen_a2] *= (1.0 - decay_rate)
        
        # Note: We do not decay the values of the unvisited state in this implementation,
        # as the agent gained no new info about time passage there relative to current state.

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```