Here are three cognitive models designed to explain the participant's behavior in the two-step task.

### Model 1: Choice Kernel Hybrid Model
This model incorporates a **Choice Kernel** (CK) for the first-stage decision. Unlike simple perseveration which only considers the immediately preceding choice, a choice kernel accumulates a history of past choices, decaying over time. This mechanism is well-suited to explain the participant's data, which features long streaks of selecting the same spaceship (e.g., trials 45-62) followed by switches.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF model with a Choice Kernel (CK) on Stage 1.
    
    The Choice Kernel captures the tendency to repeat recently chosen actions 
    based on an exponentially decaying history, rather than just the immediately 
    previous choice. This accounts for the participant's long streaks of 
    selecting the same spaceship.
    
    Parameters:
    - lr: [0, 1] Learning rate for Q-values.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - w: [0, 1] Weighting parameter (0 = Pure MF, 1 = Pure MB).
    - ck_lr: [0, 1] Decay rate for the choice kernel (1 = keep only recent, 0 = never decay).
    - ck_beta: [0, 10] Weight of the choice kernel in Stage 1 decision.
    """
    lr, beta_1, beta_2, w, ck_lr, ck_beta = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    ck_stage1 = np.zeros(2) # Choice kernel for stage 1
    
    for trial in range(n_trials):
        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add Choice Kernel bonus (beta_1 scales Q, ck_beta scales Kernel)
        logits_1 = beta_1 * q_net_1 + ck_beta * ck_stage1
        
        exp_q1 = np.exp(logits_1 - np.max(logits_1)) # subtract max for numerical stability
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Update Choice Kernel
        # CK updates: chosen gets closer to 1, unchosen decays
        # Formulation: CK(a) = (1 - ck_lr) * CK(a) + ck_lr * I(a==chosen)
        ck_stage1 *= (1 - ck_lr)
        ck_stage1[action_1[trial]] += ck_lr
        
        # Stage 2 Policy
        s_idx = state[trial]
        logits_2 = beta_2 * q_stage2_mf[s_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # Learning
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]
        
        # Stage 1 MF update
        delta_1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_1
        
        # Stage 2 MF update
        delta_2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Asymmetric Stage 2 Learning Model
This model hypothesizes that the participant learns about the aliens (Stage 2) differently depending on whether they receive a reward or not (asymmetric learning), while maintaining a standard learning process for the spaceship choice (Stage 1). This separation allows the model to capture the specific dynamics of tracking drifting alien probabilities (which drive the Model-Based value) distinct from the structural choice at Stage 1.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Asymmetric Learning Rates for Stage 2.
    
    This model posits that learning about the aliens (Stage 2) happens 
    differently for rewards versus non-rewards, while Stage 1 (Spaceship) 
    learning is symmetric. This allows for differential sensitivity to 
    positive and negative outcomes specifically in the bandit-like second stage.
    
    Parameters:
    - lr_1: [0, 1] Learning rate for Stage 1 MF values.
    - lr_2_pos: [0, 1] Learning rate for Stage 2 when reward is received (1).
    - lr_2_neg: [0, 1] Learning rate for Stage 2 when no reward is received (0).
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - w: [0, 1] Weighting parameter (0 = Pure MF, 1 = Pure MB).
    - pers: [0, 1] Perseveration bonus for Stage 1.
    """
    lr_1, lr_2_pos, lr_2_neg, beta_1, beta_2, w, pers = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1
    
    for trial in range(n_trials):
        # Stage 1
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Perseveration
        logits_1 = beta_1 * q_net_1
        if prev_action_1 != -1:
            logits_1[prev_action_1] += pers
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        prev_action_1 = action_1[trial]
        
        # Stage 2
        s_idx = state[trial]
        logits_2 = beta_2 * q_stage2_mf[s_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # Updates
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]
        
        # Stage 1 MF (Symmetric)
        delta_1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr_1 * delta_1
        
        # Stage 2 MF (Asymmetric)
        delta_2 = r - q_stage2_mf[s_idx, a2]
        lr_2 = lr_2_pos if r > 0 else lr_2_neg
        q_stage2_mf[s_idx, a2] += lr_2 * delta_2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Stage-Specific Perseveration Model
The participant data shows distinct behavioral patterns in the two stages: long habitual streaks in choosing spaceships (Stage 1) but more active switching when choosing aliens (Stage 2). This model introduces separate perseveration parameters for each stage (`pers_1` and `pers_2`) to capture these stage-specific "stickiness" levels.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Separate Perseveration for Stage 1 and Stage 2.
    
    The participant shows distinct "stickiness" patterns for spaceships (Stage 1)
    and aliens (Stage 2). This model allows the strength of the repetition 
    bias to vary between the two stages.
    
    Parameters:
    - lr: [0, 1] Learning rate.
    - beta_1: [0, 10] Inverse temperature Stage 1.
    - beta_2: [0, 10] Inverse temperature Stage 2.
    - w: [0, 1] MB/MF weight.
    - pers_1: [0, 1] Perseveration bonus for Stage 1 (Spaceship).
    - pers_2: [0, 1] Perseveration bonus for Stage 2 (Alien).
    """
    lr, beta_1, beta_2, w, pers_1, pers_2 = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1
    prev_action_2 = np.array([-1, -1]) # Track prev action for each planet (state)
    
    for trial in range(n_trials):
        # Stage 1
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta_1 * q_net_1
        if prev_action_1 != -1:
            logits_1[prev_action_1] += pers_1
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        prev_action_1 = action_1[trial]
        
        # Stage 2
        s_idx = state[trial]
        logits_2 = beta_2 * q_stage2_mf[s_idx]
        
        # Apply pers_2 if we have visited this state before
        if prev_action_2[s_idx] != -1:
            logits_2[prev_action_2[s_idx]] += pers_2
            
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        prev_action_2[s_idx] = action_2[trial]
        
        # Updates
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]
        
        delta_1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_1
        
        delta_2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```