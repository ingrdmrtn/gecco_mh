Here are three new cognitive models based on the participant data and the feedback provided.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Model-Based Learner with Value Decay and Stickiness.
    
    This model assumes the participant uses a Model-Based strategy (planning using the 
    transition matrix) to choose spaceships, but their memory of alien values (Stage 2) 
    decays over time. This explains the mixture of goal-directed behavior and 
    forgetting/perseveration observed in the data.
    
    Parameters:
    - learning_rate: [0, 1] Update rate for Stage 2 values.
    - decay_rate: [0, 1] Rate at which unchosen Q-values decay toward 0.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - stickiness: [0, 5] Bonus added to the previous Stage 1 choice.
    """
    learning_rate, decay_rate, beta_1, beta_2, stickiness = model_parameters
    n_trials = len(action_1)
    
    # Fixed transition matrix: 
    # Row 0 (Spaceship A/0) -> [0.7 Planet X/0, 0.3 Planet Y/1]
    # Row 1 (Spaceship U/1) -> [0.3 Planet X/0, 0.7 Planet Y/1]
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Stage 2 values (Planet x Alien)
    q_stage2 = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        # -- Stage 1 Policy (Model-Based) --
        # Calculate expected value of each spaceship based on current Stage 2 estimates
        # Q_MB(s1) = Sum P(s2|s1) * max Q(s2, a)
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Add stickiness to the previously chosen spaceship
        q_net_1 = q_stage1_mb.copy()
        if last_action_1 != -1:
            q_net_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta_1 * (q_net_1 - np.max(q_net_1)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_action_1 = action_1[trial]
        state_idx = state[trial]
        
        # -- Stage 2 Policy --
        exp_q2 = np.exp(beta_2 * (q_stage2[state_idx] - np.max(q_stage2[state_idx])))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # -- Updates --
        # Update chosen alien value
        rpe = reward[trial] - q_stage2[state_idx, action_2[trial]]
        q_stage2[state_idx, action_2[trial]] += learning_rate * rpe
        
        # Decay unchosen alien on current planet
        unchosen_alien = 1 - action_2[trial]
        q_stage2[state_idx, unchosen_alien] *= (1 - decay_rate)
        
        # Decay values for the unvisited planet
        unchosen_planet = 1 - state_idx
        q_stage2[unchosen_planet, :] *= (1 - decay_rate)
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Asymmetric Model-Free Learner with Decay.
    
    This model assumes the participant learns from rewards (positive RPE) and 
    lack of rewards (negative RPE) at different rates. It also includes value decay
    and stickiness. This captures biases in processing wins vs losses (e.g., optimism)
    which might explain the persistence in specific choices despite failures.
    
    Parameters:
    - alpha_pos: [0, 1] Learning rate for positive prediction errors.
    - alpha_neg: [0, 1] Learning rate for negative prediction errors.
    - decay_rate: [0, 1] Rate at which unchosen Q-values decay.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - stickiness: [0, 5] Bonus added to the previous Stage 1 choice.
    """
    alpha_pos, alpha_neg, decay_rate, beta_1, beta_2, stickiness = model_parameters
    n_trials = len(action_1)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        # -- Stage 1 Policy --
        q_net_1 = q_stage1.copy()
        if last_action_1 != -1:
            q_net_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta_1 * (q_net_1 - np.max(q_net_1)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_action_1 = action_1[trial]
        state_idx = state[trial]
        
        # -- Stage 2 Policy --
        exp_q2 = np.exp(beta_2 * (q_stage2[state_idx] - np.max(q_stage2[state_idx])))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # -- Updates --
        # 1. Stage 1 Update (TD(0)): Driven by transition to Stage 2 value
        target_stage1 = q_stage2[state_idx, action_2[trial]]
        rpe_1 = target_stage1 - q_stage1[action_1[trial]]
        alpha_1 = alpha_pos if rpe_1 > 0 else alpha_neg
        q_stage1[action_1[trial]] += alpha_1 * rpe_1
        
        # 2. Stage 2 Update: Driven by reward
        rpe_2 = reward[trial] - q_stage2[state_idx, action_2[trial]]
        alpha_2 = alpha_pos if rpe_2 > 0 else alpha_neg
        q_stage2[state_idx, action_2[trial]] += alpha_2 * rpe_2
        
        # 3. Stage 1 Second Update (Eligibility): Driven by Stage 2 RPE
        # Propagating the final outcome surprise back to the first choice
        alpha_1_elig = alpha_pos if rpe_2 > 0 else alpha_neg
        q_stage1[action_1[trial]] += alpha_1_elig * rpe_2

        # -- Decay --
        # Decay unchosen spaceship
        unchosen_sp = 1 - action_1[trial]
        q_stage1[unchosen_sp] *= (1 - decay_rate)
        
        # Decay unchosen alien
        unchosen_alien = 1 - action_2[trial]
        q_stage2[state_idx, unchosen_alien] *= (1 - decay_rate)
        
        # Decay unvisited planet
        unchosen_planet = 1 - state_idx
        q_stage2[unchosen_planet, :] *= (1 - decay_rate)
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner (MB + MF) with Value Decay.
    
    This model combines Model-Based planning and Model-Free learning. Both systems
    rely on Stage 2 values which are subject to decay (forgetting). The weight `w`
    determines the balance between the two systems for the Stage 1 choice.
    
    Parameters:
    - learning_rate: [0, 1] Update rate for MF values.
    - decay_rate: [0, 1] Decay rate for unchosen values.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - w: [0, 1] Weight for Model-Based control (1=MB, 0=MF).
    - stickiness: [0, 5] Bonus added to the previous Stage 1 choice.
    """
    learning_rate, decay_rate, beta_1, beta_2, w, stickiness = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_mf_1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        # -- Stage 1 Policy (Hybrid) --
        # MB Component
        max_q2 = np.max(q_stage2, axis=1)
        q_mb_1 = transition_matrix @ max_q2
        
        # Net Q-value: Weighted sum of MB and MF
        q_net_1 = w * q_mb_1 + (1 - w) * q_mf_1
        
        # Add stickiness
        if last_action_1 != -1:
            q_net_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta_1 * (q_net_1 - np.max(q_net_1)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_action_1 = action_1[trial]
        state_idx = state[trial]
        
        # -- Stage 2 Policy --
        exp_q2 = np.exp(beta_2 * (q_stage2[state_idx] - np.max(q_stage2[state_idx])))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # -- Updates --
        # 1. MF Stage 1 Update (Transition)
        rpe_1 = q_stage2[state_idx, action_2[trial]] - q_mf_1[action_1[trial]]
        q_mf_1[action_1[trial]] += learning_rate * rpe_1
        
        # 2. Stage 2 Update (Reward)
        rpe_2 = reward[trial] - q_stage2[state_idx, action_2[trial]]
        q_stage2[state_idx, action_2[trial]] += learning_rate * rpe_2
        
        # 3. MF Stage 1 Second Update (Eligibility)
        q_mf_1[action_1[trial]] += learning_rate * rpe_2
        
        # -- Decay --
        # Decay unchosen MF Stage 1
        q_mf_1[1 - action_1[trial]] *= (1 - decay_rate)
        
        # Decay unchosen alien
        q_stage2[state_idx, 1 - action_2[trial]] *= (1 - decay_rate)
        
        # Decay unvisited planet
        q_stage2[1 - state_idx, :] *= (1 - decay_rate)
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```