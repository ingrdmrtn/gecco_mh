Here are three new cognitive models for the two-step task, designed to capture different potential mechanisms underlying the participant's behavior, particularly focusing on how they process rewards (asymmetric learning), maintain habits (stickiness variants), and update values across stages.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Asymmetric Learning Rates and Stickiness.
    
    This model assumes the participant learns differently from better-than-expected outcomes 
    (positive prediction errors) versus worse-than-expected outcomes (negative prediction errors),
    while also exhibiting a general tendency to repeat Stage 1 choices (stickiness).
    
    Parameters:
    - alpha_pos: Learning rate for positive prediction errors [0, 1]
    - alpha_neg: Learning rate for negative prediction errors [0, 1]
    - beta_1: Inverse temperature for Stage 1 softmax [0, 10]
    - beta_2: Inverse temperature for Stage 2 softmax [0, 10]
    - w: Mixing weight between Model-Based and Model-Free values [0, 1]
    - lam: Eligibility trace parameter [0, 1]
    - stickiness: Choice perseverance bonus for Stage 1 [0, 10]
    """
    alpha_pos, alpha_neg, beta_1, beta_2, w, lam, stickiness = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # 2 states (planets), 2 actions (aliens)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Calculate logits
        logits_1 = beta_1 * q_net
        
        # Add stickiness to the previously chosen action
        if trial > 0:
            logits_1[action_1[trial-1]] += stickiness
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1)) # subtract max for numerical stability
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        
        # --- Stage 2 Policy ---
        logits_2 = beta_2 * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        # Prediction Errors
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Determine learning rates based on sign of total Prediction Error
        effective_delta_1 = delta_stage1 + lam * delta_stage2
        lr_1 = alpha_pos if effective_delta_1 > 0 else alpha_neg
        q_stage1_mf[action_1[trial]] += lr_1 * effective_delta_1
        
        lr_2 = alpha_pos if delta_stage2 > 0 else alpha_neg
        q_stage2_mf[state_idx, action_2[trial]] += lr_2 * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Reward-Dependent Stickiness (Win-Stay, Lose-Shift).
    
    This model posits that the tendency to repeat a Stage 1 choice depends on the 
    outcome of the previous trial. It separates 'stickiness' into two parameters:
    one for when the previous trial was rewarded (stick_win) and one for when it 
    was unrewarded (stick_loss).
    
    Parameters:
    - learning_rate: Rate at which Q-values are updated [0, 1]
    - beta_1: Inverse temperature for Stage 1 softmax [0, 10]
    - beta_2: Inverse temperature for Stage 2 softmax [0, 10]
    - w: Mixing weight between Model-Based and Model-Free values [0, 1]
    - lam: Eligibility trace parameter [0, 1]
    - stick_win: Stickiness bonus after a rewarded trial [0, 10]
    - stick_loss: Stickiness bonus after an unrewarded trial [0, 10]
    """
    learning_rate, beta_1, beta_2, w, lam, stick_win, stick_loss = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta_1 * q_net
        
        # Add reward-dependent stickiness
        if trial > 0:
            prev_choice = action_1[trial-1]
            prev_reward = reward[trial-1]
            if prev_reward == 1:
                logits_1[prev_choice] += stick_win
            else:
                logits_1[prev_choice] += stick_loss
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        
        # --- Stage 2 Policy ---
        logits_2 = beta_2 * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        q_stage1_mf[action_1[trial]] += learning_rate * (delta_stage1 + lam * delta_stage2)
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Stage-Specific Learning Rates (Asymmetric Stage 2).
    
    This model uses a standard learning rate for the Model-Free Stage 1 values,
    but uses asymmetric learning rates (positive vs negative) for Stage 2.
    This allows the model to capture the restless bandit nature of the aliens (Stage 2)
    with different sensitivity to gains and losses, while maintaining a separate
    learning dynamic for the spaceship choice.
    
    Parameters:
    - lr_1: Learning rate for Stage 1 Model-Free values [0, 1]
    - alpha_pos_2: Learning rate for Stage 2 positive PE [0, 1]
    - alpha_neg_2: Learning rate for Stage 2 negative PE [0, 1]
    - beta_1: Inverse temperature for Stage 1 softmax [0, 10]
    - beta_2: Inverse temperature for Stage 2 softmax [0, 10]
    - w: Mixing weight between Model-Based and Model-Free values [0, 1]
    - lam: Eligibility trace parameter [0, 1]
    """
    lr_1, alpha_pos_2, alpha_neg_2, beta_1, beta_2, w, lam = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta_1 * q_net
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        
        # --- Stage 2 Policy ---
        logits_2 = beta_2 * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Stage 1 update (standard MF with eligibility trace)
        q_stage1_mf[action_1[trial]] += lr_1 * (delta_stage1 + lam * delta_stage2)
        
        # Stage 2 update (Asymmetric based on reward prediction error)
        lr_2 = alpha_pos_2 if delta_stage2 > 0 else alpha_neg_2
        q_stage2_mf[state_idx, action_2[trial]] += lr_2 * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```