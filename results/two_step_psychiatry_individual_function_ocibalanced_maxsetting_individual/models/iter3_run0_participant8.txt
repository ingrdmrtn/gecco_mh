Here are three new cognitive models based on the participant's behavior and the provided template.

### Model 1: Outcome-Dependent Stickiness Hybrid Model
This model extends the standard hybrid model by distinguishing between "Win-Stay" and "Lose-Stay" behavior. The participant shows long streaks of choices, even after failures (e.g., repeating spaceship choice after 0 coins). A single stickiness parameter might not capture the difference in perseveration after a reward versus an omission. This model splits stickiness into `stick_rew` (bonus after reward) and `stick_unrew` (bonus after no reward).

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Outcome-Dependent Stickiness Hybrid Model.
    
    Distinguishes perseveration based on the previous trial's outcome.
    Allows the model to capture different tendencies to repeat an action 
    after a win (Win-Stay) versus after a loss (Lose-Stay/Lose-Switch).
    
    Parameters:
    lr: [0,1] - Learning rate for value updates.
    beta: [0,10] - Inverse temperature for softmax.
    w: [0,1] - Weighting parameter (0 = Pure MF, 1 = Pure MB).
    stick_rew: [0,5] - Stickiness bonus if previous trial was rewarded.
    stick_unrew: [0,5] - Stickiness bonus if previous trial was unrewarded.
    """
    lr, beta, w, stick_rew, stick_unrew = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1
    prev_r = -1
    
    for trial in range(n_trials):
        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if prev_a1 != -1:
            if prev_r == 1:
                q_net[prev_a1] += stick_rew
            else:
                q_net[prev_a1] += stick_unrew
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Choice 1 made
        chosen_a1 = action_1[trial]
        state_idx = state[trial]

        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # Choice 2 made and Reward received
        chosen_a2 = action_2[trial]
        r = reward[trial]

        # Updates
        delta_stage1 = q_stage2_mf[state_idx, chosen_a2] - q_stage1_mf[chosen_a1]
        delta_stage2 = r - q_stage2_mf[state_idx, chosen_a2]

        q_stage1_mf[chosen_a1] += lr * delta_stage1
        q_stage2_mf[state_idx, chosen_a2] += lr * delta_stage2
        
        prev_a1 = chosen_a1
        prev_r = r

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model with Decay and Stage-Specific Noise
Building on the successful "Decay" model, this model adds separate inverse temperature parameters (`beta_1` and `beta_2`) for the two stages. The participant data shows extremely consistent spaceship choices (Stage 1) but potentially more variable alien choices (Stage 2) as probabilities drift. Decoupling the noise parameters allows the model to fit the high determinism in Stage 1 without over-fitting the noise in Stage 2.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Decay and Stage-Specific Noise.
    
    Combines memory decay (forgetting of unchosen options) with separate 
    exploration parameters for Stage 1 and Stage 2. This accounts for 
    different levels of decision confidence or noise at each step.
    
    Parameters:
    lr: [0,1] - Learning rate.
    decay: [0,1] - Decay rate for unchosen actions.
    beta_1: [0,10] - Inverse temperature for Stage 1 (Spaceship).
    beta_2: [0,10] - Inverse temperature for Stage 2 (Alien).
    w: [0,1] - Weighting parameter (0 = Pure MF, 1 = Pure MB).
    stickiness: [0,5] - Choice perseveration bonus for Stage 1.
    """
    lr, decay, beta_1, beta_2, w, stickiness = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1
    
    for trial in range(n_trials):
        # Stage 1 Policy (uses beta_1)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if prev_a1 != -1:
            q_net[prev_a1] += stickiness
            
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        chosen_a1 = action_1[trial]
        state_idx = state[trial]

        # Stage 2 Policy (uses beta_2)
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        chosen_a2 = action_2[trial]
        r = reward[trial]

        # Updates
        delta_stage1 = q_stage2_mf[state_idx, chosen_a2] - q_stage1_mf[chosen_a1]
        delta_stage2 = r - q_stage2_mf[state_idx, chosen_a2]

        q_stage1_mf[chosen_a1] += lr * delta_stage1
        q_stage2_mf[state_idx, chosen_a2] += lr * delta_stage2
        
        # Decay unchosen Stage 1
        q_stage1_mf[1 - chosen_a1] *= (1 - decay)
        
        # Decay unchosen Stage 2 (all aliens not visited or not chosen)
        for s in range(2):
            for a in range(2):
                if not (s == state_idx and a == chosen_a2):
                    q_stage2_mf[s, a] *= (1 - decay)
                    
        prev_a1 = chosen_a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model with Habit Learning
The participant exhibits very long blocks of repeating the same spaceship (e.g., T36-T100). Standard stickiness (based on the immediately preceding trial) may not fully capture this inertia. This model introduces a dynamic "Habit" variable that accumulates history over time. The strength of the habit builds up with repetitions and decays when the action is not taken, creating a "momentum" effect that is distinct from value-based learning.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Habit Learning.
    
    Incorporates a dynamic habit strength variable that integrates action history 
    over time. Unlike simple stickiness (which usually looks only at n-1), 
    this habit trace builds up strength with repetition, modeling strong 
    inertia or motor perseveration.
    
    Parameters:
    lr: [0,1] - Learning rate for Q-values.
    beta: [0,10] - Inverse temperature.
    w: [0,1] - Weighting parameter (0 = Pure MF, 1 = Pure MB).
    habit_lr: [0,1] - Learning rate for the habit trace (how fast habits form/decay).
    habit_w: [0,10] - Weight of the habit strength in the decision variable.
    """
    lr, beta, w, habit_lr, habit_w = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Habit strength for Stage 1 actions
    habit_values = np.zeros(2) 
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combine MB, MF, and Habit
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf + habit_w * habit_values
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        chosen_a1 = action_1[trial]
        state_idx = state[trial]

        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        chosen_a2 = action_2[trial]
        r = reward[trial]

        # Value Updates
        delta_stage1 = q_stage2_mf[state_idx, chosen_a2] - q_stage1_mf[chosen_a1]
        delta_stage2 = r - q_stage2_mf[state_idx, chosen_a2]

        q_stage1_mf[chosen_a1] += lr * delta_stage1
        q_stage2_mf[state_idx, chosen_a2] += lr * delta_stage2
        
        # Habit Update: Chosen approaches 1, Unchosen approaches 0
        habit_values[chosen_a1] += habit_lr * (1 - habit_values[chosen_a1])
        habit_values[1 - chosen_a1] += habit_lr * (0 - habit_values[1 - chosen_a1])

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```