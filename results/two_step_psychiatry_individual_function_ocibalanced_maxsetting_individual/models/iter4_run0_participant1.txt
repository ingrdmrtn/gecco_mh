Here are the 3 new cognitive models based on the participant data and analysis.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Reward-Dependent Stickiness.
    
    Extends the standard Hybrid (MB/MF) model by splitting the choice stickiness 
    parameter into two separate components based on the previous trial's outcome.
    This allows the model to capture "Win-Stay" and "Lose-Stay" (or Lose-Shift) 
    tendencies independently.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate for Q-value updates.
    beta: [0, 10] - Inverse temperature for softmax choice.
    w: [0, 1] - Mixing weight (0 = pure MF, 1 = pure MB).
    st_win: [0, 10] - Stickiness bonus applied to the previous action if it was rewarded.
    st_lose: [0, 10] - Stickiness bonus applied to the previous action if it was unrewarded.
    """
    learning_rate, beta, w, st_win, st_lose = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Initialize Q-values
    # Stage 1: 2 actions
    q_mf_stage1 = np.zeros(2) 
    # Stage 2: 2 states x 2 actions
    q_mf_stage2 = np.zeros((2, 2))
    
    last_action_1 = -1
    last_reward = -1
    
    log_loss = 0.0
    eps = 1e-10
    
    for trial in range(n_trials):
        a1 = action_1[trial]
        s_idx = state[trial]
        a2 = action_2[trial]
        r = reward[trial]
        
        # --- STAGE 1 CHOICE ---
        # Model-Based value (Stage 1)
        max_q_stage2 = np.max(q_mf_stage2, axis=1)
        q_mb_stage1 = transition_matrix @ max_q_stage2
        
        # Net Q-value calculation
        q_net_stage1 = w * q_mb_stage1 + (1 - w) * q_mf_stage1
        
        # Apply Reward-Dependent Stickiness
        if last_action_1 != -1:
            if last_reward == 1:
                q_net_stage1[last_action_1] += st_win
            elif last_reward == 0:
                q_net_stage1[last_action_1] += st_lose
                
        # Softmax Probability Stage 1
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + eps)
        
        # --- STAGE 2 CHOICE ---
        # Softmax Probability Stage 2
        exp_q2 = np.exp(beta * q_mf_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        if a2 != -1:
            log_loss -= np.log(probs_2[a2] + eps)
            
            # --- LEARNING ---
            # Update Stage 2 Q-values (TD Error)
            delta_2 = r - q_mf_stage2[s_idx, a2]
            q_mf_stage2[s_idx, a2] += learning_rate * delta_2
            
            # Update Stage 1 Q-values (TD(1) logic similar to best model)
            # Direct reinforcement from reward to stage 1 choice
            q_mf_stage1[a1] += learning_rate * (r - q_mf_stage1[a1])
            
            last_action_1 = a1
            last_reward = r
        else:
            # If data is missing/invalid, reset history to avoid carry-over errors
            last_action_1 = -1
            last_reward = -1

    return log_loss

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Value Decay.
    
    A Hybrid MB/MF model that incorporates a decay mechanism for unchosen actions.
    Since the reward probabilities of aliens change slowly over time, this model 
    assumes the participant 'forgets' or discounts the value of options not recently 
    visited, causing them to drift towards 0.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate for chosen Q-value updates.
    beta: [0, 10] - Inverse temperature for softmax choice.
    w: [0, 1] - Mixing weight (0 = pure MF, 1 = pure MB).
    decay: [0, 1] - Decay rate for unchosen Stage 2 actions (0 = no decay, 1 = instant forget).
    """
    learning_rate, beta, w, decay = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_mf_stage1 = np.zeros(2)
    q_mf_stage2 = np.zeros((2, 2))
    
    log_loss = 0.0
    eps = 1e-10
    
    for trial in range(n_trials):
        a1 = action_1[trial]
        s_idx = state[trial]
        a2 = action_2[trial]
        r = reward[trial]
        
        # --- STAGE 1 CHOICE ---
        max_q_stage2 = np.max(q_mf_stage2, axis=1)
        q_mb_stage1 = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_mb_stage1 + (1 - w) * q_mf_stage1
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + eps)
        
        # --- STAGE 2 CHOICE ---
        exp_q2 = np.exp(beta * q_mf_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        if a2 != -1:
            log_loss -= np.log(probs_2[a2] + eps)
            
            # --- LEARNING ---
            # Update Stage 2: Chosen action learns, Unchosen actions decay
            for s in range(2):
                for a in range(2):
                    if s == s_idx and a == a2:
                        # Chosen action update
                        q_mf_stage2[s, a] += learning_rate * (r - q_mf_stage2[s, a])
                    else:
                        # Unchosen actions decay
                        q_mf_stage2[s, a] *= (1.0 - decay)
            
            # Update Stage 1 (TD(1) direct reinforcement)
            q_mf_stage1[a1] += learning_rate * (r - q_mf_stage1[a1])

    return log_loss

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Eligibility Trace (TD-Lambda).
    
    Combines Model-Based planning with a Model-Free system that uses eligibility traces (lambda).
    The parameter `lam` controls how much the Stage 2 Reward Prediction Error (RPE) 
    updates the Stage 1 Q-value. This generalizes between TD(0) (lam=0) and TD(1) (lam=1).
    
    Parameters:
    learning_rate: [0, 1] - Learning rate for Q-value updates.
    beta: [0, 10] - Inverse temperature for softmax choice.
    w: [0, 1] - Mixing weight (0 = pure MF, 1 = pure MB).
    lam: [0, 1] - Eligibility trace decay parameter (lambda).
    """
    learning_rate, beta, w, lam = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_mf_stage1 = np.zeros(2)
    q_mf_stage2 = np.zeros((2, 2))
    
    log_loss = 0.0
    eps = 1e-10
    
    for trial in range(n_trials):
        a1 = action_1[trial]
        s_idx = state[trial]
        a2 = action_2[trial]
        r = reward[trial]
        
        # --- STAGE 1 CHOICE ---
        max_q_stage2 = np.max(q_mf_stage2, axis=1)
        q_mb_stage1 = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_mb_stage1 + (1 - w) * q_mf_stage1
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + eps)
        
        # --- STAGE 2 CHOICE ---
        exp_q2 = np.exp(beta * q_mf_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        if a2 != -1:
            log_loss -= np.log(probs_2[a2] + eps)
            
            # --- LEARNING (TD-Lambda) ---
            
            # Stage 1 Prediction Error (TD(0) part): Q2 - Q1
            delta_1 = q_mf_stage2[s_idx, a2] - q_mf_stage1[a1]
            
            # Stage 2 Prediction Error: R - Q2
            delta_2 = r - q_mf_stage2[s_idx, a2]
            
            # Update Stage 1: alpha * (delta_1 + lambda * delta_2)
            q_mf_stage1[a1] += learning_rate * (delta_1 + lam * delta_2)
            
            # Update Stage 2: alpha * delta_2
            q_mf_stage2[s_idx, a2] += learning_rate * delta_2

    return log_loss
```