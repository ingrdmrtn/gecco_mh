Here are the three proposed cognitive models.

### Model 1: Hybrid MB/MF with Loss Aversion
This model extends the standard Hybrid Model-Based/Model-Free framework by introducing a parameter for **Loss Aversion**. The participant data contains trials with negative rewards (-1 coins), particularly in trials 58, 89, 90, and 180-184. Standard reinforcement learning models treat rewards linearly. However, humans often weigh losses (negative outcomes) significantly more heavily than equivalent gains. This model scales negative rewards by a `loss_sensitivity` parameter before updating values, allowing the model to capture strong avoidance behaviors triggered by the alien "stealing" coins.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF learner with Loss Aversion.
    
    Incorporates a loss sensitivity parameter. If the reward is negative,
    it is scaled by 'loss_sensitivity' before being used for value updates.
    This allows the model to react more strongly to negative outcomes (losses)
    than positive ones.

    Parameters:
    lr:               [0, 1] Learning rate for value updates.
    beta:             [0, 10] Inverse temperature (softmax randomness).
    w:                [0, 1] Mixing weight (0=Pure MF, 1=Pure MB).
    loss_sensitivity: [0, 5] Multiplier for negative rewards (>1 implies loss aversion).
    """
    lr, beta, w, loss_sensitivity = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition probabilities for the MB component
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # Handle missing data
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        # --- Stage 1 Policy ---
        # Model-Based Value: Transition prob * max(Stage 2 Values)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Value Updating ---
        # Apply Loss Aversion to the reward
        r_eff = reward[trial]
        if r_eff < 0:
            r_eff *= loss_sensitivity
            
        # TD(0) update for Stage 1 MF
        # Using the value of the state reached (max Q) as the target
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr * delta_stage1
        
        # Update Stage 2
        delta_stage2 = r_eff - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr * delta_stage2

    eps = 1e-10
    # Filter valid trials for likelihood calculation
    valid_mask = (action_1 != -1) & (state != -1) & (action_2 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss
```

### Model 2: Model-Free with Exponential Choice Trace
The participant shows significant "stickiness" or perseveration (repeating choices). While simple perseveration adds a constant bonus to the *last* choice, an **Exponential Choice Trace** maintains a running average of past choices. This allows the model to capture the momentum of habits formed over many trials (e.g., the long streaks of choosing spaceship 1) and the gradual decay of that habit when switching. This is structurally distinct from simple perseveration as it integrates history rather than just the $t-1$ action.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free learner with Exponential Choice Trace.
    
    Instead of simple 1-step perseveration, this model maintains a 'choice trace'
    that accumulates history. This accounts for habit formation where the tendency
    to repeat an action grows with consecutive repetitions and decays gradually.
    
    Parameters:
    lr:           [0, 1] Learning rate.
    beta:         [0, 10] Inverse temperature.
    trace_decay:  [0, 1] Decay rate of the choice trace (0 = instant forget, 1 = no decay).
    trace_weight: [0, 5] Weight of the choice trace in the decision variable.
    """
    lr, beta, trace_decay, trace_weight = model_parameters
    n_trials = len(action_1)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    # Choice trace for Stage 1 actions (initialized to neutral)
    choice_trace = np.zeros(2)

    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        # --- Stage 1 Policy ---
        # Decision variable combines Q-value and the habit trace
        decision_val = q_stage1 + (trace_weight * choice_trace)
        
        exp_q1 = np.exp(beta * decision_val)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Value Updating (Standard MF) ---
        delta_stage1 = q_stage2[state_idx, action_2[trial]] - q_stage1[action_1[trial]]
        q_stage1[action_1[trial]] += lr * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2[state_idx, action_2[trial]]
        q_stage2[state_idx, action_2[trial]] += lr * delta_stage2
        
        # --- Choice Trace Updating ---
        # Decay existing trace
        choice_trace *= trace_decay
        # Add to the trace of the chosen action
        choice_trace[action_1[trial]] += (1.0 - trace_decay)

    eps = 1e-10
    valid_mask = (action_1 != -1) & (state != -1) & (action_2 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss
```

### Model 3: Hybrid MB/MF with Separate Stage Noise (Dual Beta)
The participant data suggests a potential dissociation between the consistency of high-level planning (Stage 1) and low-level bandit exploitation (Stage 2). They persist in choosing Spaceship 1 even when unrewarded (suggesting a different noise level or rigidity in Stage 1), while potentially being more or less precise in selecting aliens in Stage 2. This model splits the inverse temperature parameter `beta` into `beta_stage1` and `beta_stage2`, allowing the model to fit different levels of exploration/exploitation trade-offs for the two distinct decision types.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF learner with Separate Stage Betas.
    
    Allows for different levels of decision noise (exploration) at Stage 1
    (spaceship choice) versus Stage 2 (alien choice). This can capture
    behavior where a participant is rigid in their high-level strategy but
    exploratory in the bandit phase, or vice versa.

    Parameters:
    lr:          [0, 1] Learning rate.
    beta_stage1: [0, 10] Inverse temperature for Stage 1.
    beta_stage2: [0, 10] Inverse temperature for Stage 2.
    w:           [0, 1] Mixing weight (0=Pure MF, 1=Pure MB).
    """
    lr, beta_stage1, beta_stage2, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Use beta_stage1
        exp_q1 = np.exp(beta_stage1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        # Use beta_stage2
        exp_q2 = np.exp(beta_stage2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Value Updating ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr * delta_stage2

    eps = 1e-10
    valid_mask = (action_1 != -1) & (state != -1) & (action_2 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss
```