Here are the three cognitive models as requested.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Decaying Choice Kernel.
    
    Combines Model-Based and Model-Free learning mechanisms. Instead of a simple
    1-step stickiness, it uses a decaying Choice Kernel (CK) to capture persistent
    perseveration or habit formation over multiple trials. This allows the model
    to capture runs of choices that persist despite minor fluctuations in value.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for Q-value updates.
    - beta_1: [0, 10] Inverse temperature for Stage 1 choices.
    - beta_2: [0, 10] Inverse temperature for Stage 2 choices.
    - w: [0, 1] Weight for Model-Based control (0 = pure MF, 1 = pure MB).
    - ck_decay: [0, 1] Decay rate of the choice kernel (0 = instant decay, 1 = no decay).
    - ck_stiffness: [0, 5] Influence of the choice kernel on decision making.
    """
    learning_rate, beta_1, beta_2, w, ck_decay, ck_stiffness = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    
    # Choice Kernels (Traces)
    ck_1 = np.zeros(2)
    ck_2 = np.zeros((2, 2))
    
    for trial in range(n_trials):
        # --- Stage 1 ---
        # Model-Based Value: Expected value of next stage states
        # Max Q-value at stage 2 is used as the proxy for state value
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value Calculation
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add Choice Kernel influence
        logits_1 = beta_1 * q_net_1 + ck_stiffness * ck_1
        
        # Softmax Policy
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Update Choice Kernel 1
        ck_1 *= ck_decay
        ck_1[action_1[trial]] += 1
        
        # --- Stage 2 ---
        state_idx = state[trial]
        q_net_2 = q_stage2_mf[state_idx]
        
        # Add Choice Kernel influence
        logits_2 = beta_2 * q_net_2 + ck_stiffness * ck_2[state_idx]
        
        # Softmax Policy
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # Update Choice Kernel 2
        ck_2[state_idx] *= ck_decay
        ck_2[state_idx, action_2[trial]] += 1
        
        # --- Learning ---
        # Stage 1 MF Update (TD(0))
        delta_1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_1
        
        # Stage 2 MF Update
        delta_2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Q-Learning with Reward-Dependent Stickiness.
    
    This model posits that perseveration (stickiness) is modulated by the outcome
    of the previous choice. A 'Win' (reward) reinforces the repetition of the 
    choice (Win-Stay), while a 'Loss' (no reward) may induce less stickiness 
    or even switching (Lose-Shift), represented by separate stickiness parameters.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for Q-values.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - lambda_param: [0, 1] Eligibility trace decay parameter.
    - stick_win: [0, 5] Stickiness bonus applied if the previous relevant outcome was a reward.
    - stick_loss: [0, 5] Stickiness bonus applied if the previous relevant outcome was no reward.
    """
    learning_rate, beta_1, beta_2, lambda_param, stick_win, stick_loss = model_parameters
    n_trials = len(action_1)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    last_action_1 = -1
    last_reward_1 = -1
    
    last_action_2 = np.array([-1, -1])
    last_reward_2 = np.array([-1, -1])
    
    for trial in range(n_trials):
        # --- Stage 1 ---
        q_net_1 = q_stage1.copy()
        if last_action_1 != -1:
            # Apply stickiness based on previous trial's overall reward
            bonus = stick_win if last_reward_1 == 1 else stick_loss
            q_net_1[last_action_1] += bonus
            
        exp_q1 = np.exp(beta_1 * q_net_1 - np.max(beta_1 * q_net_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 ---
        state_idx = state[trial]
        q_net_2 = q_stage2[state_idx].copy()
        if last_action_2[state_idx] != -1:
            # Apply stickiness based on reward from last visit to this specific state
            bonus = stick_win if last_reward_2[state_idx] == 1 else stick_loss
            q_net_2[last_action_2[state_idx]] += bonus
            
        exp_q2 = np.exp(beta_2 * q_net_2 - np.max(beta_2 * q_net_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        # TD(lambda) update
        delta_1 = q_stage2[state_idx, action_2[trial]] - q_stage1[action_1[trial]]
        delta_2 = reward[trial] - q_stage2[state_idx, action_2[trial]]
        
        q_stage1[action_1[trial]] += learning_rate * (delta_1 + lambda_param * delta_2)
        q_stage2[state_idx, action_2[trial]] += learning_rate * delta_2
        
        # Update history
        last_action_1 = action_1[trial]
        last_reward_1 = reward[trial]
        
        last_action_2[state_idx] = action_2[trial]
        last_reward_2[state_idx] = reward[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Q-Learning with Counterfactual Updating.
    
    Uses standard TD(lambda) learning but adds a counterfactual update step 
    at the second stage. When an alien is chosen and a reward is observed, 
    the unchosen alien's value is updated towards the opposite outcome 
    (assuming anticorrelation or exploring alternatives), scaled by a separate learning rate.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for chosen options.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - lambda_param: [0, 1] Eligibility trace decay.
    - stickiness: [0, 5] General perseveration bonus.
    - cf_lr: [0, 1] Learning rate for the counterfactual update of the unchosen option.
    """
    learning_rate, beta_1, beta_2, lambda_param, stickiness, cf_lr = model_parameters
    n_trials = len(action_1)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    last_action_1 = -1
    last_action_2 = np.array([-1, -1])
    
    for trial in range(n_trials):
        # --- Stage 1 ---
        q_net_1 = q_stage1.copy()
        if last_action_1 != -1:
            q_net_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta_1 * q_net_1 - np.max(beta_1 * q_net_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 ---
        state_idx = state[trial]
        q_net_2 = q_stage2[state_idx].copy()
        if last_action_2[state_idx] != -1:
            q_net_2[last_action_2[state_idx]] += stickiness
            
        exp_q2 = np.exp(beta_2 * q_net_2 - np.max(beta_2 * q_net_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]
        
        delta_1 = q_stage2[state_idx, a2] - q_stage1[a1]
        delta_2 = r - q_stage2[state_idx, a2]
        
        q_stage1[a1] += learning_rate * (delta_1 + lambda_param * delta_2)
        q_stage2[state_idx, a2] += learning_rate * delta_2
        
        # Counterfactual Update
        # Update the unchosen action (1-a2) towards (1-r)
        unchosen_a2 = 1 - a2
        cf_target = 1.0 - r
        q_stage2[state_idx, unchosen_a2] += cf_lr * (cf_target - q_stage2[state_idx, unchosen_a2])
        
        last_action_1 = a1
        last_action_2[state_idx] = a2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```