Here are the three proposed cognitive models.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Model 1: Asymmetric Learning Rates with Eligibility Traces and Outcome-Dependent Perseveration.
    
    This model extends the successful TD(lambda) mechanism by distinguishing between 
    positive and negative prediction errors (asymmetric learning). This allows the 
    agent to learn from rewards and omissions with different sensitivities. It also 
    includes split perseveration (Win-Stay, Lose-Stay/Shift) to capture the 
    participant's sticky behavior.

    Parameters:
    alpha_pos: [0, 1] - Learning rate for positive prediction errors (better than expected).
    alpha_neg: [0, 1] - Learning rate for negative prediction errors (worse than expected).
    lambda_eligibility: [0, 1] - Eligibility trace decay parameter (credit assignment to Stage 1).
    beta: [0, 10] - Inverse temperature for softmax choice.
    pers_win: [-3, 3] - Bias to repeat the previous Stage 1 choice after a reward.
    pers_loss: [-3, 3] - Bias to repeat the previous Stage 1 choice after no reward.
    """
    alpha_pos, alpha_neg, lambda_eligibility, beta, pers_win, pers_loss = model_parameters
    n_trials = len(action_1)
    
    q_stage1 = np.zeros(2) + 0.5
    q_stage2 = np.zeros((2, 2)) + 0.5 # State x Action
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1
    prev_reward = -1
    
    for t in range(n_trials):
        # Stage 1 Policy
        logits_1 = beta * q_stage1
        if prev_a1 != -1:
            if prev_reward == 1:
                logits_1[prev_a1] += pers_win
            else:
                logits_1[prev_a1] += pers_loss
        
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[t] = probs_1[action_1[t]]
        
        a1 = action_1[t]
        s_idx = state[t]
        
        # Stage 2 Policy
        logits_2 = beta * q_stage2[s_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[t] = probs_2[action_2[t]]
        
        a2 = action_2[t]
        r = reward[t]
        
        # Stage 1 TD error (SARSA)
        delta1 = q_stage2[s_idx, a2] - q_stage1[a1]
        lr1 = alpha_pos if delta1 > 0 else alpha_neg
        q_stage1[a1] += lr1 * delta1
        
        # Stage 2 TD error
        delta2 = r - q_stage2[s_idx, a2]
        lr2 = alpha_pos if delta2 > 0 else alpha_neg
        q_stage2[s_idx, a2] += lr2 * delta2
        
        # Eligibility Trace Update for Stage 1
        # Applies the stage 2 error to stage 1 value, scaled by lambda
        lr_trace = alpha_pos if delta2 > 0 else alpha_neg
        q_stage1[a1] += lr_trace * lambda_eligibility * delta2
        
        prev_a1 = a1
        prev_reward = r
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model 2: Dual Learning Rates with Outcome-Dependent Perseveration.
    
    This model posits that the participant learns the value of Spaceships (Stage 1) 
    and Aliens (Stage 2) at different rates. This hierarchical separation allows 
    fast adaptation to changing alien probabilities while potentially maintaining 
    stable spaceship preferences, or vice versa. It uses TD(0) updates and outcome-dependent 
    perseveration.
    
    Parameters:
    alpha_stage1: [0, 1] - Learning rate for Stage 1 (Spaceships).
    alpha_stage2: [0, 1] - Learning rate for Stage 2 (Aliens).
    beta: [0, 10] - Inverse temperature.
    pers_win: [-3, 3] - Bias to repeat Stage 1 choice after a Reward.
    pers_loss: [-3, 3] - Bias to repeat Stage 1 choice after No Reward.
    """
    alpha_stage1, alpha_stage2, beta, pers_win, pers_loss = model_parameters
    n_trials = len(action_1)
    
    q_stage1 = np.zeros(2) + 0.5
    q_stage2 = np.zeros((2, 2)) + 0.5
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1
    prev_reward = -1
    
    for t in range(n_trials):
        # Stage 1 Choice
        logits_1 = beta * q_stage1
        if prev_a1 != -1:
            if prev_reward == 1:
                logits_1[prev_a1] += pers_win
            else:
                logits_1[prev_a1] += pers_loss
        
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[t] = probs_1[action_1[t]]
        
        a1 = action_1[t]
        s_idx = state[t]
        
        # Stage 2 Choice
        logits_2 = beta * q_stage2[s_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[t] = probs_2[action_2[t]]
        
        a2 = action_2[t]
        r = reward[t]
        
        # Update Stage 1 (TD0)
        delta1 = q_stage2[s_idx, a2] - q_stage1[a1]
        q_stage1[a1] += alpha_stage1 * delta1
        
        # Update Stage 2
        delta2 = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += alpha_stage2 * delta2
        
        prev_a1 = a1
        prev_reward = r

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model 3: Decay Q-Learning with Outcome-Dependent Perseveration.
    
    Since the alien reward probabilities change slowly over trials, this model 
    incorporates a decay mechanism. Unchosen options in both stages drift towards 
    a neutral value (0.5), allowing the agent to "forget" old information and 
    adapt to non-stationarity. It is combined with outcome-dependent perseveration.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate for chosen actions.
    decay_rate: [0, 1] - Rate at which unchosen action values decay to 0.5.
    beta: [0, 10] - Inverse temperature.
    pers_win: [-3, 3] - Bias to repeat Stage 1 choice after a Reward.
    pers_loss: [-3, 3] - Bias to repeat Stage 1 choice after No Reward.
    """
    learning_rate, decay_rate, beta, pers_win, pers_loss = model_parameters
    n_trials = len(action_1)
    
    q_stage1 = np.zeros(2) + 0.5
    q_stage2 = np.zeros((2, 2)) + 0.5
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1
    prev_reward = -1
    
    for t in range(n_trials):
        # Stage 1 Choice
        logits_1 = beta * q_stage1
        if prev_a1 != -1:
            if prev_reward == 1:
                logits_1[prev_a1] += pers_win
            else:
                logits_1[prev_a1] += pers_loss
        
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[t] = probs_1[action_1[t]]
        
        a1 = action_1[t]
        s_idx = state[t]
        
        # Stage 2 Choice
        logits_2 = beta * q_stage2[s_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[t] = probs_2[action_2[t]]
        
        a2 = action_2[t]
        r = reward[t]
        
        # Decay Unchosen Stage 1
        unchosen_a1 = 1 - a1
        q_stage1[unchosen_a1] += decay_rate * (0.5 - q_stage1[unchosen_a1])
        
        # Decay Unchosen Stage 2 (in current state)
        unchosen_a2 = 1 - a2
        q_stage2[s_idx, unchosen_a2] += decay_rate * (0.5 - q_stage2[s_idx, unchosen_a2])
        
        # Update Chosen Stage 1
        delta1 = q_stage2[s_idx, a2] - q_stage1[a1]
        q_stage1[a1] += learning_rate * delta1
        
        # Update Chosen Stage 2
        delta2 = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += learning_rate * delta2
        
        prev_a1 = a1
        prev_reward = r

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```