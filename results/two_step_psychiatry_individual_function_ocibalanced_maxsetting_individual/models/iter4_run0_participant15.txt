Here are three new cognitive models for the two-step task, designed to explore different mechanisms of learning and perseverance observed in the participant data.

### Cognitive Model 1: Hybrid MB/MF with Eligibility Traces
This model represents the full canonical "Daw et al. (2011)" hybrid model. It combines Model-Based (planning) and Model-Free (trial-and-error) value estimation. Crucially, it includes an **eligibility trace** (`lambda_coeff`), which allows the outcome at Stage 2 (reward) to directly reinforce the Stage 1 choice, bridging the temporal gap. This tests if the participant uses a mixture of planning and efficient model-free credit assignment.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model (MB + MF) with Eligibility Traces.
    
    Combines Model-Based (transition-dependent) and Model-Free (TD) learning.
    Includes an eligibility trace (lambda) to allow Stage 2 reward prediction errors
    to directly update Stage 1 values, bridging the gap between choice and outcome.
    
    Parameters:
    - alpha: [0, 1] Learning rate for both stages.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - w: [0, 1] Weighting parameter (0 = Pure MF, 1 = Pure MB).
    - lambda_coeff: [0, 1] Eligibility trace decay (0 = TD(0), 1 = Monte Carlo).
    - stickiness: [0, 10] Choice perseverance bonus for Stage 1.
    """
    alpha, beta_1, beta_2, w, lambda_coeff, stickiness = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values initialized to 0.5 (chance) for symmetry
    q_stage1_mf = np.zeros(2) + 0.5
    q_stage2_mf = np.zeros((2, 2)) + 0.5
    
    last_action_1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1:
            last_action_1 = -1
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # --- Stage 1 Policy ---
        # Model-Based Value: Transition * Max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value: Mixture of MB and MF
        q_hybrid = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add Stickiness
        logits_1 = beta_1 * q_hybrid
        if last_action_1 != -1:
            logits_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Policy ---
        state_idx = state[trial]
        if action_2[trial] == -1:
            last_action_1 = action_1[trial]
            p_choice_2[trial] = 1.0
            continue
            
        logits_2 = beta_2 * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        # Stage 1 TD Error (TD(0) part for MF): Update MF Q1 towards Q2
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += alpha * delta_stage1
        
        # Stage 2 TD Error: Update Q2 towards Reward
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += alpha * delta_stage2
        
        # Eligibility Trace Update for Stage 1: Propagate Stage 2 RPE back to Stage 1 choice
        q_stage1_mf[action_1[trial]] += alpha * lambda_coeff * delta_stage2
        
        last_action_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2: Model-Based with Asymmetric Learning and Forgetting
This model builds on the Model-Based approach but introduces **Asymmetric Learning Rates** (`alpha_pos`, `alpha_neg`) to test if the participant updates values differently after wins versus losses (e.g., confirmation bias). It also includes **Forgetting**, allowing values to decay towards chance over time, which helps in environments with drifting reward probabilities.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Based RL with Asymmetric Learning and Forgetting.
    
    Combines Model-Based planning with two mechanisms for handling value changes:
    1. Asymmetric learning rates (alpha_pos, alpha_neg) to capture confirmation bias 
       or differential sensitivity to wins/losses.
    2. Passive forgetting (decay to chance) to handle the drifting environment.
    
    Parameters:
    - alpha_pos: [0, 1] Learning rate for positive RPEs.
    - alpha_neg: [0, 1] Learning rate for negative RPEs.
    - forget: [0, 1] Decay rate of Stage 2 values towards 0.5.
    - beta_mb: [0, 10] Inverse temperature for Stage 1 (MB).
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - stickiness: [0, 10] Choice perseverance bonus.
    """
    alpha_pos, alpha_neg, forget, beta_mb, beta_2, stickiness = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Initialize to 0.5 (chance)
    q_stage2_mf = np.zeros((2, 2)) + 0.5
    
    last_action_1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1:
            last_action_1 = -1
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # Forgetting / Decay step
        q_stage2_mf = (1 - forget) * q_stage2_mf + forget * 0.5

        # --- Stage 1 Policy (Pure MB) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        logits_1 = beta_mb * q_stage1_mb
        if last_action_1 != -1:
            logits_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        # --- Stage 2 Policy ---
        state_idx = state[trial]
        if action_2[trial] == -1:
            last_action_1 = action_1[trial]
            p_choice_2[trial] = 1.0
            continue

        logits_2 = beta_2 * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Update ---
        delta = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        if delta >= 0:
            q_stage2_mf[state_idx, action_2[trial]] += alpha_pos * delta
        else:
            q_stage2_mf[state_idx, action_2[trial]] += alpha_neg * delta

        last_action_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3: Model-Based with Dual Habit Mechanisms
This model distinguishes between two types of perseverance: **Choice Kernel** (a slowly decaying tendency to repeat frequently chosen actions, essentially a "long-term habit") and **Win-Stay Stickiness** (an immediate, one-trial bonus for repeating a choice that was just rewarded). This separates general motor perseverance from outcome-contingent strategy.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Based RL with Dual Habit Mechanisms.
    
    Incorporates two distinct types of choice persistence:
    1. Choice Kernel: A slowly decaying trace of past choices (long-term habit).
    2. Win-Stay Stickiness: An immediate bonus for repeating the last choice if rewarded (short-term strategy).
    
    Parameters:
    - alpha: [0, 1] Learning rate.
    - beta_mb: [0, 10] Inverse temperature for Stage 1 (MB).
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - decay_k: [0, 1] Decay rate for the Choice Kernel.
    - beta_habit: [0, 10] Weight of the Choice Kernel.
    - stick_win: [0, 10] Bonus for repeating the previous choice if it led to a reward.
    """
    alpha, beta_mb, beta_2, decay_k, beta_habit, stick_win = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage2_mf = np.zeros((2, 2)) + 0.5
    choice_kernel = np.zeros(2)
    
    last_action_1 = -1
    last_reward = 0

    for trial in range(n_trials):
        if action_1[trial] == -1:
            last_action_1 = -1
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combine MB value, Long-term Habit (Kernel), and Short-term Strategy (Win-Stay)
        logits_1 = beta_mb * q_stage1_mb + beta_habit * choice_kernel
        
        if last_action_1 != -1 and last_reward == 1:
            logits_1[last_action_1] += stick_win
            
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        # --- Stage 2 Policy ---
        state_idx = state[trial]
        if action_2[trial] == -1:
            # Update kernel even if stage 2 fails, as stage 1 choice was made
            mask = np.zeros(2)
            mask[action_1[trial]] = 1
            choice_kernel = (1 - decay_k) * choice_kernel + decay_k * mask
            
            last_action_1 = action_1[trial]
            last_reward = 0 
            p_choice_2[trial] = 1.0
            continue

        logits_2 = beta_2 * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Update Values ---
        delta = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += alpha * delta
        
        # --- Update Kernel ---
        mask = np.zeros(2)
        mask[action_1[trial]] = 1
        choice_kernel = (1 - decay_k) * choice_kernel + decay_k * mask
        
        last_action_1 = action_1[trial]
        last_reward = reward[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```