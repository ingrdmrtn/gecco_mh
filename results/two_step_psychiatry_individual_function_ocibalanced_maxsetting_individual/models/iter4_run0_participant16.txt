Here are the 3 cognitive models based on the participant data and the provided constraints.

### Model 1: Hybrid Model-Based/Model-Free with Asymmetric Learning
**Rationale**: The participant shows complex behavior where they sometimes persist after losses (suggesting low learning rate for negative outcomes) and stick to strategies. This model combines Model-Based (planning) and Model-Free (habitual) systems, but adds **asymmetric learning rates** (`alpha_pos`, `alpha_neg`) to the Model-Free component. This allows the agent to update values differently for wins versus losses, while the mixing weight `w` balances the two systems.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free system with Asymmetric Learning Rates.
    
    The Model-Free component updates values differently for positive vs negative 
    prediction errors. The Model-Based component calculates values using the 
    known transition matrix.
    
    Parameters:
    - alpha_pos: [0, 1] Learning rate for positive prediction errors.
    - alpha_neg: [0, 1] Learning rate for negative prediction errors.
    - beta: [0, 10] Inverse temperature (exploration/exploitation).
    - w: [0, 1] Weighting parameter (0 = Pure MF, 1 = Pure MB).
    """
    alpha_pos, alpha_neg, beta, w = model_parameters
    n_trials = len(action_1)
    
    # Fixed transition matrix: Row=Action, Col=State (Planet)
    # Action 0 -> 70% Planet 0, 30% Planet 1
    # Action 1 -> 30% Planet 0, 70% Planet 1
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Model-Free Q-values
    q_stage1_mf = np.zeros(2) # Q(Spaceship)
    q_stage2_mf = np.zeros((2, 2)) # Q(Planet, Alien)
    
    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # Model-Based Values: Expected value of next stage's max Q-values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax Choice 1
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Decision ---
        state_idx = state[trial]
        
        # Softmax Choice 2 (Pure MF at this stage)
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning ---
        # Stage 2 Update (Alien choice)
        # PE = Reward - Expected
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Apply asymmetric learning rates
        lr_s2 = alpha_pos if delta_stage2 >= 0 else alpha_neg
        q_stage2_mf[state_idx, action_2[trial]] += lr_s2 * delta_stage2
        
        # Stage 1 Update (Spaceship choice) - TD(1) style update using Reward
        # Note: In hybrid models, the MF component often learns directly from reward
        delta_stage1 = reward[trial] - q_stage1_mf[action_1[trial]]
        
        lr_s1 = alpha_pos if delta_stage1 >= 0 else alpha_neg
        q_stage1_mf[action_1[trial]] += lr_s1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Model-Free with Stage-Specific Betas and Stickiness
**Rationale**: The participant data shows strong "streaks" where they choose the same spaceship repeatedly (Stage 1 stickiness). Additionally, the previous best model suggested that separate inverse temperatures (`beta_1`, `beta_2`) for the two stages improve fit. This model is a pure Model-Free learner that incorporates **choice perseveration (stickiness)** at Stage 1 and allows different noise levels for the spaceship vs. alien decisions.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free RL with Stage-Specific Temperatures and Stage 1 Stickiness.
    
    Accounts for the different levels of stochasticity in spaceship vs alien 
    choices, and the tendency to repeat spaceship choices regardless of reward.
    
    Parameters:
    - learning_rate: [0, 1] Standard Q-learning rate.
    - beta_1: [0, 10] Inverse temperature for Stage 1 (Spaceship).
    - beta_2: [0, 10] Inverse temperature for Stage 2 (Alien).
    - stickiness: [0, 5] Bonus added to the Q-value of the previously chosen spaceship.
    """
    learning_rate, beta_1, beta_2, stickiness = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    # Track previous choice for stickiness (initialize to -1 or 0)
    last_action_1 = -1 

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # Add stickiness bonus to the Q-values used for selection (not learning)
        q_stage1_augmented = q_stage1.copy()
        if last_action_1 != -1:
            q_stage1_augmented[last_action_1] += stickiness

        exp_q1 = np.exp(beta_1 * q_stage1_augmented)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Update trace
        last_action_1 = action_1[trial]
        state_idx = state[trial]

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta_2 * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning ---
        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2[state_idx, action_2[trial]]
        q_stage2[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Stage 1 Update (TD(1) logic: Spaceship value updated by final reward)
        # This assumes implicit lambda=1 for robust Model-Free learning
        delta_stage1 = reward[trial] - q_stage1[action_1[trial]]
        q_stage1[action_1[trial]] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Model-Free with Asymmetric Learning and Stage-Specific Betas
**Rationale**: This model combines the insight from the previous best model (separate betas for separate stages) with the hypothesis that the participant learns differently from wins vs losses (asymmetric learning). This setup (4 parameters) provides a flexible Model-Free baseline that can capture both the varying exploration rates across stages and the biased updating of values.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free RL with Asymmetric Learning Rates and Separate Betas.
    
    Splits the learning process into positive/negative reinforcement sensitivity,
    and splits the decision process into Stage 1/Stage 2 noise levels.
    
    Parameters:
    - alpha_pos: [0, 1] Learning rate for rewards (wins).
    - alpha_neg: [0, 1] Learning rate for non-rewards (losses).
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    """
    alpha_pos, alpha_neg, beta_1, beta_2 = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        exp_q1 = np.exp(beta_1 * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta_2 * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning ---
        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2[state_idx, action_2[trial]]
        
        # Select learning rate based on sign of prediction error
        lr_s2 = alpha_pos if delta_stage2 >= 0 else alpha_neg
        q_stage2[state_idx, action_2[trial]] += lr_s2 * delta_stage2
        
        # Stage 1 Update (TD(1) update from Reward)
        delta_stage1 = reward[trial] - q_stage1[action_1[trial]]
        
        lr_s1 = alpha_pos if delta_stage1 >= 0 else alpha_neg
        q_stage1[action_1[trial]] += lr_s1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```