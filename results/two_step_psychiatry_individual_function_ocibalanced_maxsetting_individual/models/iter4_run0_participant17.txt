Here are three new cognitive models based on the participant data and the provided template.

### Model 1: Hybrid Learner with Asymmetric Learning Rates
This model extends the previous best model (Hybrid + Perseverance) by acknowledging that the participant may learn differently from positive outcomes (finding gold) versus negative outcomes (no gold). This is motivated by the participant's tendency to stick with a choice after a reward but switch variably after a loss.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid learner (MB/MF) with Perseverance and Asymmetric Learning Rates.
    Distinguishes between learning from positive prediction errors (wins) 
    and negative prediction errors (losses).

    Parameters:
    - lr_pos: [0, 1] Learning rate for positive prediction errors.
    - lr_neg: [0, 1] Learning rate for negative prediction errors.
    - beta: [0, 10] Inverse temperature for softmax choice.
    - w: [0, 1] Weighting parameter (0 = Pure MF, 1 = Pure MB).
    - perseverance: [0, 5] Bonus added to the previously chosen spaceship.
    """
    lr_pos, lr_neg, beta, w, perseverance = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_action_1 != -1:
            q_net[last_action_1] += perseverance
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        last_action_1 = action_1[trial]
  
        # Update Stage 1 MF
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        eff_lr_s1 = lr_pos if delta_stage1 > 0 else lr_neg
        q_stage1_mf[action_1[trial]] += eff_lr_s1 * delta_stage1
        
        # Update Stage 2 (and Stage 1 via eligibility)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        eff_lr_s2 = lr_pos if delta_stage2 > 0 else lr_neg
        q_stage2_mf[state_idx, action_2[trial]] += eff_lr_s2 * delta_stage2
        
        # Direct reinforcement of Stage 1 choice based on Stage 2 outcome (TD(1)-like component)
        q_stage1_mf[action_1[trial]] += eff_lr_s2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Learner with Separate Stage Betas
This model hypothesizes that the participant exhibits different levels of exploration/exploitation noise at the two different stages. For instance, they might be very precise in choosing aliens (Stage 2) but more exploratory or noisy in choosing spaceships (Stage 1). This combines the "Separate Betas" concept with the successful "Hybrid + Perseverance" baseline.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid learner with Perseverance and separate Inverse Temperatures (Betas)
    for Stage 1 and Stage 2.
    
    Parameters:
    - learning_rate: [0, 1] Update rate for value estimation.
    - beta_1: [0, 10] Inverse temperature for Stage 1 (Spaceship) choice.
    - beta_2: [0, 10] Inverse temperature for Stage 2 (Alien) choice.
    - w: [0, 1] Weighting parameter (0 = Pure MF, 1 = Pure MB).
    - perseverance: [0, 5] Bonus added to the previously chosen spaceship.
    """
    learning_rate, beta_1, beta_2, w, perseverance = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_action_1 != -1:
            q_net[last_action_1] += perseverance
            
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        
        last_action_1 = action_1[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Update Stage 1 MF
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Update Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Update Stage 1 MF with Stage 2 RPE
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Learner with Eligibility Trace (Lambda)
This model introduces an eligibility trace parameter (`lam`) into the Hybrid + Perseverance framework. The parameter `lam` modulates how strongly the Stage 1 Model-Free values are updated by the final reward (Stage 2 outcome) versus the Stage 2 value estimate. This allows the model to interpolate between TD(0) and TD(1) learning for the model-free component.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid learner with Perseverance and Eligibility Trace (Lambda).
    Controls the mix of TD(0) and TD(1) updates for the MF component.

    Parameters:
    - learning_rate: [0, 1] Update rate for value estimation.
    - beta: [0, 10] Inverse temperature for softmax choice.
    - w: [0, 1] Weighting parameter (0 = Pure MF, 1 = Pure MB).
    - lam: [0, 1] Eligibility trace parameter (0 = TD(0), 1 = TD(1)).
    - perseverance: [0, 5] Bonus added to the previously chosen spaceship.
    """
    learning_rate, beta, w, lam, perseverance = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_action_1 != -1:
            q_net[last_action_1] += perseverance
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        last_action_1 = action_1[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Update Stage 1 MF (TD(0) part)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Update Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Update Stage 1 MF with Eligibility Trace (TD(1) part scaled by lambda)
        q_stage1_mf[action_1[trial]] += learning_rate * lam * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```