Here are three new cognitive models based on the participant data and the feedback provided.

### Model 1: Hybrid MB/MF with Eligibility Trace and Perseveration
This model introduces an **eligibility trace parameter (`lam`)**. In the previous best model, the Stage 1 Model-Free value was updated by the Stage 2 reward prediction error implicitly (effectively $\lambda=1$). This model allows the strength of this "direct reinforcement" (how much the final reward reinforces the starting spaceship choice) to vary. It retains the perseveration ("stickiness") parameter which was found effective, but replaces the decay mechanism with the eligibility trace to see if the learning dynamic is better explained by reinforcement linking rather than forgetting.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF model with Eligibility Trace and Stage 1 Perseveration.
    
    Parameters:
    lr (float): Learning rate [0,1].
    beta (float): Inverse temperature [0,10].
    w (float): MB/MF Weighting [0,1].
    lam (float): Eligibility trace parameter [0,1]. Controls how much Stage 2 
                 prediction error updates Stage 1 values.
    pers (float): Perseveration bonus for repeating the previous stage 1 choice [0,5].
    """
    lr, beta, w, lam, pers = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    
    prev_a1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue

        # --- Stage 1 Policy ---
        # Model-Based Value calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Value
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf

        # Calculate Logits
        logits_1 = beta * q_net_stage1
        
        # Apply Perseveration
        if prev_a1 != -1:
            logits_1[prev_a1] += pers

        # Softmax Stage 1
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial] # 0 or 1

        # --- Stage 2 Policy ---
        logits_2 = beta * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        # Stage 1 Prediction Error (TD0)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr * delta_stage1

        # Stage 2 Prediction Error
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr * delta_stage2

        # Eligibility Trace Update for Stage 1
        # Propagate the Stage 2 error back to Stage 1 choice scaled by lambda
        q_stage1_mf[action_1[trial]] += lr * lam * delta_stage2
        
        prev_a1 = action_1[trial]

    eps = 1e-10
    valid_mask = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss
```

### Model 2: Asymmetric Learning Rates with Decay
The participant data shows streaks of behavior. This model hypothesizes that the participant updates their beliefs differently when they receive a reward versus when they do not (Asymmetric Learning). It uses `lr_pos` for trials with rewards and `lr_neg` for trials without. It combines this with the **decay** parameter found in the previous best model, but removes the explicit perseveration parameter, testing if asymmetric updating combined with memory decay is sufficient to explain the choice inertia.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF model with Asymmetric Learning Rates (Positive/Negative) and Value Decay.
    
    Parameters:
    lr_pos (float): Learning rate for rewarded trials [0,1].
    lr_neg (float): Learning rate for unrewarded trials [0,1].
    beta (float): Inverse temperature [0,10].
    w (float): MB/MF Weighting [0,1].
    decay (float): Decay rate for Q-values per trial [0,1].
    """
    lr_pos, lr_neg, beta, w, decay = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue

        # Apply Decay to all values toward 0 (forgetting)
        q_stage1_mf *= (1 - decay)
        q_stage2_mf *= (1 - decay)

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf

        logits_1 = beta * q_net_stage1
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        logits_2 = beta * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        # Determine effective learning rate
        curr_lr = lr_pos if reward[trial] > 0 else lr_neg

        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += curr_lr * delta_stage1

        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += curr_lr * delta_stage2

        # Direct reinforcement of Stage 1 choice by Stage 2 reward (implicit lambda=1)
        q_stage1_mf[action_1[trial]] += curr_lr * delta_stage2

    eps = 1e-10
    valid_mask = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss
```

### Model 3: Dual Perseveration (Stage 1 and Stage 2)
The previous best model used perseveration only at Stage 1 (Spaceship choice). However, participants often exhibit "motor stickiness" or habit at the second stage as well (e.g., if they picked Alien 1 on Planet A last time, they pick it again regardless of value). This model implements **Perseveration at both Stage 1 and Stage 2**, allowing for different strengths of habit at the abstract spaceship level versus the concrete alien level.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF model with Dual Perseveration (Stage 1 and Stage 2).
    
    Parameters:
    lr (float): Learning rate [0,1].
    beta (float): Inverse temperature [0,10].
    w (float): MB/MF Weighting [0,1].
    pers_1 (float): Perseveration bonus for Stage 1 (Spaceship) [0,5].
    pers_2 (float): Perseveration bonus for Stage 2 (Alien) [0,5].
    """
    lr, beta, w, pers_1, pers_2 = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_a1 = -1
    # Track previous action for each state (planet) separately for Stage 2 perseveration
    prev_a2_by_state = {-1: -1, 0: -1, 1: -1} 

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf

        logits_1 = beta * q_net_stage1
        # Stage 1 Perseveration
        if prev_a1 != -1:
            logits_1[prev_a1] += pers_1

        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        logits_2 = beta * q_stage2_mf[state_idx]
        # Stage 2 Perseveration (State-dependent)
        if prev_a2_by_state[state_idx] != -1:
            logits_2[prev_a2_by_state[state_idx]] += pers_2

        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr * delta_stage1

        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr * delta_stage2

        # Implicit lambda=1 update
        q_stage1_mf[action_1[trial]] += lr * delta_stage2
        
        # Update history
        prev_a1 = action_1[trial]
        prev_a2_by_state[state_idx] = action_2[trial]

    eps = 1e-10
    valid_mask = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss
```