Here are three new cognitive models for the two-step decision task, designed to capture the participant's behavior using Hybrid (Model-Based + Model-Free) architectures with specific mechanisms like asymmetric learning, eligibility traces, and separate stage learning rates.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Model 1: Hybrid Model with Asymmetric Learning Rates and Stickiness.
    
    Combines Model-Based (MB) and Model-Free (MF) value estimation.
    The Model-Free component uses asymmetric learning rates, updating values 
    differently for positive vs. negative prediction errors. This tests if the 
    participant's "Hybrid" behavior is biased by the valence of outcomes.
    
    Parameters:
    - alpha_pos: [0, 1] Learning rate for positive prediction errors.
    - alpha_neg: [0, 1] Learning rate for negative prediction errors.
    - beta: [0, 10] Inverse temperature (softmax sensitivity).
    - w: [0, 1] Weighting parameter (0 = Pure MF, 1 = Pure MB).
    - stickiness: [0, 10] Choice perseveration bonus for the first stage.
    """
    alpha_pos, alpha_neg, beta, w, stickiness = model_parameters
    n_trials = len(action_1)
    
    # Fixed transition matrix for MB calculation
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1
    
    for t in range(n_trials):
        
        # --- Stage 1 Policy ---
        # Model-Based Value: Expected value of next stage states given transitions
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Value: Weighted sum of MB and MF values
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net_stage1
        if prev_a1 != -1:
            logits_1[prev_a1] += stickiness
            
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[t] = probs_1[action_1[t]]
        
        # --- Stage 2 Policy ---
        s_idx = state[t]
        logits_2 = beta * q_stage2_mf[s_idx]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[t] = probs_2[action_2[t]]
        
        # --- Updates ---
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]
        
        # Stage 2 Update (Asymmetric)
        pe2 = r - q_stage2_mf[s_idx, a2]
        lr2 = alpha_pos if pe2 >= 0 else alpha_neg
        q_stage2_mf[s_idx, a2] += lr2 * pe2
        
        # Stage 1 MF Update (Asymmetric)
        # Driven by the difference between Stage 2 value (after update) and Stage 1 value
        # Note: Using updated Q2 makes this closer to TD(1) or Monte Carlo for this step
        pe1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        lr1 = alpha_pos if pe1 >= 0 else alpha_neg
        q_stage1_mf[a1] += lr1 * pe1
        
        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model 2: Hybrid Model with Eligibility Traces (Lambda) and Stickiness.
    
    A standard Hybrid model that includes an eligibility trace (lambda).
    This allows the reward at the second stage to directly influence the 
    Model-Free value of the first stage choice, bridging the gap between 
    action and outcome.
    
    Parameters:
    - alpha: [0, 1] Learning rate for value updates.
    - beta: [0, 10] Inverse temperature.
    - lambda_param: [0, 1] Eligibility trace decay parameter.
    - w: [0, 1] Weighting parameter (0 = Pure MF, 1 = Pure MB).
    - stickiness: [0, 10] Choice perseveration bonus.
    """
    alpha, beta, lambda_param, w, stickiness = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1
    
    for t in range(n_trials):
        
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net_stage1
        if prev_a1 != -1:
            logits_1[prev_a1] += stickiness
            
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[t] = probs_1[action_1[t]]
        
        # --- Stage 2 Policy ---
        s_idx = state[t]
        logits_2 = beta * q_stage2_mf[s_idx]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[t] = probs_2[action_2[t]]
        
        # --- Updates ---
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]
        
        # TD prediction error at stage 1
        pe1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * pe1
        
        # TD prediction error at stage 2
        pe2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += alpha * pe2
        
        # Eligibility trace update: Stage 2 reward affects Stage 1 value
        q_stage1_mf[a1] += alpha * lambda_param * pe2
        
        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model 3: Hybrid Model with Separate Stage Learning Rates and Stickiness.
    
    Combines MB and MF strategies but assumes different learning rates for 
    the first stage (spaceships) and the second stage (aliens). This accounts 
    for potential differences in learning speed between the abstract navigation 
    choice and the direct reward-based choice.
    
    Parameters:
    - alpha1: [0, 1] Learning rate for Stage 1 (Spaceships) Model-Free values.
    - alpha2: [0, 1] Learning rate for Stage 2 (Aliens) values.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weighting parameter (0 = Pure MF, 1 = Pure MB).
    - stickiness: [0, 10] Choice perseveration bonus.
    """
    alpha1, alpha2, beta, w, stickiness = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1
    
    for t in range(n_trials):
        
        # --- Stage 1 Policy ---
        # MB uses Stage 2 values (learned with alpha2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net_stage1
        if prev_a1 != -1:
            logits_1[prev_a1] += stickiness
            
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[t] = probs_1[action_1[t]]
        
        # --- Stage 2 Policy ---
        s_idx = state[t]
        logits_2 = beta * q_stage2_mf[s_idx]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[t] = probs_2[action_2[t]]
        
        # --- Updates ---
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]
        
        # Stage 2 Update (using alpha2)
        pe2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += alpha2 * pe2
        
        # Stage 1 MF Update (using alpha1)
        # Note: We use the *updated* Q2 value to drive Q1 update
        pe1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha1 * pe1
        
        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```