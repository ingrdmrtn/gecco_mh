Here are three new cognitive models for the two-step decision task.

### Model 1: Q-Learning with Eligibility Trace and Decay
This model extends the "Q-Learning with Decay" approach by adding an eligibility trace parameter (`lambda`). This allows the model to update the first-stage values based on a mixture of the second-stage value (TD learning) and the final reward (Monte Carlo learning). This bridges the gap between model-free strategies that rely on chaining values and those that rely on direct reward association, while retaining the "forgetting" mechanism that was successful in previous models.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Q-Learning with Eligibility Trace (Lambda) and Decay.
    
    Combines temporal credit assignment (lambda) with forgetting (decay).
    The eligibility trace allows the first-stage choice to be updated partially 
    by the second-stage TD error and partially by the final reward outcome directly.
    Unchosen options in both stages decay toward zero, simulating forgetting.
    
    Parameters:
    learning_rate: [0, 1] The rate at which Q-values are updated.
    decay_rate: [0, 1] The rate at which unchosen Q-values decay to 0 (Q = Q * (1-decay)).
    beta: [0, 10] Inverse temperature parameter for softmax choice.
    lambda_coeff: [0, 1] Eligibility trace weight. 0 = pure TD (chaining), 1 = pure Monte Carlo (direct reward).
    """
    learning_rate, decay_rate, beta, lambda_coeff = model_parameters
    n_trials = len(action_1)
    
    q_stage1 = np.zeros(2) # Q-values for spaceships
    q_stage2 = np.zeros((2, 2)) # Q-values for aliens (2 planets x 2 aliens)
    
    log_likelihood = 0.0
    eps = 1e-10
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = float(reward[trial])
        
        # Stage 1 Choice
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_likelihood += np.log(probs_1[a1] + eps)
        
        if a2 == -1:
            # Missing data trial, skip updates
            continue
            
        # Stage 2 Choice
        exp_q2 = np.exp(beta * q_stage2[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_likelihood += np.log(probs_2[a2] + eps)
        
        # Learning
        q2_val_old = q_stage2[s2, a2]
        delta2 = r - q2_val_old
        
        # Update Stage 2
        q_stage2[s2, a2] += learning_rate * delta2
        
        # Update Stage 1
        # Update combines TD error (from Q2) and eligibility trace of Reward error
        # Formula: Q1 += lr * ( (Q2_old - Q1) + lambda * (R - Q2_old) )
        q_stage1[a1] += learning_rate * ((q2_val_old - q_stage1[a1]) + lambda_coeff * delta2)
        
        # Decay unchosen options
        q_stage2[s2, 1-a2] *= (1.0 - decay_rate)
        q_stage2[1-s2, :] *= (1.0 - decay_rate) # Decay aliens on the other planet
        q_stage1[1-a1] *= (1.0 - decay_rate)
        
    return -log_likelihood
```

### Model 2: Q-Learning with Choice Kernel
This model hypothesizes that the participant's stickiness is not just a repetition of the very last choice, but a built-up "habit" or "momentum" represented by a Choice Kernel. The kernel accumulates when an action is chosen and decays over time. This captures the "blocking" behavior seen in the data (sticking to one spaceship for many trials) more dynamically than a simple binary stickiness parameter.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Q-Learning with Choice Kernel (Perseveration).
    
    Uses a Choice Kernel to model habit formation. The kernel tracks past choices
    with a decay, creating a 'momentum' for repeated choices independent of reward.
    This replaces simple stickiness with a history-dependent process.
    
    Parameters:
    learning_rate: [0, 1] Learning rate for Q-values.
    beta: [0, 10] Inverse temperature for Q-values.
    decay_kernel: [0, 1] Decay rate of the choice kernel (0 = instant forgetting, 1 = no forgetting).
    weight_kernel: [0, 5] Weight of the choice kernel in decision making (positive = perseveration).
    """
    learning_rate, beta, decay_kernel, weight_kernel = model_parameters
    n_trials = len(action_1)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    # Choice Kernel for Stage 1 (Spaceships)
    ck_stage1 = np.zeros(2)
    
    log_likelihood = 0.0
    eps = 1e-10
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = float(reward[trial])
        
        # Stage 1 Choice with Kernel
        # Decision value = Q + weight * CK
        net_val_1 = q_stage1 + weight_kernel * ck_stage1
        exp_q1 = np.exp(beta * net_val_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_likelihood += np.log(probs_1[a1] + eps)
        
        # Update Kernel
        ck_stage1 *= decay_kernel # Decay previous trace
        ck_stage1[a1] += 1.0      # Reinforce current choice
        
        if a2 == -1:
            continue
            
        # Stage 2 Choice (Standard Q)
        exp_q2 = np.exp(beta * q_stage2[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_likelihood += np.log(probs_2[a2] + eps)
        
        # Update Q-values (Standard MF)
        q_stage2[s2, a2] += learning_rate * (r - q_stage2[s2, a2])
        q_stage1[a1] += learning_rate * (q_stage2[s2, a2] - q_stage1[a1])
        
    return -log_likelihood
```

### Model 3: Independent Stage Learning with Decay
This model tests the hypothesis that the participant treats the two stages as separate, independent bandit problems rather than a chained Markov decision process. In this view, the spaceship choice (Stage 1) is reinforced directly by the final reward, ignoring the intermediate planet state. This is combined with decay to handle the non-stationary nature of the task. It uses separate learning rates for the spaceship and alien choices.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Independent Stage Learning with Decay.
    
    Treats Stage 1 and Stage 2 as separate bandit problems. Stage 1 updates directly
    from the final reward, ignoring the transition structure (Spaceship -> Reward). 
    Stage 2 updates from reward as well. Both include decay for unchosen options.
    
    Parameters:
    lr_s1: [0, 1] Learning rate for Stage 1 (Spaceship) based on direct reward.
    lr_s2: [0, 1] Learning rate for Stage 2 (Alien).
    decay_rate: [0, 1] Decay rate for unchosen options in both stages.
    beta: [0, 10] Inverse temperature.
    """
    lr_s1, lr_s2, decay_rate, beta = model_parameters
    n_trials = len(action_1)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    log_likelihood = 0.0
    eps = 1e-10
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = float(reward[trial])
        
        # Stage 1 Choice
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_likelihood += np.log(probs_1[a1] + eps)
        
        if a2 == -1:
            continue
            
        # Stage 2 Choice
        exp_q2 = np.exp(beta * q_stage2[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_likelihood += np.log(probs_2[a2] + eps)
        
        # Update Stage 2
        q_stage2[s2, a2] += lr_s2 * (r - q_stage2[s2, a2])
        
        # Update Stage 1 (Direct Reward Learning)
        # Ignores Q2 value/transition, uses R directly to update spaceship value
        q_stage1[a1] += lr_s1 * (r - q_stage1[a1])
        
        # Decay Unchosen
        q_stage2[s2, 1-a2] *= (1.0 - decay_rate)
        q_stage2[1-s2, :] *= (1.0 - decay_rate)
        q_stage1[1-a1] *= (1.0 - decay_rate)
        
    return -log_likelihood
```