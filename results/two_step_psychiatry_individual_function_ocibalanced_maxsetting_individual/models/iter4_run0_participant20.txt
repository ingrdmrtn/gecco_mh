Here are three new cognitive models for the two-step decision task, designed to capture different potential mechanisms underlying the participant's behavior.

### Model 1: Hybrid Model with Transition Learning
This model hypothesizes that the participant does not assume the spaceship-planet transition probabilities are fixed (70/30), but instead learns them over time based on observed transitions. This allows the "Model-Based" component to adapt if the participant perceives the environment structure as changing or uncertain.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Transition Learning.
    
    Extends the standard hybrid model by allowing the agent to update their 
    belief about the transition matrix (Spaceship -> Planet) based on experience, 
    rather than using a fixed matrix.
    
    Parameters:
    - lr_val: [0,1] Learning rate for updating Q-values.
    - lr_trans: [0,1] Learning rate for updating the transition matrix.
    - beta_1: [0,10] Inverse temperature for Stage 1 choices.
    - beta_2: [0,10] Inverse temperature for Stage 2 choices.
    - w: [0,1] Weighting parameter (1 = Pure MB, 0 = Pure MF).
    - stickiness: [0,10] Bonus added to the previously chosen spaceship.
    """
    lr_val, lr_trans, beta_1, beta_2, w, stickiness = model_parameters
    n_trials = len(action_1)
    
    # Initialize transition matrix with instructed probabilities
    # Rows: Spaceship 0, Spaceship 1. Cols: Planet 0, Planet 1.
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        # Skip missing data
        if action_1[trial] == -1:
            continue
            
        # --- Stage 1 Policy ---
        # Model-Based: Use current estimate of transition matrix
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid integration
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits = beta_1 * q_net
        if last_action_1 != -1:
            logits[last_action_1] += stickiness
            
        exp_q1 = np.exp(logits - np.max(logits))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Policy ---
        state_idx = state[trial]
        logits_2 = beta_2 * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Value Updates ---
        # Stage 1 MF (SARSA)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr_val * delta_stage1
        
        # Stage 2 MF (Reward Prediction Error)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr_val * delta_stage2
        
        # --- Transition Matrix Update (State Prediction Error) ---
        # Update belief about where the chosen spaceship leads
        chosen_ship = action_1[trial]
        observed_planet = state[trial]
        
        # Move probability mass towards the observed planet
        # T_new = T_old + lr_trans * (1 - T_old) for the observed state
        curr_prob = transition_matrix[chosen_ship, observed_planet]
        transition_matrix[chosen_ship, observed_planet] += lr_trans * (1.0 - curr_prob)
        # Ensure row sums to 1
        transition_matrix[chosen_ship, 1 - observed_planet] = 1.0 - transition_matrix[chosen_ship, observed_planet]
        
        last_action_1 = action_1[trial]

    eps = 1e-10
    valid_indices = p_choice_1 > 0
    log_loss = -(np.sum(np.log(p_choice_1[valid_indices] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_indices] + eps)))
    return log_loss
```

### Model 2: Asymmetric Learning Rate Hybrid Model
This model differentiates between learning from positive prediction errors (outcomes better than expected) and negative prediction errors (outcomes worse than expected). This asymmetry captures potential biases in how the participant processes rewards versus omissions/losses.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Asymmetric Learning Rate Hybrid Model with Stickiness.
    
    Uses separate learning rates for positive and negative prediction errors,
    allowing the agent to learn more aggressively from wins or losses.
    
    Parameters:
    - lr_pos: [0,1] Learning rate for positive prediction errors (delta > 0).
    - lr_neg: [0,1] Learning rate for negative prediction errors (delta < 0).
    - beta_1: [0,10] Inverse temperature for Stage 1.
    - beta_2: [0,10] Inverse temperature for Stage 2.
    - w: [0,1] Weighting parameter (1 = Pure MB, 0 = Pure MF).
    - stickiness: [0,10] Bonus added to the previously chosen spaceship.
    """
    lr_pos, lr_neg, beta_1, beta_2, w, stickiness = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits = beta_1 * q_net
        if last_action_1 != -1:
            logits[last_action_1] += stickiness
            
        exp_q1 = np.exp(logits - np.max(logits))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Policy ---
        state_idx = state[trial]
        logits_2 = beta_2 * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates with Asymmetric Learning ---
        
        # Stage 1 MF Update
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        lr_1 = lr_pos if delta_stage1 > 0 else lr_neg
        q_stage1_mf[action_1[trial]] += lr_1 * delta_stage1
        
        # Stage 2 MF Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        lr_2 = lr_pos if delta_stage2 > 0 else lr_neg
        q_stage2_mf[state_idx, action_2[trial]] += lr_2 * delta_stage2
        
        last_action_1 = action_1[trial]

    eps = 1e-10
    valid_indices = p_choice_1 > 0
    log_loss = -(np.sum(np.log(p_choice_1[valid_indices] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_indices] + eps)))
    return log_loss
```

### Model 3: Hybrid Model with Outcome-Dependent Stickiness
This model posits that the tendency to repeat a choice (stickiness) is not constant but depends on the outcome of the previous trial. It separates stickiness into "Win-Stay" (stickiness after reward) and "Lose-Stay" (stickiness after no reward) components, overlaying a heuristic strategy on the reinforcement learning process.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Outcome-Dependent Stickiness.
    
    Applies different stickiness bonuses depending on whether the previous trial
    resulted in a reward or not. This captures Win-Stay/Lose-Shift dynamics 
    distinct from value-based learning.
    
    Parameters:
    - learning_rate: [0,1] Learning rate for Q-values.
    - beta_1: [0,10] Inverse temperature for Stage 1.
    - beta_2: [0,10] Inverse temperature for Stage 2.
    - w: [0,1] Weighting parameter (1 = Pure MB, 0 = Pure MF).
    - stick_win: [0,10] Stickiness bonus applied after a rewarded trial.
    - stick_loss: [0,10] Stickiness bonus applied after an unrewarded trial.
    """
    learning_rate, beta_1, beta_2, w, stick_win, stick_loss = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    last_reward = -1 # Indicator for previous reward (-1 implies no valid previous trial)
    
    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits = beta_1 * q_net
        
        # Apply outcome-dependent stickiness
        if last_action_1 != -1 and last_reward != -1:
            if last_reward == 1:
                logits[last_action_1] += stick_win
            else:
                logits[last_action_1] += stick_loss
            
        exp_q1 = np.exp(logits - np.max(logits))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Policy ---
        state_idx = state[trial]
        logits_2 = beta_2 * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        last_action_1 = action_1[trial]
        last_reward = reward[trial]

    eps = 1e-10
    valid_indices = p_choice_1 > 0
    log_loss = -(np.sum(np.log(p_choice_1[valid_indices] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_indices] + eps)))
    return log_loss
```