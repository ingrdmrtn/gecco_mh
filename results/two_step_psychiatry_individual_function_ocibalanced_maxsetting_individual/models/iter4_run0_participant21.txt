Here are the three cognitive models in Python functions.

### Cognitive Model 1: Dynamic Transition Learning Model
This model relaxes the assumption that the participant knows the fixed transition probabilities (70%/30%). Instead, the participant learns the transition matrix $T(s'|s,a)$ over time based on observed transitions. The Model-Based value calculation uses this evolving internal model. This allows the agent's strategy to adapt if they perceive the spaceship reliability to be changing.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Dynamic Transition Learning Model.
    
    The agent learns the transition matrix (Spaceship -> Planet) from experience 
    rather than using the fixed 0.7/0.3 probabilities.
    
    Parameters:
    lr_reward: [0, 1] Learning rate for reward Q-values (aliens).
    lr_trans: [0, 1] Learning rate for transition probabilities.
    beta_1: [0, 10] Inverse temperature for Stage 1.
    beta_2: [0, 10] Inverse temperature for Stage 2.
    w: [0, 1] Weight for Model-Based control (0=MF, 1=MB).
    stickiness: [0, 5] Choice stickiness for Stage 1.
    """
    lr_reward, lr_trans, beta_1, beta_2, w, stickiness = model_parameters
    n_trials = len(action_1)

    # Initialize learned transition matrix (2 actions x 2 states)
    # Start with uniform prior (uncertainty)
    learned_transitions = np.array([[0.5, 0.5], [0.5, 0.5]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # 2 planets, 2 aliens

    last_action_1 = -1

    for trial in range(n_trials):
        # -- Stage 1 Policy --
        # Model-Based Value: Uses learned transition matrix
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = learned_transitions @ max_q_stage2
        
        # Net Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Stickiness
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        # Softmax Stage 1
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        # -- Stage 2 Policy --
        state_idx = state[trial] # Planet reached
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta_2 * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # -- Learning --
        
        # 1. Update Transition Matrix (Dynamic MB)
        # Action 0 or 1 led to state_idx
        # Update P(state_idx | action) towards 1, P(other | action) towards 0
        act = action_1[trial]
        # Delta rule for probability estimation
        learned_transitions[act, state_idx] += lr_trans * (1 - learned_transitions[act, state_idx])
        learned_transitions[act, 1 - state_idx] += lr_trans * (0 - learned_transitions[act, 1 - state_idx])

        # 2. Update Stage 1 MF Value (TD(0))
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr_reward * delta_stage1

        # 3. Update Stage 2 MF Value (Reward Prediction Error)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr_reward * delta_stage2

        last_action_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2: Rare Transition Suppression Model
This model hypothesizes that the participant treats "Rare" transitions as unreliable or "flukes" for Model-Free learning. When a rare transition occurs (e.g., Spaceship A going to Planet Y), the Model-Free update for the first stage is suppressed (discounted by `rare_discount`). This prevents the agent from reinforcing a spaceship choice based on a lucky outcome from a rare path, effectively allowing the Model-Based system (which understands the structure) to dominate in these situations.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Rare Transition Suppression Model.
    
    The Model-Free update for Stage 1 is modulated by the type of transition.
    If the transition was Rare, the learning rate is multiplied by `rare_discount`.
    This prevents 'lucky' rare transitions from incorrectly reinforcing the spaceship choice.
    
    Parameters:
    learning_rate: [0, 1] Base learning rate.
    beta_1: [0, 10] Inverse temperature for Stage 1.
    beta_2: [0, 10] Inverse temperature for Stage 2.
    w: [0, 1] Weight for Model-Based control.
    stickiness: [0, 5] Choice stickiness for Stage 1.
    rare_discount: [0, 1] Multiplier for MF learning rate after a rare transition.
    """
    learning_rate, beta_1, beta_2, w, stickiness, rare_discount = model_parameters
    n_trials = len(action_1)

    # Fixed transition matrix for MB calculation
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    last_action_1 = -1

    for trial in range(n_trials):
        # -- Stage 1 Policy --
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        # -- Stage 2 Policy --
        state_idx = state[trial]
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta_2 * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # -- Learning --
        
        # Determine if transition was Common or Rare
        # 0->0 and 1->1 are Common. 0->1 and 1->0 are Rare.
        is_rare = (action_1[trial] != state_idx)
        
        # Modulate learning rate for Stage 1 MF update
        current_lr_s1 = learning_rate
        if is_rare:
            current_lr_s1 *= rare_discount
            
        # Update Stage 1 MF Value
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += current_lr_s1 * delta_stage1

        # Update Stage 2 MF Value (Standard update)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

        last_action_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3: Epsilon-Greedy Mixture Model
This model incorporates an "epsilon-greedy" exploration component (or lapse rate) into the decision process. It assumes that on a small fraction of trials ($\epsilon$), the participant chooses randomly between the options, regardless of their values. This captures noise or attentional lapses that standard softmax models might try to fit by excessively lowering the inverse temperature ($\beta$).

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Epsilon-Greedy Mixture Model.
    
    Assumes the participant makes choices based on the weighted MB/MF values 
    most of the time, but occasionally 'lapses' or explores randomly with probability epsilon.
    The policy is a mixture of Softmax and Uniform random choice.
    
    Parameters:
    learning_rate: [0, 1] Learning rate.
    beta_1: [0, 10] Inverse temperature for Stage 1.
    beta_2: [0, 10] Inverse temperature for Stage 2.
    w: [0, 1] Weight for Model-Based control.
    stickiness: [0, 5] Choice stickiness for Stage 1.
    epsilon: [0, 1] Probability of random choice (lapse rate).
    """
    learning_rate, beta_1, beta_2, w, stickiness, epsilon = model_parameters
    n_trials = len(action_1)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    last_action_1 = -1

    for trial in range(n_trials):
        # -- Stage 1 Policy --
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1_softmax = exp_q1 / np.sum(exp_q1)
        
        # Mixture policy: (1-eps)*Softmax + eps*Uniform
        probs_1 = (1 - epsilon) * probs_1_softmax + epsilon * 0.5
        p_choice_1[trial] = probs_1[action_1[trial]]

        # -- Stage 2 Policy --
        state_idx = state[trial]
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta_2 * qs_current_state)
        probs_2_softmax = exp_q2 / np.sum(exp_q2)
        
        # Mixture policy for Stage 2 as well
        probs_2 = (1 - epsilon) * probs_2_softmax + epsilon * 0.5
        p_choice_2[trial] = probs_2[action_2[trial]]

        # -- Learning --
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1

        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

        last_action_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```