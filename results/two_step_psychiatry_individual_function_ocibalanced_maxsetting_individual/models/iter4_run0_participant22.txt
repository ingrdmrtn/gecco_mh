Here are three new cognitive models for the two-step task.

### Model 1: Hybrid with Asymmetric Stage 2 Learning
This model separates the learning process for the second stage (where rewards are received) into positive and negative updates. This allows the model to react differently to rewards (coins) versus omissions (no coins), which is particularly useful for tracking drifting reward probabilities. Stage 1 (navigation) uses a single learning rate.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Asymmetric Stage 2 Learning.
    
    Differentiates learning rates for positive and negative prediction errors 
    specifically in Stage 2 (reward stage), allowing the model to have different 
    sensitivities to gains and losses/omissions. Stage 1 uses a symmetric learning rate.
    
    Parameters:
    lr_1: [0,1] - Learning rate for Stage 1 (navigation).
    lr_2_pos: [0,1] - Learning rate for Stage 2 when RPE > 0 (Reward).
    lr_2_neg: [0,1] - Learning rate for Stage 2 when RPE < 0 (Omission).
    beta_1: [0,10] - Inverse temperature for Stage 1.
    beta_2: [0,10] - Inverse temperature for Stage 2.
    w: [0,1] - Weighting between Model-Based (1) and Model-Free (0).
    stick: [0,5] - Choice stickiness for Stage 1.
    """
    lr_1, lr_2_pos, lr_2_neg, beta_1, beta_2, w, stick = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    
    prev_action_1 = -1

    for trial in range(n_trials):
        # Skip invalid trials
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
            continue

        # --- Stage 1 Policy ---
        # Model-Based Value: Transition * Max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta_1 * q_net
        if prev_action_1 != -1:
            logits_1[prev_action_1] += stick
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        # --- Stage 2 Policy ---
        state_idx = state[trial]
        logits_2 = beta_2 * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        # Stage 1 Update (Standard TD)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr_1 * delta_stage1

        # Stage 2 Update (Asymmetric)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        if delta_stage2 >= 0:
            current_lr = lr_2_pos
        else:
            current_lr = lr_2_neg
            
        q_stage2_mf[state_idx, action_2[trial]] += current_lr * delta_stage2
        
        prev_action_1 = action_1[trial]

    eps = 1e-10
    # Only sum log-likelihood for valid trials where we made a prediction
    valid_trials = (p_choice_1 > 0) & (p_choice_2 > 0)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```

### Model 2: Hybrid with Choice Momentum (Kernel)
Instead of simple 1-back stickiness, this model maintains a "choice kernel" that tracks the history of recent choices. This allows the model to capture the "momentum" or longer streaks observed in the participant's behavior (e.g., repeatedly choosing the same spaceship).

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Choice Momentum (Kernel).
    
    Replaces simple 1-back stickiness with an exponentially decaying choice kernel.
    This captures longer-term perseveration or 'momentum' in Stage 1 choices.
    
    Parameters:
    lr: [0,1] - Learning rate.
    beta_1: [0,10] - Inverse temperature for Stage 1.
    beta_2: [0,10] - Inverse temperature for Stage 2.
    w: [0,1] - MB/MF weighting.
    k_decay: [0,1] - Decay rate of the choice kernel (0=no decay/infinite memory, 1=instant decay/1-back).
    k_weight: [0,5] - Weight of the choice kernel in the decision (stickiness strength).
    """
    lr, beta_1, beta_2, w, k_decay, k_weight = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Choice kernel for Stage 1 actions [0, 1]
    choice_kernel = np.zeros(2)

    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
            continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add kernel contribution to logits
        logits_1 = beta_1 * q_net + k_weight * choice_kernel
        
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        # --- Stage 2 Policy ---
        state_idx = state[trial]
        logits_2 = beta_2 * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr * delta_stage1

        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr * delta_stage2
        
        # Update Choice Kernel (Exponential Moving Average of choices)
        # Decay both
        choice_kernel *= (1 - k_decay)
        # Reinforce chosen
        choice_kernel[action_1[trial]] += k_decay * 1.0

    eps = 1e-10
    valid_trials = (p_choice_1 > 0) & (p_choice_2 > 0)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```

### Model 3: Hybrid with Forgetting to Mean
Unlike the "Decay" model which decays unchosen values to 0 (pessimism), this model decays unchosen alien values towards 0.5 (uncertainty/neutrality). This is appropriate for tracking drifting probabilities that center around 0.5, preventing the model from permanently discarding an option just because it hasn't been sampled recently.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Forgetting to Mean (Decay to 0.5).
    
    Unchosen actions in Stage 2 decay towards a neutral value of 0.5 
    (the expected mean of the drifting probabilities) rather than 0.
    This encourages re-exploration of neglected options.
    
    Parameters:
    lr: [0,1] - Learning rate.
    beta_1: [0,10] - Inverse temperature for Stage 1.
    beta_2: [0,10] - Inverse temperature for Stage 2.
    w: [0,1] - MB/MF weighting.
    stick: [0,5] - Choice stickiness for Stage 1.
    decay: [0,1] - Rate at which unchosen Q-values revert to 0.5.
    """
    lr, beta_1, beta_2, w, stick, decay = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    # Initialize Q-values at 0.5 (neutral prior)
    q_stage2_mf = np.full((2, 2), 0.5)
    
    prev_action_1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
            continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta_1 * q_net
        if prev_action_1 != -1:
            logits_1[prev_action_1] += stick
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        # --- Stage 2 Policy ---
        state_idx = state[trial]
        logits_2 = beta_2 * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr * delta_stage1

        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr * delta_stage2
        
        # Decay unchosen Stage 2 option towards 0.5
        unchosen_a2 = 1 - action_2[trial]
        q_stage2_mf[state_idx, unchosen_a2] += decay * (0.5 - q_stage2_mf[state_idx, unchosen_a2])
        
        prev_action_1 = action_1[trial]

    eps = 1e-10
    valid_trials = (p_choice_1 > 0) & (p_choice_2 > 0)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```