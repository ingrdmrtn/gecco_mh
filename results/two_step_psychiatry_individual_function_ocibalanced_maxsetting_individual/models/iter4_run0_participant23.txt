Here are 3 new cognitive models for the two-step task, designed to capture different psychological mechanisms such as outcome-dependent perseverance, dynamic learning of environmental structure, and independent valuation channels.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Q-Learning with Outcome-Dependent Stickiness.
    
    This model assumes the participant uses a Model-Free strategy (TD-learning)
    but exhibits different levels of perseverance (stickiness) depending on 
    whether the previous trial was rewarded or not. This captures 'Win-Stay' 
    and 'Lose-Shift' (or 'Lose-Stay') dynamics explicitly in the policy.
    
    Parameters:
    learning_rate: [0, 1] Update rate for Q-values.
    beta:          [0, 10] Inverse temperature (exploration/exploitation).
    stick_win:     [-5, 5] Stickiness bonus applied to the previous choice if rewarded.
    stick_loss:    [-5, 5] Stickiness bonus applied to the previous choice if unrewarded.
    """
    learning_rate, beta, stick_win, stick_loss = model_parameters
    n_trials = len(action_1)
    
    # Q-values for Stage 1 (2 actions) and Stage 2 (2 states x 2 actions)
    q_stage1 = np.zeros(2) + 0.5
    q_stage2 = np.zeros((2, 2)) + 0.5
    
    log_loss = 0
    eps = 1e-10
    
    prev_action_1 = -1
    prev_reward = -1
    
    for trial in range(n_trials):
        a1 = action_1[trial]
        s_idx = state[trial]
        a2 = action_2[trial]
        r = reward[trial]
        
        # Skip missing data
        if a1 == -1 or s_idx == -1 or a2 == -1:
            prev_action_1 = -1
            continue
        
        # --- Stage 1 Choice ---
        logits_1 = beta * q_stage1
        
        # Apply outcome-dependent stickiness
        if prev_action_1 != -1:
            if prev_reward == 1:
                logits_1[prev_action_1] += stick_win
            else:
                logits_1[prev_action_1] += stick_loss
        
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + eps)
        
        # --- Stage 2 Choice ---
        logits_2 = beta * q_stage2[s_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[a2] + eps)
        
        # --- Learning ---
        # Update Stage 1 Q-value using Stage 2 Q-value (TD(0)-like update)
        target_stage1 = q_stage2[s_idx, a2]
        q_stage1[a1] += learning_rate * (target_stage1 - q_stage1[a1])
        
        # Update Stage 2 Q-value using Reward
        q_stage2[s_idx, a2] += learning_rate * (r - q_stage2[s_idx, a2])
        
        prev_action_1 = a1
        prev_reward = r
        
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Dynamic Model-Based Learner with Transition Learning.
    
    This model assumes the participant is Model-Based but learns the 
    transition probabilities between spaceships and planets over time, 
    rather than assuming them fixed. This allows the model to capture 
    behavior where the participant might believe the spaceship destinations 
    have swapped.
    
    Parameters:
    lr_value:      [0, 1] Learning rate for Stage 2 action values.
    lr_trans:      [0, 1] Learning rate for transition probabilities.
    beta:          [0, 10] Inverse temperature.
    stickiness:    [0, 5] Choice stickiness for Stage 1.
    """
    lr_value, lr_trans, beta, stickiness = model_parameters
    n_trials = len(action_1)
    
    # Initialize transition matrix: Row=Action1, Col=State (Planet)
    # Start with standard assumption: A(0)->X(0) common (0.7), U(1)->Y(1) common (0.7).
    trans_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Stage 2 values 
    q_stage2 = np.zeros((2, 2)) + 0.5
    
    log_loss = 0
    eps = 1e-10
    prev_action_1 = -1
    
    for trial in range(n_trials):
        a1 = action_1[trial]
        s_idx = state[trial]
        a2 = action_2[trial]
        r = reward[trial]
        
        if a1 == -1 or s_idx == -1 or a2 == -1:
            prev_action_1 = -1
            continue
        
        # --- Stage 1 Choice (Model-Based) ---
        # Calculate expected value of each Stage 1 action using current transition matrix
        max_q2 = np.max(q_stage2, axis=1) # Max value for each state
        q_mb = trans_matrix @ max_q2
        
        logits_1 = beta * q_mb
        if prev_action_1 != -1:
            logits_1[prev_action_1] += stickiness
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + eps)
        
        # --- Stage 2 Choice ---
        logits_2 = beta * q_stage2[s_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[a2] + eps)
        
        # --- Learning ---
        # 1. Update Stage 2 values
        q_stage2[s_idx, a2] += lr_value * (r - q_stage2[s_idx, a2])
        
        # 2. Update Transition Matrix
        # Move probability mass toward the observed state s_idx for action a1
        trans_matrix[a1, :] *= (1.0 - lr_trans)
        trans_matrix[a1, s_idx] += lr_trans
        
        prev_action_1 = a1
        
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid RL with Independent Evaluation Channels.
    
    This model maintains separate Model-Based and Model-Free values for Stage 1.
    Instead of a weighting parameter 'w' that trades off between them, it uses 
    separate inverse temperatures (betas) for the Model-Based and Model-Free 
    components. This allows the participant to rely on both, neither, or one 
    system independently.
    
    Parameters:
    learning_rate: [0, 1] Update rate for Q-values.
    beta_mf:       [0, 10] Inverse temperature for Model-Free values (Stage 1).
    beta_mb:       [0, 10] Inverse temperature for Model-Based values (Stage 1).
    beta_s2:       [0, 10] Inverse temperature for Stage 2 choices.
    stickiness:    [0, 5] Choice stickiness for Stage 1.
    """
    learning_rate, beta_mf, beta_mb, beta_s2, stickiness = model_parameters
    n_trials = len(action_1)
    
    # Fixed transition matrix for MB
    trans_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_mf_stage1 = np.zeros(2) + 0.5
    q_stage2 = np.zeros((2, 2)) + 0.5
    
    log_loss = 0
    eps = 1e-10
    prev_action_1 = -1
    
    for trial in range(n_trials):
        a1 = action_1[trial]
        s_idx = state[trial]
        a2 = action_2[trial]
        r = reward[trial]
        
        if a1 == -1 or s_idx == -1 or a2 == -1:
            prev_action_1 = -1
            continue
        
        # --- Stage 1 Choice ---
        # Model-Based Value
        max_q2 = np.max(q_stage2, axis=1)
        q_mb_stage1 = trans_matrix @ max_q2
        
        # Combine logits independently
        logits_1 = (beta_mf * q_mf_stage1) + (beta_mb * q_mb_stage1)
        
        if prev_action_1 != -1:
            logits_1[prev_action_1] += stickiness
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + eps)
        
        # --- Stage 2 Choice ---
        logits_2 = beta_s2 * q_stage2[s_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[a2] + eps)
        
        # --- Learning ---
        # Update Stage 1 MF using Q(s, a2) as target
        target_mf = q_stage2[s_idx, a2]
        q_mf_stage1[a1] += learning_rate * (target_mf - q_mf_stage1[a1])
        
        # Update Stage 2
        q_stage2[s_idx, a2] += learning_rate * (r - q_stage2[s_idx, a2])
        
        prev_action_1 = a1
        
    return log_loss
```