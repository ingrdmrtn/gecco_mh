Here are the three proposed cognitive models. They are all variations of the "Pure Model-Based with Transition Learning" framework, which was identified as the most successful approach so far. Each model incorporates a specific cognitive mechanism (valence-dependent learning, memory decay, or stage-specific noise) to better capture the participant's behavior.

### Model 1: Asymmetric Reward Learning
This model extends the transition-learning model by allowing different learning rates for positive (reward = 1) and negative (reward = 0) outcomes. This tests the hypothesis that the participant updates their beliefs about the aliens differently when gaining a coin versus receiving nothing.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Model 1: Pure Model-Based with Transition Learning and Asymmetric Reward Learning.
    
    This model assumes the participant uses a Model-Based strategy where transition
    probabilities are learned dynamically. Crucially, it splits the learning rate for
    Stage 2 values into 'positive' (reward=1) and 'negative' (reward=0) learning rates,
    capturing potential valence-induced biases in updating value estimates.
    
    Parameters:
    alpha_pos: [0, 1] - Learning rate for Stage 2 Q-values when reward is 1.
    alpha_neg: [0, 1] - Learning rate for Stage 2 Q-values when reward is 0.
    lr_trans:  [0, 1] - Learning rate for transition probabilities.
    beta:      [0, 10] - Inverse temperature for softmax choice.
    pers:      [0, 5] - Perseverance bonus for repeating the previous Stage 1 action.
    """
    alpha_pos, alpha_neg, lr_trans, beta, pers = model_parameters
    n_trials = len(action_1)

    # Initialize Stage 2 Q-values (2 Planets x 2 Aliens)
    q_stage2 = np.zeros((2, 2))
    
    # Initialize Transition Probabilities (2 Spaceships x 2 Planets) to 0.5 (uncertain)
    trans_probs = np.ones((2, 2)) * 0.5
    
    last_action_1 = -1
    log_loss = 0.0
    eps = 1e-10
    
    for trial in range(n_trials):
        # Handle missing data
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1 or reward[trial] == -1:
            last_action_1 = -1
            continue
            
        a1 = int(action_1[trial])
        s_next = int(state[trial])
        a2 = int(action_2[trial])
        r = float(reward[trial])

        # --- Stage 1 Choice (Model-Based) ---
        # Calculate Model-Based values: Transition Probs * Max Stage 2 Values
        max_q_s2 = np.max(q_stage2, axis=1) 
        q_mb = trans_probs @ max_q_s2

        # Add Perseverance Bonus
        q_net_s1 = q_mb.copy()
        if last_action_1 != -1:
            q_net_s1[last_action_1] += pers

        # Softmax Probability for Stage 1
        exp_q1 = np.exp(beta * q_net_s1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + eps)

        # --- Stage 2 Choice (Value-Based) ---
        # Softmax Probability for Stage 2
        exp_q2 = np.exp(beta * q_stage2[s_next])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[a2] + eps)

        # --- Learning ---
        # Update Stage 2 Q-values with Asymmetric Learning Rates
        delta_2 = r - q_stage2[s_next, a2]
        if r > 0:
            q_stage2[s_next, a2] += alpha_pos * delta_2
        else:
            q_stage2[s_next, a2] += alpha_neg * delta_2

        # Update Transition Probabilities
        # Increase prob of the observed transition, decrease prob of the unobserved one
        trans_probs[a1, s_next] += lr_trans * (1.0 - trans_probs[a1, s_next])
        trans_probs[a1, 1 - s_next] += lr_trans * (0.0 - trans_probs[a1, 1 - s_next])
        
        last_action_1 = a1
        
    return log_loss
```

### Model 2: Forgetting / Decay
This model addresses the non-stationary nature of the alien reward probabilities. It incorporates a decay parameter that pulls the Q-values of *unchosen* aliens towards a neutral prior (0.5). This allows the model to "forget" old information and adapt better to the slowly changing probabilities described in the task.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model 2: Pure Model-Based with Transition Learning and Value Decay.
    
    This model incorporates a 'forgetting' mechanism. Since the alien reward 
    probabilities drift over time, keeping old Q-values fixed indefinitely is 
    suboptimal. This model decays the Q-values of unchosen aliens towards 0.5 
    (chance) on every trial, representing increasing uncertainty about unvisited states.
    
    Parameters:
    lr_val:   [0, 1] - Learning rate for chosen Stage 2 Q-values.
    lr_trans: [0, 1] - Learning rate for transition probabilities.
    decay:    [0, 1] - Decay rate for unchosen Stage 2 Q-values (0=no decay, 1=reset to 0.5).
    beta:     [0, 10] - Inverse temperature for softmax choice.
    pers:     [0, 5] - Perseverance bonus for Stage 1 choice.
    """
    lr_val, lr_trans, decay, beta, pers = model_parameters
    n_trials = len(action_1)

    # Initialize Stage 2 Q-values to 0.5 (neutral prior for decay target)
    q_stage2 = np.ones((2, 2)) * 0.5
    
    trans_probs = np.ones((2, 2)) * 0.5
    
    last_action_1 = -1
    log_loss = 0.0
    eps = 1e-10
    
    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1 or reward[trial] == -1:
            last_action_1 = -1
            continue
            
        a1 = int(action_1[trial])
        s_next = int(state[trial])
        a2 = int(action_2[trial])
        r = float(reward[trial])

        # --- Stage 1 Choice ---
        max_q_s2 = np.max(q_stage2, axis=1) 
        q_mb = trans_probs @ max_q_s2

        q_net_s1 = q_mb.copy()
        if last_action_1 != -1:
            q_net_s1[last_action_1] += pers

        exp_q1 = np.exp(beta * q_net_s1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + eps)

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[s_next])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[a2] + eps)

        # --- Learning ---
        # Update chosen alien
        delta_2 = r - q_stage2[s_next, a2]
        q_stage2[s_next, a2] += lr_val * delta_2
        
        # Decay unchosen aliens towards 0.5
        # We iterate over all aliens; if not chosen, apply decay.
        # Note: In the current state s_next, the unchosen is 1-a2.
        # Aliens in the OTHER state (1-s_next) are also unchosen.
        
        # Decay unchosen option in current state
        unchosen_a2 = 1 - a2
        q_stage2[s_next, unchosen_a2] += decay * (0.5 - q_stage2[s_next, unchosen_a2])
        
        # Decay both options in the unvisited state
        unvisited_state = 1 - s_next
        q_stage2[unvisited_state, 0] += decay * (0.5 - q_stage2[unvisited_state, 0])
        q_stage2[unvisited_state, 1] += decay * (0.5 - q_stage2[unvisited_state, 1])

        # Update Transition Probabilities
        trans_probs[a1, s_next] += lr_trans * (1.0 - trans_probs[a1, s_next])
        trans_probs[a1, 1 - s_next] += lr_trans * (0.0 - trans_probs[a1, 1 - s_next])
        
        last_action_1 = a1
        
    return log_loss
```

### Model 3: Separate Stage Temperatures
This model posits that the participant's decision noise differs between the two stages. Stage 1 involves abstract planning based on transitions, while Stage 2 is a direct bandit task. By using separate inverse temperature parameters (`beta1` and `beta2`), the model can capture different levels of exploration or consistency in these two distinct decision contexts.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model 3: Pure Model-Based with Transition Learning and Separate Stage Temperatures.
    
    This model relaxes the assumption that decision noise is constant across both 
    stages. It fits separate inverse temperature parameters (beta) for the spaceship 
    choice (Stage 1) and the alien choice (Stage 2), allowing the model to capture 
    varying degrees of exploration or stochasticity in the two different decision contexts.
    
    Parameters:
    lr_val:   [0, 1] - Learning rate for Stage 2 Q-values.
    lr_trans: [0, 1] - Learning rate for transition probabilities.
    beta1:    [0, 10] - Inverse temperature for Stage 1 (Spaceship) choice.
    beta2:    [0, 10] - Inverse temperature for Stage 2 (Alien) choice.
    pers:     [0, 5] - Perseverance bonus for Stage 1 choice.
    """
    lr_val, lr_trans, beta1, beta2, pers = model_parameters
    n_trials = len(action_1)

    q_stage2 = np.zeros((2, 2))
    trans_probs = np.ones((2, 2)) * 0.5
    
    last_action_1 = -1
    log_loss = 0.0
    eps = 1e-10
    
    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1 or reward[trial] == -1:
            last_action_1 = -1
            continue
            
        a1 = int(action_1[trial])
        s_next = int(state[trial])
        a2 = int(action_2[trial])
        r = float(reward[trial])

        # --- Stage 1 Choice ---
        max_q_s2 = np.max(q_stage2, axis=1) 
        q_mb = trans_probs @ max_q_s2

        q_net_s1 = q_mb.copy()
        if last_action_1 != -1:
            q_net_s1[last_action_1] += pers

        # Use beta1 for Stage 1
        exp_q1 = np.exp(beta1 * q_net_s1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + eps)

        # --- Stage 2 Choice ---
        # Use beta2 for Stage 2
        exp_q2 = np.exp(beta2 * q_stage2[s_next])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[a2] + eps)

        # --- Learning ---
        delta_2 = r - q_stage2[s_next, a2]
        q_stage2[s_next, a2] += lr_val * delta_2

        trans_probs[a1, s_next] += lr_trans * (1.0 - trans_probs[a1, s_next])
        trans_probs[a1, 1 - s_next] += lr_trans * (0.0 - trans_probs[a1, 1 - s_next])
        
        last_action_1 = a1
        
    return log_loss
```