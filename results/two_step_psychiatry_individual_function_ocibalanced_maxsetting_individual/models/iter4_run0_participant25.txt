Here are three new cognitive models for the two-step decision task, designed to capture specific behavioral patterns such as structural learning, outcome-dependent perseveration, and subjective valuation of losses.

### Model 1: Adaptive Model-Based RL
This model assumes the participant does not rely on a fixed transition matrix (e.g., 70/30) but learns the transition probabilities between spaceships and planets over time. This "Adaptive Model-Based" controller is mixed with a standard Model-Free controller. This explains behavior where the participant might develop idiosyncratic beliefs about spaceship destinations based on the specific history of transitions they experienced (e.g., the run of "rare" transitions).

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Adaptive Model-Based RL.
    
    The agent learns the transition matrix (Action -> State) online, rather than
    using a fixed one. This learned model is used for Model-Based evaluation, 
    mixed with Model-Free values.

    Parameters:
    - lr_val: [0, 1] Learning rate for action values (MF and Stage 2).
    - lr_trans: [0, 1] Learning rate for transition probabilities.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weighting parameter (0 = Pure MF, 1 = Pure MB).
    """
    lr_val, lr_trans, beta, w = model_parameters
    n_trials = len(action_1)

    # Initialize MF Q-values
    q_stage1_mf = np.zeros(2) + 0.5
    q_stage2_mf = np.zeros((2, 2)) + 0.5
    
    # Initialize estimated transition matrix (Action x State) to uniform 0.5
    # Row 0: Action 0 -> [Prob(S0), Prob(S1)]
    # Row 1: Action 1 -> [Prob(S0), Prob(S1)]
    trans_est = np.ones((2, 2)) * 0.5

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # 1. Calculate Model-Based values using current transition estimate
        max_q_stage2 = np.max(q_stage2_mf, axis=1) # Max value achievable in each state
        q_stage1_mb = trans_est @ max_q_stage2
        
        # 2. Mix MB and MF values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # 3. Softmax
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        s_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        r = reward[trial]

        # --- Updates ---
        
        # 1. Update Transition Matrix (Adaptive MB)
        # Increase prob of observed transition, decrease prob of unobserved
        # trans_est[a1, s_idx] moves towards 1
        trans_est[a1, s_idx] += lr_trans * (1.0 - trans_est[a1, s_idx])
        # The other state's prob must be 1 - P(observed)
        trans_est[a1, 1 - s_idx] = 1.0 - trans_est[a1, s_idx]

        # 2. Update Stage 1 MF Values (TD-0)
        # Using the value of the state reached (max Q of stage 2) as target
        # Note: Could also use SARSA (q_stage2_mf[s_idx, a2]), using max for standard MB-MF hybrid logic
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr_val * delta_stage1

        # 3. Update Stage 2 Values
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr_val * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Outcome-Sensitive Stickiness RL
This model hypothesizes that the participant's tendency to repeat a choice (stickiness) depends on the previous outcome. A "Win" (reward=1) might induce a different level of perseveration than a "Loss" (reward=0). This captures behaviors like "Win-Stay, Lose-Stay" (high stickiness on both) or "Win-Stay, Lose-Switch" (high win stickiness, low/negative loss stickiness), which simple stickiness cannot distinguish.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Outcome-Sensitive Stickiness RL.
    
    A Model-Free agent that applies a choice bonus (stickiness) to the previously 
    chosen action. The magnitude of this bonus depends on whether the previous 
    trial was rewarded (stick_win) or unrewarded (stick_loss).

    Parameters:
    - lr: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - stick_win: [0, 10] Stickiness bonus after a reward (Win-Stay).
    - stick_loss: [0, 10] Stickiness bonus after no reward (Lose-Stay).
    """
    lr, beta, stick_win, stick_loss = model_parameters
    n_trials = len(action_1)

    q_stage1 = np.zeros(2) + 0.5
    q_stage2 = np.zeros((2, 2)) + 0.5
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Track previous trial info
    prev_a1 = -1
    prev_reward = 0

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        q_net = q_stage1.copy()
        
        # Apply outcome-dependent stickiness
        if prev_a1 != -1:
            if prev_reward == 1:
                q_net[prev_a1] += stick_win
            else:
                q_net[prev_a1] += stick_loss
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        s_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2[s_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        r = reward[trial]

        # --- Updates ---
        # TD(0) updates
        delta_stage1 = q_stage2[s_idx, a2] - q_stage1[a1]
        q_stage1[a1] += lr * delta_stage1
        
        delta_stage2 = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += lr * delta_stage2
        
        # Store for next trial
        prev_a1 = a1
        prev_reward = r

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Subjective Utility RL (Loss Sensitivity)
This model introduces a subjective valuation of the "no coin" outcome. Instead of treating 0 coins as a neutral value (0), the participant might perceive it as a loss (negative value). This "Loss Sensitivity" parameter scales the effective reward of a 0-coin outcome. This is combined with standard stickiness to account for the participant's perseveration.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Subjective Utility RL (Loss Sensitivity).
    
    Model-Free RL where the reward of '0 coins' is perceived as a negative utility 
    determined by 'loss_sens'. Reward of '1 coin' remains 1. 
    Includes standard stickiness.
    
    Parameters:
    - lr: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - loss_sens: [0, 5] Magnitude of negative utility for 0 reward (Effective r = -loss_sens).
    - stickiness: [0, 5] General choice perseveration bonus.
    """
    lr, beta, loss_sens, stickiness = model_parameters
    n_trials = len(action_1)

    q_stage1 = np.zeros(2) + 0.5
    q_stage2 = np.zeros((2, 2)) + 0.5
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        q_net = q_stage1.copy()
        
        if prev_a1 != -1:
            q_net[prev_a1] += stickiness
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        s_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2[s_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        r = reward[trial]
        
        # Transform Reward: 0 becomes -loss_sens, 1 stays 1
        r_effective = 1.0 if r == 1 else -loss_sens

        # --- Updates ---
        delta_stage1 = q_stage2[s_idx, a2] - q_stage1[a1]
        q_stage1[a1] += lr * delta_stage1
        
        delta_stage2 = r_effective - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += lr * delta_stage2
        
        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```