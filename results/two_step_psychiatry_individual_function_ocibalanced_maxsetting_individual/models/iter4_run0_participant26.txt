Here are the three cognitive models implemented as Python functions.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner with Reward-Dependent Stickiness (WSLS).
    
    This model combines Model-Based (MB) and Model-Free (MF) reinforcement learning 
    strategies. It introduces a 'Reward-Dependent Stickiness' mechanism, where the 
    tendency to repeat the previous Stage 1 choice differs based on whether the 
    previous trial was rewarded or not. This captures 'Win-Stay, Lose-Shift' 
    dynamics overlaid on the RL process.

    Parameters:
    - learning_rate: [0, 1] Update rate for Stage 1 MF values and Stage 2 alien values.
    - beta_1: [0, 10] Inverse temperature for Stage 1 choice (exploration/exploitation).
    - beta_2: [0, 10] Inverse temperature for Stage 2 choice.
    - w: [0, 1] Weighting parameter (0 = Pure MF, 1 = Pure MB).
    - stick_win: [0, 5] Stickiness bonus added to the previous choice if it was rewarded.
    - stick_loss: [0, 5] Stickiness bonus added to the previous choice if it was unrewarded.
    """
    learning_rate, beta_1, beta_2, w, stick_win, stick_loss = model_parameters
    n_trials = len(action_1)

    # Fixed transition matrix: T[action, state]
    # A(0) -> X(0) is 0.7, A(0) -> Y(1) is 0.3
    # U(1) -> X(0) is 0.3, U(1) -> Y(1) is 0.7
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    # Q-values
    q_stage1_mf = np.zeros(2)      # Model-free values for Stage 1 actions
    q_stage2_mf = np.zeros((2, 2)) # Values for Aliens (Stage 2): [State, Alien]

    last_action_1 = -1
    last_reward = -1
    
    log_loss = 0.0
    eps = 1e-10

    for trial in range(n_trials):
        # Handle missing data
        if action_1[trial] == -1:
            last_action_1 = -1
            last_reward = -1
            continue

        # --- Stage 1 Choice ---
        
        # 1. Model-Based Value Calculation
        # Bellman equation: Q_MB = T * max(Q_Stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2

        # 2. Hybrid Value Calculation
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf

        # 3. Apply Reward-Dependent Stickiness
        if last_action_1 != -1:
            if last_reward == 1:
                q_net[last_action_1] += stick_win
            else:
                q_net[last_action_1] += stick_loss

        # 4. Action Probability (Softmax)
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        # Accumulate Loss
        chosen_a1 = int(action_1[trial])
        log_loss -= np.log(probs_1[chosen_a1] + eps)

        # --- Stage 2 Choice ---
        
        current_state = int(state[trial])
        
        # Softmax for Stage 2
        exp_q2 = np.exp(beta_2 * q_stage2_mf[current_state])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        chosen_a2 = int(action_2[trial])
        log_loss -= np.log(probs_2[chosen_a2] + eps)

        # --- Learning / Updates ---
        
        r = reward[trial]

        # TD(0) update for Stage 1 Model-Free value
        # Update target is the value of the chosen stage 2 option (state-action value)
        delta_stage1 = q_stage2_mf[current_state, chosen_a2] - q_stage1_mf[chosen_a1]
        q_stage1_mf[chosen_a1] += learning_rate * delta_stage1

        # TD update for Stage 2 value (Bandit update)
        delta_stage2 = r - q_stage2_mf[current_state, chosen_a2]
        q_stage2_mf[current_state, chosen_a2] += learning_rate * delta_stage2

        # Update history
        last_action_1 = chosen_a1
        last_reward = r

    return log_loss

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner with State Persistence (Planet Stickiness).
    
    This model assumes the participant has a bias to return to the *State* (Planet) 
    they visited in the previous trial, rather than just repeating the *Action* 
    (Spaceship). The model calculates which Stage 1 action is most likely to lead 
    to the previous state and adds a bonus to that action. This distinguishes 
    location-based perseverance from motor perseverance.

    Parameters:
    - learning_rate: [0, 1] Update rate for Q-values.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - w: [0, 1] MB/MF mixing weight.
    - action_stickiness: [0, 5] Standard bonus for repeating the previous action.
    - state_stickiness: [0, 5] Bonus for the action leading to the previous state.
    """
    learning_rate, beta_1, beta_2, w, action_stickiness, state_stickiness = model_parameters
    n_trials = len(action_1)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    last_action_1 = -1
    last_state = -1
    
    log_loss = 0.0
    eps = 1e-10

    for trial in range(n_trials):
        if action_1[trial] == -1:
            last_action_1 = -1
            last_state = -1
            continue

        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf

        # Apply Standard Action Stickiness
        if last_action_1 != -1:
            q_net[last_action_1] += action_stickiness
            
        # Apply State Persistence (Planet Stickiness)
        # If we were at Planet X (0) last time, we add bonus to actions leading to X.
        # Bonus = state_stickiness * P(PreviousState | Action)
        if last_state != -1:
            # For Action 0
            q_net[0] += state_stickiness * transition_matrix[0, last_state]
            # For Action 1
            q_net[1] += state_stickiness * transition_matrix[1, last_state]

        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        chosen_a1 = int(action_1[trial])
        log_loss -= np.log(probs_1[chosen_a1] + eps)

        # --- Stage 2 Choice ---
        current_state = int(state[trial])
        exp_q2 = np.exp(beta_2 * q_stage2_mf[current_state])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        chosen_a2 = int(action_2[trial])
        log_loss -= np.log(probs_2[chosen_a2] + eps)

        # --- Updates ---
        r = reward[trial]

        delta_stage1 = q_stage2_mf[current_state, chosen_a2] - q_stage1_mf[chosen_a1]
        q_stage1_mf[chosen_a1] += learning_rate * delta_stage1

        delta_stage2 = r - q_stage2_mf[current_state, chosen_a2]
        q_stage2_mf[current_state, chosen_a2] += learning_rate * delta_stage2

        last_action_1 = chosen_a1
        last_state = current_state

    return log_loss

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner with Transition-Gated Learning Rates.
    
    This model posits that the Model-Free update for the Stage 1 choice is modulated 
    by the type of transition (Common vs. Rare) that occurred. A 'Common' transition 
    might allow for clearer credit assignment (higher learning rate), while a 'Rare' 
    transition might dampen the learning signal (lower learning rate) or vice versa.

    Parameters:
    - lr_common: [0, 1] Learning rate for Stage 1 MF values after a Common transition.
    - lr_rare: [0, 1] Learning rate for Stage 1 MF values after a Rare transition.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - w: [0, 1] MB/MF mixing weight.
    - stickiness: [0, 5] Choice stickiness bonus.
    """
    lr_common, lr_rare, beta_1, beta_2, w, stickiness = model_parameters
    n_trials = len(action_1)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    last_action_1 = -1
    log_loss = 0.0
    eps = 1e-10

    for trial in range(n_trials):
        if action_1[trial] == -1:
            last_action_1 = -1
            continue

        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf

        if last_action_1 != -1:
            q_net[last_action_1] += stickiness

        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        chosen_a1 = int(action_1[trial])
        log_loss -= np.log(probs_1[chosen_a1] + eps)

        # --- Stage 2 Choice ---
        current_state = int(state[trial])
        exp_q2 = np.exp(beta_2 * q_stage2_mf[current_state])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        chosen_a2 = int(action_2[trial])
        log_loss -= np.log(probs_2[chosen_a2] + eps)

        # --- Updates ---
        r = reward[trial]
        
        # Determine transition type
        # A(0)->X(0) is Common, U(1)->Y(1) is Common. 
        # So if action == state, it's Common.
        is_common = (chosen_a1 == current_state)
        
        # Select Learning Rate based on transition
        current_lr_s1 = lr_common if is_common else lr_rare
        
        # Note: Stage 2 updates normally use a single learning rate (usually just lr_common or mean)
        # Here we assume the stage 2 update uses lr_common as the 'base' rate, or we could add a param.
        # To stick to 6 params, we use lr_common for stage 2 updates.
        lr_stage2 = lr_common 

        # Stage 1 Update (Gated by transition type)
        delta_stage1 = q_stage2_mf[current_state, chosen_a2] - q_stage1_mf[chosen_a1]
        q_stage1_mf[chosen_a1] += current_lr_s1 * delta_stage1

        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[current_state, chosen_a2]
        q_stage2_mf[current_state, chosen_a2] += lr_stage2 * delta_stage2

        last_action_1 = chosen_a1

    return log_loss
```