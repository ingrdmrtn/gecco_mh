Here are three new cognitive models for the two-step task, designed based on the participant's behavior and the provided template.

### Model 1: Counterfactual Stage 2 Learning
This model hypothesizes that the participant learns not just from the outcome of the chosen alien, but also updates their belief about the unchosen alien (counterfactual updating). Given the binary nature of the rewards (finding gold vs. not), the participant might infer that if the chosen alien yields no gold, the unchosen one likely would have (or vice versa). This helps the model adapt faster in environments with drifting, anti-correlated, or competitive reward structures.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Counterfactual Stage 2 Learning Model.
    
    Incorporates counterfactual updating for the second stage (alien choice).
    When an outcome is observed for the chosen alien, the model also updates 
    the unchosen alien's value towards the opposite outcome (1 - reward), 
    controlled by a separate learning rate.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for chosen actions.
    - cf_learning_rate: [0, 1] Counterfactual learning rate for unchosen actions.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - w: [0, 1] Weighting parameter (0=Model-Free, 1=Model-Based).
    - stickiness: [0, 5] Choice stickiness for Stage 1.
    """
    learning_rate, cf_learning_rate, beta_1, beta_2, w, stickiness = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # [State, Alien]
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        # Handle missing data
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1 or reward[trial] == -1:
            last_action_1 = -1
            continue

        # --- Stage 1 Policy ---
        # Model-Based Value: Transition * Max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Value: Weighted sum of MB and MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Apply Stickiness
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        # Softmax Choice 1
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Policy ---
        state_idx = state[trial]
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        # Update Stage 1 MF
        # SARSA-style update: Q1(a1) += lr * (Q2(s, a2) - Q1(a1))
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Update Stage 2 MF (Chosen)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Update Stage 2 MF (Unchosen - Counterfactual)
        # Assume unchosen would have yielded (1 - reward)
        unchosen_action = 1 - action_2[trial]
        # Note: reward is 0 or 1.
        fictitious_reward = 1.0 - reward[trial]
        delta_cf = fictitious_reward - q_stage2_mf[state_idx, unchosen_action]
        q_stage2_mf[state_idx, unchosen_action] += cf_learning_rate * delta_cf
        
        last_action_1 = action_1[trial]

    eps = 1e-10
    # Use mask to ignore missing trials in loss calculation
    mask = (p_choice_1 > 0) & (p_choice_2 > 0)
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss
```

### Model 2: Transition-Dependent Stickiness
This model posits that the participant's tendency to repeat their previous spaceship choice depends on whether the resulting transition was "Common" (expected) or "Rare" (unexpected). A rare transition might act as a "surprise" signal that disrupts the habitual repetition of the choice, or conversely, the participant might have different perseverance levels depending on the transition type.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Transition-Dependent Stickiness Model.
    
    Differentiates choice stickiness based on the previous trial's transition type.
    If the previous transition was Common, 'stick_common' is applied.
    If the previous transition was Rare, 'stick_rare' is applied.
    This allows the model to capture if rare transitions disrupt or enhance habits.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta_1: [0, 10] Stage 1 inverse temperature.
    - beta_2: [0, 10] Stage 2 inverse temperature.
    - w: [0, 1] MB/MF weight.
    - stick_common: [0, 5] Stickiness after a Common transition.
    - stick_rare: [0, 5] Stickiness after a Rare transition.
    """
    learning_rate, beta_1, beta_2, w, stick_common, stick_rare = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    last_transition_was_common = None # Boolean
    
    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1 or reward[trial] == -1:
            last_action_1 = -1
            last_transition_was_common = None
            continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Apply Transition-Dependent Stickiness
        if last_action_1 != -1 and last_transition_was_common is not None:
            if last_transition_was_common:
                q_net[last_action_1] += stick_common
            else:
                q_net[last_action_1] += stick_rare
            
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Policy ---
        state_idx = state[trial]
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Determine transition type for next trial's stickiness
        # Common: (Action 0 -> Planet 0) or (Action 1 -> Planet 1)
        # Rare: (Action 0 -> Planet 1) or (Action 1 -> Planet 0)
        current_transition_common = (action_1[trial] == state[trial])
        
        last_action_1 = action_1[trial]
        last_transition_was_common = current_transition_common

    eps = 1e-10
    mask = (p_choice_1 > 0) & (p_choice_2 > 0)
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss
```

### Model 3: Asymmetric Learning with Global Decay
This model combines asymmetric learning rates (separate rates for positive and negative prediction errors) with a global decay mechanism. The decay accounts for the non-stationary environment by gradually forgetting old values, while the asymmetric learning rates allow the participant to react differently to rewards (reinforcement) versus lack of rewards (punishment/omission), providing a flexible mechanism to track changing probabilities.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Asymmetric Learning with Global Decay.
    
    Combines 'Forgetting' (Decay) with Asymmetric Learning Rates.
    1. All Stage 2 Q-values decay towards 0 at the start of each trial.
    2. Updates use 'lr_pos' if the prediction error is positive (better than expected)
       and 'lr_neg' if negative (worse than expected).
    
    Parameters:
    - lr_pos: [0, 1] Learning rate for positive prediction errors.
    - lr_neg: [0, 1] Learning rate for negative prediction errors.
    - beta_1: [0, 10] Stage 1 inverse temperature.
    - beta_2: [0, 10] Stage 2 inverse temperature.
    - w: [0, 1] MB/MF weight.
    - stickiness: [0, 5] Choice stickiness.
    - decay: [0, 1] Decay rate for Stage 2 values.
    """
    lr_pos, lr_neg, beta_1, beta_2, w, stickiness, decay = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1 or reward[trial] == -1:
            last_action_1 = -1
            continue

        # --- Decay ---
        # Decay all stage 2 values towards 0 (representing uncertainty/forgetting)
        q_stage2_mf *= (1 - decay)

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Policy ---
        state_idx = state[trial]
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        # Update Stage 1 MF with Asymmetric LR
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        lr_1 = lr_pos if delta_stage1 > 0 else lr_neg
        q_stage1_mf[action_1[trial]] += lr_1 * delta_stage1
        
        # Update Stage 2 MF with Asymmetric LR
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        lr_2 = lr_pos if delta_stage2 > 0 else lr_neg
        q_stage2_mf[state_idx, action_2[trial]] += lr_2 * delta_stage2
        
        last_action_1 = action_1[trial]

    eps = 1e-10
    mask = (p_choice_1 > 0) & (p_choice_2 > 0)
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss
```