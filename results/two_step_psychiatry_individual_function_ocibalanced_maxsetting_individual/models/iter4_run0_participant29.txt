Here are the three proposed cognitive models.

### Model 1: Model-Free Q-Learning with Forgetting and Perseveration
This model assumes the participant relies on a Model-Free strategy but, unlike standard Q-learning, values of unchosen options decay (forgetting) over time. This captures the possibility that the participant becomes stuck on one option because they "forget" the potential value of the alternative, combined with a tendency to repeat choices (perseveration).

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Q-Learning with Forgetting and Perseveration.
    Unchosen Stage 1 options decay towards 0, representing forgetting.
    
    Parameters:
    - learning_rate: [0, 1] Rate of updating Q-values from reward/prediction error.
    - beta: [0, 10] Inverse temperature for softmax choice.
    - forget_rate: [0, 1] Rate at which unchosen Q-values decay (0 = no forgetting, 1 = instant reset).
    - p: [0, 5] Perseveration bonus added to the previous Stage 1 choice.
    """
    learning_rate, beta, forget_rate, p = model_parameters
    n_trials = len(action_1)
    
    # Initialize Q-values
    q_stage1_mf = np.zeros(2)      # Values for Spaceships A(0) and B(1)
    q_stage2_mf = np.zeros((2, 2)) # Values for Aliens: [Planet][Alien]
    
    log_loss = 0.0
    prev_choice_1 = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        q_net = q_stage1_mf.copy()
        
        # Add perseveration bonus
        if prev_choice_1 != -1:
            q_net[prev_choice_1] += p
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        # Accumulate Log Loss
        log_loss -= np.log(probs_1[action_1[trial]] + 1e-10)
        
        # Record choice for next trial's perseveration
        prev_choice_1 = action_1[trial]
        
        # --- Stage 2 Choice ---
        state_idx = state[trial] # Current Planet (0 or 1)
        q_s2 = q_stage2_mf[state_idx]
        
        exp_q2 = np.exp(beta * q_s2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        log_loss -= np.log(probs_2[action_2[trial]] + 1e-10)
        
        # --- Learning ---
        # 1. Update Stage 2 Q-values (Standard Q-Learning)
        # Prediction error for Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # 2. Update Stage 1 Q-values
        # SARSA-style update: Target is the value of the state-action pair actually visited in Stage 2
        target_stage1 = q_stage2_mf[state_idx, action_2[trial]]
        delta_stage1 = target_stage1 - q_stage1_mf[action_1[trial]]
        
        # Update chosen option
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Decay unchosen option
        unchosen_action = 1 - action_1[trial]
        q_stage1_mf[unchosen_action] *= (1.0 - forget_rate)

    return log_loss
```

### Model 2: Dual-Beta Hybrid Model with Perseveration
This model posits that the participant uses both Model-Based (planning) and Model-Free (habitual) systems, but assigns them independent weights (`beta_mb` and `beta_mf`) rather than a mixing parameter `w`. This allows for cases where one system might be much "noisier" or more dominant without constraining their sum.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Dual-Beta Hybrid Model.
    Uses separate inverse temperatures for Model-Based and Model-Free values in Stage 1.
    
    Parameters:
    - learning_rate: [0, 1] Rate of updating MF Q-values.
    - beta_mb: [0, 10] Inverse temperature for Model-Based values.
    - beta_mf: [0, 10] Inverse temperature for Model-Free values.
    - p: [0, 5] Perseveration bonus.
    """
    learning_rate, beta_mb, beta_mf, p = model_parameters
    n_trials = len(action_1)
    
    # Fixed transition matrix as per task structure: 
    # A(0)->X(0) is 0.7, A(0)->Y(1) is 0.3
    # U(1)->X(0) is 0.3, U(1)->Y(1) is 0.7
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    log_loss = 0.0
    prev_choice_1 = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Calculate Model-Based values: Transition * Max(Q_Stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1) # Max value for each planet
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combine values with separate betas
        logits = (beta_mb * q_stage1_mb) + (beta_mf * q_stage1_mf)
        
        # Add perseveration
        if prev_choice_1 != -1:
            logits[prev_choice_1] += p
            
        exp_logits = np.exp(logits)
        probs_1 = exp_logits / np.sum(exp_logits)
        
        log_loss -= np.log(probs_1[action_1[trial]] + 1e-10)
        prev_choice_1 = action_1[trial]
        
        # --- Stage 2 Choice ---
        # Stage 2 is purely Model-Free (choosing alien)
        state_idx = state[trial]
        q_s2 = q_stage2_mf[state_idx]
        
        # Use beta_mf for stage 2 choice as well
        exp_q2 = np.exp(beta_mf * q_s2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        log_loss -= np.log(probs_2[action_2[trial]] + 1e-10)
        
        # --- Learning ---
        # Update Stage 2 MF values
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Update Stage 1 MF values (SARSA)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1

    return log_loss
```

### Model 3: Asymmetric TD(lambda) with Perseveration
This model extends the Model-Free TD(lambda) approach by allowing different learning rates for positive prediction errors (better than expected) and negative prediction errors (worse than expected). This accounts for potential biases in how the participant processes wins versus losses/omissions.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Asymmetric TD(lambda) Learning with Perseveration.
    Uses different learning rates for positive and negative prediction errors.
    
    Parameters:
    - alpha_pos: [0, 1] Learning rate for positive prediction errors (delta > 0).
    - alpha_neg: [0, 1] Learning rate for negative prediction errors (delta <= 0).
    - beta: [0, 10] Inverse temperature.
    - lam: [0, 1] Eligibility trace parameter (0 = TD(0), 1 = Monte Carlo).
    - p: [0, 5] Perseveration bonus.
    """
    alpha_pos, alpha_neg, beta, lam, p = model_parameters
    n_trials = len(action_1)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    log_loss = 0.0
    prev_choice_1 = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        q_net = q_stage1_mf.copy()
        
        if prev_choice_1 != -1:
            q_net[prev_choice_1] += p
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        log_loss -= np.log(probs_1[action_1[trial]] + 1e-10)
        prev_choice_1 = action_1[trial]
        
        # --- Stage 2 Choice ---
        state_idx = state[trial]
        q_s2 = q_stage2_mf[state_idx]
        
        exp_q2 = np.exp(beta * q_s2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        log_loss -= np.log(probs_2[action_2[trial]] + 1e-10)
        
        # --- Learning ---
        # 1. Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Select alpha based on sign of delta_stage2
        eff_alpha2 = alpha_pos if delta_stage2 > 0 else alpha_neg
        q_stage2_mf[state_idx, action_2[trial]] += eff_alpha2 * delta_stage2
        
        # 2. Stage 1 Update
        # TD error at stage 1 (using updated stage 2 value as proxy for V(s'))
        target_val = q_stage2_mf[state_idx, action_2[trial]]
        delta_stage1 = target_val - q_stage1_mf[action_1[trial]]
        
        # Apply eligibility trace update
        # We apply specific alphas to the specific error terms
        term1_alpha = alpha_pos if delta_stage1 > 0 else alpha_neg
        term2_alpha = alpha_pos if delta_stage2 > 0 else alpha_neg
        
        # Update combines direct error (delta1) and trace error (delta2 * lam)
        update_val = (term1_alpha * delta_stage1) + (term2_alpha * lam * delta_stage2)
        
        q_stage1_mf[action_1[trial]] += update_val

    return log_loss
```