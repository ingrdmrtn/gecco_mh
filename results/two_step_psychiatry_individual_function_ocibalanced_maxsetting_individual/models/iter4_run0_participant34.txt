Here are three new cognitive models for the two-step task, formulated as Python functions.

### Model 1: Hybrid Model with Subjective Transition Belief
This model tests the hypothesis that the participantâ€™s Model-Based system relies on a **subjective belief** about the transition probabilities (`p_common`) rather than the objective ones (0.7/0.3). This allows the model to capture behavior where the participant might mistrust the "common" transition or over-interpret rare transitions.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Subjective Transition Belief.
    
    The participant uses a model-based strategy but with a learned or subjective 
    belief about the transition probabilities (p_common) which may differ from 
    the true probabilities (0.7).
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for Q-value updates.
    - beta_1: [0, 10] Inverse temperature for Stage 1 choice.
    - beta_2: [0, 10] Inverse temperature for Stage 2 choice.
    - w: [0, 1] Weight for Model-Based control (0=MF, 1=MB).
    - stickiness: [0, 5] Choice perseveration bonus.
    - p_common: [0, 1] Subjective probability of the common transition.
    """
    learning_rate, beta_1, beta_2, w, stickiness, p_common = model_parameters
    n_trials = len(action_1)
    
    # Subjective transition matrix based on parameter p_common
    # Row 0: Space 0 -> [Planet 0, Planet 1]
    # Row 1: Space 1 -> [Planet 0, Planet 1]
    # Assuming symmetry: Space 0->Planet 0 is common (p), Space 1->Planet 1 is common (p)
    transition_matrix = np.array([[p_common, 1.0 - p_common], 
                                  [1.0 - p_common, p_common]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State (Planet) x Action (Alien)
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # Model-Based Value: Expected max value of Stage 2 weighted by subjective transitions
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Apply Stickiness
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        # Softmax Policy Stage 1
        q_net_stable = q_net - np.max(q_net)
        exp_q1 = np.exp(beta_1 * q_net_stable)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        s_next = state[trial]
        last_action_1 = a1
        
        # --- Stage 2 Decision ---
        q_s2_curr = q_stage2_mf[s_next]
        q_s2_stable = q_s2_curr - np.max(q_s2_curr)
        exp_q2 = np.exp(beta_2 * q_s2_stable)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]
        
        # --- Learning ---
        # Stage 1 MF Update (TD(1) logic: updated by Stage 2 reward directly? 
        # Standard hybrid uses TD(0) from stage 2 value or TD(1) from reward.
        # Here we use TD(1) consistent with standard analyses of this task)
        # However, to be consistent with the template's placeholders, we check the logic.
        # Template had: delta_stage1 = q_stage2_mf[...] - q_stage1_mf[...] (TD0)
        # But many Hybrid implementations use TD(1). Let's use TD(1) as it fits simple MF better.
        # delta = r - q_stage1_mf[a1]
        
        # Let's use the standard TD(1) update for the MF component in Hybrid models
        delta_stage1 = r - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[s_next, a2]
        q_stage2_mf[s_next, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model with Uncertainty Sensitivity (Risk)
This model tracks not just the expected value (mean Q) of the aliens but also the **variance** of the rewards. The participant's decision in Stage 2 (and consequently the MB value for Stage 1) is influenced by a risk parameter, effectively penalizing or rewarding uncertainty.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Uncertainty Sensitivity.
    
    Tracks both the Q-values and the variance of rewards for the aliens.
    The effective value used for decisions is Q + risk_param * sqrt(Variance).
    This allows the model to capture risk-averse or risk-seeking behavior.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for Q-values and Variance.
    - beta_1: [0, 10] Inverse temperature Stage 1.
    - beta_2: [0, 10] Inverse temperature Stage 2.
    - w: [0, 1] Model-Based weight.
    - stickiness: [0, 5] Choice perseveration.
    - risk_param: [-5, 5] Weight of uncertainty (positive=seeking, negative=averse).
    """
    learning_rate, beta_1, beta_2, w, stickiness, risk_param = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    var_stage2 = np.zeros((2, 2)) # Variance tracking for Stage 2
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        
        # --- Stage 1 Decision ---
        # Calculate Effective Stage 2 Values (Mean + Risk * StdDev)
        # We use these effective values for the Model-Based calculation
        eff_q_stage2 = q_stage2_mf + risk_param * np.sqrt(var_stage2)
        
        max_q_stage2 = np.max(eff_q_stage2, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        q_net_stable = q_net - np.max(q_net)
        exp_q1 = np.exp(beta_1 * q_net_stable)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        s_next = state[trial]
        last_action_1 = a1
        
        # --- Stage 2 Decision ---
        # Use effective values for choice
        q_s2_curr = eff_q_stage2[s_next]
        q_s2_stable = q_s2_curr - np.max(q_s2_curr)
        exp_q2 = np.exp(beta_2 * q_s2_stable)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]
        
        # --- Learning ---
        # Update Stage 1 MF (TD1)
        delta_stage1 = r - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Update Stage 2 Mean and Variance
        q_val = q_stage2_mf[s_next, a2]
        delta_stage2 = r - q_val
        
        # Update Variance: V <- V + lr * ((r - Q)^2 - V)
        # Note: We use the old Q for the squared error, or updated? Standard is old Q.
        sq_error = delta_stage2 ** 2
        var_stage2[s_next, a2] += learning_rate * (sq_error - var_stage2[s_next, a2])
        
        # Update Mean
        q_stage2_mf[s_next, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model with Pearce-Hall Dynamic Learning Rate
This model incorporates a dynamic learning rate inspired by the Pearce-Hall theory. The learning rate for the aliens (Stage 2) is not fixed but increases when the prediction error is high (surprise) and decays when the prediction error is low. This fits the slowly drifting reward probabilities of the aliens.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Pearce-Hall Dynamic Learning Rate.
    
    The learning rate for Stage 2 (Aliens) adapts over time based on the 
    magnitude of the prediction error. This allows the model to become more 
    plastic when outcomes are surprising.
    
    Parameters:
    - init_alpha: [0, 1] Initial learning rate.
    - decay_alpha: [0, 1] Decay rate (gamma) for the learning rate update.
    - beta_1: [0, 10] Inverse temperature Stage 1.
    - beta_2: [0, 10] Inverse temperature Stage 2.
    - w: [0, 1] Model-Based weight.
    - stickiness: [0, 5] Choice perseveration.
    """
    init_alpha, decay_alpha, beta_1, beta_2, w, stickiness = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Dynamic learning rate (scalar, applied globally or per state - here global for parsimony)
    current_alpha = init_alpha
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        q_net_stable = q_net - np.max(q_net)
        exp_q1 = np.exp(beta_1 * q_net_stable)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        s_next = state[trial]
        last_action_1 = a1
        
        # --- Stage 2 Decision ---
        q_s2_curr = q_stage2_mf[s_next]
        q_s2_stable = q_s2_curr - np.max(q_s2_curr)
        exp_q2 = np.exp(beta_2 * q_s2_stable)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]
        
        # --- Learning ---
        # Update Stage 1 MF (using current dynamic alpha or fixed? 
        # Usually PH applies to the specific cue. We'll apply it to both stages for consistency)
        delta_stage1 = r - q_stage1_mf[a1]
        q_stage1_mf[a1] += current_alpha * delta_stage1
        
        # Update Stage 2
        delta_stage2 = r - q_stage2_mf[s_next, a2]
        q_stage2_mf[s_next, a2] += current_alpha * delta_stage2
        
        # --- Update Learning Rate (Pearce-Hall) ---
        # alpha[t+1] = gamma * |delta| + (1-gamma) * alpha[t]
        # We use the Stage 2 PE as the primary driver of surprise
        current_alpha = decay_alpha * np.abs(delta_stage2) + (1 - decay_alpha) * current_alpha
        
        # Clip alpha to [0, 1] to ensure stability
        current_alpha = np.clip(current_alpha, 0, 1)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```