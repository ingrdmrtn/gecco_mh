Here are three new cognitive models based on the participant data and feedback.

### Model 1: Choice Kernel Model
This model replaces simple one-step perseveration with a **Choice Kernel** mechanism. The participant data shows "blocky" behavior where they persist with a spaceship for many trials (e.g., trials 1-15, 57-79). A Choice Kernel accumulates a trace of past choices that decays slowly, modeling a stronger, history-dependent habit formation than a simple "repeat last choice" parameter.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Choice Kernel Model.
    
    Replaces simple one-step perseveration with a Choice Kernel (CK) that tracks 
    the history of Stage 1 choices. This accounts for 'streaky' behavior 
    (blocks of choosing the same spaceship) via a decaying trace of past actions.
    
    Parameters:
    lr: Learning rate [0,1]
    beta_1: Inverse temperature for Stage 1 [0,10]
    beta_2: Inverse temperature for Stage 2 [0,10]
    w: Weight mixing Model-Based and Model-Free values [0,1]
    lam: Eligibility trace parameter [0,1]
    alpha_k: Decay rate of the choice kernel (1 = instant forgetting, 0 = no forgetting) [0,1]
    beta_k: Weight of the choice kernel (habit strength) in decision making [0,10]
    """
    lr, beta_1, beta_2, w, lam, alpha_k, beta_k = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Choice kernel for Stage 1 actions (0 and 1)
    choice_kernel = np.zeros(2)
    
    for trial in range(n_trials):
        # -- Stage 1 Policy --
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add choice kernel bonus to the net values
        q_net_stage1 += beta_k * choice_kernel
        
        exp_q1 = np.exp(beta_1 * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        s1_choice = action_1[trial]
        state_idx = state[trial]
        
        # Update Choice Kernel: Decay all, reinforce chosen
        choice_kernel = choice_kernel * (1 - alpha_k)
        choice_kernel[s1_choice] += alpha_k
        
        # -- Stage 2 Policy --
        s2_choice = action_2[trial]
        r = reward[trial]
        
        # Handle potential missing data (e.g. trial 1 with action -1)
        if s2_choice != -1:
            qs_stage2 = q_stage2_mf[state_idx]
            exp_q2 = np.exp(beta_2 * qs_stage2)
            probs_2 = exp_q2 / np.sum(exp_q2)
            p_choice_2[trial] = probs_2[s2_choice]
            
            # -- Updates --
            delta_stage1 = q_stage2_mf[state_idx, s2_choice] - q_stage1_mf[s1_choice]
            q_stage1_mf[s1_choice] += lr * delta_stage1
            
            delta_stage2 = r - q_stage2_mf[state_idx, s2_choice]
            q_stage2_mf[state_idx, s2_choice] += lr * delta_stage2
            
            # Eligibility trace update for Stage 1
            q_stage1_mf[s1_choice] += lr * lam * delta_stage2
        else:
            p_choice_2[trial] = 1.0 

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Dual Learning Rate Decay Model
This model hypothesizes that the participant learns about spaceship reliability (Stage 1) and alien generosity (Stage 2) at different rates. For instance, alien probabilities might be perceived as volatile (high `lr_2`), while spaceship transitions are stable (low `lr_1`). It retains the `decay` mechanism from your best model to handle the slow changes in alien reward probabilities.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Dual Learning Rate Decay Model.
    
    Differentiates between the plasticity of spaceship preferences (Stage 1) and 
    alien preferences (Stage 2) using separate learning rates. Also includes 
    decay/forgetting to handle non-stationarity.
    
    Parameters:
    lr_1: Learning rate for Stage 1 [0,1]
    lr_2: Learning rate for Stage 2 [0,1]
    beta_1: Inverse temperature for Stage 1 [0,10]
    beta_2: Inverse temperature for Stage 2 [0,10]
    w: Weight mixing Model-Based and Model-Free values [0,1]
    p: Perseveration (stickiness) to previous Stage 1 choice [0,1]
    lam: Eligibility trace parameter [0,1]
    decay: Decay rate of Q-values toward 0.5 [0,1]
    """
    lr_1, lr_2, beta_1, beta_2, w, p, lam, decay = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1
    
    for trial in range(n_trials):

        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if prev_action_1 != -1:
            q_net_stage1[prev_action_1] += p
            
        exp_q1 = np.exp(beta_1 * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        s1_choice = action_1[trial]
        state_idx = state[trial]
        prev_action_1 = s1_choice

        s2_choice = action_2[trial]
        r = reward[trial]

        if s2_choice != -1:
            qs_stage2 = q_stage2_mf[state_idx]
            exp_q2 = np.exp(beta_2 * qs_stage2)
            probs_2 = exp_q2 / np.sum(exp_q2)
            p_choice_2[trial] = probs_2[s2_choice]
            
            # Updates
            # Use lr_1 for Stage 1 updates
            delta_stage1 = q_stage2_mf[state_idx, s2_choice] - q_stage1_mf[s1_choice]
            q_stage1_mf[s1_choice] += lr_1 * delta_stage1
            
            # Use lr_2 for Stage 2 updates
            delta_stage2 = r - q_stage2_mf[state_idx, s2_choice]
            q_stage2_mf[state_idx, s2_choice] += lr_2 * delta_stage2
            
            # Eligibility trace: Driven by Stage 2 error, but updates Stage 1 value.
            # Using lr_1 as it updates the Stage 1 weight.
            q_stage1_mf[s1_choice] += lr_1 * lam * delta_stage2
        else:
            p_choice_2[trial] = 1.0

        # Decay Q-values toward 0.5 (uncertainty)
        q_stage1_mf = q_stage1_mf * (1 - decay) + 0.5 * decay
        q_stage2_mf = q_stage2_mf * (1 - decay) + 0.5 * decay

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Asymmetric Learning Decay Model
This model combines the successful `decay` mechanism with **Asymmetric Learning Rates** (`lr_pos`, `lr_neg`). This allows the model to treat positive prediction errors (better than expected) and negative prediction errors (worse than expected) differently, capturing biases like optimism or pessimism which may drive the persistent behavior seen in the data.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Asymmetric Learning Decay Model.
    
    Incorporates separate learning rates for positive and negative prediction errors,
    allowing for optimism or pessimism bias. Also includes decay and perseveration.
    
    Parameters:
    lr_pos: Learning rate for positive prediction errors [0,1]
    lr_neg: Learning rate for negative prediction errors [0,1]
    beta_1: Inverse temperature for Stage 1 [0,10]
    beta_2: Inverse temperature for Stage 2 [0,10]
    w: Weight mixing Model-Based and Model-Free values [0,1]
    p: Perseveration (stickiness) to previous Stage 1 choice [0,1]
    lam: Eligibility trace parameter [0,1]
    decay: Decay rate of Q-values toward 0.5 [0,1]
    """
    lr_pos, lr_neg, beta_1, beta_2, w, p, lam, decay = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1
    
    for trial in range(n_trials):

        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if prev_action_1 != -1:
            q_net_stage1[prev_action_1] += p
            
        exp_q1 = np.exp(beta_1 * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        s1_choice = action_1[trial]
        state_idx = state[trial]
        prev_action_1 = s1_choice
        
        s2_choice = action_2[trial]
        r = reward[trial]

        if s2_choice != -1:
            qs_stage2 = q_stage2_mf[state_idx]
            exp_q2 = np.exp(beta_2 * qs_stage2)
            probs_2 = exp_q2 / np.sum(exp_q2)
            p_choice_2[trial] = probs_2[s2_choice]
            
            # Updates with asymmetric LR
            delta_stage1 = q_stage2_mf[state_idx, s2_choice] - q_stage1_mf[s1_choice]
            lr_s1 = lr_pos if delta_stage1 > 0 else lr_neg
            q_stage1_mf[s1_choice] += lr_s1 * delta_stage1
            
            delta_stage2 = r - q_stage2_mf[state_idx, s2_choice]
            lr_s2 = lr_pos if delta_stage2 > 0 else lr_neg
            q_stage2_mf[state_idx, s2_choice] += lr_s2 * delta_stage2
            
            # Eligibility trace
            q_stage1_mf[s1_choice] += lr_s2 * lam * delta_stage2
        else:
            p_choice_2[trial] = 1.0

        # Decay
        q_stage1_mf = q_stage1_mf * (1 - decay) + 0.5 * decay
        q_stage2_mf = q_stage2_mf * (1 - decay) + 0.5 * decay

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```