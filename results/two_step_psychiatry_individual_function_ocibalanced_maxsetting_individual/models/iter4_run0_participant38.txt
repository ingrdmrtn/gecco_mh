Here are the three proposed cognitive models.

### Model 1: Asymmetric TD($\lambda$) Model
This model relies on Model-Free Reinforcement Learning but distinguishes between positive and negative prediction errors (Asymmetric Learning Rates). It uses an eligibility trace ($\lambda$) to update the first-stage values based on the second-stage reward, allowing for temporal credit assignment without a full Model-Based map.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Asymmetric TD(lambda) Model.
    
    This model assumes the participant learns purely through Model-Free TD errors
    but updates values differently depending on whether the prediction error is 
    positive (better than expected) or negative (worse than expected).
    It uses an eligibility trace (lambda) to propagate the second-stage outcome 
    back to the first-stage choice.

    Parameters:
    - lr_pos: [0, 1] Learning rate for positive prediction errors.
    - lr_neg: [0, 1] Learning rate for negative prediction errors.
    - beta_1: [0, 10] Inverse temperature for Stage 1 choices.
    - beta_2: [0, 10] Inverse temperature for Stage 2 choices.
    - lam: [0, 1] Eligibility trace decay (lambda). Controls how much Stage 2 reward affects Stage 1 value.
    """
    lr_pos, lr_neg, beta_1, beta_2, lam = model_parameters
    n_trials = len(action_1)

    # Initialize Q-values
    # Stage 1: 2 spaceships
    q_stage1 = np.zeros(2) 
    # Stage 2: 2 planets x 2 aliens
    q_stage2 = np.zeros((2, 2)) 

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # Skip invalid trials
        if action_1[trial] == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta_1 * q_stage1)
        # Safety for overflow
        if np.any(np.isinf(exp_q1)):
            max_val = np.max(beta_1 * q_stage1)
            exp_q1 = np.exp(beta_1 * q_stage1 - max_val)
            
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        a1 = action_1[trial]
        s2 = state[trial] # Planet arrived at

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta_2 * q_stage2[s2])
        if np.any(np.isinf(exp_q2)):
            max_val = np.max(beta_2 * q_stage2[s2])
            exp_q2 = np.exp(beta_2 * q_stage2[s2] - max_val)
            
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        a2 = action_2[trial]
        r = reward[trial]

        # --- Updates ---
        # 1. Prediction Error for Stage 1 (TD(0) part): Q2 - Q1
        delta_1 = q_stage2[s2, a2] - q_stage1[a1]
        lr_1 = lr_pos if delta_1 > 0 else lr_neg
        q_stage1[a1] += lr_1 * delta_1

        # 2. Prediction Error for Stage 2: R - Q2
        delta_2 = r - q_stage2[s2, a2]
        lr_2 = lr_pos if delta_2 > 0 else lr_neg
        q_stage2[s2, a2] += lr_2 * delta_2

        # 3. Eligibility Trace Update for Stage 1: Propagate delta_2 back
        # We use the learning rate associated with the second stage error
        q_stage1[a1] += lr_2 * lam * delta_2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model with Unchosen Value Decay
This model combines Model-Based and Model-Free learning (Hybrid). Uniquely, it incorporates a **decay** parameter for the unchosen aliens in Stage 2. Since the alien reward probabilities drift over time, the value of an option not sampled recently becomes uncertain or reverts to a baseline (0.5).

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Unchosen Value Decay.
    
    Combines Model-Based (MB) and Model-Free (MF) valuation.
    Includes a decay mechanism for Stage 2 values: the value of the unchosen
    alien decays towards 0.5 (neutral) on every trial, helping the agent 
    adapt to the drifting probabilities of the task.

    Parameters:
    - lr: [0, 1] Learning rate for MF value updates.
    - beta_1: [0, 10] Inverse temperature for Stage 1 choices.
    - beta_2: [0, 10] Inverse temperature for Stage 2 choices.
    - w: [0, 1] Mixing weight. 0 = Pure MF, 1 = Pure MB.
    - decay: [0, 1] Decay rate for unchosen Stage 2 options towards 0.5.
    """
    lr, beta_1, beta_2, w, decay = model_parameters
    n_trials = len(action_1)

    # Fixed transition matrix for MB (Common=0.7, Rare=0.3)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    # MF Q-values
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    
    # Initialize Stage 2 values to 0.5 (neutral expectation)
    q_stage2_mf.fill(0.5)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        if action_1[trial] == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        # --- Stage 1 Choice ---
        # MB Value: Expected max value of next stage weighted by transition probs
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2

        # Net Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        a1 = action_1[trial]
        s2 = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta_2 * q_stage2_mf[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        a2 = action_2[trial]
        r = reward[trial]

        # --- Updates ---
        # Stage 2 Update (Chosen)
        delta_2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += lr * delta_2
        
        # Stage 2 Decay (Unchosen)
        unchosen_a2 = 1 - a2
        q_stage2_mf[s2, unchosen_a2] = (1 - decay) * q_stage2_mf[s2, unchosen_a2] + decay * 0.5

        # Stage 1 MF Update (TD(0))
        # Note: Standard hybrid often uses TD(0) for the MF component
        delta_1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Dynamic Transition Hybrid with Stickiness
This model builds on the previous best model (Dynamic Transition Hybrid) by adding a **Stickiness** (perseveration) parameter to the Stage 1 choice. The participant data exhibits significant streaks of repeating the same spaceship choice. This model captures that behavioral inertia while simultaneously learning the transition matrix online.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Dynamic Transition Hybrid Model with Stickiness.
    
    This model learns the transition matrix online (Dynamic Transition) and mixes 
    Model-Based and Model-Free values. Additionally, it includes a 'stickiness' 
    parameter for the first stage, biasing the agent to repeat the previously 
    chosen spaceship.

    Parameters:
    - lr_val: [0, 1] Learning rate for Stage 2 alien values (Q-values).
    - lr_trans: [0, 1] Learning rate for updating transition probabilities.
    - beta_1: [0, 10] Inverse temperature for Stage 1 choices.
    - beta_2: [0, 10] Inverse temperature for Stage 2 choices.
    - w: [0, 1] Mixing weight. 0 = Pure MF, 1 = Pure MB.
    - stick: [0, 10] Choice stickiness bonus added to the logits of the previously chosen spaceship.
    """
    lr_val, lr_trans, beta_1, beta_2, w, stick = model_parameters
    n_trials = len(action_1)

    # Initialize learned transition matrix to flat priors (0.5)
    transition_matrix = np.ones((2, 2)) * 0.5

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_a1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            last_a1 = -1
            continue

        # --- Stage 1 Choice ---
        # MB Calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2

        # Net Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Calculate Logits
        logits_1 = beta_1 * q_net
        
        # Add Stickiness
        if last_a1 != -1:
            logits_1[last_a1] += stick

        # Softmax
        exp_q1 = np.exp(logits_1)
        # Safety for overflow
        if np.any(np.isinf(exp_q1)):
            max_val = np.max(logits_1)
            exp_q1 = np.exp(logits_1 - max_val)
            
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        a1 = action_1[trial]
        last_a1 = a1 # Update last action
        s2 = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta_2 * q_stage2_mf[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        a2 = action_2[trial]
        r = reward[trial]

        # --- Updates ---
        # 1. Stage 2 Value Update
        delta_stage2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += lr_val * delta_stage2

        # 2. Stage 1 MF Value Update
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr_val * delta_stage1

        # 3. Transition Matrix Update
        # Create a target vector (e.g., if s2=1, target=[0, 1])
        target = np.zeros(2)
        target[s2] = 1.0
        # Update the row corresponding to the chosen spaceship
        transition_matrix[a1] = transition_matrix[a1] + lr_trans * (target - transition_matrix[a1])

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```