Here are the three proposed cognitive models.

### Model 1: Hybrid Model with Dynamic Transition Learning
This model extends the standard hybrid (Model-Based + Model-Free) approach by assuming the participant is not sure about the transition probabilities (spaceship to planet) and learns them online. This captures the possibility that the participant believes the "rare" transitions might be changing in frequency, or simply learns the structure from scratch.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Dynamic Transition Learning.
    
    This model combines Model-Based (MB) and Model-Free (MF) reinforcement learning.
    Crucially, it does not assume fixed transition probabilities. Instead, it learns
    the transition matrix P(State|Action) online based on experienced transitions.
    
    Parameters:
    - alpha: [0,1] Learning rate for Q-values (both stages).
    - eta: [0,1] Learning rate for transition probabilities.
    - beta: [0,10] Inverse temperature (softmax sensitivity).
    - w: [0,1] Weight of Model-Based system (0=Pure MF, 1=Pure MB).
    - p: [0,5] Perseverance (stickiness) to the previous choice.
    """
    alpha, eta, beta, w, p = model_parameters
    n_trials = len(action_1)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    # q_mf[s, a] : Value of alien a in planet s
    q_mf = np.zeros((2, 2)) 
    # q_mf1[a] : Model-free value of spaceship a
    q_mf1 = np.zeros(2)     
    
    # Transition probabilities: [P(Planet X | Spaceship A), P(Planet X | Spaceship U)]
    # Initialize based on common structure (A->X, U->Y implies P(X|U) is low)
    # We start with priors approximating the true structure (0.7, 0.3)
    trans_probs = np.array([0.7, 0.3]) 
    
    prev_a1 = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        
        # Calculate Model-Based values
        # V(X) = max(Q(X, :)), V(Y) = max(Q(Y, :))
        max_q2 = np.max(q_mf, axis=1) 
        
        # Q_MB(A) = P(X|A)*V(X) + P(Y|A)*V(Y)
        # Q_MB(U) = P(X|U)*V(X) + P(Y|U)*V(Y)
        q_mb1 = np.zeros(2)
        q_mb1[0] = trans_probs[0] * max_q2[0] + (1 - trans_probs[0]) * max_q2[1]
        q_mb1[1] = trans_probs[1] * max_q2[0] + (1 - trans_probs[1]) * max_q2[1]
        
        # Net Value mixing MB and MF
        q_net = w * q_mb1 + (1 - w) * q_mf1
        
        # Logits with perseverance
        logits = beta * q_net
        if prev_a1 != -1:
            logits[prev_a1] += p
            
        exp_q1 = np.exp(logits)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        s_curr = state[trial] # 0 for Planet X, 1 for Planet Y
        
        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_mf[s_curr])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]
        
        # --- Updates ---
        
        # 1. Update Transition Probabilities
        # If we are in state 0 (X), target is 1. If state 1 (Y), target is 0.
        is_state_0 = 1.0 if s_curr == 0 else 0.0
        trans_probs[a1] += eta * (is_state_0 - trans_probs[a1])
        
        # 2. Stage 2 Update (MF)
        delta2 = r - q_mf[s_curr, a2]
        q_mf[s_curr, a2] += alpha * delta2
        
        # 3. Stage 1 Update (MF) - using SARSA(0) logic driven by Stage 2 Q-value
        delta1 = q_mf[s_curr, a2] - q_mf1[a1]
        q_mf1[a1] += alpha * delta1
        
        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Model-Free Q(lambda) with Separate Stage Learning Rates
This model hypothesizes that the participant learns at different rates for the abstract high-level choice (spaceship) versus the concrete low-level choice (alien). It uses eligibility traces ($\lambda$) to bridge the delay between the first choice and the final reward.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Q(lambda) with Separate Stage Learning Rates.
    
    This model applies Model-Free RL but distinguishes between the learning rate
    for the high-level choice (Spaceship) and the low-level choice (Alien).
    It uses eligibility traces (lambda) to credit Stage 1 for the outcome at Stage 2.
    
    Parameters:
    - alpha1: [0,1] Learning rate for Stage 1 (Spaceships).
    - alpha2: [0,1] Learning rate for Stage 2 (Aliens).
    - beta: [0,10] Inverse temperature.
    - lam: [0,1] Eligibility trace decay parameter (lambda).
    - p: [0,5] Perseverance parameter.
    """
    alpha1, alpha2, beta, lam, p = model_parameters
    n_trials = len(action_1)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    prev_a1 = -1
    
    for trial in range(n_trials):
        # Stage 1 Choice
        logits1 = beta * q_stage1
        if prev_a1 != -1:
            logits1[prev_a1] += p
            
        exp_q1 = np.exp(logits1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        s = state[trial]
        
        # Stage 2 Choice
        exp_q2 = np.exp(beta * q_stage2[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]
        
        # Updates
        
        # Prediction Error from Stage 1 to Stage 2
        # Use Q-value of chosen next state as proxy
        delta1 = q_stage2[s, a2] - q_stage1[a1]
        
        # Prediction Error from Stage 2 to Reward
        delta2 = r - q_stage2[s, a2]
        
        # Update Stage 2 Values
        q_stage2[s, a2] += alpha2 * delta2
        
        # Update Stage 1 Values
        # Accumulates immediate error (delta1) and discounted future error (lambda * delta2)
        q_stage1[a1] += alpha1 * (delta1 + lam * delta2)
        
        prev_a1 = a1
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Asymmetric Q-Learning with Choice Kernel
This model combines asymmetric learning (different sensitivity to positive vs. negative prediction errors) with a choice kernel. The asymmetry captures potential biases (e.g., optimism or pessimism) in value updating, while the kernel captures the strong habit formation observed in the participant's long streaks of repetition.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Asymmetric Q-Learning with Choice Kernel.
    
    This model assumes the participant learns differently from positive vs negative
    prediction errors (Asymmetric Learning). It also includes a Choice Kernel to 
    capture habit formation (tendency to repeat choices regardless of reward).
    
    Parameters:
    - alpha_pos: [0,1] Learning rate for positive prediction errors.
    - alpha_neg: [0,1] Learning rate for negative prediction errors.
    - beta: [0,10] Inverse temperature for Q-values.
    - alpha_k: [0,1] Decay rate for the choice kernel.
    - beta_k: [0,10] Weight of the choice kernel in decision making.
    """
    alpha_pos, alpha_neg, beta, alpha_k, beta_k = model_parameters
    n_trials = len(action_1)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    choice_kernel = np.zeros(2)
    
    for trial in range(n_trials):
        # Stage 1 Policy: Mix of Value and Habit
        logits = beta * q_stage1 + beta_k * choice_kernel
        exp_q1 = np.exp(logits)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        s = state[trial]
        
        # Stage 2 Policy: Value only
        exp_q2 = np.exp(beta * q_stage2[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]
        
        # Updates with Asymmetry
        
        # Stage 2 Update
        delta2 = r - q_stage2[s, a2]
        lr2 = alpha_pos if delta2 > 0 else alpha_neg
        q_stage2[s, a2] += lr2 * delta2
        
        # Stage 1 Update
        # TD(0) update towards Stage 2 Q-value
        target1 = q_stage2[s, a2]
        delta1 = target1 - q_stage1[a1]
        lr1 = alpha_pos if delta1 > 0 else alpha_neg
        q_stage1[a1] += lr1 * delta1
        
        # Choice Kernel Update (Habit)
        # kernel[chosen] moves to 1, kernel[unchosen] moves to 0
        choice_kernel[a1] = choice_kernel[a1] + alpha_k * (1.0 - choice_kernel[a1])
        choice_kernel[1-a1] = choice_kernel[1-a1] + alpha_k * (0.0 - choice_kernel[1-a1])
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```