Here are the 3 new cognitive models based on your requirements.

### Model 1: Pearce-Hall Hybrid (Dynamic Learning Rate)
This model implements a Pearce-Hall style attention mechanism for the second stage updates. The learning rate for the second stage is not static; instead, it is modulated by an "associability" term that tracks the absolute prediction error. This allows the agent to increase its learning rate when outcomes are surprising (high prediction error) and decrease it when outcomes are predictable, which is particularly useful in the drifting reward environment of this task.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Pearce-Hall Hybrid Model (Dynamic Learning Rate).
    
    Incorporates a dynamic learning rate (associability) for the second stage.
    The associability tracks the absolute prediction error, scaling the learning
    rate up when outcomes are surprising.
    
    Parameters:
    - base_lr: [0, 1] Base scaling factor for the learning rate.
    - beta: [0, 10] Inverse temperature for softmax.
    - w: [0, 1] Weighting between Model-Based (1) and Model-Free (0).
    - lambda_coef: [0, 1] Eligibility trace for Stage 1 updates.
    - stickiness: [0, 10] Choice perseveration bonus.
    - ph_decay: [0, 1] Decay rate of associability (1 - eta).
    - ph_scale: [0, 1] Weight of new prediction error in updating associability.
    """
    base_lr, beta, w, lambda_coef, stickiness, ph_decay, ph_scale = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    
    # Associability for Stage 2 (State x Action), initialized to 1.0 (high attention)
    associability = np.ones((2, 2)) 
    
    last_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits = beta * q_net
        if last_action_1 != -1:
            logits[last_action_1] += stickiness
            
        exp_q1 = np.exp(logits - np.max(logits))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_action_1 = action_1[trial]
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        q_s2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * q_s2 - np.max(beta * q_s2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning ---
        # Calculate Prediction Errors
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Dynamic Learning Rate for Stage 2
        # Current associability for the chosen state-action pair
        curr_assoc = associability[state_idx, action_2[trial]]
        dynamic_lr = base_lr * curr_assoc
        
        # Update Q-values
        # Stage 1 uses fixed base_lr for simplicity, or could use dynamic. 
        # Using base_lr * lambda for consistency with standard hybrid structure
        q_stage1_mf[action_1[trial]] += base_lr * (delta_stage1 + lambda_coef * delta_stage2)
        q_stage2_mf[state_idx, action_2[trial]] += dynamic_lr * delta_stage2
        
        # Update Associability (Pearce-Hall rule)
        # alpha_new = (1-eta)*alpha_old + eta*|delta|
        # Here ph_decay acts as (1-eta) and ph_scale acts as eta/weighting
        new_assoc = ph_decay * curr_assoc + ph_scale * np.abs(delta_stage2)
        associability[state_idx, action_2[trial]] = new_assoc

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Adaptive Transition Hybrid
This model relaxes the assumption that the participant knows the fixed transition probabilities (0.7/0.3). Instead, the agent learns the transition matrix $P(\text{State}|\text{Spaceship})$ trial-by-trial based on observed transitions. The Model-Based value calculation uses these learned probabilities, which allows the model to capture behavior where the participant might believe the "rare" transition has become common due to a streak of chance events.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Adaptive Transition Hybrid Model.
    
    The agent learns the transition matrix between Stage 1 and Stage 2
    instead of using fixed probabilities. The Model-Based values are derived
    from these learned transition probabilities.
    
    Parameters:
    - learning_rate: [0, 1] Q-value learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weighting between Model-Based (1) and Model-Free (0).
    - lambda_coef: [0, 1] Eligibility trace.
    - stickiness: [0, 10] Choice perseveration.
    - trans_lr: [0, 1] Learning rate for transition probabilities.
    """
    learning_rate, beta, w, lambda_coef, stickiness, trans_lr = model_parameters
    n_trials = len(action_1)
  
    # Initialize learned transition probabilities
    # p_trans[0] = P(State 0 | Action 0)
    # p_trans[1] = P(State 1 | Action 1)
    # Initialized to 0.5 (max uncertainty)
    p_trans = np.array([0.5, 0.5])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        # Construct current transition matrix from learned probs
        # T = [[P(S0|A0), P(S1|A0)], [P(S0|A1), P(S1|A1)]]
        curr_trans_matrix = np.array([
            [p_trans[0], 1.0 - p_trans[0]],
            [1.0 - p_trans[1], p_trans[1]]
        ])

        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = curr_trans_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits = beta * q_net
        if last_action_1 != -1:
            logits[last_action_1] += stickiness
            
        exp_q1 = np.exp(logits - np.max(logits))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_action_1 = action_1[trial]
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        q_s2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * q_s2 - np.max(beta * q_s2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]

        q_stage1_mf[action_1[trial]] += learning_rate * (delta_stage1 + lambda_coef * delta_stage2)
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Update Transition Probabilities
        # If action 0 was chosen, update P(S0|A0) towards 1 if state=0, else 0
        if action_1[trial] == 0:
            target = 1.0 if state[trial] == 0 else 0.0
            p_trans[0] += trans_lr * (target - p_trans[0])
        # If action 1 was chosen, update P(S1|A1) towards 1 if state=1, else 0
        elif action_1[trial] == 1:
            target = 1.0 if state[trial] == 1 else 0.0
            p_trans[1] += trans_lr * (target - p_trans[1])

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Win-Exploit Hybrid (Outcome-Dependent Beta)
This model proposes that the participant's level of exploration/exploitation (inverse temperature $\beta$) is not constant but depends on the previous outcome. If the previous trial was rewarded ("Win"), the agent adopts a specific $\beta_{win}$ (likely higher, more exploitative). If the previous trial was unrewarded ("Lose"), the agent adopts $\beta_{loss}$ (likely lower, more explorative). This captures a "Win-Exploit, Lose-Explore" meta-strategy.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Win-Exploit Hybrid Model (Outcome-Dependent Beta).
    
    The inverse temperature (beta) changes based on the previous trial's reward.
    This allows the agent to be more exploitative after a win and more 
    explorative (random) after a loss.
    
    Parameters:
    - learning_rate: [0, 1] Q-value learning rate.
    - beta_win: [0, 10] Beta used after a rewarded trial.
    - beta_loss: [0, 10] Beta used after an unrewarded trial (or first trial).
    - w: [0, 1] Weighting between Model-Based (1) and Model-Free (0).
    - lambda_coef: [0, 1] Eligibility trace.
    - stickiness: [0, 10] Choice perseveration.
    """
    learning_rate, beta_win, beta_loss, w, lambda_coef, stickiness = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    last_reward = 0 # Assume loss state for the first trial (or neutral)

    for trial in range(n_trials):
        # Determine current beta based on previous outcome
        current_beta = beta_win if last_reward == 1 else beta_loss

        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits = current_beta * q_net
        if last_action_1 != -1:
            logits[last_action_1] += stickiness
            
        exp_q1 = np.exp(logits - np.max(logits))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_action_1 = action_1[trial]
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        q_s2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(current_beta * q_s2 - np.max(current_beta * q_s2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]

        q_stage1_mf[action_1[trial]] += learning_rate * (delta_stage1 + lambda_coef * delta_stage2)
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        last_reward = reward[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```