Here are 3 new cognitive models for the two-step task, designed to capture the specific behaviors observed in the participant data (e.g., long streaks, potential filtering of rare transitions, and structural learning).

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Streak-Based Perseveration Model.

    This model extends standard perseveration by adding a "streak" bonus that
    increases linearly with the number of consecutive identical choices.
    This accounts for the participant's tendency to lock into a specific spaceship
    for long sequences (e.g., trials 46-62, 180-200), creating a momentum effect.

    Parameters:
    - lr: [0, 1] Learning rate for Q-value updates.
    - beta_1: [0, 10] Inverse temperature for Stage 1 choices.
    - beta_2: [0, 10] Inverse temperature for Stage 2 choices.
    - w: [0, 1] Weighting parameter (0 = Pure MF, 1 = Pure MB).
    - pers_base: [0, 5] Base perseveration bonus for repeating the last choice.
    - pers_growth: [0, 2] Additional bonus added per unit of streak length.
    """
    lr, beta_1, beta_2, w, pers_base, pers_growth = model_parameters
    n_trials = len(action_1)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Streak tracking variables
    prev_action_1 = -1
    streak_count = 0

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Calculate streak bonus
        bonus = np.zeros(2)
        if prev_action_1 != -1:
            # Base perseveration
            bonus[prev_action_1] += pers_base
            # Streak bonus (streak_count is number of repeats)
            # We cap the streak impact to prevent numerical explosion, though 
            # the participant data shows very strong stickiness.
            capped_streak = min(streak_count, 20)
            bonus[prev_action_1] += pers_growth * capped_streak

        exp_q1 = np.exp(beta_1 * q_net + bonus)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Update streak for next trial
        if action_1[trial] == prev_action_1:
            streak_count += 1
        else:
            streak_count = 0
        prev_action_1 = action_1[trial]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning ---
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]

        # Stage 1 MF Update (TD)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1

        # Stage 2 MF Update
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += lr * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Rare Transition Gating Model.

    This model hypothesizes that the participant modulates their Model-Free learning
    based on the transition type. If a transition is "rare" (unexpected), they might
    discount the update (gate it) or over-weight it. This helps explain behavior where
    rewards following rare transitions do not reinforce the original choice as strongly
    as common transitions (or vice versa).

    Parameters:
    - lr: [0, 1] Base learning rate.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - w: [0, 1] Weighting (0=MF, 1=MB).
    - pers: [0, 5] Standard perseveration bonus.
    - rare_factor: [0, 5] Multiplier for the learning rate on rare transitions. 
                   <1 implies gating/ignoring, >1 implies surprise boosting.
    """
    lr, beta_1, beta_2, w, pers, rare_factor = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if prev_action_1 != -1:
            q_net[prev_action_1] += pers
            
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        prev_action_1 = action_1[trial]
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning ---
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]

        # Determine if transition was rare
        # Common: 0->0, 1->1. Rare: 0->1, 1->0.
        is_common = (a1 == state_idx) 
        
        # Apply gating to learning rate for Stage 1 update
        current_lr = lr if is_common else (lr * rare_factor)
        
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += current_lr * delta_stage1

        # Stage 2 update uses base learning rate (outcome is direct)
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += lr * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Dynamic Transition Learning Model.

    Unlike the standard model which assumes fixed transition probabilities (0.7/0.3),
    this model allows the participant to learn the transition structure over time.
    This captures potential beliefs that the spaceship reliability might be changing
    or that the participant is estimating the probabilities from experience.

    Parameters:
    - lr_val: [0, 1] Learning rate for Q-values (reward learning).
    - lr_trans: [0, 1] Learning rate for transition probabilities (structure learning).
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - w: [0, 1] Weighting (0=MF, 1=MB).
    - pers: [0, 5] Perseveration bonus.
    """
    lr_val, lr_trans, beta_1, beta_2, w, pers = model_parameters
    n_trials = len(action_1)
    
    # Initialize transition matrix with instructed prior or uniform guess
    # Using the standard 0.7/0.3 as a starting point, but it will drift.
    trans_probs = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # MB calculation uses the *current* learned transition matrix
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = trans_probs @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if prev_action_1 != -1:
            q_net[prev_action_1] += pers
            
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        prev_action_1 = action_1[trial]
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning ---
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]

        # 1. Update Transition Matrix (Structure Learning)
        # Increase prob of observed transition (a1 -> state_idx), decrease others
        # T[a, s'] = T[a, s'] + lr_trans * (1 - T[a, s'])
        # T[a, other] = T[a, other] + lr_trans * (0 - T[a, other])
        
        # Observed next state
        trans_probs[a1, state_idx] += lr_trans * (1 - trans_probs[a1, state_idx])
        # Unobserved next state
        other_state = 1 - state_idx
        trans_probs[a1, other_state] += lr_trans * (0 - trans_probs[a1, other_state])

        # 2. Update Value Functions (Reward Learning)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr_val * delta_stage1

        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += lr_val * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```