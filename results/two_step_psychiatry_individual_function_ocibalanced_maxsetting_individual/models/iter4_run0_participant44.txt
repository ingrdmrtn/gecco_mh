Here are three new cognitive models for the two-step decision task.

### Model 1: Subjective Transition Hybrid Learner
This model relaxes the assumption that the participant believes the transition probabilities are exactly 0.7 and 0.3. Instead, it estimates a `trans_prob` parameter representing the participant's subjective belief about the "common" transition. This allows the model to capture behavior where the participant might view the transitions as more deterministic (closer to 1.0) or more random (closer to 0.5) than reality.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Subjective Transition Hybrid Learner.
    
    This model allows the participant to have a subjective belief about the 
    transition probabilities (Spaceship -> Planet) that may differ from the 
    true probabilities (0.7/0.3). This affects the Model-Based value calculation.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for MF updates.
    - beta: [0, 10] Inverse temperature for softmax.
    - w: [0, 1] Weight for Model-Based values (0=MF, 1=MB).
    - trans_prob: [0, 1] Subjective probability of common transitions (A->X, U->Y).
    """
    learning_rate, beta, w, trans_prob = model_parameters
    n_trials = len(action_1)
  
    # Subjective transition matrix based on parameter
    # Row 0: Space A -> [Planet X, Planet Y]
    # Row 1: Space U -> [Planet X, Planet Y]
    # Assuming trans_prob is the probability of the "common" transition.
    transition_matrix = np.array([[trans_prob, 1-trans_prob], [1-trans_prob, trans_prob]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # Stage 1 Policy (Hybrid)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # Stage 2 Policy (Pure MF)
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # Updates
        # Stage 1 Update (TD-0)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Stage 2 Update (TD-0)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Outcome-Dependent Beta Hybrid Learner
This model hypothesizes that the participant's exploration-exploitation balance (beta) is not static but reacts to the outcome of the previous trial. A win might lead to higher confidence (higher `beta_win`, more exploitation), while a loss might trigger uncertainty or searching behavior (lower `beta_loss`, more exploration).

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Outcome-Dependent Beta Hybrid Learner.
    
    This model hypothesizes that the participant's exploration/exploitation 
    balance (beta) changes based on the previous trial's outcome.
    Winning leads to one beta (likely higher/exploit), losing to another (likely lower/explore).
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for MF updates.
    - beta_win: [0, 10] Inverse temperature after a Reward=1.
    - beta_loss: [0, 10] Inverse temperature after a Reward=0.
    - w: [0, 1] Weight for Model-Based values.
    """
    learning_rate, beta_win, beta_loss, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Initial beta (using beta_loss as conservative start)
    current_beta = beta_loss 

    for trial in range(n_trials):

        # Set beta based on previous reward
        if trial > 0:
            if reward[trial-1] == 1:
                current_beta = beta_win
            else:
                current_beta = beta_loss

        # Stage 1 Policy (Hybrid)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(current_beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # Stage 2 Policy
        exp_q2 = np.exp(current_beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # Updates
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Dual-Stage Asymmetric Learner
This model distinguishes between the two types of learning in the task: learning the value of the spaceship (Stage 1) and learning the value of the aliens (Stage 2). It applies asymmetric learning rates (optimism/pessimism) specifically to the second stage where the actual rewards are received, while using a standard single learning rate for the first stage which learns via value transfer (TD).

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Dual-Stage Asymmetric Learner.
    
    Separates the learning process into Stage 1 (Spaceship) and Stage 2 (Alien).
    Stage 2, where rewards occur, has asymmetric learning rates for wins and losses.
    Stage 1, which learns value transfer, has a single learning rate.
    
    Parameters:
    - alpha1: [0, 1] Learning rate for Stage 1 (TD update).
    - alpha2_pos: [0, 1] Learning rate for Stage 2 when Reward=1.
    - alpha2_neg: [0, 1] Learning rate for Stage 2 when Reward=0.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weight for Model-Based values.
    """
    alpha1, alpha2_pos, alpha2_neg, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # Stage 1 Policy (Hybrid)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # Updates
        # Stage 1 Update (using alpha1)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += alpha1 * delta_stage1
        
        # Stage 2 Update (using asymmetric alpha2)
        current_lr = alpha2_pos if reward[trial] == 1 else alpha2_neg
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += current_lr * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```