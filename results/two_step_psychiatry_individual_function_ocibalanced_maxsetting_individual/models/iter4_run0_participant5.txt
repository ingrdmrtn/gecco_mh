Here are three new cognitive models for the two-step task.

### Model 1: Hybrid Model with Value Decay
This model combines Model-Based (MB) planning and Model-Free (MF) learning. It introduces a `decay_rate` parameter that gradually reduces the value of unchosen actions and unvisited states. This mechanism allows the model to adapt to the drifting reward probabilities of the aliens by "forgetting" outdated information while using the transition structure to guide decisions.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model (MB/MF) with Value Decay.
    Combines model-based planning with model-free learning, incorporating
    value decay to handle drifting reward probabilities.
    
    Parameters:
    - learning_rate: [0, 1] Update rate for Q-values.
    - beta_1: [0, 10] Inverse temp for stage 1.
    - beta_2: [0, 10] Inverse temp for stage 2.
    - w: [0, 1] Weight for Model-Based control (1=Pure MB, 0=Pure MF).
    - decay_rate: [0, 1] Decay for unchosen/unvisited Q-values.
    """
    learning_rate, beta_1, beta_2, w, decay_rate = model_parameters
    n_trials = len(action_1)
    
    # Transition matrix: A(0)->X(0) 0.7, U(1)->Y(1) 0.7
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    
    for trial in range(n_trials):
        # Skip invalid trials
        if action_1[trial] == -1 or action_2[trial] == -1 or state[trial] == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue
            
        # Stage 1 Policy
        # MB values: Expected max Q2 based on transition matrix
        max_q2 = np.max(q_stage2_mf, axis=1) 
        q_stage1_mb = transition_matrix @ max_q2 
        
        # Net Q values: Weighted mix of MB and MF
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_1 * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Stage 2 Policy
        s = state[trial]
        exp_q2 = np.exp(beta_2 * q_stage2_mf[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # Updates
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]
        
        # MF Stage 1 Update (SARSA-style TD(0) driven by Q-value of state 2)
        delta_1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_1
        
        # MF Stage 2 Update
        delta_2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += learning_rate * delta_2
        
        # Value Decay
        # Decay unchosen stage 1 action
        q_stage1_mf[1 - a1] *= (1.0 - decay_rate)
        
        # Decay unchosen stage 2 action in current state
        q_stage2_mf[s, 1 - a2] *= (1.0 - decay_rate)
        
        # Decay all stage 2 actions in the unvisited state
        q_stage2_mf[1 - s, :] *= (1.0 - decay_rate)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Asymmetric Learning with Value Decay
This model posits that the participant learns differently from positive prediction errors (better than expected) versus negative ones (worse than expected). It uses `alpha_pos` and `alpha_neg` to capture this asymmetry. Additionally, it includes `decay_rate` to handle the non-stationary reward environment.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Asymmetric Learning Model with Value Decay.
    Differentiates learning from positive and negative prediction errors,
    combined with value decay for unchosen options.
    
    Parameters:
    - alpha_pos: [0, 1] Learning rate for positive RPE.
    - alpha_neg: [0, 1] Learning rate for negative RPE.
    - beta_1: [0, 10] Inverse temp for stage 1.
    - beta_2: [0, 10] Inverse temp for stage 2.
    - decay_rate: [0, 1] Decay for unchosen Q-values.
    """
    alpha_pos, alpha_neg, beta_1, beta_2, decay_rate = model_parameters
    n_trials = len(action_1)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    for trial in range(n_trials):
        if action_1[trial] == -1 or action_2[trial] == -1 or state[trial] == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue
            
        # Stage 1 Choice
        exp_q1 = np.exp(beta_1 * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Stage 2 Choice
        s = state[trial]
        exp_q2 = np.exp(beta_2 * q_stage2[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]
        
        # Update Stage 1
        delta1 = q_stage2[s, a2] - q_stage1[a1]
        alpha1 = alpha_pos if delta1 > 0 else alpha_neg
        q_stage1[a1] += alpha1 * delta1
        
        # Update Stage 2
        delta2 = r - q_stage2[s, a2]
        alpha2 = alpha_pos if delta2 > 0 else alpha_neg
        q_stage2[s, a2] += alpha2 * delta2
        
        # Decay unchosen/unvisited
        q_stage1[1 - a1] *= (1.0 - decay_rate)
        q_stage2[s, 1 - a2] *= (1.0 - decay_rate)
        q_stage2[1 - s, :] *= (1.0 - decay_rate)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Dual-Alpha Hybrid Model with Stickiness
This model separates the learning rates for Stage 1 (spaceships) and Stage 2 (aliens), acknowledging that learning the stable transition structure might occur at a different rate than learning the drifting alien rewards. It also includes a `stickiness` parameter to account for the participant's tendency to repeat the same spaceship choice, mixed with a Model-Based evaluation.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Dual-Alpha Hybrid Model with Stickiness.
    Uses separate learning rates for Stage 1 (transitions) and Stage 2 (rewards),
    mixed with model-based values and choice stickiness.
    
    Parameters:
    - alpha1: [0, 1] Learning rate for Stage 1 MF values.
    - alpha2: [0, 1] Learning rate for Stage 2 values.
    - beta_1: [0, 10] Inverse temp for stage 1.
    - beta_2: [0, 10] Inverse temp for stage 2.
    - w: [0, 1] MB/MF mixing weight.
    - stickiness: [0, 10] Choice perseverance bonus.
    """
    alpha1, alpha2, beta_1, beta_2, w, stickiness = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        if action_1[trial] == -1 or action_2[trial] == -1 or state[trial] == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue
            
        # Stage 1 Policy
        # MB Component
        max_q2 = np.max(q_stage2, axis=1)
        q_stage1_mb = transition_matrix @ max_q2
        
        # Mixed Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Apply Stickiness to logits
        logits = beta_1 * q_net
        if last_action_1 != -1:
            logits[last_action_1] += stickiness
            
        # Softmax with stability
        exp_q1 = np.exp(logits - np.max(logits))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Stage 2 Policy
        s = state[trial]
        exp_q2 = np.exp(beta_2 * q_stage2[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]
        
        # Updates
        # Stage 1 MF (using alpha1)
        delta1 = q_stage2[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha1 * delta1
        
        # Stage 2 (using alpha2)
        delta2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += alpha2 * delta2
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```