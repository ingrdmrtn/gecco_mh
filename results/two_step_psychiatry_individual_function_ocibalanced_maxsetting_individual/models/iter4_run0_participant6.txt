Here are the three proposed cognitive models.

### Model 1: Accumulating Habit with Model-Free Learning
This model hypothesizes that the participant's "stickiness" is not just a one-step repetition bias, but an accumulating "habit" strength that builds up with repeated choices and decays when not chosen. This distinguishes between the *learned value* of an action (based on reward) and the *habitual tendency* to select it (based on history).

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Accumulating Habit Model.
    
    Separates choice inertia (habit) from reward-based learning.
    Habit strength accumulates for chosen actions and decays for unchosen ones.
    
    Parameters:
    - learning_rate: [0, 1] Update rate for Q-values based on reward.
    - habit_rate: [0, 1] Rate at which habit trace updates (builds up/decays).
    - habit_strength: [0, 10] Weight of the habit trace in the decision.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    """
    learning_rate, habit_rate, habit_strength, beta_1, beta_2 = model_parameters
    n_trials = len(action_1)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values (Model-Free)
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    # Habit trace for stage 1 choices
    habit_trace = np.zeros(2)
    
    for trial in range(n_trials):
        # Handle missing data
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue
            
        # --- Stage 1 Decision ---
        # Net value is a mix of Reward Value (Q) and Habit (H)
        net_value_1 = beta_1 * q_stage1 + habit_strength * habit_trace
        
        # Softmax
        exp_q1 = np.exp(net_value_1 - np.max(net_value_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Update Habit Trace
        # Chosen: accumulates towards 1. Unchosen: decays towards 0.
        chosen_1 = action_1[trial]
        habit_trace[chosen_1] += habit_rate * (1.0 - habit_trace[chosen_1])
        habit_trace[1 - chosen_1] *= (1.0 - habit_rate)
        
        # --- Stage 2 Decision ---
        state_idx = state[trial]
        net_value_2 = beta_2 * q_stage2[state_idx]
        
        exp_q2 = np.exp(net_value_2 - np.max(net_value_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning (Model-Free) ---
        # Update Stage 2 Q-value
        r = reward[trial]
        chosen_2 = action_2[trial]
        pe_2 = r - q_stage2[state_idx, chosen_2]
        q_stage2[state_idx, chosen_2] += learning_rate * pe_2
        
        # Update Stage 1 Q-value (TD(1) logic: update with final reward)
        pe_1 = r - q_stage1[chosen_1]
        q_stage1[chosen_1] += learning_rate * pe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model-Based/Model-Free with Value Decay
This model combines the "Value Decay" mechanism (which was successful in previous iterations) with a Model-Based (MB) planning component. It tests if the participant uses knowledge of the transition structure (MB) in addition to tracking drifting rewards (MF with decay). The decay helps the agent handle the non-stationary rewards by "forgetting" old values of unvisited states.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF Model with Value Decay.
    
    Combines Model-Based planning with a Model-Free learner that experiences 
    memory decay on unchosen options.
    
    Parameters:
    - learning_rate: [0, 1] Update rate for chosen options.
    - decay_rate: [0, 1] Decay rate for unchosen options (forgetting).
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - w: [0, 1] Weighting of Model-Based values (0=Pure MF, 1=Pure MB).
    - stickiness: [0, 5] Bonus for repeating the previous Stage 1 choice.
    """
    learning_rate, decay_rate, beta_1, beta_2, w, stickiness = model_parameters
    n_trials = len(action_1)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_mf_1 = np.zeros(2)
    q_mf_2 = np.zeros((2, 2))
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue
            
        # --- Stage 1 Decision (Hybrid) ---
        # Calculate Model-Based values
        # Q_MB(s1) = Sum_s2 [ P(s2|s1) * max(Q_MF(s2)) ]
        max_q_stage2 = np.max(q_mf_2, axis=1)
        q_mb_1 = transition_matrix @ max_q_stage2
        
        # Mix MB and MF
        q_net_1 = w * q_mb_1 + (1 - w) * q_mf_1
        
        # Add stickiness
        if last_action_1 != -1:
            q_net_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta_1 * (q_net_1 - np.max(q_net_1)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_action_1 = action_1[trial]
        
        # --- Stage 2 Decision ---
        state_idx = state[trial]
        exp_q2 = np.exp(beta_2 * (q_mf_2[state_idx] - np.max(q_mf_2[state_idx])))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning & Decay ---
        r = reward[trial]
        chosen_1 = action_1[trial]
        chosen_2 = action_2[trial]
        
        # Update Stage 2 (MF)
        pe_2 = r - q_mf_2[state_idx, chosen_2]
        q_mf_2[state_idx, chosen_2] += learning_rate * pe_2
        
        # Update Stage 1 (MF) - TD(1) style
        pe_1 = r - q_mf_1[chosen_1]
        q_mf_1[chosen_1] += learning_rate * pe_1
        
        # Decay unchosen Stage 2 values
        unchosen_2 = 1 - chosen_2
        q_mf_2[state_idx, unchosen_2] *= (1 - decay_rate)
        
        # Decay values for the unvisited planet
        other_planet = 1 - state_idx
        q_mf_2[other_planet, :] *= (1 - decay_rate)
        
        # Decay unchosen Stage 1 values
        q_mf_1[1 - chosen_1] *= (1 - decay_rate)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model with Separate Learning Rates
This model proposes that the participant learns at different rates for the two stages. For instance, they might update their estimates of the aliens' coin probabilities (Stage 2) quickly, but update their spaceship preferences (Stage 1) more slowly (or vice versa). This assumes a Hybrid MB/MF architecture.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF Model with Separate Learning Rates.
    
    Uses distinct learning rates for Stage 1 (Spaceships) and Stage 2 (Aliens).
    This allows the model to capture different volatilities or learning dynamics
    at different levels of the task hierarchy.
    
    Parameters:
    - alpha_1: [0, 1] Learning rate for Stage 1 (Spaceship) MF values.
    - alpha_2: [0, 1] Learning rate for Stage 2 (Alien) MF values.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - w: [0, 1] Weighting of Model-Based values.
    - stickiness: [0, 5] Bonus for repeating the previous Stage 1 choice.
    """
    alpha_1, alpha_2, beta_1, beta_2, w, stickiness = model_parameters
    n_trials = len(action_1)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_mf_1 = np.zeros(2)
    q_mf_2 = np.zeros((2, 2))
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue
            
        # --- Stage 1 Decision ---
        # Model-Based Value
        max_q_stage2 = np.max(q_mf_2, axis=1)
        q_mb_1 = transition_matrix @ max_q_stage2
        
        # Net Value
        q_net_1 = w * q_mb_1 + (1 - w) * q_mf_1
        
        if last_action_1 != -1:
            q_net_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta_1 * (q_net_1 - np.max(q_net_1)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_action_1 = action_1[trial]
        
        # --- Stage 2 Decision ---
        state_idx = state[trial]
        exp_q2 = np.exp(beta_2 * (q_mf_2[state_idx] - np.max(q_mf_2[state_idx])))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning ---
        r = reward[trial]
        chosen_1 = action_1[trial]
        chosen_2 = action_2[trial]
        
        # Update Stage 2 with alpha_2
        pe_2 = r - q_mf_2[state_idx, chosen_2]
        q_mf_2[state_idx, chosen_2] += alpha_2 * pe_2
        
        # Update Stage 1 with alpha_1 (TD(1) update)
        pe_1 = r - q_mf_1[chosen_1]
        q_mf_1[chosen_1] += alpha_1 * pe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```