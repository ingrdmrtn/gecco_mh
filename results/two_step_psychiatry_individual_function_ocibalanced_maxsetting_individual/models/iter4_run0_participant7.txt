Here are three cognitive models developed based on the participant's data and the provided feedback. 

The participant exhibits two key behavioral signatures:
1.  **Perseveration (Stickiness):** Long streaks of repeating the same spaceship choice (Stage 1), even when rewards are intermittent.
2.  **Adaptation to Drift:** The environment (aliens) has changing reward probabilities. The best model so far utilized a "decay" mechanism to handle this non-stationarity.

The proposed models explore different combinations of these mechanisms and learning dynamics.

### Model 1: Hybrid with Stickiness and Passive Decay
This model combines the standard hybrid reinforcement learning (Model-Based + Model-Free) with two specific augmentations:
1.  **Stickiness:** A bonus added to the previously chosen spaceship to account for motor perseveration.
2.  **Passive Decay:** Unchosen aliens (Stage 2) have their Q-values decay towards 0.5 (chance level), allowing the agent to "forget" old information about dynamic bandits.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Stage 1 Stickiness and Stage 2 Passive Decay.
    
    Combines the benefits of handling non-stationary bandits (Decay) with 
    observed perseveration in spaceship choice (Stickiness).
    
    Parameters:
    - learning_rate: Rate at which Q-values are updated [0, 1]
    - beta_1: Inverse temperature for Stage 1 softmax [0, 10]
    - beta_2: Inverse temperature for Stage 2 softmax [0, 10]
    - w: Mixing weight between Model-Based and Model-Free values [0, 1]
    - lam: Eligibility trace parameter [0, 1]
    - decay: Rate at which unchosen Stage 2 Q-values decay towards 0.5 [0, 1]
    - stick: Choice stickiness bonus for repeating the previous Stage 1 choice [0, 10]
    """
    learning_rate, beta_1, beta_2, w, lam, decay, stick = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Initialize Q-values at 0.5 (neutral expectation for 0/1 rewards)
    q_stage1_mf = np.full(2, 0.5)
    q_stage2_mf = np.full((2, 2), 0.5)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta_1 * q_net
        # Add stickiness to the previously chosen action
        if trial > 0:
            logits_1[action_1[trial-1]] += stick
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1)) # Subtract max for numerical stability
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Policy ---
        state_idx = state[trial] # Planet 0 or 1
        logits_2 = beta_2 * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        # Stage 1 Prediction Error
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Stage 2 Prediction Error
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Eligibility Trace Update for Stage 1
        q_stage1_mf[action_1[trial]] += learning_rate * lam * delta_stage2
        
        # Passive Decay for unchosen Stage 2 options (Counterfactual forgetting)
        for s in range(2):
            for a in range(2):
                if not (s == state_idx and a == action_2[trial]):
                    q_stage2_mf[s, a] = (1 - decay) * q_stage2_mf[s, a] + decay * 0.5

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid with Dual Learning Rates and Passive Decay
This model hypothesizes that the participant learns at different rates for Stage 1 (Spaceship selection/Planning) and Stage 2 (Alien selection/Bandit task). Stage 2 involves tracking rapidly drifting probabilities, potentially requiring a different plasticity (`lr_2`) compared to the more structural Stage 1 (`lr_1`).

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Dual Learning Rates and Passive Decay.
    
    Differentiates plasticity between Stage 1 (Spaceships) and Stage 2 (Aliens),
    while handling Stage 2 non-stationarity via decay.
    
    Parameters:
    - lr_1: Learning rate for Stage 1 [0, 1]
    - lr_2: Learning rate for Stage 2 [0, 1]
    - beta_1: Inverse temperature for Stage 1 softmax [0, 10]
    - beta_2: Inverse temperature for Stage 2 softmax [0, 10]
    - w: Mixing weight between Model-Based and Model-Free values [0, 1]
    - lam: Eligibility trace parameter [0, 1]
    - decay: Rate at which unchosen Stage 2 Q-values decay towards 0.5 [0, 1]
    """
    lr_1, lr_2, beta_1, beta_2, w, lam, decay = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.full(2, 0.5)
    q_stage2_mf = np.full((2, 2), 0.5)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta_1 * q_net
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Policy ---
        state_idx = state[trial]
        logits_2 = beta_2 * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        # Update Stage 1 with lr_1
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr_1 * delta_stage1 
        
        # Update Stage 2 with lr_2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr_2 * delta_stage2 
        
        # Update Trace with lr_1 (Updating Stage 1 weights based on Stage 2 outcome)
        q_stage1_mf[action_1[trial]] += lr_1 * lam * delta_stage2
        
        # Decay
        for s in range(2):
            for a in range(2):
                if not (s == state_idx and a == action_2[trial]):
                    q_stage2_mf[s, a] = (1 - decay) * q_stage2_mf[s, a] + decay * 0.5
                    
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Independent Weights Hybrid Model with Stickiness and Decay
Instead of using a mixing weight `w` (which constrains the Model-Based and Model-Free contributions to sum to 1 relative to a single beta), this model assigns independent inverse temperatures (`beta_mb`, `beta_mf`) to each system. This "decomposition" approach can sometimes better capture behavior when one system is significantly noisier or more dominant than the other.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Independent Weights Hybrid Model with Stickiness and Decay.
    
    Replaces the mixing weight 'w' with independent inverse temperatures for 
    Model-Based and Model-Free systems, allowing for distinct scaling of their influences.
    
    Parameters:
    - learning_rate: Rate at which Q-values are updated [0, 1]
    - beta_mb: Inverse temperature for Model-Based component [0, 10]
    - beta_mf: Inverse temperature for Model-Free component [0, 10]
    - beta_2: Inverse temperature for Stage 2 softmax [0, 10]
    - lam: Eligibility trace parameter [0, 1]
    - decay: Rate at which unchosen Stage 2 Q-values decay towards 0.5 [0, 1]
    - stick: Choice stickiness bonus for repeating the previous Stage 1 choice [0, 10]
    """
    learning_rate, beta_mb, beta_mf, beta_2, lam, decay, stick = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.full(2, 0.5)
    q_stage2_mf = np.full((2, 2), 0.5)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Independent weighting instead of convex combination
        # Note: beta_1 is effectively split into beta_mb and beta_mf
        logits_1 = beta_mb * q_stage1_mb + beta_mf * q_stage1_mf
        
        if trial > 0:
            logits_1[action_1[trial-1]] += stick
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Policy ---
        state_idx = state[trial]
        logits_2 = beta_2 * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        q_stage1_mf[action_1[trial]] += learning_rate * lam * delta_stage2
        
        for s in range(2):
            for a in range(2):
                if not (s == state_idx and a == action_2[trial]):
                    q_stage2_mf[s, a] = (1 - decay) * q_stage2_mf[s, a] + decay * 0.5
                    
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```