Here are three new cognitive models for the two-step task, designed to capture different potential mechanisms underlying the participant's behavior, such as eligibility traces, habit formation via choice kernels, and average reward tracking.

### Model 1: Hybrid MB/MF with Eligibility Trace and Stickiness
This model extends the standard hybrid model by including an eligibility trace ($\lambda$). This allows the reward prediction error from the second stage to directly update the value of the first-stage choice, bridging the temporal gap. This is often referred to as the "full" model in two-step task literature (e.g., Daw et al., 2011).

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF model with Eligibility Trace and Stickiness.
    
    Includes an eligibility trace (lambda) allowing Stage 2 reward prediction errors
    to directly update Stage 1 Q-values. This bridges the gap between the first-stage
    choice and the final outcome, modulating the strength of the Model-Free update.
    
    Parameters:
    learning_rate: [0,1] - Learning rate for Q-values.
    beta: [0,10] - Inverse temperature for softmax.
    w: [0,1] - Mixing weight (0=MF, 1=MB).
    lambda_elig: [0,1] - Eligibility trace decay (0=no trace, 1=full trace).
    stickiness: [0,5] - Choice perseverance bonus.
    """
    learning_rate, beta, w, lambda_elig, stickiness = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1
    
    for trial in range(n_trials):
        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        if prev_action_1 != -1:
            q_net[prev_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        if action_2[trial] == -1:
            prev_action_1 = action_1[trial]
            continue

        state_idx = state[trial]
        
        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # Updates
        # Stage 1 RPE (TD(0) part)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Stage 2 RPE
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Eligibility Trace Update for Stage 1: Propagate Stage 2 RPE back to Stage 1
        q_stage1_mf[action_1[trial]] += learning_rate * lambda_elig * delta_stage2
        
        prev_action_1 = action_1[trial]

    eps = 1e-10
    valid_mask_2 = action_2 != -1
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2[valid_mask_2] + eps)))
    return log_loss
```

### Model 2: Hybrid MB/MF with Choice Kernel (Habit)
This model replaces simple "stickiness" (which usually only considers the immediately preceding trial) with a "Choice Kernel". The kernel tracks the frequency of past choices using an exponential moving average. This allows the model to capture stronger habit formation or "momentum" where the participant repeats choices over long streaks, as observed in the data.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF model with Choice Kernel (Habit).
    
    Replaces simple 1-step stickiness with a dynamic choice kernel that tracks 
    the history of past choices. This accounts for habit formation where repeated 
    selection of an action increases the tendency to select it again.
    
    Parameters:
    learning_rate: [0,1] - Learning rate for Q-values.
    beta: [0,10] - Inverse temperature.
    w: [0,1] - Mixing weight.
    ck_lr: [0,1] - Learning rate for the choice kernel (habit buildup/decay rate).
    ck_weight: [0,5] - Weight of the choice kernel in the decision (habit strength).
    """
    learning_rate, beta, w, ck_lr, ck_weight = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Choice kernel tracks the "habit" value of each stage 1 action
    choice_kernel = np.zeros(2)
    
    for trial in range(n_trials):
        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combine MB, MF, and Habit (Choice Kernel)
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf + ck_weight * choice_kernel
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Update Choice Kernel based on observed action
        a1 = action_1[trial]
        # Increase kernel for chosen, decay for unchosen
        choice_kernel[a1] += ck_lr * (1 - choice_kernel[a1])
        choice_kernel[1-a1] += ck_lr * (0 - choice_kernel[1-a1])
        
        if action_2[trial] == -1:
            continue

        state_idx = state[trial]
        
        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # Updates
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
    eps = 1e-10
    valid_mask_2 = action_2 != -1
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2[valid_mask_2] + eps)))
    return log_loss
```

### Model 3: Hybrid MB/MF with Average Reward Tracking
This model incorporates Average Reward Reinforcement Learning. It tracks a global average reward rate ($\rho$) and updates Q-values based on the difference between the immediate reward and this average. This mechanism is particularly effective in environments with drifting reward probabilities, as it centers the Q-values and helps the agent adapt to general changes in environmental richness.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF model with Average Reward Tracking.
    
    Tracks a global average reward rate (rho) and uses centered rewards (R - rho) 
    for updating Q-values. This helps the model adapt to the slowly changing 
    reward probabilities by comparing outcomes to the current global expectation.
    
    Parameters:
    learning_rate: [0,1] - Q-value learning rate.
    beta: [0,10] - Inverse temperature.
    w: [0,1] - Mixing weight.
    stickiness: [0,5] - Choice perseverance.
    rho_lr: [0,1] - Learning rate for the average reward (rho).
    """
    learning_rate, beta, w, stickiness, rho_lr = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    rho = 0.0 # Global average reward
    prev_action_1 = -1
    
    for trial in range(n_trials):
        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        if prev_action_1 != -1:
            q_net[prev_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        if action_2[trial] == -1:
            prev_action_1 = action_1[trial]
            continue

        state_idx = state[trial]
        
        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # Calculate effective reward (centered by average reward)
        eff_reward = reward[trial] - rho
        
        # Updates
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Update Stage 2 MF using effective reward
        delta_stage2 = eff_reward - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Update Average Reward (rho)
        rho += rho_lr * (reward[trial] - rho)
        
        prev_action_1 = action_1[trial]

    eps = 1e-10
    valid_mask_2 = action_2 != -1
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2[valid_mask_2] + eps)))
    return log_loss
```