Here are the three proposed cognitive models.

### Model 1: Hybrid Learner with Outcome-Dependent Perseveration
This model extends the standard hybrid (Model-Based/Model-Free) learner by splitting the perseveration (stickiness) parameter into two distinct components: one for when the previous trial was rewarded (`persev_win`) and one for when it was unrewarded (`persev_loss`). This captures the "Win-Stay, Lose-Shift" heuristic distinct from value-based learning, allowing the model to represent a participant who is, for example, highly sticky after wins but neutral or switching after losses.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner with Outcome-Dependent Perseveration.
    
    Combines Model-Based (MB) and Model-Free (MF) value estimation.
    Perseveration is split into 'win' and 'loss' components, allowing 
    distinct stickiness behavior based on the previous trial's outcome.

    Parameters:
    lr:          [0, 1]  Learning rate for MF Q-values.
    beta:        [0, 10] Inverse temperature (softmax sensitivity).
    w:           [0, 1]  Weighting parameter (0 = Pure MF, 1 = Pure MB).
    persev_win:  [0, 5]  Stickiness bonus added if previous trial was rewarded.
    persev_loss: [0, 5]  Stickiness bonus added if previous trial was unrewarded.
    """
    lr, beta, w, persev_win, persev_loss = model_parameters
    n_trials = len(action_1)
    
    # Fixed transition structure: Space 0 -> Planet 0 (0.7), Space 1 -> Planet 1 (0.7)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_mf = np.zeros(2)         # Stage 1 MF values
    q_stage2 = np.zeros((2, 2)) # Stage 2 values (State x Action)
    
    prev_action_1 = -1
    prev_reward = -1

    for trial in range(n_trials):
        # Skip missing data
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        # --- Stage 1 Choice ---
        # Calculate Model-Based values: T * max(Q_stage2)
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_mb = transition_matrix @ max_q_stage2
        
        # Mix MB and MF values
        q_net = w * q_mb + (1 - w) * q_mf
        
        # Add Outcome-Dependent Perseveration
        if prev_action_1 != -1:
            if prev_reward == 1:
                q_net[prev_action_1] += persev_win
            else:
                q_net[prev_action_1] += persev_loss
        
        # Softmax for Stage 1
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning ---
        # Update Stage 2 Q-values (Standard Q-learning)
        r = reward[trial]
        # Skip update if reward is -1 (missing)
        if r != -1:
            pe_2 = r - q_stage2[state_idx, action_2[trial]]
            q_stage2[state_idx, action_2[trial]] += lr * pe_2
            
            # Update Stage 1 MF values (TD(0) logic using stage 2 value as proxy or direct reward? 
            # Standard Hybrid often uses Stage 2 value as target for Stage 1 MF, 
            # or uses TD(lambda). Here we use simple TD(0) to best value of next state)
            
            # Note: In standard Daw 2011, MF Q(s1, a1) updates towards Q(s2, a2) or Reward.
            # Here we implement TD(0) updating towards the value of the state reached.
            pe_1 = q_stage2[state_idx, action_2[trial]] - q_mf[action_1[trial]]
            q_mf[action_1[trial]] += lr * pe_1

        prev_action_1 = action_1[trial]
        prev_reward = r

    eps = 1e-10
    valid_trials = (action_1 != -1) & (state != -1) & (action_2 != -1) & (reward != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```

### Model 2: Hybrid Learner with Subjective Transition Belief
This model tests the hypothesis that the participant uses a Model-Based strategy but has a biased or incorrect belief about the transition probabilities (the "rules" of the spaceship). Instead of assuming the true 0.7/0.3 probabilities, the model fits a parameter `p_common` representing the participant's subjective belief in the likelihood of the common transition.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner with Subjective Transition Belief.
    
    Standard Hybrid model but the Model-Based component uses a fitted 
    transition probability (subjective belief) rather than the ground truth.
    
    Parameters:
    lr:          [0, 1]  Learning rate.
    beta:        [0, 10] Inverse temperature.
    w:           [0, 1]  Weighting parameter (MB vs MF).
    p_common:    [0, 1]  Subjective probability of the 'common' transition (A->X, B->Y).
    perseveration: [0, 5] General stickiness bonus.
    """
    lr, beta, w, p_common, perseveration = model_parameters
    n_trials = len(action_1)
    
    # Subjective transition matrix based on p_common parameter
    # Assumes symmetry: Space 0->Planet 0 is p_common, Space 1->Planet 1 is p_common
    subjective_T = np.array([[p_common, 1 - p_common], 
                             [1 - p_common, p_common]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_mf = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    prev_action_1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        # --- Stage 1 Choice ---
        # MB calculation uses subjective T
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_mb = subjective_T @ max_q_stage2
        
        q_net = w * q_mb + (1 - w) * q_mf
        
        if prev_action_1 != -1:
            q_net[prev_action_1] += perseveration
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning ---
        r = reward[trial]
        if r != -1:
            # Update Stage 2
            pe_2 = r - q_stage2[state_idx, action_2[trial]]
            q_stage2[state_idx, action_2[trial]] += lr * pe_2
            
            # Update Stage 1 MF
            pe_1 = q_stage2[state_idx, action_2[trial]] - q_mf[action_1[trial]]
            q_mf[action_1[trial]] += lr * pe_1
            
        prev_action_1 = action_1[trial]

    eps = 1e-10
    valid_trials = (action_1 != -1) & (state != -1) & (action_2 != -1) & (reward != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```

### Model 3: Hybrid Learner with Dynamic Transition Learning
This model assumes the participant is actively learning the transition structure of the task (the spaceship-planet probabilities) trial-by-trial, rather than using a fixed or static belief. This allows the Model-Based component to adapt if the participant perceives the "rare" transitions as becoming common.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner with Dynamic Transition Learning.
    
    The Model-Based component updates its internal model of the transition 
    matrix based on experienced transitions using a separate learning rate.
    
    Parameters:
    lr_val:      [0, 1]  Learning rate for value estimation (Q-values).
    lr_trans:    [0, 1]  Learning rate for transition probabilities.
    beta:        [0, 10] Inverse temperature.
    w:           [0, 1]  Weighting parameter (MB vs MF).
    perseveration: [0, 5] Stickiness bonus.
    """
    lr_val, lr_trans, beta, w, perseveration = model_parameters
    n_trials = len(action_1)
    
    # Initialize transition belief (start with uniform or slight prior towards truth)
    # Here we start with 0.7/0.3 to represent initial instruction knowledge
    trans_probs = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_mf = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    prev_action_1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        # --- Stage 1 Choice ---
        # MB uses current dynamic trans_probs
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_mb = trans_probs @ max_q_stage2
        
        q_net = w * q_mb + (1 - w) * q_mf
        
        if prev_action_1 != -1:
            q_net[prev_action_1] += perseveration
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning ---
        r = reward[trial]
        if r != -1:
            # 1. Update Values
            pe_2 = r - q_stage2[state_idx, action_2[trial]]
            q_stage2[state_idx, action_2[trial]] += lr_val * pe_2
            
            pe_1 = q_stage2[state_idx, action_2[trial]] - q_mf[action_1[trial]]
            q_mf[action_1[trial]] += lr_val * pe_1
            
            # 2. Update Transition Model
            # Update P(state | action_1)
            # Increase prob of observed state, decrease others
            # trans_probs[action, state] += alpha * (1 - trans_probs[action, state])
            curr_prob = trans_probs[action_1[trial], state_idx]
            trans_probs[action_1[trial], state_idx] += lr_trans * (1 - curr_prob)
            
            # Normalize the row to ensure sum = 1
            trans_probs[action_1[trial], :] /= np.sum(trans_probs[action_1[trial], :])

        prev_action_1 = action_1[trial]

    eps = 1e-10
    valid_trials = (action_1 != -1) & (state != -1) & (action_2 != -1) & (reward != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```