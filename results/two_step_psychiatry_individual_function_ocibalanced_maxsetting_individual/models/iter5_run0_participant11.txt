Here are three new cognitive models based on the participant data and task structure.

### Model 1: Independent Model-Based/Model-Free Control with Outcome-Dependent Stickiness
This model separates the inverse temperature parameters for the Model-Based (MB) and Model-Free (MF) systems, allowing their influence on the choice to vary independently rather than trading off via a weighting parameter `w`. Additionally, it incorporates the successful "Reward-Dependent Stickiness" mechanism, where perseveration is modulated by the previous outcome (Win-Stay vs. Lose-Shift).

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Independent MB/MF Betas with Reward-Dependent Stickiness.
    
    Instead of a weighting parameter 'w' that arbitrates between MB and MF values,
    this model assigns separate inverse temperatures (beta_mf, beta_mb) to each system.
    This allows the model to capture scenarios where both systems are confident or 
    both are uncertain independently. It also features outcome-dependent stickiness.

    Parameters:
    - learning_rate: [0, 1] Learning rate for Q-value updates.
    - beta_mf: [0, 10] Inverse temperature for the Model-Free component in Stage 1.
    - beta_mb: [0, 10] Inverse temperature for the Model-Based component in Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2 choices.
    - lambda_param: [0, 1] Eligibility trace decay (connecting Stage 2 outcome to Stage 1).
    - stick_win: [0, 5] Stickiness bonus after a rewarded trial.
    - stick_loss: [0, 5] Stickiness bonus after an unrewarded trial.
    """
    learning_rate, beta_mf, beta_mb, beta_2, lambda_param, stick_win, stick_loss = model_parameters
    n_trials = len(action_1)
    
    # Fixed transition probabilities as per task structure
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    last_reward_1 = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based Value Calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combine MF and MB values with separate betas
        # Note: We sum the weighted values. 
        # The effective Q is (beta_mf * Q_mf + beta_mb * Q_mb)
        # We apply softmax to this combined 'logit'.
        logit_1 = beta_mf * q_stage1_mf + beta_mb * q_stage1_mb
        
        # Add stickiness
        if last_action_1 != -1:
            bonus = stick_win if last_reward_1 == 1 else stick_loss
            logit_1[last_action_1] += bonus
            
        # Softmax for Stage 1
        # Subtract max for numerical stability
        exp_q1 = np.exp(logit_1 - np.max(logit_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Policy ---
        state_idx = state[trial]
        q_s2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta_2 * q_s2 - np.max(beta_2 * q_s2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        # TD Errors
        # Delta 1: Difference between Stage 2 value and Stage 1 MF value
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        # Delta 2: Prediction error from final reward
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Update Stage 1 MF (with eligibility trace lambda)
        q_stage1_mf[action_1[trial]] += learning_rate * (delta_stage1 + lambda_param * delta_stage2)
        
        # Update Stage 2 MF
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        last_action_1 = action_1[trial]
        last_reward_1 = reward[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model with Subjective Transition Probability
This model posits that the participant may not perceive the transition probabilities as exactly 0.7/0.3. Instead, they operate with a "subjective" probability parameter. This distortion affects the Model-Based value calculation. The model retains the standard hybrid architecture (using `w`) and generic stickiness.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Subjective Transition Probability.
    
    This model assumes the participant uses a subjective estimate of the 
    common transition probability (subjective_p) rather than the objective 0.7.
    This captures potential over-estimation (certainty) or under-estimation 
    (randomness) of the task structure in the Model-Based planner.

    Parameters:
    - learning_rate: [0, 1] Learning rate for Q-values.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - w: [0, 1] Weighting parameter (0 = MF, 1 = MB).
    - lambda_param: [0, 1] Eligibility trace decay.
    - stickiness: [0, 5] General stickiness bonus for repeating Stage 1 choice.
    - subjective_p: [0, 1] Subjective probability of the 'common' transition.
    """
    learning_rate, beta_1, beta_2, w, lambda_param, stickiness, subjective_p = model_parameters
    n_trials = len(action_1)
    
    # Construct subjective transition matrix
    # Row 0: Space A -> [Common, Rare] (Planet 0, Planet 1)
    # Row 1: Space B -> [Rare, Common] (Planet 0, Planet 1)
    # Assuming A=0->0 is common, B=1->1 is common based on description.
    t_mat = np.array([
        [subjective_p, 1.0 - subjective_p],
        [1.0 - subjective_p, subjective_p]
    ])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        
        # Calculate MB values using subjective transitions
        q_stage1_mb = t_mat @ max_q_stage2
        
        # Hybrid Q-value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta_1 * q_net - np.max(beta_1 * q_net))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Policy ---
        state_idx = state[trial]
        q_s2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta_2 * q_s2 - np.max(beta_2 * q_s2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        q_stage1_mf[action_1[trial]] += learning_rate * (delta_stage1 + lambda_param * delta_stage2)
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        last_action_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model with Epsilon-Greedy Mixture (Lapse Rate)
This model assumes that on a small fraction of trials (`epsilon`), the participant acts randomly regardless of the value estimates. This "lapse rate" accounts for noise, inattention, or exploration that the softmax function cannot capture (e.g., occasional random choices despite high beta).

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Epsilon-Greedy Mixture.
    
    Incorporates a lapse rate (epsilon) which mixes the softmax probability 
    distribution with a uniform distribution. This makes the model robust to 
    outliers where the participant chooses a low-value option that would 
    otherwise have near-zero probability.

    Parameters:
    - learning_rate: [0, 1] Learning rate for Q-values.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - w: [0, 1] Weighting parameter (0 = MF, 1 = MB).
    - lambda_param: [0, 1] Eligibility trace decay.
    - stickiness: [0, 5] General stickiness bonus.
    - epsilon: [0, 1] Probability of choosing uniformly at random (lapse rate).
    """
    learning_rate, beta_1, beta_2, w, lambda_param, stickiness, epsilon = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta_1 * q_net - np.max(beta_1 * q_net))
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        # Apply Epsilon Mixture
        probs_1 = (1 - epsilon) * probs_1 + epsilon * 0.5
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Policy ---
        state_idx = state[trial]
        q_s2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta_2 * q_s2 - np.max(beta_2 * q_s2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        # Apply Epsilon Mixture to Stage 2 as well
        probs_2 = (1 - epsilon) * probs_2 + epsilon * 0.5
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        q_stage1_mf[action_1[trial]] += learning_rate * (delta_stage1 + lambda_param * delta_stage2)
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        last_action_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```