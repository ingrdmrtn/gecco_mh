Here are three cognitive models expressed as Python functions, designed to capture the specific behavioral patterns (particularly the strong perseveration/streaks and switching logic) observed in the participant data.

### Model 1: Q-learning with Decay and Choice Stickiness
This model incorporates a **decay mechanism for unchosen options** alongside choice stickiness. While the agent reinforces the chosen path based on rewards, the values of unchosen Stage 2 aliens passively decay. This mechanism helps explain how the participant might eventually switch from a "stuck" behavior (long runs) when the current option's value degrades or unchosen options become relatively more attractive due to the dynamic environment.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Q-learning with Decay of Unchosen Options and Choice Stickiness.
    
    Incorporates a forgetting process where the value of the unchosen alien 
    in Stage 2 decays towards 0 on each trial. This counteracts the 
    stickiness over time, facilitating switches in a changing environment.
    
    Parameters:
    lr (float): Learning rate [0, 1].
    beta (float): Inverse temperature [0, 10].
    w (float): MB/MF mixing weight [0, 1].
    lam (float): Eligibility trace parameter [0, 1].
    stick (float): Choice stickiness/perseveration [0, 5].
    decay (float): Decay rate for unchosen Stage 2 action [0, 1].
    """
    lr, beta, w, lam, stick, decay = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track the previous choice for stickiness
    last_action_1 = -1
    
    log_loss = 0
    eps = 1e-10
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based value: Transition * Max(Stage 2 Values)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Calculate logits
        logits_1 = beta * q_net
        if last_action_1 != -1:
            logits_1[last_action_1] += stick
            
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        # Accumulate loss
        if action_1[trial] != -1:
            log_loss -= np.log(probs_1[action_1[trial]] + eps)
            last_action_1 = action_1[trial]
        
        # --- Stage 2 Policy ---
        state_idx = state[trial]
        # Skip if state is missing (rare but possible in some formats)
        if state_idx == -1: 
            continue
            
        logits_2 = beta * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        if action_2[trial] != -1:
            log_loss -= np.log(probs_2[action_2[trial]] + eps)
            
            # --- Updates ---
            r = reward[trial]
            if r == -1: r = 0 # Handle missing reward
            
            a1 = action_1[trial]
            a2 = action_2[trial]
            
            # TD Errors
            delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
            delta_stage2 = r - q_stage2_mf[state_idx, a2]
            
            # Update Stage 1 MF
            q_stage1_mf[a1] += lr * delta_stage1 + lr * lam * delta_stage2
            
            # Update Stage 2 MF (Chosen)
            q_stage2_mf[state_idx, a2] += lr * delta_stage2
            
            # Decay Stage 2 MF (Unchosen)
            q_stage2_mf[state_idx, 1 - a2] *= (1.0 - decay)

    return log_loss
```

### Model 2: Expected-Value Model-Based Controller
This model alters the Model-Based planning assumption. Instead of assuming the agent perfectly maximizes the next stage (`max(Q)`), it assumes the agent calculates the **Expected Value** of the next stage based on their own softmax choice probabilities. This "softer" planning assumption often fits human data better than the "max" operator, especially when behavior is noisy or exploratory.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Expected-Value Model-Based Controller with Stickiness.
    
    Replaces the standard 'max' operator in Model-Based planning with an 
    expectation over the softmax probabilities of Stage 2 actions. This 
    assumes the agent plans based on the expected return of their own 
    stochastic policy.
    
    Parameters:
    lr (float): Learning rate [0, 1].
    beta (float): Inverse temperature [0, 10].
    w (float): MB/MF mixing weight [0, 1].
    lam (float): Eligibility trace parameter [0, 1].
    stick (float): Choice stickiness/perseveration [0, 5].
    """
    lr, beta, w, lam, stick = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    log_loss = 0
    eps = 1e-10
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Calculate Expected Value of Stage 2 for MB planning
        # V(s) = sum_a P(a|s) * Q(s,a)
        
        # State 0 Expectation
        exp_q_s0 = np.exp(beta * q_stage2_mf[0])
        probs_s0 = exp_q_s0 / np.sum(exp_q_s0)
        val_s0 = np.sum(probs_s0 * q_stage2_mf[0])
        
        # State 1 Expectation
        exp_q_s1 = np.exp(beta * q_stage2_mf[1])
        probs_s1 = exp_q_s1 / np.sum(exp_q_s1)
        val_s1 = np.sum(probs_s1 * q_stage2_mf[1])
        
        # MB Values based on Expected Values
        q_stage1_mb = transition_matrix @ np.array([val_s0, val_s1])
        
        # Net value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Logits with stickiness
        logits_1 = beta * q_net
        if last_action_1 != -1:
            logits_1[last_action_1] += stick
            
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        if action_1[trial] != -1:
            log_loss -= np.log(probs_1[action_1[trial]] + eps)
            last_action_1 = action_1[trial]
            
        # --- Stage 2 Policy ---
        state_idx = state[trial]
        if state_idx == -1: continue

        logits_2 = beta * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        if action_2[trial] != -1:
            log_loss -= np.log(probs_2[action_2[trial]] + eps)
            
            # --- Updates ---
            r = reward[trial]
            if r == -1: r = 0
            
            a1 = action_1[trial]
            a2 = action_2[trial]
            
            delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
            delta_stage2 = r - q_stage2_mf[state_idx, a2]
            
            q_stage1_mf[a1] += lr * delta_stage1 + lr * lam * delta_stage2
            q_stage2_mf[state_idx, a2] += lr * delta_stage2
            
    return log_loss
```

### Model 3: Cumulative Streak Stickiness Model
The participant demonstrates very long "runs" of choosing the same spaceship (e.g., trials 107-151). A simple constant stickiness parameter might not be sufficient to model this deep perseveration. This model introduces **cumulative stickiness**, where the tendency to repeat a choice increases linearly with the length of the current streak.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Cumulative Streak Stickiness Model.
    
    The stickiness bonus is not constant but grows linearly with the number 
    of consecutive choices of the same spaceship. This captures the 
    'momentum' or deep habit formation observed in long choice runs.
    
    Parameters:
    lr (float): Learning rate [0, 1].
    beta (float): Inverse temperature [0, 10].
    w (float): MB/MF mixing weight [0, 1].
    lam (float): Eligibility trace parameter [0, 1].
    stick_base (float): Base stickiness bonus [0, 5].
    stick_growth (float): Additional stickiness per consecutive choice [0, 2].
    """
    lr, beta, w, lam, stick_base, stick_growth = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    streak_count = 0
    
    log_loss = 0
    eps = 1e-10
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net
        
        # Apply cumulative stickiness
        if last_action_1 != -1:
            bonus = stick_base + stick_growth * streak_count
            logits_1[last_action_1] += bonus
            
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        if action_1[trial] != -1:
            log_loss -= np.log(probs_1[action_1[trial]] + eps)
            
            # Update streak logic
            if action_1[trial] == last_action_1:
                streak_count += 1
            else:
                streak_count = 1
            last_action_1 = action_1[trial]
        
        # --- Stage 2 Policy ---
        state_idx = state[trial]
        if state_idx == -1: continue

        logits_2 = beta * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        if action_2[trial] != -1:
            log_loss -= np.log(probs_2[action_2[trial]] + eps)
            
            # --- Updates ---
            r = reward[trial]
            if r == -1: r = 0
            
            a1 = action_1[trial]
            a2 = action_2[trial]
            
            delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
            delta_stage2 = r - q_stage2_mf[state_idx, a2]
            
            q_stage1_mf[a1] += lr * delta_stage1 + lr * lam * delta_stage2
            q_stage2_mf[state_idx, a2] += lr * delta_stage2
            
    return log_loss
```