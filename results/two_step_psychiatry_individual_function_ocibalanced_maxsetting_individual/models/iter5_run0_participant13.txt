Here are three new cognitive models for the two-step decision-making task. These models are designed to capture the specific behavioral patterns observed, such as the long streaks of perseveration and the potential asymmetry in learning from rewards versus omissions.

### Model 1: Asymmetric Q-Learning with Outcome-Dependent Perseveration
This model hypothesizes that the participant updates their value estimates differently depending on whether the outcome was better or worse than expected (positive vs. negative prediction errors). This is combined with outcome-dependent perseveration to account for the "sticky" behavior observed after rewards and losses.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Model 1: Asymmetric Q-Learning with Outcome-Dependent Perseveration.
    
    This model assumes the participant learns differently from positive and negative
    prediction errors (Asymmetric Learning). It also includes outcome-dependent 
    perseveration to capture the tendency to repeat choices based on the previous 
    outcome, distinguishing between 'Win-Stay' and 'Lose-Stay' biases.
    
    Parameters:
    alpha_pos: [0, 1] - Learning rate for positive prediction errors.
    alpha_neg: [0, 1] - Learning rate for negative prediction errors.
    beta: [0, 10] - Inverse temperature.
    pers_win: [-3, 3] - Bias to repeat Stage 1 choice after a Reward.
    pers_loss: [-3, 3] - Bias to repeat Stage 1 choice after No Reward.
    """
    alpha_pos, alpha_neg, beta, pers_win, pers_loss = model_parameters
    n_trials = len(action_1)
    
    # Initialize Q-values to 0.5 (neutral)
    q_stage1 = np.zeros(2) + 0.5
    q_stage2 = np.zeros((2, 2)) + 0.5
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1
    prev_reward = -1
    
    for t in range(n_trials):
        # Stage 1 Choice
        logits_1 = beta * q_stage1
        if prev_a1 != -1:
            if prev_reward == 1:
                logits_1[prev_a1] += pers_win
            else:
                logits_1[prev_a1] += pers_loss
        
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[t] = probs_1[action_1[t]]
        
        a1 = action_1[t]
        s_idx = state[t]
        
        # Stage 2 Choice
        logits_2 = beta * q_stage2[s_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[t] = probs_2[action_2[t]]
        
        a2 = action_2[t]
        r = reward[t]
        
        # Update Stage 1 (TD(0))
        delta1 = q_stage2[s_idx, a2] - q_stage1[a1]
        if delta1 > 0:
            q_stage1[a1] += alpha_pos * delta1
        else:
            q_stage1[a1] += alpha_neg * delta1
            
        # Update Stage 2
        delta2 = r - q_stage2[s_idx, a2]
        if delta2 > 0:
            q_stage2[s_idx, a2] += alpha_pos * delta2
        else:
            q_stage2[s_idx, a2] += alpha_neg * delta2
            
        prev_a1 = a1
        prev_reward = r

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Decay Q-Learning with Choice Kernel
This model replaces the simple 1-step perseveration with a "Choice Kernel" (Habit) mechanism. The participant's long streaks suggest a build-up of habit strength over multiple trials. This is combined with the "Decay" mechanism on Q-values, which helps the model adapt to the drifting reward probabilities by slowly forgetting unchosen option values.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model 2: Decay Q-Learning with Choice Kernel.
    
    This model combines the 'Decay' mechanism (which helps with non-stationarity by 
    drifting unchosen values to 0.5) with a 'Choice Kernel' (Habit). The Choice Kernel 
    tracks the history of choices with an exponential decay, providing a more 
    nuanced measure of perseveration than simple 1-step stickiness, potentially 
    explaining the long streaks in behavior.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate for Q-values.
    decay_rate: [0, 1] - Rate at which unchosen Q-values decay to 0.5.
    beta: [0, 10] - Inverse temperature for Q-values.
    k_decay: [0, 1] - Decay rate for the choice kernel (1 = full forgetting).
    k_weight: [-5, 5] - Weight of the choice kernel in the decision.
    """
    learning_rate, decay_rate, beta, k_decay, k_weight = model_parameters
    n_trials = len(action_1)
    
    q_stage1 = np.zeros(2) + 0.5
    q_stage2 = np.zeros((2, 2)) + 0.5
    choice_kernel = np.zeros(2) # For stage 1 choices
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for t in range(n_trials):
        # Stage 1 Choice
        # Logits combine Q-values and Choice Kernel
        logits_1 = beta * q_stage1 + k_weight * choice_kernel
        
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[t] = probs_1[action_1[t]]
        
        a1 = action_1[t]
        s_idx = state[t]
        
        # Stage 2 Choice (Standard Softmax on Q)
        logits_2 = beta * q_stage2[s_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[t] = probs_2[action_2[t]]
        
        a2 = action_2[t]
        r = reward[t]
        
        # Update Choice Kernel
        # CK updates: Chosen gets reinforced, Unchosen decays
        # Standard implementation: CK(a) = (1-eta)*CK(a) + eta*Ind(a)
        choice_kernel = (1 - k_decay) * choice_kernel
        choice_kernel[a1] += k_decay * 1.0

        # Update Q-values with Decay
        unchosen_a1 = 1 - a1
        q_stage1[unchosen_a1] += decay_rate * (0.5 - q_stage1[unchosen_a1])

        unchosen_a2 = 1 - a2
        q_stage2[s_idx, unchosen_a2] += decay_rate * (0.5 - q_stage2[s_idx, unchosen_a2])

        delta1 = q_stage2[s_idx, a2] - q_stage1[a1]
        q_stage1[a1] += learning_rate * delta1

        delta2 = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += learning_rate * delta2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Decay Q-Learning with Outcome-Dependent Perseveration
This model extends the best-performing "Decay Q-Learning" by adding a Model-Based (MB) component. While the participant is largely Model-Free, adding a weighted MB value allows the model to capture any sensitivity to the transition structure (Common vs. Rare). It retains the successful value decay and outcome-dependent perseveration mechanisms.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model 3: Hybrid Decay Q-Learning with Outcome-Dependent Perseveration.
    
    This model extends the best-performing 'Decay Q-Learning' by adding a Model-Based 
    (MB) component. While the participant behavior is largely Model-Free, a small 
    MB weight (w) might better capture the reaction to rare transitions. 
    It retains the successful decay mechanism and outcome-dependent perseveration.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate for Q-values.
    decay_rate: [0, 1] - Rate at which unchosen Q-values decay to 0.5.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - Weight of Model-Based values (0 = Pure MF, 1 = Pure MB).
    pers_win: [-3, 3] - Bias to repeat Stage 1 choice after a Reward.
    pers_loss: [-3, 3] - Bias to repeat Stage 1 choice after No Reward.
    """
    learning_rate, decay_rate, beta, w, pers_win, pers_loss = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]]) # Fixed transition probs
    
    q_stage1_mf = np.zeros(2) + 0.5
    q_stage2 = np.zeros((2, 2)) + 0.5
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1
    prev_reward = -1
    
    for t in range(n_trials):
        # Calculate MB values
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Q-values
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Stage 1 Choice
        logits_1 = beta * q_net_1
        if prev_a1 != -1:
            if prev_reward == 1:
                logits_1[prev_a1] += pers_win
            else:
                logits_1[prev_a1] += pers_loss
        
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[t] = probs_1[action_1[t]]
        
        a1 = action_1[t]
        s_idx = state[t]
        
        # Stage 2 Choice
        logits_2 = beta * q_stage2[s_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[t] = probs_2[action_2[t]]
        
        a2 = action_2[t]
        r = reward[t]
        
        # Updates with Decay
        unchosen_a1 = 1 - a1
        q_stage1_mf[unchosen_a1] += decay_rate * (0.5 - q_stage1_mf[unchosen_a1])

        unchosen_a2 = 1 - a2
        q_stage2[s_idx, unchosen_a2] += decay_rate * (0.5 - q_stage2[s_idx, unchosen_a2])

        delta1 = q_stage2[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta1

        delta2 = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += learning_rate * delta2
        
        prev_a1 = a1
        prev_reward = r

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```