Here are the 3 new cognitive models.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF model with Outcome-Dependent Stickiness.
    
    Distinguishes between perseveration after a reward (Win-Stay) and 
    perseveration after no reward (Lose-Stay/Shift). This allows the model 
    to capture different stickiness dynamics depending on the previous outcome.

    Parameters:
    - learning_rate: [0, 1] Learning rate for Q-values.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weight of Model-Based system (0=MF, 1=MB).
    - lam: [0, 1] Eligibility trace parameter.
    - stick_win: [0, 5] Stickiness bonus applied to the previous action if it was rewarded.
    - stick_lose: [0, 5] Stickiness bonus applied to the previous action if it was unrewarded.
    """
    learning_rate, beta, w, lam, stick_win, stick_lose = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_mf = np.zeros(2)
    q2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1
    last_reward = 0
    eps = 1e-10
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])
        
        # Skip invalid trials
        if a1 == -1:
            continue
            
        # Stage 1 Policy
        max_q2 = np.max(q2, axis=1)
        q_mb = transition_matrix @ max_q2
        q_net = w * q_mb + (1 - w) * q_mf
        
        # Apply outcome-dependent stickiness
        if last_action_1 != -1:
            if last_reward == 1:
                q_net[last_action_1] += stick_win
            else:
                q_net[last_action_1] += stick_lose
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Stage 2 Policy
        exp_q2 = np.exp(beta * q2[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # Learning
        # Stage 1 RPE (TD(0)) using Stage 2 Q-value
        delta_1 = q2[s, a2] - q_mf[a1]
        q_mf[a1] += learning_rate * delta_1
        
        # Stage 2 RPE
        delta_2 = r - q2[s, a2]
        q2[s, a2] += learning_rate * delta_2
        
        # Eligibility trace update for Stage 1
        q_mf[a1] += learning_rate * lam * delta_2
        
        last_action_1 = a1
        last_reward = r
        
    mask = p_choice_1 > 0
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF model with Habitual Stickiness (Choice Trace).
    
    Uses an exponentially decaying trace of past choices to bias the current decision.
    Unlike simple stickiness (which only considers the last trial), this models 
    habit formation or momentum over multiple trials.

    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weight of Model-Based system.
    - lam: [0, 1] Eligibility trace.
    - habit_amp: [0, 5] Strength/Amplitude of the habit bias.
    - habit_decay: [0, 1] Decay rate of the choice trace (1=no decay, 0=instant decay).
    """
    learning_rate, beta, w, lam, habit_amp, habit_decay = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_mf = np.zeros(2)
    q2 = np.zeros((2, 2))
    habit_trace = np.zeros(2)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    eps = 1e-10
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])
        
        if a1 == -1:
            continue
            
        # Stage 1 Policy
        max_q2 = np.max(q2, axis=1)
        q_mb = transition_matrix @ max_q2
        q_net = w * q_mb + (1 - w) * q_mf
        
        # Add habit bias from choice trace
        q_net += habit_amp * habit_trace
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Stage 2 Policy
        exp_q2 = np.exp(beta * q2[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # Learning
        delta_1 = q2[s, a2] - q_mf[a1]
        q_mf[a1] += learning_rate * delta_1
        
        delta_2 = r - q2[s, a2]
        q2[s, a2] += learning_rate * delta_2
        
        q_mf[a1] += learning_rate * lam * delta_2
        
        # Update Habit Trace
        habit_trace *= habit_decay
        habit_trace[a1] += 1
        
    mask = p_choice_1 > 0
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF model with Uncertainty-Based Exploration.
    
    Incorporates an exploration bonus ('phi') that increases for options that 
    have not been chosen recently (linear count of trials since last choice).
    This promotes exploring the road less traveled.

    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weight of Model-Based system.
    - lam: [0, 1] Eligibility trace.
    - stickiness: [0, 5] Standard choice perseveration (last trial).
    - phi: [0, 5] Exploration bonus weight (scales with time since last choice).
    """
    learning_rate, beta, w, lam, stickiness, phi = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_mf = np.zeros(2)
    q2 = np.zeros((2, 2))
    uncertainty = np.zeros(2)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1
    eps = 1e-10
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])
        
        if a1 == -1:
            continue
            
        # Stage 1 Policy
        max_q2 = np.max(q2, axis=1)
        q_mb = transition_matrix @ max_q2
        q_net = w * q_mb + (1 - w) * q_mf
        
        # Add Stickiness
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        # Add Uncertainty Bonus
        q_net += phi * uncertainty
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Stage 2 Policy
        exp_q2 = np.exp(beta * q2[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # Learning
        delta_1 = q2[s, a2] - q_mf[a1]
        q_mf[a1] += learning_rate * delta_1
        
        delta_2 = r - q2[s, a2]
        q2[s, a2] += learning_rate * delta_2
        
        q_mf[a1] += learning_rate * lam * delta_2
        
        # Update Uncertainty State
        last_action_1 = a1
        uncertainty += 1
        uncertainty[a1] = 0
        
    mask = p_choice_1 > 0
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss
```