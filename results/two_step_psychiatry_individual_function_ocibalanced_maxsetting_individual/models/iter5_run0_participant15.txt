Here are three new cognitive models based on the participant data and the two-step task structure.

### Model 1: Hybrid MB/MF with Asymmetric Learning
This model hypothesizes that the participant uses both Model-Based (planning) and Model-Free (habitual) strategies, but updates their value estimates differently depending on whether the outcome was better or worse than expected (asymmetric learning). This captures potential confirmation bias or differential sensitivity to gains versus losses.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF with Asymmetric Learning Rates.
    
    Combines Model-Based planning and Model-Free habits.
    Uses asymmetric learning rates (alpha_pos, alpha_neg) for value updates,
    allowing the model to learn differently from positive (RPE > 0) vs 
    negative (RPE < 0) prediction errors.
    
    Parameters:
    - alpha_pos: [0, 1] Learning rate for positive prediction errors.
    - alpha_neg: [0, 1] Learning rate for negative prediction errors.
    - beta_mb: [0, 10] Inverse temperature for Stage 1 Model-Based values.
    - beta_mf: [0, 10] Inverse temperature for Stage 1 Model-Free values.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - stickiness: [0, 10] Choice perseverance bonus for Stage 1.
    """
    alpha_pos, alpha_neg, beta_mb, beta_mf, beta_2, stickiness = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2) + 0.5
    q_stage2_mf = np.zeros((2, 2)) + 0.5
    
    last_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based Value calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combined Value (Hybrid)
        logits_1 = beta_mb * q_stage1_mb + beta_mf * q_stage1_mf
        
        # Stickiness
        if last_action_1 != -1:
            logits_1[last_action_1] += stickiness
            
        # Probability calculation
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        if action_1[trial] != -1:
            p_choice_1[trial] = probs_1[action_1[trial]]
            a1 = action_1[trial]
        else:
            p_choice_1[trial] = 1.0
            last_action_1 = -1
            continue 

        # --- Stage 2 Choice ---
        state_idx = state[trial]
        logits_2 = beta_2 * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        if action_2[trial] != -1:
            p_choice_2[trial] = probs_2[action_2[trial]]
            a2 = action_2[trial]
            r = reward[trial]
            
            # --- Updates ---
            # Stage 2 Update (RPE)
            delta_2 = r - q_stage2_mf[state_idx, a2]
            alpha_2 = alpha_pos if delta_2 >= 0 else alpha_neg
            q_stage2_mf[state_idx, a2] += alpha_2 * delta_2
            
            # Stage 1 MF Update (TD(1) style - direct reinforcement from reward)
            delta_1 = r - q_stage1_mf[a1]
            alpha_1 = alpha_pos if delta_1 >= 0 else alpha_neg
            q_stage1_mf[a1] += alpha_1 * delta_1
            
        else:
            p_choice_2[trial] = 1.0
        
        last_action_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Pure Model-Free with Eligibility Traces and Forgetting
This model tests the hypothesis that the participant ignores the transition probabilities (Pure Model-Free) but compensates with sophisticated temporal credit assignment (Eligibility Traces) and handles the drifting reward probabilities via passive forgetting of unchosen options.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free with Eligibility Traces and Forgetting.
    
    A pure habit-learning model (no transition model) that accounts for 
    temporal credit assignment via eligibility traces (lambda) and 
    environmental drift via passive forgetting.
    
    Parameters:
    - alpha: [0, 1] Learning rate.
    - lambda_trace: [0, 1] Eligibility trace decay (credit assignment to Stage 1).
    - forget: [0, 1] Decay rate of unchosen Stage 2 values towards 0.5.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - stickiness: [0, 10] Choice perseverance bonus for Stage 1.
    """
    alpha, lambda_trace, forget, beta_1, beta_2, stickiness = model_parameters
    n_trials = len(action_1)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2) + 0.5
    q_stage2 = np.zeros((2, 2)) + 0.5
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        # Stage 1 Choice (Pure MF)
        logits_1 = beta_1 * q_stage1
        if last_action_1 != -1:
            logits_1[last_action_1] += stickiness
        
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        if action_1[trial] != -1:
            p_choice_1[trial] = probs_1[action_1[trial]]
            a1 = action_1[trial]
        else:
            p_choice_1[trial] = 1.0
            last_action_1 = -1
            continue

        # Stage 2 Choice
        state_idx = state[trial]
        
        # Forgetting: Decay all Stage 2 values towards 0.5 before choice
        # This simulates uncertainty increasing over time for unvisited states
        q_stage2 = (1 - forget) * q_stage2 + forget * 0.5
        
        logits_2 = beta_2 * q_stage2[state_idx]
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        if action_2[trial] != -1:
            p_choice_2[trial] = probs_2[action_2[trial]]
            a2 = action_2[trial]
            r = reward[trial]
            
            # Updates
            # TD Error Stage 2
            delta_2 = r - q_stage2[state_idx, a2]
            
            # TD Error Stage 1 (SARSA-like: driven by value of state reached)
            delta_1 = q_stage2[state_idx, a2] - q_stage1[a1]
            
            # Update Stage 1: Direct TD update + Eligibility Trace from Stage 2
            q_stage1[a1] += alpha * delta_1 + alpha * lambda_trace * delta_2
            
            # Update Stage 2
            q_stage2[state_idx, a2] += alpha * delta_2
            
        else:
            p_choice_2[trial] = 1.0
            
        last_action_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid MB/MF with Outcome-Dependent Stickiness
This model proposes that the participant's tendency to repeat a choice (stickiness) is not constant but depends on the previous outcome. A "Win-Stay" tendency might differ from a "Lose-Stay" tendency, operating alongside standard Model-Based and Model-Free learning.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF with Outcome-Dependent Stickiness.
    
    A hybrid model where the tendency to repeat the previous Stage 1 choice 
    depends on whether the previous trial resulted in a reward (Win-Stay) 
    or not (Lose-Stay/Switch), captured by distinct stickiness parameters.
    
    Parameters:
    - alpha: [0, 1] Learning rate.
    - beta_mb: [0, 10] Inverse temperature for Stage 1 Model-Based values.
    - beta_mf: [0, 10] Inverse temperature for Stage 1 Model-Free values.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - stick_win: [0, 10] Stickiness after a rewarded trial.
    - stick_loss: [0, 10] Stickiness after an unrewarded trial.
    """
    alpha, beta_mb, beta_mf, beta_2, stick_win, stick_loss = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2) + 0.5
    q_stage2_mf = np.zeros((2, 2)) + 0.5
    
    last_action_1 = -1
    last_reward = -1 # Initialize as missing
    
    for trial in range(n_trials):
        # Stage 1
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        logits_1 = beta_mb * q_stage1_mb + beta_mf * q_stage1_mf
        
        # Outcome-Dependent Stickiness
        if last_action_1 != -1:
            if last_reward == 1:
                logits_1[last_action_1] += stick_win
            elif last_reward == 0:
                logits_1[last_action_1] += stick_loss
            # If last_reward == -1 (missing), no stickiness applied
            
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        if action_1[trial] != -1:
            p_choice_1[trial] = probs_1[action_1[trial]]
            a1 = action_1[trial]
        else:
            p_choice_1[trial] = 1.0
            last_action_1 = -1
            last_reward = -1
            continue
            
        # Stage 2
        state_idx = state[trial]
        logits_2 = beta_2 * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        if action_2[trial] != -1:
            p_choice_2[trial] = probs_2[action_2[trial]]
            a2 = action_2[trial]
            r = reward[trial]
            
            # Updates
            delta_2 = r - q_stage2_mf[state_idx, a2]
            q_stage2_mf[state_idx, a2] += alpha * delta_2
            
            # Stage 1 MF Update (TD(1) driven by reward)
            delta_1 = r - q_stage1_mf[a1]
            q_stage1_mf[a1] += alpha * delta_1
            
            last_reward = r
        else:
            p_choice_2[trial] = 1.0
            last_reward = -1 
            
        last_action_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```