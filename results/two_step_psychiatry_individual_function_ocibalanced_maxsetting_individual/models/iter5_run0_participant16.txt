Here are three new cognitive models for the two-step task, designed based on the participant's switching behavior and the "best model" feedback.

### Model 1: Model-Free RL with Independent Stage Parameters
This model hypothesizes that the participant treats the spaceship choice (Stage 1) and the alien choice (Stage 2) as distinct cognitive problems requiring different learning rates and noise levels. For instance, they might learn rapidly about aliens (high `alpha_2`) but be slow to change spaceship preferences (low `alpha_1`), or explore spaceships more randomly (low `beta_1`) than aliens. It uses a SARSA(0) update for Stage 1 to chain values.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free RL with Separate Learning Rates and Betas for each stage.
    
    Distinguishes between the plasticity (alpha) and decision noise (beta) of the
    navigation step (Stage 1) versus the harvesting step (Stage 2).
    
    Parameters:
    - alpha_1: [0, 1] Learning rate for Stage 1 (Spaceship choice).
    - alpha_2: [0, 1] Learning rate for Stage 2 (Alien choice).
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    """
    alpha_1, alpha_2, beta_1, beta_2 = model_parameters
    n_trials = len(action_1)
    
    # Q-values
    q_stage1 = np.zeros(2)      # Values for Spaceships 0, 1
    q_stage2 = np.zeros((2, 2)) # Values for Planet 0 (Aliens 0,1) and Planet 1 (Aliens 0,1)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        # Handle missing/invalid data
        if action_1[trial] == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue
            
        # --- Stage 1 Decision ---
        exp_q1 = np.exp(beta_1 * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        
        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta_2 * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        # Stage 2 Update (Direct Reward)
        delta_stage2 = reward[trial] - q_stage2[state_idx, action_2[trial]]
        q_stage2[state_idx, action_2[trial]] += alpha_2 * delta_stage2
        
        # Stage 1 Update (SARSA(0): Update towards value of chosen Stage 2 option)
        # Note: We use the value of the state-action pair actually experienced in Stage 2
        target_stage1 = q_stage2[state_idx, action_2[trial]]
        delta_stage1 = target_stage1 - q_stage1[action_1[trial]]
        q_stage1[action_1[trial]] += alpha_1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model-Based/Model-Free RL with Passive Decay
This model introduces a memory decay mechanism. The participant switches spaceships frequently, which might be explained by the value of unchosen options "decaying" or being forgotten over time, prompting a return to them (or abandonment of the current one). This is combined with a Model-Based weight `w` to capture structural knowledge.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based/Model-Free RL with Passive Decay.
    
    Incorporates a decay parameter that degrades Q-values of unchosen options,
    promoting exploration or switching, combined with model-based planning.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for value updates.
    - beta: [0, 10] Inverse temperature for decision making (shared).
    - w: [0, 1] Weight of model-based values (0=MF, 1=MB).
    - decay: [0, 1] Decay rate for unchosen action values (0=no decay, 1=full reset).
    """
    learning_rate, beta, w, decay = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        if action_1[trial] == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # --- Stage 1 Decision (Hybrid) ---
        # Model-Based Value: Expected max value of next stage
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        
        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2[state_idx, action_2[trial]]
        q_stage2[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Stage 1 MF Update (SARSA(0))
        delta_stage1 = q_stage2[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # --- Passive Decay ---
        # Decay unchosen spaceship value
        unchosen_s1 = 1 - action_1[trial]
        q_stage1_mf[unchosen_s1] *= (1 - decay)
        
        # Decay unchosen alien in current state
        unchosen_s2 = 1 - action_2[trial]
        q_stage2[state_idx, unchosen_s2] *= (1 - decay)
        
        # Decay all aliens in the unvisited state
        unvisited_state = 1 - state_idx
        q_stage2[unvisited_state, :] *= (1 - decay)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid MB/MF with Asymmetric Learning and Separate Betas
This model extends the "best model so far" (which was MF with asymmetric learning and separate betas) by adding a Model-Based component (`w`). It tests whether the participant's robust switching and learning is better explained by a combination of structural knowledge (MB) and a reward-sensitive, asymmetric MF system. The MF part of Stage 1 uses a direct reward update (TD(1)-like) to mimic the behavior of the successful feedback model.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF RL with Asymmetric Learning and Separate Betas.
    
    Combines Model-Based planning with a Model-Free system that learns 
    from positive and negative prediction errors at different rates,
    and uses distinct noise levels for each stage.
    
    Parameters:
    - alpha_pos: [0, 1] Learning rate for positive prediction errors.
    - alpha_neg: [0, 1] Learning rate for negative prediction errors.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - w: [0, 1] Weight of model-based values (0=MF, 1=MB).
    """
    alpha_pos, alpha_neg, beta_1, beta_2, w = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        if action_1[trial] == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # --- Stage 1 Decision (Hybrid) ---
        # Model-Based Value
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        
        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta_2 * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        # Stage 2 Update (Asymmetric)
        delta_stage2 = reward[trial] - q_stage2[state_idx, action_2[trial]]
        lr_s2 = alpha_pos if delta_stage2 >= 0 else alpha_neg
        q_stage2[state_idx, action_2[trial]] += lr_s2 * delta_stage2
        
        # Stage 1 MF Update (Direct Reward / TD(1) Asymmetric)
        # Using direct reward update for MF part as in the 'best model' reference
        delta_stage1 = reward[trial] - q_stage1_mf[action_1[trial]]
        lr_s1 = alpha_pos if delta_stage1 >= 0 else alpha_neg
        q_stage1_mf[action_1[trial]] += lr_s1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```