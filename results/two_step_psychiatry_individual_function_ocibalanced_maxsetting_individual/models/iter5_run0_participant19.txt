Here are the three proposed cognitive models.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Model 1: Dual-Stage Learning Rates with Stickiness.
    
    A Model-Free reinforcement learning model that uses separate learning rates 
    for the first stage (Spaceship choice) and the second stage (Alien choice).
    This accounts for the possibility that the participant learns the value of 
    transition-predicting cues (Spaceships) at a different rate than 
    reward-generating cues (Aliens).

    Parameters:
    - alpha_1: [0, 1] Learning rate for Stage 1 (Spaceship choice).
    - alpha_2: [0, 1] Learning rate for Stage 2 (Alien choice).
    - beta: [0, 10] Inverse temperature (softmax sensitivity).
    - stickiness: [0, 10] Choice stickiness (perseverance) for Stage 1.
    """
    alpha_1, alpha_2, beta, stickiness = model_parameters
    n_trials = len(action_1)
    
    # Initialize Q-values
    # Stage 1: 2 choices (Spaceships)
    q_stage1 = np.zeros(2)
    # Stage 2: 2 states (Planets) x 2 choices (Aliens)
    q_stage2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1
    
    for t in range(n_trials):
        
        # --- Stage 1 Decision ---
        logits_1 = beta * q_stage1.copy()
        
        # Apply stickiness to the previously chosen spaceship
        if prev_a1 != -1:
            logits_1[prev_a1] += stickiness
            
        # Softmax for Stage 1
        logits_1 = logits_1 - np.max(logits_1) # Numerical stability
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]
        
        # --- Stage 2 Decision ---
        s_idx = state[t]
        logits_2 = beta * q_stage2[s_idx]
        
        # Softmax for Stage 2
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]
        
        # --- Learning ---
        r = reward[t]
        
        # Update Stage 2 (Alien) using alpha_2
        # PE = Reward - Q_Stage2
        pe_2 = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += alpha_2 * pe_2
        
        # Update Stage 1 (Spaceship) using alpha_1
        # TD-0 update: Target is the value of the state arrived at (Q_Stage2 of chosen action)
        # Note: In standard TD(0) for this task, target is often Q_Stage2[s, a2]
        pe_1 = q_stage2[s_idx, a2] - q_stage1[a1]
        q_stage1[a1] += alpha_1 * pe_1
        
        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model 2: Forgetting Q-Learning with Stickiness.
    
    A Model-Free learner that incorporates a decay ('forgetting') mechanism for 
    unchosen actions. In a changing environment, values of unvisited states 
    or unchosen actions may become outdated. This model decays the Q-values 
    of unchosen options towards zero on every trial.

    Parameters:
    - alpha: [0, 1] Learning rate for chosen actions.
    - beta: [0, 10] Inverse temperature.
    - forget_rate: [0, 1] Decay rate for unchosen actions (0 = no forgetting, 1 = instant forgetting).
      Q_unchosen <- Q_unchosen * (1 - forget_rate)
    - stickiness: [0, 10] Choice stickiness for Stage 1.
    """
    alpha, beta, forget_rate, stickiness = model_parameters
    n_trials = len(action_1)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1
    
    for t in range(n_trials):
        
        # --- Stage 1 Decision ---
        logits_1 = beta * q_stage1.copy()
        if prev_a1 != -1:
            logits_1[prev_a1] += stickiness
            
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]
        
        # --- Stage 2 Decision ---
        s_idx = state[t]
        logits_2 = beta * q_stage2[s_idx]
        
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]
        
        # --- Learning and Forgetting ---
        r = reward[t]
        
        # Stage 2 Update
        # Update chosen alien
        q_stage2[s_idx, a2] += alpha * (r - q_stage2[s_idx, a2])
        # Decay unchosen alien in the current state
        unchosen_a2 = 1 - a2
        q_stage2[s_idx, unchosen_a2] *= (1.0 - forget_rate)
        
        # Stage 1 Update
        # Update chosen spaceship
        # Using Q-value of the second stage choice as the proxy for state value
        target_val = q_stage2[s_idx, a2]
        q_stage1[a1] += alpha * (target_val - q_stage1[a1])
        # Decay unchosen spaceship
        unchosen_a1 = 1 - a1
        q_stage1[unchosen_a1] *= (1.0 - forget_rate)
        
        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model 3: Outcome-Dependent Stickiness.
    
    A Model-Free learner where the tendency to repeat the previous choice (stickiness)
    depends on the outcome of the previous trial. This allows the model to capture 
    strategies like "Win-Stay, Lose-Shift" more explicitly by having separate 
    stickiness parameters for rewarded (Win) and unrewarded (Loss) previous trials.

    Parameters:
    - alpha: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - stick_win: [0, 10] Stickiness bonus added to previous choice if it was rewarded.
    - stick_loss: [0, 10] Stickiness bonus added to previous choice if it was unrewarded.
    """
    alpha, beta, stick_win, stick_loss = model_parameters
    n_trials = len(action_1)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1
    prev_reward = 0 # Initialize previous reward
    
    for t in range(n_trials):
        
        # --- Stage 1 Decision ---
        logits_1 = beta * q_stage1.copy()
        
        if prev_a1 != -1:
            # Apply different stickiness based on previous reward
            if prev_reward == 1:
                logits_1[prev_a1] += stick_win
            else:
                logits_1[prev_a1] += stick_loss
            
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]
        
        # --- Stage 2 Decision ---
        s_idx = state[t]
        logits_2 = beta * q_stage2[s_idx]
        
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]
        
        # --- Learning ---
        r = reward[t]
        
        # Standard Q-learning updates
        pe_2 = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += alpha * pe_2
        
        pe_1 = q_stage2[s_idx, a2] - q_stage1[a1]
        q_stage1[a1] += alpha * pe_1
        
        prev_a1 = a1
        prev_reward = r

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```