Here are three new cognitive models based on the participant's behavior and the provided feedback.

### Cognitive Model 1: Dual-Beta Q-Learning
This model separates the decision noise (inverse temperature) for the two stages of the task. The participant may exhibit different levels of exploration or consistency when choosing between spaceships (Stage 1) versus aliens (Stage 2). This accounts for the possibility that one stage is perceived as more "solvable" or deterministic than the other.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Dual-Beta Q-Learning.
    
    This model assumes the participant uses different levels of exploration/exploitation
    (inverse temperature) for the first stage (Spaceship choice) versus the second stage
    (Alien choice). This reflects potentially different cognitive processes or 
    subjective value scales for the two decisions.
    
    Parameters:
    learning_rate: [0, 1] Update rate for Q-values.
    beta_1: [0, 10] Inverse temperature for Stage 1 (Spaceship) choices.
    beta_2: [0, 10] Inverse temperature for Stage 2 (Alien) choices.
    stickiness: [0, 5] Choice perseverance bonus for Stage 1.
    """
    learning_rate, beta_1, beta_2, stickiness = model_parameters
    n_trials = len(action_1)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2)) # state x action
    
    log_likelihood = 0.0
    eps = 1e-10
    
    last_action_1 = -1 

    for trial in range(n_trials):
        a1 = action_1[trial]
        s2 = state[trial]
        a2 = action_2[trial]
        r = reward[trial]

        # Stage 1 Choice (Spaceships)
        q_eff_1 = q_stage1.copy()
        if last_action_1 != -1:
            q_eff_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta_1 * q_eff_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_likelihood += np.log(probs_1[a1] + eps)
        
        last_action_1 = a1

        if a2 == -1:
            continue

        # Stage 2 Choice (Aliens)
        exp_q2 = np.exp(beta_2 * q_stage2[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_likelihood += np.log(probs_2[a2] + eps)

        # Value Updates (Direct Reinforcement)
        # Updates are based on the reward received at the end of the trial
        q_stage2[s2, a2] += learning_rate * (r - q_stage2[s2, a2])
        q_stage1[a1] += learning_rate * (r - q_stage1[a1])

    return -log_likelihood
```

### Cognitive Model 2: Global Decay Q-Learning
Unlike the previous "Decay of Unchosen" model, this model implements a **global decay** where *all* Q-values (both chosen and unchosen) decay towards zero on every trial. This simulates a general memory constraint or volatility in the environment, where option values must be constantly reinforced to maintain their magnitude, otherwise they fade to neutrality.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Global Decay Q-Learning.
    
    This model incorporates a global decay mechanism where ALL Q-values (both chosen 
    and unchosen) decay towards zero on every trial. This simulates a general memory 
    leak or volatility, requiring constant reinforcement to maintain high value estimates.
    
    Parameters:
    learning_rate: [0, 1] Update rate for chosen options.
    global_decay: [0, 1] Decay rate applied to all Q-values each trial.
    beta: [0, 10] Inverse temperature.
    stickiness: [0, 5] Choice perseverance for Stage 1.
    """
    learning_rate, global_decay, beta, stickiness = model_parameters
    n_trials = len(action_1)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    log_likelihood = 0.0
    eps = 1e-10
    
    last_action_1 = -1 

    for trial in range(n_trials):
        a1 = action_1[trial]
        s2 = state[trial]
        a2 = action_2[trial]
        r = reward[trial]

        # Apply Global Decay to all values before choice/update
        q_stage1 *= (1.0 - global_decay)
        q_stage2 *= (1.0 - global_decay)

        # Stage 1 Choice
        q_eff_1 = q_stage1.copy()
        if last_action_1 != -1:
            q_eff_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_eff_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_likelihood += np.log(probs_1[a1] + eps)
        
        last_action_1 = a1

        if a2 == -1:
            continue

        # Stage 2 Choice
        exp_q2 = np.exp(beta * q_stage2[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_likelihood += np.log(probs_2[a2] + eps)

        # Value Updates
        q_stage2[s2, a2] += learning_rate * (r - q_stage2[s2, a2])
        q_stage1[a1] += learning_rate * (r - q_stage1[a1])

    return -log_likelihood
```

### Cognitive Model 3: Reference Point Q-Learning
This model posits that the participant evaluates outcomes not as absolute values (0 or 1) but relative to a dynamic **reference point** (average reward). If the participant expects a high reward, a 0 outcome is perceived as a loss (negative prediction error), whereas if expectations are low, it is neutral. This can explain the switching behavior after non-rewarded trials more dynamically than fixed Q-learning.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Reference Point Q-Learning.
    
    This model assumes the participant evaluates rewards relative to a running average 
    reward (reference point). Outcomes are encoded as gains or losses relative to this 
    reference, which affects the magnitude and direction of value updates.
    
    Parameters:
    learning_rate: [0, 1] Update rate for Q-values.
    ref_learning_rate: [0, 1] Update rate for the reference point (average reward).
    beta: [0, 10] Inverse temperature.
    stickiness: [0, 5] Choice perseverance for Stage 1.
    """
    learning_rate, ref_learning_rate, beta, stickiness = model_parameters
    n_trials = len(action_1)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    reference_point = 0.5 # Initialize at neutral expectation (between 0 and 1)
    
    log_likelihood = 0.0
    eps = 1e-10
    
    last_action_1 = -1 

    for trial in range(n_trials):
        a1 = action_1[trial]
        s2 = state[trial]
        a2 = action_2[trial]
        r = reward[trial]

        # Stage 1 Choice
        q_eff_1 = q_stage1.copy()
        if last_action_1 != -1:
            q_eff_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_eff_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_likelihood += np.log(probs_1[a1] + eps)
        
        last_action_1 = a1

        if a2 == -1:
            continue

        # Stage 2 Choice
        exp_q2 = np.exp(beta * q_stage2[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_likelihood += np.log(probs_2[a2] + eps)

        # Calculate Relative Reward (Outcome - Expectation)
        relative_reward = r - reference_point

        # Value Updates using Relative Reward
        q_stage2[s2, a2] += learning_rate * (relative_reward - q_stage2[s2, a2])
        q_stage1[a1] += learning_rate * (relative_reward - q_stage1[a1])
        
        # Update Reference Point (Running Average of Reward)
        reference_point += ref_learning_rate * (r - reference_point)

    return -log_likelihood
```