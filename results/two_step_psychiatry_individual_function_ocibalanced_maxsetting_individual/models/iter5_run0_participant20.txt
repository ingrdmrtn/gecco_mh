Here are the 3 new cognitive models based on the participant data and the feedback provided.

### Model 1: Independent Model-Based and Model-Free Betas
**Concept:** Instead of a weighting parameter `w` that trades off between Model-Based (MB) and Model-Free (MF) control within a single softmax temperature (`beta_1`), this model assigns independent inverse temperatures (`beta_mb` and `beta_mf`) to each system. This allows the agent to have different levels of "confidence" or noise for the MB and MF systems independently. For example, the agent might be very precise in using MF values but very noisy/random when using MB values.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Independent MB/MF Betas Model.
    
    Replaces the weighting parameter 'w' and stage 1 'beta' with two separate 
    inverse temperatures. This decouples the signal-to-noise ratio of the 
    two systems.
    
    Parameters:
    - learning_rate: [0,1] Update rate for Q-values.
    - beta_mb: [0,10] Inverse temperature for the Model-Based component.
    - beta_mf: [0,10] Inverse temperature for the Model-Free component.
    - beta_2: [0,10] Inverse temperature for Stage 2.
    - stickiness: [0,10] Choice stickiness for Stage 1.
    """
    learning_rate, beta_mb, beta_mf, beta_2, stickiness = model_parameters
    n_trials = len(action_1)
    
    # Subjective transition matrix (fixed as per standard task structure)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        # Handle missing data
        if action_1[trial] == -1:
            continue
            
        # --- Policy for Choice 1 ---
        # 1. Calculate MB Values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Combine using independent betas (no 'w' parameter)
        logits_1 = (beta_mb * q_stage1_mb) + (beta_mf * q_stage1_mf)
        
        # 3. Add stickiness
        if last_action_1 != -1:
            logits_1[last_action_1] += stickiness
            
        # 4. Softmax
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_action_1 = action_1[trial]
        state_idx = state[trial]
        
        # --- Policy for Choice 2 ---
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Value Updates ---
        # Update Stage 1 MF value using Stage 2 value (TD(0)-like)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Update Stage 2 MF value using Reward
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

    eps = 1e-10
    # Filter out skipped trials for log-loss calculation
    valid_idx = p_choice_1 > 0
    log_loss = -(np.sum(np.log(p_choice_1[valid_idx] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_idx] + eps)))
    return log_loss
```

### Model 2: Subjective Transition Belief
**Concept:** The standard model assumes the agent knows the transition probabilities are exactly 0.7/0.3. However, participants often have distorted beliefs about these probabilities (e.g., believing the transitions are more deterministic or more random than they are). This model introduces a `subjective_prob` parameter that replaces the fixed 0.7 in the Model-Based calculation, allowing the agent's internal model of the task structure to vary.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Subjective Transition Belief Model.
    
    The agent uses a subjective probability for the 'common' transition 
    instead of the ground truth (0.7). This distorts the MB value calculation.
    
    Parameters:
    - learning_rate: [0,1] Update rate for Q-values.
    - beta_1: [0,10] Inverse temperature for Stage 1.
    - beta_2: [0,10] Inverse temperature for Stage 2.
    - w: [0,1] Weighting (1=MB, 0=MF).
    - stickiness: [0,10] Choice stickiness for Stage 1.
    - subjective_prob: [0,1] The agent's belief about the common transition probability.
    """
    learning_rate, beta_1, beta_2, w, stickiness, subjective_prob = model_parameters
    n_trials = len(action_1)
    
    # Construct subjective transition matrix based on parameter
    # Row 0: Prob(Planet 0|Space 0), Prob(Planet 1|Space 0)
    # Row 1: Prob(Planet 0|Space 1), Prob(Planet 1|Space 1)
    p = subjective_prob
    transition_matrix = np.array([[p, 1-p], [1-p, p]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue

        # --- Policy for Choice 1 ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        
        # MB values derived from Subjective Transition Matrix
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        logits = beta_1 * q_net
        
        if last_action_1 != -1:
            logits[last_action_1] += stickiness
            
        exp_q1 = np.exp(logits - np.max(logits))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_action_1 = action_1[trial]
        state_idx = state[trial]
        
        # --- Policy for Choice 2 ---
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Value Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

    eps = 1e-10
    valid_idx = p_choice_1 > 0
    log_loss = -(np.sum(np.log(p_choice_1[valid_idx] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_idx] + eps)))
    return log_loss
```

### Model 3: Dual Stage Stickiness (Global Motor Perseveration)
**Concept:** The participant data shows a strong tendency to repeat specific actions not just in Stage 1, but also in Stage 2 (e.g., repeatedly choosing "Alien 0" regardless of the planet). Standard models usually assume Stage 2 is purely value-driven. This model adds a `stick_2` parameter to capture "motor perseveration" or global preference for the second-stage action (Alien 0 vs Alien 1), independent of the planet context.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Dual Stage Stickiness Model.
    
    Incorporates stickiness for BOTH the spaceship choice (Stage 1) and the 
    alien choice (Stage 2). The Stage 2 stickiness is global (motor perseveration),
    meaning if they chose Alien 0 on the previous trial, they are biased to 
    choose Alien 0 again, even if on a different planet.
    
    Parameters:
    - learning_rate: [0,1] Update rate.
    - beta_1: [0,10] Inv temp Stage 1.
    - beta_2: [0,10] Inv temp Stage 2.
    - w: [0,1] MB/MF weight.
    - stick_1: [0,10] Stickiness for Stage 1 (Spaceship).
    - stick_2: [0,10] Stickiness for Stage 2 (Alien).
    """
    learning_rate, beta_1, beta_2, w, stick_1, stick_2 = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    last_action_2 = -1 # Track the specific alien chosen (0 or 1) globally
    
    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue

        # --- Policy for Choice 1 ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        logits_1 = beta_1 * q_net
        
        # Stage 1 Stickiness
        if last_action_1 != -1:
            logits_1[last_action_1] += stick_1
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_action_1 = action_1[trial]
        state_idx = state[trial]
        
        # --- Policy for Choice 2 ---
        logits_2 = beta_2 * q_stage2_mf[state_idx]
        
        # Stage 2 Stickiness (Global/Motor)
        if last_action_2 != -1:
            logits_2[last_action_2] += stick_2
            
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        last_action_2 = action_2[trial]
        
        # --- Value Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

    eps = 1e-10
    valid_idx = p_choice_1 > 0
    log_loss = -(np.sum(np.log(p_choice_1[valid_idx] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_idx] + eps)))
    return log_loss
```