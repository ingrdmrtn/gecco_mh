Here are three new cognitive models for the two-step task.

### Model 1: Choice Kernel (Choice Trace) Model
This model hypothesizes that the participant's "blocky" behavior (sticking to one spaceship for many trials) is driven by a slowly decaying "Choice Kernel" or habit trace, rather than just immediate stickiness. This kernel accumulates over multiple trials, creating a stronger inertia than simple one-trial stickiness.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Choice Kernel Model.
    
    Incorporates a 'Choice Kernel' that tracks the long-term frequency of past choices.
    This explains strong perseveration (blocks) by adding a bias to the chosen option 
    that builds up and decays slowly, distinct from simple one-trial stickiness.
    
    Parameters:
    lr: [0, 1] Learning rate for Q-values (Stage 1 and Stage 2).
    beta_1: [0, 10] Inverse temperature for Stage 1.
    beta_2: [0, 10] Inverse temperature for Stage 2.
    w: [0, 1] Weight for Model-Based control (0=MF, 1=MB).
    ck_decay: [0, 1] Decay rate of the choice kernel (retention parameter).
    ck_weight: [0, 10] Weight of the choice kernel in the Stage 1 decision.
    """
    lr, beta_1, beta_2, w, ck_decay, ck_weight = model_parameters
    n_trials = len(action_1)

    # Fixed transition matrix as per task structure
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # [Planet, Alien]
    
    # Initialize Choice Kernel (2 options)
    choice_kernel = np.zeros(2)

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # Model-Based Value
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Q-value: Weighted sum of MB, MF, and Choice Kernel
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf + ck_weight * choice_kernel
        
        # Softmax
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Update Choice Kernel
        # The chosen action's trace increases, unchosen decays (or both decay then add)
        # Lau & Glimcher style: CK_t+1 = decay * CK_t + (1-decay) * I(choice)
        # Here we use a simplified version: decay existing, add 1 to chosen.
        choice_kernel *= ck_decay
        choice_kernel[action_1[trial]] += (1.0 - ck_decay)

        # --- Stage 2 Decision ---
        state_idx = state[trial] # Planet
        qs_current_state = q_stage2_mf[state_idx]
        
        exp_q2 = np.exp(beta_2 * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning ---
        # Stage 1 MF Update (TD(0))
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr * delta_stage1
        
        # Stage 2 MF Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Context-Dependent State Representation
This model proposes that the participant is "superstitious" and treats the planets as different states depending on which spaceship was used to reach them. For example, "Planet X reached via Spaceship A" is treated as a separate context from "Planet X reached via Spaceship B". This splits the Stage 2 state space into 4 contexts, allowing the agent to learn context-specific alien values.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Context-Dependent State Representation Model.
    
    The agent treats the Stage 2 states (Planets) as context-dependent on the 
    Stage 1 choice (Spaceship). This results in 4 effective Stage 2 states:
    (S0->P0, S0->P1, S1->P0, S1->P1). This allows learning distinct reward 
    probabilities for the same alien depending on the arrival path.
    
    Parameters:
    lr: [0, 1] Learning rate.
    beta_1: [0, 10] Inverse temperature for Stage 1.
    beta_2: [0, 10] Inverse temperature for Stage 2.
    w: [0, 1] Weight for Model-Based control.
    stickiness: [0, 5] Simple choice stickiness.
    """
    lr, beta_1, beta_2, w, stickiness = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    
    # Split State Q-values: [Spaceship, Planet, Alien]
    # Dimensions: 2 (Stage 1 choices) x 2 (Planets) x 2 (Aliens)
    q_stage2_context = np.zeros((2, 2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # Model-Based Value Calculation with Split States
        # V(S0) = 0.7 * Max(Q(S0, P0)) + 0.3 * Max(Q(S0, P1))
        # V(S1) = 0.3 * Max(Q(S1, P0)) + 0.7 * Max(Q(S1, P1))
        
        max_q_s0_p0 = np.max(q_stage2_context[0, 0])
        max_q_s0_p1 = np.max(q_stage2_context[0, 1])
        max_q_s1_p0 = np.max(q_stage2_context[1, 0])
        max_q_s1_p1 = np.max(q_stage2_context[1, 1])
        
        q_stage1_mb = np.array([
            transition_matrix[0, 0] * max_q_s0_p0 + transition_matrix[0, 1] * max_q_s0_p1,
            transition_matrix[1, 0] * max_q_s1_p0 + transition_matrix[1, 1] * max_q_s1_p1
        ])
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Decision ---
        # State depends on both spaceship (action_1) and planet (state)
        s1_choice = action_1[trial]
        planet_idx = state[trial]
        
        qs_current_context = q_stage2_context[s1_choice, planet_idx]
        
        exp_q2 = np.exp(beta_2 * qs_current_context)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning ---
        # Current Q value for Stage 2 (specific to context)
        q_val_s2 = q_stage2_context[s1_choice, planet_idx, action_2[trial]]
        
        # Stage 1 Update
        delta_stage1 = q_val_s2 - q_stage1_mf[s1_choice]
        q_stage1_mf[s1_choice] += lr * delta_stage1
        
        # Stage 2 Update
        delta_stage2 = reward[trial] - q_val_s2
        q_stage2_context[s1_choice, planet_idx, action_2[trial]] += lr * delta_stage2
        
        last_action_1 = s1_choice

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Pearce-Hall Dynamic Learning Rate
This model implements a dynamic learning rate mechanism inspired by the Pearce-Hall theory. The learning rate for the Stage 1 Model-Free values is not constant but scales with the absolute prediction error (surprise) from the previous trial. This allows the participant to update their beliefs rapidly when outcomes are surprising (e.g., unexpected transitions or rewards) and stabilize when outcomes are predictable.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Pearce-Hall Dynamic Learning Rate Model.
    
    The learning rate for Stage 1 is modulated by the absolute prediction error
    (surprise). High surprise leads to faster learning (higher alpha), while 
    low surprise leads to stability.
    
    Parameters:
    lr_base: [0, 1] Baseline learning rate.
    beta_1: [0, 10] Inverse temperature for Stage 1.
    beta_2: [0, 10] Inverse temperature for Stage 2.
    w: [0, 1] Weight for Model-Based control.
    stickiness: [0, 5] Choice stickiness.
    ph_k: [0, 10] Pearce-Hall scaling factor (sensitivity to surprise).
    """
    lr_base, beta_1, beta_2, w, stickiness, ph_k = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    last_delta_abs = 0.0 # Store absolute prediction error

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Decision ---
        state_idx = state[trial]
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta_2 * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning ---
        # Calculate Dynamic Learning Rate for Stage 1
        # alpha_t = lr_base + k * |delta_{t-1}|
        # We clip it to [0, 1]
        lr_dynamic = lr_base + ph_k * last_delta_abs
        if lr_dynamic > 1.0: lr_dynamic = 1.0
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr_dynamic * delta_stage1
        
        # Stage 2 Update (using baseline learning rate for stability in reward estimation)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr_base * delta_stage2
        
        # Store surprise for next trial's learning rate
        # We use the Stage 1 prediction error as the measure of 'surprise' regarding the choice
        last_delta_abs = np.abs(delta_stage1)
        
        last_action_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```