An implementation of three cognitive models for the two-step task, formulated as standalone Python functions.

### Model 1: Hybrid MB/MF with Outcome-Dependent Stickiness
This model combines Model-Based (planning) and Model-Free (habitual) value estimation. It introduces outcome-dependent stickiness (Win-Stay, Lose-Shift) to capture the participant's tendency to repeat choices based on the immediate previous outcome, distinct from value learning.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Control with Outcome-Dependent Stickiness.
    
    Combines a Model-Based (MB) planner that uses the transition matrix and a 
    Model-Free (MF) learner that updates based on reward. The final policy is a 
    weighted mixture. Additionally, it includes separate stickiness parameters 
    for rewarded (Win-Stay) and unrewarded (Lose-Shift/Stay) previous trials.
    
    Parameters:
    learning_rate: [0, 1] Learning rate for value updates.
    beta:          [0, 10] Inverse temperature for softmax choice.
    w:             [0, 1] Weighting parameter (0 = Pure MF, 1 = Pure MB).
    stick_win:     [-5, 5] Stickiness bonus after a rewarded trial.
    stick_loss:    [-5, 5] Stickiness bonus after an unrewarded trial.
    """
    learning_rate, beta, w, stick_win, stick_loss = model_parameters
    n_trials = len(action_1)
    
    # Transition matrix: 0->0 (70%), 1->1 (70%)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Initialize Q-values
    # MF Q-values for Stage 1 (Spaceships)
    q_mf_stage1 = np.zeros(2) + 0.5
    # Q-values for Stage 2 (Aliens at Planets) - shared by MB and MF
    q_stage2 = np.zeros((2, 2)) + 0.5
    
    log_loss = 0
    eps = 1e-10
    
    prev_action_1 = -1
    prev_reward = -1

    for trial in range(n_trials):
        a1 = action_1[trial]
        s_idx = state[trial]
        a2 = action_2[trial]
        r = reward[trial]

        # Handle missing data
        if a1 == -1 or s_idx == -1 or a2 == -1:
            prev_action_1 = -1
            continue

        # --- Stage 1 Policy ---
        
        # 1. Model-Based Value: V_MB(s1) = T * max(Q_S2)
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_mb_stage1 = transition_matrix @ max_q_stage2
        
        # 2. Hybrid Value: w * MB + (1-w) * MF
        q_net_stage1 = w * q_mb_stage1 + (1 - w) * q_mf_stage1
        
        # 3. Stickiness
        logits_1 = beta * q_net_stage1
        if prev_action_1 != -1:
            if prev_reward == 1:
                logits_1[prev_action_1] += stick_win
            else:
                logits_1[prev_action_1] += stick_loss
        
        # Softmax Choice 1
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + eps)
        
        # --- Stage 2 Policy ---
        
        logits_2 = beta * q_stage2[s_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[a2] + eps)
        
        # --- Updates ---
        
        # Update Stage 2 Q-values (Standard Q-learning)
        # RPE stage 2
        pe_2 = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += learning_rate * pe_2
        
        # Update Stage 1 MF Q-values (TD(1) style - driven by final reward)
        # This enforces the "habitual" nature of MF ignoring transitions
        pe_1 = r - q_mf_stage1[a1]
        q_mf_stage1[a1] += learning_rate * pe_1
        
        prev_action_1 = a1
        prev_reward = r

    return log_loss
```

### Model 2: Hybrid MB/MF with Asymmetric Learning Rates
This model hypothesizes that the participant learns differently from positive outcomes (rewards) versus negative outcomes (omissions). It applies separate learning rates for positive and negative prediction errors to both the Model-Free and stage 2 value updates.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Control with Asymmetric Learning Rates.
    
    Differentiates between learning from positive prediction errors (lr_pos)
    and negative prediction errors (lr_neg). This captures valence-dependent
    learning biases (e.g., learning more from gains than missed opportunities).
    
    Parameters:
    lr_pos:     [0, 1] Learning rate for positive prediction errors (RPE > 0).
    lr_neg:     [0, 1] Learning rate for negative prediction errors (RPE < 0).
    beta:       [0, 10] Inverse temperature.
    w:          [0, 1] Weighting parameter (0 = Pure MF, 1 = Pure MB).
    stickiness: [-5, 5] General choice perseverance bonus.
    """
    lr_pos, lr_neg, beta, w, stickiness = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_mf_stage1 = np.zeros(2) + 0.5
    q_stage2 = np.zeros((2, 2)) + 0.5
    
    log_loss = 0
    eps = 1e-10
    
    prev_action_1 = -1

    for trial in range(n_trials):
        a1 = action_1[trial]
        s_idx = state[trial]
        a2 = action_2[trial]
        r = reward[trial]

        if a1 == -1 or s_idx == -1 or a2 == -1:
            prev_action_1 = -1
            continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_mb_stage1 = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_mb_stage1 + (1 - w) * q_mf_stage1
        
        logits_1 = beta * q_net_stage1
        if prev_action_1 != -1:
            logits_1[prev_action_1] += stickiness
        
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + eps)
        
        # --- Stage 2 Policy ---
        logits_2 = beta * q_stage2[s_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[a2] + eps)
        
        # --- Updates ---
        
        # Stage 2 Update with Asymmetric LR
        pe_2 = r - q_stage2[s_idx, a2]
        lr_2 = lr_pos if pe_2 >= 0 else lr_neg
        q_stage2[s_idx, a2] += lr_2 * pe_2
        
        # Stage 1 MF Update with Asymmetric LR
        pe_1 = r - q_mf_stage1[a1]
        lr_1 = lr_pos if pe_1 >= 0 else lr_neg
        q_mf_stage1[a1] += lr_1 * pe_1
        
        prev_action_1 = a1

    return log_loss
```

### Model 3: Hybrid MB/MF with Distinct Stage Temperatures
This model acknowledges that the exploration-exploitation balance may differ between the high-level planning stage (Stage 1) and the local bandit stage (Stage 2). It uses separate inverse temperature parameters (`beta`) for each stage.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free Control with Distinct Stage Temperatures.
    
    Uses separate softmax inverse temperature parameters for Stage 1 (beta_stage1)
    and Stage 2 (beta_stage2). This allows the model to capture different levels 
    of stochasticity or decision noise in the spaceship choice vs the alien choice.
    
    Parameters:
    learning_rate: [0, 1] Learning rate.
    beta_stage1:   [0, 10] Inverse temperature for Stage 1 choice.
    beta_stage2:   [0, 10] Inverse temperature for Stage 2 choice.
    w:             [0, 1] Weighting parameter (0 = Pure MF, 1 = Pure MB).
    stickiness:    [-5, 5] Choice perseverance bonus for Stage 1.
    """
    learning_rate, beta_stage1, beta_stage2, w, stickiness = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_mf_stage1 = np.zeros(2) + 0.5
    q_stage2 = np.zeros((2, 2)) + 0.5
    
    log_loss = 0
    eps = 1e-10
    
    prev_action_1 = -1

    for trial in range(n_trials):
        a1 = action_1[trial]
        s_idx = state[trial]
        a2 = action_2[trial]
        r = reward[trial]

        if a1 == -1 or s_idx == -1 or a2 == -1:
            prev_action_1 = -1
            continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_mb_stage1 = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_mb_stage1 + (1 - w) * q_mf_stage1
        
        # Use beta_stage1
        logits_1 = beta_stage1 * q_net_stage1
        if prev_action_1 != -1:
            logits_1[prev_action_1] += stickiness
        
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + eps)
        
        # --- Stage 2 Policy ---
        # Use beta_stage2
        logits_2 = beta_stage2 * q_stage2[s_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[a2] + eps)
        
        # --- Updates ---
        pe_2 = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += learning_rate * pe_2
        
        pe_1 = r - q_mf_stage1[a1]
        q_mf_stage1[a1] += learning_rate * pe_1
        
        prev_action_1 = a1

    return log_loss
```