Here are three new cognitive models based on the participant data and the two-step task structure.

### Cognitive Model 1: Dual Learning Rate Hybrid Model
This model hypothesizes that the participant updates their Model-Free (habitual) values and their Model-Based (explicit reward map) values at different rates. For instance, a participant might quickly learn which alien is currently rewarding (high `lr_mb`) but be slow to update their habitual spaceship preference (low `lr_mf`), or vice versa.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Dual Learning Rate Hybrid Model.
    
    Distinguishes between the learning rate for the Model-Free system (TD updates)
    and the Model-Based system (Reward value estimation). This allows the agent
    to update their 'map' of the world (Alien values) at a different speed than
    their habitual caching of action values.

    Parameters:
    - lr_mf: [0, 1] Learning rate for Model-Free TD updates.
    - lr_mb: [0, 1] Learning rate for Model-Based reward estimation.
    - beta: [0, 10] Inverse temperature (softmax).
    - w: [0, 1] Weight for Model-Based control (0=Pure MF, 1=Pure MB).
    - lam: [0, 1] Eligibility trace parameter for MF updates.
    """
    lr_mf, lr_mb, beta, w, lam = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Q-values for Model-Free system (Habitual)
    q_mf_s1 = np.zeros(2) + 0.5
    q_mf_s2 = np.zeros((2, 2)) + 0.5
    
    # Reward values for Model-Based system (Map)
    v_mb_s2 = np.zeros((2, 2)) + 0.5
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # MB Value: Expected value of next state given transition matrix
        max_v_mb_s2 = np.max(v_mb_s2, axis=1) # Max value available in each planet
        q_mb_s1 = transition_matrix @ max_v_mb_s2
        
        # Net Value blending MB and MF
        q_net_s1 = w * q_mb_s1 + (1 - w) * q_mf_s1
        
        # Softmax Stage 1
        exp_q1 = np.exp(beta * q_net_s1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Transition ---
        s_idx = int(state[trial])
        
        # --- Stage 2 Decision ---
        # Blend MB (Reward Map) and MF (Cached Value) for the second stage as well
        q_net_s2 = w * v_mb_s2[s_idx] + (1 - w) * q_mf_s2[s_idx]
        
        exp_q2 = np.exp(beta * q_net_s2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        r = reward[trial]
        
        # 1. Model-Based Update (Reward Learning)
        # Update the explicit estimate of the alien's reward probability
        v_mb_s2[s_idx, a2] += lr_mb * (r - v_mb_s2[s_idx, a2])
        
        # 2. Model-Free Update (TD with Eligibility Trace)
        # Stage 1 Prediction Error
        pe_1 = q_mf_s2[s_idx, a2] - q_mf_s1[a1]
        q_mf_s1[a1] += lr_mf * pe_1
        
        # Stage 2 Prediction Error
        pe_2 = r - q_mf_s2[s_idx, a2]
        q_mf_s2[s_idx, a2] += lr_mf * pe_2
        
        # Eligibility Trace: Update Stage 1 Q with Stage 2 PE
        q_mf_s1[a1] += lr_mf * lam * pe_2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2: Planet Stickiness Hybrid Model
This model introduces a specific type of perseveration: "Planet Stickiness". Instead of just repeating the last action, the agent has a bias to return to the *Planet* (State 2) they just visited. This bias is projected back to Stage 1 actions via the transition matrix. If the participant visited Planet X, they are more likely to choose the spaceship that leads to Planet X, regardless of whether the previous outcome was a reward.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Planet Stickiness Hybrid Model.
    
    Incorporates a bias to return to the previously visited planet (State 2),
    mediated by the known transition matrix. This captures perseveration on the 
    outcome of the transition (the state reached) rather than just the action taken.
    
    Parameters:
    - lr: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Model-Based weight.
    - lam: [0, 1] Eligibility trace decay.
    - stick_planet: [0, 10] Stickiness parameter for the previous planet.
    """
    lr, beta, w, lam, stick_planet = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_mf_s1 = np.zeros(2) + 0.5
    q_mf_s2 = np.zeros((2, 2)) + 0.5
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_state = -1 # Initialize with no previous state
    
    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        max_q_s2 = np.max(q_mf_s2, axis=1)
        q_mb_s1 = transition_matrix @ max_q_s2
        
        # Calculate Planet Stickiness Bonus
        # We bias the action that makes reaching the previous planet more likely.
        # Bonus(a) = stick_planet * P(prev_state | a)
        planet_bonus = np.zeros(2)
        if prev_state != -1:
            # transition_matrix[a, s] gives P(s|a)
            planet_bonus[0] = stick_planet * transition_matrix[0, prev_state]
            planet_bonus[1] = stick_planet * transition_matrix[1, prev_state]
        
        q_net_s1 = w * q_mb_s1 + (1 - w) * q_mf_s1 + planet_bonus
        
        exp_q1 = np.exp(beta * q_net_s1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Transition ---
        s_idx = int(state[trial])
        
        # --- Stage 2 Decision ---
        q_net_s2 = q_mf_s2[s_idx]
        
        exp_q2 = np.exp(beta * q_net_s2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        r = reward[trial]
        
        # TD Updates
        pe_1 = q_mf_s2[s_idx, a2] - q_mf_s1[a1]
        q_mf_s1[a1] += lr * pe_1
        
        pe_2 = r - q_mf_s2[s_idx, a2]
        q_mf_s2[s_idx, a2] += lr * pe_2
        
        # Eligibility trace
        q_mf_s1[a1] += lr * lam * pe_2
        
        prev_state = s_idx

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3: Stage-Separated Beta Hybrid Model
This model posits that the participant's level of decision noise (exploration vs. exploitation) differs between the two stages. They might be very consistent/sticky in selecting a spaceship (Stage 1) but more exploratory or noisy when selecting an alien (Stage 2), or vice versa.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Stage-Separated Beta Hybrid Model.
    
    Uses distinct inverse temperature (beta) parameters for the first stage (spaceship choice)
    and the second stage (alien choice). This allows the model to capture different
    levels of exploration or decision noise at different hierarchical levels of the task.
    
    Parameters:
    - lr: [0, 1] Learning rate.
    - beta1: [0, 10] Inverse temperature for Stage 1 (Spaceship).
    - beta2: [0, 10] Inverse temperature for Stage 2 (Alien).
    - w: [0, 1] Model-Based weight.
    - lam: [0, 1] Eligibility trace decay.
    """
    lr, beta1, beta2, w, lam = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_mf_s1 = np.zeros(2) + 0.5
    q_mf_s2 = np.zeros((2, 2)) + 0.5
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        max_q_s2 = np.max(q_mf_s2, axis=1)
        q_mb_s1 = transition_matrix @ max_q_s2
        
        q_net_s1 = w * q_mb_s1 + (1 - w) * q_mf_s1
        
        # Use beta1 for Stage 1
        exp_q1 = np.exp(beta1 * q_net_s1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Transition ---
        s_idx = int(state[trial])
        
        # --- Stage 2 Decision ---
        q_net_s2 = q_mf_s2[s_idx]
        
        # Use beta2 for Stage 2
        exp_q2 = np.exp(beta2 * q_net_s2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        r = reward[trial]
        
        pe_1 = q_mf_s2[s_idx, a2] - q_mf_s1[a1]
        q_mf_s1[a1] += lr * pe_1
        
        pe_2 = r - q_mf_s2[s_idx, a2]
        q_mf_s2[s_idx, a2] += lr * pe_2
        
        q_mf_s1[a1] += lr * lam * pe_2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```