Here are three new cognitive models for the two-step task, formulated as Python functions.

### Model 1: Hybrid Model with Dynamic Transition Learning
This model hypothesizes that the participant does not assume a fixed 70/30 transition structure but actively learns the transition probabilities ($P(State|Action)$) over time. This accounts for potential "gambler's fallacy" or perceived structural changes (reversal learning) regarding which spaceship goes to which planet.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Dynamic Transition Learning.
    
    The participant updates their internal model of the spaceship-planet transition 
    probabilities based on experienced transitions, rather than assuming a fixed 
    0.7/0.3 structure. This allows the model-based system to adapt to runs of 
    "rare" transitions.
    
    Parameters:
    lr_val: [0, 1] Learning rate for value updates (TD).
    lr_trans: [0, 1] Learning rate for transition probability updates.
    beta_1: [0, 10] Inverse temperature for Stage 1.
    beta_2: [0, 10] Inverse temperature for Stage 2.
    w: [0, 1] Weight for Model-Based values (0=MF, 1=MB).
    lam: [0, 1] Eligibility trace decay.
    """
    lr_val, lr_trans, beta_1, beta_2, w, lam = model_parameters
    n_trials = len(action_1)
    
    # Initialize transition matrix (belief) with standard prior
    # Rows: Actions (0:A, 1:U), Cols: States (0:X, 1:Y)
    trans_probs = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # MB Value: Expected Max Q(State 2) based on current transition belief
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = trans_probs @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Policy ---
        state_idx = state[trial]
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        chosen_a1 = action_1[trial]
        
        # 1. Transition Model Update (State Prediction Error)
        # Create one-hot vector for observed state (1 for observed, 0 for other)
        state_one_hot = np.zeros(2)
        state_one_hot[state_idx] = 1.0
        
        # Update the row corresponding to the chosen action towards the observed outcome
        trans_probs[chosen_a1] += lr_trans * (state_one_hot - trans_probs[chosen_a1])
        
        # 2. Value Updates (Standard Hybrid)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[chosen_a1]
        q_stage1_mf[chosen_a1] += lr_val * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr_val * delta_stage2
        
        # Eligibility Trace for Stage 1
        q_stage1_mf[chosen_a1] += lr_val * lam * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model with Choice Kernel (Habit)
This model adds a "Choice Kernel" to the standard hybrid model. Unlike simple 1-back stickiness, the choice kernel integrates the history of past choices to form a "habit" strength. This explains the participant's tendency to repeat choices (perseveration) even when rewards are intermittent, modeling a dissociation between goal-directed values and habitual motor responses.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Choice Kernel (Habit).
    
    Incorporates a "habit" or "choice kernel" mechanism that reinforces recently 
    chosen actions in Stage 1, independent of reward. This captures perseverance 
    that builds up over multiple trials (unlike simple 1-back stickiness).
    
    Parameters:
    lr: [0, 1] Learning rate for value updates.
    beta_1: [0, 10] Inverse temperature for Stage 1.
    beta_2: [0, 10] Inverse temperature for Stage 2.
    w: [0, 1] Weight for Model-Based values.
    lam: [0, 1] Eligibility trace.
    lr_k: [0, 1] Learning rate for the choice kernel (habit formation rate).
    w_k: [0, 10] Weight of the choice kernel in the decision (habit strength).
    """
    lr, beta_1, beta_2, w, lam, lr_k, w_k = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Choice kernel for Stage 1 actions (A and U), initialized to 0
    choice_kernel = np.zeros(2)
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add choice kernel influence to logits
        logits_1 = beta_1 * q_net + w_k * choice_kernel
        
        # Softmax with stability fix
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Policy ---
        state_idx = state[trial]
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        chosen_a1 = action_1[trial]
        
        # Update Choice Kernel (Habit Learning)
        # K(chosen) moves towards 1, K(unchosen) decays towards 0
        choice_kernel *= (1 - lr_k)
        choice_kernel[chosen_a1] += lr_k
        
        # Value Updates
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[chosen_a1]
        q_stage1_mf[chosen_a1] += lr * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr * delta_stage2
        
        q_stage1_mf[chosen_a1] += lr * lam * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model with Outcome-Dependent Stickiness
This model separates the "stickiness" (tendency to repeat the last action) into two parameters based on the previous trial's outcome. This allows the model to capture "Win-Stay, Lose-Switch" heuristics explicitly, where a participant might be highly sticky after a win but less so (or switch) after a loss.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Outcome-Dependent Stickiness.
    
    Implements a "Win-Stay, Lose-Switch" bias by applying different stickiness 
    weights depending on whether the previous trial was rewarded. This modulates
    the repetition bias based on the most recent reinforcement.
    
    Parameters:
    lr: [0, 1] Learning rate.
    beta_1: [0, 10] Stage 1 inverse temperature.
    beta_2: [0, 10] Stage 2 inverse temperature.
    w: [0, 1] Model-based weight.
    lam: [0, 1] Eligibility trace.
    stick_win: [0, 10] Stickiness bonus after a reward (Win-Stay strength).
    stick_loss: [0, 10] Stickiness bonus after no reward (Lose-Stay strength).
    """
    lr, beta_1, beta_2, w, lam, stick_win, stick_loss = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1
    prev_reward = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta_1 * q_net
        
        # Apply outcome-dependent stickiness to the previous action
        if prev_action_1 != -1:
            if prev_reward == 1:
                logits_1[prev_action_1] += stick_win
            else:
                logits_1[prev_action_1] += stick_loss
        
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Policy ---
        state_idx = state[trial]
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        chosen_a1 = action_1[trial]
        
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[chosen_a1]
        q_stage1_mf[chosen_a1] += lr * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr * delta_stage2
        
        q_stage1_mf[chosen_a1] += lr * lam * delta_stage2
        
        # Update history for next trial's stickiness
        prev_action_1 = chosen_a1
        prev_reward = reward[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```