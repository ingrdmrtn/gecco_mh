Here are three new cognitive models for the two-step task.

### Model 1: Strategy Mixture Model
This model posits that instead of integrating Model-Based (MB) and Model-Free (MF) values into a single utility, the agent runs two distinct decision strategies (policies) in parallel and probabilistically selects which strategy to enact on each trial. This allows for distinct "temperatures" (betas) for the MB and MF systems, reflecting potentially different levels of confidence or noise in each system.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Strategy Mixture Model.
    
    Hypothesis: The agent computes two separate policy distributions (one Model-Based, 
    one Model-Free) and mixes them probabilistically, rather than mixing the Q-values.
    This allows for independent exploration rates (betas) for the habitual (MF) 
    and planning (MB) systems.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate for MF Q-value updates.
    beta_mb: [0, 10] - Inverse temperature for the Model-Based strategy.
    beta_mf: [0, 10] - Inverse temperature for the Model-Free strategy.
    w: [0, 1] - Probability of selecting the Model-Based strategy (Mixture weight).
    """
    learning_rate, beta_mb, beta_mf, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # state x action

    for trial in range(n_trials):
        # Handle missing/timeout trials
        if action_1[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        # --- STAGE 1 CHOICE ---
        # 1. Calculate Model-Based Values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Calculate Policy Probabilities separately
        # Model-Based Policy
        exp_mb = np.exp(beta_mb * q_stage1_mb)
        probs_mb = exp_mb / np.sum(exp_mb)
        
        # Model-Free Policy
        exp_mf = np.exp(beta_mf * q_stage1_mf)
        probs_mf = exp_mf / np.sum(exp_mf)
        
        # 3. Mix Policies
        probs_1 = w * probs_mb + (1 - w) * probs_mf
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- STAGE 2 CHOICE ---
        state_idx = state[trial]
        # Standard Softmax on Stage 2 Q-values (using MF beta for consistency with habit)
        exp_q2 = np.exp(beta_mf * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- UPDATES ---
        # TD(0) updates
        chosen_a1 = action_1[trial]
        chosen_a2 = action_2[trial]
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, chosen_a2] - q_stage1_mf[chosen_a1]
        q_stage1_mf[chosen_a1] += learning_rate * delta_stage1
        
        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, chosen_a2]
        q_stage2_mf[state_idx, chosen_a2] += learning_rate * delta_stage2

    eps = 1e-10
    # Clip probabilities to avoid log(0)
    p_choice_1 = np.clip(p_choice_1, eps, 1 - eps)
    p_choice_2 = np.clip(p_choice_2, eps, 1 - eps)
    
    log_loss = -(np.sum(np.log(p_choice_1)) + np.sum(np.log(p_choice_2)))
    return log_loss
```

### Model 2: Global Reward Rate Modulated Temperature
This model proposes that the participant's balance between exploration and exploitation is dynamic and regulated by the recent history of rewards. A high average reward rate encourages exploitation (higher `beta`), leading to "sticky" behavior, while a low reward rate triggers exploration (lower `beta`) and switching.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Global Reward Rate Modulated Temperature.
    
    Hypothesis: The inverse temperature (beta) is not static but modulated by the 
    average global reward rate. High recent rewards increase beta (exploitation/stickiness), 
    while low rewards decrease beta (exploration/randomness).
    
    Parameters:
    learning_rate: [0, 1] - Learning rate for Q-value updates.
    beta_base: [0, 10] - Baseline inverse temperature.
    w: [0, 1] - Weight for Model-Based values.
    rho: [0, 5] - Sensitivity of beta to reward rate (scaling factor).
    lr_avg: [0, 1] - Learning rate for tracking the global average reward.
    """
    learning_rate, beta_base, w, rho, lr_avg = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    avg_reward = 0.5 # Initialize average reward expectation at chance

    for trial in range(n_trials):
        if action_1[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        # Calculate Dynamic Beta
        # If avg_reward > 0.5, beta increases. If < 0.5, beta decreases.
        beta_eff = beta_base * np.exp(rho * (avg_reward - 0.5))
        # Clamp beta to prevent overflow
        beta_eff = np.clip(beta_eff, 0, 20)

        # --- STAGE 1 CHOICE ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_eff * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- STAGE 2 CHOICE ---
        # Using dynamic beta here as well implies general arousal/engagement state
        exp_q2 = np.exp(beta_eff * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- UPDATES ---
        chosen_a1 = action_1[trial]
        chosen_a2 = action_2[trial]
        r = reward[trial]

        # Q-learning updates
        delta_stage1 = q_stage2_mf[state_idx, chosen_a2] - q_stage1_mf[chosen_a1]
        q_stage1_mf[chosen_a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[state_idx, chosen_a2]
        q_stage2_mf[state_idx, chosen_a2] += learning_rate * delta_stage2
        
        # Update Global Average Reward
        # Only update if reward is valid (0 or 1)
        if r != -1:
            avg_reward += lr_avg * (r - avg_reward)

    eps = 1e-10
    p_choice_1 = np.clip(p_choice_1, eps, 1 - eps)
    p_choice_2 = np.clip(p_choice_2, eps, 1 - eps)
    log_loss = -(np.sum(np.log(p_choice_1)) + np.sum(np.log(p_choice_2)))
    return log_loss
```

### Model 3: Transition Reliability Weighted Model-Based Control
This model adjusts the reliance on Model-Based planning (`w`) based on the experienced reliability of the transition structure. If the agent encounters "rare" transitions frequently, they lose confidence in the map (the transition matrix) and degrade towards Model-Free behavior. Conversely, consistent "common" transitions boost reliance on the model.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Transition Reliability Weighted Model-Based Control.
    
    Hypothesis: The weight of the Model-Based component (w) is modulated by a 
    'reliability' signal. Common transitions increase reliability, while rare 
    transitions decrease it. This allows the agent to abandon planning when the 
    environment feels chaotic.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate for Q-values.
    beta: [0, 10] - Inverse temperature.
    w_base: [0, 1] - Baseline weight for Model-Based values (maximum w).
    alpha_rel: [0, 1] - Learning rate for the reliability signal.
    """
    learning_rate, beta, w_base, alpha_rel = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    reliability = 0.5 # Start with neutral reliability

    for trial in range(n_trials):
        if action_1[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        # --- STAGE 1 CHOICE ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Calculate effective w based on reliability
        w_eff = w_base * reliability
        
        q_net = w_eff * q_stage1_mb + (1 - w_eff) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- STAGE 2 CHOICE ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- UPDATES ---
        chosen_a1 = action_1[trial]
        chosen_a2 = action_2[trial]
        
        # Determine if transition was common or rare
        # Common: (0->0) or (1->1). Rare: (0->1) or (1->0).
        is_common = 1.0 if chosen_a1 == state_idx else 0.0
        
        # Update Reliability
        reliability += alpha_rel * (is_common - reliability)
        
        # Q-learning updates
        delta_stage1 = q_stage2_mf[state_idx, chosen_a2] - q_stage1_mf[chosen_a1]
        q_stage1_mf[chosen_a1] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, chosen_a2]
        q_stage2_mf[state_idx, chosen_a2] += learning_rate * delta_stage2

    eps = 1e-10
    p_choice_1 = np.clip(p_choice_1, eps, 1 - eps)
    p_choice_2 = np.clip(p_choice_2, eps, 1 - eps)
    log_loss = -(np.sum(np.log(p_choice_1)) + np.sum(np.log(p_choice_2)))
    return log_loss
```