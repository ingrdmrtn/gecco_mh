Here are the three proposed cognitive models.

### Model 1: Pure Model-Based with Asymmetric Learning and Decay
This model builds on the success of the Pure Model-Based (MB) strategy with decay. It adds **asymmetric learning rates** for positive (reward) and negative (no reward) outcomes. This allows the model to update value estimates differently when finding gold versus finding nothing, capturing potential optimism or pessimism biases, while maintaining the decay mechanism to handle probability drift.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Based with Asymmetric Learning Rates and Passive Decay.
    
    Distinguishes between learning from rewards (lr_pos) and no-rewards (lr_neg).
    Unchosen Stage 2 Q-values decay towards 0.5.
    
    Parameters:
    - lr_pos: [0, 1] Learning rate for Reward=1 outcomes.
    - lr_neg: [0, 1] Learning rate for Reward=0 outcomes.
    - beta1: [0, 10] Inverse temperature for Stage 1.
    - beta2: [0, 10] Inverse temperature for Stage 2.
    - decay: [0, 1] Decay rate for unchosen options (0 = no decay).
    - perseveration: [0, 5] Stickiness bonus for Stage 1 choice.
    """
    lr_pos, lr_neg, beta1, beta2, decay, perseveration = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Initialize Stage 2 values (aliens) to 0.5 (chance)
    q_stage2_mf = np.ones((2, 2)) * 0.5 
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Decision (Model-Based) ---
        # Calculate expected value of each spaceship based on transitions and max stage 2 values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        logits_1 = beta1 * q_stage1_mb
        
        # Apply perseveration bonus
        if last_action_1 != -1:
            logits_1[last_action_1] += perseveration
            
        # Softmax for Stage 1
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_action_1 = action_1[trial]
        state_idx = state[trial]

        # --- Stage 2 Decision ---
        logits_2 = beta2 * q_stage2_mf[state_idx]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning ---
        # Prediction error
        pe = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Asymmetric learning rate
        if pe > 0: # Technically R=1 implies PE > 0 since Q < 1
            current_lr = lr_pos
        else:
            current_lr = lr_neg
            
        q_stage2_mf[state_idx, action_2[trial]] += current_lr * pe

        # Decay unchosen option in current state
        unchosen_action = 1 - action_2[trial]
        q_stage2_mf[state_idx, unchosen_action] = (1 - decay) * q_stage2_mf[state_idx, unchosen_action] + decay * 0.5

        # Decay options in the other state (since they were not visited)
        other_state = 1 - state_idx
        q_stage2_mf[other_state, :] = (1 - decay) * q_stage2_mf[other_state, :] + decay * 0.5
            
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Pure Model-Based with Choice Kernel and Decay
This model replaces the simple "one-step back" perseveration with an **Exponential Choice Kernel**. This kernel tracks the history of choices with a decaying trace, allowing the model to capture longer-term habituation or motor perseveration effects that might influence the first-stage decision, alongside the model-based planning and value decay.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Based with Exponential Choice Kernel and Passive Decay.
    
    Uses a choice kernel (trace) to model stickiness over a longer history 
    than simple perseveration.
    
    Parameters:
    - lr: [0, 1] Learning rate for Stage 2 values.
    - beta1: [0, 10] Inverse temperature for Stage 1.
    - beta2: [0, 10] Inverse temperature for Stage 2.
    - decay: [0, 1] Value decay rate for unchosen options.
    - ck_lr: [0, 1] Decay rate for the choice kernel (higher = faster forgetting of past choices).
    - ck_beta: [0, 10] Weight of the choice kernel in Stage 1 decision.
    """
    lr, beta1, beta2, decay, ck_lr, ck_beta = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage2_mf = np.ones((2, 2)) * 0.5 
    
    # Choice kernel for Stage 1 actions (0 and 1)
    choice_kernel = np.zeros(2)
    
    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combine Model-Based value and Choice Kernel
        logits_1 = beta1 * q_stage1_mb + ck_beta * choice_kernel
        
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Decision ---
        logits_2 = beta2 * q_stage2_mf[state_idx]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning (Values) ---
        pe = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr * pe

        # Decay unchosen values
        unchosen_action = 1 - action_2[trial]
        q_stage2_mf[state_idx, unchosen_action] = (1 - decay) * q_stage2_mf[state_idx, unchosen_action] + decay * 0.5
        other_state = 1 - state_idx
        q_stage2_mf[other_state, :] = (1 - decay) * q_stage2_mf[other_state, :] + decay * 0.5
        
        # --- Learning (Choice Kernel) ---
        # Update choice kernel: reinforce chosen, decay unchosen
        a1 = action_1[trial]
        choice_kernel[a1] = (1 - ck_lr) * choice_kernel[a1] + ck_lr * 1.0
        choice_kernel[1 - a1] = (1 - ck_lr) * choice_kernel[1 - a1]
            
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model with Value Decay
This model re-introduces the Model-Free (MF) component for the first stage decision (Hybrid Model), but crucially adds the **passive decay** mechanism to both Stage 2 (aliens) and Stage 1 MF values. This tests if the participant relies on a mixture of habits (MF) and planning (MB), provided that the habit-based values are also allowed to decay when not reinforced, preventing stale habits from interfering with adaptation to drift.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model (MB + MF) with Passive Value Decay.
    
    Combines Model-Based planning with Model-Free TD learning for Stage 1.
    Applies decay to unchosen options in both Stage 2 (aliens) and Stage 1 (spaceships).
    
    Parameters:
    - lr: [0, 1] Learning rate.
    - beta1: [0, 10] Inverse temperature for Stage 1.
    - beta2: [0, 10] Inverse temperature for Stage 2.
    - w: [0, 1] Weighting parameter (1 = Pure MB, 0 = Pure MF).
    - decay: [0, 1] Decay rate for unchosen options.
    - perseveration: [0, 5] Stickiness bonus for Stage 1.
    """
    lr, beta1, beta2, w, decay, perseveration = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Q-values
    q_stage1_mf = np.ones(2) * 0.5
    q_stage2_mf = np.ones((2, 2)) * 0.5 
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # Model-Based Value
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta1 * q_net
        if last_action_1 != -1:
            logits_1[last_action_1] += perseveration
            
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_action_1 = action_1[trial]
        state_idx = state[trial]

        # --- Stage 2 Decision ---
        logits_2 = beta2 * q_stage2_mf[state_idx]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning ---
        # Stage 1 MF Update (TD-0 using Stage 2 value as proxy for state value)
        # Note: In hybrid models, MF for stage 1 is often updated via 
        # Q1(a1) += lr * (Q2(s2, a2) - Q1(a1)) or similar.
        # Here we use the realized value of the next state (Q2 of chosen action).
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr * delta_stage1
        
        # Decay unchosen Stage 1 MF
        unchosen_a1 = 1 - action_1[trial]
        q_stage1_mf[unchosen_a1] = (1 - decay) * q_stage1_mf[unchosen_a1] + decay * 0.5

        # Stage 2 MF Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr * delta_stage2

        # Decay unchosen Stage 2 MF (current state)
        unchosen_a2 = 1 - action_2[trial]
        q_stage2_mf[state_idx, unchosen_a2] = (1 - decay) * q_stage2_mf[state_idx, unchosen_a2] + decay * 0.5
        
        # Decay Stage 2 MF (other state)
        other_state = 1 - state_idx
        q_stage2_mf[other_state, :] = (1 - decay) * q_stage2_mf[other_state, :] + decay * 0.5
            
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```