Here are three new cognitive models for the two-step task.

### Model 1: Hybrid Learner with Dynamic Transition Learning
This model relaxes the assumption that the participant believes the transition probabilities are fixed (0.7/0.3). Instead, the model-based system learns the transition matrix ($T$) from experience. It updates the probability of reaching a planet given a spaceship choice based on observed transitions. This allows the agent to adapt if they perceive the transition structure to be changing or different from instructions. It also includes choice stickiness.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Dynamic Transition Learning and Stickiness.
    
    The Model-Based system learns the transition matrix T(s'|s,a) online 
    instead of using fixed probabilities. 
    
    Parameters:
    - lr_reward: [0,1] Learning rate for Q-value updates (Model-Free).
    - lr_trans: [0,1] Learning rate for transition matrix updates (Model-Based).
    - beta: [0,10] Inverse temperature for softmax.
    - w: [0,1] Weight of Model-Based system (0=MF, 1=MB).
    - stickiness: [0,5] Bonus added to the previously chosen action's logit.
    """
    lr_reward, lr_trans, beta, w, stickiness = model_parameters
    n_trials = len(action_1)
  
    # Initialize transition matrix (Action x Planet)
    # Rows: Action 0, Action 1. Cols: Planet 0, Planet 1.
    # Initialize with the common/rare structure or uniform. 
    # Here we start with the true prior but allow it to drift.
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State (Planet) x Action (Alien)
    
    last_action_1 = -1

    for trial in range(n_trials):
        # Handle missing data
        if action_1[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            last_action_1 = -1
            continue

        # --- Stage 1 Decision ---
        # Model-Based Value Calculation using current estimated transition matrix
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2

        # Hybrid Value
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf

        # Action Selection
        logits_1 = beta * q_net_stage1
        if last_action_1 != -1:
            logits_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        # --- Stage 2 Decision ---
        state_idx = state[trial] # Planet 0 or 1
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]

        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        
        # 1. Model-Free Q-Learning Update
        # Stage 1 TD error
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr_reward * delta_stage1
        
        # Stage 2 TD error
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += lr_reward * delta_stage2
        
        # 2. Model-Based Transition Update
        # Update probability of reaching 'state_idx' given 'a1'
        # T[a1, state_idx] moves towards 1
        curr_prob = transition_matrix[a1, state_idx]
        transition_matrix[a1, state_idx] += lr_trans * (1.0 - curr_prob)
        # Normalize the row (the other planet probability decreases)
        transition_matrix[a1, 1 - state_idx] = 1.0 - transition_matrix[a1, state_idx]

        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Learner with Dual Learning Rates
This model posits that learning happens at different rates for the high-level spaceship choice (Stage 1) and the concrete alien interaction (Stage 2). Stage 1 involves abstract planning, while Stage 2 involves direct reward feedback. It uses separate learning rates for these two stages, combined with choice stickiness to account for the participant's inertia.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Separate Stage 1 and Stage 2 Learning Rates.
    
    Differentiates between learning the value of spaceships (Stage 1)
    and learning the value of aliens (Stage 2).
    
    Parameters:
    - lr_stage1: [0,1] Learning rate for Stage 1 (Spaceships) Model-Free values.
    - lr_stage2: [0,1] Learning rate for Stage 2 (Aliens) values.
    - beta: [0,10] Inverse temperature.
    - w: [0,1] Weight of Model-Based system.
    - stickiness: [0,5] Bonus added to the previously chosen action's logit.
    """
    lr_stage1, lr_stage2, beta, w, stickiness = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            last_action_1 = -1
            continue

        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf

        logits_1 = beta * q_net_stage1
        if last_action_1 != -1:
            logits_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        # --- Stage 2 Decision ---
        state_idx = state[trial]
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]

        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        
        # Stage 1 Update (using lr_stage1)
        # Note: Standard MF update uses Max Q(s2) or Q(s2, a2). 
        # Here we use SARSA-style or Q-learning style. Given the task structure, 
        # using the value of the chosen state-action pair is standard.
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr_stage1 * delta_stage1
        
        # Stage 2 Update (using lr_stage2)
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += lr_stage2 * delta_stage2
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Learner with Eligibility Traces
This model implements a hybrid agent with eligibility traces (TD($\lambda$)). Unlike the standard agent which updates Stage 1 values only based on the transition to Stage 2, this agent allows the reward received at Stage 2 to directly update the Stage 1 choice via an eligibility trace. This bridges the temporal gap between the first choice and the final reward. It also includes stickiness.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Eligibility Traces (Lambda) and Stickiness.
    
    Uses an eligibility trace parameter (lam) to allow the Stage 2 reward 
    prediction error to directly update Stage 1 values.
    
    Parameters:
    - learning_rate: [0,1] Learning rate for all updates.
    - beta: [0,10] Inverse temperature.
    - w: [0,1] Weight of Model-Based system.
    - lam: [0,1] Eligibility trace decay parameter (lambda). 
           Controls how much the Stage 2 RPE affects Stage 1 values.
    - stickiness: [0,5] Bonus added to the previously chosen action's logit.
    """
    learning_rate, beta, w, lam, stickiness = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            last_action_1 = -1
            continue

        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf

        logits_1 = beta * q_net_stage1
        if last_action_1 != -1:
            logits_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        # --- Stage 2 Decision ---
        state_idx = state[trial]
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]

        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        
        # 1. Stage 1 Update (TD(0) part)
        # Update based on prediction of next state value
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # 2. Stage 2 Update
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # 3. Eligibility Trace Update for Stage 1
        # The Stage 2 RPE (delta_stage2) also updates Stage 1 value, scaled by lambda
        q_stage1_mf[a1] += learning_rate * lam * delta_stage2
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```