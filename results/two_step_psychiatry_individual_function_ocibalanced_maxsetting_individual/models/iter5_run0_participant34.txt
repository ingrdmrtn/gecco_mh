Here are three cognitive models designed to explain the participant's behavior in the two-step task.

### Model 1: Outcome-Dependent Exploration Model
This model hypothesizes that the participant's balance between exploration and exploitation in the first stage is modulated by the outcome of the previous trial. Specifically, a "Win" (reward) induces a higher inverse temperature (more deterministic/exploitative behavior), while a "Loss" (no reward) induces a lower inverse temperature (more random/exploratory behavior). This aligns with the participant's tendency to stick to a choice in long blocks (high beta) but switch or explore after failures.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Outcome-Dependent Exploration Model.
    
    Adjusts the inverse temperature (beta) for the first stage choice based on 
    whether the previous trial was rewarded. This captures 'Win-Exploit' and 
    'Lose-Explore' dynamics.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for value updates.
    - beta_1_win: [0, 10] Inverse temperature for Stage 1 after a reward.
    - beta_1_loss: [0, 10] Inverse temperature for Stage 1 after no reward.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - w: [0, 1] Weight for Model-Based control.
    - stickiness: [0, 5] General choice perseveration bonus.
    """
    learning_rate, beta_1_win, beta_1_loss, beta_2, w, stickiness = model_parameters
    n_trials = len(action_1)
    
    # Fixed transition matrix: Row 0 -> [0.7, 0.3], Row 1 -> [0.3, 0.7]
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_mf1 = np.zeros(2)       # Model-free Q-values for Stage 1
    q_s2 = np.zeros((2, 2))   # Q-values for Stage 2 (State x Action)
    
    last_action_1 = -1
    last_reward = 0  # Initialize assuming no reward prior to trial 1
    
    for t in range(n_trials):
        # --- STAGE 1 CHOICE ---
        # Model-Based Value Calculation
        max_q_s2 = np.max(q_s2, axis=1)
        q_mb1 = transition_matrix @ max_q_s2
        
        # Hybrid Value
        q_net = w * q_mb1 + (1 - w) * q_mf1
        
        # Apply Stickiness
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        # Select Beta based on previous outcome
        current_beta_1 = beta_1_win if last_reward == 1 else beta_1_loss
        
        # Softmax Policy Stage 1
        q_net_stable = q_net - np.max(q_net)
        exp_q1 = np.exp(current_beta_1 * q_net_stable)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[t] = probs_1[action_1[t]]
        
        # Record Stage 1 Data
        a1 = action_1[t]
        s_next = state[t]
        last_action_1 = a1
        
        # --- STAGE 2 CHOICE ---
        q_s2_curr = q_s2[s_next]
        q_s2_stable = q_s2_curr - np.max(q_s2_curr)
        exp_q2 = np.exp(beta_2 * q_s2_stable)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[t] = probs_2[action_2[t]]
        
        # Record Stage 2 Data
        a2 = action_2[t]
        r = reward[t]
        last_reward = r
        
        # --- UPDATES ---
        # TD Update for Stage 1 (Model-Free)
        pe1 = q_s2[s_next, a2] - q_mf1[a1]
        q_mf1[a1] += learning_rate * pe1
        
        # TD Update for Stage 2
        pe2 = r - q_s2[s_next, a2]
        q_s2[s_next, a2] += learning_rate * pe2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Fully Asymmetric Learning & Stickiness
This model extends the "best so far" concept by allowing asymmetry in *both* the value updating (learning rate) and the habitual persistence (stickiness). This allows the model to capture complex behaviors, such as learning quickly from wins but slowly from losses (or vice versa), while simultaneously having a different tendency to repeat a choice depending on whether it was previously successful.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Fully Asymmetric Model.
    
    Differentiates between positive and negative prediction errors (learning rates)
    AND between rewarded and unrewarded previous actions (stickiness).
    
    Parameters:
    - lr_pos: [0, 1] Learning rate for positive prediction errors.
    - lr_neg: [0, 1] Learning rate for negative prediction errors.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - w: [0, 1] Weight for Model-Based control.
    - stick_win: [0, 5] Stickiness bias if previous trial was rewarded.
    - stick_loss: [0, 5] Stickiness bias if previous trial was unrewarded.
    """
    lr_pos, lr_neg, beta_1, beta_2, w, stick_win, stick_loss = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_mf1 = np.zeros(2)
    q_s2 = np.zeros((2, 2))
    
    last_action_1 = -1
    last_reward = 0
    
    for t in range(n_trials):
        # --- STAGE 1 CHOICE ---
        max_q_s2 = np.max(q_s2, axis=1)
        q_mb1 = transition_matrix @ max_q_s2
        
        q_net = w * q_mb1 + (1 - w) * q_mf1
        
        # Apply Asymmetric Stickiness
        if last_action_1 != -1:
            if last_reward == 1:
                q_net[last_action_1] += stick_win
            else:
                q_net[last_action_1] += stick_loss
        
        q_net_stable = q_net - np.max(q_net)
        exp_q1 = np.exp(beta_1 * q_net_stable)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[t] = probs_1[action_1[t]]
        
        a1 = action_1[t]
        s_next = state[t]
        last_action_1 = a1
        
        # --- STAGE 2 CHOICE ---
        q_s2_curr = q_s2[s_next]
        q_s2_stable = q_s2_curr - np.max(q_s2_curr)
        exp_q2 = np.exp(beta_2 * q_s2_stable)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[t] = probs_2[action_2[t]]
        
        a2 = action_2[t]
        r = reward[t]
        last_reward = r
        
        # --- UPDATES (Asymmetric Learning Rates) ---
        
        # Stage 1 Update
        pe1 = q_s2[s_next, a2] - q_mf1[a1]
        lr_1 = lr_pos if pe1 >= 0 else lr_neg
        q_mf1[a1] += lr_1 * pe1
        
        # Stage 2 Update
        pe2 = r - q_s2[s_next, a2]
        lr_2 = lr_pos if pe2 >= 0 else lr_neg
        q_s2[s_next, a2] += lr_2 * pe2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Dynamic Transition Learning Model
The standard model assumes the participant knows the transition probabilities (0.7/0.3) are fixed. However, the participant might be learning these probabilities over time, especially given the "rare transitions" mentioned. This model updates its internal model of the spaceship-planet transitions based on experience, which influences the Model-Based value calculation.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Dynamic Transition Learning Model.
    
    The participant estimates the transition probabilities (Spaceship -> Planet)
    online, rather than assuming fixed 0.7/0.3 values. These estimated transitions
    are used for the Model-Based value calculation.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for Q-values (Stage 1 & 2).
    - lr_trans: [0, 1] Learning rate for transition probabilities.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - w: [0, 1] Weight for Model-Based control.
    - stickiness: [0, 5] General choice perseveration bonus.
    """
    learning_rate, lr_trans, beta_1, beta_2, w, stickiness = model_parameters
    n_trials = len(action_1)
    
    # Initialize transitions with prior knowledge (commonly A->X, U->Y)
    # Row 0 (A) -> [0.7, 0.3], Row 1 (U) -> [0.3, 0.7]
    est_trans_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_mf1 = np.zeros(2)
    q_s2 = np.zeros((2, 2))
    
    last_action_1 = -1
    
    for t in range(n_trials):
        # --- STAGE 1 CHOICE ---
        # Use ESTIMATED transition matrix for MB calculation
        max_q_s2 = np.max(q_s2, axis=1)
        q_mb1 = est_trans_matrix @ max_q_s2
        
        q_net = w * q_mb1 + (1 - w) * q_mf1
        
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        q_net_stable = q_net - np.max(q_net)
        exp_q1 = np.exp(beta_1 * q_net_stable)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[t] = probs_1[action_1[t]]
        
        a1 = action_1[t]
        s_next = state[t]
        last_action_1 = a1
        
        # --- STAGE 2 CHOICE ---
        q_s2_curr = q_s2[s_next]
        q_s2_stable = q_s2_curr - np.max(q_s2_curr)
        exp_q2 = np.exp(beta_2 * q_s2_stable)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[t] = probs_2[action_2[t]]
        
        a2 = action_2[t]
        r = reward[t]
        
        # --- UPDATES ---
        
        # 1. Update Transition Matrix Estimate
        # Move probability mass towards the observed transition (a1 -> s_next)
        est_trans_matrix[a1, s_next] += lr_trans * (1.0 - est_trans_matrix[a1, s_next])
        # Ensure probabilities sum to 1 (update the other outcome)
        est_trans_matrix[a1, 1 - s_next] = 1.0 - est_trans_matrix[a1, s_next]
        
        # 2. Update Q-values
        pe1 = q_s2[s_next, a2] - q_mf1[a1]
        q_mf1[a1] += learning_rate * pe1
        
        pe2 = r - q_s2[s_next, a2]
        q_s2[s_next, a2] += learning_rate * pe2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```