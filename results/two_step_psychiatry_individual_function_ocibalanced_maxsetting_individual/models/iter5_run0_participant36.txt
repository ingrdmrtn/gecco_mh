Here are the three proposed cognitive models.

### Model 1: Dynamic Transition Learning
This model assumes the participant does not treat the transition probabilities (Spaceship $\to$ Planet) as fixed constants (0.7/0.3), but instead updates their internal model of the transition matrix based on observed transitions. This allows the "Model-Based" component to evolve, potentially explaining reactions to "rare" transitions as updates to the world model.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Dynamic Transition Learning Model.
    
    The agent updates the transition matrix T(s'|s,a) online.
    
    Parameters:
    lr: Learning rate for Q-values [0,1]
    lr_trans: Learning rate for transition probabilities [0,1]
    beta: Inverse temperature [0,10]
    w: Model-based weight [0,1]
    lam: Eligibility trace [0,1]
    p: Perseveration (stickiness) [0,1]
    """
    lr, lr_trans, beta, w, lam, p = model_parameters
    n_trials = len(action_1)
    
    # Initialize transition matrix (rows: spaceship A/U, cols: planet X/Y)
    # Start with the instructed probabilities
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1
    
    for trial in range(n_trials):
        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add perseveration to the net value of the previous action
        if prev_action_1 != -1:
            q_net_stage1[prev_action_1] += p
            
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        s1_choice = action_1[trial]
        state_idx = state[trial] # Actual planet reached (0 or 1)
        prev_action_1 = s1_choice
        
        # Update Transition Matrix
        # T(observed_planet | chosen_ship) moves towards 1
        # T(other_planet | chosen_ship) moves towards 0
        # Note: s1_choice 0 -> row 0, s1_choice 1 -> row 1
        # We update the row corresponding to the chosen spaceship
        transition_matrix[s1_choice, state_idx] += lr_trans * (1 - transition_matrix[s1_choice, state_idx])
        transition_matrix[s1_choice, 1 - state_idx] = 1.0 - transition_matrix[s1_choice, state_idx]
        
        # Stage 2 Policy
        s2_choice = action_2[trial]
        r = reward[trial]

        if s2_choice != -1:
            qs_stage2 = q_stage2_mf[state_idx]
            exp_q2 = np.exp(beta * qs_stage2)
            probs_2 = exp_q2 / np.sum(exp_q2)
            p_choice_2[trial] = probs_2[s2_choice]

            # Updates
            delta_stage1 = q_stage2_mf[state_idx, s2_choice] - q_stage1_mf[s1_choice]
            q_stage1_mf[s1_choice] += lr * delta_stage1
            
            delta_stage2 = r - q_stage2_mf[state_idx, s2_choice]
            q_stage2_mf[state_idx, s2_choice] += lr * delta_stage2
            
            # Eligibility trace update for Stage 1
            q_stage1_mf[s1_choice] += lr * lam * delta_stage2
        else:
            p_choice_2[trial] = 1.0

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Transition-Dependent Learning Rates
This model posits that the participant updates their Model-Free values differently depending on whether the transition was "Common" or "Rare". This modulation allows the agent to suppress misleading Model-Free updates that occur after rare transitions (a behavior mimicking Model-Based control but implemented via learning rate modulation).

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Transition-Dependent Learning Rates Model.
    
    Uses separate learning rates for Stage 1 MF updates depending on whether
    the transition was Common or Rare.
    
    Parameters:
    lr_common: Learning rate for common transitions [0,1]
    lr_rare: Learning rate for rare transitions [0,1]
    beta: Inverse temperature [0,10]
    w: Model-based weight [0,1]
    lam: Eligibility trace [0,1]
    p: Perseveration (stickiness) [0,1]
    """
    lr_common, lr_rare, beta, w, lam, p = model_parameters
    n_trials = len(action_1)
    
    # Fixed transition structure for MB calculation
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1
    
    for trial in range(n_trials):
        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if prev_action_1 != -1:
            q_net_stage1[prev_action_1] += p
            
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        s1_choice = action_1[trial]
        state_idx = state[trial]
        prev_action_1 = s1_choice
        
        # Determine if transition was Common or Rare
        # Common: (Action 0 -> State 0) or (Action 1 -> State 1)
        is_common = (s1_choice == state_idx)
        current_lr = lr_common if is_common else lr_rare
        
        # Stage 2 Policy
        s2_choice = action_2[trial]
        r = reward[trial]

        if s2_choice != -1:
            qs_stage2 = q_stage2_mf[state_idx]
            exp_q2 = np.exp(beta * qs_stage2)
            probs_2 = exp_q2 / np.sum(exp_q2)
            p_choice_2[trial] = probs_2[s2_choice]

            # Update Stage 1 MF using the transition-dependent learning rate
            delta_stage1 = q_stage2_mf[state_idx, s2_choice] - q_stage1_mf[s1_choice]
            q_stage1_mf[s1_choice] += current_lr * delta_stage1
            
            # Update Stage 2 (standard learning rate, using average of lr_common/rare or just lr_common? 
            # Let's use lr_common for stage 2 as it's a direct experience)
            delta_stage2 = r - q_stage2_mf[state_idx, s2_choice]
            q_stage2_mf[state_idx, s2_choice] += lr_common * delta_stage2
            
            # Eligibility trace update for Stage 1 (also modulated by current_lr)
            q_stage1_mf[s1_choice] += current_lr * lam * delta_stage2
        else:
            p_choice_2[trial] = 1.0

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Accumulated Perseveration
This model replaces the standard 1-step perseveration parameter with a "Perseveration Trace". This trace accumulates when an action is repeated and decays over time. This mechanism is designed to capture the "streaky" behavior (long runs of choosing the same spaceship) observed in the participant's data better than a static bonus for the immediately preceding choice.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Accumulated Perseveration Model.
    
    Features a dynamic perseveration trace that builds up with repeated choices
    and decays over time, encouraging 'streaky' behavior.
    
    Parameters:
    lr: Learning rate [0,1]
    beta: Inverse temperature [0,10]
    w: Model-based weight [0,1]
    lam: Eligibility trace [0,1]
    w_p: Weight of the accumulated perseveration trace [0,1]
    decay_p: Decay rate of the perseveration trace [0,1]
    """
    lr, beta, w, lam, w_p, decay_p = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Perseveration trace for the two spaceships
    pers_trace = np.zeros(2)
    
    for trial in range(n_trials):
        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net value includes MB, MF, and the Accumulated Perseveration Trace
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf + w_p * pers_trace
            
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        s1_choice = action_1[trial]
        state_idx = state[trial]
        
        # Update Perseveration Trace
        # Decay both traces
        pers_trace *= decay_p
        # Increment the trace for the chosen action
        pers_trace[s1_choice] += 1.0
        
        # Stage 2 Policy
        s2_choice = action_2[trial]
        r = reward[trial]

        if s2_choice != -1:
            qs_stage2 = q_stage2_mf[state_idx]
            exp_q2 = np.exp(beta * qs_stage2)
            probs_2 = exp_q2 / np.sum(exp_q2)
            p_choice_2[trial] = probs_2[s2_choice]

            delta_stage1 = q_stage2_mf[state_idx, s2_choice] - q_stage1_mf[s1_choice]
            q_stage1_mf[s1_choice] += lr * delta_stage1
            
            delta_stage2 = r - q_stage2_mf[state_idx, s2_choice]
            q_stage2_mf[state_idx, s2_choice] += lr * delta_stage2
            
            q_stage1_mf[s1_choice] += lr * lam * delta_stage2
        else:
            p_choice_2[trial] = 1.0

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```