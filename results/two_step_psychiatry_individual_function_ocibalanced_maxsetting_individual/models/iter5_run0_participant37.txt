Here are 3 new cognitive models based on the participant data and the provided feedback. These models introduce specific mechanisms to capture nuances in learning and perseveration (stickiness) that go beyond the standard hybrid model.

### Model 1: Reward-Dependent Stickiness (WSLS-RL Hybrid)
This model hypothesizes that the participant's tendency to repeat a choice (stickiness) is not constant but depends on the outcome of the previous trial. It integrates a "Win-Stay, Lose-Shift" logic directly into the reinforcement learning value estimation by having separate stickiness parameters for rewarded (`stick_win`) and unrewarded (`stick_loss`) previous trials.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid model with Reward-Dependent Stickiness.
    Differentiates perseveration based on the previous outcome (Win-Stay vs Lose-Stay).
    
    Parameters:
    learning_rate: [0,1] - Learning rate for value updates.
    beta: [0,10] - Inverse temperature (exploration/exploitation).
    w: [0,1] - Weight of Model-Based system (0=MF, 1=MB).
    lam: [0,1] - Eligibility trace (credit assignment to Stage 1).
    stick_win: [0,5] - Stickiness bonus applied if the previous trial was rewarded.
    stick_loss: [0,5] - Stickiness bonus applied if the previous trial was unrewarded.
    """
    learning_rate, beta, w, lam, stick_win, stick_loss = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    last_reward = -1

    for trial in range(n_trials):
        if action_1[trial] == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue
            
        # --- STAGE 1 CHOICE ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_hybrid = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Apply Reward-Dependent Stickiness
        q_choice_1 = q_hybrid.copy()
        if last_action_1 != -1:
            if last_reward == 1:
                q_choice_1[last_action_1] += stick_win
            else:
                q_choice_1[last_action_1] += stick_loss
        
        exp_q1 = np.exp(beta * q_choice_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- STAGE 2 CHOICE ---
        state_idx = state[trial]
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- LEARNING ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        q_stage1_mf[action_1[trial]] += learning_rate * (delta_stage1 + lam * delta_stage2)
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        last_action_1 = action_1[trial]
        last_reward = reward[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Asymmetric Eligibility Trace
This model investigates if the participant assigns credit from the Stage 2 outcome to the Stage 1 choice differently depending on whether the outcome was positive or negative. It splits the eligibility trace parameter `lam` into `lam_win` (for rewards) and `lam_loss` (for omissions), allowing for asymmetric reinforcement effects.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid model with Asymmetric Eligibility Trace.
    The strength of the link between Stage 2 outcome and Stage 1 value update
    depends on the valence of the outcome (Reward vs No Reward).
    
    Parameters:
    learning_rate: [0,1] - Learning rate.
    beta: [0,10] - Inverse temperature.
    w: [0,1] - Weight of Model-Based system.
    lam_win: [0,1] - Eligibility trace strength when reward is received.
    lam_loss: [0,1] - Eligibility trace strength when reward is NOT received.
    stickiness: [0,5] - General choice stickiness for Stage 1.
    """
    learning_rate, beta, w, lam_win, lam_loss, stickiness = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue
            
        # --- STAGE 1 CHOICE ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_hybrid = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        q_choice_1 = q_hybrid.copy()
        if last_action_1 != -1:
            q_choice_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_choice_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- STAGE 2 CHOICE ---
        state_idx = state[trial]
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- LEARNING ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Select lambda based on reward outcome
        current_lam = lam_win if reward[trial] > 0 else lam_loss
        
        q_stage1_mf[action_1[trial]] += learning_rate * (delta_stage1 + current_lam * delta_stage2)
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        last_action_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Dual-Stage Stickiness
The participant data shows long runs of choosing the same alien in Stage 2. This model adds a dedicated stickiness parameter for Stage 2 (`stick_2`) in addition to the standard Stage 1 stickiness (`stick_1`). This captures the tendency to repeat choices at the specific alien level (within a planet), distinct from repeating the spaceship choice.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid model with Dual-Stage Stickiness.
    Includes separate stickiness parameters for the spaceship choice (Stage 1)
    and the alien choice (Stage 2).
    
    Parameters:
    learning_rate: [0,1] - Learning rate.
    beta: [0,10] - Inverse temperature.
    w: [0,1] - Weight of Model-Based system.
    lam: [0,1] - Eligibility trace.
    stick_1: [0,5] - Stickiness for Stage 1 (Spaceship choice).
    stick_2: [0,5] - Stickiness for Stage 2 (Alien choice, per state).
    """
    learning_rate, beta, w, lam, stick_1, stick_2 = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    # Track last action for each state in stage 2 to apply local stickiness
    last_action_2_per_state = [-1, -1] 

    for trial in range(n_trials):
        if action_1[trial] == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue
            
        # --- STAGE 1 CHOICE ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_hybrid = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        q_choice_1 = q_hybrid.copy()
        if last_action_1 != -1:
            q_choice_1[last_action_1] += stick_1
            
        exp_q1 = np.exp(beta * q_choice_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- STAGE 2 CHOICE ---
        state_idx = state[trial]
        q_choice_2 = q_stage2_mf[state_idx].copy()
        
        # Apply Stage 2 stickiness (stick to the alien chosen last time on this planet)
        prev_a2 = last_action_2_per_state[state_idx]
        if prev_a2 != -1:
            q_choice_2[prev_a2] += stick_2
            
        exp_q2 = np.exp(beta * q_choice_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- LEARNING ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        q_stage1_mf[action_1[trial]] += learning_rate * (delta_stage1 + lam * delta_stage2)
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        last_action_1 = action_1[trial]
        last_action_2_per_state[state_idx] = action_2[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```