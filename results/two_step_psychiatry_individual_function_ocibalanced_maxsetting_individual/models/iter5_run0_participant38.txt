Here are the 3 proposed cognitive models.

### Model 1: Hybrid Model with Separate Stage Learning Rates
This model distinguishes between the learning dynamics of the two stages. It hypothesizes that the participant may update their values for the spaceships (Stage 1) at a different rate than they update their values for the aliens (Stage 2). This allows the model to capture different sensitivities to the transition structure versus the drifting reward probabilities.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Separate Learning Rates for Stage 1 and Stage 2.
    
    Allows for different plasticity in the spaceship preference (Stage 1) 
    versus the alien reward estimation (Stage 2).
    
    Parameters:
    - lr_1: [0, 1] Learning rate for Stage 1 (Spaceship) MF value updates.
    - lr_2: [0, 1] Learning rate for Stage 2 (Alien) MF value updates.
    - beta_1: [0, 10] Inverse temperature for Stage 1 choices.
    - beta_2: [0, 10] Inverse temperature for Stage 2 choices.
    - w: [0, 1] Mixing weight. 0 = Pure MF, 1 = Pure MB.
    """
    lr_1, lr_2, beta_1, beta_2, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Initialize Q-values to 0.5 (neutral) as rewards are 0/1
    q_stage1_mf = np.full(2, 0.5)
    q_stage2_mf = np.full((2, 2), 0.5)

    for trial in range(n_trials):
        # Handle missing data
        if action_1[trial] == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        # --- Stage 1 Policy ---
        # Model-Based value: Expected value of next state given transition matrix
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net value: Weighted sum of MB and MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Value Updates ---
        # Update Stage 1 MF values using TD-error from Stage 2 values
        # Uses lr_1
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr_1 * delta_stage1
        
        # Update Stage 2 MF values using Reward Prediction Error
        # Uses lr_2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr_2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model with Counterfactual Updating
This model posits that when the participant chooses an alien and receives a reward (or lack thereof), they also update the value of the *unchosen* alien on that planet. They assume the unchosen alien would have provided the opposite outcome (e.g., if chosen gave 0, unchosen would have given 1). This "fictitious play" or counterfactual updating is common in binary choice tasks.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Counterfactual Updating (Fictitious Play).
    
    When an outcome is observed for the chosen alien, the unchosen alien's 
    value is updated assuming the opposite reward (1 - reward).
    
    Parameters:
    - lr: [0, 1] Learning rate for the chosen option.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - w: [0, 1] Mixing weight. 0 = Pure MF, 1 = Pure MB.
    - cf_lr: [0, 1] Counterfactual learning rate for the unchosen option.
    """
    lr, beta_1, beta_2, w, cf_lr = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Initialize to 0.5
    q_stage1_mf = np.full(2, 0.5)
    q_stage2_mf = np.full((2, 2), 0.5)

    for trial in range(n_trials):
        if action_1[trial] == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Value Updates ---
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr * delta_stage1
        
        # Stage 2 Update (Chosen)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr * delta_stage2
        
        # Stage 2 Update (Unchosen / Counterfactual)
        unchosen_action = 1 - action_2[trial]
        # Assume unchosen would have yielded (1 - reward)
        fictitious_reward = 1 - reward[trial]
        delta_cf = fictitious_reward - q_stage2_mf[state_idx, unchosen_action]
        q_stage2_mf[state_idx, unchosen_action] += cf_lr * delta_cf
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model with Decay and Choice Stickiness
This model combines two effective mechanisms: value decay for unchosen aliens (to handle drifting probabilities) and choice stickiness (perseveration) for spaceships. The stickiness parameter captures the tendency to repeat the previous Stage 1 choice regardless of the outcome, a common behavioral heuristic.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Unchosen Value Decay and Stage 1 Stickiness.
    
    Combines 'decay' (unchosen Stage 2 values decay to 0.5) to track drift,
    and 'stick' (bonus for repeating Stage 1 choice) to capture perseveration.
    
    Parameters:
    - lr: [0, 1] Learning rate.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - w: [0, 1] Mixing weight.
    - decay: [0, 1] Decay rate for unchosen Stage 2 values towards 0.5.
    - stick: [0, 5] Stickiness bonus added to logits of the previously chosen spaceship.
    """
    lr, beta_1, beta_2, w, decay, stick = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.full(2, 0.5)
    q_stage2_mf = np.full((2, 2), 0.5)
    
    prev_a1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            prev_a1 = -1
            continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Calculate logits and add stickiness bonus
        logits_1 = beta_1 * q_net
        if prev_a1 != -1:
            logits_1[prev_a1] += stick
            
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Value Updates ---
        # Stage 1
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr * delta_stage1
        
        # Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr * delta_stage2
        
        # Decay unchosen Stage 2 option
        unchosen_a2 = 1 - action_2[trial]
        q_stage2_mf[state_idx, unchosen_a2] = (1 - decay) * q_stage2_mf[state_idx, unchosen_a2] + decay * 0.5
        
        prev_a1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```