Here are the three proposed cognitive models.

### Model 1: Learned Transition Structure
This model assumes the participant does not rely on the fixed transition probabilities (0.7/0.3) but learns them dynamically over time. This allows the Model-Based system to adjust its beliefs about which spaceship leads to which planet, potentially explaining behavior where the participant reacts to "rare" transitions as if the structure has changed.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF learner with Dynamic Transition Learning.
    
    The participant learns the transition matrix T(s'|s,a) online using a
    separate learning rate. The Model-Based value calculation uses this 
    learned matrix instead of the fixed true probabilities.
    
    Parameters:
    - lr_rew: Reward learning rate for value updates [0,1]
    - lr_trans: Transition probability learning rate [0,1]
    - beta_1: Inverse temperature for stage 1 [0,10]
    - beta_2: Inverse temperature for stage 2 [0,10]
    - w: Mixing weight (0=MF, 1=MB) [0,1]
    - stick: Choice stickiness for stage 1 [0,5]
    """
    lr_rew, lr_trans, beta_1, beta_2, w, stick = model_parameters
    n_trials = len(action_1)
    
    # Initialize learned transition matrix: [Action, Planet]
    # Start with uniform prior (0.5) or slight bias. Using 0.5 assumes no prior knowledge.
    learned_transitions = np.ones((2, 2)) * 0.5
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # 2 planets, 2 aliens
    
    log_loss = 0.0
    eps = 1e-10
    prev_action_1 = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        
        # Model-Based Value: Use LEARNED transitions
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = learned_transitions @ max_q_stage2
        
        # Net Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness
        if prev_action_1 != -1:
            q_net[prev_action_1] += stick
            
        # Softmax
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = action_1[trial]
        log_loss -= np.log(probs_1[a1] + eps)
        
        # --- Transition Update ---
        s_next = state[trial]
        
        # Update transition probabilities for the chosen action
        # Linear interpolation update ensures row sums to 1
        # P_new(s'|a) = (1-alpha)*P_old(s'|a) + alpha*Indicator(s')
        learned_transitions[a1, :] *= (1 - lr_trans)
        learned_transitions[a1, s_next] += lr_trans
        
        prev_action_1 = a1
        
        # --- Stage 2 Decision ---
        a2 = action_2[trial]
        if a2 == -1: # Missing data handle
            continue
            
        qs_2 = q_stage2_mf[s_next]
        exp_q2 = np.exp(beta_2 * qs_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        log_loss -= np.log(probs_2[a2] + eps)
        
        # --- Reward Update ---
        r = reward[trial]
        
        # TD Update Stage 2
        delta_stage2 = r - q_stage2_mf[s_next, a2]
        q_stage2_mf[s_next, a2] += lr_rew * delta_stage2
        
        # TD Update Stage 1 (Model-Free)
        delta_stage1 = q_stage2_mf[s_next, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr_rew * delta_stage1
        
    return log_loss
```

### Model 2: Accumulating Habit (Trace)
The participant exhibits long "runs" of choosing the same spaceship (inertia). Standard stickiness applies a constant bonus based only on the *immediate* previous trial. This model replaces simple stickiness with an **accumulating choice trace**, allowing habits to build up over repeated choices and decay slowly when not reinforced, better capturing the observed momentum.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF learner with Accumulating Choice Trace (Habit).
    
    Instead of 1-step stickiness, this model maintains a 'trace' of past choices
    that decays over time. Repeatedly choosing an option builds up a strong
    habit (trace) that is harder to break than simple stickiness.
    
    Parameters:
    - lr: Learning rate [0,1]
    - beta_1: Inverse temperature for stage 1 [0,10]
    - beta_2: Inverse temperature for stage 2 [0,10]
    - w: Mixing weight (0=MF, 1=MB) [0,1]
    - trace_decay: Decay rate of the choice trace (0=instant decay, 1=no decay) [0,1]
    - trace_weight: Weight of the trace in the decision [0,5]
    """
    lr, beta_1, beta_2, w, trace_decay, trace_weight = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Choice trace for the two spaceships
    choice_trace = np.zeros(2)
    
    log_loss = 0.0
    eps = 1e-10
    
    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add accumulating trace bonus
        q_net += trace_weight * choice_trace
        
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = action_1[trial]
        log_loss -= np.log(probs_1[a1] + eps)
        
        # Update Choice Trace
        # Decay both traces
        choice_trace *= trace_decay
        # Increment trace for chosen action (additive update)
        choice_trace[a1] += 1.0
        
        # --- Stage 2 Decision ---
        s_next = state[trial]
        a2 = action_2[trial]
        if a2 == -1:
            continue
            
        qs_2 = q_stage2_mf[s_next]
        exp_q2 = np.exp(beta_2 * qs_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        log_loss -= np.log(probs_2[a2] + eps)
        
        # --- Reward Update ---
        r = reward[trial]
        
        delta_stage2 = r - q_stage2_mf[s_next, a2]
        q_stage2_mf[s_next, a2] += lr * delta_stage2
        
        delta_stage1 = q_stage2_mf[s_next, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
    return log_loss
```

### Model 3: Outcome-Based (Planet) Stickiness
This model distinguishes between sticking to a *motor action* (Spaceship A) and sticking to a *goal state* (Planet X). If the participant develops a preference for a specific planet (e.g., because they were just there), they should choose the spaceship most likely to take them back to that planet. This model includes both action stickiness and **outcome stickiness**.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF learner with Outcome-Based (Planet) Stickiness.
    
    Adds a bias to the stage 1 choice based on the planet visited in the 
    previous trial. If the participant wants to return to the same planet,
    they are biased towards the action that leads there (Outcome Stickiness).
    This is distinct from simply repeating the same key press (Action Stickiness).
    
    Parameters:
    - lr: Learning rate [0,1]
    - beta_1: Inverse temperature for stage 1 [0,10]
    - beta_2: Inverse temperature for stage 2 [0,10]
    - w: Mixing weight (0=MF, 1=MB) [0,1]
    - stick_action: Bias for repeating the previous spaceship choice [0,5]
    - stick_outcome: Bias for the spaceship leading to the previous planet [0,5]
    """
    lr, beta_1, beta_2, w, stick_action, stick_outcome = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    log_loss = 0.0
    eps = 1e-10
    prev_action_1 = -1
    prev_state = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Action Stickiness (Motor perseveration)
        if prev_action_1 != -1:
            q_net[prev_action_1] += stick_action
            
        # Outcome Stickiness (Goal perseveration)
        # If we were at Planet P last time, boost actions leading to P
        if prev_state != -1:
            # Calculate probability of reaching prev_state for each action
            # transition_matrix[a, s']
            prob_reach_prev = transition_matrix[:, prev_state]
            q_net += stick_outcome * prob_reach_prev
            
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = action_1[trial]
        log_loss -= np.log(probs_1[a1] + eps)
        
        prev_action_1 = a1
        
        # --- Stage 2 Decision ---
        s_next = state[trial]
        prev_state = s_next # Store for next trial
        
        a2 = action_2[trial]
        if a2 == -1:
            continue
            
        qs_2 = q_stage2_mf[s_next]
        exp_q2 = np.exp(beta_2 * qs_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        log_loss -= np.log(probs_2[a2] + eps)
        
        # --- Reward Update ---
        r = reward[trial]
        
        delta_stage2 = r - q_stage2_mf[s_next, a2]
        q_stage2_mf[s_next, a2] += lr * delta_stage2
        
        delta_stage1 = q_stage2_mf[s_next, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
    return log_loss
```