Here are three new cognitive models for the two-step decision task.

### Model 1: Adaptive Transition Model-Based Learner
This model assumes the participant is a pure Model-Based learner who does not assume the transition probabilities (Spaceship $\to$ Planet) are fixed. Instead, they learn the transition matrix $T(s'|s,a)$ trial-by-trial using a delta rule. This allows the agent to adapt if they perceive the transition structure as volatile or unknown.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Adaptive Transition Model-Based Learner.
    The agent learns the transition probabilities between spaceships and planets
    instead of assuming them fixed at 0.7/0.3. It uses these learned transitions 
    for Model-Based planning.
    
    Parameters:
    lr_val: [0, 1] Learning rate for Stage 2 Q-values (aliens).
    lr_trans: [0, 1] Learning rate for updating transition probabilities.
    beta_1: [0, 10] Inverse temperature for Stage 1 choice.
    beta_2: [0, 10] Inverse temperature for Stage 2 choice.
    stickiness: [0, 5] Perseveration bonus for Stage 1.
    """
    lr_val, lr_trans, beta_1, beta_2, stickiness = model_parameters
    n_trials = len(action_1)
    
    # Initialize transition matrix: T[action_1, next_state]
    # Start with uniform beliefs (0.5) representing uncertainty
    trans_probs = np.ones((2, 2)) * 0.5
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Stage 2 Q-values (State x Alien)
    q_stage2 = np.zeros((2, 2)) 
    prev_a1 = -1
    
    for t in range(n_trials):
        # --- Stage 1 Policy (Model-Based) ---
        # Q_MB(a) = Sum_s' T(s'|a) * V(s')
        # where V(s') = max_a' Q2(s', a')
        max_q2 = np.max(q_stage2, axis=1)
        q_mb = trans_probs @ max_q2
        
        logits_1 = beta_1 * q_mb
        if prev_a1 != -1:
            logits_1[prev_a1] += stickiness
        
        exp_1 = np.exp(logits_1 - np.max(logits_1))
        prob_1 = exp_1 / np.sum(exp_1)
        p_choice_1[t] = prob_1[action_1[t]]
        
        # --- Transition Update ---
        a1 = action_1[t]
        s_curr = state[t]
        
        # Update transition belief for the chosen spaceship
        # Increase prob of observed state: P_new = P_old + alpha * (1 - P_old)
        trans_probs[a1, s_curr] += lr_trans * (1.0 - trans_probs[a1, s_curr])
        
        # Decrease prob of unobserved state: P_new = P_old + alpha * (0 - P_old)
        s_other = 1 - s_curr
        trans_probs[a1, s_other] += lr_trans * (0.0 - trans_probs[a1, s_other])
        
        # --- Stage 2 Policy ---
        a2 = action_2[t]
        if a2 != -1:
            logits_2 = beta_2 * q_stage2[s_curr]
            exp_2 = np.exp(logits_2 - np.max(logits_2))
            prob_2 = exp_2 / np.sum(exp_2)
            p_choice_2[t] = prob_2[a2]
            
            # Update Stage 2 Values
            pe = reward[t] - q_stage2[s_curr, a2]
            q_stage2[s_curr, a2] += lr_val * pe
        else:
            p_choice_2[t] = 1.0 # Handle missing data
        
        prev_a1 = a1
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Dual Stickiness Hybrid Learner
This model extends the standard Hybrid (MB/MF) learner by including "Stickiness" (perseveration) on **both** stages. The participant data suggests they often repeat choices for specific aliens within a planet, not just spaceships. This model captures habit-like repetition at both levels of the decision tree.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner with Dual Stickiness.
    Combines Model-Based and Model-Free learning with perseveration (stickiness)
    applied to both Stage 1 (Spaceship) and Stage 2 (Alien) choices.
    
    Parameters:
    learning_rate: [0, 1] Learning rate for Q-values.
    beta_1: [0, 10] Inverse temperature for Stage 1.
    beta_2: [0, 10] Inverse temperature for Stage 2.
    w: [0, 1] Mixing weight (0=MF, 1=MB).
    stick_1: [0, 5] Stickiness for Stage 1 choice.
    stick_2: [0, 5] Stickiness for Stage 2 choice.
    """
    learning_rate, beta_1, beta_2, w, stick_1, stick_2 = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_mf_1 = np.zeros(2)
    q_mf_2 = np.zeros((2, 2))
    
    prev_a1 = -1
    prev_a2 = np.array([-1, -1]) # Last choice for each state (planet)
    
    for t in range(n_trials):
        # --- Stage 1 Policy (Hybrid) ---
        max_q_mf_2 = np.max(q_mf_2, axis=1)
        q_mb_1 = transition_matrix @ max_q_mf_2
        q_net_1 = w * q_mb_1 + (1 - w) * q_mf_1
        
        logits_1 = beta_1 * q_net_1
        if prev_a1 != -1:
            logits_1[prev_a1] += stick_1
        
        exp_1 = np.exp(logits_1 - np.max(logits_1))
        prob_1 = exp_1 / np.sum(exp_1)
        p_choice_1[t] = prob_1[action_1[t]]
        
        a1 = action_1[t]
        s_curr = state[t]
        
        # --- Stage 2 Policy ---
        a2 = action_2[t]
        if a2 != -1:
            logits_2 = beta_2 * q_mf_2[s_curr]
            
            # Apply stickiness for the specific planet's previous choice
            if prev_a2[s_curr] != -1:
                logits_2[prev_a2[s_curr]] += stick_2
            
            exp_2 = np.exp(logits_2 - np.max(logits_2))
            prob_2 = exp_2 / np.sum(exp_2)
            p_choice_2[t] = prob_2[a2]
            
            # --- Updates ---
            # Stage 1 MF Update (TD(0)-like: towards Q2 of chosen action)
            pe1 = q_mf_2[s_curr, a2] - q_mf_1[a1]
            q_mf_1[a1] += learning_rate * pe1
            
            # Stage 2 Update
            pe2 = reward[t] - q_mf_2[s_curr, a2]
            q_mf_2[s_curr, a2] += learning_rate * pe2
            
            prev_a2[s_curr] = a2
        else:
            p_choice_2[t] = 1.0
        
        prev_a1 = a1
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Asymmetric Hybrid Learner
This model is a Hybrid learner that updates values differently depending on whether the prediction error is positive (better than expected) or negative (worse than expected). This captures potential optimism (learning more from wins) or pessimism (learning more from losses) biases in both the Model-Free and Model-Based value components.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner with Asymmetric Learning Rates.
    Uses different learning rates for positive and negative prediction errors.
    This captures potential optimism or pessimism biases in value updating.
    
    Parameters:
    alpha_pos: [0, 1] Learning rate for positive prediction errors.
    alpha_neg: [0, 1] Learning rate for negative prediction errors.
    beta_1: [0, 10] Inverse temperature for Stage 1.
    beta_2: [0, 10] Inverse temperature for Stage 2.
    w: [0, 1] Mixing weight (0=MF, 1=MB).
    stickiness: [0, 5] Perseveration bonus for Stage 1.
    """
    alpha_pos, alpha_neg, beta_1, beta_2, w, stickiness = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_mf_1 = np.zeros(2)
    q_mf_2 = np.zeros((2, 2))
    
    prev_a1 = -1
    
    for t in range(n_trials):
        # --- Stage 1 Policy (Hybrid) ---
        max_q_mf_2 = np.max(q_mf_2, axis=1)
        q_mb_1 = transition_matrix @ max_q_mf_2
        q_net_1 = w * q_mb_1 + (1 - w) * q_mf_1
        
        logits_1 = beta_1 * q_net_1
        if prev_a1 != -1:
            logits_1[prev_a1] += stickiness
            
        exp_1 = np.exp(logits_1 - np.max(logits_1))
        prob_1 = exp_1 / np.sum(exp_1)
        p_choice_1[t] = prob_1[action_1[t]]
        
        a1 = action_1[t]
        s_curr = state[t]
        
        # --- Stage 2 Policy ---
        a2 = action_2[t]
        if a2 != -1:
            logits_2 = beta_2 * q_mf_2[s_curr]
            exp_2 = np.exp(logits_2 - np.max(logits_2))
            prob_2 = exp_2 / np.sum(exp_2)
            p_choice_2[t] = prob_2[a2]
            
            # --- Updates with Asymmetry ---
            
            # Stage 1 Update (MF)
            pe1 = q_mf_2[s_curr, a2] - q_mf_1[a1]
            lr1 = alpha_pos if pe1 > 0 else alpha_neg
            q_mf_1[a1] += lr1 * pe1
            
            # Stage 2 Update
            pe2 = reward[t] - q_mf_2[s_curr, a2]
            lr2 = alpha_pos if pe2 > 0 else alpha_neg
            q_mf_2[s_curr, a2] += lr2 * pe2
            
        else:
            p_choice_2[t] = 1.0
            
        prev_a1 = a1
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```