Here are three cognitive models represented as Python functions. They all incorporate the **Choice Kernel** mechanism (which was successful in your previous best model) but combine it with different learning strategies (Eligibility Traces, Hybrid Model-Based planning, and Forgetting) to better capture the participant's behavior in the non-stationary environment.

### Cognitive Model 1: Q(lambda) with Choice Kernel
This model extends the Model-Free approach by adding **Eligibility Traces** ($\lambda$). This allows the reward received at the second stage to directly influence the value of the first-stage choice, bridging the temporal gap. This is combined with a choice kernel to account for the participant's strong perseveration.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Q-Learning with Eligibility Traces (Q(lambda)) and Choice Kernel.
    
    Combines temporal difference learning with eligibility traces to credit Stage 1 choices 
    directly for Stage 2 outcomes, alongside a choice kernel for habituation.
    
    Parameters:
    - alpha: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature (softmax sensitivity).
    - lam: [0, 1] Eligibility trace decay (lambda). Controls how much the stage 2 reward updates stage 1.
    - alpha_k: [0, 1] Choice kernel decay rate.
    - beta_k: [0, 10] Choice kernel weight (habit strength).
    """
    alpha, beta, lam, alpha_k, beta_k = model_parameters
    n_trials = len(action_1)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values initialized to 0
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2)) # state x action
    
    # Choice Kernel
    choice_kernel = np.zeros(2)
    
    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # Combine Q-value and Choice Kernel
        logits_1 = beta * q_stage1 + beta_k * choice_kernel
        # Softmax with numerical stability
        exp_q1 = np.exp(logits_1 - np.max(logits_1)) 
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        s = state[trial]
        
        # --- Stage 2 Decision ---
        logits_2 = beta * q_stage2[s]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]
        
        # --- Learning (Q-lambda) ---
        # 1. TD Error Stage 1: Difference between expected value (Q2) and current value (Q1)
        target_stage1 = q_stage2[s, a2]
        delta1 = target_stage1 - q_stage1[a1]
        q_stage1[a1] += alpha * delta1
        
        # 2. TD Error Stage 2: Difference between reward and current value (Q2)
        delta2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += alpha * delta2
        
        # 3. Eligibility Trace Update for Q1
        # The Stage 2 RPE (delta2) is passed back to Update Stage 1 choice, scaled by lambda
        q_stage1[a1] += alpha * lam * delta2
        
        # --- Update Choice Kernel ---
        # Tracks frequency of choices (exponential moving average)
        choice_kernel[a1] += alpha_k * (1.0 - choice_kernel[a1])
        choice_kernel[1-a1] += alpha_k * (0.0 - choice_kernel[1-a1])
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2: Hybrid MB/MF with Choice Kernel
This model assumes the participant uses a mixture of **Model-Based** planning (utilizing the known transition structure) and **Model-Free** learning. It replaces the simple "stickiness" parameter often used in hybrid models with the more dynamic **Choice Kernel**, allowing for the accumulation of habit over time.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free with Choice Kernel.
    
    Combines a Model-Based planner (using fixed transition probabilities), a Model-Free 
    learner (TD), and a Choice Kernel for habituation.
    
    Parameters:
    - alpha: [0, 1] Learning rate for MF Q-values.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weight of Model-Based values (0=Pure MF, 1=Pure MB).
    - alpha_k: [0, 1] Choice kernel decay rate.
    - beta_k: [0, 10] Choice kernel weight.
    """
    alpha, beta, w, alpha_k, beta_k = model_parameters
    n_trials = len(action_1)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # MF Q-values
    q_mf_stage1 = np.zeros(2)
    q_mf_stage2 = np.zeros((2, 2))
    
    # Transition Matrix (Fixed: Common=0.7, Rare=0.3)
    # Row 0 (Space A/0) -> [Planet X/0, Planet Y/1]
    # Row 1 (Space U/1) -> [Planet X/0, Planet Y/1]
    # A->X is common (0.7), U->Y is common (0.7 implies U->X is 0.3)
    trans_probs = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    choice_kernel = np.zeros(2)
    
    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # Model-Based Value: Expected max value of next stage based on transition matrix
        # Q_MB(a1) = Sum(P(s'|a1) * max_a2 Q_MF2(s', a2))
        max_q2 = np.max(q_mf_stage2, axis=1) # [max(Q(s0)), max(Q(s1))]
        q_mb_stage1 = trans_probs @ max_q2
        
        # Hybrid Value: Weighted mix of MB and MF
        q_net_stage1 = w * q_mb_stage1 + (1 - w) * q_mf_stage1
        
        # Combine with Kernel
        logits_1 = beta * q_net_stage1 + beta_k * choice_kernel
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        s = state[trial]
        
        # --- Stage 2 Decision ---
        # Pure MF at stage 2
        logits_2 = beta * q_mf_stage2[s]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]
        
        # --- Learning ---
        # MF Stage 1 Update (TD)
        delta1 = q_mf_stage2[s, a2] - q_mf_stage1[a1]
        q_mf_stage1[a1] += alpha * delta1
        
        # MF Stage 2 Update (TD)
        delta2 = r - q_mf_stage2[s, a2]
        q_mf_stage2[s, a2] += alpha * delta2
        
        # Kernel Update
        choice_kernel[a1] += alpha_k * (1.0 - choice_kernel[a1])
        choice_kernel[1-a1] += alpha_k * (0.0 - choice_kernel[1-a1])
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3: Q-Learning with Forgetting and Choice Kernel
Since the aliens' reward probabilities change slowly over trials, standard Q-learning might get stuck with outdated values. This model introduces **Forgetting** (decay) for unchosen actions, pulling them toward a neutral value (0.5). This helps the model adapt to the non-stationary environment, combined with the choice kernel for stability.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Q-Learning with Forgetting and Choice Kernel.
    
    Q-values decay towards a neutral point (0.5) for unchosen actions, helping the model 
    adapt to changing reward probabilities (non-stationarity). Also includes a choice kernel.
    
    Parameters:
    - alpha: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - decay: [0, 1] Forgetting rate for unchosen Q-values.
    - alpha_k: [0, 1] Choice kernel decay rate.
    - beta_k: [0, 10] Choice kernel weight.
    """
    alpha, beta, decay, alpha_k, beta_k = model_parameters
    n_trials = len(action_1)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    choice_kernel = np.zeros(2)
    
    # Neutral value for forgetting (0.5 since rewards are 0 or 1)
    neutral_val = 0.5
    
    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        logits_1 = beta * q_stage1 + beta_k * choice_kernel
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        s = state[trial]
        
        # --- Stage 2 Decision ---
        logits_2 = beta * q_stage2[s]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]
        
        # --- Learning & Forgetting ---
        
        # Stage 1 Updates
        target_stage1 = q_stage2[s, a2]
        # Update chosen
        q_stage1[a1] += alpha * (target_stage1 - q_stage1[a1])
        # Forget unchosen
        q_stage1[1-a1] = (1 - decay) * q_stage1[1-a1] + decay * neutral_val
        
        # Stage 2 Updates
        # Update chosen
        q_stage2[s, a2] += alpha * (r - q_stage2[s, a2])
        # Forget unchosen (in the current state only)
        q_stage2[s, 1-a2] = (1 - decay) * q_stage2[s, 1-a2] + decay * neutral_val
        
        # Kernel Update
        choice_kernel[a1] += alpha_k * (1.0 - choice_kernel[a1])
        choice_kernel[1-a1] += alpha_k * (0.0 - choice_kernel[1-a1])
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```