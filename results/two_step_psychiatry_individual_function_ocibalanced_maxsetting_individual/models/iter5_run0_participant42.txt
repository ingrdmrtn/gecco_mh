Here are the three proposed cognitive models. They build upon the success of the "forgetting" mechanism (which handles the drifting reward probabilities) by integrating it with structural variations that have been explored in isolation but not in combination with memory decay.

### Model 1: Asymmetric Learning Hybrid with Forgetting
This model combines the memory decay mechanism with separate learning rates for positive and negative prediction errors. This tests the hypothesis that the participant learns differently from wins versus losses (valence asymmetry) while simultaneously struggling to retain outdated reward information due to the drifting environment.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Agent with Asymmetric Learning and Forgetting.
    
    Combines a 'forgetting_rate' (to handle reward drift) with separate learning 
    rates for positive and negative prediction errors. This allows the agent to 
    update values differently based on the valence of the outcome (e.g., learning 
    faster from wins than losses), while continuously decaying outdated information.

    Parameters:
    - lr_pos: [0, 1] Learning rate for positive prediction errors.
    - lr_neg: [0, 1] Learning rate for negative prediction errors.
    - beta: [0, 10] Inverse temperature for softmax choice.
    - w: [0, 1] Weighting parameter (0 = Pure Model-Free, 1 = Pure Model-Based).
    - lambda_coef: [0, 1] Eligibility trace parameter for Stage 1 updates.
    - stickiness: [0, 10] Choice perseveration bonus for Stage 1.
    - forgetting_rate: [0, 1] Rate at which stored Q-values decay toward 0 each trial.
    """
    lr_pos, lr_neg, beta, w, lambda_coef, stickiness, forgetting_rate = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    
    last_action_1 = -1

    for trial in range(n_trials):
        # 1. Forgetting (Decay)
        q_stage1_mf *= (1 - forgetting_rate)
        q_stage2_mf *= (1 - forgetting_rate)

        # 2. Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net
        if last_action_1 != -1:
            logits_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_action_1 = action_1[trial]
        state_idx = state[trial]

        # 3. Stage 2 Policy
        q_s2 = q_stage2_mf[state_idx]
        logits_2 = beta * q_s2
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # 4. Updates
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]

        # Update Stage 2 with asymmetric learning rates
        lr_s2 = lr_pos if delta_stage2 > 0 else lr_neg
        q_stage2_mf[state_idx, action_2[trial]] += lr_s2 * delta_stage2
        
        # Update Stage 1 with asymmetric learning rates
        # Using the sign of the total effective error for Stage 1
        total_error_s1 = delta_stage1 + lambda_coef * delta_stage2
        lr_s1 = lr_pos if total_error_s1 > 0 else lr_neg
        q_stage1_mf[action_1[trial]] += lr_s1 * total_error_s1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Dual-Beta Hybrid with Forgetting
This model distinguishes between the decision noise (exploration/exploitation balance) in the planning stage (Stage 1) versus the bandit stage (Stage 2). By adding the forgetting rate, it controls for the drift, allowing the model to isolate whether the participant is specifically more random/directed in their high-level spaceship choice compared to their low-level alien choice.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Agent with Dual Betas and Forgetting.
    
    Differentiates decision noise (exploration/exploitation balance) between 
    Stage 1 (Spaceship choice) and Stage 2 (Alien choice). The 'forgetting_rate'
    is included to handle the drifting reward probabilities. This allows the 
    model to capture if the participant is more consistent in one stage than the other.

    Parameters:
    - learning_rate: [0, 1] Rate at which Q-values are updated.
    - beta_1: [0, 10] Inverse temperature for Stage 1 choice.
    - beta_2: [0, 10] Inverse temperature for Stage 2 choice.
    - w: [0, 1] Weighting parameter (0 = Pure Model-Free, 1 = Pure Model-Based).
    - lambda_coef: [0, 1] Eligibility trace parameter.
    - stickiness: [0, 10] Choice perseveration bonus for Stage 1.
    - forgetting_rate: [0, 1] Rate at which stored Q-values decay toward 0 each trial.
    """
    learning_rate, beta_1, beta_2, w, lambda_coef, stickiness, forgetting_rate = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        # Forgetting
        q_stage1_mf *= (1 - forgetting_rate)
        q_stage2_mf *= (1 - forgetting_rate)

        # Stage 1
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Use beta_1 for Stage 1
        logits_1 = beta_1 * q_net
        if last_action_1 != -1:
            logits_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_action_1 = action_1[trial]
        state_idx = state[trial]

        # Stage 2
        q_s2 = q_stage2_mf[state_idx]
        # Use beta_2 for Stage 2
        logits_2 = beta_2 * q_s2
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # Updates
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]

        q_stage1_mf[action_1[trial]] += learning_rate * (delta_stage1 + lambda_coef * delta_stage2)
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Stage-Specific Learning Hybrid with Forgetting
This model posits that the learning dynamics for the stable spaceship transitions (Stage 1) and the drifting alien rewards (Stage 2) are fundamentally different. It assigns separate learning rates to each stage while using the forgetting rate to manage the volatility of the stored values.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Agent with Stage-Specific Learning Rates and Forgetting.
    
    Acknowledges that Stage 1 (Spaceship transitions) and Stage 2 (Alien rewards)
    have different dynamics. Stage 1 is stable (only requires mapping values), 
    while Stage 2 is non-stationary (drifting rewards). Uses separate learning 
    rates for each stage, combined with forgetting to handle the drift.

    Parameters:
    - lr_stage1: [0, 1] Learning rate for Stage 1 updates.
    - lr_stage2: [0, 1] Learning rate for Stage 2 updates.
    - beta: [0, 10] Inverse temperature for softmax choice.
    - w: [0, 1] Weighting parameter.
    - lambda_coef: [0, 1] Eligibility trace parameter.
    - stickiness: [0, 10] Choice perseveration bonus for Stage 1.
    - forgetting_rate: [0, 1] Rate at which stored Q-values decay toward 0 each trial.
    """
    lr_stage1, lr_stage2, beta, w, lambda_coef, stickiness, forgetting_rate = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        # Forgetting
        q_stage1_mf *= (1 - forgetting_rate)
        q_stage2_mf *= (1 - forgetting_rate)

        # Stage 1
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net
        if last_action_1 != -1:
            logits_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_action_1 = action_1[trial]
        state_idx = state[trial]

        # Stage 2
        q_s2 = q_stage2_mf[state_idx]
        logits_2 = beta * q_s2
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # Updates
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]

        # Use lr_stage1 for first stage MF update
        q_stage1_mf[action_1[trial]] += lr_stage1 * (delta_stage1 + lambda_coef * delta_stage2)
        
        # Use lr_stage2 for second stage MF update
        q_stage2_mf[state_idx, action_2[trial]] += lr_stage2 * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```