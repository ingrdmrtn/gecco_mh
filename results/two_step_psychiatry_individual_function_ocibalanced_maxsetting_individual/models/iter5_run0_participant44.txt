Here are three new cognitive models for the two-step task.

### Model 1: Probability Mixture Hybrid
This model proposes that the brain has two distinct controllers (Model-Based and Model-Free) that compete to control the output channel. Instead of integrating their value estimates into a single "net value" (Value Integration), each system generates its own probability distribution over actions, and the final choice probability is a weighted mixture of these two distributions. This captures a "competition" dynamic rather than an "integration" dynamic.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Probability Mixture Hybrid Model.
    
    Instead of mixing Q-values (Value Integration) before the softmax step, 
    this model calculates choice probabilities for the Model-Based (MB) and 
    Model-Free (MF) systems independently. The final choice probability is 
    a weighted average of the two systems' probabilities (System Competition).
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for MF value updates.
    - beta: [0, 10] Inverse temperature (shared by both systems).
    - w: [0, 1] Mixing weight (1 = Pure MB probability, 0 = Pure MF probability).
    """
    learning_rate, beta, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        
        # 1. Model-Based Probability
        # MB computes values based on transitions and max stage 2 values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        exp_q1_mb = np.exp(beta * q_stage1_mb)
        probs_1_mb = exp_q1_mb / np.sum(exp_q1_mb)
        
        # 2. Model-Free Probability
        exp_q1_mf = np.exp(beta * q_stage1_mf)
        probs_1_mf = exp_q1_mf / np.sum(exp_q1_mf)
        
        # 3. Probability Mixture
        probs_1_net = w * probs_1_mb + (1 - w) * probs_1_mf
        
        p_choice_1[trial] = probs_1_net[action_1[trial]]
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        # Standard Softmax on Stage 2 MF values
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning Updates ---
        # SARSA-style update for Stage 1 (using actual stage 2 value)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Prediction Error update for Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Choice Trace (Habit) Hybrid
This model extends the concept of "stickiness" (repeating the last action) to a longer timescale. It maintains a "Choice Trace" that decays slowly over time. This accounts for the strong streaks of repeated choices observed in the participant data (e.g., sticking with Spaceship 1 for many trials), which might be driven by a habit mechanism distinct from value learning.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Decaying Choice Trace (Habit).
    
    Incorporates a 'Choice Trace' that accumulates when an action is chosen 
    and decays over time. This captures perseveration (habit) over longer 
    timescales than simple 1-step stickiness.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for value updates.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight (MB vs MF).
    - trace_decay: [0, 1] Decay rate of the choice trace (0 = instant decay, 1 = no decay).
    - trace_weight: [0, 5] Strength of the habit influence on choice.
    """
    learning_rate, beta, w, trace_decay, trace_weight = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Initialize choice trace for stage 1 actions
    choice_trace = np.zeros(2)

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net value includes MB, MF, and the Choice Trace (Habit)
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf + trace_weight * choice_trace
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        
        # Update Choice Trace
        # Decay existing trace
        choice_trace *= trace_decay
        # Increment trace for chosen action
        choice_trace[action_1[trial]] += 1.0
        
        # Value Updates
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Noisy Planning Hybrid
Standard Model-Based learning assumes the agent calculates the value of a state by assuming they will act optimally in the future ($V(s') = \max Q(s', a')$). This model proposes that the participant simulates the future "noisily" or imperfectly. The "Planning Beta" parameter controls how optimally the agent expects to perform in Stage 2 during the planning phase. A low `beta_plan` implies the agent expects to choose randomly (averaging values), while a high `beta_plan` approximates the standard max operator.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Noisy Planning (Simulation).
    
    The Model-Based system estimates Stage 1 values by simulating Stage 2.
    Instead of assuming optimal future choices (max), it assumes choices 
    are made according to a specific 'planning' softmax temperature.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature for actual choice execution.
    - w: [0, 1] Mixing weight.
    - beta_plan: [0, 10] Inverse temperature used during MB simulation/planning.
    """
    learning_rate, beta, w, beta_plan = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        
        # Calculate Expected Value of Stage 2 states under the 'Planning Policy'
        # Instead of np.max(), we calculate the Softmax expected value
        v_stage2_planned = np.zeros(2)
        for s_idx in range(2):
            q_vals = q_stage2_mf[s_idx]
            # Probabilities of choosing aliens in simulation
            exp_q_plan = np.exp(beta_plan * q_vals)
            probs_plan = exp_q_plan / np.sum(exp_q_plan)
            # Expected value is weighted sum
            v_stage2_planned[s_idx] = np.sum(probs_plan * q_vals)
            
        q_stage1_mb = transition_matrix @ v_stage2_planned
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        # Execution uses the standard 'beta', not 'beta_plan'
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```