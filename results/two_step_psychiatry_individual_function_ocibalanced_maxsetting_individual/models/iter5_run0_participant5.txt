Here are three new cognitive models for the two-step task.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Asymmetric Learning Rates.
    
    This model combines Model-Based (MB) and Model-Free (MF) reinforcement learning strategies.
    It distinguishes between positive and negative prediction errors using separate learning rates,
    allowing the agent to learn differently from better-than-expected vs. worse-than-expected outcomes.
    This is particularly relevant for handling the drifting reward probabilities and potential 
    risk-sensitivity in the participant's behavior.
    
    Parameters:
    - alpha_pos: [0, 1] Learning rate for positive prediction errors.
    - alpha_neg: [0, 1] Learning rate for negative prediction errors.
    - beta_1: [0, 10] Inverse temperature for Stage 1 choices.
    - beta_2: [0, 10] Inverse temperature for Stage 2 choices.
    - w: [0, 1] Weighting parameter mixing MB (1) and MF (0) values.
    """
    alpha_pos, alpha_neg, beta_1, beta_2, w = model_parameters
    n_trials = len(action_1)
    
    # Fixed transition matrix: 
    # A(0) -> X(0) (0.7), Y(1) (0.3)
    # U(1) -> X(0) (0.3), Y(1) (0.7)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        # Skip invalid trials
        if action_1[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue
            
        # --- Stage 1 Decision ---
        # Model-Based: Compute expected value of stage 2 states
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax Stage 1
        exp_q1 = np.exp(beta_1 * q_net - np.max(beta_1 * q_net)) # Numerical stability
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Decision ---
        state_idx = state[trial]
        
        # Softmax Stage 2
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx] - np.max(beta_2 * q_stage2_mf[state_idx]))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning ---
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]
        
        # Stage 1 Update (MF) - SARSA-style: Q1(a1) -> Q2(s, a2)
        delta_1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        lr_1 = alpha_pos if delta_1 > 0 else alpha_neg
        q_stage1_mf[a1] += lr_1 * delta_1
        
        # Stage 2 Update (MF) - Q2(s, a2) -> r
        delta_2 = r - q_stage2_mf[state_idx, a2]
        lr_2 = alpha_pos if delta_2 > 0 else alpha_neg
        q_stage2_mf[state_idx, a2] += lr_2 * delta_2
        
    eps = 1e-10
    valid_mask = (p_choice_1 > 0) & (p_choice_2 > 0)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    TD-Lambda with Asymmetric Learning and Stickiness.
    
    A Temporal Difference (TD) learning model that uses an eligibility trace (lambda) 
    to connect Stage 2 outcomes to Stage 1 values. It incorporates asymmetric learning rates 
    for positive/negative errors to capture valence biases, and a stickiness parameter 
    to model the participant's observed tendency to repeat choices (perseverance).
    
    Parameters:
    - alpha_pos: [0, 1] Learning rate for positive prediction errors.
    - alpha_neg: [0, 1] Learning rate for negative prediction errors.
    - beta_1: [0, 10] Inverse temperature for Stage 1 choices.
    - beta_2: [0, 10] Inverse temperature for Stage 2 choices.
    - lambd: [0, 1] Eligibility trace decay (0 = pure TD, 1 = Monte Carlo-like).
    - stickiness: [0, 10] Bonus added to the previously chosen action's value (perseverance).
    """
    alpha_pos, alpha_neg, beta_1, beta_2, lambd, stickiness = model_parameters
    n_trials = len(action_1)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    prev_a1 = -1
    
    for trial in range(n_trials):
        if action_1[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            prev_a1 = -1
            continue
            
        # --- Stage 1 Decision ---
        # Add stickiness bonus to the logits of the previously chosen action
        logits_1 = beta_1 * q_stage1.copy()
        if prev_a1 != -1:
            logits_1[prev_a1] += stickiness
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Decision ---
        state_idx = state[trial]
        logits_2 = beta_2 * q_stage2[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning ---
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]
        
        # TD Error 1: Q2(s, a2) - Q1(a1)
        delta_1 = q_stage2[state_idx, a2] - q_stage1[a1]
        lr_1 = alpha_pos if delta_1 > 0 else alpha_neg
        q_stage1[a1] += lr_1 * delta_1
        
        # TD Error 2: r - Q2(s, a2)
        delta_2 = r - q_stage2[state_idx, a2]
        lr_2 = alpha_pos if delta_2 > 0 else alpha_neg
        q_stage2[state_idx, a2] += lr_2 * delta_2
        
        # Eligibility Trace Update for Q1
        # Propagate delta_2 back to Q1 scaled by lambda
        # Using lr_2 to match the source of the error
        q_stage1[a1] += lr_2 * lambd * delta_2
        
        prev_a1 = a1
        
    eps = 1e-10
    valid_mask = (p_choice_1 > 0) & (p_choice_2 > 0)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Value Decay and Stickiness.
    
    A Hybrid MB/MF model that includes a decay mechanism for unchosen/unvisited state-action values.
    This helps the model adapt to the slowly drifting reward probabilities by actively 'forgetting' 
    outdated information. It also includes choice stickiness to capture the participant's 
    strong tendency to repeat choices (perseverance).
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for value updates.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - w: [0, 1] Weighting between MB (1) and MF (0).
    - stickiness: [0, 10] Choice perseverance bonus.
    - decay_rate: [0, 1] Rate at which unobserved Q-values decay toward 0.
    """
    learning_rate, beta_1, beta_2, w, stickiness, decay_rate = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_a1 = -1
    
    for trial in range(n_trials):
        if action_1[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            prev_a1 = -1
            continue
            
        # --- Stage 1 Decision ---
        # MB Calculation: Expected value based on transition matrix and current Stage 2 values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Logits with stickiness
        logits_1 = beta_1 * q_net
        if prev_a1 != -1:
            logits_1[prev_a1] += stickiness
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Decision ---
        state_idx = state[trial]
        logits_2 = beta_2 * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning ---
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]
        
        # Stage 1 MF Update (SARSA)
        delta_1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_1
        
        # Stage 2 MF Update
        delta_2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_2
        
        # --- Decay ---
        # Decay unchosen stage 1 MF value
        q_stage1_mf[1 - a1] *= (1.0 - decay_rate)
        
        # Decay unchosen stage 2 MF value in current state
        q_stage2_mf[state_idx, 1 - a2] *= (1.0 - decay_rate)
        
        # Decay all values in the unvisited state
        q_stage2_mf[1 - state_idx, :] *= (1.0 - decay_rate)
        
        prev_a1 = a1

    eps = 1e-10
    valid_mask = (p_choice_1 > 0) & (p_choice_2 > 0)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss
```