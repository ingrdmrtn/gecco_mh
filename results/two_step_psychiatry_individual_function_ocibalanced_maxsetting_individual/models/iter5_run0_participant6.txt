Here are three cognitive models expressed as Python functions.

### Model 1: Hybrid Model with Eligibility Traces and Stickiness
This model integrates Model-Based (MB) and Model-Free (MF) learning. It includes an eligibility trace parameter (`lambda_eligibility`) that modulates how much the Stage 2 Reward Prediction Error (RPE) updates the Stage 1 choice value. This allows for a continuous gradation between pure TD(0) learning and TD(1) / Monte Carlo learning for the MF component. It also includes a stickiness parameter to account for choice perseveration.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free learner with Eligibility Traces and Stickiness.
    
    Combines MB and MF values using a weighting parameter 'w'.
    The MF component uses an eligibility trace 'lambda_eligibility' to update 
    Stage 1 values based on Stage 2 outcomes.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for MF values.
    - beta_1: [0, 10] Inverse temperature for Stage 1 softmax.
    - beta_2: [0, 10] Inverse temperature for Stage 2 softmax.
    - w: [0, 1] Weight of Model-Based values (1=Pure MB, 0=Pure MF).
    - lambda_eligibility: [0, 1] Eligibility trace decay (0=TD(0), 1=Monte Carlo).
    - stickiness: [-5, 5] Choice perseveration bonus for Stage 1.
    """
    learning_rate, beta_1, beta_2, w, lambda_eligibility, stickiness = model_parameters
    n_trials = len(action_1)
    
    # Transition matrix: A->X (0->0) 0.7, A->Y (0->1) 0.3, etc.
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Alien
    
    last_action_1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        # --- Stage 1 Policy ---
        # Model-Based Value Calculation
        # Max Q-value for each state in Stage 2
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combined Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        # Softmax Stage 1
        exp_q1 = np.exp(beta_1 * (q_net - np.max(q_net)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_action_1 = action_1[trial]
        state_idx = state[trial]
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta_2 * (q_stage2_mf[state_idx] - np.max(q_stage2_mf[state_idx])))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning ---
        # Stage 1 RPE (TD(0))
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Stage 2 RPE
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Eligibility Trace Update for Stage 1
        # Propagate Stage 2 RPE back to Stage 1 choice scaled by lambda
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage2 * lambda_eligibility

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: MF Learner with Outcome-Dependent Stickiness
This model assumes a Model-Free learning process but hypothesizes that the participant's tendency to repeat choices (stickiness) depends on the previous outcome. It differentiates between "Win-Stay" (`stick_win`) and "Lose-Stay" (`stick_loss`) tendencies. This addresses the observation that the participant often repeats choices even after losses.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Learner with Outcome-Dependent Stickiness (Win-Stay / Lose-Stay).
    
    Applies different stickiness bonuses depending on whether the previous trial
    resulted in a reward or not.
    
    Parameters:
    - learning_rate: [0, 1] Update rate for Q-values.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - stick_win: [-5, 5] Stickiness bonus after a rewarded trial.
    - stick_loss: [-5, 5] Stickiness bonus after an unrewarded trial.
    """
    learning_rate, beta_1, beta_2, stick_win, stick_loss = model_parameters
    n_trials = len(action_1)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    last_reward = -1

    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        # --- Stage 1 Policy ---
        q_net = q_stage1_mf.copy()
        
        # Apply outcome-dependent stickiness
        if last_action_1 != -1:
            if last_reward == 1:
                q_net[last_action_1] += stick_win
            else:
                q_net[last_action_1] += stick_loss
        
        exp_q1 = np.exp(beta_1 * (q_net - np.max(q_net)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_action_1 = action_1[trial]
        state_idx = state[trial]
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta_2 * (q_stage2_mf[state_idx] - np.max(q_stage2_mf[state_idx])))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning ---
        # Stage 1 Update (TD(0))
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Propagate Stage 2 RPE to Stage 1 (TD(1) logic implies full propagation, 
        # but here we stick to simple MF TD(1) equivalent or just TD(0) + stickiness. 
        # Standard simple MF often includes this term).
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage2
        
        last_reward = reward[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Asymmetric Learning with Decay and Stickiness
This model hypothesizes that the participant learns differently from positive prediction errors (better than expected) versus negative prediction errors (worse than expected), while also forgetting the value of unchosen actions over time. This structure can explain "staying on loss" if the negative learning rate is very low, combined with value decay that erodes the value of alternatives.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Learner with Asymmetric Learning Rates, Value Decay, and Stickiness.
    
    Uses separate learning rates for positive (alpha_pos) and negative (alpha_neg)
    prediction errors. Also decays unchosen Q-values toward 0.
    
    Parameters:
    - alpha_pos: [0, 1] Learning rate for positive prediction errors (RPE > 0).
    - alpha_neg: [0, 1] Learning rate for negative prediction errors (RPE < 0).
    - decay_rate: [0, 1] Rate at which unchosen Q-values decay.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - stickiness: [-5, 5] Choice perseveration bonus.
    """
    alpha_pos, alpha_neg, decay_rate, beta_1, beta_2, stickiness = model_parameters
    n_trials = len(action_1)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        # --- Stage 1 Policy ---
        q_net = q_stage1_mf.copy()
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta_1 * (q_net - np.max(q_net)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_action_1 = action_1[trial]
        state_idx = state[trial]
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta_2 * (q_stage2_mf[state_idx] - np.max(q_stage2_mf[state_idx])))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning ---
        # Helper to apply asymmetric learning
        def update_q(current_val, target_val):
            delta = target_val - current_val
            alpha = alpha_pos if delta > 0 else alpha_neg
            return current_val + alpha * delta, delta

        # Stage 1 Update
        q_stage1_mf[action_1[trial]], delta_s1 = update_q(q_stage1_mf[action_1[trial]], q_stage2_mf[state_idx, action_2[trial]])
        
        # Stage 2 Update
        old_q2 = q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]], delta_s2 = update_q(old_q2, reward[trial])
        
        # Direct reinforcement of Stage 1 choice by Stage 2 reward (TD(1)-like)
        # Using the same asymmetric logic on the propagated error
        delta_prop = delta_s2 
        alpha_prop = alpha_pos if delta_prop > 0 else alpha_neg
        q_stage1_mf[action_1[trial]] += alpha_prop * delta_prop
        
        # --- Decay ---
        # Decay unchosen options
        unchosen_alien = 1 - action_2[trial]
        q_stage2_mf[state_idx, unchosen_alien] *= (1 - decay_rate)

        unchosen_planet = 1 - state_idx
        q_stage2_mf[unchosen_planet, 0] *= (1 - decay_rate)
        q_stage2_mf[unchosen_planet, 1] *= (1 - decay_rate)

        unchosen_spaceship = 1 - action_1[trial]
        q_stage1_mf[unchosen_spaceship] *= (1 - decay_rate)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```