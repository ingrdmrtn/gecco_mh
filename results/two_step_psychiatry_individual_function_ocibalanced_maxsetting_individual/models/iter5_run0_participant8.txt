Here are three new cognitive models that extend the best-performing "Hybrid Model with Decay" by incorporating mechanisms for asymmetric learning, stage-specific learning rates, and eligibility traces.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Asymmetric Learning and Decay.
    
    This model extends the hybrid learner by allowing different learning rates 
    for positive and negative prediction errors (valence-dependent learning). 
    It incorporates a decay parameter to handle the non-stationary nature of 
    the reward probabilities, preventing the agent from relying on outdated information.
    
    Parameters:
    lr_pos: [0,1] - Learning rate for positive prediction errors.
    lr_neg: [0,1] - Learning rate for negative prediction errors.
    decay: [0,1] - Decay rate for unchosen options (0 = no decay).
    beta: [0,10] - Inverse temperature for softmax.
    w: [0,1] - Weighting parameter (0 = Pure MF, 1 = Pure MB).
    stickiness: [0,5] - Choice perseveration bonus for Stage 1.
    """
    lr_pos, lr_neg, decay, beta, w, stickiness = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if prev_a1 != -1:
            q_net[prev_a1] += stickiness
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        prev_a1 = action_1[trial]
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning ---
        chosen_a1 = action_1[trial]
        chosen_a2 = action_2[trial]

        # Stage 1 Update (MF)
        delta_stage1 = q_stage2_mf[state_idx, chosen_a2] - q_stage1_mf[chosen_a1]
        eff_lr_1 = lr_pos if delta_stage1 >= 0 else lr_neg
        q_stage1_mf[chosen_a1] += eff_lr_1 * delta_stage1
        
        # Stage 2 Update (MF)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, chosen_a2]
        eff_lr_2 = lr_pos if delta_stage2 >= 0 else lr_neg
        q_stage2_mf[state_idx, chosen_a2] += eff_lr_2 * delta_stage2

        # Decay unchosen options
        q_stage1_mf[1 - chosen_a1] *= (1 - decay)
        for s in range(2):
            for a in range(2):
                if not (s == state_idx and a == chosen_a2):
                    q_stage2_mf[s, a] *= (1 - decay)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Stage-Specific Learning Rates and Decay.
    
    This model assumes the participant learns Stage 1 values (navigation/transition) 
    and Stage 2 values (alien rewards) at different rates. This separation allows 
    flexibility in how quickly the agent updates value estimates at different 
    levels of the task hierarchy, combined with decay for forgetting old values.
    
    Parameters:
    lr_1: [0,1] - Learning rate for Stage 1 updates.
    lr_2: [0,1] - Learning rate for Stage 2 updates.
    decay: [0,1] - Decay rate for unchosen options.
    beta: [0,10] - Inverse temperature.
    w: [0,1] - Weighting parameter (0 = Pure MF, 1 = Pure MB).
    stickiness: [0,5] - Choice perseveration bonus.
    """
    lr_1, lr_2, decay, beta, w, stickiness = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if prev_a1 != -1:
            q_net[prev_a1] += stickiness
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        prev_a1 = action_1[trial]
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning ---
        chosen_a1 = action_1[trial]
        chosen_a2 = action_2[trial]

        delta_stage1 = q_stage2_mf[state_idx, chosen_a2] - q_stage1_mf[chosen_a1]
        q_stage1_mf[chosen_a1] += lr_1 * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, chosen_a2]
        q_stage2_mf[state_idx, chosen_a2] += lr_2 * delta_stage2

        # Decay unchosen options
        q_stage1_mf[1 - chosen_a1] *= (1 - decay)
        for s in range(2):
            for a in range(2):
                if not (s == state_idx and a == chosen_a2):
                    q_stage2_mf[s, a] *= (1 - decay)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Decay and Eligibility Traces.
    
    Incorporates an eligibility trace parameter (lambda) which allows the 
    reward prediction error from Stage 2 to directly update the Stage 1 
    action values. This improves temporal credit assignment. Also includes 
    decay for unchosen options to handle drifting rewards.
    
    Parameters:
    lr: [0,1] - Learning rate.
    decay: [0,1] - Decay rate for unchosen options.
    beta: [0,10] - Inverse temperature.
    w: [0,1] - Weighting parameter (0 = Pure MF, 1 = Pure MB).
    lam: [0,1] - Eligibility trace parameter (0 = TD(0), 1 = TD(1)).
    stickiness: [0,5] - Choice perseveration bonus.
    """
    lr, decay, beta, w, lam, stickiness = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if prev_a1 != -1:
            q_net[prev_a1] += stickiness
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        prev_a1 = action_1[trial]
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning ---
        chosen_a1 = action_1[trial]
        chosen_a2 = action_2[trial]

        delta_stage1 = q_stage2_mf[state_idx, chosen_a2] - q_stage1_mf[chosen_a1]
        q_stage1_mf[chosen_a1] += lr * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, chosen_a2]
        q_stage2_mf[state_idx, chosen_a2] += lr * delta_stage2
        
        # Eligibility Trace Update: Stage 2 RPE affects Stage 1 value
        q_stage1_mf[chosen_a1] += lr * lam * delta_stage2

        # Decay unchosen options
        q_stage1_mf[1 - chosen_a1] *= (1 - decay)
        for s in range(2):
            for a in range(2):
                if not (s == state_idx and a == chosen_a2):
                    q_stage2_mf[s, a] *= (1 - decay)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```