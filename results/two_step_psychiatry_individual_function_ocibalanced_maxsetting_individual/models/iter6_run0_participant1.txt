Here are three new cognitive models based on the participant's behavior and the two-step task structure.

### Cognitive Model 1: Hybrid Model with Q-value Decay
This model modifies the standard hybrid learner by introducing a **decay** parameter. In addition to learning from rewards, the Q-values of unchosen options (both spaceships and aliens) decay towards 0 on each trial. This captures a "forgetting" process or counterfactual pessimism, where the participant loses confidence in options they haven't sampled recently. This complements the stickiness parameter to explain the participant's long streaks of choosing one option.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Q-value Decay and Stickiness.
    
    Combines Model-Based and Model-Free learning with a decay mechanism.
    Unchosen options' Q-values decay towards 0, simulating forgetting or 
    decreasing confidence in unexplored paths.
    
    Parameters:
    learning_rate: [0, 1] - Rate at which Q-values are updated.
    beta: [0, 10] - Inverse temperature for softmax choice.
    w: [0, 1] - Weighting between MB (1) and MF (0) values.
    stickiness: [0, 5] - Bonus for repeating the previous Stage 1 choice.
    decay: [0, 1] - Rate at which unchosen Q-values decay (1 = full reset to 0).
    """
    learning_rate, beta, w, stickiness, decay = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Initialize Q-values
    q_mf_stage1 = np.zeros(2)
    q_mf_stage2 = np.zeros((2, 2)) # State x Action
    
    last_action_1 = -1
    log_loss = 0.0
    eps = 1e-10
    
    for trial in range(n_trials):
        a1 = action_1[trial]
        s_idx = state[trial]
        a2 = action_2[trial]
        r = reward[trial]

        # --- Stage 1 Choice ---
        # Model-Based Value
        max_q_stage2 = np.max(q_mf_stage2, axis=1)
        q_mb_stage1 = transition_matrix @ max_q_stage2

        # Net Value
        q_net_stage1 = w * q_mb_stage1 + (1 - w) * q_mf_stage1

        # Apply Stickiness
        if last_action_1 != -1:
            q_net_stage1[last_action_1] += stickiness
            
        # Probability calculation
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + eps)
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_mf_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        # Handle missing trials (e.g. timeouts)
        if a2 != -1 and r != -1:
            log_loss -= np.log(probs_2[a2] + eps)

            # --- Updates ---
            # Stage 2 Update (TD(0))
            delta_2 = r - q_mf_stage2[s_idx, a2]
            q_mf_stage2[s_idx, a2] += learning_rate * delta_2
            
            # Stage 2 Decay (Unchosen option on current planet)
            unchosen_a2 = 1 - a2
            q_mf_stage2[s_idx, unchosen_a2] *= (1 - decay)

            # Stage 1 Update (TD(1) proxy)
            q_mf_stage1[a1] += learning_rate * (r - q_mf_stage1[a1])
            
            # Stage 1 Decay (Unchosen spaceship)
            unchosen_a1 = 1 - a1
            q_mf_stage1[unchosen_a1] *= (1 - decay)
            
        last_action_1 = a1

    return log_loss
```

### Cognitive Model 2: Dual-Stage Parameter Hybrid
This model posits that the participant uses distinct cognitive parameters for the two different stages of the task. The "Planning" stage (Spaceship choice) and the "Harvesting" stage (Alien choice) are governed by separate learning rates (`lr_s1`, `lr_s2`) and separate noise levels (`beta_s1`, `beta_s2`). This accounts for the possibility that the participant is more exploratory in the bandit phase (Stage 2) while being more rigid or learning differently in the navigation phase (Stage 1).

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Stage-Separated Learning and Noise.
    
    Uses distinct learning rates and inverse temperatures for Stage 1 and Stage 2,
    allowing for different plasticity and exploration dynamics in the navigation
    versus harvesting phases.
    
    Parameters:
    lr_s1: [0, 1] - Learning rate for Stage 1.
    lr_s2: [0, 1] - Learning rate for Stage 2.
    beta_s1: [0, 10] - Inverse temperature for Stage 1 choice.
    beta_s2: [0, 10] - Inverse temperature for Stage 2 choice.
    w: [0, 1] - Model-Based / Model-Free weight.
    stickiness: [0, 5] - Choice perseveration bonus for Stage 1.
    """
    lr_s1, lr_s2, beta_s1, beta_s2, w, stickiness = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_mf_stage1 = np.zeros(2)
    q_mf_stage2 = np.zeros((2, 2))
    
    last_action_1 = -1
    log_loss = 0.0
    eps = 1e-10
    
    for trial in range(n_trials):
        a1 = action_1[trial]
        s_idx = state[trial]
        a2 = action_2[trial]
        r = reward[trial]

        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_mf_stage2, axis=1)
        q_mb_stage1 = transition_matrix @ max_q_stage2

        q_net_stage1 = w * q_mb_stage1 + (1 - w) * q_mf_stage1

        if last_action_1 != -1:
            q_net_stage1[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta_s1 * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + eps)
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta_s2 * q_mf_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        if a2 != -1 and r != -1:
            log_loss -= np.log(probs_2[a2] + eps)

            # --- Updates ---
            # Stage 2 Update using lr_s2
            delta_2 = r - q_mf_stage2[s_idx, a2]
            q_mf_stage2[s_idx, a2] += lr_s2 * delta_2

            # Stage 1 Update using lr_s1
            q_mf_stage1[a1] += lr_s1 * (r - q_mf_stage1[a1])
            
        last_action_1 = a1

    return log_loss
```

### Cognitive Model 3: Hybrid Model with Side Bias
The participant data shows a strong preference for Spaceship 0 (Action 0), even persisting through some failures. While stickiness explains repetition, it doesn't explain the overall skew towards one specific option. This model adds a static **side_bias** parameter that increases the value of Spaceship 0, representing an intrinsic preference or motor bias, independent of the reward history.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Fixed Side Bias.
    
    Incorporates a static bias towards Spaceship 0 (Action 0) alongside stickiness.
    This accounts for intrinsic preferences or motor biases that are independent 
    of the learned values.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - MB/MF weight.
    stickiness: [0, 5] - Perseveration bonus for the previously chosen action.
    side_bias: [0, 5] - Constant value added to Spaceship 0's Q-value.
    """
    learning_rate, beta, w, stickiness, side_bias = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_mf_stage1 = np.zeros(2)
    q_mf_stage2 = np.zeros((2, 2))
    
    last_action_1 = -1
    log_loss = 0.0
    eps = 1e-10
    
    for trial in range(n_trials):
        a1 = action_1[trial]
        s_idx = state[trial]
        a2 = action_2[trial]
        r = reward[trial]

        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_mf_stage2, axis=1)
        q_mb_stage1 = transition_matrix @ max_q_stage2

        q_net_stage1 = w * q_mb_stage1 + (1 - w) * q_mf_stage1

        # Apply Side Bias (Bias towards Action 0)
        q_net_stage1[0] += side_bias

        # Apply Stickiness
        if last_action_1 != -1:
            q_net_stage1[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + eps)
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_mf_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        if a2 != -1 and r != -1:
            log_loss -= np.log(probs_2[a2] + eps)

            # --- Updates ---
            delta_2 = r - q_mf_stage2[s_idx, a2]
            q_mf_stage2[s_idx, a2] += learning_rate * delta_2

            q_mf_stage1[a1] += learning_rate * (r - q_mf_stage1[a1])
            
        last_action_1 = a1

    return log_loss
```