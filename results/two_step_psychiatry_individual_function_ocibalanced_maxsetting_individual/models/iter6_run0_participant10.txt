To explain the participant's behavior in the two-step task, I propose three cognitive models.

### Cognitive Model 1: Habitual Hybrid Learner
This model combines Model-Based (MB) and Model-Free (MF) reinforcement learning with a "Choice Kernel" (Habit). Unlike simple one-step perseveration, the habit strength builds up over multiple repetitions of a choice and decays slowly when not chosen. This accounts for the participant's tendency to stick with one spaceship for long blocks of trials (e.g., trials 1-88) before switching.

*   **Parameters**:
    *   `lr`: Learning rate for Q-value updates.
    *   `beta`: Inverse temperature (exploration/exploitation trade-off).
    *   `w`: Weighting parameter (0 = pure MF, 1 = pure MB).
    *   `habit_lr`: Learning rate for the habit trace (how fast habits form/decay).
    *   `habit_w`: Weight of the habit strength in the decision policy.

### Cognitive Model 2: Dual-Beta TD($\lambda$) Learner
This is a Model-Free learner using Temporal Difference learning with eligibility traces (TD($\lambda$)) and Choice Perseveration. It distinguishes between the exploration levels in Stage 1 and Stage 2 by using separate inverse temperature parameters (`beta_1` and `beta_2`). The participant might exhibit different levels of randomness in the high-stakes spaceship choice compared to the alien choice.

*   **Parameters**:
    *   `lr`: Learning rate.
    *   `beta_1`: Inverse temperature for Stage 1 (Spaceship choice).
    *   `beta_2`: Inverse temperature for Stage 2 (Alien choice).
    *   `lambda_param`: Eligibility trace decay factor (0 = TD(0), 1 = Monte Carlo).
    *   `perseveration`: Bonus added to the previously chosen Stage 1 action.

### Cognitive Model 3: Asymmetric Hybrid Learner
This model is a Hybrid MB/MF learner that updates value estimates differently based on whether the prediction error is positive (better than expected) or negative (worse than expected). This "confirmation bias" or risk-sensitivity can explain why the participant maintains a choice despite occasional lack of rewards. It includes standard perseveration.

*   **Parameters**:
    *   `lr_pos`: Learning rate for positive prediction errors.
    *   `lr_neg`: Learning rate for negative prediction errors.
    *   `beta`: Inverse temperature.
    *   `w`: Weighting parameter between MB and MF.
    *   `perseveration`: Bonus added to the previously chosen Stage 1 action.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Habitual Hybrid Learner.
    
    Combines Model-Based and Model-Free values with a slowly decaying 
    Choice Kernel (Habit) to explain long streaks of choices.
    
    Parameters:
    lr:       [0, 1] Learning rate for Q-values.
    beta:     [0, 10] Inverse temperature.
    w:        [0, 1] Weight for Model-Based values (0=MF, 1=MB).
    habit_lr: [0, 1] Learning rate for habit trace (buildup/decay speed).
    habit_w:  [0, 5] Weight of habit strength in decision.
    """
    lr, beta, w, habit_lr, habit_w = model_parameters
    n_trials = len(action_1)
    
    # Transition matrix: A(0)->X(0) 0.7, U(1)->Y(1) 0.7
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_mf = np.zeros(2)          # Stage 1 MF values
    q_mb = np.zeros(2)          # Stage 1 MB values
    q2 = np.zeros((2, 2))       # Stage 2 values
    habit = np.zeros(2)         # Choice kernel
    
    for trial in range(n_trials):
        # Handle missing data
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue
            
        # --- Stage 1 Decision ---
        # Calculate MB values: T * max(Q2)
        max_q2 = np.max(q2, axis=1)
        q_mb = transition_matrix @ max_q2
        
        # Combined value: w*MB + (1-w)*MF + habit
        q_net = w * q_mb + (1 - w) * q_mf + habit_w * habit
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Decision ---
        state_idx = state[trial]
        exp_q2 = np.exp(beta * q2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        # Update Habit (Choice Kernel)
        # Chosen action approaches 1, unchosen approaches 0
        chosen = action_1[trial]
        habit[chosen] += habit_lr * (1.0 - habit[chosen])
        habit[1 - chosen] += habit_lr * (0.0 - habit[1 - chosen])
        
        # Update Q-values (SARSA-like for MF)
        # Stage 1 PE
        delta_1 = q2[state_idx, action_2[trial]] - q_mf[action_1[trial]]
        q_mf[action_1[trial]] += lr * delta_1
        
        # Stage 2 PE
        delta_2 = reward[trial] - q2[state_idx, action_2[trial]]
        q2[state_idx, action_2[trial]] += lr * delta_2
        
    eps = 1e-10
    valid_trials = (action_1 != -1) & (state != -1) & (action_2 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Dual-Beta TD(lambda) Learner.
    
    Model-Free learner with eligibility traces and perseveration.
    Uses separate temperature parameters for Stage 1 and Stage 2 choices.
    
    Parameters:
    lr:           [0, 1] Learning rate.
    beta_1:       [0, 10] Inverse temperature for Stage 1.
    beta_2:       [0, 10] Inverse temperature for Stage 2.
    lambda_param: [0, 1] Eligibility trace decay.
    perseveration:[0, 5] Sticky choice bonus.
    """
    lr, beta_1, beta_2, lambda_param, perseveration = model_parameters
    n_trials = len(action_1)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))
    
    prev_action_1 = -1
    
    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue
            
        # --- Stage 1 Decision ---
        q1_eff = q1.copy()
        if prev_action_1 != -1:
            q1_eff[prev_action_1] += perseveration
            
        exp_q1 = np.exp(beta_1 * q1_eff)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Decision ---
        state_idx = state[trial]
        exp_q2 = np.exp(beta_2 * q2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        # TD(lambda) updates
        delta_1 = q2[state_idx, action_2[trial]] - q1[action_1[trial]]
        delta_2 = reward[trial] - q2[state_idx, action_2[trial]]
        
        # Update Stage 2
        q2[state_idx, action_2[trial]] += lr * delta_2
        
        # Update Stage 1 with trace
        combined_delta = delta_1 + lambda_param * delta_2
        q1[action_1[trial]] += lr * combined_delta
        
        prev_action_1 = action_1[trial]
        
    eps = 1e-10
    valid_trials = (action_1 != -1) & (state != -1) & (action_2 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Asymmetric Hybrid Learner.
    
    Hybrid MB/MF learner with separate learning rates for positive and 
    negative prediction errors, plus choice perseveration.
    
    Parameters:
    lr_pos:       [0, 1] Learning rate for positive prediction errors.
    lr_neg:       [0, 1] Learning rate for negative prediction errors.
    beta:         [0, 10] Inverse temperature.
    w:            [0, 1] Weight for Model-Based values.
    perseveration:[0, 5] Sticky choice bonus.
    """
    lr_pos, lr_neg, beta, w, perseveration = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_mf = np.zeros(2)
    q_mb = np.zeros(2)
    q2 = np.zeros((2, 2))
    
    prev_action_1 = -1
    
    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue
            
        # --- Stage 1 Decision ---
        # MB Calculation
        max_q2 = np.max(q2, axis=1)
        q_mb = transition_matrix @ max_q2
        
        # Net Value
        q_net = w * q_mb + (1 - w) * q_mf
        if prev_action_1 != -1:
            q_net[prev_action_1] += perseveration
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Decision ---
        state_idx = state[trial]
        exp_q2 = np.exp(beta * q2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        # Stage 1 MF Update
        delta_1 = q2[state_idx, action_2[trial]] - q_mf[action_1[trial]]
        lr_1 = lr_pos if delta_1 > 0 else lr_neg
        q_mf[action_1[trial]] += lr_1 * delta_1
        
        # Stage 2 Update
        delta_2 = reward[trial] - q2[state_idx, action_2[trial]]
        lr_2 = lr_pos if delta_2 > 0 else lr_neg
        q2[state_idx, action_2[trial]] += lr_2 * delta_2
        
        prev_action_1 = action_1[trial]
        
    eps = 1e-10
    valid_trials = (action_1 != -1) & (state != -1) & (action_2 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```