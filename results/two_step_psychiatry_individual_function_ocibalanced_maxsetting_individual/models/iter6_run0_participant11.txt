Here are three new cognitive models implemented as Python functions.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Outcome-Dependent Stickiness.
    
    Combines Model-Based (MB) and Model-Free (MF) control with an eligibility trace.
    Perseveration (stickiness) is modulated by the previous outcome (Win-Stay, Lose-Shift logic).
    This allows the model to capture different tendencies to repeat choices after rewards vs. omissions,
    while maintaining the hybrid architecture to explain planning behavior.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for MF Q-values.
    - beta_1: [0, 10] Inverse temperature for Stage 1 choice.
    - beta_2: [0, 10] Inverse temperature for Stage 2 choice.
    - lambda_param: [0, 1] Eligibility trace decay (credits Stage 1 for Stage 2 outcome).
    - w: [0, 1] Weight of MB system (0=MF, 1=MB).
    - stick_win: [0, 5] Stickiness bonus after a rewarded trial.
    - stick_loss: [0, 5] Stickiness bonus after an unrewarded trial.
    """
    learning_rate, beta_1, beta_2, lambda_param, w, stick_win, stick_loss = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    last_reward_1 = -1
    
    last_action_2 = np.array([-1, -1]) # For each state
    last_reward_2 = np.array([-1, -1])
    
    for trial in range(n_trials):
        # Stage 1 Choice Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Apply Outcome-Dependent Stickiness
        if last_action_1 != -1:
            bonus = stick_win if last_reward_1 == 1 else stick_loss
            q_net_1[last_action_1] += bonus
            
        exp_q1 = np.exp(beta_1 * q_net_1 - np.max(beta_1 * q_net_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        
        # Stage 2 Choice Policy
        q_net_2 = q_stage2_mf[state_idx].copy()
        
        if last_action_2[state_idx] != -1:
            bonus = stick_win if last_reward_2[state_idx] == 1 else stick_loss
            q_net_2[last_action_2[state_idx]] += bonus
            
        exp_q2 = np.exp(beta_2 * q_net_2 - np.max(beta_2 * q_net_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # Updates
        delta_1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        delta_2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Stage 1 MF update (SARSA-like + eligibility trace)
        q_stage1_mf[action_1[trial]] += learning_rate * delta_1
        q_stage1_mf[action_1[trial]] += learning_rate * lambda_param * delta_2
        
        # Stage 2 MF update
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_2
        
        # Update history
        last_action_1 = action_1[trial]
        last_reward_1 = reward[trial]
        last_action_2[state_idx] = action_2[trial]
        last_reward_2[state_idx] = reward[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Asymmetric Learning Rates.
    
    Incorporates separate learning rates for positive and negative prediction errors,
    allowing the model to learn differently from gains versus losses (or lack of gains).
    This asymmetry is applied to the Model-Free updates within a Hybrid MB/MF architecture.
    
    Parameters:
    - lr_pos: [0, 1] Learning rate for positive prediction errors (delta > 0).
    - lr_neg: [0, 1] Learning rate for negative prediction errors (delta <= 0).
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - lambda_param: [0, 1] Eligibility trace decay.
    - w: [0, 1] MB/MF mixing weight.
    - stickiness: [0, 5] General stickiness bonus.
    """
    lr_pos, lr_neg, beta_1, beta_2, lambda_param, w, stickiness = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    last_action_2 = np.array([-1, -1])
    
    for trial in range(n_trials):
        # Stage 1 Choice Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_action_1 != -1:
            q_net_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta_1 * q_net_1 - np.max(beta_1 * q_net_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        
        # Stage 2 Choice Policy
        q_net_2 = q_stage2_mf[state_idx].copy()
        if last_action_2[state_idx] != -1:
            q_net_2[last_action_2[state_idx]] += stickiness
            
        exp_q2 = np.exp(beta_2 * q_net_2 - np.max(beta_2 * q_net_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # Updates
        delta_1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        lr_1 = lr_pos if delta_1 > 0 else lr_neg
        q_stage1_mf[action_1[trial]] += lr_1 * delta_1
        
        delta_2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        lr_2 = lr_pos if delta_2 > 0 else lr_neg
        
        # Eligibility trace update using the LR associated with the outcome (delta_2)
        q_stage1_mf[action_1[trial]] += lr_2 * lambda_param * delta_2
        q_stage2_mf[state_idx, action_2[trial]] += lr_2 * delta_2
        
        last_action_1 = action_1[trial]
        last_action_2[state_idx] = action_2[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Choice Kernel.
    
    Replaces simple "last-choice" stickiness with a Choice Kernel (CK) that integrates
    the history of past choices with a decay rate. This allows for longer-term
    perseveration or "habit" formation beyond just the immediately preceding trial.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for MF Q-values.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - lambda_param: [0, 1] Eligibility trace decay.
    - w: [0, 1] MB/MF mixing weight.
    - ck_decay: [0, 1] Decay rate for choice kernel (1 = no decay, 0 = instant decay).
    - ck_weight: [0, 5] Weight contribution of CK to decision value.
    """
    learning_rate, beta_1, beta_2, lambda_param, w, ck_decay, ck_weight = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    ck_1 = np.zeros(2)
    ck_2 = np.zeros((2, 2))
    
    for trial in range(n_trials):
        # Stage 1 Choice Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combine MB, MF, and Choice Kernel
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf + ck_weight * ck_1
        
        exp_q1 = np.exp(beta_1 * q_net_1 - np.max(beta_1 * q_net_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        
        # Stage 2 Choice Policy
        q_net_2 = q_stage2_mf[state_idx] + ck_weight * ck_2[state_idx]
        exp_q2 = np.exp(beta_2 * q_net_2 - np.max(beta_2 * q_net_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # Updates
        delta_1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        delta_2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        q_stage1_mf[action_1[trial]] += learning_rate * delta_1
        q_stage1_mf[action_1[trial]] += learning_rate * lambda_param * delta_2
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_2
        
        # Update Choice Kernels
        ck_1 *= ck_decay
        ck_1[action_1[trial]] += 1
        
        ck_2[state_idx] *= ck_decay
        ck_2[state_idx][action_2[trial]] += 1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```