def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Asymmetric Learning Rates with Choice Stickiness.

    This model differentiates between learning from positive (better than expected)
    and negative (worse than expected) prediction errors. This asymmetry allows
    the model to capture biases like optimism or pessimism. It also includes
    choice stickiness to account for the participant's tendency to repeat choices.

    Parameters:
    lr_pos (float): Learning rate for positive prediction errors [0, 1].
    lr_neg (float): Learning rate for negative prediction errors [0, 1].
    beta (float): Inverse temperature for softmax choice [0, 10].
    w (float): Weighting parameter for Model-Based vs Model-Free values [0, 1].
    lam (float): Eligibility trace parameter for Stage 1 updates [0, 1].
    stick (float): Choice stickiness parameter [0, 5].
    """
    lr_pos, lr_neg, beta, w, lam, stick = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    log_loss = 0
    eps = 1e-10
    
    for trial in range(n_trials):
        # Stage 1: Spaceship Choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        logits_1 = beta * q_net_1
        
        # Add stickiness bonus to the previously chosen action
        if last_action_1 != -1:
            logits_1[last_action_1] += stick
            
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        if action_1[trial] != -1:
            log_loss -= np.log(probs_1[action_1[trial]] + eps)
            last_action_1 = action_1[trial]
        
        state_idx = state[trial]
        if state_idx == -1: 
            continue

        # Stage 2: Alien Choice
        logits_2 = beta * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        if action_2[trial] != -1:
            log_loss -= np.log(probs_2[action_2[trial]] + eps)
            
            r = reward[trial]
            if r == -1: r = 0
            
            a1 = action_1[trial]
            a2 = action_2[trial]
            
            # Prediction Errors
            delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
            delta_stage2 = r - q_stage2_mf[state_idx, a2]
            
            # Determine Learning Rates based on sign of PE
            lr_1 = lr_pos if delta_stage1 > 0 else lr_neg
            lr_2 = lr_pos if delta_stage2 > 0 else lr_neg
            
            # Update Q-values
            q_stage1_mf[a1] += lr_1 * delta_stage1 + lr_2 * lam * delta_stage2
            q_stage2_mf[state_idx, a2] += lr_2 * delta_stage2
            
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Separate Stage Learning Rates with Stickiness.

    This model posits that learning rates differ between the two stages of the task.
    The abstract choice of spaceship (Stage 1) might be updated at a different rate
    than the concrete choice of alien (Stage 2). Includes stickiness for Stage 1.

    Parameters:
    lr_1 (float): Learning rate for Stage 1 updates [0, 1].
    lr_2 (float): Learning rate for Stage 2 updates [0, 1].
    beta (float): Inverse temperature for softmax choice [0, 10].
    w (float): Weighting parameter for Model-Based vs Model-Free values [0, 1].
    lam (float): Eligibility trace parameter [0, 1].
    stick (float): Choice stickiness parameter [0, 5].
    """
    lr_1, lr_2, beta, w, lam, stick = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    log_loss = 0
    eps = 1e-10
    
    for trial in range(n_trials):
        # Stage 1: Spaceship Choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        logits_1 = beta * q_net_1
        
        if last_action_1 != -1:
            logits_1[last_action_1] += stick
            
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        if action_1[trial] != -1:
            log_loss -= np.log(probs_1[action_1[trial]] + eps)
            last_action_1 = action_1[trial]
        
        state_idx = state[trial]
        if state_idx == -1: 
            continue

        # Stage 2: Alien Choice
        logits_2 = beta * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        if action_2[trial] != -1:
            log_loss -= np.log(probs_2[action_2[trial]] + eps)
            
            r = reward[trial]
            if r == -1: r = 0
            
            a1 = action_1[trial]
            a2 = action_2[trial]
            
            # Prediction Errors
            delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
            delta_stage2 = r - q_stage2_mf[state_idx, a2]
            
            # Update Q-values using stage-specific learning rates
            q_stage1_mf[a1] += lr_1 * delta_stage1 + lr_1 * lam * delta_stage2
            q_stage2_mf[state_idx, a2] += lr_2 * delta_stage2
            
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Dynamic Transition Learning with Stickiness.

    This model assumes the participant does not view the transition probabilities
    as fixed. Instead, they learn the transition matrix over time based on 
    observed transitions. This affects the Model-Based value calculation.
    Includes choice stickiness.

    Parameters:
    lr (float): Learning rate for action value updates [0, 1].
    lr_trans (float): Learning rate for transition probability updates [0, 1].
    beta (float): Inverse temperature for softmax choice [0, 10].
    w (float): Weighting parameter for Model-Based vs Model-Free values [0, 1].
    lam (float): Eligibility trace parameter [0, 1].
    stick (float): Choice stickiness parameter [0, 5].
    """
    lr, lr_trans, beta, w, lam, stick = model_parameters
    n_trials = len(action_1)
    
    # Start with standard transition beliefs
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    log_loss = 0
    eps = 1e-10
    
    for trial in range(n_trials):
        # Stage 1: Spaceship Choice (MB uses dynamic transitions)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        logits_1 = beta * q_net_1
        
        if last_action_1 != -1:
            logits_1[last_action_1] += stick
            
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        if action_1[trial] != -1:
            log_loss -= np.log(probs_1[action_1[trial]] + eps)
            
        state_idx = state[trial]
        if state_idx == -1: 
            continue

        # Stage 2: Alien Choice
        logits_2 = beta * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        if action_2[trial] != -1:
            log_loss -= np.log(probs_2[action_2[trial]] + eps)
            
            r = reward[trial]
            if r == -1: r = 0
            
            a1 = action_1[trial]
            a2 = action_2[trial]
            
            # Value Updates
            delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
            delta_stage2 = r - q_stage2_mf[state_idx, a2]
            
            q_stage1_mf[a1] += lr * delta_stage1 + lr * lam * delta_stage2
            q_stage2_mf[state_idx, a2] += lr * delta_stage2
            
            # Transition Probability Update
            # We observed transition from a1 to state_idx.
            # Update the belief: P(state_idx | a1) moves towards 1.
            curr_p = transition_matrix[a1, state_idx]
            transition_matrix[a1, state_idx] += lr_trans * (1.0 - curr_p)
            transition_matrix[a1, 1 - state_idx] = 1.0 - transition_matrix[a1, state_idx]
        
        if action_1[trial] != -1:
            last_action_1 = action_1[trial]
            
    return log_loss