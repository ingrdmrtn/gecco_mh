Here are the three proposed cognitive models.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Model 1: Transition-Dependent Model-Free Learning with Decay.
    
    This model posits that the agent learns Stage 1 values via a Model-Free update,
    but applies different learning rates depending on whether the transition to Stage 2
    was Common or Rare. This allows the agent to empirically weigh information from
    rare transitions differently (e.g., ignoring them or treating them as counterfactuals)
    without a full internal model. Includes decay for non-stationarity.
    
    Parameters:
    lr_common: [0, 1] - Learning rate for Stage 1 update after a Common transition.
    lr_rare: [0, 1] - Learning rate for Stage 1 update after a Rare transition.
    lr_stage2: [0, 1] - Learning rate for Stage 2 value updates.
    decay_rate: [0, 1] - Decay rate for unchosen actions (Stage 1 & 2) toward 0.5.
    beta: [0, 10] - Inverse temperature for softmax choice.
    perseveration: [-3, 3] - Stickiness to previous Stage 1 choice.
    """
    lr_common, lr_rare, lr_stage2, decay_rate, beta, perseveration = model_parameters
    n_trials = len(action_1)
    
    # Initialize values
    q_stage1 = np.zeros(2) + 0.5
    q_stage2 = np.zeros((2, 2)) + 0.5
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Transition structure for reference: S0->P0 (0.7), S1->P1 (0.7)
    # We define Common transitions: (A1=0, S=0) and (A1=1, S=1)
    
    prev_a1 = -1
    
    for t in range(n_trials):
        # --- Stage 1 Choice ---
        logits_1 = beta * q_stage1
        if prev_a1 != -1:
            logits_1[prev_a1] += perseveration
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[t] = probs_1[action_1[t]]
        
        a1 = action_1[t]
        s_idx = state[t]
        
        # --- Stage 2 Choice ---
        logits_2 = beta * q_stage2[s_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[t] = probs_2[action_2[t]]
        
        a2 = action_2[t]
        r = reward[t]
        
        # --- Decay ---
        # Decay unchosen Stage 1
        q_stage1[1-a1] += decay_rate * (0.5 - q_stage1[1-a1])
        # Decay unchosen Stage 2 (only for the visited state)
        q_stage2[s_idx, 1-a2] += decay_rate * (0.5 - q_stage2[s_idx, 1-a2])
        
        # --- Updates ---
        # Update Stage 2
        delta2 = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += lr_stage2 * delta2
        
        # Update Stage 1
        # Determine if transition was common
        is_common = (a1 == 0 and s_idx == 0) or (a1 == 1 and s_idx == 1)
        
        current_lr = lr_common if is_common else lr_rare
        
        # SARSA-style update using the value of the action taken in Stage 2
        delta1 = q_stage2[s_idx, a2] - q_stage1[a1]
        q_stage1[a1] += current_lr * delta1
        
        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model 2: Asymmetric Hybrid Model with Decay.
    
    A Hybrid Model-Based / Model-Free agent that learns values asymmetrically based on 
    prediction error sign (Positive vs Negative). This asymmetry applies to the value 
    learning in Stage 2 (which drives the MB planner) and the MF updates in Stage 1.
    This captures potential biases in processing wins vs losses.
    
    Parameters:
    alpha_pos: [0, 1] - Learning rate for positive prediction errors.
    alpha_neg: [0, 1] - Learning rate for negative prediction errors.
    decay_rate: [0, 1] - Decay rate for unchosen actions.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - Mixing weight (0=MF, 1=MB).
    perseveration: [-3, 3] - Stickiness to previous Stage 1 choice.
    """
    alpha_pos, alpha_neg, decay_rate, beta, w, perseveration = model_parameters
    n_trials = len(action_1)
    
    # Transition matrix
    trans_probs = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_mf = np.zeros(2) + 0.5
    q_s2 = np.zeros((2, 2)) + 0.5
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1
    
    for t in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based Value Calculation
        # Max value of each state in stage 2
        v_s2 = np.max(q_s2, axis=1) # [V(State0), V(State1)]
        q_mb = trans_probs @ v_s2   # [Q_MB(A0), Q_MB(A1)]
        
        # Net Value
        q_net = w * q_mb + (1 - w) * q_mf
        
        logits_1 = beta * q_net
        if prev_a1 != -1:
            logits_1[prev_a1] += perseveration
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[t] = probs_1[action_1[t]]
        
        a1 = action_1[t]
        s_idx = state[t]
        
        # --- Stage 2 Choice ---
        logits_2 = beta * q_s2[s_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[t] = probs_2[action_2[t]]
        
        a2 = action_2[t]
        r = reward[t]
        
        # --- Decay ---
        q_mf[1-a1] += decay_rate * (0.5 - q_mf[1-a1])
        q_s2[s_idx, 1-a2] += decay_rate * (0.5 - q_s2[s_idx, 1-a2])
        
        # --- Updates ---
        # Update Stage 2
        pe2 = r - q_s2[s_idx, a2]
        lr2 = alpha_pos if pe2 > 0 else alpha_neg
        q_s2[s_idx, a2] += lr2 * pe2
        
        # Update Stage 1 MF
        # Using SARSA-like update from Stage 2 value
        pe1 = q_s2[s_idx, a2] - q_mf[a1]
        lr1 = alpha_pos if pe1 > 0 else alpha_neg
        q_mf[a1] += lr1 * pe1
        
        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model 3: Dual-Rate Hybrid Model with Decay.
    
    A Hybrid agent where the Model-Based system (driven by Stage 2 values) and the 
    Model-Free system (Stage 1 cached values) learn at different rates. This captures 
    potential differences in the adaptability of the internal model vs. the habit.
    For example, the agent might update their map of the environment (aliens) quickly
    while their spaceship preference changes slowly.
    
    Parameters:
    lr_mf: [0, 1] - Learning rate for Stage 1 MF values.
    lr_mb: [0, 1] - Learning rate for Stage 2 values (used for MB planning).
    decay_rate: [0, 1] - Decay rate for unchosen actions.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - Mixing weight (0=MF, 1=MB).
    perseveration: [-3, 3] - Stickiness to previous Stage 1 choice.
    """
    lr_mf, lr_mb, decay_rate, beta, w, perseveration = model_parameters
    n_trials = len(action_1)
    
    trans_probs = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_mf = np.zeros(2) + 0.5
    q_s2 = np.zeros((2, 2)) + 0.5
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1
    
    for t in range(n_trials):
        # --- Stage 1 Choice ---
        v_s2 = np.max(q_s2, axis=1)
        q_mb = trans_probs @ v_s2
        
        q_net = w * q_mb + (1 - w) * q_mf
        
        logits_1 = beta * q_net
        if prev_a1 != -1:
            logits_1[prev_a1] += perseveration
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[t] = probs_1[action_1[t]]
        
        a1 = action_1[t]
        s_idx = state[t]
        
        # --- Stage 2 Choice ---
        logits_2 = beta * q_s2[s_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[t] = probs_2[action_2[t]]
        
        a2 = action_2[t]
        r = reward[t]
        
        # --- Decay ---
        q_mf[1-a1] += decay_rate * (0.5 - q_mf[1-a1])
        q_s2[s_idx, 1-a2] += decay_rate * (0.5 - q_s2[s_idx, 1-a2])
        
        # --- Updates ---
        # Update Stage 2 (The "Model")
        delta2 = r - q_s2[s_idx, a2]
        q_s2[s_idx, a2] += lr_mb * delta2
        
        # Update Stage 1 MF (The "Habit")
        delta1 = q_s2[s_idx, a2] - q_mf[a1]
        q_mf[a1] += lr_mf * delta1
        
        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```