Here are the three proposed cognitive models.

### Model 1: Hybrid MB/MF with Asymmetric Learning and Dual Stickiness
This model builds on the previous best model by incorporating **Dual Stickiness** (stickiness for both Stage 1 and Stage 2 choices) alongside **Asymmetric Learning Rates**. The participant data shows strong repetition in Stage 2 (choosing Alien 0 repeatedly), suggesting that stickiness operates at both decision stages. Separating positive and negative learning rates allows the model to capture the participant's differential sensitivity to rewards versus omissions.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF model with Asymmetric Learning Rates and Dual Stickiness.
    
    Incorporates separate stickiness parameters for Stage 1 (Spaceship) and Stage 2 (Alien),
    allowing the model to capture perseveration at both levels.
    Uses asymmetric learning rates for positive and negative prediction errors.

    Parameters:
    - lr_pos: [0, 1] Learning rate for positive prediction errors (better than expected).
    - lr_neg: [0, 1] Learning rate for negative prediction errors (worse than expected).
    - beta: [0, 10] Inverse temperature (exploration/exploitation trade-off).
    - w: [0, 1] Weight of Model-Based system (0 = Pure MF, 1 = Pure MB).
    - lam: [0, 1] Eligibility trace (credit assignment to Stage 1 choice from Stage 2 outcome).
    - stick1: [0, 5] Stickiness (perseveration) bonus for Stage 1 choice.
    - stick2: [0, 5] Stickiness (perseveration) bonus for Stage 2 choice.
    """
    lr_pos, lr_neg, beta, w, lam, stick1, stick2 = model_parameters
    n_trials = len(action_1)
    
    # Fixed transition matrix as per task description: A->X (0.7), U->Y (0.7)
    # Row 0: Spaceship 0 (A) -> [Planet 0 (X), Planet 1 (Y)]
    # Row 1: Spaceship 1 (U) -> [Planet 0 (X), Planet 1 (Y)]
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    q_mf = np.zeros(2)          # Stage 1 Model-Free Q-values
    q2 = np.zeros((2, 2))       # Stage 2 Q-values (State x Action)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1
    last_action_2 = -1
    
    eps = 1e-10

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])

        # Skip missing trials
        if a1 == -1 or a2 == -1:
            continue

        # --- Stage 1 Policy ---
        # Model-Based Value: Expected max Q2 value weighted by transition probs
        max_q2 = np.max(q2, axis=1)
        q_mb = transition_matrix @ max_q2
        
        # Net Value: Weighted sum of MB and MF
        q_net_1 = w * q_mb + (1 - w) * q_mf
        
        # Add Stickiness for Stage 1
        if last_action_1 != -1:
            q_net_1[last_action_1] += stick1
            
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        q_net_2 = q2[s].copy()
        
        # Add Stickiness for Stage 2
        # Note: Stickiness applies to the action index (Alien 0 or 1) regardless of planet,
        # assuming motor perseveration.
        if last_action_2 != -1:
            q_net_2[last_action_2] += stick2

        exp_q2 = np.exp(beta * q_net_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        
        # Stage 1 Prediction Error (TD(0))
        delta_1 = q2[s, a2] - q_mf[a1]
        lr_1 = lr_pos if delta_1 > 0 else lr_neg
        q_mf[a1] += lr_1 * delta_1
        
        # Stage 2 Prediction Error
        delta_2 = r - q2[s, a2]
        lr_2 = lr_pos if delta_2 > 0 else lr_neg
        q2[s, a2] += lr_2 * delta_2
        
        # Eligibility Trace Update for Stage 1
        # Updates Stage 1 MF value based on Stage 2 RPE
        q_mf[a1] += lr_2 * lam * delta_2
        
        last_action_1 = a1
        last_action_2 = a2

    # Filter out skipped trials for log loss calculation
    mask = p_choice_1 > 0
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss
```

### Model 2: Hybrid MB/MF with Outcome-Dependent Beta
This model hypothesizes that the participant's exploration/exploitation balance shifts based on the previous outcome. A "Win-Stay" strategy implies high determinism (high beta) after a reward, while a "Lose-Shift" or "Lose-Explore" strategy implies higher randomness (low beta) after a loss. This model captures dynamic changes in decision noise that static parameters miss.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF model with Outcome-Dependent Beta (Inverse Temperature).
    
    The inverse temperature parameter (beta) switches based on the previous trial's outcome.
    This models the 'Win-Stay, Lose-Explore' dynamic where participants may become more
    deterministic (exploit) after wins and more random (explore) after losses.

    Parameters:
    - lr: [0, 1] Learning rate.
    - beta_win: [0, 10] Inverse temperature after a rewarded trial (Exploitation).
    - beta_lose: [0, 10] Inverse temperature after an unrewarded trial (Exploration).
    - w: [0, 1] Weight of Model-Based system.
    - lam: [0, 1] Eligibility trace.
    - stickiness: [0, 5] Choice perseveration bonus.
    """
    lr, beta_win, beta_lose, w, lam, stickiness = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    q_mf = np.zeros(2)
    q2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1
    prev_reward = 0 # Assume loss/neutral for start
    
    eps = 1e-10

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])

        if a1 == -1 or a2 == -1:
            continue
            
        # Select beta based on previous reward
        current_beta = beta_win if prev_reward == 1 else beta_lose

        # --- Stage 1 Policy ---
        max_q2 = np.max(q2, axis=1)
        q_mb = transition_matrix @ max_q2
        q_net_1 = w * q_mb + (1 - w) * q_mf
        
        if last_action_1 != -1:
            q_net_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(current_beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        # Apply the same beta modulation to Stage 2
        exp_q2 = np.exp(current_beta * q2[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        delta_1 = q2[s, a2] - q_mf[a1]
        q_mf[a1] += lr * delta_1
        
        delta_2 = r - q2[s, a2]
        q2[s, a2] += lr * delta_2
        
        q_mf[a1] += lr * lam * delta_2
        
        last_action_1 = a1
        prev_reward = r

    mask = p_choice_1 > 0
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss
```

### Model 3: Hybrid MB/MF with Asymmetric Eligibility Trace (Lambda)
This model introduces asymmetry in the eligibility trace parameter ($\lambda$). This allows the model to differentiate how credit is assigned to the Stage 1 choice based on whether the Stage 2 outcome was better or worse than expected. For instance, a participant might strongly reinforce their spaceship choice after a "lucky" win (high $\lambda_{pos}$) but not penalize it after a loss (low $\lambda_{neg}$), attributing the failure to the alien rather than the spaceship.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF model with Asymmetric Eligibility Traces (Lambda).
    
    Differentiates the strength of the eligibility trace based on the sign of the 
    Stage 2 prediction error. This allows the model to update the Stage 1 Model-Free 
    value differently for positive surprises (wins/better than expected) versus 
    negative surprises (losses/worse than expected).

    Parameters:
    - lr: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weight of Model-Based system.
    - lam_pos: [0, 1] Eligibility trace for positive Stage 2 prediction errors.
    - lam_neg: [0, 1] Eligibility trace for negative Stage 2 prediction errors.
    - stickiness: [0, 5] Choice perseveration bonus.
    """
    lr, beta, w, lam_pos, lam_neg, stickiness = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    q_mf = np.zeros(2)
    q2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1
    
    eps = 1e-10

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])

        if a1 == -1 or a2 == -1:
            continue

        # --- Stage 1 Policy ---
        max_q2 = np.max(q2, axis=1)
        q_mb = transition_matrix @ max_q2
        q_net_1 = w * q_mb + (1 - w) * q_mf
        
        if last_action_1 != -1:
            q_net_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q2[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        delta_1 = q2[s, a2] - q_mf[a1]
        q_mf[a1] += lr * delta_1
        
        delta_2 = r - q2[s, a2]
        q2[s, a2] += lr * delta_2
        
        # Select lambda based on the sign of the prediction error
        current_lam = lam_pos if delta_2 > 0 else lam_neg
        
        # Update Stage 1 MF value using the specific lambda
        q_mf[a1] += lr * current_lam * delta_2
        
        last_action_1 = a1

    mask = p_choice_1 > 0
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss
```