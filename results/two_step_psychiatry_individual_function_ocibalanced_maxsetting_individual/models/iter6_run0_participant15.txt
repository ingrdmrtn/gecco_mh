Here are three cognitive models based on the participant's data and the two-step task structure.

### Model 1: Hybrid MF/MB with Adaptive Transition Learning
This model hypothesizes that the participant does not assume a fixed transition structure (0.7/0.3) but learns it over time. The "Win-Switch" behavior on rare transitions (e.g., Trial 61-62) suggests the participant might update their belief about which spaceship leads to which planet based on observed transitions.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MF/MB model with Adaptive Transition Learning.
    
    The agent learns the transition probabilities (Spaceship -> Planet) 
    dynamically, rather than assuming they are fixed at 0.7/0.3.
    This allows the MB system to react to streaks of 'rare' transitions.
    
    Parameters:
    - alpha: [0, 1] Learning rate for Q-values (Stage 1 and 2).
    - alpha_T: [0, 1] Learning rate for transition probabilities.
    - beta_mb: [0, 10] Weight for Model-Based values in Stage 1.
    - beta_mf: [0, 10] Weight for Model-Free values in Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2 choices.
    - stickiness: [0, 10] Perseverance bonus for repeated Stage 1 choices.
    """
    alpha, alpha_T, beta_mb, beta_mf, beta_2, stickiness = model_parameters
    n_trials = len(action_1)
    
    # Initialize Transition Matrix: T[action, next_state]
    # Start with the common prior 0.7/0.3
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Q-values
    q_stage1_mf = np.zeros(2) + 0.5
    q_stage2_mf = np.zeros((2, 2)) + 0.5
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based Value: Expected max stage 2 value given learned transitions
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        logits_1 = (beta_mb * q_stage1_mb) + (beta_mf * q_stage1_mf)
        
        # Stickiness
        if last_action_1 != -1:
            logits_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = action_1[trial]
        if a1 == -1: # Missing data handling
            last_action_1 = -1
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue
            
        p_choice_1[trial] = probs_1[a1]
        last_action_1 = a1
        
        # --- Transition ---
        s_curr = state[trial]
        
        # Update Transition Matrix (Adaptive MB)
        # Increase prob of observed transition (a1 -> s_curr)
        transition_matrix[a1, s_curr] += alpha_T * (1.0 - transition_matrix[a1, s_curr])
        # Decrease prob of unobserved transition
        transition_matrix[a1, 1 - s_curr] = 1.0 - transition_matrix[a1, s_curr]
        
        # --- Stage 2 Choice ---
        logits_2 = beta_2 * q_stage2_mf[s_curr]
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = action_2[trial]
        if a2 == -1:
            p_choice_2[trial] = 1.0
            continue
            
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        r = reward[trial]
        
        # Stage 2 Update (TD-0)
        q_stage2_mf[s_curr, a2] += alpha * (r - q_stage2_mf[s_curr, a2])
        
        # Stage 1 MF Update (TD-1 / Sarsa)
        # Using the updated Stage 2 value as the target for Stage 1 MF
        # (Standard hybrid implementation often uses Q(s2, a2) or Reward)
        # Here we use Q(s2, a2) as the proxy for the value of the transition
        q_stage1_mf[a1] += alpha * (q_stage2_mf[s_curr, a2] - q_stage1_mf[a1])

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid MF/MB with Dual Stickiness
The participant shows strong persistence in both choosing spaceships (Stage 1) and specific aliens (Stage 2). This model incorporates stickiness parameters for both stages to capture this "double habit" behavior.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MF/MB model with Stickiness in both Stage 1 and Stage 2.
    
    Captures the tendency to repeat choices at both the spaceship level
    and the alien level (perseveration), independent of value learning.
    
    Parameters:
    - alpha: [0, 1] Learning rate.
    - beta_mb: [0, 10] Weight for Model-Based values.
    - beta_mf: [0, 10] Weight for Model-Free values.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - stick_1: [0, 10] Stickiness for Stage 1 (Spaceship).
    - stick_2: [0, 10] Stickiness for Stage 2 (Alien).
    """
    alpha, beta_mb, beta_mf, beta_2, stick_1, stick_2 = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2) + 0.5
    q_stage2_mf = np.zeros((2, 2)) + 0.5
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1
    # Track last action for each state in stage 2 (2 planets)
    last_action_2_per_state = [-1, -1] 
    
    for trial in range(n_trials):
        # --- Stage 1 ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        logits_1 = (beta_mb * q_stage1_mb) + (beta_mf * q_stage1_mf)
        
        if last_action_1 != -1:
            logits_1[last_action_1] += stick_1
            
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = action_1[trial]
        if a1 == -1:
            last_action_1 = -1
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue
            
        p_choice_1[trial] = probs_1[a1]
        last_action_1 = a1
        
        # --- Stage 2 ---
        s_curr = state[trial]
        
        logits_2 = beta_2 * q_stage2_mf[s_curr]
        
        # Apply Stage 2 stickiness if we have a previous choice for this planet
        if last_action_2_per_state[s_curr] != -1:
            logits_2[last_action_2_per_state[s_curr]] += stick_2
            
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = action_2[trial]
        if a2 == -1:
            p_choice_2[trial] = 1.0
            continue
            
        p_choice_2[trial] = probs_2[a2]
        last_action_2_per_state[s_curr] = a2
        
        # --- Learning ---
        r = reward[trial]
        q_stage2_mf[s_curr, a2] += alpha * (r - q_stage2_mf[s_curr, a2])
        q_stage1_mf[a1] += alpha * (q_stage2_mf[s_curr, a2] - q_stage1_mf[a1])

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid MB with Dual MF Strategies (TD and Direct)
This model splits the Model-Free component into two distinct strategies:
1. **TD-Learning (SARSA):** Updates Stage 1 value based on the value of the State reached (chaining).
2. **Direct Reinforcement (Monte Carlo):** Updates Stage 1 value based directly on the final Reward obtained (ignoring the planet structure).
This allows the model to differentiate between participants who associate spaceships with planets (TD) versus those who just associate spaceships with winning (Direct).

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB with Dual MF strategies: TD-Learning and Direct Reinforcement.
    
    Distinguishes between two types of Model-Free learning:
    1. TD (Temporal Difference): Updates Stage 1 based on Stage 2 Q-values.
    2. Direct: Updates Stage 1 based strictly on obtained Reward (skipping state).
    
    Parameters:
    - alpha: [0, 1] Learning rate.
    - beta_mb: [0, 10] Weight for Model-Based values.
    - beta_td: [0, 10] Weight for TD-based MF values.
    - beta_direct: [0, 10] Weight for Direct-Reward MF values.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - stickiness: [0, 10] Choice perseverance bonus.
    """
    alpha, beta_mb, beta_td, beta_direct, beta_2, stickiness = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Three sets of values
    q_mf_td = np.zeros(2) + 0.5      # Learned via Stage 2 value
    q_mf_direct = np.zeros(2) + 0.5  # Learned via direct reward
    q_stage2 = np.zeros((2, 2)) + 0.5
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        # --- Stage 1 ---
        # MB Value
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_mb = transition_matrix @ max_q_stage2
        
        # Combine MB, TD-MF, and Direct-MF
        logits_1 = (beta_mb * q_mb) + (beta_td * q_mf_td) + (beta_direct * q_mf_direct)
        
        if last_action_1 != -1:
            logits_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = action_1[trial]
        if a1 == -1:
            last_action_1 = -1
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue
            
        p_choice_1[trial] = probs_1[a1]
        last_action_1 = a1
        
        # --- Stage 2 ---
        s_curr = state[trial]
        logits_2 = beta_2 * q_stage2[s_curr]
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = action_2[trial]
        if a2 == -1:
            p_choice_2[trial] = 1.0
            continue
        
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        r = reward[trial]
        
        # Update Stage 2
        q_stage2[s_curr, a2] += alpha * (r - q_stage2[s_curr, a2])
        
        # Update MF TD: Driven by the value of the state reached (q_stage2)
        q_mf_td[a1] += alpha * (q_stage2[s_curr, a2] - q_mf_td[a1])
        
        # Update MF Direct: Driven by the reward directly
        q_mf_direct[a1] += alpha * (r - q_mf_direct[a1])

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```