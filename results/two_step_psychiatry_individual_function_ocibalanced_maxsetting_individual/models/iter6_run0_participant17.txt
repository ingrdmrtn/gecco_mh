Here are three new cognitive models based on the participant data and the two-step task structure.

### Model 1: Hybrid Learner with Spaceship Bias
This model hypothesizes that in addition to Model-Based (MB) and Model-Free (MF) learning, the participant has a fixed, intrinsic preference (bias) for one spaceship over the other, independent of the values. This accounts for potential "handedness" or color preference in the choice of Spaceship A (0) vs Spaceship U (1).

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid learner (MB + MF) with a fixed bias towards Spaceship 1.
    
    Parameters:
    - learning_rate: [0, 1] Update rate for values.
    - beta: [0, 10] Inverse temperature for softmax choice.
    - w: [0, 1] Weighting parameter (0 = Pure MF, 1 = Pure MB).
    - bias: [-1, 1] Intrinsic bias towards Spaceship 1 (positive) or 0 (negative).
    """
    learning_rate, beta, w, bias = model_parameters
    n_trials = len(action_1)
    
    # Transition matrix: 0->0 (0.7), 0->1 (0.3); 1->1 (0.7), 1->0 (0.3)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_mf = np.zeros(2)        # Stage 1 Model-Free values
    q_stage2 = np.zeros((2, 2)) # Stage 2 values (State x Alien)
    
    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # Model-Based Value: Expected max value of next stage
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_mb = transition_matrix @ max_q_stage2
        
        # Net Value: Weighted sum of MB and MF
        q_net = w * q_mb + (1 - w) * q_mf
        
        # Apply Fixed Bias
        q_net[1] += bias
        
        # Softmax Choice 1
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Decision ---
        s_idx = state[trial]
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning ---
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]
        
        # Update Stage 1 MF value (TD(1)-like update combining S1->S2 and S2->Reward)
        # Prediction Error 1: Value of chosen state_action - expected
        delta1 = q_stage2[s_idx, a2] - q_mf[a1]
        q_mf[a1] += learning_rate * delta1
        
        # Prediction Error 2: Reward - Value of chosen state_action
        delta2 = r - q_stage2[s_idx, a2]
        
        # Update Stage 2 value
        q_stage2[s_idx, a2] += learning_rate * delta2
        
        # Propagate second error to Stage 1 MF value (Eligibility Trace lambda=1)
        q_mf[a1] += learning_rate * delta2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Pure Model-Free with Asymmetric Learning and Perseverance
This model tests the hypothesis that the participant is not using Model-Based planning ($w=0$) but relies on a complex Model-Free strategy. It features **asymmetric learning rates** (updating differently for positive vs. negative prediction errors) and **perseverance** (tendency to repeat the last choice). This captures the potential difference in how wins and losses impact behavior.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Free learner with Asymmetric Learning Rates and Perseverance.
    
    Parameters:
    - lr_pos: [0, 1] Learning rate for positive prediction errors.
    - lr_neg: [0, 1] Learning rate for negative prediction errors.
    - beta: [0, 10] Inverse temperature.
    - perseverance: [0, 5] Bonus for repeating the previous Stage 1 choice.
    """
    lr_pos, lr_neg, beta, perseverance = model_parameters
    n_trials = len(action_1)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_mf = np.zeros(2)        # Stage 1 MF values
    q_stage2 = np.zeros((2, 2)) # Stage 2 values
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        q_net = q_mf.copy()
        
        # Apply Perseverance Bonus
        if last_action_1 != -1:
            q_net[last_action_1] += perseverance
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_action_1 = action_1[trial]
        
        # --- Stage 2 Decision ---
        s_idx = state[trial]
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning ---
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]
        
        # Stage 2 Update
        delta2 = r - q_stage2[s_idx, a2]
        lr_2 = lr_pos if delta2 > 0 else lr_neg
        q_stage2[s_idx, a2] += lr_2 * delta2
        
        # Stage 1 Update (Direct Reinforcement from Reward, TD(1))
        # We model this as updating Q_MF towards the Reward directly
        delta_total = r - q_mf[a1]
        lr_1 = lr_pos if delta_total > 0 else lr_neg
        q_mf[a1] += lr_1 * delta_total

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Learner with Win-Stay Perseverance
This model modifies the perseverance mechanism. Instead of a constant "stickiness" regardless of outcome, this model posits that the participant only perseveres (receives a choice bonus) if the previous trial resulted in a **win** (Reward = 1). If the previous trial was a loss, no perseverance bonus is applied, allowing the underlying value-based learning to drive switching.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid learner with Outcome-Dependent Perseverance (Win-Stay).
    
    Parameters:
    - learning_rate: [0, 1] Update rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weighting (0=MF, 1=MB).
    - pers_win: [0, 5] Perseverance bonus applied only if the previous trial was rewarded.
    """
    learning_rate, beta, w, pers_win = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_mf = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    last_action_1 = -1
    last_reward = 0
    
    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_mb + (1 - w) * q_mf
        
        # Apply Perseverance only if last trial was a Win
        if last_action_1 != -1 and last_reward == 1:
            q_net[last_action_1] += pers_win
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_action_1 = action_1[trial]
        
        # --- Stage 2 Decision ---
        s_idx = state[trial]
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning ---
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]
        last_reward = r
        
        delta1 = q_stage2[s_idx, a2] - q_mf[a1]
        q_mf[a1] += learning_rate * delta1
        
        delta2 = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += learning_rate * delta2
        q_mf[a1] += learning_rate * delta2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```