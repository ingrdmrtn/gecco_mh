Here are the three proposed cognitive models.

### Model 1: Dual Beta Hybrid Model with Decay and Perseveration
This model hypothesizes that the participant exhibits different levels of exploration/exploitation noise in Stage 1 (choosing a spaceship) versus Stage 2 (choosing an alien). It separates the inverse temperature parameter into `beta1` and `beta2`, while retaining the successful decay and perseveration mechanisms.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Dual Beta Hybrid MB/MF model with Value Decay and Perseveration.
    Distinguishes decision noise between Stage 1 (spaceship choice) and Stage 2 (alien choice).
    
    Parameters:
    lr (float): Learning rate [0,1].
    beta1 (float): Inverse temperature for Stage 1 [0,10].
    beta2 (float): Inverse temperature for Stage 2 [0,10].
    w (float): MB/MF Weighting [0,1].
    decay (float): Decay rate for Q-values per trial [0,1].
    pers (float): Perseveration bonus for repeating the previous stage 1 choice [0,10].
    """
    lr, beta1, beta2, w, decay, pers = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_a1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue

        # Decay Q-values
        q_stage1_mf *= (1 - decay)
        q_stage2_mf *= (1 - decay)

        # Stage 1 Policy (Model-Based + Model-Free)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf

        logits_1 = beta1 * q_net_stage1
        if prev_a1 != -1:
            logits_1[prev_a1] += pers

        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # Stage 2 Policy
        logits_2 = beta2 * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # Updates
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr * delta_stage1

        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr * delta_stage2

        # Eligibility trace update for Stage 1 using Stage 2 RPE
        q_stage1_mf[action_1[trial]] += lr * delta_stage2
        
        prev_a1 = action_1[trial]

    eps = 1e-10
    valid_mask = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss
```

### Model 2: Asymmetric Learning Rate Hybrid Model with Decay
This model incorporates "asymmetric learning" where the participant updates values differently based on whether the prediction error is positive (better than expected) or negative (worse than expected). This is combined with value decay and perseveration to capture the full behavioral dynamic.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Asymmetric Learning Rate Hybrid MB/MF model with Value Decay and Perseveration.
    Applies different learning rates for positive and negative prediction errors.
    
    Parameters:
    lr_pos (float): Learning rate for positive prediction errors [0,1].
    lr_neg (float): Learning rate for negative prediction errors [0,1].
    beta (float): Inverse temperature [0,10].
    w (float): MB/MF Weighting [0,1].
    decay (float): Decay rate for Q-values per trial [0,1].
    pers (float): Perseveration bonus for repeating the previous stage 1 choice [0,10].
    """
    lr_pos, lr_neg, beta, w, decay, pers = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_a1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue

        # Decay
        q_stage1_mf *= (1 - decay)
        q_stage2_mf *= (1 - decay)

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf

        logits_1 = beta * q_net_stage1
        if prev_a1 != -1:
            logits_1[prev_a1] += pers

        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # Stage 2 Policy
        logits_2 = beta * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # Updates with Asymmetric Learning Rates
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        current_lr_s1 = lr_pos if delta_stage1 > 0 else lr_neg
        q_stage1_mf[action_1[trial]] += current_lr_s1 * delta_stage1

        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        current_lr_s2 = lr_pos if delta_stage2 > 0 else lr_neg
        q_stage2_mf[state_idx, action_2[trial]] += current_lr_s2 * delta_stage2

        # Eligibility trace update for Stage 1 (using Stage 2 RPE)
        q_stage1_mf[action_1[trial]] += current_lr_s2 * delta_stage2
        
        prev_a1 = action_1[trial]

    eps = 1e-10
    valid_mask = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss
```

### Model 3: Subjective Transition Hybrid Model with Decay
This model relaxes the assumption that the participant perfectly understands the 70/30 transition probabilities. It introduces a `trans_prob` parameter, allowing the model-based system to operate on a "subjective" belief about the spaceship reliability (e.g., believing the transition is 90% reliable or only 50% reliable).

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Subjective Transition Hybrid MB/MF model with Value Decay and Perseveration.
    Allows the model-based component to use a subjective transition probability 
    instead of the fixed objective one.
    
    Parameters:
    lr (float): Learning rate [0,1].
    beta (float): Inverse temperature [0,10].
    w (float): MB/MF Weighting [0,1].
    decay (float): Decay rate for Q-values per trial [0,1].
    pers (float): Perseveration bonus for repeating the previous stage 1 choice [0,10].
    trans_prob (float): Subjective probability of common transition [0,1].
    """
    lr, beta, w, decay, pers, trans_prob = model_parameters
    n_trials = len(action_1)
  
    # Subjective transition matrix based on parameter
    transition_matrix = np.array([[trans_prob, 1-trans_prob], [1-trans_prob, trans_prob]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_a1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue

        # Decay
        q_stage1_mf *= (1 - decay)
        q_stage2_mf *= (1 - decay)

        # Stage 1 Policy (using Subjective Transition Matrix)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf

        logits_1 = beta * q_net_stage1
        if prev_a1 != -1:
            logits_1[prev_a1] += pers

        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # Stage 2 Policy
        logits_2 = beta * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # Updates
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr * delta_stage1

        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr * delta_stage2

        # Eligibility trace update
        q_stage1_mf[action_1[trial]] += lr * delta_stage2
        
        prev_a1 = action_1[trial]

    eps = 1e-10
    valid_mask = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss
```