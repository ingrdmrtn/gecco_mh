Here are three new cognitive models based on the participant data and the provided template structure.

### Model 1: Hybrid Model with Forgetting
This model extends the "best model so far" (Forgetting Q-learning) by integrating a Model-Based (MB) component. It tests whether the participant combines structural knowledge (MB) with decaying Model-Free (MF) values. In this model, the Q-values for unchosen actions decay over time, capturing the transient nature of the participant's learned values in a changing environment.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Model 1: Hybrid Model with Forgetting.
    
    Combines Model-Based (MB) and Model-Free (MF) learning mechanisms.
    Incorporates a 'forgetting' parameter that decays the Q-values of unchosen 
    actions and unvisited states, allowing the model to adapt to changing 
    reward probabilities by reducing the influence of old information.

    Parameters:
    - learning_rate: [0, 1] Learning rate for Q-value updates.
    - beta: [0, 10] Inverse temperature (softness of choice).
    - w: [0, 1] Weighting parameter (0 = Pure MF, 1 = Pure MB).
    - forget_rate: [0, 1] Decay rate for unchosen Q-values (0 = no forgetting).
    """
    learning_rate, beta, w, forget_rate = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based Value: Expected value of next stage (max Q2) weighted by transitions
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value: Weighted mixture of MB and MF
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax choice
        logits_1 = beta * q_net_stage1
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Policy ---
        state_idx = state[trial]
        logits_2 = beta * q_stage2_mf[state_idx]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        r = reward[trial]
        a1 = action_1[trial]
        a2 = action_2[trial]
        
        # Stage 2 Update (TD-0)
        q_stage2_mf[state_idx, a2] += learning_rate * (r - q_stage2_mf[state_idx, a2])
        
        # Forgetting for Stage 2 (decay unchosen option in the current state)
        unchosen_a2 = 1 - a2
        q_stage2_mf[state_idx, unchosen_a2] *= (1.0 - forget_rate)
        
        # Stage 1 Update (TD-1 / SARSA-like update towards Stage 2 value)
        target_val = q_stage2_mf[state_idx, a2]
        q_stage1_mf[a1] += learning_rate * (target_val - q_stage1_mf[a1])
        
        # Forgetting for Stage 1 (decay unchosen option)
        unchosen_a1 = 1 - a1
        q_stage1_mf[unchosen_a1] *= (1.0 - forget_rate)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model with Separate Stage Inverse Temperatures
The participant might exhibit different levels of exploration or decision noise in the abstract first stage (choosing a spaceship) versus the concrete second stage (choosing an alien). This model introduces separate inverse temperature parameters (`beta_1` and `beta_2`) for each stage within a Hybrid framework.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model 2: Hybrid Model with Separate Stage Inverse Temperatures.
    
    A Hybrid model that allows for different levels of decision noise (exploration)
    in Stage 1 and Stage 2. This accounts for potential differences in task 
    difficulty or strategy between the spaceship choice and the alien choice.

    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta_1: [0, 10] Inverse temperature for Stage 1 (Spaceship choice).
    - beta_2: [0, 10] Inverse temperature for Stage 2 (Alien choice).
    - w: [0, 1] Weighting between MB and MF.
    """
    learning_rate, beta_1, beta_2, w = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy (using beta_1) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta_1 * q_net_stage1
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Policy (using beta_2) ---
        state_idx = state[trial]
        logits_2 = beta_2 * q_stage2_mf[state_idx]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        r = reward[trial]
        a1 = action_1[trial]
        a2 = action_2[trial]
        
        # Update Stage 2
        q_stage2_mf[state_idx, a2] += learning_rate * (r - q_stage2_mf[state_idx, a2])
        
        # Update Stage 1
        target_val = q_stage2_mf[state_idx, a2]
        q_stage1_mf[a1] += learning_rate * (target_val - q_stage1_mf[a1])

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model with Win-Stay Lose-Shift Stickiness
This model investigates whether the participant's "stickiness" (tendency to repeat a choice) is outcome-dependent. In a Hybrid framework, it adds a bonus to the previous choice if it was rewarded (`stick_win`) and a different bonus (or lack thereof) if it was unrewarded (`stick_loss`). This captures "Win-Stay, Lose-Shift" dynamics explicitly.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model 3: Hybrid Model with Win-Stay Lose-Shift Stickiness.
    
    Incorporates outcome-dependent perseveration in Stage 1. The model applies
    different stickiness bonuses to the previously chosen spaceship depending on 
    whether the previous trial resulted in a reward (stick_win) or not (stick_loss).

    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weighting between MB and MF.
    - stick_win: [0, 10] Stickiness bonus after a reward.
    - stick_loss: [0, 10] Stickiness bonus after no reward (loss).
    """
    learning_rate, beta, w, stick_win, stick_loss = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_a1 = -1
    prev_reward = 0

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net_stage1
        
        # Apply Outcome-Dependent Stickiness
        if prev_a1 != -1:
            if prev_reward == 1:
                logits_1[prev_a1] += stick_win
            else:
                logits_1[prev_a1] += stick_loss
        
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Policy ---
        state_idx = state[trial]
        logits_2 = beta * q_stage2_mf[state_idx]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        r = reward[trial]
        a1 = action_1[trial]
        a2 = action_2[trial]
        
        # Update Stage 2
        q_stage2_mf[state_idx, a2] += learning_rate * (r - q_stage2_mf[state_idx, a2])
        
        # Update Stage 1
        target_val = q_stage2_mf[state_idx, a2]
        q_stage1_mf[a1] += learning_rate * (target_val - q_stage1_mf[a1])
        
        prev_a1 = a1
        prev_reward = r

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```