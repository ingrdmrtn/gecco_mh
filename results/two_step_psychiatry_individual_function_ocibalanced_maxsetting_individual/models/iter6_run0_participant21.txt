Here are 3 new cognitive models based on the participant data and the two-step task structure.

### Model 1: Risk-Sensitive Hybrid Model
This model incorporates risk sensitivity into the decision-making process. The participant's behavior, characterized by long streaks (perseveration) and sudden switches, might be explained by tracking not just the expected value ($Q$) of options, but also their variance (uncertainty/risk). The agent combines mean value and variance into a utility function. A risk-seeking agent might stick with a high-variance option hoping for a payout, while a risk-averse agent might avoid it.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Risk-Sensitive Hybrid Model.
    
    In addition to tracking Q-values, the agent tracks the variance (risk) 
    of the rewards associated with the second-stage aliens. 
    The Stage 1 decision is based on a utility function: U = Q + w_risk * sqrt(Variance).
    
    Parameters:
    learning_rate: [0, 1] Learning rate for Q-values and Variance.
    beta_1: [0, 10] Inverse temperature for Stage 1.
    beta_2: [0, 10] Inverse temperature for Stage 2.
    w: [0, 1] Weight for Model-Based control (0=MF, 1=MB).
    stickiness: [0, 5] Choice stickiness for Stage 1.
    risk_weight: [0, 10] Weight for risk. Values < 5 imply risk aversion, > 5 imply risk seeking.
                         (Implemented as: weight - 5).
    """
    learning_rate, beta_1, beta_2, w, stickiness, risk_weight = model_parameters
    n_trials = len(action_1)
    
    # Map risk_weight from [0, 10] to [-5, 5] centered at 0
    risk_param = risk_weight - 5.0

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # 2 planets, 2 aliens
    var_stage2 = np.zeros((2, 2))  # Variance tracking for aliens

    last_action_1 = -1

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        # Calculate Utility for Stage 2 options: U = Q + risk * std_dev
        # We use these Utilities for the Model-Based calculation
        stage2_std = np.sqrt(var_stage2)
        stage2_utility = q_stage2_mf + risk_param * stage2_std
        
        max_utility_stage2 = np.max(stage2_utility, axis=1)
        
        # Model-Based Value (based on Utility)
        q_stage1_mb = transition_matrix @ max_utility_stage2
        
        # Net Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness

        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        # Standard Softmax on Q-values (risk is usually a planning factor, 
        # but here we assume immediate choice is value-driven or utility-driven. 
        # We'll use simple value for local choice to keep it standard).
        qs_current = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta_2 * qs_current)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # Update Stage 1 MF
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Update Stage 2 MF (Value)
        prediction_error = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * prediction_error
        
        # Update Stage 2 Variance
        # Var_new = (1-lr)*Var_old + lr*(Reward - Q_old)^2
        # Note: We use the Q value *before* the update for the error calc in variance
        sq_error = prediction_error ** 2
        var_stage2[state_idx, action_2[trial]] += learning_rate * (sq_error - var_stage2[state_idx, action_2[trial]])
        
        last_action_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Transition-Dependent Learning Rates
This model addresses the "Credit Assignment" problem by using different learning rates for the Model-Free update depending on whether the transition was Common or Rare. Participants often struggle to correctly update Stage 1 values after a Rare transition (e.g., ignoring the result or treating it as if it came from the common path). This model allows the agent to learn aggressively from Common transitions (`lr_common`) while potentially discounting or learning differently from Rare transitions (`lr_rare`).

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Split Learning Rate Model (Common vs. Rare).
    
    The agent uses different learning rates to update the Stage 1 Model-Free values
    depending on whether the transition experienced was Common or Rare.
    
    Parameters:
    lr_common: [0, 1] Learning rate for Stage 1 update after a Common transition.
    lr_rare: [0, 1] Learning rate for Stage 1 update after a Rare transition.
    beta_1: [0, 10] Inverse temperature for Stage 1.
    beta_2: [0, 10] Inverse temperature for Stage 2.
    w: [0, 1] Weight for Model-Based control.
    stickiness: [0, 5] Choice stickiness for Stage 1.
    """
    lr_common, lr_rare, beta_1, beta_2, w, stickiness = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    last_action_1 = -1

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        chosen_a1 = action_1[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # Determine if transition was Common or Rare
        # Common: (A0->S0) or (A1->S1). Rare: (A0->S1) or (A1->S0).
        is_common = (chosen_a1 == state_idx)
        
        current_lr_stage1 = lr_common if is_common else lr_rare
        
        # Stage 1 Update (using split LR)
        # Note: We use the max of stage 2 for SARSA/Q-learning hybrid, 
        # but strictly here we use the chosen value (SARSA-like) or best (Q-learning).
        # Template implies simple TD. We use value of chosen stage 2 option.
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[chosen_a1]
        q_stage1_mf[chosen_a1] += current_lr_stage1 * delta_stage1
        
        # Stage 2 Update (Standard, using lr_common as base or average? 
        # Usually Stage 2 is just bandit learning. We will use lr_common for simplicity 
        # or we could add a 7th param, but we stick to 6. Let's use lr_common).
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr_common * delta_stage2
        
        last_action_1 = chosen_a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Prospect Theory Probability Distortion
This model modifies the Model-Based component. Instead of using the objective transition probabilities (0.7 / 0.3), the agent distorts these probabilities using a one-parameter weighting function derived from Prospect Theory. This explains behaviors where participants might overweight the rare transition (treating it as more likely than it is) or underweight it (ignoring it entirely), affecting their Model-Based planning.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Prospect Theory Probability Distortion Model.
    
    The agent distorts the objective transition probabilities in the Model-Based 
    calculation using a probability weighting function: w(p) = p^gamma / (p^gamma + (1-p)^gamma)^(1/gamma).
    
    Parameters:
    learning_rate: [0, 1] Learning rate.
    beta_1: [0, 10] Inverse temperature for Stage 1.
    beta_2: [0, 10] Inverse temperature for Stage 2.
    w: [0, 1] Weight for Model-Based control.
    stickiness: [0, 5] Choice stickiness.
    gamma: [0, 5] Distortion parameter. gamma < 1 overweights low probs (rare events).
                  gamma = 1 is linear (objective). gamma > 1 underweights low probs.
    """
    learning_rate, beta_1, beta_2, w, stickiness, gamma = model_parameters
    n_trials = len(action_1)
  
    # Objective transition matrix
    obj_trans = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Apply distortion
    # Avoid division by zero or overflow
    gamma = max(gamma, 0.01) 
    
    def distort(p, g):
        return (p**g) / ((p**g + (1-p)**g)**(1/g))
        
    # Calculate subjective transition matrix
    subj_trans = distort(obj_trans, gamma)
    
    # Re-normalize rows to ensure they sum to 1 (agent must believe probs sum to 1)
    row_sums = subj_trans.sum(axis=1, keepdims=True)
    subj_trans = subj_trans / row_sums

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    last_action_1 = -1

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        
        # Use Subjective Transition Matrix for Planning
        q_stage1_mb = subj_trans @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        last_action_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```