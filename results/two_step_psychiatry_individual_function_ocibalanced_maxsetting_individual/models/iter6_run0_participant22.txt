Here are three new cognitive models for the two-step task, designed based on the participant's behavior and the provided constraints.

### Model 1: Hybrid Model with Subjective Transition Belief
This model introduces a `trans_belief` parameter. While the true transition probabilities are fixed (0.7/0.3), participants may subjectively estimate this probability differently (e.g., believing the transitions are more deterministic or more random than they are). This parameter modulates the Model-Based value calculation, allowing the agent's internal model of the task structure to deviate from the objective reality.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Subjective Transition Belief.
    
    Incorporates a 'trans_belief' parameter representing the participant's 
    subjective probability of the common transition. This replaces the fixed 
    0.7/0.3 matrix in the Model-Based calculation, allowing for under- or 
    over-estimation of transition reliability.
    
    Parameters:
    lr: [0,1] - Learning rate for prediction errors.
    beta_1: [0,10] - Inverse temperature for Stage 1.
    beta_2: [0,10] - Inverse temperature for Stage 2.
    w: [0,1] - MB/MF weighting (0=MF, 1=MB).
    stick: [0,5] - Choice stickiness for Stage 1.
    decay: [0,1] - Decay rate for unchosen Q-values (forgetting).
    trans_belief: [0,1] - Subjective probability of common transition (A->X, U->Y).
    """
    lr, beta_1, beta_2, w, stick, decay, trans_belief = model_parameters
    n_trials = len(action_1)
  
    # Subjective transition matrix based on parameter
    # Row 0: Space A -> [Planet X, Planet Y]
    # Row 1: Space U -> [Planet X, Planet Y]
    transition_matrix = np.array([[trans_belief, 1.0 - trans_belief], 
                                  [1.0 - trans_belief, trans_belief]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1

    for trial in range(n_trials):
        # Skip missing data
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
            continue

        # --- Stage 1 Policy ---
        # Model-Based Value: Expected max value of next stage using subjective transitions
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta_1 * q_net
        if prev_action_1 != -1:
            logits_1[prev_action_1] += stick
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        # --- Stage 2 Policy ---
        state_idx = state[trial]
        logits_2 = beta_2 * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        # Stage 1 MF Update (TD-0)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr * delta_stage1
        
        # Decay unchosen Stage 1
        q_stage1_mf[1 - action_1[trial]] *= (1 - decay)

        # Stage 2 MF Update (Reward)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr * delta_stage2

        # Decay unchosen Stage 2
        q_stage2_mf[state_idx, 1 - action_2[trial]] *= (1 - decay)
        
        prev_action_1 = action_1[trial]

    eps = 1e-10
    valid_trials = (p_choice_1 > 0) & (p_choice_2 > 0)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```

### Model 2: Hybrid Model with Dual-Stage Stickiness and Decay
The participant data shows distinct perseveration not just in spaceship choice (Stage 1) but also in alien choice (Stage 2). This model includes separate stickiness parameters for each stage (`stick_1` and `stick_2`) combined with value decay. This allows the model to capture the tendency to repeat specific aliens within a planet, independent of the learning process.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Dual-Stage Stickiness and Decay.
    
    Includes separate stickiness parameters for Stage 1 (Spaceship) and 
    Stage 2 (Alien). Stage 2 stickiness tracks the previous choice made 
    in that specific state (planet).
    
    Parameters:
    lr: [0,1] - Learning rate.
    beta_1: [0,10] - Inverse temperature for Stage 1.
    beta_2: [0,10] - Inverse temperature for Stage 2.
    w: [0,1] - MB/MF weighting.
    stick_1: [0,5] - Choice stickiness for Stage 1 (Spaceship).
    stick_2: [0,5] - Choice stickiness for Stage 2 (Alien, per planet).
    decay: [0,1] - Decay rate for unchosen Q-values.
    """
    lr, beta_1, beta_2, w, stick_1, stick_2, decay = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1
    # Track previous action for each state (planet) for Stage 2 stickiness
    prev_action_2_per_state = np.array([-1, -1]) 

    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
            continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta_1 * q_net
        if prev_action_1 != -1:
            logits_1[prev_action_1] += stick_1
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        # --- Stage 2 Policy ---
        state_idx = state[trial]
        logits_2 = beta_2 * q_stage2_mf[state_idx]
        
        # Apply Stage 2 stickiness if we have visited this state before
        if prev_action_2_per_state[state_idx] != -1:
            logits_2[prev_action_2_per_state[state_idx]] += stick_2
            
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr * delta_stage1
        q_stage1_mf[1 - action_1[trial]] *= (1 - decay)

        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr * delta_stage2
        q_stage2_mf[state_idx, 1 - action_2[trial]] *= (1 - decay)
        
        prev_action_1 = action_1[trial]
        prev_action_2_per_state[state_idx] = action_2[trial]

    eps = 1e-10
    valid_trials = (p_choice_1 > 0) & (p_choice_2 > 0)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```

### Model 3: Hybrid Model with Decay to Equilibrium Bias
Standard decay pushes unchosen values to 0. However, participants might revert to a "neutral" or "optimistic" prior expectation rather than assuming an unchosen option is worthless. This model decays unchosen Q-values towards a learned parameter `q_bias` (e.g., 0.5 for chance level, or higher for optimism), effectively implementing a "return to prior" mechanism.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Decay to Equilibrium Bias.
    
    Instead of decaying unchosen values to 0, this model decays them 
    towards a 'q_bias' parameter. This allows the agent to maintain 
    optimism (or pessimism) about unvisited states, preventing permanent 
    avoidance of options due to passive forgetting.
    
    Parameters:
    lr: [0,1] - Learning rate.
    beta_1: [0,10] - Inverse temperature for Stage 1.
    beta_2: [0,10] - Inverse temperature for Stage 2.
    w: [0,1] - MB/MF weighting.
    stick: [0,5] - Choice stickiness for Stage 1.
    decay: [0,1] - Rate of reversion to q_bias.
    q_bias: [0,1] - The equilibrium value unchosen options decay towards.
    """
    lr, beta_1, beta_2, w, stick, decay, q_bias = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
            continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta_1 * q_net
        if prev_action_1 != -1:
            logits_1[prev_action_1] += stick
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        # --- Stage 2 Policy ---
        state_idx = state[trial]
        logits_2 = beta_2 * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        # Stage 1 MF Update
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr * delta_stage1
        
        # Decay unchosen Stage 1 towards q_bias
        # Formula: Q = Q * (1 - decay) + Target * decay
        unchosen_1 = 1 - action_1[trial]
        q_stage1_mf[unchosen_1] = q_stage1_mf[unchosen_1] * (1 - decay) + q_bias * decay

        # Stage 2 MF Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr * delta_stage2

        # Decay unchosen Stage 2 towards q_bias
        unchosen_2 = 1 - action_2[trial]
        q_stage2_mf[state_idx, unchosen_2] = q_stage2_mf[state_idx, unchosen_2] * (1 - decay) + q_bias * decay
        
        prev_action_1 = action_1[trial]

    eps = 1e-10
    valid_trials = (p_choice_1 > 0) & (p_choice_2 > 0)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```