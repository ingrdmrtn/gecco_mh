Here are three new cognitive models for the two-step task. These models explore different mechanisms—valence-based learning asymmetry, structural (model-based) planning, and stage-specific learning rates—while retaining the outcome-dependent stickiness mechanism that was successful in previous iterations.

### Model 1: Asymmetric Model-Free Q-Learning with Outcome-Dependent Stickiness
This model extends the previous best Model-Free approach by allowing for different learning rates for positive and negative prediction errors ("valence"). This captures the possibility that the participant learns differently from rewards (confirmation) versus lack of rewards (disappointment), combined with their tendency to repeat choices based on the previous outcome.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Asymmetric Model-Free Q-Learning with Outcome-Dependent Stickiness.
    
    This model distinguishes between learning from positive prediction errors 
    (better than expected) and negative prediction errors (worse than expected).
    It retains outcome-dependent stickiness to capture perseverance.
    
    Parameters:
    lr_pos:    [0, 1] Learning rate for positive prediction errors.
    lr_neg:    [0, 1] Learning rate for negative prediction errors.
    beta:      [0, 10] Inverse temperature (exploration/exploitation).
    stick_win: [-5, 5] Stickiness bonus after a rewarded trial.
    stick_loss:[-5, 5] Stickiness bonus after an unrewarded trial.
    """
    lr_pos, lr_neg, beta, stick_win, stick_loss = model_parameters
    n_trials = len(action_1)

    # Q-values initialization
    q_stage1 = np.zeros(2) + 0.5
    q_stage2 = np.zeros((2, 2)) + 0.5
    
    log_loss = 0
    eps = 1e-10
    
    prev_action_1 = -1
    prev_reward = -1
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])

        # Handle missing data
        if a1 == -1 or s_idx == -1 or a2 == -1:
            prev_action_1 = -1
            continue

        # --- Stage 1 Choice ---
        logits_1 = beta * q_stage1
        
        # Apply outcome-dependent stickiness
        if prev_action_1 != -1:
            if prev_reward == 1:
                logits_1[prev_action_1] += stick_win
            else:
                logits_1[prev_action_1] += stick_loss
        
        # Softmax for Stage 1
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + eps)

        # --- Stage 2 Choice ---
        logits_2 = beta * q_stage2[s_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[a2] + eps)

        # --- Updates ---
        # Stage 1 Update (TD)
        target_stage1 = q_stage2[s_idx, a2]
        delta_1 = target_stage1 - q_stage1[a1]
        if delta_1 > 0:
            q_stage1[a1] += lr_pos * delta_1
        else:
            q_stage1[a1] += lr_neg * delta_1

        # Stage 2 Update (Reward)
        delta_2 = r - q_stage2[s_idx, a2]
        if delta_2 > 0:
            q_stage2[s_idx, a2] += lr_pos * delta_2
        else:
            q_stage2[s_idx, a2] += lr_neg * delta_2
        
        prev_action_1 = a1
        prev_reward = r
        
    return log_loss
```

### Model 2: Pure Model-Based with Outcome-Dependent Stickiness
This model tests the hypothesis that the participant relies on the known structure of the task (the transition matrix) to make decisions at Stage 1, rather than learning Stage 1 values via trial and error. Stickiness is added to account for the behavioral inertia and win-stay/lose-shift patterns observed, which pure Model-Based learning often fails to capture on its own.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Based Reinforcement Learning with Outcome-Dependent Stickiness.
    
    This model assumes the participant uses the fixed transition matrix to 
    calculate the value of Stage 1 choices based on Stage 2 values (planning),
    rather than caching Stage 1 values (Model-Free).
    
    Parameters:
    learning_rate: [0, 1] Update rate for Stage 2 Q-values.
    beta:          [0, 10] Inverse temperature.
    stick_win:     [-5, 5] Stickiness bonus after a rewarded trial.
    stick_loss:    [-5, 5] Stickiness bonus after an unrewarded trial.
    """
    learning_rate, beta, stick_win, stick_loss = model_parameters
    n_trials = len(action_1)

    # Transition matrix: Row 0 (Act 0) -> [0.7, 0.3], Row 1 (Act 1) -> [0.3, 0.7]
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Only Stage 2 Q-values are learned directly
    q_stage2 = np.zeros((2, 2)) + 0.5
    
    log_loss = 0
    eps = 1e-10
    
    prev_action_1 = -1
    prev_reward = -1
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])

        if a1 == -1 or s_idx == -1 or a2 == -1:
            prev_action_1 = -1
            continue

        # --- Stage 1 Choice (Model-Based) ---
        # Calculate MB values: expected max value of next stage
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        logits_1 = beta * q_stage1_mb
        
        # Apply outcome-dependent stickiness
        if prev_action_1 != -1:
            if prev_reward == 1:
                logits_1[prev_action_1] += stick_win
            else:
                logits_1[prev_action_1] += stick_loss
        
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + eps)

        # --- Stage 2 Choice ---
        logits_2 = beta * q_stage2[s_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[a2] + eps)

        # --- Updates ---
        # Only Stage 2 is updated via prediction error
        q_stage2[s_idx, a2] += learning_rate * (r - q_stage2[s_idx, a2])
        
        prev_action_1 = a1
        prev_reward = r
        
    return log_loss
```

### Model 3: Split-Stage Model-Free Q-Learning with Stickiness
This model assumes that learning dynamics differ between the spaceship choice (Stage 1) and the alien choice (Stage 2). It assigns separate learning rates to each stage. This allows the model to capture scenarios where the participant might rapidly change preferences for aliens (high Stage 2 LR) but be slow to change spaceship preferences (low Stage 1 LR), or vice versa.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Split-Stage Model-Free Q-Learning with Outcome-Dependent Stickiness.
    
    This model applies different learning rates to Stage 1 (spaceships) and 
    Stage 2 (aliens), reflecting potentially different plasticity in 
    learning transition values vs. reward values.
    
    Parameters:
    lr_s1:      [0, 1] Learning rate for Stage 1 (Spaceship choice).
    lr_s2:      [0, 1] Learning rate for Stage 2 (Alien choice).
    beta:       [0, 10] Inverse temperature.
    stick_win:  [-5, 5] Stickiness bonus after a rewarded trial.
    stick_loss: [-5, 5] Stickiness bonus after an unrewarded trial.
    """
    lr_s1, lr_s2, beta, stick_win, stick_loss = model_parameters
    n_trials = len(action_1)

    q_stage1 = np.zeros(2) + 0.5
    q_stage2 = np.zeros((2, 2)) + 0.5
    
    log_loss = 0
    eps = 1e-10
    
    prev_action_1 = -1
    prev_reward = -1
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])

        if a1 == -1 or s_idx == -1 or a2 == -1:
            prev_action_1 = -1
            continue

        # --- Stage 1 Choice ---
        logits_1 = beta * q_stage1
        
        if prev_action_1 != -1:
            if prev_reward == 1:
                logits_1[prev_action_1] += stick_win
            else:
                logits_1[prev_action_1] += stick_loss
        
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + eps)

        # --- Stage 2 Choice ---
        logits_2 = beta * q_stage2[s_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[a2] + eps)

        # --- Updates ---
        # Stage 1 Update with lr_s1
        target_stage1 = q_stage2[s_idx, a2]
        q_stage1[a1] += lr_s1 * (target_stage1 - q_stage1[a1])

        # Stage 2 Update with lr_s2
        q_stage2[s_idx, a2] += lr_s2 * (r - q_stage2[s_idx, a2])
        
        prev_action_1 = a1
        prev_reward = r
        
    return log_loss
```