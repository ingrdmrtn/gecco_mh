Here are three new cognitive models for the two-step decision task.

### Model 1: Hybrid Learner with Dynamic Transition Learning and Separate Habit Learning Rate
This model extends the hybrid framework by incorporating dynamic transition learning (as in the best-performing model) but separates the learning process for Stage 1 habits (Model-Free) from Stage 2 value estimation. This decoupling allows the model to capture scenarios where habit formation (`lr_hab`) occurs at a different timescale than the learning of alien reward probabilities (`lr_val`).

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Model 1: Hybrid Learner with Dynamic Transition Learning and Separate Habit Learning Rate.
    
    Combines Model-Based planning (using learned transitions) with Model-Free habits.
    Crucially, it uses a distinct learning rate for the Stage 1 Model-Free values (habits)
    versus the Stage 2 values (alien rewards), allowing habit formation to decouple from 
    reward value estimation.
    
    Parameters:
    lr_hab:   [0, 1] - Learning rate for Stage 1 Model-Free Q-values (Habit).
    lr_val:   [0, 1] - Learning rate for Stage 2 Q-values (Alien Values).
    lr_trans: [0, 1] - Learning rate for transition probabilities.
    beta:     [0, 10] - Inverse temperature for softmax.
    w:        [0, 1] - Weighting parameter (0=Pure MF, 1=Pure MB).
    pers:     [0, 5] - Perseverance bonus for Stage 1 choice.
    """
    lr_hab, lr_val, lr_trans, beta, w, pers = model_parameters
    n_trials = len(action_1)

    # Initialization
    q_mf1 = np.zeros(2)          # Stage 1 MF values
    q_s2 = np.zeros((2, 2))      # Stage 2 values (used for both MF2 and MB)
    trans_probs = np.ones((2, 2)) * 0.5 # Learned transition matrix
    
    last_action_1 = -1
    log_loss = 0.0
    eps = 1e-10
    
    for trial in range(n_trials):
        # Data extraction
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1 or reward[trial] == -1:
            last_action_1 = -1
            continue
        
        a1 = int(action_1[trial])
        s_next = int(state[trial])
        a2 = int(action_2[trial])
        r = float(reward[trial])

        # --- Stage 1 Decision ---
        # Model-Based Value calculation
        max_q_s2 = np.max(q_s2, axis=1)
        q_mb = trans_probs @ max_q_s2
        
        # Net Value: Mixture of MB and MF1
        q_net = w * q_mb + (1 - w) * q_mf1
        
        # Perseverance
        if last_action_1 != -1:
            q_net[last_action_1] += pers
            
        # Softmax Stage 1
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + eps)
        
        # --- Stage 2 Decision ---
        # Softmax Stage 2
        exp_q2 = np.exp(beta * q_s2[s_next])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[a2] + eps)
        
        # --- Updates ---
        
        # 1. Update Stage 2 Values (Reward Prediction Error)
        delta_2 = r - q_s2[s_next, a2]
        q_s2[s_next, a2] += lr_val * delta_2
        
        # 2. Update Stage 1 MF Values (TD(0) - driven by Stage 2 value)
        # We update the MF value of the chosen spaceship towards the value of the state reached.
        delta_1 = max_q_s2[s_next] - q_mf1[a1]
        q_mf1[a1] += lr_hab * delta_1
        
        # 3. Update Transition Probabilities
        trans_probs[a1, s_next] += lr_trans * (1.0 - trans_probs[a1, s_next])
        trans_probs[a1, 1 - s_next] += lr_trans * (0.0 - trans_probs[a1, 1 - s_next])
        
        last_action_1 = a1
        
    return log_loss
```

### Model 2: Pure Model-Based with Transition Learning and Probability Distortion
This model posits that the participant is a Model-Based learner who learns transition probabilities over time but perceives these probabilities subjectively. It applies a probability weighting function (inspired by Prospect Theory) to the learned transitions before calculating expected values. This can explain behavior where the participant overweights rare transitions or underweights common ones, affecting their spaceship choice.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model 2: Pure Model-Based with Transition Learning and Probability Distortion.
    
    This model assumes the participant learns transitions dynamically but perceives 
    probabilities subjectively according to a distortion function (p^gamma / ...).
    This can explain risk-seeking or risk-averse behavior regarding the transition uncertainty.
    
    Parameters:
    lr_val:   [0, 1] - Learning rate for Stage 2 Q-values.
    lr_trans: [0, 1] - Learning rate for transition probabilities.
    beta:     [0, 10] - Inverse temperature.
    pers:     [0, 5] - Perseverance bonus.
    gamma:    [0.1, 5] - Probability distortion parameter (<1 overweights low probs, >1 underweights).
    """
    lr_val, lr_trans, beta, pers, gamma = model_parameters
    n_trials = len(action_1)

    q_s2 = np.zeros((2, 2))
    trans_probs = np.ones((2, 2)) * 0.5
    
    last_action_1 = -1
    log_loss = 0.0
    eps = 1e-10
    
    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1 or reward[trial] == -1:
            last_action_1 = -1
            continue
            
        a1 = int(action_1[trial])
        s_next = int(state[trial])
        a2 = int(action_2[trial])
        r = float(reward[trial])

        # Distort probabilities
        # p_weighted = p^gamma / (p^gamma + (1-p)^gamma)^(1/gamma)
        p_distorted = np.zeros_like(trans_probs)
        for i in range(2):
            for j in range(2):
                p = trans_probs[i, j]
                # Clamp p for stability
                p = np.clip(p, 1e-5, 1.0 - 1e-5)
                w_p = (p ** gamma) / ((p ** gamma + (1 - p) ** gamma) ** (1 / gamma))
                p_distorted[i, j] = w_p
        
        # Normalize distorted probs row-wise to treat them as valid subjective probabilities
        row_sums = np.sum(p_distorted, axis=1, keepdims=True)
        p_distorted = p_distorted / row_sums

        max_q_s2 = np.max(q_s2, axis=1)
        q_mb = p_distorted @ max_q_s2
        
        if last_action_1 != -1:
            q_mb[last_action_1] += pers

        exp_q1 = np.exp(beta * q_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + eps)

        exp_q2 = np.exp(beta * q_s2[s_next])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[a2] + eps)

        delta_2 = r - q_s2[s_next, a2]
        q_s2[s_next, a2] += lr_val * delta_2

        trans_probs[a1, s_next] += lr_trans * (1.0 - trans_probs[a1, s_next])
        trans_probs[a1, 1 - s_next] += lr_trans * (0.0 - trans_probs[a1, 1 - s_next])
        
        last_action_1 = a1
        
    return log_loss
```

### Model 3: Pure Model-Based with Transition Learning and Planet Perseverance
This model distinguishes between two types of stickiness: "Action Perseverance" (repeating the same spaceship choice) and "Planet Perseverance" (attempting to return to the same planet). Since transitions are probabilistic and learned, the "Planet Perseverance" bonus is added dynamically to actions based on the current estimated probability that they will lead to the previously visited planet.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model 3: Pure Model-Based with Transition Learning and Planet Perseverance.
    
    Distinguishes between repeating a motor action (Spaceship Perseverance) and 
    attempting to return to the same state (Planet Perseverance). The Planet Perseverance
    bonus is applied to actions based on their current estimated probability of leading
    to the previously visited planet.
    
    Parameters:
    lr_val:   [0, 1] - Learning rate for Stage 2 Q-values.
    lr_trans: [0, 1] - Learning rate for transition probabilities.
    beta:     [0, 10] - Inverse temperature.
    pers_a:   [0, 5] - Action Perseverance (stickiness to spaceship).
    pers_p:   [0, 5] - Planet Perseverance (stickiness to planet).
    """
    lr_val, lr_trans, beta, pers_a, pers_p = model_parameters
    n_trials = len(action_1)

    q_s2 = np.zeros((2, 2))
    trans_probs = np.ones((2, 2)) * 0.5
    
    last_action_1 = -1
    last_state = -1
    log_loss = 0.0
    eps = 1e-10
    
    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1 or reward[trial] == -1:
            last_action_1 = -1
            last_state = -1
            continue
            
        a1 = int(action_1[trial])
        s_next = int(state[trial])
        a2 = int(action_2[trial])
        r = float(reward[trial])

        max_q_s2 = np.max(q_s2, axis=1)
        q_mb = trans_probs @ max_q_s2
        
        # Apply Perseverance
        if last_action_1 != -1:
            # Action Perseverance: Bonus to the previously chosen spaceship
            q_mb[last_action_1] += pers_a
            
            # Planet Perseverance: Bonus to actions leading to the previous planet
            if last_state != -1:
                # For each candidate action, add bonus proportional to probability of reaching last_state
                for a_cand in [0, 1]:
                    q_mb[a_cand] += pers_p * trans_probs[a_cand, last_state]

        exp_q1 = np.exp(beta * q_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + eps)

        exp_q2 = np.exp(beta * q_s2[s_next])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[a2] + eps)

        delta_2 = r - q_s2[s_next, a2]
        q_s2[s_next, a2] += lr_val * delta_2

        trans_probs[a1, s_next] += lr_trans * (1.0 - trans_probs[a1, s_next])
        trans_probs[a1, 1 - s_next] += lr_trans * (0.0 - trans_probs[a1, 1 - s_next])
        
        last_action_1 = a1
        last_state = s_next
        
    return log_loss
```