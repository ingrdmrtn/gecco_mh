Here are the 3 new cognitive models based on the participant data and the analysis of potential learning mechanisms.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Separate Learning Rates for Stages.
    
    This model assumes that the participant learns the value of the spaceships (Stage 1)
    and the value of the aliens (Stage 2) at different rates. This captures potential
    differences in plasticity between the structural learning (spaceships) and the 
    bandit task (aliens). It combines Model-Based (MB) and Model-Free (MF) evaluations.

    Parameters:
    - lr_s1: [0, 1] Learning rate for Stage 1 (MF) updates.
    - lr_s2: [0, 1] Learning rate for Stage 2 updates.
    - beta: [0, 10] Inverse temperature for softmax choice policy.
    - w: [0, 1] Weighting parameter (0 = Pure MF, 1 = Pure MB).
    - stick: [0, 10] Choice stickiness (perseveration) for Stage 1.
    """
    lr_s1, lr_s2, beta, w, stick = model_parameters
    n_trials = len(action_1)
    
    # Initialize Q-values
    # Stage 1 MF: 2 actions
    q_mf = np.zeros(2) + 0.5
    # Stage 2: 2 states x 2 actions
    q_s2 = np.zeros((2, 2)) + 0.5
    
    # Transition matrix: T[action, state]
    # 0->0 (0.7), 0->1 (0.3); 1->0 (0.3), 1->1 (0.7)
    trans_mat = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_a1 = -1
    
    for t in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based Value: Expected max value of next stage
        max_q_s2 = np.max(q_s2, axis=1)
        q_mb = trans_mat @ max_q_s2
        
        # Net Value: Weighted sum of MB and MF
        q_net = w * q_mb + (1 - w) * q_mf
        
        # Logits and Stickiness
        logits = beta * q_net
        if last_a1 != -1:
            logits[last_a1] += stick
            
        # Softmax for Stage 1
        logits = logits - np.max(logits)
        exp_q = np.exp(logits)
        probs_1 = exp_q / np.sum(exp_q)
        
        a1 = int(action_1[t])
        p_choice_1[t] = probs_1[a1]
        
        # --- Stage 2 Choice ---
        s_idx = int(state[t])
        
        # Softmax for Stage 2
        logits_2 = beta * q_s2[s_idx, :]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[t])
        p_choice_2[t] = probs_2[a2]
        
        # --- Updates ---
        r = reward[t]
        
        # Store old value for Stage 1 update (TD(0))
        old_val_s2 = q_s2[s_idx, a2]
        
        # Update Stage 2 value
        pe2 = r - old_val_s2
        q_s2[s_idx, a2] += lr_s2 * pe2
        
        # Update Stage 1 MF value
        # Target is the value of the state reached (SARSA-like TD(0))
        pe1 = old_val_s2 - q_mf[a1]
        q_mf[a1] += lr_s1 * pe1
        
        last_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Asymmetric Hybrid Model with Stickiness.
    
    This model extends the standard Hybrid MB/MF model by allowing for asymmetric 
    learning rates based on prediction error sign (positive vs negative). 
    This captures biases such as optimism (learning more from better-than-expected outcomes)
    or pessimism, applied to both the MF and Stage 2 updates.

    Parameters:
    - lr_pos: [0, 1] Learning rate for positive prediction errors.
    - lr_neg: [0, 1] Learning rate for negative prediction errors.
    - beta: [0, 10] Inverse temperature for softmax.
    - w: [0, 1] Weighting parameter (0 = Pure MF, 1 = Pure MB).
    - stick: [0, 10] Choice stickiness (perseveration) for Stage 1.
    """
    lr_pos, lr_neg, beta, w, stick = model_parameters
    n_trials = len(action_1)
    
    q_mf = np.zeros(2) + 0.5
    q_s2 = np.zeros((2, 2)) + 0.5
    trans_mat = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_a1 = -1
    
    for t in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_s2 = np.max(q_s2, axis=1)
        q_mb = trans_mat @ max_q_s2
        
        q_net = w * q_mb + (1 - w) * q_mf
        
        logits = beta * q_net
        if last_a1 != -1:
            logits[last_a1] += stick
            
        logits = logits - np.max(logits)
        exp_q = np.exp(logits)
        probs_1 = exp_q / np.sum(exp_q)
        
        a1 = int(action_1[t])
        p_choice_1[t] = probs_1[a1]
        
        # --- Stage 2 Choice ---
        s_idx = int(state[t])
        logits_2 = beta * q_s2[s_idx, :]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[t])
        p_choice_2[t] = probs_2[a2]
        
        # --- Updates ---
        r = reward[t]
        old_val_s2 = q_s2[s_idx, a2]
        
        # Stage 2 Update (Asymmetric)
        pe2 = r - old_val_s2
        lr2 = lr_pos if pe2 > 0 else lr_neg
        q_s2[s_idx, a2] += lr2 * pe2
        
        # Stage 1 MF Update (Asymmetric)
        pe1 = old_val_s2 - q_mf[a1]
        lr1 = lr_pos if pe1 > 0 else lr_neg
        q_mf[a1] += lr1 * pe1
        
        last_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Stage-Specific Inverse Temperatures.
    
    This model assumes that the level of exploration/noise differs between the 
    spaceship choice (Stage 1) and the alien choice (Stage 2). 
    It allows for different 'beta' parameters for each stage, combined with 
    standard Hybrid learning and stickiness.

    Parameters:
    - lr: [0, 1] Learning rate for value updates.
    - beta1: [0, 10] Inverse temperature for Stage 1 choice.
    - beta2: [0, 10] Inverse temperature for Stage 2 choice.
    - w: [0, 1] Weighting parameter (0 = Pure MF, 1 = Pure MB).
    - stick: [0, 10] Choice stickiness (perseveration) for Stage 1.
    """
    lr, beta1, beta2, w, stick = model_parameters
    n_trials = len(action_1)
    
    q_mf = np.zeros(2) + 0.5
    q_s2 = np.zeros((2, 2)) + 0.5
    trans_mat = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_a1 = -1
    
    for t in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_s2 = np.max(q_s2, axis=1)
        q_mb = trans_mat @ max_q_s2
        
        q_net = w * q_mb + (1 - w) * q_mf
        
        # Use beta1 for Stage 1
        logits = beta1 * q_net
        if last_a1 != -1:
            logits[last_a1] += stick
            
        logits = logits - np.max(logits)
        exp_q = np.exp(logits)
        probs_1 = exp_q / np.sum(exp_q)
        
        a1 = int(action_1[t])
        p_choice_1[t] = probs_1[a1]
        
        # --- Stage 2 Choice ---
        s_idx = int(state[t])
        
        # Use beta2 for Stage 2
        logits_2 = beta2 * q_s2[s_idx, :]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[t])
        p_choice_2[t] = probs_2[a2]
        
        # --- Updates ---
        r = reward[t]
        old_val_s2 = q_s2[s_idx, a2]
        
        # Stage 2 Update
        pe2 = r - old_val_s2
        q_s2[s_idx, a2] += lr * pe2
        
        # Stage 1 MF Update
        pe1 = old_val_s2 - q_mf[a1]
        q_mf[a1] += lr * pe1
        
        last_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```