Here are three new cognitive models for the two-step task.

### Model 1: Model-Based with Goal-Directed Stickiness
This model tests the hypothesis that the participant's "stickiness" is directed towards the *goal state* (the planet) rather than the *action* (the spaceship). If a participant visits Planet X and is rewarded, they may form a goal to return to Planet X, which the Model-Based system translates into an action choice (A or U) based on the transition structure.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Model-Based Learner with Goal-Directed Stickiness.
    
    This model assumes a Model-Based strategy but incorporates 'Goal Stickiness'
    instead of 'Action Stickiness'. Stickiness is applied to the value of the 
    previously visited planet (state) before calculating Stage 1 expected values.
    This captures a desire to return to (or avoid) a specific planet based on 
    the outcome, regardless of which spaceship (action) is required to get there.
    
    Parameters:
    - learning_rate: [0, 1] Update rate for Stage 2 alien values.
    - beta_1: [0, 10] Inverse temperature for Stage 1 choice.
    - beta_2: [0, 10] Inverse temperature for Stage 2 choice.
    - goal_stick_win: [-5, 5] Bias added to the value of the previous planet if rewarded.
    - goal_stick_loss: [-5, 5] Bias added to the value of the previous planet if unrewarded.
    """
    learning_rate, beta_1, beta_2, goal_stick_win, goal_stick_loss = model_parameters
    n_trials = len(action_1)
    
    # Fixed transition matrix as per task structure
    # A(0) -> X(0) 0.7, Y(1) 0.3
    # U(1) -> X(0) 0.3, Y(1) 0.7
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Q-values for Stage 2 (Aliens): 2 Planets x 2 Aliens
    q_stage2 = np.zeros((2, 2))
    
    log_loss = 0.0
    eps = 1e-10
    
    last_planet = -1
    last_reward = -1
    
    for trial in range(n_trials):
        # Skip missing data
        if action_1[trial] == -1:
            continue
            
        # --- Stage 1 Decision ---
        # Get max value of each planet (Model-Based)
        max_q_stage2 = np.max(q_stage2, axis=1) # Shape (2,)
        
        # Apply Goal Stickiness to the values of the planets
        # We copy to avoid modifying the actual learned values
        current_planet_values = max_q_stage2.copy()
        
        if last_planet != -1:
            if last_reward == 1:
                current_planet_values[last_planet] += goal_stick_win
            else:
                current_planet_values[last_planet] += goal_stick_loss
        
        # Compute Q-values for spaceships using transition matrix
        q_stage1 = transition_matrix @ current_planet_values
        
        # Softmax for Stage 1
        exp_q1 = np.exp(beta_1 * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        chosen_a1 = int(action_1[trial])
        log_loss -= np.log(probs_1[chosen_a1] + eps)
        
        # --- Stage 2 Decision ---
        current_state = int(state[trial])
        
        # Softmax for Stage 2
        exp_q2 = np.exp(beta_2 * q_stage2[current_state])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        chosen_a2 = int(action_2[trial])
        log_loss -= np.log(probs_2[chosen_a2] + eps)
        
        # --- Learning ---
        r = reward[trial]
        delta = r - q_stage2[current_state, chosen_a2]
        q_stage2[current_state, chosen_a2] += learning_rate * delta
        
        # Update history
        last_planet = current_state
        last_reward = r
        
    return log_loss
```

### Model 2: Model-Based with Subjective Transition Beliefs
This model investigates if the participant operates with a distorted belief about the transition probabilities (e.g., believing they have more or less control than reality). It combines this with outcome-dependent action stickiness, which was identified as a key feature in previous best models.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Based Learner with Subjective Transition Beliefs and Action Stickiness.
    
    This model assumes the participant uses a Model-Based strategy but may have
    a subjective belief about the transition probabilities (trans_prob) that differs 
    from the true 0.7/0.3 split. It also includes outcome-dependent action stickiness
    to capture perseveration or switching biases on the spaceship choice.
    
    Parameters:
    - learning_rate: [0, 1] Update rate for Stage 2 alien values.
    - beta_1: [0, 10] Inverse temperature for Stage 1 choice.
    - beta_2: [0, 10] Inverse temperature for Stage 2 choice.
    - trans_prob: [0, 1] Subjective probability of common transition (A->X, U->Y).
    - stick_win: [-5, 5] Bias added to previous spaceship if rewarded.
    - stick_loss: [-5, 5] Bias added to previous spaceship if unrewarded.
    """
    learning_rate, beta_1, beta_2, trans_prob, stick_win, stick_loss = model_parameters
    n_trials = len(action_1)
    
    # Subjective transition matrix
    # A(0) -> X(0) with p, Y(1) with 1-p
    transition_matrix = np.array([[trans_prob, 1.0 - trans_prob], 
                                  [1.0 - trans_prob, trans_prob]])
    
    q_stage2 = np.zeros((2, 2))
    
    log_loss = 0.0
    eps = 1e-10
    
    last_action_1 = -1
    last_reward = -1
    
    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_stage1 = transition_matrix @ max_q_stage2
        
        # Apply Action Stickiness
        if last_action_1 != -1:
            if last_reward == 1:
                q_stage1[last_action_1] += stick_win
            else:
                q_stage1[last_action_1] += stick_loss
        
        exp_q1 = np.exp(beta_1 * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        chosen_a1 = int(action_1[trial])
        log_loss -= np.log(probs_1[chosen_a1] + eps)
        
        # --- Stage 2 Decision ---
        current_state = int(state[trial])
        exp_q2 = np.exp(beta_2 * q_stage2[current_state])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        chosen_a2 = int(action_2[trial])
        log_loss -= np.log(probs_2[chosen_a2] + eps)
        
        # --- Learning ---
        r = reward[trial]
        delta = r - q_stage2[current_state, chosen_a2]
        q_stage2[current_state, chosen_a2] += learning_rate * delta
        
        last_action_1 = chosen_a1
        last_reward = r
        
    return log_loss
```

### Model 3: Model-Based with Asymmetric Learning Rates
This model posits that the participant learns differently from positive outcomes (rewards) versus neutral outcomes (omissions), a phenomenon often linked to confirmation bias or differential sensitivity to gains/losses. It integrates this with outcome-dependent stickiness.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Based Learner with Asymmetric Learning Rates and Action Stickiness.
    
    This model allows for different learning rates depending on whether the 
    outcome was a reward (win) or no reward (loss). This captures potential 
    biases in how positive vs negative feedback is integrated. It retains 
    outcome-dependent action stickiness.
    
    Parameters:
    - lr_win: [0, 1] Learning rate when reward is 1.
    - lr_loss: [0, 1] Learning rate when reward is 0.
    - beta_1: [0, 10] Inverse temperature for Stage 1 choice.
    - beta_2: [0, 10] Inverse temperature for Stage 2 choice.
    - stick_win: [-5, 5] Bias added to previous spaceship if rewarded.
    - stick_loss: [-5, 5] Bias added to previous spaceship if unrewarded.
    """
    lr_win, lr_loss, beta_1, beta_2, stick_win, stick_loss = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage2 = np.zeros((2, 2))
    
    log_loss = 0.0
    eps = 1e-10
    
    last_action_1 = -1
    last_reward = -1
    
    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_stage1 = transition_matrix @ max_q_stage2
        
        if last_action_1 != -1:
            if last_reward == 1:
                q_stage1[last_action_1] += stick_win
            else:
                q_stage1[last_action_1] += stick_loss
        
        exp_q1 = np.exp(beta_1 * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        chosen_a1 = int(action_1[trial])
        log_loss -= np.log(probs_1[chosen_a1] + eps)
        
        # --- Stage 2 Decision ---
        current_state = int(state[trial])
        exp_q2 = np.exp(beta_2 * q_stage2[current_state])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        chosen_a2 = int(action_2[trial])
        log_loss -= np.log(probs_2[chosen_a2] + eps)
        
        # --- Learning ---
        r = reward[trial]
        delta = r - q_stage2[current_state, chosen_a2]
        
        # Asymmetric learning rate
        if r == 1:
            q_stage2[current_state, chosen_a2] += lr_win * delta
        else:
            q_stage2[current_state, chosen_a2] += lr_loss * delta
        
        last_action_1 = chosen_a1
        last_reward = r
        
    return log_loss
```