Here are the 3 proposed cognitive models as Python functions.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Unchosen Decay Model.
    
    This model implements a 'use-it-or-lose-it' forgetting mechanism. Unlike global decay 
    where all values deteriorate, here only the Q-values of unchosen options (both unchosen 
    aliens on the current planet and aliens on the unvisited planet) decay towards 0. 
    The chosen option is updated via standard reinforcement learning without decay interference.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for chosen options.
    - beta_1: [0, 10] Inverse temperature for Stage 1 (Spaceships).
    - beta_2: [0, 10] Inverse temperature for Stage 2 (Aliens).
    - w: [0, 1] Weighting parameter (0 = Model-Free, 1 = Model-Based).
    - stickiness: [0, 5] Perseveration bonus for repeating the Stage 1 choice.
    - decay_unchosen: [0, 1] Decay rate applied only to unchosen Stage 2 options.
    """
    learning_rate, beta_1, beta_2, w, stickiness, decay_unchosen = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1 or reward[trial] == -1:
            last_action_1 = -1
            continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Policy ---
        state_idx = state[trial]
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        # 1. Decay Unchosen Stage 2 Values
        # Create a mask for all Stage 2 options
        mask = np.ones((2, 2), dtype=bool)
        # Exclude the chosen option from decay
        mask[state_idx, action_2[trial]] = False
        # Apply decay to unchosen options
        q_stage2_mf[mask] *= (1 - decay_unchosen)
        
        # 2. Update Chosen Stage 2 Value (Standard RL)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # 3. Update Stage 1 Value (Standard TD)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        last_action_1 = action_1[trial]

    eps = 1e-10
    mask = (p_choice_1 > 0) & (p_choice_2 > 0)
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Dual Learning Rates with Global Decay.
    
    This model posits that the participant learns Stage 1 (structure/caching) and 
    Stage 2 (drifting rewards) dynamics at different rates. It incorporates a global 
    decay mechanism to handle the non-stationary reward probabilities of the aliens.
    
    Parameters:
    - lr_stage1: [0, 1] Learning rate for Stage 1 (Spaceships).
    - lr_stage2: [0, 1] Learning rate for Stage 2 (Aliens).
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - w: [0, 1] MB/MF weight.
    - stickiness: [0, 5] Stage 1 choice stickiness.
    - decay: [0, 1] Global decay rate for Stage 2 Q-values.
    """
    lr_stage1, lr_stage2, beta_1, beta_2, w, stickiness, decay = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1 or reward[trial] == -1:
            last_action_1 = -1
            continue

        # Global Decay on Stage 2 values (forgetting old reward info)
        q_stage2_mf *= (1 - decay)

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        # --- Stage 2 Policy ---
        state_idx = state[trial]
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        # Update Stage 1 with lr_stage1
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr_stage1 * delta_stage1
        
        # Update Stage 2 with lr_stage2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr_stage2 * delta_stage2
        
        last_action_1 = action_1[trial]
        
    eps = 1e-10
    mask = (p_choice_1 > 0) & (p_choice_2 > 0)
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Choice Kernel and Value Decay Model.
    
    This model separates habit formation (motor perseveration) from value learning (reward history).
    It replaces simple stickiness with a 'Choice Kernel' that accumulates a trace of past choices 
    and decays independently. It also includes 'Value Decay' for the Q-values to handle reward drift.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for Q-values.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - w: [0, 1] MB/MF weight.
    - ck_decay: [0, 1] Decay rate for the choice kernel (habit memory).
    - ck_weight: [0, 10] Weight of the choice kernel in decision making.
    - val_decay: [0, 1] Decay rate for Stage 2 Q-values (reward memory).
    """
    learning_rate, beta_1, beta_2, w, ck_decay, ck_weight, val_decay = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Choice Kernel for Stage 1 actions
    choice_kernel = np.zeros(2)
    
    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1 or reward[trial] == -1:
            continue

        # Decay Q-values (Value Memory)
        q_stage2_mf *= (1 - val_decay)

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combine MB, MF, and Choice Kernel
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf + ck_weight * choice_kernel
            
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        # --- Stage 2 Policy ---
        state_idx = state[trial]
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        # Update Choice Kernel
        choice_kernel *= (1 - ck_decay)
        choice_kernel[action_1[trial]] += 1
        
        # Update Q-values
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
    eps = 1e-10
    mask = (p_choice_1 > 0) & (p_choice_2 > 0)
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss
```