Here are three new cognitive models for the two-step decision-making task.

### Model 1: Outcome-Dependent Exploration (Beta-Switching)
This model hypothesizes that the participant's level of exploration in the first stage is dynamic and depends on the outcome of the previous trial. Specifically, a "Win-Stay, Lose-Shift" strategy is implemented not via a bias, but via the softmax inverse temperature ($\beta$). A reward promotes exploitation (higher $\beta$, more deterministic), while a lack of reward promotes exploration (lower $\beta$, more stochastic).

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Outcome-Dependent Exploration Model.
    
    This model assumes the participant's exploration in Stage 1 is modulated 
    by the previous trial's outcome. A win encourages exploitation (high beta), while 
    a loss encourages exploration (low beta).
    
    Parameters:
    lr: [0, 1] Learning rate.
    beta_win: [0, 10] Stage 1 inverse temperature after a reward (Exploit).
    beta_loss: [0, 10] Stage 1 inverse temperature after no reward (Explore).
    beta_2: [0, 10] Stage 2 inverse temperature.
    w: [0, 1] Model-based weight.
    lam: [0, 1] Eligibility trace.
    """
    lr, beta_win, beta_loss, beta_2, w, lam = model_parameters
    n_trials = len(action_1)
    
    # Transition probabilities: 0->0 and 1->1 are common (0.7)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Initialize previous reward (assume 0/loss for the very first trial)
    prev_reward = 0 
    
    for trial in range(n_trials):
        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Select beta based on previous reward
        current_beta_1 = beta_win if prev_reward == 1 else beta_loss
        
        exp_q1 = np.exp(current_beta_1 * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        
        # Stage 2 Policy
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # Updates
        # Stage 1 MF Update
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr * delta_stage1
        
        # Stage 2 MF Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr * delta_stage2
        
        # Eligibility Trace Update for Stage 1
        q_stage1_mf[action_1[trial]] += lr * lam * delta_stage2
        
        prev_reward = reward[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Transition-Dependent Learning Rates
This model proposes that the participant gates their learning based on whether the transition experienced was Common or Rare. Participants might discount information from Rare transitions (regarding them as noise/flukes) or overweight them. This is implemented by having two separate learning rates (`lr_c` and `lr_r`) that apply to the updates depending on the transition type observed in the trial.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Transition-Dependent Learning Rate Model.
    
    This model assumes the participant updates their values differently depending 
    on whether the transition experienced was Common or Rare. This captures strategies 
    where participants might ignore rare transitions or learn more from them.
    
    Parameters:
    lr_c: [0, 1] Learning rate for Common transitions.
    lr_r: [0, 1] Learning rate for Rare transitions.
    beta_1: [0, 10] Stage 1 inverse temperature.
    beta_2: [0, 10] Stage 2 inverse temperature.
    w: [0, 1] Model-based weight.
    lam: [0, 1] Eligibility trace.
    """
    lr_c, lr_r, beta_1, beta_2, w, lam = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_1 * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        
        # Stage 2 Policy
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # Determine Transition Type
        # Common: Action 0 -> State 0, Action 1 -> State 1
        is_common = (action_1[trial] == state_idx)
        current_lr = lr_c if is_common else lr_r
        
        # Updates using the transition-dependent learning rate
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += current_lr * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += current_lr * delta_stage2
        
        q_stage1_mf[action_1[trial]] += current_lr * lam * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Asymmetric Eligibility Traces
This model posits that the credit assignment from the final reward back to the first stage choice (the eligibility trace $\lambda$) depends on the valence of the outcome. A participant might strongly associate the spaceship choice with the outcome when they win (`lam_win`), but dissociate them when they lose (`lam_loss`), or vice versa.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Asymmetric Eligibility Trace Model.
    
    This model allows the strength of the eligibility trace (credit assignment from 
    reward to Stage 1 choice) to vary depending on whether the outcome was a win or a loss.
    
    Parameters:
    lr: [0, 1] Learning rate.
    beta_1: [0, 10] Stage 1 inverse temperature.
    beta_2: [0, 10] Stage 2 inverse temperature.
    w: [0, 1] Model-based weight.
    lam_win: [0, 1] Eligibility trace for rewarded trials.
    lam_loss: [0, 1] Eligibility trace for unrewarded trials.
    """
    lr, beta_1, beta_2, w, lam_win, lam_loss = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_1 * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        
        # Stage 2 Policy
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # Updates
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr * delta_stage2
        
        # Select lambda based on reward
        current_lam = lam_win if reward[trial] == 1 else lam_loss
        q_stage1_mf[action_1[trial]] += lr * current_lam * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```