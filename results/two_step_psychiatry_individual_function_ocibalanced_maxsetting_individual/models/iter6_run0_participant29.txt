Here are three cognitive models based on the participant data and the provided template.

### Model 1: Hybrid Model with Separate Stage Learning Rates and Forgetting
This model separates the learning rates for Stage 1 (Spaceships) and Stage 2 (Aliens). The participant data shows extreme stickiness/habit in Stage 1 (choosing Spaceship 0 for 60+ trials) while showing some sensitivity to reward changes in Stage 2. Independent learning rates allow the model to capture the slow plasticity of spaceship preferences versus the faster (or different) tracking of alien reward probabilities. The forgetting mechanism is included to handle the non-stationary nature of the rewards, decaying unchosen options.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Separate Stage Learning Rates and Forgetting.
    
    Distinguishes between learning dynamics for the first-stage choice (Spaceships)
    and the second-stage choice (Aliens). Includes a forgetting mechanism for 
    unchosen options to handle non-stationary rewards.

    Parameters:
    - alpha_s1: [0, 1] Learning rate for Stage 1 (Spaceship) MF values.
    - alpha_s2: [0, 1] Learning rate for Stage 2 (Alien) MF values.
    - beta: [0, 10] Inverse temperature for choice stochasticity.
    - w: [0, 1] Weight for Model-Based values in Stage 1 (0=MF, 1=MB).
    - forget_rate: [0, 1] Decay rate for unchosen Q-values.
    """
    alpha_s1, alpha_s2, beta, w, forget_rate = model_parameters
    n_trials = len(action_1)
    
    # Fixed transition matrix as per task description
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Initialize Q-values
    q_stage1_mf = np.zeros(2)      # Values for Spaceship 0, 1
    q_stage2_mf = np.zeros((2, 2)) # Values for State 0 (Aliens 0,1), State 1 (Aliens 0,1)
    
    log_loss = 0.0
    
    for trial in range(n_trials):
        
        # --- Stage 1 Decision ---
        # Model-Based Value: Expected max value of next stage
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Value: Weighted sum of MB and MF
        q_net_s1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax Choice Probability
        exp_q1 = np.exp(beta * q_net_s1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        # Accumulate Loss
        log_loss -= np.log(probs_1[action_1[trial]] + 1e-10)
        
        # --- Stage 2 Decision ---
        state_idx = state[trial]
        q_s2 = q_stage2_mf[state_idx]
        
        # Softmax Choice Probability (Pure MF at Stage 2)
        exp_q2 = np.exp(beta * q_s2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        # Accumulate Loss
        log_loss -= np.log(probs_2[action_2[trial]] + 1e-10)
        
        # --- Updates ---
        
        # 1. Update Stage 2 (Aliens)
        # Prediction error: Reward - Current Value
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += alpha_s2 * delta_stage2
        
        # Forgetting for unchosen alien in this state
        unchosen_a2 = 1 - action_2[trial]
        q_stage2_mf[state_idx, unchosen_a2] *= (1.0 - forget_rate)
        
        # 2. Update Stage 1 (Spaceships)
        # TD(0) update: Target is the value of the state actually reached (Q_s2 of chosen alien)
        target_stage1 = q_stage2_mf[state_idx, action_2[trial]]
        delta_stage1 = target_stage1 - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += alpha_s1 * delta_stage1
        
        # Forgetting for unchosen spaceship
        unchosen_a1 = 1 - action_1[trial]
        q_stage1_mf[unchosen_a1] *= (1.0 - forget_rate)
        
    return log_loss
```

### Model 2: Hybrid Model with Stage-Specific Stickiness
This model addresses the observation that the participant exhibits very different levels of perseverance in the two stages. They repeat the spaceship choice (Stage 1) almost exclusively for the first 60 trials, suggesting high habit/stickiness, whereas their alien choice (Stage 2) varies more based on outcomes. By parameterizing stickiness separately for Stage 1 and Stage 2, the model can account for the "locked-in" behavior in the first step without over-predicting repetition in the second step.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Stage-Specific Stickiness.
    
    Incorporates distinct perseverance (stickiness) bonuses for Stage 1 and Stage 2 
    choices. This allows the model to capture strong habitual behavior in spaceship 
    selection independent of the sensitivity to alien rewards.
    
    Parameters:
    - learning_rate: [0, 1] Update rate for Q-values (shared across stages).
    - beta: [0, 10] Inverse temperature for choices.
    - w: [0, 1] Weight for Model-Based values in Stage 1.
    - stick_s1: [0, 10] Stickiness bonus for repeating the last Stage 1 choice.
    - stick_s2: [0, 10] Stickiness bonus for repeating the last Stage 2 choice (per state).
    """
    learning_rate, beta, w, stick_s1, stick_s2 = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track last choices for stickiness
    # Initialize with -1 (no previous choice)
    last_choice_s1 = -1
    last_choice_s2 = np.full(2, -1) # One for each state (planet)
    
    log_loss = 0.0
    
    for trial in range(n_trials):
        
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_s1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness bonus to logits
        logits_1 = beta * q_net_s1
        if last_choice_s1 != -1:
            logits_1[last_choice_s1] += stick_s1
            
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[action_1[trial]] + 1e-10)
        
        # --- Stage 2 Decision ---
        state_idx = state[trial]
        q_s2 = q_stage2_mf[state_idx]
        
        # Add stickiness bonus to logits
        logits_2 = beta * q_s2
        if last_choice_s2[state_idx] != -1:
            logits_2[last_choice_s2[state_idx]] += stick_s2
            
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[action_2[trial]] + 1e-10)
        
        # --- Updates ---
        
        # Update Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Update Stage 1
        target_stage1 = q_stage2_mf[state_idx, action_2[trial]]
        delta_stage1 = target_stage1 - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Update History
        last_choice_s1 = action_1[trial]
        last_choice_s2[state_idx] = action_2[trial]
        
    return log_loss
```

### Model 3: Hybrid Model with Asymmetric Learning and Forgetting
This model combines asymmetric learning rates (positive vs. negative prediction errors) with the forgetting mechanism. The participant may weigh positive outcomes (finding gold) differently from negative outcomes (no gold), a common bias in reinforcement learning. By combining this with forgetting (decay of unchosen options), the model creates a robust mechanism for tracking the drifting reward probabilities while allowing for differential sensitivity to wins and losses.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Asymmetric Learning and Forgetting.
    
    Uses separate learning rates for positive and negative prediction errors to 
    capture potential confirmation bias or risk sensitivity. Also includes 
    forgetting for unchosen options to maintain flexibility in a changing environment.
    
    Parameters:
    - alpha_pos: [0, 1] Learning rate for positive prediction errors (better than expected).
    - alpha_neg: [0, 1] Learning rate for negative prediction errors (worse than expected).
    - beta: [0, 10] Inverse temperature for choices.
    - w: [0, 1] Weight for Model-Based values in Stage 1.
    - forget_rate: [0, 1] Decay rate for unchosen Q-values.
    """
    alpha_pos, alpha_neg, beta, w, forget_rate = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    log_loss = 0.0
    
    for trial in range(n_trials):
        
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_s1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_s1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[action_1[trial]] + 1e-10)
        
        # --- Stage 2 Decision ---
        state_idx = state[trial]
        q_s2 = q_stage2_mf[state_idx]
        
        exp_q2 = np.exp(beta * q_s2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[action_2[trial]] + 1e-10)
        
        # --- Updates ---
        
        # Update Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        if delta_stage2 >= 0:
            q_stage2_mf[state_idx, action_2[trial]] += alpha_pos * delta_stage2
        else:
            q_stage2_mf[state_idx, action_2[trial]] += alpha_neg * delta_stage2
            
        # Forgetting Stage 2
        unchosen_a2 = 1 - action_2[trial]
        q_stage2_mf[state_idx, unchosen_a2] *= (1.0 - forget_rate)
        
        # Update Stage 1
        target_stage1 = q_stage2_mf[state_idx, action_2[trial]]
        delta_stage1 = target_stage1 - q_stage1_mf[action_1[trial]]
        if delta_stage1 >= 0:
            q_stage1_mf[action_1[trial]] += alpha_pos * delta_stage1
        else:
            q_stage1_mf[action_1[trial]] += alpha_neg * delta_stage1
            
        # Forgetting Stage 1
        unchosen_a1 = 1 - action_1[trial]
        q_stage1_mf[unchosen_a1] *= (1.0 - forget_rate)
        
    return log_loss
```