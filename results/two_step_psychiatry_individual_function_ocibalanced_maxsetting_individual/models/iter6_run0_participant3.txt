Here are the three cognitive models based on the participant data and the analysis of their behavioral patterns.

### Cognitive Model 1: Hybrid Model with Unchosen Action Decay and Stickiness
This model builds upon the "best model" identified in the feedback (Decay) by adding a **stickiness** parameter. The participant data shows strong perseverance (sticking to one spaceship for blocks of trials), which simple decay might not fully capture. Stickiness adds a bonus to the previously chosen action, modeling the inertia observed in the choice sequences.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Unchosen Action Decay and Stickiness.
    
    Combines Model-Based and Model-Free learning with two additional mechanisms:
    1. Decay: Unchosen action values decay towards zero, promoting flexibility in a changing environment.
    2. Stickiness: A bonus is added to the logit of the previously chosen action, capturing behavioral inertia.

    Parameters:
    learning_rate: [0, 1] - Learning rate for updating Q-values.
    beta: [0, 10] - Inverse temperature for softmax choice (exploration/exploitation).
    w: [0, 1] - Weight for Model-Based values (0 = pure MF, 1 = pure MB).
    decay: [0, 1] - Decay rate for unchosen action values.
    stickiness: [0, 10] - Bonus added to the previously chosen action's value.
    """
    learning_rate, beta, w, decay, stickiness = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # state x action
    
    last_action_1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1: # Skip invalid trials if any
            continue

        # --- Stage 1 Policy ---
        # Model-Based Value Calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Q-value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Calculate Logits
        logits_1 = beta * q_net
        
        # Apply Stickiness
        if last_action_1 != -1:
            logits_1[last_action_1] += stickiness
            
        # Softmax
        exp_q1 = np.exp(logits_1 - np.max(logits_1)) # Subtract max for stability
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Policy ---
        state_idx = state[trial]
        logits_2 = beta * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        chosen_a1 = action_1[trial]
        chosen_a2 = action_2[trial]
        
        # Update Stage 1 MF
        delta_stage1 = q_stage2_mf[state_idx, chosen_a2] - q_stage1_mf[chosen_a1]
        q_stage1_mf[chosen_a1] += learning_rate * delta_stage1
        
        # Decay Unchosen Stage 1
        unchosen_a1 = 1 - chosen_a1
        q_stage1_mf[unchosen_a1] *= (1 - decay)

        # Update Stage 2 MF
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, chosen_a2]
        q_stage2_mf[state_idx, chosen_a2] += learning_rate * delta_stage2
        
        # Decay Unchosen Stage 2
        unchosen_a2 = 1 - chosen_a2
        q_stage2_mf[state_idx, unchosen_a2] *= (1 - decay)
        
        last_action_1 = chosen_a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2: Hybrid Model with Stage-Specific Learning Rates and Stickiness
The task has two distinct dynamics: Stage 1 involves transitions that are stable, while Stage 2 involves alien rewards that change slowly (non-stationary). This model uses **separate learning rates** for Stage 1 and Stage 2 to capture the different adaptation speeds required. It also includes **stickiness** to account for the participant's tendency to repeat Step 1 choices.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Stage-Specific Learning Rates and Stickiness.
    
    Differentiates learning between the two stages of the task:
    1. lr_1: Learning rate for updating spaceship preferences (Stage 1).
    2. lr_2: Learning rate for updating alien reward estimates (Stage 2).
    This allows the model to tune adaptation to the specific volatility of each stage.
    Stickiness is included to model choice perseverance in Stage 1.

    Parameters:
    lr_1: [0, 1] - Learning rate for Stage 1 (Spaceships).
    lr_2: [0, 1] - Learning rate for Stage 2 (Aliens).
    beta: [0, 10] - Inverse temperature for softmax choice.
    w: [0, 1] - Weight for Model-Based values.
    stickiness: [0, 10] - Choice perseverance bonus for Stage 1.
    """
    lr_1, lr_2, beta, w, stickiness = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net
        if last_action_1 != -1:
            logits_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Policy ---
        state_idx = state[trial]
        logits_2 = beta * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        chosen_a1 = action_1[trial]
        chosen_a2 = action_2[trial]
        
        # Update Stage 1 with lr_1
        delta_stage1 = q_stage2_mf[state_idx, chosen_a2] - q_stage1_mf[chosen_a1]
        q_stage1_mf[chosen_a1] += lr_1 * delta_stage1
        
        # Update Stage 2 with lr_2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, chosen_a2]
        q_stage2_mf[state_idx, chosen_a2] += lr_2 * delta_stage2
        
        last_action_1 = chosen_a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3: Hybrid Model with Eligibility Trace and Stickiness
This model incorporates an **eligibility trace parameter (lambda)**. In a standard hybrid model, the Model-Free component updates Stage 1 values based on the *value* of Stage 2 (TD(0)). With an eligibility trace, the final reward outcome can directly update the Stage 1 choice (TD($\lambda$)). This creates a spectrum between updating based on expected future value vs. realized outcome. **Stickiness** is included to account for the participant's block-like switching behavior.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Eligibility Trace (Lambda) and Stickiness.
    
    Incorporates an eligibility trace to bridge the credit assignment between Stage 2 outcomes
    and Stage 1 choices.
    1. lambda_param: Controls the weight of the direct outcome in the Stage 1 update.
       Higher lambda makes Stage 1 values more sensitive to the immediate reward received.
    2. Stickiness: Captures the tendency to repeat the previous Stage 1 choice.

    Parameters:
    learning_rate: [0, 1] - Learning rate.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - Weight for Model-Based values.
    lambda_param: [0, 1] - Eligibility trace parameter (0 = TD(0), 1 = Monte Carlo-like).
    stickiness: [0, 10] - Choice perseverance bonus.
    """
    learning_rate, beta, w, lambda_param, stickiness = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net
        if last_action_1 != -1:
            logits_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Policy ---
        state_idx = state[trial]
        logits_2 = beta * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        chosen_a1 = action_1[trial]
        chosen_a2 = action_2[trial]
        
        # Calculate Prediction Errors
        # Stage 2 PE (Reward - Stage 2 Value)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, chosen_a2]
        
        # Stage 1 PE (Stage 2 Value - Stage 1 Value)
        delta_stage1 = q_stage2_mf[state_idx, chosen_a2] - q_stage1_mf[chosen_a1]
        
        # Update Stage 1 MF with Eligibility Trace
        # Combines the standard TD(0) update (delta_stage1) with the TD(1) component (lambda * delta_stage2)
        q_stage1_mf[chosen_a1] += learning_rate * (delta_stage1 + lambda_param * delta_stage2)
        
        # Update Stage 2 MF
        q_stage2_mf[state_idx, chosen_a2] += learning_rate * delta_stage2
        
        last_action_1 = chosen_a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```