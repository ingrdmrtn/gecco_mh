Here are three new cognitive models for the two-step decision task, designed to capture different potential mechanisms underlying the participant's behavior, specifically focusing on how they handle the two stages of the task (spaceships vs. aliens) and how they integrate model-based information with robust learning strategies.

### Model 1: Hybrid Model with Asymmetric Learning and Decay
This model combines Model-Based (MB) planning with Model-Free (MF) learning. It uses asymmetric learning rates (learning differently from positive vs. negative errors) to capture optimism/pessimism biases, and a decay parameter to handle the drifting reward probabilities. Crucially, it simplifies the MF component to a direct stage-to-stage update (no eligibility trace parameter `lambda`), focusing the complexity on the interaction between the MB system and the robust, asymmetric MF values.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model (MB/MF) with Asymmetric Learning Rates and Decay.
    Combines Model-Based planning with Model-Free learning, using asymmetric 
    updates for positive/negative prediction errors and passive decay.
    Unlike full Hybrid models, this omits the eligibility trace (lambda) to 
    test if Stage 1 updates are sufficiently driven by the MB system and 
    direct Stage 2 value caching.
    
    Parameters:
    lr_pos: Learning rate for positive prediction errors [0,1]
    lr_neg: Learning rate for negative prediction errors [0,1]
    beta: Inverse temperature [0,10]
    w: Model-based weight [0,1]
    decay: Decay rate for Q-values [0,1]
    perseveration: Stickiness to previous choice (Stage 1) [0,5]
    """
    lr_pos, lr_neg, beta, w, decay, perseveration = model_parameters
    n_trials = len(action_1)
    
    # Initialize values
    q_mf_s1 = np.full(2, 0.5)
    q_mf_s2 = np.full((2, 2), 0.5)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        if action_1[trial] == -1:
            last_action_1 = -1
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue
            
        # Passive Decay of Q-values towards 0.5
        q_mf_s1 = q_mf_s1 * (1 - decay) + 0.5 * decay
        q_mf_s2 = q_mf_s2 * (1 - decay) + 0.5 * decay
        
        # --- Stage 1 Choice ---
        # Model-Based Calculation: Expected value based on transition matrix
        max_q_s2 = np.max(q_mf_s2, axis=1)
        q_mb_s1 = transition_matrix @ max_q_s2
        
        # Net Q-value: Weighted sum of MB and MF
        q_net_s1 = w * q_mb_s1 + (1 - w) * q_mf_s1
        
        # Perseveration bonus
        if last_action_1 != -1:
            q_net_s1[last_action_1] += perseveration
            
        exp_q1 = np.exp(beta * q_net_s1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Choice ---
        s_idx = state[trial]
        exp_q2 = np.exp(beta * q_mf_s2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        # Stage 1 Prediction Error (SARSA-0 style for MF component)
        # Updates Stage 1 MF value based on the value of the chosen Stage 2 state-action
        delta_1 = q_mf_s2[s_idx, action_2[trial]] - q_mf_s1[action_1[trial]]
        lr_1 = lr_pos if delta_1 > 0 else lr_neg
        q_mf_s1[action_1[trial]] += lr_1 * delta_1
        
        # Stage 2 Prediction Error
        # Updates Stage 2 value based on received reward
        delta_2 = reward[trial] - q_mf_s2[s_idx, action_2[trial]]
        lr_2 = lr_pos if delta_2 > 0 else lr_neg
        q_mf_s2[s_idx, action_2[trial]] += lr_2 * delta_2
        
        last_action_1 = action_1[trial]
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Model-Free Q-Learning with Stage-Specific Decay
This model hypothesizes that the participant forgets or discounts the value of Spaceships (Stage 1) and Aliens (Stage 2) at different rates. Since alien rewards drift over time while spaceship transition probabilities are relatively stable (though probabilistic), treating these two types of information with different decay rates (`decay_s1` vs. `decay_s2`) allows the model to balance structural stability with reward adaptability.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Q-Learning with Stage-Specific Decay.
    Applies different decay rates to Stage 1 (Spaceships) and Stage 2 (Aliens) values,
    allowing the model to handle structural stability vs reward drift differently.
    
    Parameters:
    learning_rate: Learning rate [0,1]
    beta: Inverse temperature [0,10]
    lambda_eligibility: Eligibility trace decay [0,1]
    decay_s1: Decay rate for Stage 1 values [0,1]
    decay_s2: Decay rate for Stage 2 values [0,1]
    perseveration: Stickiness to previous choice (Stage 1) [0,5]
    """
    learning_rate, beta, lambda_eligibility, decay_s1, decay_s2, perseveration = model_parameters
    n_trials = len(action_1)
    
    q_s1 = np.full(2, 0.5)
    q_s2 = np.full((2, 2), 0.5)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    last_action_1 = -1
    
    for trial in range(n_trials):
        if action_1[trial] == -1:
            last_action_1 = -1
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue
            
        # Independent Decay for Stage 1 and Stage 2
        q_s1 = q_s1 * (1 - decay_s1) + 0.5 * decay_s1
        q_s2 = q_s2 * (1 - decay_s2) + 0.5 * decay_s2
        
        # --- Stage 1 Choice ---
        q_net_s1 = q_s1.copy()
        if last_action_1 != -1:
            q_net_s1[last_action_1] += perseveration
            
        exp_q1 = np.exp(beta * q_net_s1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Choice ---
        s_idx = state[trial]
        exp_q2 = np.exp(beta * q_s2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        # Update Stage 1 based on Stage 2 value (TD learning)
        delta_1 = q_s2[s_idx, action_2[trial]] - q_s1[action_1[trial]]
        q_s1[action_1[trial]] += learning_rate * delta_1
        
        # Update Stage 2 based on Reward
        delta_2 = reward[trial] - q_s2[s_idx, action_2[trial]]
        q_s2[s_idx, action_2[trial]] += learning_rate * delta_2
        
        # Eligibility Trace Update: Reward directly impacts Stage 1 choice
        q_s1[action_1[trial]] += learning_rate * lambda_eligibility * delta_2
        
        last_action_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Model-Free Q-Learning with Stage-Specific Perseveration
This model extends the concept of "stickiness" to both stages independently. While standard models usually include perseveration only for the spaceship choice, this model allows for separate perseveration on the alien choice (`pers_s2`), capturing the tendency to repeat the same alien choice when revisiting the same planet, distinct from the tendency to repeat the spaceship choice (`pers_s1`).

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Q-Learning with Stage-Specific Perseveration.
    Includes separate perseveration parameters for the first stage (Spaceship choice)
    and the second stage (Alien choice within a planet), capturing different 
    stickiness behaviors at each level.
    
    Parameters:
    learning_rate: Learning rate [0,1]
    beta: Inverse temperature [0,10]
    lambda_eligibility: Eligibility trace decay [0,1]
    decay: Decay rate for Q-values [0,1]
    pers_s1: Perseveration for Stage 1 [0,5]
    pers_s2: Perseveration for Stage 2 [0,5]
    """
    learning_rate, beta, lambda_eligibility, decay, pers_s1, pers_s2 = model_parameters
    n_trials = len(action_1)
    
    q_s1 = np.full(2, 0.5)
    q_s2 = np.full((2, 2), 0.5)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1
    last_action_2 = np.full(2, -1) # Track last action for each state (planet) separately
    
    for trial in range(n_trials):
        if action_1[trial] == -1:
            last_action_1 = -1
            last_action_2 = np.full(2, -1)
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue
            
        # Decay
        q_s1 = q_s1 * (1 - decay) + 0.5 * decay
        q_s2 = q_s2 * (1 - decay) + 0.5 * decay
        
        # --- Stage 1 Choice ---
        q_net_s1 = q_s1.copy()
        if last_action_1 != -1:
            q_net_s1[last_action_1] += pers_s1
            
        exp_q1 = np.exp(beta * q_net_s1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Choice ---
        s_idx = state[trial]
        q_net_s2 = q_s2[s_idx].copy()
        # Apply perseveration only if we have a previous action for this specific planet
        if last_action_2[s_idx] != -1:
            q_net_s2[last_action_2[s_idx]] += pers_s2
            
        exp_q2 = np.exp(beta * q_net_s2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        delta_1 = q_s2[s_idx, action_2[trial]] - q_s1[action_1[trial]]
        q_s1[action_1[trial]] += learning_rate * delta_1
        
        delta_2 = reward[trial] - q_s2[s_idx, action_2[trial]]
        q_s2[s_idx, action_2[trial]] += learning_rate * delta_2
        
        q_s1[action_1[trial]] += learning_rate * lambda_eligibility * delta_2
        
        last_action_1 = action_1[trial]
        last_action_2[s_idx] = action_2[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```