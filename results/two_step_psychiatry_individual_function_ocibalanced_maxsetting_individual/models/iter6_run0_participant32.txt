Here are three new cognitive models for the two-step decision task.

### Model 1: Hybrid Model with Independent Stage Learning Rates
This model extends the standard Hybrid MB/MF model by decoupling the learning rates for the first-stage (spaceship) and second-stage (alien) Model-Free values. This allows the model to capture differences in how quickly the participant learns "cached" habits (Stage 1) versus how they update the value of specific outcomes (Stage 2).

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Independent Stage Learning Rates.
    
    Distinguishes between the learning rate for the Model-Free first-stage values (caching)
    and the Model-Free second-stage values (alien values).
    
    Parameters:
    - lr_stg1: [0, 1] Learning rate for Stage 1 MF values.
    - lr_stg2: [0, 1] Learning rate for Stage 2 MF values.
    - beta1: [0, 10] Inverse temperature for Stage 1.
    - beta2: [0, 10] Inverse temperature for Stage 2.
    - w: [0, 1] Mixing weight (0 = Pure MB, 1 = Pure MF).
    - perseveration: [0, 5] Stickiness bonus for Stage 1.
    """
    lr_stg1, lr_stg2, beta1, beta2, w, perseveration = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Initialize MF values to 0.5 (chance expectation)
    q_stage1_mf = np.ones(2) * 0.5
    q_stage2_mf = np.ones((2, 2)) * 0.5 
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based Value calculation (Bellman)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = w * q_stage1_mf + (1 - w) * q_stage1_mb
        
        logits_1 = beta1 * q_net
        if last_action_1 != -1:
            logits_1[last_action_1] += perseveration
            
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_action_1 = action_1[trial]
        state_idx = state[trial]
        a1 = action_1[trial]

        # --- Stage 2 Choice ---
        logits_2 = beta2 * q_stage2_mf[state_idx]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]

        # --- Learning ---
        # Stage 1 Update (TD-0 error from Stage 2 value)
        pe1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr_stg1 * pe1
        
        # Stage 2 Update (TD error from Reward)
        pe2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += lr_stg2 * pe2
        
        # Stage 1 Update (Eligibility trace / Lambda=1 logic: propagate Reward error)
        q_stage1_mf[a1] += lr_stg1 * pe2
            
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model with Asymmetric Learning Rates
This model introduces separate learning rates for positive and negative prediction errors (`lr_pos`, `lr_neg`) applied to both stages. It also uses separate inverse temperature parameters (`beta1`, `beta2`) to account for different noise levels in the spaceship choice versus the alien choice.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Asymmetric Learning Rates and Separate Betas.
    
    Incorporates separate learning rates for positive and negative prediction errors,
    capturing valence-dependent update biases.
    
    Parameters:
    - lr_pos: [0, 1] Learning rate for positive prediction errors.
    - lr_neg: [0, 1] Learning rate for negative prediction errors.
    - beta1: [0, 10] Inverse temperature for Stage 1.
    - beta2: [0, 10] Inverse temperature for Stage 2.
    - w: [0, 1] Mixing weight (0 = Pure MB, 1 = Pure MF).
    - perseveration: [0, 5] Stickiness bonus for Stage 1.
    """
    lr_pos, lr_neg, beta1, beta2, w, perseveration = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.ones(2) * 0.5
    q_stage2_mf = np.ones((2, 2)) * 0.5 
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mf + (1 - w) * q_stage1_mb
        
        logits_1 = beta1 * q_net
        if last_action_1 != -1:
            logits_1[last_action_1] += perseveration
            
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_action_1 = action_1[trial]
        state_idx = state[trial]
        a1 = action_1[trial]

        # --- Stage 2 Choice ---
        logits_2 = beta2 * q_stage2_mf[state_idx]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]

        # --- Learning ---
        # Stage 1 PE
        pe1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        lr1 = lr_pos if pe1 > 0 else lr_neg
        q_stage1_mf[a1] += lr1 * pe1
        
        # Stage 2 PE
        pe2 = r - q_stage2_mf[state_idx, a2]
        lr2 = lr_pos if pe2 > 0 else lr_neg
        q_stage2_mf[state_idx, a2] += lr2 * pe2
        
        # Trace update for Stage 1 (using outcome PE)
        lr1_trace = lr_pos if pe2 > 0 else lr_neg
        q_stage1_mf[a1] += lr1_trace * pe2
            
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Pure Model-Based with Decay and Outcome-Dependent Perseveration
This model builds on the "Pure MB with Decay" model by refining the perseveration mechanism. Instead of a single stickiness parameter, it uses separate parameters for `pers_win` (Win-Stay) and `pers_loss` (Lose-Stay/Shift). This allows the model to capture simple heuristic strategies (like repeating a winning choice) without requiring a full Model-Free value system.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Based with Passive Value Decay and Outcome-Dependent Perseveration.
    
    Combines Model-Based planning with a decay mechanism for drifting values,
    and replaces the generic perseveration with specific Win-Stay and Lose-Stay/Shift biases.
    
    Parameters:
    - lr: [0, 1] Learning rate for chosen option.
    - beta1: [0, 10] Inverse temperature for Stage 1.
    - beta2: [0, 10] Inverse temperature for Stage 2.
    - decay: [0, 1] Decay rate for unchosen options.
    - pers_win: [0, 5] Perseveration bonus after a rewarded trial.
    - pers_loss: [0, 5] Perseveration bonus after an unrewarded trial.
    """
    lr, beta1, beta2, decay, pers_win, pers_loss = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage2_mf = np.ones((2, 2)) * 0.5 
    
    last_action_1 = -1
    last_reward = -1
    
    for trial in range(n_trials):

        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        logits_1 = beta1 * q_stage1_mb
        
        # Outcome-dependent perseveration
        if last_action_1 != -1:
            if last_reward == 1:
                logits_1[last_action_1] += pers_win
            else:
                logits_1[last_action_1] += pers_loss
            
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_action_1 = action_1[trial]
        state_idx = state[trial]

        logits_2 = beta2 * q_stage2_mf[state_idx]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]
        last_reward = r

        # Update Chosen
        pe = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += lr * pe

        # Decay Unchosen
        unchosen_action = 1 - a2
        q_stage2_mf[state_idx, unchosen_action] = (1 - decay) * q_stage2_mf[state_idx, unchosen_action] + decay * 0.5

        other_state = 1 - state_idx
        q_stage2_mf[other_state, :] = (1 - decay) * q_stage2_mf[other_state, :] + decay * 0.5
            
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```