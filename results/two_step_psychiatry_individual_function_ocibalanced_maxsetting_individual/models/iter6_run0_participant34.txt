Here are the three proposed cognitive models.

### Model 1: Split Learning Rates with Outcome-Specific Stickiness
This model extends the "best model" by acknowledging that learning dynamics may differ between the decision stages. Stage 1 involves integrating Model-Based structure with Model-Free values, while Stage 2 is a pure bandit task with slowly drifting probabilities. Therefore, this model assigns separate learning rates to Stage 1 and Stage 2 updates, combined with the successful outcome-specific stickiness mechanism.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Split Learning Rates (Stage 1 vs 2) with Outcome-Specific Stickiness.
    
    Separates the learning rate for the first stage (choice between spaceships)
    from the second stage (choice between aliens), as the second stage tracks 
    drifting reward probabilities while the first stage aggregates values.
    Also includes separate stickiness for winning and losing previous trials.

    Parameters:
    - lr_s1: [0, 1] Learning rate for Stage 1 Q-values.
    - lr_s2: [0, 1] Learning rate for Stage 2 Q-values.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - w: [0, 1] Weight for Model-Based control.
    - stick_win: [0, 5] Stickiness bias if the previous trial was rewarded.
    - stick_loss: [0, 5] Stickiness bias if the previous trial was unrewarded.
    """
    lr_s1, lr_s2, beta_1, beta_2, w, stick_win, stick_loss = model_parameters
    n_trials = len(action_1)
    
    # Transition matrix: T[0,0]=0.7 (Common), T[0,1]=0.3 (Rare), etc.
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_mf1 = np.zeros(2)      # Stage 1 Model-Free values
    q_s2 = np.zeros((2, 2))  # Stage 2 values (State x Action)
    
    last_action_1 = -1
    last_reward = 0
    
    for t in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based Value: Expected value of next stage states
        max_q_s2 = np.max(q_s2, axis=1)
        q_mb1 = transition_matrix @ max_q_s2
        
        # Hybrid Value
        q_net = w * q_mb1 + (1 - w) * q_mf1
        
        # Apply Outcome-Specific Stickiness
        if last_action_1 != -1:
            if last_reward == 1:
                q_net[last_action_1] += stick_win
            else:
                q_net[last_action_1] += stick_loss
        
        # Softmax Choice 1
        q_net_stable = q_net - np.max(q_net)
        exp_q1 = np.exp(beta_1 * q_net_stable)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[t] = probs_1[action_1[t]]
        
        a1 = action_1[t]
        s_next = state[t]
        last_action_1 = a1
        
        # --- Stage 2 Policy ---
        q_s2_curr = q_s2[s_next]
        q_s2_stable = q_s2_curr - np.max(q_s2_curr)
        exp_q2 = np.exp(beta_2 * q_s2_stable)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[t] = probs_2[action_2[t]]
        
        a2 = action_2[t]
        r = reward[t]
        last_reward = r
        
        # --- Updates ---
        # Stage 1 Update (TD-0 using Stage 2 Q-value)
        pe1 = q_s2[s_next, a2] - q_mf1[a1]
        q_mf1[a1] += lr_s1 * pe1
        
        # Stage 2 Update (Reward Prediction Error)
        pe2 = r - q_s2[s_next, a2]
        q_s2[s_next, a2] += lr_s2 * pe2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Mixture with Win-Stay Lose-Shift
This model proposes that the participant's behavior is a mixture of a complex Reinforcement Learning process (Hybrid MB/MF) and a simple heuristic strategy: "Win-Stay, Lose-Shift" (WSLS). Instead of implementing stickiness as a value bias, this model treats WSLS as a competing policy that dictates choices with probability $\epsilon$.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model mixed with explicit Win-Stay Lose-Shift (WSLS) Strategy.
    
    The policy is a mixture of the Hybrid MB/MF softmax distribution and a 
    deterministic WSLS rule. 
    P(choice) = (1 - eps) * P_Hybrid + eps * P_WSLS
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for Q-values.
    - beta_1: [0, 10] Inverse temperature for Stage 1 Hybrid model.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - w: [0, 1] Weight for Model-Based control.
    - stickiness: [0, 5] Standard stickiness bias within the Hybrid model.
    - epsilon_wsls: [0, 1] Weight of the heuristic WSLS strategy in the mixture.
    """
    learning_rate, beta_1, beta_2, w, stickiness, epsilon_wsls = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_mf1 = np.zeros(2)
    q_s2 = np.zeros((2, 2))
    
    last_action_1 = -1
    last_reward = 0
    
    for t in range(n_trials):
        # --- Stage 1 Policy (Hybrid Component) ---
        max_q_s2 = np.max(q_s2, axis=1)
        q_mb1 = transition_matrix @ max_q_s2
        
        q_net = w * q_mb1 + (1 - w) * q_mf1
        
        # Standard stickiness in the RL component
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        q_net_stable = q_net - np.max(q_net)
        exp_q1 = np.exp(beta_1 * q_net_stable)
        probs_hybrid = exp_q1 / np.sum(exp_q1)
        
        # --- Stage 1 Policy (WSLS Component) ---
        probs_wsls = np.zeros(2)
        if last_action_1 == -1:
            probs_wsls = np.array([0.5, 0.5]) # First trial uniform
        else:
            if last_reward == 1:
                # Win-Stay
                probs_wsls[last_action_1] = 1.0
            else:
                # Lose-Shift
                probs_wsls[1 - last_action_1] = 1.0
                
        # --- Mixture Policy ---
        probs_final = (1 - epsilon_wsls) * probs_hybrid + epsilon_wsls * probs_wsls
        p_choice_1[t] = probs_final[action_1[t]]
        
        a1 = action_1[t]
        s_next = state[t]
        last_action_1 = a1
        
        # --- Stage 2 Policy ---
        q_s2_curr = q_s2[s_next]
        q_s2_stable = q_s2_curr - np.max(q_s2_curr)
        exp_q2 = np.exp(beta_2 * q_s2_stable)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[t] = probs_2[action_2[t]]
        
        a2 = action_2[t]
        r = reward[t]
        last_reward = r
        
        # --- Updates ---
        pe1 = q_s2[s_next, a2] - q_mf1[a1]
        q_mf1[a1] += learning_rate * pe1
        
        pe2 = r - q_s2[s_next, a2]
        q_s2[s_next, a2] += learning_rate * pe2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Transition-Gated Model-Free Learning
This model introduces a mechanism where the Model-Free update for the first stage is modulated by the type of transition (Common vs. Rare). Intelligent agents might suppress Model-Free reinforcement after Rare transitions to avoid reinforcing a choice that led to the "wrong" state.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Transition-Gated Model-Free Learning.
    
    The Model-Free update for Stage 1 is modulated by whether the transition 
    was Common or Rare. This allows the model to dampen 'incorrect' reinforcement 
    that occurs when a choice leads to a rare state and is rewarded.
    
    Parameters:
    - learning_rate: [0, 1] Base learning rate.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - w: [0, 1] Weight for Model-Based control.
    - stickiness: [0, 5] Choice stickiness bias.
    - rare_weight: [0, 1] Multiplier for the learning rate on Rare transitions. 
                      (1.0 = standard MF, 0.0 = no update on rare).
    """
    learning_rate, beta_1, beta_2, w, stickiness, rare_weight = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_mf1 = np.zeros(2)
    q_s2 = np.zeros((2, 2))
    
    last_action_1 = -1
    
    for t in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_s2 = np.max(q_s2, axis=1)
        q_mb1 = transition_matrix @ max_q_s2
        
        q_net = w * q_mb1 + (1 - w) * q_mf1
        
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        q_net_stable = q_net - np.max(q_net)
        exp_q1 = np.exp(beta_1 * q_net_stable)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[t] = probs_1[action_1[t]]
        
        a1 = action_1[t]
        s_next = state[t]
        last_action_1 = a1
        
        # --- Stage 2 Policy ---
        q_s2_curr = q_s2[s_next]
        q_s2_stable = q_s2_curr - np.max(q_s2_curr)
        exp_q2 = np.exp(beta_2 * q_s2_stable)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[t] = probs_2[action_2[t]]
        
        a2 = action_2[t]
        r = reward[t]
        
        # --- Updates ---
        # Determine if transition was common or rare
        # 0->0 and 1->1 are Common. 0->1 and 1->0 are Rare.
        is_common = (a1 == s_next)
        
        # Modulate learning rate for Stage 1 based on transition type
        current_lr_s1 = learning_rate if is_common else (learning_rate * rare_weight)
        
        pe1 = q_s2[s_next, a2] - q_mf1[a1]
        q_mf1[a1] += current_lr_s1 * pe1
        
        # Stage 2 update is standard
        pe2 = r - q_s2[s_next, a2]
        q_s2[s_next, a2] += learning_rate * pe2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```