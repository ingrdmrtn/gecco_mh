Here are three new cognitive models for the two-step task.

### Model 1: Transition-Dependent Eligibility Trace Model
This model hypothesizes that the participant modulates their eligibility trace (how much credit the first choice gets for the second stage outcome) depending on whether the transition was Common or Rare. A sophisticated or heuristic-driven agent might inhibit learning from Rare transitions to avoid reinforcing the wrong action.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Transition-Dependent Eligibility Trace Model.
    
    Modulates the eligibility trace (lambda) based on whether the transition
    from Stage 1 to Stage 2 was Common or Rare. This allows the agent to 
    gate the credit assignment to the first stage choice depending on the 
    reliability of the transition.

    Parameters:
    lr: Learning rate [0,1]
    beta_1: Inverse temperature Stage 1 [0,10]
    beta_2: Inverse temperature Stage 2 [0,10]
    w: Model-based weight [0,1]
    p: Perseveration [0,1]
    lam_c: Eligibility trace for Common transitions [0,1]
    lam_r: Eligibility trace for Rare transitions [0,1]
    """
    lr, beta_1, beta_2, w, p, lam_c, lam_r = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1
    
    for trial in range(n_trials):
        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if prev_action_1 != -1:
            q_net_stage1[prev_action_1] += p
            
        exp_q1 = np.exp(beta_1 * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        s1_choice = action_1[trial]
        state_idx = state[trial]
        prev_action_1 = s1_choice
        
        # Stage 2 Policy
        s2_choice = action_2[trial]
        if s2_choice != -1:
            qs_stage2 = q_stage2_mf[state_idx]
            exp_q2 = np.exp(beta_2 * qs_stage2)
            probs_2 = exp_q2 / np.sum(exp_q2)
            p_choice_2[trial] = probs_2[s2_choice]
            
            # Updates
            r = reward[trial]
            
            # Stage 1 MF Update (TD(0))
            delta_stage1 = q_stage2_mf[state_idx, s2_choice] - q_stage1_mf[s1_choice]
            q_stage1_mf[s1_choice] += lr * delta_stage1
            
            # Stage 2 MF Update
            delta_stage2 = r - q_stage2_mf[state_idx, s2_choice]
            q_stage2_mf[state_idx, s2_choice] += lr * delta_stage2
            
            # Eligibility Trace Update (TD(1))
            # Determine if transition was common or rare
            # Common: 0->0, 1->1. Rare: 0->1, 1->0.
            is_common = (s1_choice == state_idx)
            lam = lam_c if is_common else lam_r
            
            q_stage1_mf[s1_choice] += lr * lam * delta_stage2
            
        else:
            p_choice_2[trial] = 1.0

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Option-Specific Learning Rate Model
This model allows for different learning rates for the two Stage 1 options (Spaceships). The participant data shows distinct blocks of behavior where they stick to Spaceship 0 or Spaceship 1 for different durations. This model posits that the volatility or habit formation associated with each spaceship is learned differently.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Option-Specific Learning Rate Model.
    
    Uses different learning rates for the two Stage 1 options (Spaceships).
    This captures potential differences in volatility or habit formation 
    associated with each specific option.

    Parameters:
    lr_s1_0: Learning rate for Spaceship 0 [0,1]
    lr_s1_1: Learning rate for Spaceship 1 [0,1]
    lr_s2: Learning rate for Stage 2 [0,1]
    beta_1: Inverse temperature Stage 1 [0,10]
    beta_2: Inverse temperature Stage 2 [0,10]
    w: Model-based weight [0,1]
    p: Perseveration [0,1]
    lam: Eligibility trace [0,1]
    """
    lr_s1_0, lr_s1_1, lr_s2, beta_1, beta_2, w, p, lam = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1
    
    for trial in range(n_trials):
        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if prev_action_1 != -1:
            q_net_stage1[prev_action_1] += p
            
        exp_q1 = np.exp(beta_1 * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        s1_choice = action_1[trial]
        state_idx = state[trial]
        prev_action_1 = s1_choice
        
        # Stage 2 Policy
        s2_choice = action_2[trial]
        if s2_choice != -1:
            qs_stage2 = q_stage2_mf[state_idx]
            exp_q2 = np.exp(beta_2 * qs_stage2)
            probs_2 = exp_q2 / np.sum(exp_q2)
            p_choice_2[trial] = probs_2[s2_choice]
            
            r = reward[trial]
            
            # Select learning rate for Stage 1 based on choice
            lr_s1 = lr_s1_0 if s1_choice == 0 else lr_s1_1
            
            # Stage 1 MF Update
            delta_stage1 = q_stage2_mf[state_idx, s2_choice] - q_stage1_mf[s1_choice]
            q_stage1_mf[s1_choice] += lr_s1 * delta_stage1
            
            # Stage 2 MF Update
            delta_stage2 = r - q_stage2_mf[state_idx, s2_choice]
            q_stage2_mf[state_idx, s2_choice] += lr_s2 * delta_stage2
            
            # Eligibility Trace Update (using Stage 1 specific LR)
            q_stage1_mf[s1_choice] += lr_s1 * lam * delta_stage2
        else:
            p_choice_2[trial] = 1.0

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Reward-Dependent Model-Based Weighting
This model assumes the participant shifts their strategy (arbitration between Model-Based and Model-Free systems) dynamically based on recent success. If they were rewarded on the previous trial, they use one weight (`w_win`), and if unrewarded, they use another (`w_loss`).

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Reward-Dependent Model-Based Weighting Model.
    
    The weight (w) assigned to the Model-Based system changes dynamically 
    based on the outcome of the previous trial (Win vs Loss). This simulates
    strategy switching based on immediate performance.
    
    Parameters:
    lr: Learning rate [0,1]
    beta_1: Inverse temperature Stage 1 [0,10]
    beta_2: Inverse temperature Stage 2 [0,10]
    w_win: Model-based weight after a reward [0,1]
    w_loss: Model-based weight after no reward [0,1]
    p: Perseveration [0,1]
    lam: Eligibility trace [0,1]
    """
    lr, beta_1, beta_2, w_win, w_loss, p, lam = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1
    prev_reward = 0 # Assume neutral/loss start
    
    for trial in range(n_trials):
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Dynamic w based on previous reward
        w = w_win if prev_reward == 1 else w_loss
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if prev_action_1 != -1:
            q_net_stage1[prev_action_1] += p
            
        exp_q1 = np.exp(beta_1 * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        s1_choice = action_1[trial]
        state_idx = state[trial]
        prev_action_1 = s1_choice
        
        s2_choice = action_2[trial]
        if s2_choice != -1:
            qs_stage2 = q_stage2_mf[state_idx]
            exp_q2 = np.exp(beta_2 * qs_stage2)
            probs_2 = exp_q2 / np.sum(exp_q2)
            p_choice_2[trial] = probs_2[s2_choice]
            
            r = reward[trial]
            prev_reward = r # Update for next trial
            
            # Updates
            delta_stage1 = q_stage2_mf[state_idx, s2_choice] - q_stage1_mf[s1_choice]
            q_stage1_mf[s1_choice] += lr * delta_stage1
            
            delta_stage2 = r - q_stage2_mf[state_idx, s2_choice]
            q_stage2_mf[state_idx, s2_choice] += lr * delta_stage2
            
            q_stage1_mf[s1_choice] += lr * lam * delta_stage2
        else:
            p_choice_2[trial] = 1.0
            prev_reward = 0

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```