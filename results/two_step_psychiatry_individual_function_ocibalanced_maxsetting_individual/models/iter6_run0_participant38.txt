Here are the 3 proposed cognitive models.

### Cognitive Model 1
```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Separate Stage 1 and Stage 2 Stickiness.
    
    This model distinguishes between perseveration (stickiness) in the spaceship 
    choice (Stage 1) and the alien choice (Stage 2). This allows the model to 
    capture the participant's tendency to repeat spaceship choices independently 
    of their tendency to repeat alien choices.
    
    Parameters:
    - lr: [0, 1] Learning rate for MF value updates.
    - beta_1: [0, 10] Inverse temperature for Stage 1 choices.
    - beta_2: [0, 10] Inverse temperature for Stage 2 choices.
    - w: [0, 1] Mixing weight. 0 = Pure MF, 1 = Pure MB.
    - stick_1: [0, 5] Stickiness parameter for Stage 1 (Spaceship).
    - stick_2: [0, 5] Stickiness parameter for Stage 2 (Alien).
    """
    lr, beta_1, beta_2, w, stick_1, stick_2 = model_parameters
    n_trials = len(action_1)

    # Fixed transition matrix as per task description
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    q_stage2_mf.fill(0.5) # Initialize with neutral values

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = -1
    prev_a2 = -1 # Needs to be tracked per state, or generally? 
                 # Usually stickiness is "repeat last motor action". 
                 # Given the task structure, we'll track last action taken in that specific state.
    prev_a2_per_state = [-1, -1]

    for trial in range(n_trials):
        # Handle missing/invalid data
        if action_1[trial] == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness to Stage 1
        q_net_1_stick = q_net_1.copy()
        if prev_a1 != -1:
            q_net_1_stick[prev_a1] += stick_1
            
        exp_q1 = np.exp(beta_1 * q_net_1_stick)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        s2 = state[trial]

        # --- Stage 2 Choice ---
        q_stage2_stick = q_stage2_mf[s2].copy()
        
        # Add stickiness to Stage 2 (repeat choice in this state)
        if prev_a2_per_state[s2] != -1:
            q_stage2_stick[prev_a2_per_state[s2]] += stick_2
            
        exp_q2 = np.exp(beta_2 * q_stage2_stick)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]

        # --- Value Updates ---
        # Stage 2 update (TD)
        delta_2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += lr * delta_2
        
        # Stage 1 update (TD)
        delta_1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_1
        
        # Update history
        prev_a1 = a1
        prev_a2_per_state[s2] = a2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2
```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Asymmetric Learning Rates and Unchosen Decay.
    
    Combines differential sensitivity to positive and negative prediction errors
    (asymmetric learning rates) with a decay mechanism for unchosen Stage 2 values.
    This helps the model adapt to drifting probabilities while capturing potential
    biases in learning from gains vs. non-gains.
    
    Parameters:
    - lr_pos: [0, 1] Learning rate for positive prediction errors.
    - lr_neg: [0, 1] Learning rate for negative prediction errors.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - w: [0, 1] Mixing weight (0=MF, 1=MB).
    - decay: [0, 1] Decay rate for unchosen Stage 2 options towards 0.5.
    """
    lr_pos, lr_neg, beta_1, beta_2, w, decay = model_parameters
    n_trials = len(action_1)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    q_stage2_mf.fill(0.5)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        if action_1[trial] == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_1 * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        s2 = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta_2 * q_stage2_mf[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]

        # --- Value Updates ---
        
        # Stage 2 Update
        delta_2 = r - q_stage2_mf[s2, a2]
        if delta_2 > 0:
            q_stage2_mf[s2, a2] += lr_pos * delta_2
        else:
            q_stage2_mf[s2, a2] += lr_neg * delta_2
            
        # Unchosen Decay for Stage 2
        unchosen_a2 = 1 - a2
        q_stage2_mf[s2, unchosen_a2] = (1 - decay) * q_stage2_mf[s2, unchosen_a2] + decay * 0.5

        # Stage 1 Update
        delta_1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        if delta_1 > 0:
            q_stage1_mf[a1] += lr_pos * delta_1
        else:
            q_stage1_mf[a1] += lr_neg * delta_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3
```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Dynamic Transition Learning Hybrid Model with Decay.
    
    In this model, the agent does not assume a fixed transition matrix (0.7/0.3).
    Instead, it learns the transition probabilities (Spaceship -> Planet) 
    dynamically based on experience. It also includes decay for Stage 2 values
    to handle the drifting reward probabilities.
    
    Parameters:
    - lr_val: [0, 1] Learning rate for value updates (MF).
    - lr_trans: [0, 1] Learning rate for transition probability updates.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - w: [0, 1] Mixing weight (0=MF, 1=MB).
    - decay: [0, 1] Decay rate for unchosen Stage 2 options.
    """
    lr_val, lr_trans, beta_1, beta_2, w, decay = model_parameters
    n_trials = len(action_1)

    # Initialize transition matrix to uniform (0.5), representing no prior knowledge
    # row 0: spaceship 0 -> [prob planet 0, prob planet 1]
    # row 1: spaceship 1 -> [prob planet 0, prob planet 1]
    trans_probs = np.array([[0.5, 0.5], [0.5, 0.5]])

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    q_stage2_mf.fill(0.5)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        if action_1[trial] == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # --- Stage 1 Choice ---
        # Calculate MB values using the *learned* transition matrix
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = trans_probs @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_1 * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        s2 = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta_2 * q_stage2_mf[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]

        # --- Updates ---
        
        # 1. Value Update (Stage 2)
        delta_2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += lr_val * delta_2
        
        # Unchosen Decay
        unchosen_a2 = 1 - a2
        q_stage2_mf[s2, unchosen_a2] = (1 - decay) * q_stage2_mf[s2, unchosen_a2] + decay * 0.5
        
        # 2. Value Update (Stage 1 MF)
        delta_1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr_val * delta_1
        
        # 3. Transition Probability Update
        # Update the row corresponding to the chosen spaceship (a1)
        # Move probability towards the observed state (s2)
        # The probability of the unobserved state becomes 1 - P(observed)
        trans_probs[a1, s2] += lr_trans * (1 - trans_probs[a1, s2])
        trans_probs[a1, 1 - s2] = 1.0 - trans_probs[a1, s2]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```