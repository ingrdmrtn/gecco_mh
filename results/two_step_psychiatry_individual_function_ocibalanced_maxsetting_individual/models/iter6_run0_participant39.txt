Here are three new cognitive models for the two-step task, designed to capture the participant's behavior through different mechanisms than those previously tried.

### Model 1: Reward-Modulated Inverse Temperature (Win-Focus)
This model hypothesizes that the participant's exploration/exploitation balance is dynamic. After a reward, the participant becomes more deterministic (exploiting the "winning" strategy), effectively increasing `beta`. After a non-reward, they become more stochastic (exploring), decreasing `beta`. This offers a mechanism for "Win-Stay, Lose-Switch" behavior mediated by decision noise rather than value updates.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    MB/MF hybrid with Reward-Modulated Inverse Temperature.
    
    The inverse temperature (beta) for the first stage choice is modulated by the 
    previous trial's reward. A reward increases beta (exploitation), while no 
    reward keeps it at baseline (exploration).
    
    Parameters:
    - lr: Learning rate [0,1]
    - beta_base: Baseline inverse temperature [0,10]
    - beta_bonus: Increase in beta after a reward [0,10]
    - w: Mixing weight (0=MF, 1=MB) [0,1]
    - stick: Choice stickiness [0,5]
    """
    lr, beta_base, beta_bonus, w, stick = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # 2 planets, 2 aliens
    
    log_loss = 0.0
    eps = 1e-10
    prev_action_1 = -1
    prev_reward = 0
    
    for trial in range(n_trials):
        # Stage 1 Choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if prev_action_1 != -1:
            q_net[prev_action_1] += stick
            
        # Modulate beta based on previous reward
        # If prev_reward is 1, beta is higher (more exploitative)
        current_beta = beta_base + beta_bonus * prev_reward
        
        exp_q1 = np.exp(current_beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = action_1[trial]
        log_loss -= np.log(probs_1[a1] + eps)
        
        prev_action_1 = a1
        
        # Stage 2 Choice
        a2 = action_2[trial]
        if a2 == -1: 
            continue
            
        state_idx = state[trial]
        qs_2 = q_stage2_mf[state_idx]
        
        # Use baseline beta for stage 2 to isolate stage 1 switching dynamics
        exp_q2 = np.exp(beta_base * qs_2) 
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        log_loss -= np.log(probs_2[a2] + eps)
        
        r = reward[trial]
        prev_reward = r if r >= 0 else 0 # Handle potential -1 reward as 0 for modulation
        
        # Updates
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += lr * delta_stage2
        
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
    return log_loss
```

### Model 2: Policy Mixture (Probability Mixing)
Instead of mixing Q-values (which can be sensitive to the scale of values), this model assumes the agent computes full probability distributions for the Model-Based and Model-Free strategies separately and then mixes the *probabilities*. This represents an agent who probabilistically selects a strategy (MB or MF) at the start of the trial rather than integrating information into a single value.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid model using Policy Mixing (Probability Mixture) instead of Value Mixing.
    
    Computes probabilities for MB and MF strategies separately and blends them.
    Stickiness is applied to the values before probability calculation.
    
    Parameters:
    - lr: Learning rate [0,1]
    - beta: Inverse temperature [0,10]
    - w: Mixing weight for MB policy (probability of using MB strategy) [0,1]
    - stick: Choice stickiness [0,5]
    """
    lr, beta, w, stick = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    log_loss = 0.0
    eps = 1e-10
    prev_action_1 = -1
    
    for trial in range(n_trials):
        # Stage 1 Choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Apply stickiness to values before softmax
        q_mb_mod = q_stage1_mb.copy()
        q_mf_mod = q_stage1_mf.copy()
        
        if prev_action_1 != -1:
            q_mb_mod[prev_action_1] += stick
            q_mf_mod[prev_action_1] += stick
            
        # Calculate policy for MB
        exp_q1_mb = np.exp(beta * q_mb_mod)
        probs_1_mb = exp_q1_mb / np.sum(exp_q1_mb)
        
        # Calculate policy for MF
        exp_q1_mf = np.exp(beta * q_mf_mod)
        probs_1_mf = exp_q1_mf / np.sum(exp_q1_mf)
        
        # Mix Policies
        probs_1 = w * probs_1_mb + (1 - w) * probs_1_mf
        
        a1 = action_1[trial]
        log_loss -= np.log(probs_1[a1] + eps)
        
        prev_action_1 = a1
        
        # Stage 2 Choice
        a2 = action_2[trial]
        if a2 == -1:
            continue
            
        state_idx = state[trial]
        
        qs_2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        log_loss -= np.log(probs_2[a2] + eps)
        
        r = reward[trial]
        
        # Updates
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += lr * delta_stage2
        
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
    return log_loss
```

### Model 3: Stage-Specific Learning Rates
This model separates the learning rate for the "Spaceships" (Stage 1) from the "Aliens" (Stage 2). The aliens' reward probabilities drift slowly, while the spaceships' values are aggregates of these. Separating the learning rates allows the model to capture different timescales of update for the bandit task (Stage 2) versus the structural choice (Stage 1).

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    MB/MF hybrid with Stage-Specific Learning Rates.
    
    Uses separate learning rates for updating Stage 1 (Spaceship) values and 
    Stage 2 (Alien) values, allowing for different update speeds for the 
    drifting reward probabilities vs the action values.
    
    Parameters:
    - lr_1: Learning rate for Stage 1 (Spaceships) [0,1]
    - lr_2: Learning rate for Stage 2 (Aliens) [0,1]
    - beta: Inverse temperature (shared) [0,10]
    - w: Mixing weight (0=MF, 1=MB) [0,1]
    - stick: Choice stickiness [0,5]
    """
    lr_1, lr_2, beta, w, stick = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    log_loss = 0.0
    eps = 1e-10
    prev_action_1 = -1
    
    for trial in range(n_trials):
        # Stage 1 Choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if prev_action_1 != -1:
            q_net[prev_action_1] += stick
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = action_1[trial]
        log_loss -= np.log(probs_1[a1] + eps)
        
        prev_action_1 = a1
        
        # Stage 2 Choice
        a2 = action_2[trial]
        if a2 == -1:
            continue
            
        state_idx = state[trial]
        
        qs_2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        log_loss -= np.log(probs_2[a2] + eps)
        
        r = reward[trial]
        
        # Updates
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        # Update Stage 2 with lr_2
        q_stage2_mf[state_idx, a2] += lr_2 * delta_stage2
        
        # Update Stage 1 with lr_1
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr_1 * delta_stage1
        
    return log_loss
```