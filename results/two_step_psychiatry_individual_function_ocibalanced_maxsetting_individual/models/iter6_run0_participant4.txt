Here are three cognitive models expressed as Python functions.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model (MB+MF) with Choice Kernel.
    
    This model combines Model-Based (MB) planning and Model-Free (MF) learning.
    Instead of simple 1-step stickiness, it uses a 'Choice Kernel' to track the 
    history of choices, creating a habit strength that decays over time. 
    This accounts for the participant's tendency to stick with a spaceship 
    choice for long streaks (perseveration).

    Parameters:
    learning_rate: [0, 1] Learning rate for MF Q-value updates.
    beta_1: [0, 10] Inverse temperature for Stage 1 choices.
    beta_2: [0, 10] Inverse temperature for Stage 2 choices.
    w: [0, 1] Weighting parameter (0 = Pure MF, 1 = Pure MB).
    k_decay: [0, 1] Decay rate of the choice kernel (1 = instant forget, 0 = no decay).
    k_weight: [0, 5] Weight of the choice kernel in the decision policy.
    """
    learning_rate, beta_1, beta_2, w, k_decay, k_weight = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    choice_kernel = np.zeros(2)

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # Model-Based Value (Bellman equation)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Policy: Value + Habit (Kernel)
        logits_1 = beta_1 * q_net + k_weight * choice_kernel
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Update Choice Kernel (Accumulating trace)
        choice_kernel *= (1 - k_decay)
        choice_kernel[action_1[trial]] += 1.0

        state_idx = state[trial]
        a2 = action_2[trial]
        
        # --- Stage 2 Decision ---
        if a2 != -1:
            logits_2 = beta_2 * q_stage2_mf[state_idx]
            exp_q2 = np.exp(logits_2 - np.max(logits_2))
            probs_2 = exp_q2 / np.sum(exp_q2)
            p_choice_2[trial] = probs_2[a2]
            
            # --- Learning ---
            # Update Stage 1 MF (TD(0))
            pe_1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[action_1[trial]]
            q_stage1_mf[action_1[trial]] += learning_rate * pe_1
            
            # Update Stage 2 MF (TD(1))
            pe_2 = reward[trial] - q_stage2_mf[state_idx, a2]
            q_stage2_mf[state_idx, a2] += learning_rate * pe_2
        else:
            p_choice_2[trial] = 1.0

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Learner with Cross-State Generalization.
    
    The agent learns via Model-Free TD prediction errors but assumes a structure 
    where aliens with the same index (e.g., Alien 0) across different planets share 
    properties. When updating the value of an alien on the current planet, the agent 
    also updates the value of the corresponding alien on the other planet by a 
    fraction (generalization parameter).
    
    Parameters:
    learning_rate: [0, 1] Learning rate.
    beta_1: [0, 10] Inverse temperature for Stage 1.
    beta_2: [0, 10] Inverse temperature for Stage 2.
    generalization: [0, 1] Fraction of learning transferred to the other state.
    stickiness: [0, 5] Perseveration bonus for Stage 1 (1-step).
    """
    learning_rate, beta_1, beta_2, generalization, stickiness = model_parameters
    n_trials = len(action_1)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    prev_a1 = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        logits_1 = beta_1 * q_stage1
        if prev_a1 != -1:
            logits_1[prev_a1] += stickiness
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        prev_a1 = action_1[trial]
        state_idx = state[trial]
        a2 = action_2[trial]
        
        # --- Stage 2 Decision ---
        if a2 != -1:
            logits_2 = beta_2 * q_stage2[state_idx]
            exp_q2 = np.exp(logits_2 - np.max(logits_2))
            probs_2 = exp_q2 / np.sum(exp_q2)
            p_choice_2[trial] = probs_2[a2]
            
            # --- Learning ---
            # Update Stage 1 (TD(0))
            pe_1 = q_stage2[state_idx, a2] - q_stage1[action_1[trial]]
            q_stage1[action_1[trial]] += learning_rate * pe_1
            
            # Update Stage 2
            pe_2 = reward[trial] - q_stage2[state_idx, a2]
            q_stage2[state_idx, a2] += learning_rate * pe_2
            
            # Generalization: Update the same action in the OTHER state
            other_state = 1 - state_idx
            q_stage2[other_state, a2] += learning_rate * generalization * pe_2
            
        else:
            p_choice_2[trial] = 1.0

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Based Learner with Forgetting and Choice Kernel.
    
    This model assumes the participant uses the transition structure (Model-Based)
    but faces a volatile environment where old information becomes unreliable 
    (Forgetting of Stage 2 values). It replaces simple stickiness with a Choice 
    Kernel to better explain the long behavioral streaks observed in the data.
    
    Parameters:
    learning_rate: [0, 1] Learning rate for Stage 2 updates.
    beta_1: [0, 10] Inverse temperature for Stage 1.
    beta_2: [0, 10] Inverse temperature for Stage 2.
    forget_rate: [0, 1] Decay rate applied to all Stage 2 Q-values each trial.
    k_decay: [0, 1] Decay rate of the choice kernel.
    k_weight: [0, 5] Weight of the choice kernel.
    """
    learning_rate, beta_1, beta_2, forget_rate, k_decay, k_weight = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage2 = np.zeros((2, 2))
    choice_kernel = np.zeros(2)
    
    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # Model-Based Value calculation
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Policy: MB Value + Habit (Kernel)
        logits_1 = beta_1 * q_stage1_mb + k_weight * choice_kernel
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Update Choice Kernel
        choice_kernel *= (1 - k_decay)
        choice_kernel[action_1[trial]] += 1.0
        
        state_idx = state[trial]
        a2 = action_2[trial]
        
        # --- Stage 2 Decision ---
        if a2 != -1:
            logits_2 = beta_2 * q_stage2[state_idx]
            exp_q2 = np.exp(logits_2 - np.max(logits_2))
            probs_2 = exp_q2 / np.sum(exp_q2)
            p_choice_2[trial] = probs_2[a2]
            
            # Forgetting: Decay values towards 0 before update
            q_stage2 *= (1 - forget_rate)
            
            # Learning: Update chosen option
            pe = reward[trial] - q_stage2[state_idx, a2]
            q_stage2[state_idx, a2] += learning_rate * pe
        else:
            p_choice_2[trial] = 1.0
            # Apply forgetting even if no choice was made
            q_stage2 *= (1 - forget_rate)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```