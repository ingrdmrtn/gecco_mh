Here are three cognitive models based on the participant's behavior and the two-step task structure.

### Model 1: Hybrid MB/MF with Stage-Specific Inverse Temperatures
This model hypothesizes that the participant's exploration-exploitation balance differs between the high-level planning stage (choosing a spaceship) and the low-level bandit stage (choosing an alien). It uses a hybrid Model-Based/Model-Free architecture but assigns distinct `beta` (inverse temperature) parameters to each stage. This allows the model to capture if the participant is, for example, more random in the second stage while being more deterministic in the first, or vice versa.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF Model with Stage-Specific Inverse Temperatures.
    
    Distinguishes between exploration levels in Stage 1 (Spaceship) and Stage 2 (Alien).
    
    Parameters:
    - learning_rate: [0, 1] Update rate for Q-values.
    - beta_1: [0, 10] Inverse temperature for Stage 1 choice.
    - beta_2: [0, 10] Inverse temperature for Stage 2 choice.
    - w: [0, 1] Weight of Model-Based values in Stage 1 (0=Pure MF, 1=Pure MB).
    """
    learning_rate, beta_1, beta_2, w = model_parameters
    n_trials = len(action_1)
    
    # Fixed transition probabilities as per task description
    # Row 0: Spaceship 0 (A) -> [Planet 0 (X), Planet 1 (Y)]
    # Row 1: Spaceship 1 (U) -> [Planet 0 (X), Planet 1 (Y)]
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)      # Model-Free values for Stage 1
    q_stage2_mf = np.zeros((2, 2)) # Model-Free values for Stage 2 (2 planets, 2 aliens)
    
    # Initialize Stage 2 Q-values to 0.5 (midpoint) to prevent strong initial bias
    q_stage2_mf.fill(0.5)
    q_stage1_mf.fill(0.5)

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # Model-Based Value Calculation: T * max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax with beta_1
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Decision ---
        s = state[trial] # Planet arrived at
        
        # Softmax with beta_2
        exp_q2 = np.exp(beta_2 * q_stage2_mf[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning ---
        # Stage 1 Update (TD)
        # Note: Standard hybrid models often update MF Q1 using Q2 as target
        target_stage1 = q_stage2_mf[s, action_2[trial]]
        delta1 = target_stage1 - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta1
        
        # Stage 2 Update (Reward)
        delta2 = reward[trial] - q_stage2_mf[s, action_2[trial]]
        q_stage2_mf[s, action_2[trial]] += learning_rate * delta2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Model-Free Q-Learning with Forgetting and Perseverance
This model introduces a **forgetting** mechanism (decay) for unchosen actions. In standard Q-learning, the Q-value of an unchosen option remains static. Here, unchosen options slowly decay, which can explain shifts in preference over long blocks of trials. It also includes a simple perseverance parameter to account for the participant's "stickiness" to choices.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    MF Q-Learning with Forgetting (Decay) and Perseverance.
    
    Unchosen Q-values decay towards 0, simulating memory loss or passive devaluation.
    Includes a perseverance bonus for the previously chosen action.
    
    Parameters:
    - learning_rate: [0, 1] Update rate for chosen Q-values.
    - beta: [0, 10] Inverse temperature.
    - decay: [0, 1] Decay rate for unchosen Q-values (0=no decay, 1=instant forget).
    - perseverance: [0, 5] Additional bias added to the logits of the repeated choice.
    """
    learning_rate, beta, decay, perseverance = model_parameters
    n_trials = len(action_1)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    # Initialize with 0.5
    q_stage1.fill(0.5)
    q_stage2.fill(0.5)
    
    prev_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        logits = beta * q_stage1
        
        # Add perseverance bonus
        if prev_action_1 != -1:
            logits[prev_action_1] += perseverance
            
        exp_q1 = np.exp(logits)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Decision ---
        s = state[trial]
        exp_q2 = np.exp(beta * q_stage2[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning ---
        # Stage 1 Update
        a1 = action_1[trial]
        target_stage1 = q_stage2[s, action_2[trial]]
        
        # Update chosen
        q_stage1[a1] += learning_rate * (target_stage1 - q_stage1[a1])
        # Decay unchosen
        q_stage1[1-a1] *= (1.0 - decay)
        
        # Stage 2 Update
        a2 = action_2[trial]
        # Update chosen
        q_stage2[s, a2] += learning_rate * (reward[trial] - q_stage2[s, a2])
        # Decay unchosen (in the current state)
        q_stage2[s, 1-a2] *= (1.0 - decay)
        
        prev_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Model-Free Q-Learning with Outcome-Dependent Perseverance
This model refines the concept of perseverance by splitting it into **Win-Stay** and **Lose-Stay** components. The strength of the "stickiness" to the previous choice depends on whether the previous trial was rewarded. This allows the model to capture behaviors where the participant might be very sticky after a win but more flexible (or conversely, still sticky) after a loss.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    MF Q-Learning with Outcome-Dependent Perseverance.
    
    Applies different perseverance bonuses depending on whether the previous trial
    resulted in a reward (Win) or no reward (Loss).
    
    Parameters:
    - learning_rate: [0, 1] Update rate for Q-values.
    - beta: [0, 10] Inverse temperature.
    - p_win: [0, 5] Perseverance bonus after a rewarded trial.
    - p_loss: [0, 5] Perseverance bonus after an unrewarded trial.
    """
    learning_rate, beta, p_win, p_loss = model_parameters
    n_trials = len(action_1)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    q_stage1.fill(0.5)
    q_stage2.fill(0.5)
    
    prev_action_1 = -1
    prev_reward = 0 # Assume loss/neutral for first trial context

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        logits = beta * q_stage1
        
        # Add outcome-dependent perseverance bonus
        if prev_action_1 != -1:
            if prev_reward == 1:
                logits[prev_action_1] += p_win
            else:
                logits[prev_action_1] += p_loss
            
        exp_q1 = np.exp(logits)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Decision ---
        s = state[trial]
        exp_q2 = np.exp(beta * q_stage2[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning ---
        # Stage 1
        target_stage1 = q_stage2[s, action_2[trial]]
        q_stage1[action_1[trial]] += learning_rate * (target_stage1 - q_stage1[action_1[trial]])
        
        # Stage 2
        q_stage2[s, action_2[trial]] += learning_rate * (reward[trial] - q_stage2[s, action_2[trial]])
        
        prev_action_1 = action_1[trial]
        prev_reward = reward[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```