Here are three new cognitive models for the two-step task, designed to capture different potential mechanisms underlying the participant's behavior.

### Model 1: Asymmetric Learning Rate Model-Free Q-Learning
This model posits that the participant learns differently from positive prediction errors (better than expected) versus negative prediction errors (worse than expected). This "positivity bias" or "negativity bias" is combined with eligibility traces and simple stickiness.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Q-Learning with Asymmetric Learning Rates.
    
    Distinguishes between learning from positive prediction errors (alpha_pos)
    and negative prediction errors (alpha_neg).
    
    Parameters:
    alpha_pos: [0,1] - Learning rate for positive prediction errors (delta > 0).
    alpha_neg: [0,1] - Learning rate for negative prediction errors (delta < 0).
    beta_1: [0,10] - Inverse temperature for Stage 1 choices.
    beta_2: [0,10] - Inverse temperature for Stage 2 choices.
    lambda_param: [0,1] - Eligibility trace parameter connecting Stage 2 outcome to Stage 1.
    stickiness: [0,10] - Choice perseverance bonus for Stage 1.
    """
    alpha_pos, alpha_neg, beta_1, beta_2, lambda_param, stickiness = model_parameters
    n_trials = len(action_1)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_a1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        logits_1 = beta_1 * q_stage1_mf
        
        # Apply stickiness
        if prev_a1 != -1:
            logits_1[prev_a1] += stickiness
            
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        if np.isnan(action_1[trial]):
            p_choice_1[trial] = 0.5
            prev_a1 = -1
            continue
            
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        prev_a1 = a1
        
        state_idx = int(state[trial])

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        if np.isnan(action_2[trial]):
            p_choice_2[trial] = 0.5
            continue
            
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        r = reward[trial]

        # --- Updates ---
        # Stage 1 prediction error (TD(0))
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        lr_1 = alpha_pos if delta_stage1 > 0 else alpha_neg
        q_stage1_mf[a1] += lr_1 * delta_stage1
        
        # Stage 2 prediction error
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        lr_2 = alpha_pos if delta_stage2 > 0 else alpha_neg
        q_stage2_mf[state_idx, a2] += lr_2 * delta_stage2
        
        # Eligibility trace update for Stage 1 based on Stage 2 outcome
        # Note: We use the same asymmetry logic for the trace update
        lr_trace = alpha_pos if delta_stage2 > 0 else alpha_neg
        q_stage1_mf[a1] += lr_trace * lambda_param * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model with Dynamic Transition Learning
This model assumes the participant is essentially Model-Based (or Hybrid) but does not trust the fixed transition probabilities (70/30). Instead, they update their internal model of the spaceship-planet transitions based on experience.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Dynamic Transition Learning.
    
    The agent learns the transition matrix P(State|Action) online.
    
    Parameters:
    lr_value: [0,1] - Learning rate for Q-values (MF and Stage 2).
    lr_trans: [0,1] - Learning rate for the transition matrix.
    beta_1: [0,10] - Inverse temperature for Stage 1.
    beta_2: [0,10] - Inverse temperature for Stage 2.
    w: [0,1] - Weighting parameter (0=Pure MF, 1=Pure MB).
    stickiness: [0,10] - Choice perseverance bonus.
    """
    lr_value, lr_trans, beta_1, beta_2, w, stickiness = model_parameters
    n_trials = len(action_1)
    
    # Initialize transition matrix: [Action, NextState]
    # Start with the true prior, but allow it to drift
    trans_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_mf = np.zeros(2) # Stage 1 MF values
    q_stage2 = np.zeros((2, 2)) # Stage 2 values (shared by MB and MF)
    
    prev_a1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based Value: Expected max value of next stage
        max_q2 = np.max(q_stage2, axis=1) # [max(Q(s0)), max(Q(s1))]
        q_mb = trans_matrix @ max_q2
        
        # Hybrid Value
        q_net = w * q_mb + (1 - w) * q_mf
        
        logits_1 = beta_1 * q_net
        if prev_a1 != -1:
            logits_1[prev_a1] += stickiness
            
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        if np.isnan(action_1[trial]):
            p_choice_1[trial] = 0.5
            prev_a1 = -1
            continue
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        prev_a1 = a1
        
        state_idx = int(state[trial])

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta_2 * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        if np.isnan(action_2[trial]):
            p_choice_2[trial] = 0.5
            continue
            
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        r = reward[trial]

        # --- Updates ---
        # 1. Update Transition Matrix
        # Move probability mass towards the observed transition
        # T[a, s'] = T[a, s'] + lr_trans * (1 - T[a, s'])
        trans_matrix[a1, state_idx] += lr_trans * (1.0 - trans_matrix[a1, state_idx])
        # Decrease probability of the unobserved transition to keep sum=1
        trans_matrix[a1, 1-state_idx] += lr_trans * (0.0 - trans_matrix[a1, 1-state_idx])
        
        # 2. Update Stage 2 Values
        delta_2 = r - q_stage2[state_idx, a2]
        q_stage2[state_idx, a2] += lr_value * delta_2
        
        # 3. Update Stage 1 MF Values (TD(1) style, direct reward update)
        delta_1 = r - q_mf[a1]
        q_mf[a1] += lr_value * delta_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model with Outcome-Dependent Stickiness
This model combines the Model-Based/Model-Free mixture with the specific "win-stay, lose-switch" (outcome-dependent) stickiness pattern found to be effective in pure MF models. It tests if the participant uses a hybrid strategy that is masked by strong outcome-dependent repetition.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Outcome-Dependent Stickiness.
    
    Combines MB/MF weighting with separate stickiness parameters for 
    rewarded (win) and unrewarded (lose) previous trials.
    
    Parameters:
    learning_rate: [0,1] - Update rate for Q-values.
    beta_1: [0,10] - Inverse temperature for Stage 1.
    beta_2: [0,10] - Inverse temperature for Stage 2.
    w: [0,1] - Weighting parameter (0=Pure MF, 1=Pure MB).
    sticky_win: [0,10] - Stickiness bonus after a Reward=1.
    sticky_lose: [0,10] - Stickiness bonus after a Reward!=1.
    """
    learning_rate, beta_1, beta_2, w, sticky_win, sticky_lose = model_parameters
    n_trials = len(action_1)
    
    # Fixed transition matrix for MB calculation
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_mf = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    prev_a1 = -1
    prev_reward = -1

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # MB Component
        max_q2 = np.max(q_stage2, axis=1)
        q_mb = transition_matrix @ max_q2
        
        # Hybrid Mix
        q_net = w * q_mb + (1 - w) * q_mf
        
        logits_1 = beta_1 * q_net
        
        # Outcome-Dependent Stickiness
        if prev_a1 != -1:
            if prev_reward == 1:
                logits_1[prev_a1] += sticky_win
            else:
                logits_1[prev_a1] += sticky_lose
            
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        if np.isnan(action_1[trial]):
            p_choice_1[trial] = 0.5
            prev_a1 = -1
            continue
            
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        prev_a1 = a1
        
        state_idx = int(state[trial])

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta_2 * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        if np.isnan(action_2[trial]):
            p_choice_2[trial] = 0.5
            continue
            
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        r = reward[trial]
        prev_reward = r

        # --- Updates ---
        # Stage 2 Value Update
        delta_2 = r - q_stage2[state_idx, a2]
        q_stage2[state_idx, a2] += learning_rate * delta_2
        
        # Stage 1 MF Update (Direct reward update)
        delta_1 = r - q_mf[a1]
        q_mf[a1] += learning_rate * delta_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```