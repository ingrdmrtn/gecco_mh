Here are three new cognitive models for the two-step decision task, designed to be distinct from previously tried parameter combinations while addressing the participant's behavioral patterns (streaks, switching, and reward sensitivity).

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF with Eligibility Trace (Lambda) and Decay.
    
    Combines the eligibility trace mechanism (allowing stage 2 outcomes to directly 
    update stage 1 values via MF) with value decay (forgetting) for unchosen options.
    This tests if the participant connects outcomes to initial choices (via Lambda)
    while simultaneously forgetting the value of unchosen options over time.
    
    Parameters:
    - lr: [0, 1] Learning rate for Q-value updates.
    - decay: [0, 1] Decay rate for unchosen Q-values (pulls towards 0.5).
    - beta_1: [0, 10] Inverse temperature for Stage 1 softmax.
    - beta_2: [0, 10] Inverse temperature for Stage 2 softmax.
    - w: [0, 1] Weight for Model-Based values (0=MF, 1=MB).
    - lam: [0, 1] Eligibility trace parameter (Lambda), scaling the Stage 2 RPE effect on Stage 1.
    - pers: [0, 1] Perseveration bonus for repeating the previous Stage 1 choice.
    """
    lr, decay, beta_1, beta_2, w, lam, pers = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if prev_action_1 != -1:
            q_net[prev_action_1] += pers
            
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        prev_action_1 = a1
        state_idx = state[trial]
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]
        
        # --- Updates ---
        # Stage 1 MF Update with Eligibility Trace
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        
        # Q(s1,a1) updated by immediate prediction error + lambda * stage 2 prediction error
        q_stage1_mf[a1] += lr * delta_stage1 + lr * lam * delta_stage2
        
        # Decay unchosen Stage 1
        unchosen_a1 = 1 - a1
        q_stage1_mf[unchosen_a1] = (1 - decay) * q_stage1_mf[unchosen_a1] + decay * 0.5
        
        # Stage 2 MF Update
        q_stage2_mf[state_idx, a2] += lr * delta_stage2
        
        # Decay unchosen Stage 2
        unchosen_a2 = 1 - a2
        q_stage2_mf[state_idx, unchosen_a2] = (1 - decay) * q_stage2_mf[state_idx, unchosen_a2] + decay * 0.5

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Asymmetric Learning with Outcome-Dependent Perseveration.
    
    Differentiates learning from positive vs negative prediction errors (Asymmetric Learning)
    AND applies different perseveration bonuses depending on whether the previous trial 
    was rewarded. This captures 'Win-Stay' vs 'Lose-Stay' dynamics distinctly from 
    value learning.
    
    Parameters:
    - lr_pos: [0, 1] Learning rate for positive prediction errors.
    - lr_neg: [0, 1] Learning rate for negative prediction errors.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - w: [0, 1] Weight for Model-Based values.
    - pers_rew: [0, 1] Perseveration bonus if the previous trial yielded a reward.
    - pers_unrew: [0, 1] Perseveration bonus if the previous trial yielded no reward.
    """
    lr_pos, lr_neg, beta_1, beta_2, w, pers_rew, pers_unrew = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1
    prev_reward = 0
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if prev_action_1 != -1:
            if prev_reward == 1:
                q_net[prev_action_1] += pers_rew
            else:
                q_net[prev_action_1] += pers_unrew
                
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        prev_action_1 = a1
        state_idx = state[trial]
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]
        prev_reward = r
        
        # --- Updates ---
        # Stage 1 Update with Asymmetric LR
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        alpha_1 = lr_pos if delta_stage1 > 0 else lr_neg
        q_stage1_mf[a1] += alpha_1 * delta_stage1
        
        # Stage 2 Update with Asymmetric LR
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        alpha_2 = lr_pos if delta_stage2 > 0 else lr_neg
        q_stage2_mf[state_idx, a2] += alpha_2 * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF with Choice Kernel and Value Decay.
    
    Replaces the simple 1-step perseveration with a Choice Kernel (habit trace) that 
    accumulates and decays over time, modeling the 'momentum' of choices seen in the 
    data. This is combined with value decay for Q-values to handle non-stationary rewards.
    
    Parameters:
    - lr: [0, 1] Learning rate for Q-values.
    - decay: [0, 1] Decay rate for unchosen Q-values (pulls towards 0.5).
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - w: [0, 1] Weight for Model-Based values.
    - ck_lr: [0, 1] Learning rate for the Choice Kernel (how fast habits form/decay).
    - ck_w: [0, 10] Weight for the Choice Kernel in the decision function.
    """
    lr, decay, beta_1, beta_2, w, ck_lr, ck_w = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    choice_kernel = np.zeros(2)
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combine MB, MF, and Choice Kernel
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf + ck_w * choice_kernel
        
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        state_idx = state[trial]
        
        # Update Choice Kernel (Leaky integration of choices)
        # ck[a] <- (1-ck_lr)*ck[a] + ck_lr * 1 (if chosen)
        # ck[a] <- (1-ck_lr)*ck[a] (if not chosen)
        choice_kernel[a1] = (1 - ck_lr) * choice_kernel[a1] + ck_lr * 1
        choice_kernel[1 - a1] = (1 - ck_lr) * choice_kernel[1 - a1]
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]
        
        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        unchosen_a1 = 1 - a1
        q_stage1_mf[unchosen_a1] = (1 - decay) * q_stage1_mf[unchosen_a1] + decay * 0.5
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += lr * delta_stage2
        
        unchosen_a2 = 1 - a2
        q_stage2_mf[state_idx, unchosen_a2] = (1 - decay) * q_stage2_mf[state_idx, unchosen_a2] + decay * 0.5

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```