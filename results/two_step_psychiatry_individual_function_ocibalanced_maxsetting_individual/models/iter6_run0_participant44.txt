Here are three new cognitive models for the two-step task.

### Model 1: Hybrid Learner with Inverse Recency Exploration
This model posits that the participant is motivated by novelty or curiosity. In addition to the standard Model-Based (MB) and Model-Free (MF) value estimation, the agent adds an "exploration bonus" to actions in Stage 1 based on how long it has been since they were last chosen. This captures a tendency to periodically check the unchosen spaceship, preventing premature convergence to a local optimum.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid learner with Inverse Recency Exploration Bonus.
    
    Incorporates an exploration bonus that increases logarithmically with the 
    number of trials since an action was last chosen. This promotes visiting 
    options that have been neglected.
    
    Parameters:
    - lr: [0, 1] Learning rate for value updates.
    - beta: [0, 10] Inverse temperature (softness of choice).
    - w: [0, 1] Weighting parameter (0=Pure MF, 1=Pure MB).
    - phi: [0, 5] Exploration bonus scaling factor.
    """
    lr, beta, w, phi = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track the last trial each Stage 1 action was chosen. Initialize to -1.
    last_chosen_s1 = np.array([-1, -1])

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Calculate exploration bonus: phi * log(time_since_last_choice + 1)
        # time_since = trial - last_chosen
        time_since = trial - last_chosen_s1
        expl_bonus = phi * np.log(time_since + 1)
        
        # Combine Q-values with exploration bonus
        logits_1 = beta * q_net + expl_bonus
        
        # Softmax for Stage 1
        exp_q1 = np.exp(logits_1 - np.max(logits_1)) # subtract max for numerical stability
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Update recency tracker
        last_chosen_s1[action_1[trial]] = trial
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        # Standard Softmax on Stage 2 MF values
        logits_2 = beta * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning Updates ---
        # Stage 1 Update (TD-0)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr * delta_stage1
        
        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid with Outcome-Dependent Control Weights
This model proposes that the balance between Model-Based (planning) and Model-Free (habit) systems is dynamic and depends on recent success. Specifically, the parameter `w` switches between `w_win` (after a reward) and `w_loss` (after an omission). This captures the hypothesis that the participant may rely more on their model (or conversely, their habits) when under the stress of a loss versus the validation of a win.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid learner with Outcome-Dependent Strategy Weighting.
    
    The mixing weight 'w' adapts based on the previous trial's outcome.
    Allows the agent to shift between Model-Based and Model-Free control 
    differently after wins vs. losses.
    
    Parameters:
    - lr: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w_win: [0, 1] MB weight applied if the previous trial was rewarded.
    - w_loss: [0, 1] MB weight applied if the previous trial was unrewarded.
    """
    lr, beta, w_win, w_loss = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_reward = 0 # Initialize assuming no reward (or neutral)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Select current w based on previous reward
        current_w = w_win if prev_reward == 1 else w_loss
        
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = current_w * q_stage1_mb + (1 - current_w) * q_stage1_mf
        
        logits_1 = beta * q_net
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        logits_2 = beta * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr * delta_stage2
        
        # Update memory of previous reward
        prev_reward = reward[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid with Hierarchical Stickiness
This model introduces separate perseverance (stickiness) parameters for the two stages of the task. It accounts for the possibility that habit formation—the tendency to repeat a choice regardless of value—operates differently when choosing a spaceship (Stage 1) versus choosing a specific alien (Stage 2). 

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid learner with Hierarchical Perseverance (Stage 1 and Stage 2).
    
    Applies distinct stickiness biases to the spaceship choice and the 
    alien choice. This disentangles high-level choice inertia from 
    low-level motor/object perseverance.
    
    Parameters:
    - lr: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] MB/MF weight.
    - stick_1: [0, 5] Perseverance bonus for Stage 1 (Spaceship).
    - stick_2: [0, 5] Perseverance bonus for Stage 2 (Alien).
    """
    lr, beta, w, stick_1, stick_2 = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track previous choices. Initialize to -1 (no previous choice).
    prev_a1 = -1
    # Track previous choice for Stage 2 separately for each state (Planet X vs Y)
    prev_a2 = np.array([-1, -1]) 

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Calculate logits and add stickiness if applicable
        logits_1 = beta * q_net
        if prev_a1 != -1:
            logits_1[prev_a1] += stick_1
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        logits_2 = beta * q_stage2_mf[state_idx]
        
        # Add stickiness for the specific alien chosen last time IN THIS STATE
        if prev_a2[state_idx] != -1:
            logits_2[prev_a2[state_idx]] += stick_2
            
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr * delta_stage2
        
        # Update history
        prev_a1 = action_1[trial]
        prev_a2[state_idx] = action_2[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```