Here are the three proposed cognitive models.

### Model 1: Hybrid Model with Asymmetric Learning Rates and Stickiness
This model hypothesizes that the participant integrates both Model-Based (planning) and Model-Free (habitual) value estimation, but updates their Model-Free values differently depending on whether the prediction error is positive (better than expected) or negative (worse than expected). This asymmetry captures potential biases like loss aversion or optimism. It also includes a stickiness parameter to account for choice perseveration.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Asymmetric Learning Rates and Stickiness.
    
    Combines Model-Based and Model-Free learning with separate learning rates
    for positive and negative prediction errors, and a stickiness bonus.
    
    Parameters:
    - alpha_pos: [0, 1] Learning rate for positive prediction errors.
    - alpha_neg: [0, 1] Learning rate for negative prediction errors.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - w: [0, 1] Mixing weight (0=MF, 1=MB).
    - stickiness: [0, 5] Bonus for repeating the last Stage 1 choice.
    """
    alpha_pos, alpha_neg, beta_1, beta_2, w, stickiness = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        # Handle missing data
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # Policy for the first choice
        # Model-Based value calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid value integration with stickiness
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        if last_action_1 != -1:
            q_net_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta_1 * (q_net_1 - np.max(q_net_1)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_action_1 = action_1[trial]
        state_idx = state[trial]

        # Policy for the second choice (Pure Model-Free)
        exp_q2 = np.exp(beta_2 * (q_stage2_mf[state_idx] - np.max(q_stage2_mf[state_idx])))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Updates with Asymmetric Learning Rates
        # Stage 1 Update (SARSA-style)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        alpha_1 = alpha_pos if delta_stage1 > 0 else alpha_neg
        q_stage1_mf[action_1[trial]] += alpha_1 * delta_stage1
        
        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        alpha_2 = alpha_pos if delta_stage2 > 0 else alpha_neg
        q_stage2_mf[state_idx, action_2[trial]] += alpha_2 * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Model-Free Learner with Separate Stage Learning Rates and Value Decay
This model refines the purely model-free approach by acknowledging that learning dynamics might differ between the abstract choice of spaceships (Stage 1) and the concrete choice of aliens (Stage 2). It assigns separate learning rates to each stage. Additionally, it incorporates a value decay mechanism for unchosen options to handle the non-stationary nature of the reward probabilities (drifting bandits).

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Learner with Separate Stage Learning Rates and Value Decay.
    
    Differentiates plasticity between the two stages (spaceship choice vs alien choice)
    and incorporates value decay for unchosen options to handle non-stationarity.
    
    Parameters:
    - alpha_1: [0, 1] Learning rate for Stage 1.
    - alpha_2: [0, 1] Learning rate for Stage 2.
    - decay_rate: [0, 1] Decay rate for unchosen Q-values.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - stickiness: [0, 5] Bonus for repeating the last Stage 1 choice.
    """
    alpha_1, alpha_2, decay_rate, beta_1, beta_2, stickiness = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # Policy for the first choice
        q_net_1 = q_stage1_mf.copy()
        if last_action_1 != -1:
            q_net_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta_1 * (q_net_1 - np.max(q_net_1)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_action_1 = action_1[trial]
        state_idx = state[trial]

        # Policy for the second choice
        exp_q2 = np.exp(beta_2 * (q_stage2_mf[state_idx] - np.max(q_stage2_mf[state_idx])))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # Updates
        # Stage 1 Update using alpha_1
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += alpha_1 * delta_stage1
        
        # Stage 2 Update using alpha_2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += alpha_2 * delta_stage2
        
        # Value Decay for unchosen options
        # Decay unchosen alien on current planet
        unchosen_alien = 1 - action_2[trial]
        q_stage2_mf[state_idx, unchosen_alien] *= (1 - decay_rate)

        # Decay aliens on the unvisited planet
        unchosen_planet = 1 - state_idx
        q_stage2_mf[unchosen_planet, 0] *= (1 - decay_rate)
        q_stage2_mf[unchosen_planet, 1] *= (1 - decay_rate)

        # Decay unchosen spaceship
        unchosen_spaceship = 1 - action_1[trial]
        q_stage1_mf[unchosen_spaceship] *= (1 - decay_rate)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Model-Free Learner with Stickiness and Exploration Bonus
This model posits that in addition to reward-based learning and perseveration (stickiness), the participant employs an explicit exploration strategy. It adds an exploration bonus to options based on how long it has been since they were last chosen (recency-based exploration), encouraging the agent to revisit neglected options to check for changes in reward probability.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Learner with Stickiness and Exploration Bonus.
    
    Incorporates a recency-based exploration bonus (linear in time since last choice)
    to encourage visiting unsampled options, alongside stickiness.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - stickiness: [0, 5] Perseveration bonus.
    - exploration_weight: [0, 1] Weight for the exploration bonus (time since last chosen).
    """
    learning_rate, beta_1, beta_2, stickiness, exploration_weight = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track time since last choice. Initialize to 0.
    # We will increment all counters at step start, then reset chosen one.
    time_since_choice_1 = np.zeros(2)
    time_since_choice_2 = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue
            
        # Increment time counters (since last trial)
        time_since_choice_1 += 1
        time_since_choice_2 += 1

        # Policy for the first choice
        # Q_net = Q_value + Stickiness + Exploration Bonus
        q_net_1 = q_stage1_mf.copy()
        
        if last_action_1 != -1:
            q_net_1[last_action_1] += stickiness
            
        # Add exploration bonus: weight * time_since_last_chosen
        q_net_1 += exploration_weight * time_since_choice_1
            
        exp_q1 = np.exp(beta_1 * (q_net_1 - np.max(q_net_1)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Update trackers
        last_action_1 = action_1[trial]
        time_since_choice_1[action_1[trial]] = 0 # Reset counter for chosen
        
        state_idx = state[trial]

        # Policy for the second choice
        q_net_2 = q_stage2_mf[state_idx].copy()
        # Add exploration bonus for aliens on current planet
        q_net_2 += exploration_weight * time_since_choice_2[state_idx]
        
        exp_q2 = np.exp(beta_2 * (q_net_2 - np.max(q_net_2)))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        time_since_choice_2[state_idx, action_2[trial]] = 0 # Reset counter

        # Updates
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```