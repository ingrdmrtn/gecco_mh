Here are three new cognitive models based on the participant data and the analysis of previous successful models.

### Model 1: Dual Alpha with Decay and Stickiness
This model builds on the success of the decay and stickiness mechanisms but introduces **asymmetric learning rates** (Dual Alpha) for positive and negative prediction errors. This allows the model to capture potential biases in how the participant learns from wins versus losses (e.g., "optimism" or "pessimism"), while maintaining the necessary forgetting (decay) and perseveration (stickiness) features.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Dual Alpha Model with Stickiness and Stage 2 Decay.
    
    Incorporates asymmetric learning rates for positive and negative prediction errors,
    along with choice stickiness and passive decay of unchosen Stage 2 values.
    
    Parameters:
    - alpha_pos: Learning rate for positive prediction errors [0, 1]
    - alpha_neg: Learning rate for negative prediction errors [0, 1]
    - beta_1: Inverse temperature for Stage 1 softmax [0, 10]
    - beta_2: Inverse temperature for Stage 2 softmax [0, 10]
    - w: Mixing weight between Model-Based and Model-Free values [0, 1]
    - lam: Eligibility trace parameter connecting Stage 2 outcome to Stage 1 [0, 1]
    - decay: Rate at which unchosen Stage 2 Q-values decay towards 0.5 [0, 1]
    - stick: Choice stickiness bonus for repeating the previous Stage 1 choice [0, 10]
    """
    alpha_pos, alpha_neg, beta_1, beta_2, w, lam, decay, stick = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.full(2, 0.5)
    q_stage2_mf = np.full((2, 2), 0.5)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        logits_1 = beta_1 * q_net
        
        # Apply stickiness
        if trial > 0:
            logits_1[action_1[trial-1]] += stick
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        state_idx = state[trial] # Planet 0 or 1

        # --- Stage 2 Policy ---
        logits_2 = beta_2 * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        lr_1 = alpha_pos if delta_stage1 > 0 else alpha_neg
        q_stage1_mf[action_1[trial]] += lr_1 * delta_stage1

        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        lr_2 = alpha_pos if delta_stage2 > 0 else alpha_neg
        q_stage2_mf[state_idx, action_2[trial]] += lr_2 * delta_stage2

        # Eligibility Trace Update for Stage 1
        # We use the learning rate associated with the Stage 2 error
        q_stage1_mf[action_1[trial]] += lr_2 * lam * delta_stage2

        # Decay unchosen Stage 2 values
        for s in range(2):
            for a in range(2):
                if not (s == state_idx and a == action_2[trial]):
                    q_stage2_mf[s, a] = (1 - decay) * q_stage2_mf[s, a] + decay * 0.5

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Stage-Specific Learning Rates with Decay and Stickiness
This model separates the learning rates for **Stage 1 (Spaceships)** and **Stage 2 (Aliens)**. The rationale is that Stage 2 involves tracking rapidly drifting reward probabilities, which may require a different learning rate than the value aggregation occurring in Stage 1. This is combined with decay (to handle the drift) and stickiness (to handle motor perseveration).

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Stage-Specific Learning Rates with Stickiness and Decay.
    
    Uses different learning rates for Stage 1 (Spaceship choice) and Stage 2 (Alien choice),
    recognizing that learning dynamics may differ between structural choices and reward estimation.
    
    Parameters:
    - lr_1: Learning rate for Stage 1 updates [0, 1]
    - lr_2: Learning rate for Stage 2 updates [0, 1]
    - beta_1: Inverse temperature for Stage 1 softmax [0, 10]
    - beta_2: Inverse temperature for Stage 2 softmax [0, 10]
    - w: Mixing weight between Model-Based and Model-Free values [0, 1]
    - lam: Eligibility trace parameter [0, 1]
    - decay: Rate at which unchosen Stage 2 Q-values decay towards 0.5 [0, 1]
    - stick: Choice stickiness bonus for repeating the previous Stage 1 choice [0, 10]
    """
    lr_1, lr_2, beta_1, beta_2, w, lam, decay, stick = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.full(2, 0.5)
    q_stage2_mf = np.full((2, 2), 0.5)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        logits_1 = beta_1 * q_net
        
        # Apply stickiness
        if trial > 0:
            logits_1[action_1[trial-1]] += stick
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        state_idx = state[trial]

        # --- Stage 2 Policy ---
        logits_2 = beta_2 * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        # Stage 1 Update using lr_1
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr_1 * delta_stage1

        # Stage 2 Update using lr_2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr_2 * delta_stage2

        # Eligibility Trace Update
        # Modulates Stage 1 value by Stage 2 error, scaled by lambda and lr_1
        q_stage1_mf[action_1[trial]] += lr_1 * lam * delta_stage2

        # Decay unchosen Stage 2 values
        for s in range(2):
            for a in range(2):
                if not (s == state_idx and a == action_2[trial]):
                    q_stage2_mf[s, a] = (1 - decay) * q_stage2_mf[s, a] + decay * 0.5

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Choice Kernel and Stage 2 Decay
This model replaces the simple "last-choice" stickiness with a **Choice Kernel** (exponentially decaying trace of past choices). This accounts for the long streaks of repeated choices observed in the participant data (e.g., choosing spaceship 1 for 10+ trials). It retains the Stage 2 decay to handle the non-stationary reward probabilities.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Choice Kernel Model with Stage 2 Decay.
    
    Replaces simple stickiness with a Choice Kernel (trace) to capture longer-term 
    perseveration trends, combined with Stage 2 value decay for non-stationary rewards.
    
    Parameters:
    - learning_rate: Global learning rate [0, 1]
    - beta_1: Inverse temperature for Stage 1 softmax [0, 10]
    - beta_2: Inverse temperature for Stage 2 softmax [0, 10]
    - w: Mixing weight between Model-Based and Model-Free values [0, 1]
    - lam: Eligibility trace parameter [0, 1]
    - decay: Rate at which unchosen Stage 2 Q-values decay towards 0.5 [0, 1]
    - ck_decay: Decay rate of the choice kernel trace [0, 1]
    - ck_weight: Weight of the choice kernel in Stage 1 decision making [0, 10]
    """
    learning_rate, beta_1, beta_2, w, lam, decay, ck_decay, ck_weight = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.full(2, 0.5)
    q_stage2_mf = np.full((2, 2), 0.5)
    
    # Initialize choice trace for Stage 1 actions (Spaceship 0 and 1)
    choice_trace = np.zeros(2)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add Choice Kernel influence to logits
        logits_1 = beta_1 * q_net + ck_weight * choice_trace
        
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Update Choice Kernel
        # Trace decays and reinforces the chosen action
        choice_trace = (1 - ck_decay) * choice_trace
        choice_trace[action_1[trial]] += ck_decay * 1.0

        state_idx = state[trial]

        # --- Stage 2 Policy ---
        logits_2 = beta_2 * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1

        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

        q_stage1_mf[action_1[trial]] += learning_rate * lam * delta_stage2

        # Decay unchosen Stage 2 values
        for s in range(2):
            for a in range(2):
                if not (s == state_idx and a == action_2[trial]):
                    q_stage2_mf[s, a] = (1 - decay) * q_stage2_mf[s, a] + decay * 0.5

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```