Here are three new cognitive models for the two-step task.

### Model 1: Dual Inverse Temperatures with Eligibility Trace
This model posits that the participant uses different levels of exploration (randomness) for the spaceship choice (Stage 1) compared to the alien choice (Stage 2). The spaceship choice involves model-based planning and habit, while the alien choice is a simpler bandit task. This separation, combined with an eligibility trace, allows for fine-grained control over how outcomes propagate back to the initial decision.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF model with separate inverse temperatures (beta) for Stage 1 and Stage 2,
    Eligibility Trace, and Stickiness.
    
    Allows for different exploration-exploitation balances in the spaceship choice (Stage 1)
    versus the alien choice (Stage 2).
    
    Parameters:
    learning_rate: [0,1] - Learning rate for Q-values.
    beta_1: [0,10] - Inverse temperature for Stage 1 (Spaceship choice).
    beta_2: [0,10] - Inverse temperature for Stage 2 (Alien choice).
    w: [0,1] - Mixing weight (0=MF, 1=MB).
    lambda_elig: [0,1] - Eligibility trace decay (0=no trace, 1=full trace).
    stickiness: [0,5] - Choice perseverance bonus for Stage 1.
    """
    learning_rate, beta_1, beta_2, w, lambda_elig, stickiness = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1
    
    for trial in range(n_trials):
        # --- Stage 1: Spaceship Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Apply stickiness to the net value of the previously chosen action
        q_net_stick = q_net.copy()
        if prev_action_1 != -1:
            q_net_stick[prev_action_1] += stickiness
            
        exp_q1 = np.exp(beta_1 * q_net_stick)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Handle missing Stage 2 data
        if action_2[trial] == -1:
            prev_action_1 = action_1[trial]
            continue

        state_idx = state[trial]

        # --- Stage 2: Alien Choice ---
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        # Stage 1 MF Update (TD(0))
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1

        # Stage 2 MF Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

        # Eligibility Trace Update (Direct S2 outcome -> S1 value)
        q_stage1_mf[action_1[trial]] += learning_rate * lambda_elig * delta_stage2
        
        prev_action_1 = action_1[trial]

    eps = 1e-10
    valid_mask_2 = action_2 != -1
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2[valid_mask_2] + eps)))
    return log_loss
```

### Model 2: Subjective Transition Probability
This model tests the hypothesis that the participant does not use the objective transition probabilities (70%/30%) but instead learns or assumes a subjective transition probability. This parameter `trans_prob` replaces the fixed probability in the Model-Based calculation, allowing the model to capture over- or under-estimation of the transition structure.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF model with Subjective Transition Probability.
    
    The agent may misestimate the transition probabilities (e.g., treating them as more
    deterministic or more random than reality). The parameter `trans_prob` replaces the
    fixed 0.7 probability in the Model-Based calculation.
    
    Parameters:
    learning_rate: [0,1] - Learning rate for Q-values.
    beta: [0,10] - Inverse temperature.
    w: [0,1] - Mixing weight (0=MF, 1=MB).
    trans_prob: [0,1] - Subjective probability of common transition (Planet X|Space A).
    stickiness: [0,5] - Choice perseverance bonus.
    """
    learning_rate, beta, w, trans_prob, stickiness = model_parameters
    n_trials = len(action_1)
    
    # Subjective transition matrix based on parameter
    # Assumes symmetry: P(State 0|Action 0) = P(State 1|Action 1) = trans_prob
    transition_matrix = np.array([[trans_prob, 1-trans_prob], [1-trans_prob, trans_prob]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1
    
    for trial in range(n_trials):

        # --- Stage 1 ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        # Use subjective matrix for MB calculation
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        q_net_stick = q_net.copy()
        if prev_action_1 != -1:
            q_net_stick[prev_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_net_stick)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        if action_2[trial] == -1:
            prev_action_1 = action_1[trial]
            continue

        state_idx = state[trial]

        # --- Stage 2 ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1

        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        prev_action_1 = action_1[trial]

    eps = 1e-10
    valid_mask_2 = action_2 != -1
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2[valid_mask_2] + eps)))
    return log_loss
```

### Model 3: Decay and Eligibility Trace
Since the alien reward probabilities drift slowly over time, simply updating the chosen option might not be sufficient. This model implements a decay mechanism where the Q-values of **unchosen** aliens in Stage 2 decay towards zero. This "forgetting" helps the agent adapt to the non-stationary environment. It is combined with an eligibility trace to maintain effective Stage 1 learning.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF model with Decay for unchosen Stage 2 options and Eligibility Trace.
    
    Since alien reward probabilities drift slowly, this model decays the Q-values of
    unchosen aliens towards zero, allowing the agent to 'forget' old information.
    
    Parameters:
    learning_rate: [0,1] - Learning rate for chosen options.
    decay_rate: [0,1] - Decay rate for unchosen Stage 2 options (1 = full reset, 0 = no decay).
    beta: [0,10] - Inverse temperature.
    w: [0,1] - Mixing weight (0=MF, 1=MB).
    lambda_elig: [0,1] - Eligibility trace decay.
    stickiness: [0,5] - Choice perseverance bonus.
    """
    learning_rate, decay_rate, beta, w, lambda_elig, stickiness = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1
    
    for trial in range(n_trials):

        # --- Stage 1 ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        q_net_stick = q_net.copy()
        if prev_action_1 != -1:
            q_net_stick[prev_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_net_stick)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        if action_2[trial] == -1:
            prev_action_1 = action_1[trial]
            continue

        state_idx = state[trial]

        # --- Stage 2 ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        # Update Stage 1
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1

        # Update Stage 2 Chosen
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Decay Stage 2 Unchosen
        unchosen_action_2 = 1 - action_2[trial]
        q_stage2_mf[state_idx, unchosen_action_2] *= (1 - decay_rate)

        # Eligibility Trace Update
        q_stage1_mf[action_1[trial]] += learning_rate * lambda_elig * delta_stage2
        
        prev_action_1 = action_1[trial]

    eps = 1e-10
    valid_mask_2 = action_2 != -1
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2[valid_mask_2] + eps)))
    return log_loss
```