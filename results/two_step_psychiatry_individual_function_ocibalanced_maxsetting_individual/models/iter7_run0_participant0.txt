Here are 3 new cognitive models as Python functions, designed based on the participant's data and previous model performance.

### Model 1: Asymmetric MF with Value Decay
This model builds on the success of Model-Free (MF) learning with decay but hypothesizes that the participant learns differently from positive prediction errors (better than expected) versus negative ones (worse than expected) at the reward stage (Stage 2). It uses a standard learning rate for Stage 1.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free RL with Asymmetric Learning Rates (Stage 2), Eligibility Trace, Value Decay, and Perseveration.
    
    Splits Stage 2 learning into positive and negative rates to capture differential sensitivity to 
    wins vs. losses, while maintaining value decay to handle non-stationarity.
    
    Parameters:
    - alpha_1: [0, 1] Learning rate for Stage 1.
    - alpha_2_pos: [0, 1] Learning rate for Stage 2 positive prediction errors (RPE > 0).
    - alpha_2_neg: [0, 1] Learning rate for Stage 2 negative prediction errors (RPE < 0).
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - lambda_eligibility: [0, 1] Eligibility trace decay connecting Stage 2 outcome to Stage 1.
    - decay: [0, 1] Decay rate for unchosen Stage 2 options (forgetting).
    - perseveration: [0, 10] Stickiness bonus for Stage 1 repetition.
    """
    alpha_1, alpha_2_pos, alpha_2_neg, beta_1, beta_2, lambda_eligibility, decay, perseveration = model_parameters
    n_trials = len(action_1)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2)) # state x action
    
    log_likelihood = 0.0
    eps = 1e-10
    last_action_1 = -1
    
    for t in range(n_trials):
        a1 = int(action_1[t])
        s2 = int(state[t])
        a2 = int(action_2[t])
        r = int(reward[t])

        # Stage 1 Choice
        logits_1 = beta_1 * q_stage1
        if last_action_1 != -1:
            logits_1[last_action_1] += perseveration
            
        probs_1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 /= np.sum(probs_1)
        log_likelihood += np.log(probs_1[a1] + eps)

        # Stage 2 Choice
        logits_2 = beta_2 * q_stage2[s2]
        probs_2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 /= np.sum(probs_2)
        log_likelihood += np.log(probs_2[a2] + eps)

        # Learning
        # Calculate errors before update
        delta_2 = r - q_stage2[s2, a2]
        delta_1 = q_stage2[s2, a2] - q_stage1[a1]
        
        # Update Stage 2 with asymmetric alphas
        alpha_2 = alpha_2_pos if delta_2 > 0 else alpha_2_neg
        q_stage2[s2, a2] += alpha_2 * delta_2
        
        # Decay unchosen Stage 2 option
        unchosen_a2 = 1 - a2
        q_stage2[s2, unchosen_a2] *= (1 - decay)
        
        # Update Stage 1
        q_stage1[a1] += alpha_1 * (delta_1 + lambda_eligibility * delta_2)

        last_action_1 = a1
        
    return -log_likelihood
```

### Model 2: Hybrid MB/MF with Value Decay
This model integrates Model-Based (MB) planning with the Model-Free (MF) decay mechanism. It tests if the participant uses knowledge of the transition structure (`w_mb`) while simultaneously forgetting MF values (`decay`) due to the drifting rewards.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free RL with Value Decay.
    
    Combines MB planning (using the known transition matrix) and MF learning.
    Crucially includes 'decay' on MF values to handle drifting rewards within the Hybrid framework.
    
    Parameters:
    - alpha_1: [0, 1] MF Learning rate for Stage 1.
    - alpha_2: [0, 1] MF Learning rate for Stage 2.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - w_mb: [0, 1] Weight of Model-Based control (0=Pure MF, 1=Pure MB).
    - lambda_eligibility: [0, 1] Eligibility trace for MF Stage 1 update.
    - decay: [0, 1] Decay rate for unchosen Stage 2 MF values.
    - perseveration: [0, 10] Stickiness bonus for Stage 1.
    """
    alpha_1, alpha_2, beta_1, beta_2, w_mb, lambda_eligibility, decay, perseveration = model_parameters
    n_trials = len(action_1)
    
    # Transition matrix: T[action, state]
    # Action 0 (A) -> Planet 0 (Common 0.7)
    # Action 1 (U) -> Planet 1 (Common 0.7)
    trans_probs = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_mf_1 = np.zeros(2)
    q_mf_2 = np.zeros((2, 2)) # state x action
    
    log_likelihood = 0.0
    eps = 1e-10
    last_action_1 = -1
    
    for t in range(n_trials):
        a1 = int(action_1[t])
        s2 = int(state[t])
        a2 = int(action_2[t])
        r = int(reward[t])
        
        # Model-Based Values for Stage 1
        # V_MB(s1) = Sum_s2 P(s2|s1,a1) * max_a2 Q_MF(s2, a2)
        v_stage2 = np.max(q_mf_2, axis=1) 
        q_mb_1 = trans_probs @ v_stage2 
        
        # Integrated Q for Stage 1
        q_net_1 = w_mb * q_mb_1 + (1 - w_mb) * q_mf_1
        
        logits_1 = beta_1 * q_net_1
        if last_action_1 != -1:
            logits_1[last_action_1] += perseveration
            
        probs_1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 /= np.sum(probs_1)
        log_likelihood += np.log(probs_1[a1] + eps)
        
        # Stage 2 Choice (Pure MF)
        logits_2 = beta_2 * q_mf_2[s2]
        probs_2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 /= np.sum(probs_2)
        log_likelihood += np.log(probs_2[a2] + eps)
        
        # Learning (MF)
        delta_2 = r - q_mf_2[s2, a2]
        delta_1 = q_mf_2[s2, a2] - q_mf_1[a1]
        
        q_mf_2[s2, a2] += alpha_2 * delta_2
        
        # Decay unchosen
        unchosen_a2 = 1 - a2
        q_mf_2[s2, unchosen_a2] *= (1 - decay)
        
        q_mf_1[a1] += alpha_1 * (delta_1 + lambda_eligibility * delta_2)
        
        last_action_1 = a1
        
    return -log_likelihood
```

### Model 3: MF with Value Decay and Choice Kernel
This model separates the concept of "forgetting values" (Value Decay) from "habit formation" (Choice Kernel). Unlike simple perseveration (1-step), the Choice Kernel accumulates a history of past choices that decays over time, modeling a dynamic habit strength.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free RL with Value Decay and Choice Kernel (Habit).
    
    Replaces simple perseveration with a Choice Kernel that tracks a decaying history 
    of past choices, modeling habit formation alongside value learning with forgetting.
    
    Parameters:
    - alpha_1: [0, 1] Learning rate for Stage 1.
    - alpha_2: [0, 1] Learning rate for Stage 2.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - lambda_eligibility: [0, 1] Eligibility trace.
    - val_decay: [0, 1] Decay rate for unchosen Stage 2 Q-values (Value Forgetting).
    - ck_weight: [0, 10] Weight of the choice kernel in Stage 1 decision.
    - ck_decay: [0, 1] Decay rate of the choice kernel (Habit Persistence).
    """
    alpha_1, alpha_2, beta_1, beta_2, lambda_eligibility, val_decay, ck_weight, ck_decay = model_parameters
    n_trials = len(action_1)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    choice_kernel = np.zeros(2)
    
    log_likelihood = 0.0
    eps = 1e-10
    
    for t in range(n_trials):
        a1 = int(action_1[t])
        s2 = int(state[t])
        a2 = int(action_2[t])
        r = int(reward[t])
        
        # Stage 1 Choice: Combine Value and Habit
        logits_1 = beta_1 * q_stage1 + ck_weight * choice_kernel
        probs_1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 /= np.sum(probs_1)
        log_likelihood += np.log(probs_1[a1] + eps)
        
        # Stage 2 Choice
        logits_2 = beta_2 * q_stage2[s2]
        probs_2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 /= np.sum(probs_2)
        log_likelihood += np.log(probs_2[a2] + eps)
        
        # Value Learning
        delta_2 = r - q_stage2[s2, a2]
        delta_1 = q_stage2[s2, a2] - q_stage1[a1]
        
        q_stage2[s2, a2] += alpha_2 * delta_2
        
        # Value Decay (forgetting unchosen)
        unchosen_a2 = 1 - a2
        q_stage2[s2, unchosen_a2] *= (1 - val_decay)
        
        q_stage1[a1] += alpha_1 * (delta_1 + lambda_eligibility * delta_2)
        
        # Choice Kernel Update (Habit)
        choice_kernel *= ck_decay
        choice_kernel[a1] += 1.0 
        
    return -log_likelihood
```