Here are the 3 proposed cognitive models.

### Model 1: Hybrid Model with Lapse Rate
This model extends the standard hybrid learner by adding a **lapse rate** parameter ($\epsilon$). In many cognitive tasks, participants occasionally make random errors or "lapses" in attention that are not captured by the softmax temperature ($\beta$). The lapse parameter mixes the softmax distribution with a uniform distribution, preventing the model from assigning zero probability (infinite log loss) to any choice and capturing "noise" that is distinct from exploration.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Lapse Rate.
    
    Combines Model-Based and Model-Free learning with a 'lapse' parameter.
    The lapse parameter acts as a "trembling hand," mixing the softmax 
    policy with a uniform random policy.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate for Q-value updates.
    beta: [0, 10] - Inverse temperature for softmax choice.
    w: [0, 1] - Mixing weight (0 = pure MF, 1 = pure MB).
    stickiness: [0, 5] - Bonus added to the Q-value of the previously chosen action (Stage 1).
    lapse: [0, 1] - Probability of choosing randomly (epsilon-greedy-like mixture).
    """
    learning_rate, beta, w, stickiness, lapse = model_parameters
    n_trials = len(action_1)

    # Fixed transition matrix: P(State|Action)
    # Action 0 -> State 0 (0.7), State 1 (0.3)
    # Action 1 -> State 1 (0.7), State 0 (0.3)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_mf_stage1 = np.zeros(2)
    q_mf_stage2 = np.zeros((2, 2)) # State x Action
    
    last_action_1 = -1
    log_loss = 0.0
    eps = 1e-10
    
    for trial in range(n_trials):
        a1 = action_1[trial]
        s_idx = state[trial]
        a2 = action_2[trial]
        r = reward[trial]

        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_mf_stage2, axis=1)
        q_mb_stage1 = transition_matrix @ max_q_stage2

        q_net_stage1 = w * q_mb_stage1 + (1 - w) * q_mf_stage1

        # Add stickiness
        if last_action_1 != -1:
            q_net_stage1[last_action_1] += stickiness

        # Softmax
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1_softmax = exp_q1 / np.sum(exp_q1)
        
        # Apply Lapse: P = (1-lapse) * Softmax + lapse * Uniform
        probs_1 = (1 - lapse) * probs_1_softmax + lapse * 0.5
        
        if a1 != -1: # Handle missing data if any
            log_loss -= np.log(probs_1[a1] + eps)
        
        # --- Stage 2 Choice ---
        # Stage 2 is purely Model-Free
        exp_q2 = np.exp(beta * q_mf_stage2[s_idx])
        probs_2_softmax = exp_q2 / np.sum(exp_q2)
        
        # Apply Lapse to Stage 2 as well
        probs_2 = (1 - lapse) * probs_2_softmax + lapse * 0.5
        
        if a2 != -1:
            log_loss -= np.log(probs_2[a2] + eps)

            # --- Learning ---
            # Update Stage 2 Q-values (TD error)
            delta_2 = r - q_mf_stage2[s_idx, a2]
            q_mf_stage2[s_idx, a2] += learning_rate * delta_2

            # Update Stage 1 MF Q-values (TD(1): driven by reward)
            # Note: Standard hybrid often uses TD(1) for stage 1 MF
            q_mf_stage1[a1] += learning_rate * (r - q_mf_stage1[a1])
            
        last_action_1 = a1

    return log_loss
```

### Model 2: Hybrid Model with Stage-Specific Stickiness
The participant data shows different patterns of repetition for spaceship choice (Stage 1) versus alien choice (Stage 2). Standard models usually only include stickiness for the first stage. This model introduces **two separate stickiness parameters**: one for the spaceship choice (`stick_s1`) and one for the alien choice (`stick_s2`), allowing the model to capture different levels of perseveration (or habit) at each level of the task hierarchy.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Stage-Specific Stickiness.
    
    Differentiates between perseveration in Stage 1 (Spaceship choice) and 
    Stage 2 (Alien choice). 
    
    Parameters:
    learning_rate: [0, 1] - Learning rate for Q-value updates.
    beta: [0, 10] - Inverse temperature for softmax choice.
    w: [0, 1] - Mixing weight (0 = pure MF, 1 = pure MB).
    stick_s1: [0, 5] - Stickiness bonus for Stage 1 choice.
    stick_s2: [0, 5] - Stickiness bonus for Stage 2 choice (state-dependent).
    """
    learning_rate, beta, w, stick_s1, stick_s2 = model_parameters
    n_trials = len(action_1)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_mf_stage1 = np.zeros(2)
    q_mf_stage2 = np.zeros((2, 2)) # State x Action
    
    last_action_1 = -1
    # Track last action chosen in each state (planet) for Stage 2 stickiness
    last_action_2_in_state = np.array([-1, -1]) 
    
    log_loss = 0.0
    eps = 1e-10
    
    for trial in range(n_trials):
        a1 = action_1[trial]
        s_idx = state[trial]
        a2 = action_2[trial]
        r = reward[trial]

        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_mf_stage2, axis=1)
        q_mb_stage1 = transition_matrix @ max_q_stage2

        q_net_stage1 = w * q_mb_stage1 + (1 - w) * q_mf_stage1

        # Stage 1 Stickiness
        if last_action_1 != -1:
            q_net_stage1[last_action_1] += stick_s1

        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        if a1 != -1:
            log_loss -= np.log(probs_1[a1] + eps)
        
        # --- Stage 2 Choice ---
        # Calculate net Q for stage 2
        q_net_stage2 = q_mf_stage2[s_idx].copy()
        
        # Stage 2 Stickiness (if we have visited this state before)
        if last_action_2_in_state[s_idx] != -1:
            q_net_stage2[last_action_2_in_state[s_idx]] += stick_s2
            
        exp_q2 = np.exp(beta * q_net_stage2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        if a2 != -1:
            log_loss -= np.log(probs_2[a2] + eps)

            # --- Learning ---
            delta_2 = r - q_mf_stage2[s_idx, a2]
            q_mf_stage2[s_idx, a2] += learning_rate * delta_2

            q_mf_stage1[a1] += learning_rate * (r - q_mf_stage1[a1])
            
            # Update history
            last_action_2_in_state[s_idx] = a2
            
        last_action_1 = a1

    return log_loss
```

### Model 3: Independent MB/MF Betas (Additive Model)
Instead of a mixing weight $w$ that trades off Model-Based (MB) and Model-Free (MF) control, this model assumes the two systems contribute additively to the decision value, each with its own independent strength (inverse temperature). `beta_mb` scales the MB value, and `beta_mf` scales the MF value. This allows the model to represent states where both systems are weak or both are strong, rather than enforcing a zero-sum relationship. Stage 2 is governed by the MF system (`beta_mf`).

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Independent MB/MF Betas Model.
    
    Replaces the mixing parameter 'w' and shared 'beta' with independent 
    scaling factors (betas) for the Model-Based and Model-Free components.
    This allows for independent strength/precision of each system.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate for Q-value updates.
    beta_mb: [0, 10] - Scaling factor (inverse temp) for Model-Based values in Stage 1.
    beta_mf: [0, 10] - Scaling factor (inverse temp) for Model-Free values (Stage 1 & 2).
    stickiness: [0, 5] - Bonus added to the Q-value of the previously chosen action.
    """
    learning_rate, beta_mb, beta_mf, stickiness = model_parameters
    n_trials = len(action_1)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_mf_stage1 = np.zeros(2)
    q_mf_stage2 = np.zeros((2, 2)) 
    
    last_action_1 = -1
    log_loss = 0.0
    eps = 1e-10
    
    for trial in range(n_trials):
        a1 = action_1[trial]
        s_idx = state[trial]
        a2 = action_2[trial]
        r = reward[trial]

        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_mf_stage2, axis=1)
        q_mb_stage1 = transition_matrix @ max_q_stage2

        # Linear combination of MB and MF values with independent betas
        # Note: We do not multiply by a common beta here, the betas are separate.
        logits_stage1 = (beta_mb * q_mb_stage1) + (beta_mf * q_mf_stage1)

        if last_action_1 != -1:
            logits_stage1[last_action_1] += stickiness

        # Softmax (beta is implicitly 1 because it's baked into logits)
        exp_q1 = np.exp(logits_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        if a1 != -1:
            log_loss -= np.log(probs_1[a1] + eps)
        
        # --- Stage 2 Choice ---
        # Stage 2 is purely Model-Free, governed by beta_mf
        logits_stage2 = beta_mf * q_mf_stage2[s_idx]
        
        exp_q2 = np.exp(logits_stage2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        if a2 != -1:
            log_loss -= np.log(probs_2[a2] + eps)

            # --- Learning ---
            delta_2 = r - q_mf_stage2[s_idx, a2]
            q_mf_stage2[s_idx, a2] += learning_rate * delta_2

            q_mf_stage1[a1] += learning_rate * (r - q_mf_stage1[a1])
            
        last_action_1 = a1

    return log_loss
```