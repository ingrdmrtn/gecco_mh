Here are three new cognitive models for the two-step decision task, designed based on the participant's data and the feedback provided.

### Cognitive Model 1
**TD(lambda) Learner with Stage 1 and Stage 2 Perseveration**
This model extends the best-performing TD(lambda) framework by adding a "stickiness" parameter to the second stage (alien choice). The participant data shows repeated selection of the same alien even after failures, suggesting that perseveration occurs at both decision stages, not just the first.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    TD(lambda) Learner with Stage 1 and Stage 2 Perseveration.
    
    This model assumes the participant learns using an eligibility trace (TD-lambda)
    but exhibits 'stickiness' in both the spaceship choice and the alien choice.
    
    Parameters:
    lr:            [0, 1] Learning rate.
    beta:          [0, 10] Inverse temperature.
    lambda_param:  [0, 1] Eligibility trace decay (0 = TD(0), 1 = Monte Carlo).
    persev_1:      [0, 5] Perseveration bonus for Stage 1 (Spaceship).
    persev_2:      [0, 5] Perseveration bonus for Stage 2 (Alien).
    """
    lr, beta, lambda_param, persev_1, persev_2 = model_parameters
    n_trials = len(action_1)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2)) # State x Action
    
    prev_action_1 = -1
    prev_action_2 = np.full(2, -1) # Store prev action for each state (0 and 1)
    
    for trial in range(n_trials):
        # Skip missing data
        if action_1[trial] == -1:
            continue
            
        # --- Stage 1 Decision ---
        q_stage1_eff = q_stage1.copy()
        if prev_action_1 != -1:
            q_stage1_eff[prev_action_1] += persev_1
            
        exp_q1 = np.exp(beta * q_stage1_eff)
        # Prevent overflow
        if np.any(np.isinf(exp_q1)):
             max_q = np.max(q_stage1_eff)
             exp_q1 = np.exp(beta * (q_stage1_eff - max_q))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Decision ---
        s_idx = state[trial]
        if s_idx == -1: continue
        
        q_stage2_eff = q_stage2[s_idx].copy()
        if prev_action_2[s_idx] != -1:
            q_stage2_eff[prev_action_2[s_idx]] += persev_2
            
        exp_q2 = np.exp(beta * q_stage2_eff)
        if np.any(np.isinf(exp_q2)):
             max_q = np.max(q_stage2_eff)
             exp_q2 = np.exp(beta * (q_stage2_eff - max_q))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning ---
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]
        
        delta_1 = q_stage2[s_idx, a2] - q_stage1[a1]
        delta_2 = r - q_stage2[s_idx, a2]
        
        q_stage2[s_idx, a2] += lr * delta_2
        q_stage1[a1] += lr * (delta_1 + lambda_param * delta_2)
        
        prev_action_1 = a1
        prev_action_2[s_idx] = a2
        
    eps = 1e-10
    valid_trials = (action_1 != -1) & (state != -1) & (action_2 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```

### Cognitive Model 2
**Dual Learning Rate TD(lambda) with Perseveration**
This model differentiates the learning speed for the first stage (choosing a spaceship) and the second stage (choosing an alien). It allows the model to capture scenarios where the participant might update their preferences for aliens quickly (or slowly) while their spaceship preferences remain more rigid.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Dual Learning Rate TD(lambda) with Perseveration.
    
    Allows for different learning rates for the first stage (spaceship) and second stage (alien),
    combined with an eligibility trace and choice perseveration on stage 1.
    
    Parameters:
    lr_1:          [0, 1] Learning rate for Stage 1.
    lr_2:          [0, 1] Learning rate for Stage 2.
    beta:          [0, 10] Inverse temperature.
    lambda_param:  [0, 1] Eligibility trace decay.
    perseveration: [0, 5] Perseveration bonus for Stage 1.
    """
    lr_1, lr_2, beta, lambda_param, perseveration = model_parameters
    n_trials = len(action_1)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    prev_action_1 = -1
    
    for trial in range(n_trials):
        if action_1[trial] == -1: continue
        
        # --- Stage 1 Decision ---
        q_stage1_eff = q_stage1.copy()
        if prev_action_1 != -1:
            q_stage1_eff[prev_action_1] += perseveration
            
        exp_q1 = np.exp(beta * q_stage1_eff)
        if np.any(np.isinf(exp_q1)):
             exp_q1 = np.exp(beta * (q_stage1_eff - np.max(q_stage1_eff)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Decision ---
        s_idx = state[trial]
        if s_idx == -1: continue
        
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        if np.any(np.isinf(exp_q2)):
             exp_q2 = np.exp(beta * (q_stage2[s_idx] - np.max(q_stage2[s_idx])))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning ---
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]
        
        delta_1 = q_stage2[s_idx, a2] - q_stage1[a1]
        delta_2 = r - q_stage2[s_idx, a2]
        
        q_stage2[s_idx, a2] += lr_2 * delta_2
        # Stage 1 update uses lr_1, mixing delta_1 and delta_2 via lambda
        q_stage1[a1] += lr_1 * (delta_1 + lambda_param * delta_2)
        
        prev_action_1 = a1

    eps = 1e-10
    valid_trials = (action_1 != -1) & (state != -1) & (action_2 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```

### Cognitive Model 3
**Hybrid Model-Based / TD(lambda) with Perseveration**
This model integrates the structural knowledge of the task (Model-Based) with the eligibility-trace learning mechanism (TD-lambda). It tests whether the participant combines explicit knowledge of transition probabilities with temporal difference learning, while also accounting for choice perseveration.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / TD(lambda) with Perseveration.
    
    Combines a Model-Based planner (using fixed transition probabilities) with a 
    Model-Free TD(lambda) learner. The 'w' parameter controls the balance between MB and MF.
    
    Parameters:
    lr:            [0, 1] Learning rate.
    beta:          [0, 10] Inverse temperature.
    w:             [0, 1] Weight for Model-Based values (0=Pure MF, 1=Pure MB).
    lambda_param:  [0, 1] Eligibility trace decay for the MF component.
    perseveration: [0, 5] Perseveration bonus for Stage 1.
    """
    lr, beta, w, lambda_param, perseveration = model_parameters
    n_trials = len(action_1)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_mf_stage1 = np.zeros(2)
    q_mf_stage2 = np.zeros((2, 2))
    
    # Transition matrix: Row 0 (A) -> [0.7 (X), 0.3 (Y)], Row 1 (U) -> [0.3 (X), 0.7 (Y)]
    trans_probs = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    prev_action_1 = -1
    
    for trial in range(n_trials):
        if action_1[trial] == -1: continue
        
        # --- Stage 1 Decision (Hybrid) ---
        # MB Value: Expected value of next stage's max MF value
        v_stage2 = np.max(q_mf_stage2, axis=1)
        q_mb_stage1 = trans_probs @ v_stage2
        
        # Net Value
        q_net_stage1 = w * q_mb_stage1 + (1 - w) * q_mf_stage1
        
        # Perseveration
        if prev_action_1 != -1:
            q_net_stage1[prev_action_1] += perseveration
            
        exp_q1 = np.exp(beta * q_net_stage1)
        if np.any(np.isinf(exp_q1)):
             exp_q1 = np.exp(beta * (q_net_stage1 - np.max(q_net_stage1)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Decision (MF) ---
        s_idx = state[trial]
        if s_idx == -1: continue
        
        exp_q2 = np.exp(beta * q_mf_stage2[s_idx])
        if np.any(np.isinf(exp_q2)):
             exp_q2 = np.exp(beta * (q_mf_stage2[s_idx] - np.max(q_mf_stage2[s_idx])))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning ---
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]
        
        delta_1 = q_mf_stage2[s_idx, a2] - q_mf_stage1[a1]
        delta_2 = r - q_mf_stage2[s_idx, a2]
        
        q_mf_stage2[s_idx, a2] += lr * delta_2
        q_mf_stage1[a1] += lr * (delta_1 + lambda_param * delta_2)
        
        prev_action_1 = a1

    eps = 1e-10
    valid_trials = (action_1 != -1) & (state != -1) & (action_2 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```