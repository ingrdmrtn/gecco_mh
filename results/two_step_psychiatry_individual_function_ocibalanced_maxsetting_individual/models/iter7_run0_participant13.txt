Here are 3 new cognitive models for the two-step task.

### Model 1: Learned Transition Model-Based Controller
This model assumes the participant is Model-Based but does not trust the fixed transition probabilities (70/30) provided in the instructions. Instead, the agent learns the transition matrix `P(Planet|Spaceship)` trial-by-trial based on observed transitions. This allows the agent to adapt if they perceive the spaceship reliability to be drifting.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Model 1: Learned Transition Model-Based Controller.
    
    The agent uses a Model-Based strategy but updates its internal model of the 
    spaceship-to-planet transitions (T) based on experience, rather than assuming 
    fixed 0.7/0.3 probabilities.
    
    Parameters:
    lr_trans: [0, 1] - Learning rate for updating transition probabilities.
    lr_value: [0, 1] - Learning rate for updating Stage 2 (Alien) values.
    beta: [0, 10] - Inverse temperature for softmax choice.
    perseveration: [-3, 3] - Stickiness to the previous Stage 1 choice.
    """
    lr_trans, lr_value, beta, perseveration = model_parameters
    n_trials = len(action_1)
    
    # Initialize transition matrix with prior knowledge (0.7 common)
    # T[spaceship, planet]
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Stage 2 values (Q-values for aliens) initialized to 0.5
    q_stage2 = np.zeros((2, 2)) + 0.5
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1
    
    for t in range(n_trials):
        # --- Stage 1 Choice (Model-Based) ---
        # Calculate Model-Based values for Stage 1: V_MB(s1) = Sum(T(s1, s2) * max(Q(s2)))
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        logits_1 = beta * q_stage1_mb
        if prev_a1 != -1:
            logits_1[prev_a1] += perseveration
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[t] = probs_1[action_1[t]]
        
        # --- Transition ---
        a1 = action_1[t]
        s_idx = state[t] # Planet reached
        
        # --- Stage 2 Choice ---
        logits_2 = beta * q_stage2[s_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[t] = probs_2[action_2[t]]
        
        # --- Learning ---
        a2 = action_2[t]
        r = reward[t]
        
        # 1. Update Stage 2 values (Reward Learning)
        delta = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += lr_value * delta
        
        # 2. Update Transition Matrix (Structure Learning)
        # Increase probability of the observed transition (a1 -> s_idx)
        # Decrease probability of the unobserved transition (a1 -> 1-s_idx)
        transition_matrix[a1, s_idx] += lr_trans * (1.0 - transition_matrix[a1, s_idx])
        transition_matrix[a1, 1 - s_idx] = 1.0 - transition_matrix[a1, s_idx]
        
        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Pearce-Hall Model-Free Controller
This model implements a dynamic learning rate based on the Pearce-Hall theory of attention. The "associability" (learning rate) of the agent increases when prediction errors are high (surprise) and decreases when outcomes are well-predicted. This allows the agent to learn quickly during volatile periods and stabilize during stable periods.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model 2: Pearce-Hall Model-Free Controller.
    
    A Model-Free agent where the learning rate (associability) is dynamic.
    The associability 'k' tracks the magnitude of recent prediction errors.
    High surprise leads to higher learning rates for subsequent trials.
    
    Parameters:
    lr_scale: [0, 1] - Scaling factor for the associability (max effective learning rate).
    gamma: [0, 1] - Decay rate of the associability trace (memory for past errors).
    beta: [0, 10] - Inverse temperature for softmax choice.
    perseveration: [-3, 3] - Stickiness to the previous Stage 1 choice.
    """
    lr_scale, gamma, beta, perseveration = model_parameters
    n_trials = len(action_1)
    
    q_stage1 = np.zeros(2) + 0.5
    q_stage2 = np.zeros((2, 2)) + 0.5
    
    # Initial associability (k)
    k = 0.5
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1
    
    for t in range(n_trials):
        # Calculate current effective learning rate
        alpha = k * lr_scale
        
        # --- Stage 1 Choice ---
        logits_1 = beta * q_stage1
        if prev_a1 != -1:
            logits_1[prev_a1] += perseveration
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[t] = probs_1[action_1[t]]
        
        a1 = action_1[t]
        s_idx = state[t]
        
        # --- Stage 2 Choice ---
        logits_2 = beta * q_stage2[s_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[t] = probs_2[action_2[t]]
        
        a2 = action_2[t]
        r = reward[t]
        
        # --- Learning ---
        # Stage 2 Prediction Error
        delta_2 = r - q_stage2[s_idx, a2]
        
        # Update Stage 2 Value
        q_stage2[s_idx, a2] += alpha * delta_2
        
        # Stage 1 Prediction Error (TD-0 using updated Stage 2 value as proxy for V(s'))
        # Using q_stage2[s_idx, a2] as the value of the state reached
        delta_1 = q_stage2[s_idx, a2] - q_stage1[a1]
        
        # Update Stage 1 Value
        q_stage1[a1] += alpha * delta_1
        
        # --- Update Associability (Pearce-Hall) ---
        # k updates based on the absolute prediction error of the final outcome
        k = gamma * k + (1 - gamma) * np.abs(delta_2)
        
        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Outcome-Dependent Beta Model-Free Controller
This model hypothesizes that the participant's exploration/exploitation balance (inverse temperature `beta`) is modulated by the previous trial's outcome. A win ("confidence") leads to a higher `beta` (more deterministic/exploitative), while a loss leads to a lower `beta` (more random/exploratory). This captures "Win-Stay, Lose-Random" dynamics mechanistically.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model 3: Outcome-Dependent Beta Model-Free Controller.
    
    A Model-Free agent where the inverse temperature (beta) changes based on 
    the previous reward. This allows the agent to be more deterministic after 
    wins (high confidence) and more exploratory after losses (low confidence).
    
    Parameters:
    learning_rate: [0, 1] - Learning rate for value updates.
    beta_win: [0, 10] - Inverse temperature applied after a rewarded trial.
    beta_loss: [0, 10] - Inverse temperature applied after an unrewarded trial.
    perseveration: [-3, 3] - Stickiness to the previous Stage 1 choice.
    """
    learning_rate, beta_win, beta_loss, perseveration = model_parameters
    n_trials = len(action_1)
    
    q_stage1 = np.zeros(2) + 0.5
    q_stage2 = np.zeros((2, 2)) + 0.5
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1
    prev_reward = 0 # Assume neutral/loss start or could be parameter
    
    for t in range(n_trials):
        # Determine current beta based on previous reward
        current_beta = beta_win if prev_reward == 1 else beta_loss
        
        # --- Stage 1 Choice ---
        logits_1 = current_beta * q_stage1
        if prev_a1 != -1:
            logits_1[prev_a1] += perseveration
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[t] = probs_1[action_1[t]]
        
        a1 = action_1[t]
        s_idx = state[t]
        
        # --- Stage 2 Choice ---
        # Using the same beta for Stage 2
        logits_2 = current_beta * q_stage2[s_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[t] = probs_2[action_2[t]]
        
        a2 = action_2[t]
        r = reward[t]
        
        # --- Learning ---
        # Update Stage 2
        delta_2 = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += learning_rate * delta_2
        
        # Update Stage 1 (TD-0)
        delta_1 = q_stage2[s_idx, a2] - q_stage1[a1]
        q_stage1[a1] += learning_rate * delta_1
        
        prev_a1 = a1
        prev_reward = r

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```