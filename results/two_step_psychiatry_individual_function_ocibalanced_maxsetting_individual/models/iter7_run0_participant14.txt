Here are the three proposed cognitive models.

### Cognitive Model 1
**Model Description**: Hybrid Model-Based/Model-Free reinforcement learning with **Asymmetric Learning Rates** and **Stage-Specific Inverse Temperatures (Dual Betas)**.
This model hypothesizes that the participant has different levels of exploration/exploitation trade-offs for the high-level decision (Spaceship choice, Stage 1) versus the low-level decision (Alien choice, Stage 2). It also accounts for different learning updates from positive (wins) versus negative (losses) prediction errors.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF model with Asymmetric Learning Rates and Dual Betas.
    
    Distinguishes between exploration at Stage 1 and Stage 2, and learning from
    positive vs negative outcomes.

    Parameters:
    - lr_pos: [0, 1] Learning rate for positive prediction errors.
    - lr_neg: [0, 1] Learning rate for negative prediction errors.
    - beta_1: [0, 10] Inverse temperature for Stage 1 (Spaceship) choice.
    - beta_2: [0, 10] Inverse temperature for Stage 2 (Alien) choice.
    - w: [0, 1] Weight of Model-Based system (0=MF, 1=MB).
    - lam: [0, 1] Eligibility trace (credit assignment to Stage 1).
    """
    lr_pos, lr_neg, beta_1, beta_2, w, lam = model_parameters
    n_trials = len(action_1)

    # Transition probabilities: 0->0 (0.7), 0->1 (0.3), etc.
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    # Initialize Q-values
    q_mf = np.zeros(2)         # Stage 1 MF values (Spaceship A vs U)
    q2 = np.zeros((2, 2))      # Stage 2 values (State 0/1, Alien 0/1)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    eps = 1e-10

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])

        # Skip invalid trials
        if a1 == -1:
            continue

        # --- Stage 1 Policy ---
        # Model-Based valuation: Expected value of best action in next state
        max_q2 = np.max(q2, axis=1)
        q_mb = transition_matrix @ max_q2
        
        # Net value
        q_net = w * q_mb + (1 - w) * q_mf

        # Softmax for Stage 1 using beta_1
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        # Softmax for Stage 2 using beta_2
        exp_q2 = np.exp(beta_2 * q2[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updating ---
        # Stage 1 RPE (TD(0))
        delta_1 = q2[s, a2] - q_mf[a1]
        lr_1 = lr_pos if delta_1 > 0 else lr_neg
        q_mf[a1] += lr_1 * delta_1

        # Stage 2 RPE
        delta_2 = r - q2[s, a2]
        lr_2 = lr_pos if delta_2 > 0 else lr_neg
        q2[s, a2] += lr_2 * delta_2

        # Eligibility Trace update for Stage 1
        # Propagate Stage 2 error back to Stage 1 choice
        q_mf[a1] += lr_2 * lam * delta_2

    # Calculate Negative Log Likelihood
    mask = p_choice_1 > 0
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss
```

### Cognitive Model 2
**Model Description**: Hybrid Model-Based/Model-Free reinforcement learning with **Asymmetric Learning Rates** and **Outcome-Dependent Stickiness**.
This model posits that the participant's tendency to repeat a choice (stickiness) depends on the previous outcome. "Win-Stay" and "Loss-Stay" (or Shift) behaviors are modeled explicitly with separate parameters (`stick_win`, `stick_lose`), combined with asymmetric value updating (`lr_pos`, `lr_neg`).

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF model with Asymmetric Learning and Outcome-Dependent Stickiness.
    
    Applies different stickiness bonuses depending on whether the previous trial
    resulted in a win (reward=1) or a loss (reward=0).

    Parameters:
    - lr_pos: [0, 1] Learning rate for positive prediction errors.
    - lr_neg: [0, 1] Learning rate for negative prediction errors.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weight of Model-Based system.
    - lam: [0, 1] Eligibility trace.
    - stick_win: [0, 5] Stickiness bonus after a win.
    - stick_lose: [0, 5] Stickiness bonus after a loss.
    """
    lr_pos, lr_neg, beta, w, lam, stick_win, stick_lose = model_parameters
    n_trials = len(action_1)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    q_mf = np.zeros(2)
    q2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1
    last_reward = 0
    eps = 1e-10

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])

        if a1 == -1:
            continue

        # --- Stage 1 Policy ---
        max_q2 = np.max(q2, axis=1)
        q_mb = transition_matrix @ max_q2
        q_net = w * q_mb + (1 - w) * q_mf

        # Apply Outcome-Dependent Stickiness
        if last_action_1 != -1:
            if last_reward == 1:
                q_net[last_action_1] += stick_win
            else:
                q_net[last_action_1] += stick_lose

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q2[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updating ---
        delta_1 = q2[s, a2] - q_mf[a1]
        lr_1 = lr_pos if delta_1 > 0 else lr_neg
        q_mf[a1] += lr_1 * delta_1

        delta_2 = r - q2[s, a2]
        lr_2 = lr_pos if delta_2 > 0 else lr_neg
        q2[s, a2] += lr_2 * delta_2

        q_mf[a1] += lr_2 * lam * delta_2

        # Update history
        last_action_1 = a1
        last_reward = r

    mask = p_choice_1 > 0
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss
```

### Cognitive Model 3
**Model Description**: **Decoupled Planning/Action Model** with Asymmetric Learning and Stickiness.
This model introduces a structural change to the Model-Based valuation. Instead of assuming the agent expects to take the greedy `max` action at Stage 2 (standard MB), this model assumes the agent simulates the second stage using a specific "Planning Inverse Temperature" (`beta_plan`). This allows the agent to be "soft" or "aware of their own noise" during planning, distinct from the noise in their actual choices (`beta_choice`).

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Decoupled Planning/Action Model with Asymmetric Learning.
    
    Uses a 'Soft-Maximization' for the Model-Based calculation controlled by
    beta_plan, separate from the beta_choice used for actual action selection.
    
    Parameters:
    - lr_pos: [0, 1] Learning rate for positive prediction errors.
    - lr_neg: [0, 1] Learning rate for negative prediction errors.
    - beta_choice: [0, 10] Inverse temperature for action selection (Stage 1 & 2).
    - beta_plan: [0, 10] Inverse temperature used for internal MB valuation (planning).
    - w: [0, 1] Weight of Model-Based system.
    - lam: [0, 1] Eligibility trace.
    - stickiness: [0, 5] Choice perseveration bonus.
    """
    lr_pos, lr_neg, beta_choice, beta_plan, w, lam, stickiness = model_parameters
    n_trials = len(action_1)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    q_mf = np.zeros(2)
    q2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1
    eps = 1e-10

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])

        if a1 == -1:
            continue

        # --- Stage 1 Policy ---
        
        # Soft-Maximization for MB Planning
        # Calculate the expected value of the next state under a policy defined by beta_plan
        v_stage2_plan = np.zeros(2)
        for s_prime in range(2):
            # Probabilities of actions in s_prime given beta_plan
            exp_q_plan = np.exp(beta_plan * q2[s_prime])
            probs_plan = exp_q_plan / np.sum(exp_q_plan)
            # Value is the weighted sum of Q-values
            v_stage2_plan[s_prime] = np.sum(probs_plan * q2[s_prime])
            
        q_mb = transition_matrix @ v_stage2_plan
        q_net = w * q_mb + (1 - w) * q_mf

        if last_action_1 != -1:
            q_net[last_action_1] += stickiness

        # Action Selection uses beta_choice
        exp_q1 = np.exp(beta_choice * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        # Action Selection uses beta_choice
        exp_q2 = np.exp(beta_choice * q2[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updating ---
        delta_1 = q2[s, a2] - q_mf[a1]
        lr_1 = lr_pos if delta_1 > 0 else lr_neg
        q_mf[a1] += lr_1 * delta_1

        delta_2 = r - q2[s, a2]
        lr_2 = lr_pos if delta_2 > 0 else lr_neg
        q2[s, a2] += lr_2 * delta_2

        q_mf[a1] += lr_2 * lam * delta_2

        last_action_1 = a1

    mask = p_choice_1 > 0
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss
```