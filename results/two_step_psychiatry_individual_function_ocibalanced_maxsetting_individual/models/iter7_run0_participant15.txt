Here are three new cognitive models for the two-step decision task, designed to capture different psychological mechanisms based on the participant data and theoretical extensions of the standard hybrid model.

### Cognitive Model 1: Hybrid MB/MF with Correlated Counterfactual Updates
This model hypothesizes that the participant believes in an anti-correlation between the rewards of the two aliens on a planet (a common structural feature in such tasks). When the participant updates the value of the chosen alien based on a prediction error, they also update the unchosen alien's value in the opposite direction, scaled by a parameter `kappa`.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF with Correlated Counterfactual Updates.
    
    Includes a 'kappa' parameter that updates the unchosen alien's value 
    in the opposite direction of the chosen alien's prediction error.
    This captures the belief that if one alien is good, the other is likely bad.
    
    Parameters:
    - alpha: [0, 1] Learning rate.
    - kappa: [0, 1] Counterfactual update rate (0 = none, 1 = full opposite update).
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - w: [0, 1] Weighting of MB vs MF (1 = pure MB, 0 = pure MF).
    - stickiness: [0, 10] Choice perseverance bonus for Stage 1.
    """
    alpha, kappa, beta_1, beta_2, w, stickiness = model_parameters
    n_trials = len(action_1)
    
    # Transition matrix: A(0)->X(0) is 0.7, U(1)->Y(1) is 0.7
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Initialize values
    q_stage1_mf = np.zeros(2) + 0.5
    q_stage2_mf = np.zeros((2, 2)) + 0.5 # [planet, alien]
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based Value: Expected max value of next stage
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Logits
        logits_1 = beta_1 * q_net
        if last_action_1 != -1:
            logits_1[last_action_1] += stickiness
            
        # Probabilities
        exp_q1 = np.exp(logits_1 - np.max(logits_1)) # numeric stability
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        # Store prob of chosen action
        if action_1[trial] != -1:
            p_choice_1[trial] = probs_1[action_1[trial]]
        else:
            p_choice_1[trial] = 1.0
            last_action_1 = -1
            continue
            
        # --- Stage 2 Policy ---
        current_state = state[trial]
        # Skip if invalid state
        if current_state == -1: 
            p_choice_2[trial] = 1.0
            continue
            
        logits_2 = beta_2 * q_stage2_mf[current_state]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        if action_2[trial] != -1:
            p_choice_2[trial] = probs_2[action_2[trial]]
        else:
            p_choice_2[trial] = 1.0
            last_action_1 = action_1[trial]
            continue
            
        # --- Updates ---
        # Stage 2 RPE
        r = reward[trial]
        chosen_a2 = action_2[trial]
        delta_2 = r - q_stage2_mf[current_state, chosen_a2]
        
        # Update Chosen Stage 2
        q_stage2_mf[current_state, chosen_a2] += alpha * delta_2
        
        # Update Unchosen Stage 2 (Counterfactual)
        # We assume anti-correlation: if chosen is better than expected (delta>0), 
        # unchosen is assumed to be worse (subtracted).
        unchosen_a2 = 1 - chosen_a2
        q_stage2_mf[current_state, unchosen_a2] -= alpha * kappa * delta_2
        
        # Stage 1 MF Update (TD-learning)
        # Using max Q of the arrived state (SARSA-max/Q-learning style)
        v_stage2 = np.max(q_stage2_mf[current_state])
        delta_1 = v_stage2 - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += alpha * delta_1
        
        last_action_1 = action_1[trial]
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2: Hybrid MB/MF with Outcome-Dependent Exploration
This model proposes that the participant's exploration-exploitation balance (inverse temperature `beta`) at Stage 1 is dynamic and depends on the outcome of the previous trial. Specifically, they may explore more (lower beta) after a loss compared to after a win.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF with Outcome-Dependent Exploration (Beta-Win/Loss).
    
    Uses different inverse temperature parameters for Stage 1 choices depending on 
    whether the previous trial resulted in a reward (win) or no reward (loss).
    
    Parameters:
    - alpha: [0, 1] Learning rate.
    - beta_win: [0, 10] Stage 1 inverse temperature after a reward > 0.
    - beta_loss: [0, 10] Stage 1 inverse temperature after a reward <= 0.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - w: [0, 1] Weighting of MB vs MF.
    - stickiness: [0, 10] Choice perseverance bonus.
    """
    alpha, beta_win, beta_loss, beta_2, w, stickiness = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2) + 0.5
    q_stage2_mf = np.zeros((2, 2)) + 0.5
    
    last_action_1 = -1
    last_reward = 0 # Assume neutral/loss start
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Select beta based on previous outcome
        beta_1 = beta_win if last_reward > 0 else beta_loss
        
        logits_1 = beta_1 * q_net
        if last_action_1 != -1:
            logits_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        if action_1[trial] != -1:
            p_choice_1[trial] = probs_1[action_1[trial]]
        else:
            p_choice_1[trial] = 1.0
            last_action_1 = -1
            continue

        # --- Stage 2 Policy ---
        current_state = state[trial]
        # Skip invalid state
        if current_state == -1:
             p_choice_2[trial] = 1.0
             continue

        logits_2 = beta_2 * q_stage2_mf[current_state]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        if action_2[trial] != -1:
            p_choice_2[trial] = probs_2[action_2[trial]]
        else:
            p_choice_2[trial] = 1.0
            last_action_1 = action_1[trial]
            last_reward = 0 
            continue
            
        # --- Updates ---
        r = reward[trial]
        chosen_a2 = action_2[trial]
        
        # Stage 2 Update
        delta_2 = r - q_stage2_mf[current_state, chosen_a2]
        q_stage2_mf[current_state, chosen_a2] += alpha * delta_2
        
        # Stage 1 MF Update
        v_stage2 = np.max(q_stage2_mf[current_state])
        delta_1 = v_stage2 - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += alpha * delta_1
        
        last_action_1 = action_1[trial]
        last_reward = r

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3: Hybrid MB/MF with Goal-Directed Stickiness
This model differentiates between simple motor perseverance (`stick_action`) and "goal stickiness" (`stick_goal`). Goal stickiness biases the participant to choose the spaceship that leads to the *planet* they visited on the previous trial, regardless of which spaceship they originally chose. This mechanism relies on the transition matrix to map the previous goal (planet) back to the current action.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF with Goal-Directed Stickiness.
    
    Differentiates between repeating the same motor action (stick_action)
    and repeating the choice that leads to the previous planet (stick_goal).
    The latter uses the transition matrix to reinforce the spaceship leading 
    to the previously visited state.
    
    Parameters:
    - alpha: [0, 1] Learning rate.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - w: [0, 1] Weighting of MB vs MF.
    - stick_action: [0, 10] Perseverance on the previous spaceship choice.
    - stick_goal: [0, 10] Perseverance on the spaceship leading to previous planet.
    """
    alpha, beta_1, beta_2, w, stick_action, stick_goal = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2) + 0.5
    q_stage2_mf = np.zeros((2, 2)) + 0.5
    
    last_action_1 = -1
    last_state = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta_1 * q_net
        
        # Apply Stickiness
        if last_action_1 != -1:
            # Action Stickiness: Repeat the button press
            logits_1[last_action_1] += stick_action
            
            # Goal Stickiness: Reinforce action leading to last_state
            if last_state != -1:
                # Add bonus proportional to probability of reaching last_state from each action
                # For action 0: T[0, last_state]
                # For action 1: T[1, last_state]
                logits_1[0] += stick_goal * transition_matrix[0, last_state]
                logits_1[1] += stick_goal * transition_matrix[1, last_state]
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        if action_1[trial] != -1:
            p_choice_1[trial] = probs_1[action_1[trial]]
        else:
            p_choice_1[trial] = 1.0
            last_action_1 = -1
            last_state = -1
            continue

        # --- Stage 2 Policy ---
        current_state = state[trial]
        # Skip invalid state
        if current_state == -1:
             p_choice_2[trial] = 1.0
             continue

        logits_2 = beta_2 * q_stage2_mf[current_state]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        if action_2[trial] != -1:
            p_choice_2[trial] = probs_2[action_2[trial]]
        else:
            p_choice_2[trial] = 1.0
            last_action_1 = action_1[trial]
            last_state = current_state
            continue
            
        # --- Updates ---
        r = reward[trial]
        chosen_a2 = action_2[trial]
        
        delta_2 = r - q_stage2_mf[current_state, chosen_a2]
        q_stage2_mf[current_state, chosen_a2] += alpha * delta_2
        
        v_stage2 = np.max(q_stage2_mf[current_state])
        delta_1 = v_stage2 - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += alpha * delta_1
        
        last_action_1 = action_1[trial]
        last_state = current_state

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```