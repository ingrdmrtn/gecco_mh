Here are the three cognitive models based on the participant data and standard reinforcement learning hypotheses.

### Model 1: Hybrid Learner with Choice Kernel (Habit)
This model extends the standard hybrid learner by replacing simple one-step perseverance with a dynamic **Choice Kernel** (or Habit). This kernel tracks the history of chosen spaceships with a decaying trace, allowing the model to capture the strong "streaks" of choices observed in the participant data (e.g., repeated choices of spaceship 0 or 1 over many trials) better than a simple indicator function.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid learner with a Choice Kernel (Habit) for Stage 1.
    The Choice Kernel tracks a running average of past choices, creating a 'habit' 
    influence that decays over time, allowing for modeling of choice streaks.

    Parameters:
    - learning_rate: [0, 1] Update rate for MF values and Stage 2 values.
    - beta: [0, 10] Inverse temperature for softmax choice.
    - w: [0, 1] Weighting parameter (0 = Pure MF, 1 = Pure MB).
    - habit_decay: [0, 1] Decay rate of the choice trace (1 = only last choice matters, 0 = no forgetting).
    - habit_strength: [0, 10] Weight of the habit on the decision variable.
    """
    learning_rate, beta, w, habit_decay, habit_strength = model_parameters
    n_trials = len(action_1)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_mf = np.zeros(2)        # Stage 1 Model-Free values
    q_stage2 = np.zeros((2, 2)) # Stage 2 values (State, Alien)
    choice_kernel = np.zeros(2) # Habit trace for Spaceships
    
    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_mb = transition_matrix @ max_q_stage2
        
        # Net value combines MB, MF, and the Habit (Choice Kernel)
        q_net = w * q_mb + (1 - w) * q_mf + habit_strength * choice_kernel
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        s_idx = state[trial]
        
        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        a2 = action_2[trial]
        r = reward[trial]

        # --- Updates ---
        # 1. Update Stage 1 MF Value
        delta1 = q_stage2[s_idx, a2] - q_mf[a1]
        q_mf[a1] += learning_rate * delta1
        
        # 2. Update Stage 2 Value
        delta2 = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += learning_rate * delta2
        
        # 3. Update Stage 1 MF with second stage RPE (TD-1 like)
        q_mf[a1] += learning_rate * delta2
        
        # 4. Update Choice Kernel (Habit)
        # Increase trace for chosen action, decay both (or decay unchosen)
        # Implementation: Standard choice trace update
        choice_kernel[a1] = choice_kernel[a1] + habit_decay * (1 - choice_kernel[a1])
        choice_kernel[1 - a1] = choice_kernel[1 - a1] * (1 - habit_decay)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Learner with Transition Learning
This model relaxes the assumption that the participant perfectly knows the transition probabilities (0.7/0.3). Instead, the participant learns the transition matrix $T$ from experience. This allows the model to capture behavior where the participant might momentarily doubt the "commonness" of transitions after a string of rare outcomes, or if they build their model of the task from scratch. It includes perseverance to account for stickiness.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid learner that estimates the transition matrix (Transition Learning) 
    instead of using fixed probabilities. Includes perseverance.
    
    Parameters:
    - learning_rate: [0, 1] Update rate for value functions.
    - lr_trans: [0, 1] Learning rate for the transition matrix.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weighting parameter (MB vs MF).
    - perseverance: [0, 5] Stickiness bonus for the previously chosen spaceship.
    """
    learning_rate, lr_trans, beta, w, perseverance = model_parameters
    n_trials = len(action_1)

    # Initialize transition matrix to uniform (no prior knowledge) or biased.
    # Here we start uniform to allow learning to drive the MB values.
    # Rows: Action (0/1), Cols: State (0/1)
    trans_matrix = np.ones((2, 2)) * 0.5 
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_mf = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2, axis=1)
        
        # MB value uses the learned transition matrix
        q_mb = trans_matrix @ max_q_stage2
        
        q_net = w * q_mb + (1 - w) * q_mf
        
        if last_action_1 != -1:
            q_net[last_action_1] += perseverance

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        s_idx = state[trial]
        last_action_1 = a1

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        a2 = action_2[trial]
        r = reward[trial]

        # --- Updates ---
        # 1. Update Value Functions
        delta1 = q_stage2[s_idx, a2] - q_mf[a1]
        q_mf[a1] += learning_rate * delta1
        
        delta2 = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += learning_rate * delta2
        q_mf[a1] += learning_rate * delta2
        
        # 2. Update Transition Matrix (State Prediction Error)
        # Increase prob of observed state s_idx given action a1, decrease others
        trans_matrix[a1, s_idx] += lr_trans * (1 - trans_matrix[a1, s_idx])
        trans_matrix[a1, 1 - s_idx] += lr_trans * (0 - trans_matrix[a1, 1 - s_idx])

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Learner with Dual Perseverance
The participant data shows stickiness not just in spaceship choice (Stage 1), but also potentially in alien choice (Stage 2). This model includes separate perseverance parameters for Stage 1 (Spaceship) and Stage 2 (Alien), testing the hypothesis that habit formation occurs at both decision steps independently.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid learner with distinct perseverance (stickiness) parameters for 
    Stage 1 (Spaceship) and Stage 2 (Alien).
    
    Parameters:
    - learning_rate: [0, 1] Update rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] MB vs MF weight.
    - pers_1: [0, 5] Perseverance bonus for Stage 1 (Spaceship).
    - pers_2: [0, 5] Perseverance bonus for Stage 2 (Alien).
    """
    learning_rate, beta, w, pers_1, pers_2 = model_parameters
    n_trials = len(action_1)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_mf = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    last_action_1 = -1
    last_action_2 = -1 # Tracks last chosen alien (0 or 1) regardless of state

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_mb + (1 - w) * q_mf
        
        if last_action_1 != -1:
            q_net[last_action_1] += pers_1

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        last_action_1 = a1
        s_idx = state[trial]

        # --- Stage 2 Decision ---
        # Apply perseverance to the Stage 2 Q-values temporarily for choice
        q_s2_curr = q_stage2[s_idx].copy()
        
        if last_action_2 != -1:
            # Assuming motor perseverance: bias towards left/right alien (0/1)
            q_s2_curr[last_action_2] += pers_2
            
        exp_q2 = np.exp(beta * q_s2_curr)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        a2 = action_2[trial]
        last_action_2 = a2
        r = reward[trial]

        # --- Updates ---
        delta1 = q_stage2[s_idx, a2] - q_mf[a1]
        q_mf[a1] += learning_rate * delta1
        
        delta2 = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += learning_rate * delta2
        q_mf[a1] += learning_rate * delta2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```