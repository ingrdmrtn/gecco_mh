Here are three new cognitive models for the two-step decision task, designed to capture the participant's behavior using mechanisms like dynamic transition learning, habit formation, and multi-stage perseveration.

### Model 1: Hybrid MB/MF with Dynamic Transition Learning
This model hypothesizes that the participant does not assume fixed transition probabilities (0.7/0.3) but learns them over time. This accounts for the "rare transitions" and allows the Model-Based system to adapt its planning based on experienced spaceship-planet contingencies. It combines this with value decay and perseveration.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF model with Dynamic Transition Learning, Decay, and Perseveration.
    Learns transition probabilities (MB) online rather than using fixed priors.
    
    Parameters:
    lr (float): Learning rate for Q-values [0,1].
    beta (float): Inverse temperature [0,10].
    w (float): MB/MF Weighting [0,1].
    decay (float): Decay rate for Q-values per trial [0,1].
    pers (float): Perseveration bonus for Stage 1 choice [0,5].
    lr_trans (float): Learning rate for transition probabilities [0,1].
    """
    lr, beta, w, decay, pers, lr_trans = model_parameters
    n_trials = len(action_1)
  
    # Initialize estimated transitions (start agnostic or slightly biased)
    # Rows: Action (0/1), Cols: State (0/1). 
    # Initialize to 0.7/0.3 to match task instruction priors, but allow update.
    trans_probs = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # state x action
    
    prev_a1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
        
        # Decay Q-values
        q_stage1_mf *= (1 - decay)
        q_stage2_mf *= (1 - decay)

        # --- Stage 1 Policy ---
        # Model-Based: Use current estimated transition matrix
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = trans_probs @ max_q_stage2
        
        # Hybrid Value
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Selection
        logits_1 = beta * q_net_stage1
        if prev_a1 != -1:
            logits_1[prev_a1] += pers
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Policy ---
        state_idx = state[trial]
        logits_2 = beta * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        # 1. Transition Learning (MB Update)
        # Update probability of reaching observed state 'state_idx' given 'action_1'
        # P(s'|a) <- P(s'|a) + lr_trans * (1 - P(s'|a))
        a1 = action_1[trial]
        trans_probs[a1, state_idx] += lr_trans * (1 - trans_probs[a1, state_idx])
        trans_probs[a1, 1 - state_idx] = 1 - trans_probs[a1, state_idx] # Ensure sum to 1

        # 2. TD Updates (MF)
        # Stage 1 MF update using Stage 2 max value (TD(0))
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1

        # Stage 2 MF update using Reward
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr * delta_stage2
        
        # Eligibility Trace: Update Stage 1 again with Stage 2 RPE (TD(1))
        q_stage1_mf[a1] += lr * delta_stage2
        
        prev_a1 = a1

    eps = 1e-10
    valid_mask = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss
```

### Model 2: Hybrid MB/MF with Choice Kernel (Habit)
This model replaces simple one-step perseveration with a "Choice Kernel" or Habit mechanism. This tracks the frequency of past choices using an exponential moving average. This allows the model to capture the strength of streaks (stronger habit after multiple repetitions) rather than just a binary "repeat previous" bias.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF model with Choice Kernel (Habit) and Value Decay.
    Uses an exponential moving average to track choice habits instead of 1-back perseveration.
    
    Parameters:
    lr (float): Learning rate for Q-values [0,1].
    beta (float): Inverse temperature [0,10].
    w (float): MB/MF Weighting [0,1].
    decay (float): Decay rate for Q-values per trial [0,1].
    lr_habit (float): Learning rate for habit (choice kernel) formation [0,1].
    habit_w (float): Weight of the habit strength in decision making [0,10].
    """
    lr, beta, w, decay, lr_habit, habit_w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Habit values for Stage 1 actions (Choice Kernel)
    habit_s1 = np.zeros(2)

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue

        # Decay Q-values
        q_stage1_mf *= (1 - decay)
        q_stage2_mf *= (1 - decay)

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add Habit bonus to logits
        logits_1 = beta * (q_net_stage1 + habit_w * habit_s1)
        
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Policy ---
        state_idx = state[trial]
        logits_2 = beta * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        a1 = action_1[trial]
        a2 = action_2[trial]
        
        # Update Habit (Choice Kernel)
        # Chosen action increases towards 1, unchosen decays towards 0
        habit_s1[a1] += lr_habit * (1 - habit_s1[a1])
        habit_s1[1 - a1] *= (1 - lr_habit)

        # MF Updates
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1

        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += lr * delta_stage2
        
        q_stage1_mf[a1] += lr * delta_stage2

    eps = 1e-10
    valid_mask = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss
```

### Model 3: Hybrid MB/MF with Dual-Stage Perseveration
This model posits that "stickiness" applies to both decision stages. The participant may have a tendency to repeat their choice of Spaceship (Stage 1) and also their choice of Alien (Stage 2) within a specific planet. This captures habitual behavior at the level of specific motor responses or alien preferences, combined with global value decay.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF model with Dual-Stage Perseveration and Value Decay.
    Applies perseveration bonuses to both spaceship choices and alien choices (per planet).
    
    Parameters:
    lr (float): Learning rate for Q-values [0,1].
    beta (float): Inverse temperature [0,10].
    w (float): MB/MF Weighting [0,1].
    decay (float): Decay rate for Q-values per trial [0,1].
    pers_s1 (float): Perseveration bonus for Stage 1 (Spaceship) [0,5].
    pers_s2 (float): Perseveration bonus for Stage 2 (Alien) [0,5].
    """
    lr, beta, w, decay, pers_s1, pers_s2 = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_a1 = -1
    # Track previous choice for Stage 2 separately for each state (Planet X and Y)
    prev_a2 = np.array([-1, -1]) 

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue

        q_stage1_mf *= (1 - decay)
        q_stage2_mf *= (1 - decay)

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net_stage1
        if prev_a1 != -1:
            logits_1[prev_a1] += pers_s1
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Policy ---
        state_idx = state[trial]
        logits_2 = beta * q_stage2_mf[state_idx]
        
        # Apply Stage 2 perseveration if a previous choice exists for this state
        if prev_a2[state_idx] != -1:
            logits_2[prev_a2[state_idx]] += pers_s2
            
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        a1 = action_1[trial]
        a2 = action_2[trial]

        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1

        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += lr * delta_stage2
        
        q_stage1_mf[a1] += lr * delta_stage2
        
        prev_a1 = a1
        prev_a2[state_idx] = a2

    eps = 1e-10
    valid_mask = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss
```