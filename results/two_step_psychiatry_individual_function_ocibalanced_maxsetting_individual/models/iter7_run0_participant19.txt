Here are the three proposed cognitive models.

### Model 1: Momentum Q-Learning
This model replaces simple one-step stickiness with a "Choice Kernel" or momentum term. The kernel accumulates the history of choices and decays over time, allowing the model to capture the participant's tendency to persist in a strategy (streaks) more effectively than simple repetition.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Model 1: Momentum Q-Learning (Choice Kernel).
    
    A Model-Free learner that tracks a 'Choice Kernel' for the first stage.
    Instead of simple stickiness (repeating the last choice), the kernel 
    accumulates a history of past choices, decaying over time. This captures 
    momentum or longer-term perseveration.

    Parameters:
    - alpha: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - stick_decay: [0, 1] Decay rate of the choice kernel (0 = instant decay/standard stickiness, 1 = no decay).
    - stick_weight: [0, 10] Weight of the choice kernel in the decision.
    """
    alpha, beta, stick_decay, stick_weight = model_parameters
    n_trials = len(action_1)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    choice_kernel = np.zeros(2)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for t in range(n_trials):
        # --- Stage 1 Decision ---
        # Combine Q-values and Choice Kernel
        logits_1 = beta * q_stage1 + stick_weight * choice_kernel
        
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]
        
        # Update Choice Kernel
        # The chosen action's kernel increases; both decay.
        # k(t+1) = k(t) * decay + 1(if chosen)
        choice_kernel *= stick_decay
        choice_kernel[a1] += 1.0

        # --- Stage 2 Decision ---
        s_idx = state[t]
        logits_2 = beta * q_stage2[s_idx]
        
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # --- Learning ---
        r = reward[t]
        
        # TD(0) updates
        # Stage 2 update
        delta2 = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += alpha * delta2
        
        # Stage 1 update (based on Stage 2 value)
        target_val = q_stage2[s_idx, a2]
        delta1 = target_val - q_stage1[a1]
        q_stage1[a1] += alpha * delta1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model with Forgetting and Stickiness
This model integrates Model-Based (MB) and Model-Free (MF) systems. It incorporates "Forgetting" (decay of unchosen options) to handle the drifting reward probabilities of the aliens, and "Stickiness" to account for behavioral inertia. This combines the structural knowledge of the task with the necessary adaptability for the changing environment.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model 2: Hybrid Model with Forgetting and Stickiness.
    
    Combines Model-Based (MB) and Model-Free (MF) reinforcement learning.
    Includes a 'forgetting' mechanism to handle drifting reward probabilities
    by decaying unchosen Q-values, and 'stickiness' to capture choice persistence.
    
    Parameters:
    - alpha: [0, 1] Learning rate for MF updates.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight (0 = pure MF, 1 = pure MB).
    - forget_rate: [0, 1] Decay rate for unchosen actions.
    - stickiness: [0, 10] Choice stickiness parameter.
    """
    alpha, beta, w, forget_rate, stickiness = model_parameters
    n_trials = len(action_1)
    
    # Fixed transition matrix as per task description: A->X (0->0) is 0.7
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1
    
    for t in range(n_trials):
        # --- Stage 1 Decision ---
        # Model-Based Value: Expected max value of next stage
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net
        if prev_a1 != -1:
            logits_1[prev_a1] += stickiness
            
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]
        
        # --- Stage 2 Decision ---
        s_idx = state[t]
        logits_2 = beta * q_stage2_mf[s_idx]
        
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]
        
        # --- Learning ---
        r = reward[t]
        
        # Update Stage 2 MF
        q_stage2_mf[s_idx, a2] += alpha * (r - q_stage2_mf[s_idx, a2])
        # Forgetting for unchosen Stage 2 action
        q_stage2_mf[s_idx, 1 - a2] *= (1.0 - forget_rate)
        
        # Update Stage 1 MF (TD(0))
        target_val = q_stage2_mf[s_idx, a2]
        q_stage1_mf[a1] += alpha * (target_val - q_stage1_mf[a1])
        # Forgetting for unchosen Stage 1 action
        q_stage1_mf[1 - a1] *= (1.0 - forget_rate)
        
        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: TD(Lambda) with Forgetting and Stickiness
This model uses Eligibility Traces (Lambda) to bridge the two stages. It allows the outcome (reward) at the second stage to directly influence the value of the first stage choice, weighted by `lambda_param`. It also includes forgetting to manage the non-stationary rewards and stickiness for behavioral persistence.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model 3: TD(Lambda) with Forgetting and Stickiness.
    
    A Model-Free learner using Eligibility Traces (TD-Lambda). This allows
    the reward at Stage 2 to directly update Stage 1 values, bridging the 
    temporal gap. Includes forgetting for unchosen options and stickiness.
    
    Parameters:
    - alpha: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - lambda_param: [0, 1] Eligibility trace decay parameter (0 = TD(0), 1 = Monte Carlo).
    - forget_rate: [0, 1] Decay rate for unchosen actions.
    - stickiness: [0, 10] Choice stickiness parameter.
    """
    alpha, beta, lambda_param, forget_rate, stickiness = model_parameters
    n_trials = len(action_1)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1
    
    for t in range(n_trials):
        # --- Stage 1 Decision ---
        logits_1 = beta * q_stage1.copy()
        if prev_a1 != -1:
            logits_1[prev_a1] += stickiness
            
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]
        
        # --- Stage 2 Decision ---
        s_idx = state[t]
        logits_2 = beta * q_stage2[s_idx]
        
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]
        
        # --- Learning ---
        r = reward[t]
        
        # Prediction errors
        # Delta 2: Reward Prediction Error at Stage 2
        delta2 = r - q_stage2[s_idx, a2]
        
        # Delta 1: State Prediction Error (Stage 1 value vs Stage 2 value)
        delta1 = q_stage2[s_idx, a2] - q_stage1[a1]
        
        # Update Stage 2
        q_stage2[s_idx, a2] += alpha * delta2
        # Forgetting Stage 2
        q_stage2[s_idx, 1 - a2] *= (1.0 - forget_rate)
        
        # Update Stage 1 with Eligibility Trace
        # Q1(a1) <- Q1(a1) + alpha * (delta1 + lambda * delta2)
        q_stage1[a1] += alpha * (delta1 + lambda_param * delta2)
        # Forgetting Stage 1
        q_stage1[1 - a1] *= (1.0 - forget_rate)
        
        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```