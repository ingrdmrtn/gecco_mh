Here are three cognitive models formulated as Python functions.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Q-Learning with Q-Decay and Decaying Choice Trace.
    
    This model assumes the participant is Model-Free but has two forms of memory dynamics:
    1. Forgetting: Unchosen options' Q-values decay over time (Decay).
    2. Habit: Stickiness is modeled as a 'choice trace' that accumulates and decays, 
       capturing longer-term perseverance beyond just the immediately previous trial.
    
    Parameters:
    learning_rate: [0, 1] Learning rate for Q-value updates.
    q_decay: [0, 1] Decay rate for unchosen Q-values (Q_new = Q_old * (1 - q_decay)).
    beta: [0, 10] Inverse temperature (softmax sensitivity).
    stick_decay: [0, 1] Decay rate for the choice trace (Trace_new = Trace_old * stick_decay + choice).
    stick_weight: [0, 5] Weight of the choice trace added to Q-values for Stage 1 selection.
    """
    learning_rate, q_decay, beta, stick_decay, stick_weight = model_parameters
    n_trials = len(action_1)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    choice_trace = np.zeros(2)
    
    log_likelihood = 0.0
    eps = 1e-10
    
    for trial in range(n_trials):
        a1 = action_1[trial]
        s2 = state[trial]
        a2 = action_2[trial]
        r = reward[trial]

        # Stage 1 Choice Policy
        # Effective Q includes the accumulated habit (choice trace)
        q_eff_1 = q_stage1 + stick_weight * choice_trace
        
        exp_q1 = np.exp(beta * q_eff_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_likelihood += np.log(probs_1[a1] + eps)
        
        # Update Choice Trace
        choice_trace *= stick_decay
        choice_trace[a1] += 1.0
        
        if a2 == -1:
            continue

        # Stage 2 Choice Policy
        exp_q2 = np.exp(beta * q_stage2[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_likelihood += np.log(probs_2[a2] + eps)

        # Updates
        
        # Stage 2 Update (Direct Reward Learning)
        rpe_2 = r - q_stage2[s2, a2]
        q_stage2[s2, a2] += learning_rate * rpe_2
        
        # Decay unchosen options in Stage 2
        q_stage2[s2, 1-a2] *= (1.0 - q_decay)
        q_stage2[1-s2, :] *= (1.0 - q_decay)

        # Stage 1 Update (TD Learning)
        # Using the value of the chosen state-action in stage 2 as target
        rpe_1 = q_stage2[s2, a2] - q_stage1[a1]
        q_stage1[a1] += learning_rate * rpe_1
        
        # Decay unchosen options in Stage 1
        q_stage1[1-a1] *= (1.0 - q_decay)

    return -log_likelihood

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF with Asymmetric Learning Rates.
    
    A hybrid Model-Based (MB) and Model-Free (MF) agent that learns differently from 
    positive vs. negative prediction errors. This captures potential biases in 
    processing rewards (wins) versus omissions/punishments (losses).
    
    Parameters:
    alpha_pos: [0, 1] Learning rate for positive prediction errors (RPE > 0).
    alpha_neg: [0, 1] Learning rate for negative prediction errors (RPE <= 0).
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Weighting parameter for MB vs MF (0 = Pure MF, 1 = Pure MB).
    stickiness: [0, 5] Choice perseverance bonus for the previously chosen spaceship.
    """
    alpha_pos, alpha_neg, beta, w, stickiness = model_parameters
    n_trials = len(action_1)
    
    # Fixed transition probabilities based on task structure:
    # Action 0 -> State 0 (0.7), State 1 (0.3)
    # Action 1 -> State 0 (0.3), State 1 (0.7)
    trans_probs = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_mf_stage1 = np.zeros(2)
    q_mf_stage2 = np.zeros((2, 2))
    
    log_likelihood = 0.0
    eps = 1e-10
    last_action_1 = -1
    
    for trial in range(n_trials):
        a1 = action_1[trial]
        s2 = state[trial]
        a2 = action_2[trial]
        r = reward[trial]
        
        # Stage 1 Choice Policy
        # Calculate Model-Based values: V_MB(s1) = T * max(Q_MF(s2))
        max_q2 = np.max(q_mf_stage2, axis=1)
        q_mb_stage1 = trans_probs @ max_q2
        
        # Integrated Value
        q_net_1 = w * q_mb_stage1 + (1 - w) * q_mf_stage1
        
        # Apply Stickiness
        if last_action_1 != -1:
            q_net_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_likelihood += np.log(probs_1[a1] + eps)
        
        last_action_1 = a1
        
        if a2 == -1:
            continue
            
        # Stage 2 Choice Policy
        exp_q2 = np.exp(beta * q_mf_stage2[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_likelihood += np.log(probs_2[a2] + eps)
        
        # Updates with Asymmetric Learning Rates
        
        # Stage 2 Update
        rpe_2 = r - q_mf_stage2[s2, a2]
        alpha_2 = alpha_pos if rpe_2 > 0 else alpha_neg
        q_mf_stage2[s2, a2] += alpha_2 * rpe_2
        
        # Stage 1 Update (TD)
        rpe_1 = q_mf_stage2[s2, a2] - q_mf_stage1[a1]
        alpha_1 = alpha_pos if rpe_1 > 0 else alpha_neg
        q_mf_stage1[a1] += alpha_1 * rpe_1
        
    return -log_likelihood

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Transition-Dependent MF Learning with Decay.
    
    A Model-Free agent that modulates its Stage 1 learning rate based on whether 
    the observed transition was Common or Rare. This approximates structure learning
    by allowing the agent to discount (or boost) updates from 'surprising' transitions.
    Also includes decay for unchosen options.
    
    Parameters:
    learning_rate: [0, 1] Base learning rate (for Stage 2 and Common Stage 1 updates).
    rare_multiplier: [0, 2] Multiplier applied to learning rate for Rare transitions in Stage 1.
    beta: [0, 10] Inverse temperature.
    stickiness: [0, 5] Choice perseverance bonus.
    decay_rate: [0, 1] Decay rate for unchosen Q-values.
    """
    learning_rate, rare_multiplier, beta, stickiness, decay_rate = model_parameters
    n_trials = len(action_1)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    log_likelihood = 0.0
    eps = 1e-10
    last_action_1 = -1
    
    for trial in range(n_trials):
        a1 = action_1[trial]
        s2 = state[trial]
        a2 = action_2[trial]
        r = reward[trial]
        
        # Stage 1 Choice Policy
        q_eff_1 = q_stage1.copy()
        if last_action_1 != -1:
            q_eff_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_eff_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_likelihood += np.log(probs_1[a1] + eps)
        
        last_action_1 = a1
        
        if a2 == -1:
            continue
            
        # Stage 2 Choice Policy
        exp_q2 = np.exp(beta * q_stage2[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_likelihood += np.log(probs_2[a2] + eps)
        
        # Updates
        
        # Stage 2 Update
        rpe_2 = r - q_stage2[s2, a2]
        q_stage2[s2, a2] += learning_rate * rpe_2
        
        # Decay unchosen Stage 2
        q_stage2[s2, 1-a2] *= (1.0 - decay_rate)
        q_stage2[1-s2, :] *= (1.0 - decay_rate)
        
        # Stage 1 Update
        # Determine if transition was Common (0->0 or 1->1) or Rare
        is_common = (a1 == s2)
        
        # Modulate learning rate based on transition type
        lr_s1 = learning_rate if is_common else (learning_rate * rare_multiplier)
        
        rpe_1 = q_stage2[s2, a2] - q_stage1[a1]
        q_stage1[a1] += lr_s1 * rpe_1
        
        # Decay unchosen Stage 1
        q_stage1[1-a1] *= (1.0 - decay_rate)
        
    return -log_likelihood
```