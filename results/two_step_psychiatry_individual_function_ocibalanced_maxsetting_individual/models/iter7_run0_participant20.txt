Here are three new cognitive models for the two-step task, designed to capture specific behavioral mechanisms such as differential learning rates, outcome-based perseveration, and reward-dependent stickiness.

### Model 1: Dual-Rate Dual-Stickiness Model
This model hypothesizes that the participant learns the value of spaceships (Stage 1) and aliens (Stage 2) at different rates. It also includes separate stickiness parameters for each stage to account for different levels of motor perseveration in the high-level (spaceship) versus low-level (alien) choices.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Dual-Rate Dual-Stickiness Model.
    
    Distinguishes between learning rates for the spaceship choice (Stage 1)
    and the alien choice (Stage 2). Also includes separate stickiness
    parameters for each stage to capture different perseveration tendencies.
    
    Parameters:
    - lr_1: [0,1] Learning rate for Stage 1 (Spaceship value).
    - lr_2: [0,1] Learning rate for Stage 2 (Alien value).
    - beta_1: [0,10] Inverse temperature for Stage 1.
    - beta_2: [0,10] Inverse temperature for Stage 2.
    - w: [0,1] Weighting of Model-Based system (0=MF, 1=MB).
    - stick_1: [0,10] Stickiness for Stage 1 (Spaceship).
    - stick_2: [0,10] Stickiness for Stage 2 (Alien).
    """
    lr_1, lr_2, beta_1, beta_2, w, stick_1, stick_2 = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # state x action
    
    last_action_1 = -1
    last_action_2 = -1
    
    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        logits_1 = beta_1 * q_net
        
        if last_action_1 != -1:
            logits_1[last_action_1] += stick_1
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_action_1 = action_1[trial]
        state_idx = state[trial]
        
        # --- Stage 2 Policy ---
        logits_2 = beta_2 * q_stage2_mf[state_idx]
        
        if last_action_2 != -1:
            logits_2[last_action_2] += stick_2
            
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        last_action_2 = action_2[trial]
        
        # --- Updates ---
        # Stage 1 Update (TD(0))
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr_1 * delta_stage1
        
        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr_2 * delta_stage2
        
    eps = 1e-10
    valid_idx = p_choice_1 > 0
    log_loss = -(np.sum(np.log(p_choice_1[valid_idx] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_idx] + eps)))
    return log_loss
```

### Model 2: Action and Outcome Stickiness Model
This model separates perseveration into two components: "Action Stickiness" (repeating the same motor response) and "Outcome Stickiness" (repeating the choice that leads to the previously visited planet). This captures the participant's potential desire to return to a specific context (planet) regardless of the transition probability that got them there.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Action and Outcome Stickiness Model.
    
    Separates stickiness into 'Action Stickiness' (repeating the motor choice)
    and 'Outcome Stickiness' (repeating the choice that leads to the previous planet).
    This captures the tendency to want to return to a previously visited state,
    distinct from just repeating the button press.
    
    Parameters:
    - learning_rate: [0,1] Update rate.
    - beta_1: [0,10] Inv temp Stage 1.
    - beta_2: [0,10] Inv temp Stage 2.
    - w: [0,1] MB/MF weight.
    - stick_action: [0,10] Stickiness for Stage 1 motor choice.
    - stick_outcome: [0,10] Stickiness for the action leading to the previous planet.
    """
    learning_rate, beta_1, beta_2, w, stick_action, stick_outcome = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    last_state = -1
    
    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        logits_1 = beta_1 * q_net
        
        # Action Stickiness
        if last_action_1 != -1:
            logits_1[last_action_1] += stick_action
            
        # Outcome Stickiness
        if last_state != -1:
            # Add stickiness to the action that commonly leads to last_state
            # Common transitions: Action 0 -> State 0, Action 1 -> State 1 (0.7 prob)
            logits_1[last_state] += stick_outcome
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_action_1 = action_1[trial]
        state_idx = state[trial]
        last_state = state_idx
        
        # --- Stage 2 Policy ---
        logits_2 = beta_2 * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
    eps = 1e-10
    valid_idx = p_choice_1 > 0
    log_loss = -(np.sum(np.log(p_choice_1[valid_idx] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_idx] + eps)))
    return log_loss
```

### Model 3: Win-Stay Lose-Shift Stickiness Model
This model refines the concept of stickiness by making it dependent on the previous outcome. It applies different "sticky" weights to the previous Stage 1 choice depending on whether the previous trial resulted in a reward (Win) or not (Loss).

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Win-Stay Lose-Shift Stickiness Model.
    
    Modulates Stage 1 stickiness based on the reward received in the previous trial.
    A 'Win' (reward=1) might induce stronger stickiness than a 'Loss' (reward=0).
    Also includes a global stickiness parameter for Stage 2.
    
    Parameters:
    - learning_rate: [0,1] Update rate.
    - beta_1: [0,10] Inv temp Stage 1.
    - beta_2: [0,10] Inv temp Stage 2.
    - w: [0,1] MB/MF weight.
    - stick_1_win: [0,10] Stage 1 stickiness after a reward.
    - stick_1_loss: [0,10] Stage 1 stickiness after no reward.
    - stick_2: [0,10] Stage 2 stickiness (global).
    """
    learning_rate, beta_1, beta_2, w, stick_1_win, stick_1_loss, stick_2 = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    last_action_2 = -1
    last_reward = -1
    
    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        logits_1 = beta_1 * q_net
        
        if last_action_1 != -1:
            if last_reward == 1:
                logits_1[last_action_1] += stick_1_win
            else:
                logits_1[last_action_1] += stick_1_loss
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_action_1 = action_1[trial]
        state_idx = state[trial]
        
        # --- Stage 2 Policy ---
        logits_2 = beta_2 * q_stage2_mf[state_idx]
        
        if last_action_2 != -1:
            logits_2[last_action_2] += stick_2
            
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        last_action_2 = action_2[trial]
        last_reward = reward[trial]
        
        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
    eps = 1e-10
    valid_idx = p_choice_1 > 0
    log_loss = -(np.sum(np.log(p_choice_1[valid_idx] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_idx] + eps)))
    return log_loss
```