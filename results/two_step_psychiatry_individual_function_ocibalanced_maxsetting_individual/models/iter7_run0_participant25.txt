Here are the three proposed cognitive models.

### Model 1: Asymmetric TD($\lambda$) with Stickiness
This model builds upon the "best model so far" (Asymmetric TD($\lambda$)) by incorporating **Choice Stickiness**. The participant data shows strong evidence of perseveration (streaks of choosing the same spaceship), which simple value-based models often fail to capture. This model hypothesizes that the participant learns asymmetrically from positive vs. negative prediction errors (optimism/pessimism bias) and simultaneously exhibits a tendency to repeat previous actions regardless of value.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Asymmetric TD(lambda) with Choice Stickiness.
    
    Combines asymmetric learning rates (valence-dependent updates) with an eligibility trace
    and a choice stickiness parameter (perseveration).
    
    Parameters:
    - lr_pos: [0, 1] Learning rate for positive prediction errors.
    - lr_neg: [0, 1] Learning rate for negative prediction errors.
    - beta: [0, 10] Inverse temperature (softmax sensitivity).
    - lam: [0, 1] Eligibility trace decay (0 = TD(0), 1 = Monte Carlo).
    - stickiness: [0, 10] Bonus added to the logits of the previously chosen action.
    """
    lr_pos, lr_neg, beta, lam, stickiness = model_parameters
    n_trials = len(action_1)
    
    # Initialize Q-values (0.5 represents neutral expectation for 0/1 rewards)
    q_stage1 = np.zeros(2) + 0.5
    q_stage2 = np.zeros((2, 2)) + 0.5
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Track last choice for stickiness (initialize with no previous choice)
    last_action_1 = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        q_s1_modified = q_stage1.copy()
        if last_action_1 != -1:
            q_s1_modified[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_s1_modified)
        # Safety for overflow
        if np.any(np.isinf(exp_q1)):
            exp_q1 = np.nan_to_num(exp_q1) 
        
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # Update last action
        last_action_1 = a1
        
        # --- Stage 2 Decision ---
        s_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2[s_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        r = reward[trial]
        
        # --- Learning ---
        # Stage 1 TD Error (TD(0) part)
        pe_1 = q_stage2[s_idx, a2] - q_stage1[a1]
        lr_1 = lr_pos if pe_1 > 0 else lr_neg
        q_stage1[a1] += lr_1 * pe_1
        
        # Stage 2 TD Error
        pe_2 = r - q_stage2[s_idx, a2]
        lr_2 = lr_pos if pe_2 > 0 else lr_neg
        q_stage2[s_idx, a2] += lr_2 * pe_2
        
        # Eligibility Trace Update for Stage 1 (TD(lambda) part)
        # Note: We use the learning rate associated with the Stage 2 outcome
        q_stage1[a1] += lr_2 * lam * pe_2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Reward-Dependent Eligibility Trace
This model hypothesizes that the "credit assignment" process itself is biased by the outcome. Instead of a fixed eligibility trace parameter $\lambda$, the participant uses different trace decay rates for wins ($\lambda_{win}$) and losses ($\lambda_{loss}$). For example, they might strongly associate a win with their initial choice (high $\lambda_{win}$), but treat a loss as "bad luck" or noise, failing to propagate the negative signal back to the first stage choice (low $\lambda_{loss}$).

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Reward-Dependent Eligibility Trace (Asymmetric Lambda).
    
    Hypothesizes that the strength of the link between the Stage 2 outcome and 
    the Stage 1 choice depends on the reward valence.
    
    Parameters:
    - lr: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - lam_win: [0, 1] Eligibility trace strength when Reward = 1.
    - lam_loss: [0, 1] Eligibility trace strength when Reward = 0.
    - stickiness: [0, 10] Choice stickiness bonus.
    """
    lr, beta, lam_win, lam_loss, stickiness = model_parameters
    n_trials = len(action_1)
    
    q_stage1 = np.zeros(2) + 0.5
    q_stage2 = np.zeros((2, 2)) + 0.5
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        q_s1_modified = q_stage1.copy()
        if last_action_1 != -1:
            q_s1_modified[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_s1_modified)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        last_action_1 = a1
        
        # --- Stage 2 Decision ---
        s_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2[s_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        r = reward[trial]
        
        # --- Learning ---
        # Standard TD updates for values
        pe_1 = q_stage2[s_idx, a2] - q_stage1[a1]
        q_stage1[a1] += lr * pe_1
        
        pe_2 = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += lr * pe_2
        
        # Outcome-Dependent Trace Update
        # If reward is 1, use lam_win; if 0, use lam_loss
        lam_curr = lam_win if r == 1 else lam_loss
        q_stage1[a1] += lr * lam_curr * pe_2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Transition-Gated Model-Free Learning
This model proposes that the participant modulates their learning based on the transition structure (Common vs. Rare). Rather than a full Model-Based planner, the participant uses a simpler heuristic: they pay full attention to outcomes following "Common" transitions but may suppress (or enhance) learning from "Rare" transitions. This "Transition Gating" acts as a filter, allowing a Model-Free system to mimic Model-Based behavior (by ignoring misleading rare transitions) or fail in specific ways.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Transition-Gated Model-Free Learning.
    
    The learning rate for the Stage 1 update is modulated by whether the 
    transition was Common or Rare. This allows the model to filter out 
    noise from rare transitions (or over-weight them).
    
    Parameters:
    - lr_common: [0, 1] Learning rate after a Common transition.
    - lr_rare: [0, 1] Learning rate after a Rare transition.
    - beta: [0, 10] Inverse temperature.
    - lam: [0, 1] Eligibility trace decay.
    - stickiness: [0, 10] Choice stickiness bonus.
    """
    lr_common, lr_rare, beta, lam, stickiness = model_parameters
    n_trials = len(action_1)
    
    q_stage1 = np.zeros(2) + 0.5
    q_stage2 = np.zeros((2, 2)) + 0.5
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        q_s1_modified = q_stage1.copy()
        if last_action_1 != -1:
            q_s1_modified[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_s1_modified)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        last_action_1 = a1
        
        # --- Stage 2 Decision ---
        s_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2[s_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        r = reward[trial]
        
        # --- Determine Transition Type ---
        # Assuming 0->0 and 1->1 are Common, 0->1 and 1->0 are Rare
        # This aligns with standard 2-step task structures.
        is_common = (a1 == s_idx)
        lr_curr = lr_common if is_common else lr_rare
        
        # --- Learning ---
        # Stage 1 Update: Gated by transition type
        pe_1 = q_stage2[s_idx, a2] - q_stage1[a1]
        q_stage1[a1] += lr_curr * pe_1
        
        # Stage 2 Update: Standard bandit learning (using lr_common as base rate)
        pe_2 = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += lr_common * pe_2
        
        # Eligibility Trace: Also gated by transition type for the Stage 1 update
        q_stage1[a1] += lr_curr * lam * pe_2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```