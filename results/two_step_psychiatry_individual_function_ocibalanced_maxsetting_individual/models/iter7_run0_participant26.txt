Here are three new cognitive models based on the participant's behavior, specifically addressing the strong alternation patterns (Win-Switch) and potential dynamic beliefs about the environment.

### Model 1: Dynamic Transition Perception Model-Based Learner
This model assumes the participant does not view the spaceship transition probabilities as fixed (70/30) but learns them over time. This captures "Gambler's Fallacy" or "Hot Hand" beliefs about the spaceships' reliability, which modulates the model-based value calculation.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Dynamic Transition Perception Model-Based Learner.
    
    The participant uses a Model-Based strategy but updates their belief about 
    the transition matrix (probability of Common transition) trial-by-trial.
    
    Parameters:
    - learning_rate: [0, 1] Update rate for Stage 2 alien values.
    - lr_prob: [0, 1] Learning rate for the transition probability p(Common).
    - beta_1: [0, 10] Inverse temperature for Stage 1 choice.
    - beta_2: [0, 10] Inverse temperature for Stage 2 choice.
    - stickiness: [-5, 5] Choice stickiness (positive=perseveration, negative=alternation).
    """
    learning_rate, lr_prob, beta_1, beta_2, stickiness = model_parameters
    n_trials = len(action_1)
    
    # Initialize Stage 2 Q-values (2 planets x 2 aliens)
    q_stage2 = np.zeros((2, 2))
    
    # Initialize belief about transition probability (starts at true value 0.7)
    p_common = 0.7
    
    log_loss = 0.0
    eps = 1e-10
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        # 1. Calculate Stage 1 Q-values using dynamic transition belief
        # Transition matrix: [[p_c, 1-p_c], [1-p_c, p_c]]
        # Row 0 is Action 0 (Spaceship A), Row 1 is Action 1 (Spaceship U)
        # Col 0 is Planet 0, Col 1 is Planet 1
        transition_matrix = np.array([[p_common, 1.0 - p_common], 
                                      [1.0 - p_common, p_common]])
        
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_stage1 = transition_matrix @ max_q_stage2
        
        # Add stickiness
        if last_action_1 != -1:
            q_stage1[last_action_1] += stickiness
            
        # Stage 1 Choice Probability
        exp_q1 = np.exp(beta_1 * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        chosen_a1 = int(action_1[trial])
        log_loss -= np.log(probs_1[chosen_a1] + eps)
        
        # 2. Stage 2 Choice Probability
        current_state = int(state[trial])
        exp_q2 = np.exp(beta_2 * q_stage2[current_state])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        chosen_a2 = int(action_2[trial])
        log_loss -= np.log(probs_2[chosen_a2] + eps)
        
        # 3. Updates
        r = reward[trial]
        
        # Update Stage 2 Q-values (Standard RW)
        delta_stage2 = r - q_stage2[current_state, chosen_a2]
        q_stage2[current_state, chosen_a2] += learning_rate * delta_stage2
        
        # Update Transition Belief
        # Determine if the observed transition was 'Common'
        # Action 0 -> State 0 is Common; Action 1 -> State 1 is Common
        is_common = 1.0 if (chosen_a1 == current_state) else 0.0
        
        p_common += lr_prob * (is_common - p_common)
        # Clamp probability to avoid numerical issues
        p_common = np.clip(p_common, 0.01, 0.99)
        
        last_action_1 = chosen_a1

    return log_loss
```

### Model 2: Reward-Rate Modulated Stickiness (MB)
This model posits that the participant's tendency to repeat or switch choices depends on their recent success rate. High reward rates might induce "Win-Switch" behavior (exploration/foraging), while low reward rates might induce "Lose-Stay" (perseveration). This is captured by a stickiness parameter that varies dynamically with the average reward rate.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Reward-Rate Modulated Stickiness Model-Based Learner.
    
    Stickiness is not constant but a linear function of the exponentially 
    weighted average reward rate. This captures shifting strategies 
    (e.g., switching more often when winning).
    
    Parameters:
    - learning_rate: [0, 1] Update rate for Stage 2 alien values.
    - beta_1: [0, 10] Inverse temperature for Stage 1 choice.
    - beta_2: [0, 10] Inverse temperature for Stage 2 choice.
    - alpha_r: [0, 1] Learning rate for the average reward trace.
    - stick_base: [-5, 5] Baseline stickiness.
    - stick_slope: [-10, 10] Change in stickiness per unit of average reward.
    """
    learning_rate, beta_1, beta_2, alpha_r, stick_base, stick_slope = model_parameters
    n_trials = len(action_1)
    
    q_stage2 = np.zeros((2, 2))
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Initialize average reward rate (assume neutral start)
    avg_reward = 0.5
    
    log_loss = 0.0
    eps = 1e-10
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        # Calculate dynamic stickiness
        current_stickiness = stick_base + stick_slope * avg_reward
        
        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_stage1 = transition_matrix @ max_q_stage2
        
        if last_action_1 != -1:
            q_stage1[last_action_1] += current_stickiness
            
        exp_q1 = np.exp(beta_1 * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        chosen_a1 = int(action_1[trial])
        log_loss -= np.log(probs_1[chosen_a1] + eps)
        
        # Stage 2 Policy
        current_state = int(state[trial])
        exp_q2 = np.exp(beta_2 * q_stage2[current_state])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        chosen_a2 = int(action_2[trial])
        log_loss -= np.log(probs_2[chosen_a2] + eps)
        
        # Updates
        r = reward[trial]
        
        # Update Stage 2 Values
        delta_stage2 = r - q_stage2[current_state, chosen_a2]
        q_stage2[current_state, chosen_a2] += learning_rate * delta_stage2
        
        # Update Average Reward Rate
        avg_reward += alpha_r * (r - avg_reward)
        
        last_action_1 = chosen_a1

    return log_loss
```

### Model 3: Novelty Bonus Model-Based Learner
This model addresses the participant's alternation behavior by adding a "Novelty Bonus" to the Stage 1 Q-values. The bonus is proportional to the number of trials since an option was last chosen. This creates an increasing pressure to switch as an option remains unchosen, distinct from simple one-step alternation.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Novelty Bonus Model-Based Learner.
    
    Adds a bonus to the Stage 1 Q-values proportional to the time (number of trials)
    since each action was last chosen. This mechanism enforces exploration 
    and can generate sustained alternation behavior.
    
    Parameters:
    - learning_rate: [0, 1] Update rate for Stage 2 alien values.
    - beta_1: [0, 10] Inverse temperature for Stage 1 choice.
    - beta_2: [0, 10] Inverse temperature for Stage 2 choice.
    - stickiness: [-5, 5] Standard choice stickiness (perseveration).
    - w_novelty: [0, 5] Weight of the novelty bonus (time since last chosen).
    """
    learning_rate, beta_1, beta_2, stickiness, w_novelty = model_parameters
    n_trials = len(action_1)
    
    q_stage2 = np.zeros((2, 2))
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Track time since last choice for [Action 0, Action 1]
    # Initialize high to encourage initial exploration or 0
    time_since_chosen = np.zeros(2)
    
    log_loss = 0.0
    eps = 1e-10
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_stage1 = transition_matrix @ max_q_stage2
        
        # Add Stickiness
        if last_action_1 != -1:
            q_stage1[last_action_1] += stickiness
            
        # Add Novelty Bonus
        q_stage1 += w_novelty * time_since_chosen
            
        exp_q1 = np.exp(beta_1 * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        chosen_a1 = int(action_1[trial])
        log_loss -= np.log(probs_1[chosen_a1] + eps)
        
        # Stage 2 Policy
        current_state = int(state[trial])
        exp_q2 = np.exp(beta_2 * q_stage2[current_state])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        chosen_a2 = int(action_2[trial])
        log_loss -= np.log(probs_2[chosen_a2] + eps)
        
        # Updates
        r = reward[trial]
        delta_stage2 = r - q_stage2[current_state, chosen_a2]
        q_stage2[current_state, chosen_a2] += learning_rate * delta_stage2
        
        # Update Time Since Chosen
        # Reset chosen to 0, increment unchosen
        time_since_chosen[chosen_a1] = 0
        time_since_chosen[1 - chosen_a1] += 1
        
        last_action_1 = chosen_a1

    return log_loss
```