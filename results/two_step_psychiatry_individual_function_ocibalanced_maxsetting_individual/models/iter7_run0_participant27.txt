Here are three new cognitive models for the two-step task.

### Model 1: Same-Planet Generalization Model
This model posits that the participant generalizes outcomes from one alien to the other alien on the same planet. If an alien returns gold, the participant infers that the planet itself is "rich," slightly increasing the value of the unchosen alien as well. Conversely, if an alien returns nothing, the value of the unchosen alien decreases. This creates a spatial correlation in the value updates.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Same-Planet Generalization Model.
    
    This model assumes that learning about one alien on a planet generalizes to the other alien 
    on the same planet. If an alien yields a reward, the participant increases the value of 
    the unchosen alien as well, scaled by a generalization parameter. This reflects a belief 
    that the "Planet" is rich or poor, rather than just the specific alien.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for chosen options.
    - beta_1: [0, 10] Inverse temperature for Stage 1 (Spaceships).
    - beta_2: [0, 10] Inverse temperature for Stage 2 (Aliens).
    - w: [0, 1] Weighting parameter (0 = Model-Free, 1 = Model-Based).
    - stickiness: [0, 5] Perseveration bonus for repeating the Stage 1 choice.
    - generalization: [0, 1] Fraction of the update applied to the unchosen alien on the same planet.
    """
    learning_rate, beta_1, beta_2, w, stickiness, generalization = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1 or reward[trial] == -1:
            last_action_1 = -1
            continue

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        # Stage 2 Policy
        state_idx = state[trial]
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # Learning Stage 2 (with Generalization)
        # Update chosen
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Update unchosen on same planet
        unchosen_a2 = 1 - action_2[trial]
        delta_gen = reward[trial] - q_stage2_mf[state_idx, unchosen_a2]
        q_stage2_mf[state_idx, unchosen_a2] += learning_rate * generalization * delta_gen

        # Learning Stage 1
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        last_action_1 = action_1[trial]

    eps = 1e-10
    mask = (p_choice_1 > 0) & (p_choice_2 > 0)
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss
```

### Model 2: Reward-Dependent Stickiness with Unchosen Decay
This model combines the effective "Unchosen Decay" mechanism (from your best model) with a nuanced "Win-Stay, Lose-Switch" dynamic at Stage 1. Instead of a single stickiness parameter, it differentiates between stickiness after a reward (`stick_rew`) and stickiness after no reward (`stick_unrew`). This allows the model to capture the participant's tendency to persevere strongly after successes while still allowing for forgetting of unvisited states.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Reward-Dependent Stickiness with Unchosen Decay.
    
    This model combines two mechanisms:
    1. Reward-Dependent Stickiness: The tendency to repeat the previous Stage 1 choice depends on 
       whether the previous trial was rewarded (Win-Stay) or unrewarded.
    2. Unchosen Decay: The Q-values of unchosen Stage 2 options decay towards 0, simulating forgetting.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta_1: [0, 10] Inverse temperature Stage 1.
    - beta_2: [0, 10] Inverse temperature Stage 2.
    - w: [0, 1] MB/MF weight.
    - stick_rew: [0, 5] Stickiness if previous trial was rewarded.
    - stick_unrew: [0, 5] Stickiness if previous trial was unrewarded.
    - decay_unchosen: [0, 1] Decay rate for unchosen Stage 2 options.
    """
    learning_rate, beta_1, beta_2, w, stick_rew, stick_unrew, decay_unchosen = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    last_reward = -1
    
    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1 or reward[trial] == -1:
            last_action_1 = -1
            last_reward = -1
            continue

        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_action_1 != -1:
            if last_reward == 1:
                q_net[last_action_1] += stick_rew
            else:
                q_net[last_action_1] += stick_unrew
            
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        state_idx = state[trial]
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # Decay unchosen Stage 2
        mask = np.ones((2, 2), dtype=bool)
        mask[state_idx, action_2[trial]] = False
        q_stage2_mf[mask] *= (1 - decay_unchosen)

        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        last_action_1 = action_1[trial]
        last_reward = reward[trial]

    eps = 1e-10
    mask = (p_choice_1 > 0) & (p_choice_2 > 0)
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss
```

### Model 3: Exploration Bonus Model
This model introduces an explicit "curiosity" or novelty-seeking mechanism. It tracks the time since each option (spaceship or alien) was last chosen. An exploration bonus, proportional to the log of the time elapsed, is added to the Q-values. This mechanism helps explain why the participant might switch strategies after long blocks of repeating the same choice, even if values haven't changed drastically.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Exploration Bonus Model.
    
    This model adds an exploration bonus to options that have not been chosen recently.
    A counter tracks the number of trials since each option was last selected. 
    The bonus is proportional to the log of this counter, encouraging the participant 
    to revisit neglected spaceships and aliens.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta_1: [0, 10] Inverse temperature Stage 1.
    - beta_2: [0, 10] Inverse temperature Stage 2.
    - w: [0, 1] MB/MF weight.
    - stickiness: [0, 5] Perseveration bonus.
    - exploration_weight: [0, 5] Weight of the exploration bonus (log(days_since_visited)).
    """
    learning_rate, beta_1, beta_2, w, stickiness, exploration_weight = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    tau_1 = np.zeros(2) # Time since last choice for stage 1
    tau_2 = np.zeros((2, 2)) # Time since last choice for stage 2
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1 or reward[trial] == -1:
            last_action_1 = -1
            continue

        # Stage 1
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add exploration bonus
        bonus_1 = exploration_weight * np.log(tau_1 + 1)
        q_net += bonus_1
        
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        # Stage 2
        state_idx = state[trial]
        q_2 = q_stage2_mf[state_idx].copy()
        
        # Add exploration bonus for stage 2
        bonus_2 = exploration_weight * np.log(tau_2[state_idx] + 1)
        q_2 += bonus_2
        
        exp_q2 = np.exp(beta_2 * q_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # Updates
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Update counters
        tau_1 += 1
        tau_1[action_1[trial]] = 0
        
        tau_2[state_idx] += 1
        tau_2[state_idx, action_2[trial]] = 0
        
        last_action_1 = action_1[trial]

    eps = 1e-10
    mask = (p_choice_1 > 0) & (p_choice_2 > 0)
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss
```