Here are the 3 new cognitive models as Python functions.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Habit Model.
    Combines Model-Based and Model-Free learning with a decaying choice trace (habit).
    The habit trace accumulates for chosen actions and decays over time, capturing 
    perseveration (stickiness) that persists beyond a single trial, explaining the 
    participant's strong tendency to stick with specific spaceships.

    Parameters:
    - learning_rate: [0, 1] Update rate for Q-values.
    - beta: [0, 10] Inverse temperature (softness of choice).
    - w: [0, 1] Weighting between Model-Based (1) and Model-Free (0) values in Stage 1.
    - habit_decay: [0, 1] Persistence of the habit trace (0 = instant reset, 1 = no decay).
    - habit_weight: [0, 10] Strength of the habit influence on Stage 1 choices.
    """
    learning_rate, beta, w, habit_decay, habit_weight = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    habit_trace = np.zeros(2)
    
    log_loss = 0.0
    
    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based Value
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Value (Hybrid)
        q_net_s1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add Habit Trace to Logits
        logits_1 = beta * q_net_s1 + habit_weight * habit_trace
        
        # Softmax
        logits_1 = logits_1 - np.max(logits_1) # Stability
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        log_loss -= np.log(probs_1[action_1[trial]] + 1e-10)
        
        # Update Habit Trace
        # Decay existing trace and increment chosen action
        habit_trace *= habit_decay
        habit_trace[action_1[trial]] += 1.0
        
        # --- Stage 2 Choice ---
        state_idx = state[trial]
        q_s2 = q_stage2_mf[state_idx]
        
        logits_2 = beta * q_s2
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        log_loss -= np.log(probs_2[action_2[trial]] + 1e-10)
        
        # --- Value Updates ---
        # Stage 2 Update (Standard Q-learning)
        q2_val_chosen = q_stage2_mf[state_idx, action_2[trial]]
        delta_stage2 = reward[trial] - q2_val_chosen
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Stage 1 Update (TD-0 like update towards updated Stage 2 value)
        # We update Q1 towards the value of the action chosen in Stage 2
        target_stage1 = q_stage2_mf[state_idx, action_2[trial]]
        delta_stage1 = target_stage1 - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Dual-Alpha Hybrid Model with Eligibility Trace.
    Uses separate learning rates for Stage 1 (Spaceship choice) and Stage 2 (Alien choice),
    connected by an eligibility trace. This allows the model to capture differences in 
    plasticity between transition-prediction (S1) and reward-prediction (S2).

    Parameters:
    - alpha1: [0, 1] Learning rate for Stage 1.
    - alpha2: [0, 1] Learning rate for Stage 2.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weight for Model-Based values.
    - lam: [0, 1] Eligibility trace parameter (0 = no propagation, 1 = full propagation of RPE).
    """
    alpha1, alpha2, beta, w, lam = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    log_loss = 0.0
    
    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_s1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net_s1
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[action_1[trial]] + 1e-10)
        
        # --- Stage 2 Choice ---
        state_idx = state[trial]
        q_s2 = q_stage2_mf[state_idx]
        
        logits_2 = beta * q_s2
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[action_2[trial]] + 1e-10)
        
        # --- Value Updates (TD-Lambda) ---
        q1_val = q_stage1_mf[action_1[trial]]
        q2_val = q_stage2_mf[state_idx, action_2[trial]]
        
        # Prediction Errors
        delta1 = q2_val - q1_val
        delta2 = reward[trial] - q2_val
        
        # Update Stage 1: Driven by S1 PE and eligibility trace of S2 PE
        q_stage1_mf[action_1[trial]] += alpha1 * (delta1 + lam * delta2)
        
        # Update Stage 2: Driven by S2 PE
        q_stage2_mf[state_idx, action_2[trial]] += alpha2 * delta2
        
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Dual-Beta Hybrid Model with Eligibility Trace.
    Separates the exploration/exploitation trade-off for Stage 1 and Stage 2.
    This accounts for the participant's behavior of being highly consistent (sticky) 
    in Stage 1 (Spaceships) while being more reactive/exploratory in Stage 2 (Aliens).

    Parameters:
    - learning_rate: [0, 1] Update rate for Q-values.
    - beta1: [0, 10] Inverse temperature for Stage 1.
    - beta2: [0, 10] Inverse temperature for Stage 2.
    - w: [0, 1] Weight for Model-Based values.
    - lam: [0, 1] Eligibility trace parameter.
    """
    learning_rate, beta1, beta2, w, lam = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    log_loss = 0.0
    
    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_s1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta1 * q_net_s1
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[action_1[trial]] + 1e-10)
        
        # --- Stage 2 Choice ---
        state_idx = state[trial]
        q_s2 = q_stage2_mf[state_idx]
        
        logits_2 = beta2 * q_s2
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[action_2[trial]] + 1e-10)
        
        # --- Value Updates (TD-Lambda) ---
        q1_val = q_stage1_mf[action_1[trial]]
        q2_val = q_stage2_mf[state_idx, action_2[trial]]
        
        delta1 = q2_val - q1_val
        delta2 = reward[trial] - q2_val
        
        # Stage 1 update includes eligibility trace from Stage 2 outcome
        q_stage1_mf[action_1[trial]] += learning_rate * (delta1 + lam * delta2)
        
        # Stage 2 update
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta2
        
    return log_loss
```