Here are three new cognitive models for the two-step task, designed to capture the participant's specific behavioral patterns such as block-switching, potential differences in stage dynamics, and subjective structural beliefs.

### Model 1: Stage-Separated Learning with Unchosen Decay
This model hypothesizes that the participant learns at different rates for the high-level planning stage (Spaceships) versus the low-level execution stage (Aliens). Additionally, it incorporates the "unchosen decay" mechanism (which was successful in previous iterations) to handle the non-stationary reward probabilities, applying it to both stages to encourage re-exploration.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Stage-Separated Learning with Unchosen Decay.
    
    This model distinguishes between the learning dynamics of the first stage (choosing a spaceship)
    and the second stage (choosing an alien). It applies a decay to unchosen actions in both stages
    to promote flexibility in a changing environment.
    
    Parameters:
    lr_1: [0, 1] - Learning rate for Stage 1 (Spaceship choice).
    lr_2: [0, 1] - Learning rate for Stage 2 (Alien choice).
    beta: [0, 10] - Inverse temperature for softmax choice (exploration/exploitation).
    w: [0, 1] - Weight for Model-Based values (0 = pure MF, 1 = pure MB).
    decay: [0, 1] - Decay rate for unchosen action values (0 = no decay, 1 = full reset).
    """
    lr_1, lr_2, beta, w, decay = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # state x action

    for trial in range(n_trials):
        # -- Stage 1 Policy --
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # -- Stage 2 Policy --
        state_idx = state[trial]
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # -- Updates --
        # Stage 1 Update
        chosen_a1 = action_1[trial]
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[chosen_a1]
        q_stage1_mf[chosen_a1] += lr_1 * delta_stage1
        
        # Decay unchosen Stage 1 action
        unchosen_a1 = 1 - chosen_a1
        q_stage1_mf[unchosen_a1] *= (1 - decay)

        # Stage 2 Update
        chosen_a2 = action_2[trial]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, chosen_a2]
        q_stage2_mf[state_idx, chosen_a2] += lr_2 * delta_stage2
        
        # Decay unchosen Stage 2 action (for the current state)
        unchosen_a2 = 1 - chosen_a2
        q_stage2_mf[state_idx, unchosen_a2] *= (1 - decay)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model with Cumulative Perseveration
The participant exhibits "blocky" behavior, staying with one spaceship for many trials before switching. This model implements a cumulative stickiness mechanism where the perseveration bonus increases with the number of consecutive choices of the same option. Unlike simple stickiness (which is just based on the last trial), this creates a "momentum" effect that makes it harder to switch the longer an option has been chosen, but resets immediately upon switching.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Cumulative Perseveration.
    
    Incorporates a 'momentum' stickiness where the bias towards an action scales with
    the number of consecutive times it has been chosen. This captures the strong
    persistence observed in the participant's data.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate for Q-value updates.
    beta: [0, 10] - Inverse temperature for softmax choice.
    w: [0, 1] - Weight for Model-Based values.
    stick_growth: [0, 5] - The amount of stickiness added per consecutive choice.
    """
    learning_rate, beta, w, stick_growth = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track consecutive choices for Stage 1 actions [Action 0, Action 1]
    consecutive_counts = np.zeros(2) 

    for trial in range(n_trials):
        # -- Stage 1 Policy --
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add cumulative stickiness bonus
        stickiness_bonus = stick_growth * consecutive_counts
        
        exp_q1 = np.exp(beta * q_net + stickiness_bonus)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Update consecutive counts
        chosen_a1 = action_1[trial]
        consecutive_counts[chosen_a1] += 1
        consecutive_counts[1 - chosen_a1] = 0
        
        # -- Stage 2 Policy --
        state_idx = state[trial]
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # -- Updates --
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[chosen_a1]
        q_stage1_mf[chosen_a1] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model with Subjective Transition Belief
Standard models assume the participant knows the true transition probabilities (0.7/0.3). This model allows the participant's belief about the transition structure to be a free parameter. They might perceive the common transition as deterministic (1.0), random (0.5), or any other value. This subjective belief distorts the Model-Based value calculation.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Subjective Transition Belief.
    
    Allows the participant's internal model of the spaceship-planet transition probabilities
    to differ from the true probabilities (0.7/0.3). The participant acts according to a
    subjective transition matrix defined by `trans_prob`.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate for Q-value updates.
    beta: [0, 10] - Inverse temperature for softmax choice.
    w: [0, 1] - Weight for Model-Based values.
    trans_prob: [0, 1] - Subjective probability of the common transition (A->X, U->Y).
                         (e.g., 1.0 implies belief in deterministic transitions).
    """
    learning_rate, beta, w, trans_prob = model_parameters
    n_trials = len(action_1)
  
    # Subjective transition matrix based on parameter
    # Row 0 (A): [Prob(X), Prob(Y)] = [p, 1-p]
    # Row 1 (U): [Prob(X), Prob(Y)] = [1-p, p]
    subjective_transition_matrix = np.array([[trans_prob, 1 - trans_prob], 
                                             [1 - trans_prob, trans_prob]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # -- Stage 1 Policy --
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        
        # Use subjective matrix for MB calculation
        q_stage1_mb = subjective_transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # -- Stage 2 Policy --
        state_idx = state[trial]
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # -- Updates --
        chosen_a1 = action_1[trial]
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[chosen_a1]
        q_stage1_mf[chosen_a1] += learning_rate * delta_stage1
        
        chosen_a2 = action_2[trial]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, chosen_a2]
        q_stage2_mf[state_idx, chosen_a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```