Here are three new cognitive models for the two-step task.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Q-Learning with Decay and Stage-Specific Perseveration.
    Combines Model-Based (planning) and Model-Free (habit) learning.
    Critically, it includes separate perseveration parameters for Stage 1 (Spaceship)
    and Stage 2 (Alien), allowing for different levels of "stickiness" at the abstract
    vs concrete choice levels.
    
    Parameters:
    learning_rate: Learning rate for value updates [0,1]
    beta: Inverse temperature (decision noise) [0,10]
    w: Weighting parameter for Model-Based values (0=Pure MF, 1=Pure MB) [0,1]
    lambda_eligibility: Eligibility trace decay for multi-step learning [0,1]
    decay: Passive decay rate of Q-values toward 0.5 [0,1]
    pers_s1: Perseveration bonus for Stage 1 choices [0,5]
    pers_s2: Perseveration bonus for Stage 2 choices [0,5]
    """
    learning_rate, beta, w, lambda_eligibility, decay, pers_s1, pers_s2 = model_parameters
    n_trials = len(action_1)
    
    # Initialize Q-values to neutral (0.5)
    q_s1_mf = np.full(2, 0.5)
    q_s2 = np.full((2, 2), 0.5) # Used for both MB estimation and MF values
    
    # Transition matrix: T[action, state]
    # Action 0 (A) -> 70% Planet 0, 30% Planet 1
    # Action 1 (U) -> 30% Planet 0, 70% Planet 1
    trans_probs = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1
    last_action_2 = np.full(2, -1) # Track last action per planet
    
    for trial in range(n_trials):
        # Handle missing data
        if action_1[trial] == -1:
            last_action_1 = -1
            last_action_2 = np.full(2, -1)
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue
            
        # Decay values toward 0.5
        q_s1_mf = q_s1_mf * (1 - decay) + 0.5 * decay
        q_s2 = q_s2 * (1 - decay) + 0.5 * decay
        
        # --- Stage 1 Decision ---
        # Model-Based Value: Expected max value of next stage
        max_q_s2 = np.max(q_s2, axis=1)
        q_s1_mb = trans_probs @ max_q_s2
        
        # Hybrid Value
        q_net_s1 = w * q_s1_mb + (1 - w) * q_s1_mf
        
        # Apply Stage 1 Perseveration
        if last_action_1 != -1:
            q_net_s1[last_action_1] += pers_s1
            
        exp_q1 = np.exp(beta * q_net_s1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Decision ---
        s_idx = state[trial]
        q_net_s2 = q_s2[s_idx].copy()
        
        # Apply Stage 2 Perseveration
        if last_action_2[s_idx] != -1:
            q_net_s2[last_action_2[s_idx]] += pers_s2
            
        exp_q2 = np.exp(beta * q_net_s2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning Updates ---
        # Stage 1 MF Update (SARSA-style)
        delta_1 = q_s2[s_idx, action_2[trial]] - q_s1_mf[action_1[trial]]
        q_s1_mf[action_1[trial]] += learning_rate * delta_1
        
        # Stage 2 Update
        delta_2 = reward[trial] - q_s2[s_idx, action_2[trial]]
        q_s2[s_idx, action_2[trial]] += learning_rate * delta_2
        
        # Eligibility Trace: Propagate Stage 2 RPE to Stage 1 Choice
        q_s1_mf[action_1[trial]] += learning_rate * lambda_eligibility * delta_2
        
        # Update history
        last_action_1 = action_1[trial]
        last_action_2[s_idx] = action_2[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Q-Learning with Asymmetric Learning Rates and Stage-Specific Perseveration.
    Differentiates learning from positive vs negative prediction errors (valence bias),
    while also maintaining separate perseveration parameters for spaceships vs aliens.
    
    Parameters:
    lr_pos: Learning rate for positive prediction errors [0,1]
    lr_neg: Learning rate for negative prediction errors [0,1]
    beta: Inverse temperature [0,10]
    lambda_eligibility: Eligibility trace decay [0,1]
    decay: Decay rate for Q-values [0,1]
    pers_s1: Perseveration for Stage 1 [0,5]
    pers_s2: Perseveration for Stage 2 [0,5]
    """
    lr_pos, lr_neg, beta, lambda_eligibility, decay, pers_s1, pers_s2 = model_parameters
    n_trials = len(action_1)
    
    q_s1 = np.full(2, 0.5)
    q_s2 = np.full((2, 2), 0.5)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1
    last_action_2 = np.full(2, -1)
    
    for trial in range(n_trials):
        if action_1[trial] == -1:
            last_action_1 = -1
            last_action_2 = np.full(2, -1)
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue
            
        q_s1 = q_s1 * (1 - decay) + 0.5 * decay
        q_s2 = q_s2 * (1 - decay) + 0.5 * decay
        
        # Stage 1 Policy
        q_net_s1 = q_s1.copy()
        if last_action_1 != -1:
            q_net_s1[last_action_1] += pers_s1
        
        exp_q1 = np.exp(beta * q_net_s1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Stage 2 Policy
        s_idx = state[trial]
        q_net_s2 = q_s2[s_idx].copy()
        if last_action_2[s_idx] != -1:
            q_net_s2[last_action_2[s_idx]] += pers_s2
            
        exp_q2 = np.exp(beta * q_net_s2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # Update Stage 1
        delta_1 = q_s2[s_idx, action_2[trial]] - q_s1[action_1[trial]]
        lr_1 = lr_pos if delta_1 > 0 else lr_neg
        q_s1[action_1[trial]] += lr_1 * delta_1
        
        # Update Stage 2
        delta_2 = reward[trial] - q_s2[s_idx, action_2[trial]]
        lr_2 = lr_pos if delta_2 > 0 else lr_neg
        q_s2[s_idx, action_2[trial]] += lr_2 * delta_2
        
        # Eligibility Trace (using the learning rate associated with the second stage error)
        q_s1[action_1[trial]] += lr_2 * lambda_eligibility * delta_2
        
        last_action_1 = action_1[trial]
        last_action_2[s_idx] = action_2[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Q-Learning with Separate Betas and Decay.
    A hybrid model that allows for different levels of exploration/decision noise 
    at Stage 1 (Spaceship choice) versus Stage 2 (Alien choice).
    
    Parameters:
    learning_rate: Learning rate [0,1]
    beta_s1: Inverse temperature for Stage 1 [0,10]
    beta_s2: Inverse temperature for Stage 2 [0,10]
    w: Weight for Model-Based values [0,1]
    lambda_eligibility: Eligibility trace decay [0,1]
    decay: Decay rate for Q-values [0,1]
    perseveration: General perseveration bonus [0,5]
    """
    learning_rate, beta_s1, beta_s2, w, lambda_eligibility, decay, perseveration = model_parameters
    n_trials = len(action_1)
    
    q_s1_mf = np.full(2, 0.5)
    q_s2 = np.full((2, 2), 0.5)
    
    trans_probs = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1
    last_action_2 = np.full(2, -1)
    
    for trial in range(n_trials):
        if action_1[trial] == -1:
            last_action_1 = -1
            last_action_2 = np.full(2, -1)
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue
            
        q_s1_mf = q_s1_mf * (1 - decay) + 0.5 * decay
        q_s2 = q_s2 * (1 - decay) + 0.5 * decay
        
        # Stage 1 (Hybrid)
        max_q_s2 = np.max(q_s2, axis=1)
        q_s1_mb = trans_probs @ max_q_s2
        
        q_net_s1 = w * q_s1_mb + (1 - w) * q_s1_mf
        
        if last_action_1 != -1:
            q_net_s1[last_action_1] += perseveration
            
        exp_q1 = np.exp(beta_s1 * q_net_s1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Stage 2
        s_idx = state[trial]
        q_net_s2 = q_s2[s_idx].copy()
        
        if last_action_2[s_idx] != -1:
            q_net_s2[last_action_2[s_idx]] += perseveration
            
        exp_q2 = np.exp(beta_s2 * q_net_s2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # Updates
        delta_1 = q_s2[s_idx, action_2[trial]] - q_s1_mf[action_1[trial]]
        q_s1_mf[action_1[trial]] += learning_rate * delta_1
        
        delta_2 = reward[trial] - q_s2[s_idx, action_2[trial]]
        q_s2[s_idx, action_2[trial]] += learning_rate * delta_2
        
        q_s1_mf[action_1[trial]] += learning_rate * lambda_eligibility * delta_2
        
        last_action_1 = action_1[trial]
        last_action_2[s_idx] = action_2[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```