Here are the three proposed cognitive models.

### Model 1: Hybrid Model with Unchosen Decay, Stickiness, and Static Bias
This model extends the previous best-performing model (which used unchosen decay and stickiness) by adding a **static bias** parameter. The participant data shows a strong, persistent preference for Spaceship 0 (Action 0) that goes beyond simple repetition (stickiness). This model hypothesizes that the participant has a baseline preference for Option 0 (intercept), in addition to trial-by-trial perseveration and value-based learning with forgetting.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Unchosen Decay, Stickiness, and Static Bias.
    
    Combines Q-learning with decay for unchosen options, a stickiness bonus for 
    repetition, and a static bias towards Action 0 to account for the overall skew.

    Parameters:
    - learning_rate: [0,1] Update rate for chosen options.
    - decay_rate: [0,1] Decay rate for unchosen options (forgetting).
    - beta: [0,10] Inverse temperature.
    - w: [0,1] Weight of Model-Based system.
    - stickiness: [0,5] Bonus for repeating the last choice.
    - bias: [-5,5] Static bias added to Action 0 logits (preference for Spaceship 0).
    """
    learning_rate, decay_rate, beta, w, stickiness, bias = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        # Handle missing data
        if action_1[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            last_action_1 = -1
            continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2

        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net_stage1
        
        # Add Stickiness
        if last_action_1 != -1:
            logits_1[last_action_1] += stickiness
            
        # Add Static Bias for Action 0
        logits_1[0] += bias

        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]

        # --- Stage 2 Policy ---
        logits_2 = beta * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Value Updates ---
        # Stage 1 MF Update
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Stage 2 MF Update
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # --- Unchosen Decay ---
        # Decay unchosen Stage 1 action
        unchosen_a1 = 1 - a1
        q_stage1_mf[unchosen_a1] *= (1.0 - decay_rate)

        # Decay unchosen Stage 2 action (for the current state)
        unchosen_a2 = 1 - a2
        q_stage2_mf[state_idx, unchosen_a2] *= (1.0 - decay_rate)
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model with Asymmetric Learning, Stickiness, and Bias
This model tests the hypothesis that the participant learns differently from positive vs. negative prediction errors (asymmetric learning rates), while also accounting for the strong static preference for Spaceship 0 and perseveration. It replaces the "decay" mechanism with "asymmetric updating" to see if outcome sensitivity better explains the value evolution.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Asymmetric Learning (Pos/Neg), Stickiness, and Static Bias.
    
    Differentiates learning rates for positive and negative prediction errors.
    Includes static bias for Action 0 and stickiness.

    Parameters:
    - lr_pos: [0,1] Learning rate for positive prediction errors (better than expected).
    - lr_neg: [0,1] Learning rate for negative prediction errors (worse than expected).
    - beta: [0,10] Inverse temperature.
    - w: [0,1] Weight of Model-Based system.
    - stickiness: [0,5] Bonus for repeating the last choice.
    - bias: [-5,5] Static bias added to Action 0 logits.
    """
    lr_pos, lr_neg, beta, w, stickiness, bias = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            last_action_1 = -1
            continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2

        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net_stage1
        
        if last_action_1 != -1:
            logits_1[last_action_1] += stickiness
            
        logits_1[0] += bias

        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]

        # --- Stage 2 Policy ---
        logits_2 = beta * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Value Updates with Asymmetry ---
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        alpha_1 = lr_pos if delta_stage1 > 0 else lr_neg
        q_stage1_mf[a1] += alpha_1 * delta_stage1
        
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        alpha_2 = lr_pos if delta_stage2 > 0 else lr_neg
        q_stage2_mf[state_idx, a2] += alpha_2 * delta_stage2
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model with Stickiness and Dual-Stage Bias
This model investigates whether the participant exhibits specific biases at **both** stages of the task. It includes a bias for Spaceship 0 (Stage 1) and a bias for Alien 0 (Stage 2). This tests if the participant has a generalized preference for "Option 0" across different contexts, which might distort the model-free values and the model-based planning (via stage 2 selection probabilities).

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Stickiness and Dual-Stage Bias.
    
    Includes a static bias for Action 0 at Stage 1 (Spaceship) AND 
    a static bias for Action 0 at Stage 2 (Alien).
    
    Parameters:
    - learning_rate: [0,1] Learning rate.
    - beta: [0,10] Inverse temperature.
    - w: [0,1] Weight of Model-Based system.
    - stickiness: [0,5] Bonus for repeating the last Stage 1 choice.
    - bias_1: [-5,5] Static bias for Action 0 at Stage 1.
    - bias_2: [-5,5] Static bias for Action 0 at Stage 2.
    """
    learning_rate, beta, w, stickiness, bias_1, bias_2 = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            last_action_1 = -1
            continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2

        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net_stage1
        
        if last_action_1 != -1:
            logits_1[last_action_1] += stickiness
            
        logits_1[0] += bias_1  # Stage 1 Bias

        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]

        # --- Stage 2 Policy ---
        logits_2 = beta * q_stage2_mf[state_idx]
        logits_2[0] += bias_2  # Stage 2 Bias
        
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Value Updates ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```