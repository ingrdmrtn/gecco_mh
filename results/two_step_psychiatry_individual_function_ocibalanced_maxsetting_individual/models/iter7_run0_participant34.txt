Here are three new cognitive models for the two-step task, designed based on the participant's behavior and distinct from previously tried configurations.

### Model 1: Dual Stickiness Hybrid Model
**Rationale:** The participant data shows strong perseveration not just in the choice of spaceship (Stage 1) but also in the choice of alien (Stage 2). For instance, they often repeat the same alien choice for many trials (e.g., trials 4-11). This model introduces a separate stickiness parameter for the second stage (`stick_2`) to capture this inertia independently of the first stage stickiness (`stick_1`).

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Dual Stickiness Hybrid Model.
    
    Incorporates separate stickiness parameters for Stage 1 (Spaceship) and 
    Stage 2 (Alien) choices. This accounts for perseveration at both 
    levels of the decision hierarchy.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for Q-value updates.
    - beta_1: [0, 10] Inverse temperature for Stage 1 choices.
    - beta_2: [0, 10] Inverse temperature for Stage 2 choices.
    - w: [0, 1] Weight for Model-Based control (0=MF, 1=MB).
    - stick_1: [0, 5] Stickiness bias for Stage 1 (Spaceship).
    - stick_2: [0, 5] Stickiness bias for Stage 2 (Alien).
    """
    learning_rate, beta_1, beta_2, w, stick_1, stick_2 = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_mf1 = np.zeros(2)
    q_s2 = np.zeros((2, 2)) # State x Action
    
    last_action_1 = -1
    last_action_2 = -1 # Tracks last chosen alien (across any state or specific? Usually specific, but here abstract action index)
    # Note: In this task, action_2 is 0 or 1 (Left/Right Alien). 
    # Stickiness usually applies to the motor action (Left/Right).
    
    for t in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_s2 = np.max(q_s2, axis=1)
        q_mb1 = transition_matrix @ max_q_s2
        
        q_net = w * q_mb1 + (1 - w) * q_mf1
        
        # Apply Stage 1 Stickiness
        logits_1 = beta_1 * q_net
        if last_action_1 != -1:
            logits_1[last_action_1] += stick_1
            
        # Numerical stability
        logits_1 = logits_1 - np.max(logits_1)
        probs_1 = np.exp(logits_1) / np.sum(np.exp(logits_1))
        p_choice_1[t] = probs_1[action_1[t]]
        
        a1 = action_1[t]
        s_curr = state[t]
        last_action_1 = a1
        
        # --- Stage 2 Policy ---
        q_s2_curr = q_s2[s_curr]
        
        # Apply Stage 2 Stickiness
        logits_2 = beta_2 * q_s2_curr
        if last_action_2 != -1:
            logits_2[last_action_2] += stick_2
            
        logits_2 = logits_2 - np.max(logits_2)
        probs_2 = np.exp(logits_2) / np.sum(np.exp(logits_2))
        p_choice_2[t] = probs_2[action_2[t]]
        
        a2 = action_2[t]
        r = reward[t]
        last_action_2 = a2
        
        # --- Updates ---
        # SARSA-style update for Stage 1 MF
        pe1 = q_s2[s_curr, a2] - q_mf1[a1]
        q_mf1[a1] += learning_rate * pe1
        
        # Q-learning update for Stage 2
        pe2 = r - q_s2[s_curr, a2]
        q_s2[s_curr, a2] += learning_rate * pe2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model with Value Forgetting
**Rationale:** The reward probabilities of the aliens drift over time. A standard Q-learning model only updates the chosen option. This model includes a `forget_rate` which decays the Q-values of unchosen aliens towards zero. This allows the model to "forget" old information about unvisited options, facilitating re-exploration when the environment changes, which aligns with the participant's eventual switching behavior.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Value Forgetting.
    
    Includes a passive decay (forgetting) for unchosen actions in Stage 2.
    This helps the model handle non-stationary reward probabilities by 
    gradually reducing confidence in options that haven't been tried recently.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for chosen options.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - w: [0, 1] Weight for Model-Based control.
    - stickiness: [0, 5] General stickiness for Stage 1.
    - forget_rate: [0, 1] Decay rate for unchosen Stage 2 values.
    """
    learning_rate, beta_1, beta_2, w, stickiness, forget_rate = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_mf1 = np.zeros(2)
    q_s2 = np.zeros((2, 2))
    
    last_action_1 = -1
    
    for t in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_s2 = np.max(q_s2, axis=1)
        q_mb1 = transition_matrix @ max_q_s2
        
        q_net = w * q_mb1 + (1 - w) * q_mf1
        
        logits_1 = beta_1 * q_net
        if last_action_1 != -1:
            logits_1[last_action_1] += stickiness
            
        logits_1 = logits_1 - np.max(logits_1)
        probs_1 = np.exp(logits_1) / np.sum(np.exp(logits_1))
        p_choice_1[t] = probs_1[action_1[t]]
        
        a1 = action_1[t]
        s_curr = state[t]
        last_action_1 = a1
        
        # --- Stage 2 Policy ---
        q_s2_curr = q_s2[s_curr]
        logits_2 = beta_2 * q_s2_curr
        logits_2 = logits_2 - np.max(logits_2)
        probs_2 = np.exp(logits_2) / np.sum(np.exp(logits_2))
        p_choice_2[t] = probs_2[action_2[t]]
        
        a2 = action_2[t]
        r = reward[t]
        
        # --- Updates ---
        pe1 = q_s2[s_curr, a2] - q_mf1[a1]
        q_mf1[a1] += learning_rate * pe1
        
        # Update chosen Stage 2 option
        pe2 = r - q_s2[s_curr, a2]
        q_s2[s_curr, a2] += learning_rate * pe2
        
        # Decay unchosen Stage 2 option
        unchosen_a2 = 1 - a2
        q_s2[s_curr, unchosen_a2] *= (1.0 - forget_rate)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid with Asymmetric Stage 2 Learning
**Rationale:** The participant appears to stick with a winning alien strategy but switches after losses (e.g., trial 12 loss -> trial 13 switch). This model implements asymmetric learning rates (`lr_pos`, `lr_neg`) specifically for the second stage (aliens). This allows the model to react differently to wins versus losses when estimating alien values, while keeping a separate learning rate (`lr_s1`) for the Stage 1 habit.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Asymmetric Stage 2 Learning.
    
    Uses separate learning rates for positive and negative prediction errors
    specifically in Stage 2 (Alien estimation). This allows the model to 
    capture 'Win-Stay, Lose-Shift' dynamics in the second stage via value 
    learning asymmetries, while maintaining a separate learning rate for 
    Stage 1 habits.
    
    Parameters:
    - lr_s1: [0, 1] Learning rate for Stage 1 Model-Free values.
    - lr_s2_pos: [0, 1] Learning rate for Stage 2 Positive PE (Reward).
    - lr_s2_neg: [0, 1] Learning rate for Stage 2 Negative PE (No Reward).
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - w: [0, 1] Weight for Model-Based control.
    - stickiness: [0, 5] Stickiness bias for Stage 1.
    """
    lr_s1, lr_s2_pos, lr_s2_neg, beta_1, beta_2, w, stickiness = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_mf1 = np.zeros(2)
    q_s2 = np.zeros((2, 2))
    
    last_action_1 = -1
    
    for t in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_s2 = np.max(q_s2, axis=1)
        q_mb1 = transition_matrix @ max_q_s2
        
        q_net = w * q_mb1 + (1 - w) * q_mf1
        
        logits_1 = beta_1 * q_net
        if last_action_1 != -1:
            logits_1[last_action_1] += stickiness
            
        logits_1 = logits_1 - np.max(logits_1)
        probs_1 = np.exp(logits_1) / np.sum(np.exp(logits_1))
        p_choice_1[t] = probs_1[action_1[t]]
        
        a1 = action_1[t]
        s_curr = state[t]
        last_action_1 = a1
        
        # --- Stage 2 Policy ---
        q_s2_curr = q_s2[s_curr]
        logits_2 = beta_2 * q_s2_curr
        logits_2 = logits_2 - np.max(logits_2)
        probs_2 = np.exp(logits_2) / np.sum(np.exp(logits_2))
        p_choice_2[t] = probs_2[action_2[t]]
        
        a2 = action_2[t]
        r = reward[t]
        
        # --- Updates ---
        # Stage 1 Update (Standard)
        pe1 = q_s2[s_curr, a2] - q_mf1[a1]
        q_mf1[a1] += lr_s1 * pe1
        
        # Stage 2 Update (Asymmetric)
        pe2 = r - q_s2[s_curr, a2]
        if pe2 >= 0:
            q_s2[s_curr, a2] += lr_s2_pos * pe2
        else:
            q_s2[s_curr, a2] += lr_s2_neg * pe2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```