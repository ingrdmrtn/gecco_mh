Here are the three proposed cognitive models.

### Model 1: Asymmetric Eligibility Trace Model
This model tests the hypothesis that the participant assigns credit to the first-stage choice differently depending on whether the final outcome was a "win" (positive prediction error) or a "loss" (negative prediction error). This "fair-weather" or "blame-avoidant" mechanism helps explain why the participant might stick to a spaceship despite occasional losses or switch rapidly.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Asymmetric Eligibility Trace Model.
    
    Allows for different strengths of credit assignment (eligibility traces) 
    depending on whether the Stage 2 outcome was better (positive RPE) or 
    worse (negative RPE) than expected.
    
    Parameters:
    lr: Learning rate [0,1]
    beta_1: Inverse temperature Stage 1 [0,10]
    beta_2: Inverse temperature Stage 2 [0,10]
    w: MB weight [0,1]
    p: Perseveration [0,1]
    decay: Decay rate to 0.5 [0,1]
    lam_pos: Eligibility trace for positive Stage 2 RPE [0,1]
    lam_neg: Eligibility trace for negative Stage 2 RPE [0,1]
    """
    lr, beta_1, beta_2, w, p, decay, lam_pos, lam_neg = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    q_stage1_mf[:] = 0.5
    q_stage2_mf[:] = 0.5
    
    prev_action_1 = -1
    
    for trial in range(n_trials):
        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if prev_action_1 != -1:
            q_net_stage1[prev_action_1] += p
            
        exp_q1 = np.exp(beta_1 * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        s1_choice = action_1[trial]
        state_idx = state[trial]
        prev_action_1 = s1_choice
        
        s2_choice = action_2[trial]
        r = reward[trial]

        # Stage 2 Policy and Updates
        if s2_choice != -1:
            qs_stage2 = q_stage2_mf[state_idx]
            exp_q2 = np.exp(beta_2 * qs_stage2)
            probs_2 = exp_q2 / np.sum(exp_q2)
            p_choice_2[trial] = probs_2[s2_choice]

            # Stage 1 Update (TD(0))
            delta_stage1 = q_stage2_mf[state_idx, s2_choice] - q_stage1_mf[s1_choice]
            q_stage1_mf[s1_choice] += lr * delta_stage1
            
            # Stage 2 Update
            delta_stage2 = r - q_stage2_mf[state_idx, s2_choice]
            q_stage2_mf[state_idx, s2_choice] += lr * delta_stage2

            # Stage 1 Eligibility Trace Update (Asymmetric)
            lam = lam_pos if delta_stage2 > 0 else lam_neg
            q_stage1_mf[s1_choice] += lr * lam * delta_stage2
        else:
            p_choice_2[trial] = 1.0

        # Decay
        q_stage1_mf = q_stage1_mf * (1 - decay) + 0.5 * decay
        q_stage2_mf = q_stage2_mf * (1 - decay) + 0.5 * decay

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Counterfactual Stage 2 Learning Model
This model assumes the participant learns not only from the chosen alien's outcome but also infers the value of the unchosen alien. Given the binary nature of the rewards (often anti-correlated in such tasks), receiving 0 coins from one alien might imply the other alien has coins. The parameter `cf_w` controls the strength of this inference.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Counterfactual Stage 2 Learning Model.
    
    Incorporates counterfactual updating in Stage 2: the unchosen alien's value 
    is updated based on the assumption that the unchosen reward is 1-received_reward.
    
    Parameters:
    lr: Learning rate [0,1]
    beta_1: Inverse temperature Stage 1 [0,10]
    beta_2: Inverse temperature Stage 2 [0,10]
    w: MB weight [0,1]
    p: Perseveration [0,1]
    lam: Eligibility trace [0,1]
    decay: Decay rate to 0.5 [0,1]
    cf_w: Counterfactual weight (scaling of learning for unchosen option) [0,1]
    """
    lr, beta_1, beta_2, w, p, lam, decay, cf_w = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    q_stage1_mf[:] = 0.5
    q_stage2_mf[:] = 0.5
    
    prev_action_1 = -1
    
    for trial in range(n_trials):
        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if prev_action_1 != -1:
            q_net_stage1[prev_action_1] += p
            
        exp_q1 = np.exp(beta_1 * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        s1_choice = action_1[trial]
        state_idx = state[trial]
        prev_action_1 = s1_choice
        
        s2_choice = action_2[trial]
        r = reward[trial]

        # Stage 2 Policy and Updates
        if s2_choice != -1:
            qs_stage2 = q_stage2_mf[state_idx]
            exp_q2 = np.exp(beta_2 * qs_stage2)
            probs_2 = exp_q2 / np.sum(exp_q2)
            p_choice_2[trial] = probs_2[s2_choice]

            # Stage 1 Update
            delta_stage1 = q_stage2_mf[state_idx, s2_choice] - q_stage1_mf[s1_choice]
            q_stage1_mf[s1_choice] += lr * delta_stage1
            
            # Stage 2 Update (Chosen)
            delta_stage2 = r - q_stage2_mf[state_idx, s2_choice]
            q_stage2_mf[state_idx, s2_choice] += lr * delta_stage2
            
            # Stage 2 Update (Unchosen/Counterfactual)
            unchosen_s2 = 1 - s2_choice
            # Assume complementary reward (if I got 0, other was 1)
            r_cf = 1 - r
            delta_cf = r_cf - q_stage2_mf[state_idx, unchosen_s2]
            q_stage2_mf[state_idx, unchosen_s2] += lr * cf_w * delta_cf

            # Stage 1 Eligibility Trace
            q_stage1_mf[s1_choice] += lr * lam * delta_stage2
        else:
            p_choice_2[trial] = 1.0

        # Decay
        q_stage1_mf = q_stage1_mf * (1 - decay) + 0.5 * decay
        q_stage2_mf = q_stage2_mf * (1 - decay) + 0.5 * decay

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Action-Specific Stage 1 Learning Rate Model
The participant shows a very strong preference for Spaceship 0 (Action 0), maintaining it for long blocks. This model proposes that the participant learns about the two spaceships with different rates (`lr_a0` vs `lr_a1`), allowing them to treat one option as having a stable value while treating the other as more volatile or uncertain.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Action-Specific Stage 1 Learning Rate Model.
    
    Uses different learning rates for the two spaceships (Stage 1 actions),
    allowing the model to capture different volatility or stability beliefs 
    associated with each spaceship.
    
    Parameters:
    lr_a0: Learning rate for Spaceship 0 [0,1]
    lr_a1: Learning rate for Spaceship 1 [0,1]
    lr_2: Learning rate for Stage 2 [0,1]
    beta: Inverse temperature (shared) [0,10]
    w: MB weight [0,1]
    p: Perseveration [0,1]
    lam: Eligibility trace [0,1]
    decay: Decay rate to 0.5 [0,1]
    """
    lr_a0, lr_a1, lr_2, beta, w, p, lam, decay = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    q_stage1_mf[:] = 0.5
    q_stage2_mf[:] = 0.5
    
    prev_action_1 = -1
    
    for trial in range(n_trials):
        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if prev_action_1 != -1:
            q_net_stage1[prev_action_1] += p
            
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        s1_choice = action_1[trial]
        state_idx = state[trial]
        prev_action_1 = s1_choice
        
        s2_choice = action_2[trial]
        r = reward[trial]
        
        # Determine LR for Stage 1 update
        current_lr_s1 = lr_a0 if s1_choice == 0 else lr_a1

        # Stage 2 Policy and Updates
        if s2_choice != -1:
            qs_stage2 = q_stage2_mf[state_idx]
            exp_q2 = np.exp(beta * qs_stage2)
            probs_2 = exp_q2 / np.sum(exp_q2)
            p_choice_2[trial] = probs_2[s2_choice]

            # Stage 1 Update
            delta_stage1 = q_stage2_mf[state_idx, s2_choice] - q_stage1_mf[s1_choice]
            q_stage1_mf[s1_choice] += current_lr_s1 * delta_stage1
            
            # Stage 2 Update
            delta_stage2 = r - q_stage2_mf[state_idx, s2_choice]
            q_stage2_mf[state_idx, s2_choice] += lr_2 * delta_stage2

            # Stage 1 Eligibility Trace
            q_stage1_mf[s1_choice] += current_lr_s1 * lam * delta_stage2
        else:
            p_choice_2[trial] = 1.0

        # Decay
        q_stage1_mf = q_stage1_mf * (1 - decay) + 0.5 * decay
        q_stage2_mf = q_stage2_mf * (1 - decay) + 0.5 * decay

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```