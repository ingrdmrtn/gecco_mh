Here are three new cognitive models designed to capture the specific behaviors observed in the participant data, particularly the strong persistence/inertia and the specific learning dynamics.

### Model 1: Hybrid with Habitual Persistence
This model replaces simple "stickiness" with a **habit trace** (leaky integrator). The participant shows long streaks of choosing the same spaceship even when rewards are sparse. A habit trace that accumulates slowly and decays slowly can model this inertia better than a simple one-trial repetition bonus.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid model with Habitual Persistence (Leaky Integrator).
    Uses a habit trace that accumulates with choices and decays slowly, 
    modeling the participant's strong inertia/perseveration more dynamically 
    than simple one-step stickiness.

    Parameters:
    learning_rate: [0,1] - Q-learning rate.
    beta: [0,10] - Inverse temperature.
    w: [0,1] - Weight for Model-Based system (1=Pure MB, 0=Pure MF).
    lam: [0,1] - Eligibility trace parameter.
    habit_lr: [0,1] - Learning rate for the habit trace (how fast habits form/decay).
    habit_weight: [0,5] - Strength of the habit bias on choice.
    """
    learning_rate, beta, w, lam, habit_lr, habit_weight = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    habit_values = np.zeros(2)
    
    for trial in range(n_trials):
        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Q-value includes MB, MF, and Habit
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf + habit_weight * habit_values
        
        exp_q1 = np.exp(beta * q_net_1)
        sum_exp_q1 = np.sum(exp_q1)
        if sum_exp_q1 < 1e-9: probs_1 = np.ones(2)/2
        else: probs_1 = exp_q1 / sum_exp_q1
        
        if action_1[trial] != -1:
            p_choice_1[trial] = probs_1[action_1[trial]]
        else:
            p_choice_1[trial] = 1.0

        # Stage 2 Policy & Updates
        if action_1[trial] != -1 and state[trial] != -1 and action_2[trial] != -1:
            state_idx = state[trial]
            exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
            sum_exp_q2 = np.sum(exp_q2)
            if sum_exp_q2 < 1e-9: probs_2 = np.ones(2)/2
            else: probs_2 = exp_q2 / sum_exp_q2
            p_choice_2[trial] = probs_2[action_2[trial]]
            
            if reward[trial] != -1:
                a1 = action_1[trial]
                a2 = action_2[trial]
                
                # Update Habit Trace
                # Chosen action trace moves to 1, unchosen to 0
                habit_values[a1] += habit_lr * (1.0 - habit_values[a1])
                habit_values[1-a1] += habit_lr * (0.0 - habit_values[1-a1])

                # Q-Learning
                delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
                delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
                
                q_stage1_mf[a1] += learning_rate * (delta_stage1 + lam * delta_stage2)
                q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        else:
            p_choice_2[trial] = 1.0

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid with Stage-Specific Learning Rates and Unchosen Decay
This model separates the learning dynamics for the high-level decision (Spaceships) and the low-level bandit task (Aliens). It incorporates the **decay of unchosen options** mechanism (which worked well previously) as the driver of persistence, but allows the two stages to adapt at different rates (`lr_1` vs `lr_2`).

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid model with Stage-Specific Learning Rates and Unchosen Decay.
    Differentiates learning speed for Spaceships vs Aliens and includes 
    forgetting for unchosen options to induce persistence.

    Parameters:
    lr_1: [0,1] - Learning rate for Stage 1 (Spaceships).
    lr_2: [0,1] - Learning rate for Stage 2 (Aliens).
    beta: [0,10] - Inverse temperature.
    w: [0,1] - Weight for Model-Based system.
    lam: [0,1] - Eligibility trace.
    decay: [0,1] - Decay rate for unchosen action values (0=no decay).
    """
    lr_1, lr_2, beta, w, lam, decay = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_1)
        sum_exp_q1 = np.sum(exp_q1)
        if sum_exp_q1 < 1e-9: probs_1 = np.ones(2)/2
        else: probs_1 = exp_q1 / sum_exp_q1
        
        if action_1[trial] != -1:
            p_choice_1[trial] = probs_1[action_1[trial]]
        else:
            p_choice_1[trial] = 1.0

        # Stage 2 Policy & Updates
        if action_1[trial] != -1 and state[trial] != -1 and action_2[trial] != -1:
            state_idx = state[trial]
            exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
            sum_exp_q2 = np.sum(exp_q2)
            if sum_exp_q2 < 1e-9: probs_2 = np.ones(2)/2
            else: probs_2 = exp_q2 / sum_exp_q2
            p_choice_2[trial] = probs_2[action_2[trial]]
            
            if reward[trial] != -1:
                a1 = action_1[trial]
                a2 = action_2[trial]
                
                delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
                delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
                
                # Update with specific learning rates
                q_stage1_mf[a1] += lr_1 * (delta_stage1 + lam * delta_stage2)
                q_stage2_mf[state_idx, a2] += lr_2 * delta_stage2
                
                # Decay unchosen options
                q_stage1_mf[1-a1] *= (1.0 - decay)
                q_stage2_mf[state_idx, 1-a2] *= (1.0 - decay)
        else:
            p_choice_2[trial] = 1.0

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid with Independent Betas and Stickiness
Instead of a weighting parameter `w` that trades off Model-Based and Model-Free control (forcing them to sum to 1), this model uses **independent inverse temperatures** (`beta_mb`, `beta_mf`). This allows the model to capture if one system is much more confident or variable than the other, combined with standard stickiness.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid model with Independent MB/MF Betas and Stickiness.
    Decouples the strength of the Model-Based and Model-Free controls using 
    separate inverse temperatures instead of a weighting parameter, plus stickiness.

    Parameters:
    learning_rate: [0,1] - Learning rate.
    beta_mb: [0,10] - Inverse temperature for Model-Based values.
    beta_mf: [0,10] - Inverse temperature for Model-Free values.
    lam: [0,1] - Eligibility trace.
    stickiness: [0,5] - Bonus added to the logits of the previously chosen action.
    """
    learning_rate, beta_mb, beta_mf, lam, stickiness = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combine logits independently
        logits_1 = beta_mb * q_stage1_mb + beta_mf * q_stage1_mf
        
        if last_action_1 != -1:
            logits_1[last_action_1] += stickiness
        
        # Softmax on logits
        # Subtract max for stability
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        if action_1[trial] != -1:
            p_choice_1[trial] = probs_1[action_1[trial]]
        else:
            p_choice_1[trial] = 1.0

        # Stage 2 Policy & Updates
        if action_1[trial] != -1 and state[trial] != -1 and action_2[trial] != -1:
            state_idx = state[trial]
            # Use beta_mf for stage 2 (pure MF)
            exp_q2 = np.exp(beta_mf * q_stage2_mf[state_idx])
            sum_exp_q2 = np.sum(exp_q2)
            if sum_exp_q2 < 1e-9: probs_2 = np.ones(2)/2
            else: probs_2 = exp_q2 / sum_exp_q2
            p_choice_2[trial] = probs_2[action_2[trial]]
            
            if reward[trial] != -1:
                a1 = action_1[trial]
                a2 = action_2[trial]
                last_action_1 = a1
                
                delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
                delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
                
                q_stage1_mf[a1] += learning_rate * (delta_stage1 + lam * delta_stage2)
                q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        else:
            p_choice_2[trial] = 1.0
            if action_1[trial] != -1: last_action_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```