Here are three new cognitive models based on the participant data and the two-step task structure.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Eligibility Traces and Stickiness.
    
    This model combines Model-Based (MB) and Model-Free (MF) reinforcement learning.
    It includes an eligibility trace (lambda) which allows the reward prediction error 
    at Stage 2 to directly update the value of the Stage 1 choice, bridging the gap 
    between the two stages. It also includes a stickiness parameter to account for 
    motor perseveration (tendency to repeat Stage 1 choices).

    Parameters:
    - lr: [0, 1] Learning rate for value updates.
    - beta_1: [0, 10] Inverse temperature for Stage 1 softmax.
    - beta_2: [0, 10] Inverse temperature for Stage 2 softmax.
    - w: [0, 1] Mixing weight (0 = pure MF, 1 = pure MB).
    - lam: [0, 1] Eligibility trace parameter (0 = TD(0), 1 = Monte Carlo).
    - stick: [0, 10] Stickiness bonus for repeating the previous Stage 1 action.
    """
    lr, beta_1, beta_2, w, lam, stick = model_parameters
    n_trials = len(action_1)
    
    # Fixed transition matrix: 0.7 probability of common transition
    # Row 0: Space 0 -> [Planet 0 (0.7), Planet 1 (0.3)]
    # Row 1: Space 1 -> [Planet 0 (0.3), Planet 1 (0.7)]
    trans_probs = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # 2 planets, 2 aliens
    q_stage2_mf.fill(0.5) # Initialize values to 0.5
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1
    
    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        # --- Stage 1 Policy ---
        # Model-Based Value calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = trans_probs @ max_q_stage2
        
        # Weighted hybrid value
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Calculate logits with stickiness
        logits_1 = beta_1 * q_net_1
        if prev_a1 != -1:
            logits_1[prev_a1] += stick
            
        # Softmax Stage 1
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        s2 = state[trial]
        
        # --- Stage 2 Policy ---
        logits_2 = beta_2 * q_stage2_mf[s2]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]
        
        # --- Value Updates ---
        # Stage 1 MF Update (TD error using Stage 2 Q-value)
        delta_1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_1
        
        # Stage 2 MF Update (Reward prediction error)
        delta_2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += lr * delta_2
        
        # Eligibility Trace: Propagate Stage 2 RPE back to Stage 1
        q_stage1_mf[a1] += lr * lam * delta_2
        
        prev_a1 = a1

    eps = 1e-10
    # Use mask to ignore missing trials where p_choice is 0
    valid_mask = p_choice_1 > 0
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Asymmetric Learning Hybrid Model with Stickiness.
    
    This model investigates whether the participant learns differently from positive 
    outcomes (rewards) versus negative outcomes (omissions). It uses separate learning 
    rates for positive and negative prediction errors. It also includes the hybrid 
    MB/MF mechanism and choice stickiness.
    
    Parameters:
    - lr_pos: [0, 1] Learning rate for positive prediction errors (delta > 0).
    - lr_neg: [0, 1] Learning rate for negative prediction errors (delta < 0).
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - w: [0, 1] Mixing weight (0=MF, 1=MB).
    - stick: [0, 10] Stickiness bonus for repeating Stage 1 choice.
    """
    lr_pos, lr_neg, beta_1, beta_2, w, stick = model_parameters
    n_trials = len(action_1)
    
    trans_probs = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    q_stage2_mf.fill(0.5)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1
    
    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        # Stage 1 Selection
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = trans_probs @ max_q_stage2
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta_1 * q_net_1
        if prev_a1 != -1:
            logits_1[prev_a1] += stick
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        s2 = state[trial]
        
        # Stage 2 Selection
        logits_2 = beta_2 * q_stage2_mf[s2]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]
        
        # Updates with Asymmetric Learning Rates
        
        # Stage 1 Update
        delta_1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        eff_lr_1 = lr_pos if delta_1 > 0 else lr_neg
        q_stage1_mf[a1] += eff_lr_1 * delta_1
        
        # Stage 2 Update
        delta_2 = r - q_stage2_mf[s2, a2]
        eff_lr_2 = lr_pos if delta_2 > 0 else lr_neg
        q_stage2_mf[s2, a2] += eff_lr_2 * delta_2
        
        prev_a1 = a1

    eps = 1e-10
    valid_mask = p_choice_1 > 0
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Eligibility Traces and Value Decay.
    
    This model addresses the drifting reward probabilities of the task. It includes
    a decay parameter that slowly returns the values of unchosen aliens (Stage 2) 
    towards a neutral probability (0.5). It also uses eligibility traces (lambda) 
    to robustly credit Stage 2 outcomes to Stage 1 choices.
    
    Parameters:
    - lr: [0, 1] Learning rate.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - w: [0, 1] Mixing weight (0=MF, 1=MB).
    - lam: [0, 1] Eligibility trace parameter.
    - decay: [0, 1] Decay rate for unchosen Stage 2 options (0 = no decay).
    """
    lr, beta_1, beta_2, w, lam, decay = model_parameters
    n_trials = len(action_1)
    
    trans_probs = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    q_stage2_mf.fill(0.5)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        # Stage 1 Selection
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = trans_probs @ max_q_stage2
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta_1 * q_net_1
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        s2 = state[trial]
        
        # Stage 2 Selection
        logits_2 = beta_2 * q_stage2_mf[s2]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]
        
        # Updates
        delta_1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_1
        
        delta_2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += lr * delta_2
        
        # Eligibility trace update for Stage 1
        q_stage1_mf[a1] += lr * lam * delta_2
        
        # Decay for unchosen Stage 2 option
        # Decays towards 0.5 (chance level) to handle drifting probabilities
        unchosen_a2 = 1 - a2
        q_stage2_mf[s2, unchosen_a2] = (1 - decay) * q_stage2_mf[s2, unchosen_a2] + decay * 0.5

    eps = 1e-10
    valid_mask = p_choice_1 > 0
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss
```