Here are the three proposed cognitive models.

### Model 1: Hybrid Learner with Counterfactual Updating
This model incorporates a "counterfactual" learning mechanism. Given the binary nature of the rewards (mostly 0 or 1), the participant may infer that if the chosen alien yielded a poor outcome (0 or -1), the unchosen alien would likely have yielded a good outcome (1), and vice versa. This effectively assumes an anti-correlated reward structure. This model updates the unchosen Stage 2 option based on this inferred fictitious outcome, scaled by a weight parameter.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner with Counterfactual Updating.
    
    The agent assumes outcomes are potentially anti-correlated: if the chosen 
    alien gives a poor reward, the unchosen one is updated as if it gave a 
    good reward (and vice versa). This captures 'fictitious play' or 
    counterfactual reasoning in the bandit stage.
    
    Parameters:
    learning_rate: [0, 1] Update rate for chosen options.
    beta_1: [0, 10] Inverse temperature for Stage 1.
    beta_2: [0, 10] Inverse temperature for Stage 2.
    w: [0, 1] Mixing weight (0=MF, 1=MB).
    stickiness: [0, 5] Perseveration bonus for Stage 1.
    cf_weight: [0, 1] Weight of the counterfactual update for the unchosen alien.
    """
    learning_rate, beta_1, beta_2, w, stickiness, cf_weight = model_parameters
    n_trials = len(action_1)
    
    # Fixed transition matrix for MB calculation (participant knows structure)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # Used for both MB and MF calculations
    
    prev_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based Value
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Value
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Logits and Stickiness
        logits_1 = beta_1 * q_net_1
        if prev_action_1 != -1:
            logits_1[prev_action_1] += stickiness
            
        # Softmax Stage 1
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Store for next trial stickiness
        prev_action_1 = action_1[trial]
        
        # --- Stage 2 Choice ---
        state_idx = state[trial]
        a2 = action_2[trial]
        
        if a2 != -1: # Valid trial
            logits_2 = beta_2 * q_stage2_mf[state_idx]
            exp_q2 = np.exp(logits_2 - np.max(logits_2))
            probs_2 = exp_q2 / np.sum(exp_q2)
            p_choice_2[trial] = probs_2[a2]
            
            # --- Learning ---
            r = reward[trial]
            
            # Update Chosen Stage 2
            pe_2 = r - q_stage2_mf[state_idx, a2]
            q_stage2_mf[state_idx, a2] += learning_rate * pe_2
            
            # Update Unchosen Stage 2 (Counterfactual)
            # Logic: If reward is low (<=0), assume unchosen was high (1).
            # If reward is high (>0), assume unchosen was low (0).
            cf_target = 1.0 if r <= 0 else 0.0
            pe_cf = cf_target - q_stage2_mf[state_idx, 1-a2]
            q_stage2_mf[state_idx, 1-a2] += learning_rate * cf_weight * pe_cf
            
            # Update Stage 1 MF (TD(0))
            # Using Q-value of chosen stage 2 option as proxy for state value
            pe_1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[action_1[trial]]
            q_stage1_mf[action_1[trial]] += learning_rate * pe_1
            
        else:
            # Missing data case
            p_choice_2[trial] = 1.0

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Learner with Dynamic Associability (Pearce-Hall)
This model implements a dynamic learning rate inspired by the Pearce-Hall theory. Instead of a fixed learning rate, the agent maintains an "associability" or attention weight for each option. This weight increases when prediction errors are high (surprise/volatility) and decays when errors are low (stability). This allows the model to adapt its learning speed to the changing probabilities of the aliens.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner with Dynamic Associability (Pearce-Hall style).
    
    The learning rate is dynamic, scaling with the magnitude of the prediction error.
    This allows the agent to react quickly to volatility (high surprise) and 
    stabilize when errors are low.
    
    Parameters:
    base_lr: [0, 1] Scaling factor for the dynamic learning rate.
    assoc_decay: [0, 1] Decay rate for the associability (eta).
    beta_1: [0, 10] Inverse temperature for Stage 1.
    beta_2: [0, 10] Inverse temperature for Stage 2.
    w: [0, 1] Mixing weight (0=MF, 1=MB).
    stickiness: [0, 5] Perseveration bonus for Stage 1.
    """
    base_lr, assoc_decay, beta_1, beta_2, w, stickiness = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Initialize associability (attention/learning weight) for each state-action pair
    # Shape: Stage 1 (2), Stage 2 (2,2)
    assoc_1 = np.ones(2) * 0.5 
    assoc_2 = np.ones((2, 2)) * 0.5
    
    prev_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta_1 * q_net_1
        if prev_action_1 != -1:
            logits_1[prev_action_1] += stickiness
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        prev_action_1 = action_1[trial]
        
        # --- Stage 2 Choice ---
        state_idx = state[trial]
        a2 = action_2[trial]
        
        if a2 != -1:
            logits_2 = beta_2 * q_stage2_mf[state_idx]
            exp_q2 = np.exp(logits_2 - np.max(logits_2))
            probs_2 = exp_q2 / np.sum(exp_q2)
            p_choice_2[trial] = probs_2[a2]
            
            # --- Learning ---
            r = reward[trial]
            
            # Dynamic Learning Rate for Stage 2
            # alpha = base_lr * associability
            curr_assoc_2 = assoc_2[state_idx, a2]
            eff_lr_2 = base_lr * curr_assoc_2
            
            pe_2 = r - q_stage2_mf[state_idx, a2]
            q_stage2_mf[state_idx, a2] += eff_lr_2 * pe_2
            
            # Update Associability 2 (Pearce-Hall rule)
            # k_{t+1} = (1-eta)*k_t + eta*|delta|
            assoc_2[state_idx, a2] = (1 - assoc_decay) * curr_assoc_2 + assoc_decay * abs(pe_2)
            
            # Stage 1 Update
            a1 = action_1[trial]
            pe_1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
            
            curr_assoc_1 = assoc_1[a1]
            eff_lr_1 = base_lr * curr_assoc_1
            q_stage1_mf[a1] += eff_lr_1 * pe_1
            
            assoc_1[a1] = (1 - assoc_decay) * curr_assoc_1 + assoc_decay * abs(pe_1)
            
        else:
            p_choice_2[trial] = 1.0

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Learner with Transition Learning
This model relaxes the assumption that the participant perfectly understands the transition matrix (0.7/0.3). Instead, the Model-Based component relies on an *estimated* transition matrix that is updated trial-by-trial based on observed transitions. This allows the model to capture behavior where the participant might initially explore or have incorrect beliefs about which spaceship goes to which planet, or if they believe the transitions are changing.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner with Transition Learning.
    
    The agent does not assume a fixed transition matrix (0.7/0.3) but learns it 
    from experience. This allows the Model-Based component to adapt if the 
    participant's beliefs about spaceship destinations change or are initially uniform.
    
    Parameters:
    lr_val: [0, 1] Learning rate for Q-values (Stage 1 MF and Stage 2).
    lr_trans: [0, 1] Learning rate for the transition matrix.
    beta_1: [0, 10] Inverse temperature for Stage 1.
    beta_2: [0, 10] Inverse temperature for Stage 2.
    w: [0, 1] Mixing weight (0=MF, 1=MB).
    stickiness: [0, 5] Perseveration bonus for Stage 1.
    """
    lr_val, lr_trans, beta_1, beta_2, w, stickiness = model_parameters
    n_trials = len(action_1)
    
    # Initialize transitions uniformly (no initial bias)
    # Rows: Spaceship 0, Spaceship 1. Cols: Planet 0, Planet 1.
    est_trans_matrix = np.ones((2, 2)) * 0.5
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # MB uses estimated transition matrix
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = est_trans_matrix @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta_1 * q_net_1
        if prev_action_1 != -1:
            logits_1[prev_action_1] += stickiness
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        prev_action_1 = action_1[trial]
        
        # --- Transition Update ---
        # Update beliefs about where the spaceship goes
        # Before Stage 2 choice, the agent observes the state
        obs_state = state[trial]
        # Update the row corresponding to the chosen spaceship
        # T[s1, s'] += lr * (1 - T[s1, s'])
        # T[s1, other] += lr * (0 - T[s1, other])
        
        # Vectorized update for the row to maintain sum to 1
        row = est_trans_matrix[prev_action_1]
        target = np.zeros(2)
        target[obs_state] = 1.0
        
        est_trans_matrix[prev_action_1] += lr_trans * (target - row)
        
        # --- Stage 2 Choice ---
        a2 = action_2[trial]
        
        if a2 != -1:
            logits_2 = beta_2 * q_stage2_mf[obs_state]
            exp_q2 = np.exp(logits_2 - np.max(logits_2))
            probs_2 = exp_q2 / np.sum(exp_q2)
            p_choice_2[trial] = probs_2[a2]
            
            # --- Value Learning ---
            r = reward[trial]
            
            pe_2 = r - q_stage2_mf[obs_state, a2]
            q_stage2_mf[obs_state, a2] += lr_val * pe_2
            
            pe_1 = q_stage2_mf[obs_state, a2] - q_stage1_mf[prev_action_1]
            q_stage1_mf[prev_action_1] += lr_val * pe_1
            
        else:
            p_choice_2[trial] = 1.0

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```