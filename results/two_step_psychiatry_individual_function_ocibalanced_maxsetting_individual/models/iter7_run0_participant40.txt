Here are three new cognitive models based on the participant data and the feedback provided.

### Cognitive Model 1
**Hybrid Model-Based / Model-Free with Decay**
This model combines the Model-Based (MB) planning mechanism with the Model-Free (MF) decay mechanism found in the best-performing model so far. It tests whether the participant uses knowledge of the transition structure (MB) in conjunction with a tendency to forget unchosen option values (Decay). It uses a single learning rate for MF updates and a mixing weight `w` to integrate MB and MF values.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free with Decay.
    
    Combines MB planning with MF values that decay over time.
    Calculates Stage 1 values as a weighted mix of MB (transition-based) 
    and MF (experience-based) values. Unchosen MF values decay.
    
    Parameters:
    - learning_rate: [0, 1] Update rate for MF Q-values.
    - beta: [0, 10] Inverse temperature for softmax.
    - w: [0, 1] Weight for Model-Based control (0=Pure MF, 1=Pure MB).
    - decay: [0, 1] Decay rate for unchosen Q-values (0=no decay, 1=instant forget).
    """
    learning_rate, beta, w, decay = model_parameters
    n_trials = len(action_1)
    
    # Fixed transition probabilities for the task
    # Row 0: Spaceship 0 (A) -> [Planet 0 (X), Planet 1 (Y)]
    # Row 1: Spaceship 1 (U) -> [Planet 0 (X), Planet 1 (Y)]
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Initialize Q-values to 0.5 (neutral)
    q_stage1_mf = np.zeros(2) + 0.5
    q_stage2_mf = np.zeros((2, 2)) + 0.5
    
    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        
        # Model-Based Value: V_MB(s1) = Sum P(s2|s1) * Max_a2 Q_MF(s2, a2)
        # We use the MF stage 2 values as the estimate of state value
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Net Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax Choice 1
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Decision ---
        s = state[trial]
        exp_q2 = np.exp(beta * q_stage2_mf[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning ---
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]
        
        # Stage 1 MF Update (SARSA-style from Stage 2 Q-value)
        target_s1 = q_stage2_mf[s, a2]
        q_stage1_mf[a1] += learning_rate * (target_s1 - q_stage1_mf[a1])
        # Decay unchosen Stage 1 option
        q_stage1_mf[1-a1] *= (1.0 - decay)
        
        # Stage 2 MF Update
        q_stage2_mf[s, a2] += learning_rate * (r - q_stage2_mf[s, a2])
        # Decay unchosen Stage 2 option
        q_stage2_mf[s, 1-a2] *= (1.0 - decay)
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2
**MF Q-Learning with Eligibility Traces (Lambda) and Perseverance**
This model investigates if the participant reinforces the first-stage choice directly based on the final reward (Outcome-based), rather than just the second-stage value. This is implemented via an eligibility trace parameter `lam` (lambda). It also includes a perseverance bonus to account for the participant's tendency to repeat choices.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    MF Q-Learning with Eligibility Traces (Lambda) and Perseverance.
    
    Uses TD(lambda) logic where Stage 1 is updated by both the Stage 2 value
    and the final reward directly. Includes a stickiness bonus.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - lam: [0, 1] Eligibility trace decay (0 = TD(0), 1 = Monte Carlo).
    - perseverance: [0, 5] Stickiness bonus added to the previously chosen action's logits.
    """
    learning_rate, beta, lam, perseverance = model_parameters
    n_trials = len(action_1)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2) + 0.5
    q_stage2 = np.zeros((2, 2)) + 0.5
    
    prev_a1 = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        logits = beta * q_stage1
        # Add perseverance bonus to the previously chosen action
        if prev_a1 != -1:
            logits[prev_a1] += perseverance
        
        exp_q1 = np.exp(logits)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Decision ---
        s = state[trial]
        exp_q2 = np.exp(beta * q_stage2[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]
        
        # Prediction Error 1: Difference between Stage 2 value and Stage 1 value
        pe1 = q_stage2[s, a2] - q_stage1[a1]
        
        # Prediction Error 2: Difference between Reward and Stage 2 value
        pe2 = r - q_stage2[s, a2]
        
        # Stage 2 Update: Standard TD
        q_stage2[s, a2] += learning_rate * pe2
        
        # Stage 1 Update: TD(lambda)
        # Combines the immediate PE (pe1) and the propagated PE (pe2) weighted by lambda
        q_stage1[a1] += learning_rate * (pe1 + lam * pe2)
        
        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3
**MF Q-Learning with Asymmetric Learning Rates and Decay**
This model posits that the participant learns differently from positive prediction errors (better than expected) versus negative prediction errors (worse than expected). This asymmetry is combined with the decay mechanism for unchosen options, providing a robust mechanism for value estimation in drifting environments.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    MF Q-Learning with Asymmetric Learning Rates and Decay.
    
    Differentiates learning rates for positive and negative prediction errors.
    Unchosen options decay towards zero.
    
    Parameters:
    - alpha_pos: [0, 1] Learning rate for positive prediction errors (Reward > Expectation).
    - alpha_neg: [0, 1] Learning rate for negative prediction errors (Reward < Expectation).
    - beta: [0, 10] Inverse temperature.
    - decay: [0, 1] Decay rate for unchosen Q-values.
    """
    alpha_pos, alpha_neg, beta, decay = model_parameters
    n_trials = len(action_1)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2) + 0.5
    q_stage2 = np.zeros((2, 2)) + 0.5
    
    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Decision ---
        s = state[trial]
        exp_q2 = np.exp(beta * q_stage2[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]
        
        # Stage 1 Update
        target_s1 = q_stage2[s, a2]
        pe1 = target_s1 - q_stage1[a1]
        # Choose learning rate based on sign of PE
        lr1 = alpha_pos if pe1 >= 0 else alpha_neg
        q_stage1[a1] += lr1 * pe1
        # Decay unchosen
        q_stage1[1-a1] *= (1.0 - decay)
        
        # Stage 2 Update
        pe2 = r - q_stage2[s, a2]
        lr2 = alpha_pos if pe2 >= 0 else alpha_neg
        q_stage2[s, a2] += lr2 * pe2
        # Decay unchosen
        q_stage2[s, 1-a2] *= (1.0 - decay)
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```