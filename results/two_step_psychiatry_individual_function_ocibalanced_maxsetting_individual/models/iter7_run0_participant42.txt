Here are the three cognitive models as Python functions.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Dual-Stage Learning and Stickiness Model.
    
    This model distinguishes between the learning dynamics of Stage 1 (Spaceships) 
    and Stage 2 (Aliens). It hypothesizes that the participant learns the stable 
    Stage 1 transitions and the volatile Stage 2 rewards at different rates 
    (lr_s1 vs lr_s2). Additionally, it includes separate stickiness parameters 
    for each stage, capturing both the tendency to repeat spaceship choices and 
    the tendency to repeat alien choices (response stickiness) independently.

    Parameters:
    - lr_s1: [0, 1] Learning rate for Stage 1 (Model-Free) values.
    - lr_s2: [0, 1] Learning rate for Stage 2 values.
    - beta: [0, 10] Inverse temperature for softmax (shared).
    - w: [0, 1] Weight for Model-Based (1) vs Model-Free (0) control.
    - lambda_coef: [0, 1] Eligibility trace decay parameter.
    - stick_s1: [0, 10] Stickiness (perseveration) for Stage 1 choices.
    - stick_s2: [0, 10] Stickiness (perseveration) for Stage 2 choices.
    """
    lr_s1, lr_s2, beta, w, lambda_coef, stick_s1, stick_s2 = model_parameters
    n_trials = len(action_1)
    
    # Fixed transition matrix (common/rare)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2 = np.zeros((2, 2)) # State x Action
    
    last_action_1 = -1
    last_action_2 = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based value: expected max value of next stage
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Stage 1 value
        q_net_s1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        logits_1 = beta * q_net_s1
        
        # Stage 1 Stickiness
        if last_action_1 != -1:
            logits_1[last_action_1] += stick_s1
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Policy ---
        state_idx = state[trial]
        q_s2 = q_stage2[state_idx]
        logits_2 = beta * q_s2
        
        # Stage 2 Stickiness
        if last_action_2 != -1:
            logits_2[last_action_2] += stick_s2
            
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Value Updates ---
        chosen_q2_val = q_stage2[state_idx, action_2[trial]]
        
        # Stage 1 TD error
        delta_stage1 = chosen_q2_val - q_stage1_mf[action_1[trial]]
        
        # Stage 2 TD error
        delta_stage2 = reward[trial] - chosen_q2_val
        
        # Update Stage 1 MF (with eligibility trace from Stage 2)
        # Using lr_s1 for Stage 1 updates
        q_stage1_mf[action_1[trial]] += lr_s1 * (delta_stage1 + lambda_coef * delta_stage2)
        
        # Update Stage 2
        # Using lr_s2 for Stage 2 updates
        q_stage2[state_idx, action_2[trial]] += lr_s2 * delta_stage2
        
        last_action_1 = action_1[trial]
        last_action_2 = action_2[trial]
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Generalized Alien Values with Passive Decay (Forgetting).
    
    This model assumes the participant generalizes values across planets (using a 
    mixture of specific and general Q-values) but also exhibits passive forgetting 
    of unchosen specific options. This decay mechanism helps the agent adapt to 
    the changing probabilities of the aliens (restless bandit structure) by 
    gradually discounting old information about unvisited options.
    
    Parameters:
    - learning_rate: [0, 1] Update rate for chosen options.
    - beta: [0, 10] Inverse temperature for softmax.
    - w: [0, 1] Weight for Model-Based (1) vs Model-Free (0).
    - lambda_coef: [0, 1] Eligibility trace decay.
    - stickiness: [0, 10] Stickiness for Stage 1 choices.
    - gen_weight: [0, 1] Weight for general (context-free) Q-values vs specific.
    - decay: [0, 1] Decay rate for unchosen specific options (0 = no decay).
    """
    learning_rate, beta, w, lambda_coef, stickiness, gen_weight, decay = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_specific = np.zeros((2, 2)) # State x Action
    q_stage2_general = np.zeros(2)       # Action only
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        # Mix specific and general values for decision making
        q_net_s2 = (1 - gen_weight) * q_stage2_specific + \
                   gen_weight * q_stage2_general[np.newaxis, :]
                   
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_net_s2, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_s1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        logits_1 = beta * q_net_s1
        if last_action_1 != -1:
            logits_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Policy ---
        state_idx = state[trial]
        q_s2 = q_net_s2[state_idx]
        logits_2 = beta * q_s2
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        chosen_a2 = action_2[trial]
        chosen_q2_val = q_net_s2[state_idx, chosen_a2]
        
        # Stage 1 Update
        delta_stage1 = chosen_q2_val - q_stage1_mf[action_1[trial]]
        delta_net = reward[trial] - chosen_q2_val
        q_stage1_mf[action_1[trial]] += learning_rate * (delta_stage1 + lambda_coef * delta_net)
        
        # Stage 2 Update (Chosen)
        delta_spec = reward[trial] - q_stage2_specific[state_idx, chosen_a2]
        delta_gen = reward[trial] - q_stage2_general[chosen_a2]
        
        q_stage2_specific[state_idx, chosen_a2] += learning_rate * delta_spec
        q_stage2_general[chosen_a2] += learning_rate * delta_gen
        
        # Stage 2 Update (Unchosen) - Decay specific value in current state
        unchosen_a2 = 1 - chosen_a2
        q_stage2_specific[state_idx, unchosen_a2] *= (1 - decay)
        
        last_action_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Generalized Hybrid with Outcome-Dependent Stickiness.
    
    This model extends the generalized value learning hypothesis by refining 
    the stickiness mechanism. Instead of a single stickiness parameter, it uses 
    separate 'Win-Stay' (stick_win) and 'Lose-Stay' (stick_loss) parameters. 
    This allows the model to capture the participant's specific "Win-Stay, 
    Lose-Switch" behavioral strategy on Stage 1, modulated by the Model-Based 
    and Model-Free value expectations.
    
    Parameters:
    - learning_rate: [0, 1]
    - beta: [0, 10]
    - w: [0, 1]
    - lambda_coef: [0, 1]
    - gen_weight: [0, 1] Weight for general Q-values.
    - stick_win: [0, 10] Stickiness bonus applied if the previous trial was rewarded.
    - stick_loss: [0, 10] Stickiness bonus applied if the previous trial was unrewarded.
    """
    learning_rate, beta, w, lambda_coef, gen_weight, stick_win, stick_loss = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_specific = np.zeros((2, 2))
    q_stage2_general = np.zeros(2)
    
    last_action_1 = -1
    last_reward = 0 # Initialize assuming neutral/no reward for trial 0 context
    
    for trial in range(n_trials):
        # Mix values
        q_net_s2 = (1 - gen_weight) * q_stage2_specific + \
                   gen_weight * q_stage2_general[np.newaxis, :]
                   
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_net_s2, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_s1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        logits_1 = beta * q_net_s1
        
        # Outcome-dependent Stickiness
        if last_action_1 != -1:
            if last_reward == 1:
                logits_1[last_action_1] += stick_win
            else:
                logits_1[last_action_1] += stick_loss
                
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Policy ---
        state_idx = state[trial]
        q_s2 = q_net_s2[state_idx]
        logits_2 = beta * q_s2
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        chosen_a2 = action_2[trial]
        chosen_q2_val = q_net_s2[state_idx, chosen_a2]
        
        delta_stage1 = chosen_q2_val - q_stage1_mf[action_1[trial]]
        delta_net = reward[trial] - chosen_q2_val
        
        q_stage1_mf[action_1[trial]] += learning_rate * (delta_stage1 + lambda_coef * delta_net)
        
        delta_spec = reward[trial] - q_stage2_specific[state_idx, chosen_a2]
        delta_gen = reward[trial] - q_stage2_general[chosen_a2]
        
        q_stage2_specific[state_idx, chosen_a2] += learning_rate * delta_spec
        q_stage2_general[chosen_a2] += learning_rate * delta_gen
        
        last_action_1 = action_1[trial]
        last_reward = reward[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```