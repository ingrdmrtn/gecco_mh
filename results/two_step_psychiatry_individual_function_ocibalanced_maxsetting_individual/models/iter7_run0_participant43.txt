Here are three new cognitive models based on the participant data and the feedback provided.

### Model 1: Asymmetric TD($\lambda$) Hybrid
**Hypothesis:** This model extends the reinforcement learning mechanism by incorporating **eligibility traces ($\lambda$)** combined with **asymmetric learning rates**. The parameter $\lambda$ allows the Stage 2 prediction error to directly influence Stage 1 values (Model-Free eligibility), bridging the gap between pure MF and MB. The asymmetric learning rates capture the participant's differential sensitivity to gains versus losses (seen in the data where streaks of rewards lead to exploitation, but losses cause switching).

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Asymmetric TD(lambda) Hybrid.
    
    Combines Model-Based/Model-Free hybrid with Eligibility Traces (lambda) and
    Asymmetric Learning Rates. Lambda allows the reward at Stage 2 to update 
    Stage 1 values directly, scaled by the trace.
    
    Parameters:
    - lr_pos: [0, 1] Learning rate for positive prediction errors.
    - lr_neg: [0, 1] Learning rate for negative prediction errors.
    - lam: [0, 1] Eligibility trace parameter (0 = pure TD(0), 1 = full Monte Carlo).
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - w: [0, 1] Weighting parameter (0 = Pure MF/TD, 1 = Pure MB).
    - pers: [0, 1] Perseveration bonus for Stage 1.
    """
    lr_pos, lr_neg, lam, beta_1, beta_2, w, pers = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if prev_action_1 != -1:
            q_net[prev_action_1] += pers
            
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        prev_action_1 = a1
        state_idx = state[trial] # 0 or 1

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]

        # --- Updates ---
        # 1. Stage 1 MF Update (TD(0) part)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        alpha_1 = lr_pos if delta_stage1 > 0 else lr_neg
        q_stage1_mf[a1] += alpha_1 * delta_stage1
        
        # 2. Stage 2 MF Update
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        alpha_2 = lr_pos if delta_stage2 > 0 else lr_neg
        q_stage2_mf[state_idx, a2] += alpha_2 * delta_stage2
        
        # 3. Eligibility Trace Update (Stage 2 RPE applied to Stage 1)
        # The trace allows Stage 1 to learn from the final reward via lambda
        q_stage1_mf[a1] += alpha_1 * lam * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Dynamic Transition Learning with Asymmetric Value Learning
**Hypothesis:** The participant may not assume the transition probabilities are fixed at 70/30, especially given the "rare" transitions observed in the data. This model assumes the participant **learns the transition matrix** (`lr_trans`) over time. Combined with **asymmetric value learning** (`lr_pos`, `lr_neg`), this allows the model to capture both the structural learning (which spaceship goes where) and the reward valuation (sensitivity to gold coins).

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Dynamic Transition Learning with Asymmetric Value Learning.
    
    The participant estimates the transition matrix online rather than assuming
    fixed probabilities. Values are updated using asymmetric learning rates.
    
    Parameters:
    - lr_pos: [0, 1] Value learning rate for positive RPE.
    - lr_neg: [0, 1] Value learning rate for negative RPE.
    - lr_trans: [0, 1] Learning rate for updating transition probabilities.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - w: [0, 1] Weighting parameter (MB vs MF).
    - pers: [0, 1] Perseveration bonus.
    """
    lr_pos, lr_neg, lr_trans, beta_1, beta_2, w, pers = model_parameters
    n_trials = len(action_1)
    
    # Initialize estimated transition matrix (Action x Planet)
    # Start with flat prior or weak prior. 
    # Row 0: Space A -> [Prob Planet X, Prob Planet Y]
    # Row 1: Space B -> [Prob Planet X, Prob Planet Y]
    est_trans_matrix = np.array([[0.5, 0.5], [0.5, 0.5]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Use ESTIMATED transition matrix for MB calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = est_trans_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if prev_action_1 != -1:
            q_net[prev_action_1] += pers
            
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        prev_action_1 = a1
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]

        # --- Value Updates (Asymmetric) ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        alpha_1 = lr_pos if delta_stage1 > 0 else lr_neg
        q_stage1_mf[a1] += alpha_1 * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        alpha_2 = lr_pos if delta_stage2 > 0 else lr_neg
        q_stage2_mf[state_idx, a2] += alpha_2 * delta_stage2
        
        # --- Transition Matrix Update ---
        # Update belief about where spaceship a1 goes
        # State estimation error: 1 for the observed state, 0 for the other
        # We update the row corresponding to action a1
        
        # Increase prob of observed state
        est_trans_matrix[a1, state_idx] += lr_trans * (1 - est_trans_matrix[a1, state_idx])
        # Decrease prob of unobserved state
        est_trans_matrix[a1, 1 - state_idx] += lr_trans * (0 - est_trans_matrix[a1, 1 - state_idx])

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Outcome-Dependent Strategy Weighting
**Hypothesis:** The participant may shift between Model-Based (planning) and Model-Free (habit) strategies based on recent success. This model implements **outcome-dependent weighting**, where the balance parameter `w` changes depending on whether the previous trial was rewarded (`w_rew`) or unrewarded (`w_unrew`). This reflects a "Win-Stay (Habit/MF), Lose-Shift (Plan/MB)" cognitive style, or conversely, relying on the internal model only when habits fail.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Outcome-Dependent Strategy Weighting.
    
    The weight (w) determining the balance between MB and MF control changes 
    based on the outcome of the previous trial.
    
    Parameters:
    - lr: [0, 1] Learning rate.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - w_rew: [0, 1] MB weight after a rewarded trial.
    - w_unrew: [0, 1] MB weight after an unrewarded trial.
    - pers: [0, 1] Perseveration bonus.
    """
    lr, beta_1, beta_2, w_rew, w_unrew, pers = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1
    prev_reward = 0 # Assume unrewarded start or neutral
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Select weight based on previous outcome
        current_w = w_rew if prev_reward == 1 else w_unrew
        
        q_net = current_w * q_stage1_mb + (1 - current_w) * q_stage1_mf
        
        if prev_action_1 != -1:
            q_net[prev_action_1] += pers
            
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        prev_action_1 = a1
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]
        prev_reward = r

        # --- Updates (Standard Hybrid) ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += lr * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```