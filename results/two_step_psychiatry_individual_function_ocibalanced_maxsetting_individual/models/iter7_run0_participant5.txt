Here are the three proposed cognitive models.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Asymmetric TD-Lambda with Value Decay.
    
    This model extends the TD-Lambda learner by introducing:
    1. Asymmetric learning rates (alpha_pos, alpha_neg) to distinguish learning 
       from positive versus non-positive prediction errors.
    2. Value decay, which gradually reduces the value of unchosen actions/states,
       mimicking a forgetting process or tracking non-stationary rewards.
    
    Parameters:
    - alpha_pos: [0, 1] Learning rate for positive prediction errors.
    - alpha_neg: [0, 1] Learning rate for negative prediction errors.
    - beta_1: [0, 10] Inverse temperature for Stage 1 choices.
    - beta_2: [0, 10] Inverse temperature for Stage 2 choices.
    - lambd: [0, 1] Eligibility trace decay (credit assignment to Stage 1).
    - decay_rate: [0, 1] Decay rate for unvisited state-action values.
    """
    alpha_pos, alpha_neg, beta_1, beta_2, lambd, decay_rate = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # Handle missing data
        if action_1[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        # Stage 1 Choice
        exp_q1 = np.exp(beta_1 * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Transition
        state_idx = state[trial]
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]

        # Stage 2 Choice
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Stage 1 Update (TD-0 part)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        alpha_1 = alpha_pos if delta_stage1 > 0 else alpha_neg
        q_stage1_mf[a1] = q_stage1_mf[a1] + alpha_1 * delta_stage1

        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        alpha_2 = alpha_pos if delta_stage2 > 0 else alpha_neg
        q_stage2_mf[state_idx, a2] = q_stage2_mf[state_idx, a2] + alpha_2 * delta_stage2

        # Eligibility Trace Update for Stage 1 (TD-Lambda part)
        # We use alpha_1 for the trace update consistency, or re-evaluate alpha based on delta2
        # Standard implementation applies the stage 2 alpha to the propagated error
        q_stage1_mf[a1] = q_stage1_mf[a1] + alpha_2 * lambd * delta_stage2

        # Value Decay
        # Decay unchosen option in stage 1
        q_stage1_mf[1 - a1] *= (1.0 - decay_rate)
        # Decay unchosen option in visited state
        q_stage2_mf[state_idx, 1 - a2] *= (1.0 - decay_rate)
        # Decay all options in unvisited state
        q_stage2_mf[1 - state_idx, :] *= (1.0 - decay_rate)

    eps = 1e-10
    valid_mask = (p_choice_1 > 0) & (p_choice_2 > 0)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Dual-Alpha TD-Lambda with Stickiness.
    
    This model assumes the participant learns at different rates during the navigation 
    phase (Stage 1) versus the harvesting phase (Stage 2). It also includes a stickiness 
    parameter to account for choice perseveration in Stage 1.
    
    Parameters:
    - alpha1: [0, 1] Learning rate for Stage 1 updates.
    - alpha2: [0, 1] Learning rate for Stage 2 updates.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - lambd: [0, 1] Eligibility trace decay.
    - stickiness: [-5, 5] Choice perseveration bonus for Stage 1.
    """
    alpha1, alpha2, beta_1, beta_2, lambd, stickiness = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            prev_action_1 = -1
            continue

        # Stage 1 Choice with Stickiness
        q_net_1 = q_stage1_mf.copy()
        if prev_action_1 != -1:
            q_net_1[prev_action_1] += stickiness
            
        exp_q1 = np.exp(beta_1 * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]

        # Stage 2 Choice
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] = q_stage1_mf[a1] + alpha1 * delta_stage1

        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] = q_stage2_mf[state_idx, a2] + alpha2 * delta_stage2

        # Eligibility Trace Update (uses alpha1 for consistency with Stage 1 value plasticity)
        q_stage1_mf[a1] = q_stage1_mf[a1] + alpha1 * lambd * delta_stage2
        
        prev_action_1 = a1

    eps = 1e-10
    valid_mask = (p_choice_1 > 0) & (p_choice_2 > 0)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Asymmetric Hybrid Model with Stickiness.
    
    A hybrid Model-Based / Model-Free agent. It uses asymmetric learning rates for 
    updating the Stage 2 values (which drive both the MB planning and MF learning).
    The Stage 1 MF values are updated via TD(1) (direct reinforcement) using the 
    same asymmetric rates. Includes stickiness.
    
    Parameters:
    - alpha_pos: [0, 1] Learning rate for positive RPEs.
    - alpha_neg: [0, 1] Learning rate for negative RPEs.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - w: [0, 1] Weighting parameter (0=Pure MF, 1=Pure MB).
    - stickiness: [-5, 5] Choice perseveration bonus.
    """
    alpha_pos, alpha_neg, beta_1, beta_2, w, stickiness = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Q_MF for stage 1 (model-free cache)
    q_stage1_mf = np.zeros(2)
    # Q values for stage 2 (used by both MB and MF)
    q_stage2 = np.zeros((2, 2))
    
    # Fixed transition matrix for MB step
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    prev_action_1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            prev_action_1 = -1
            continue

        # --- Stage 1 Policy ---
        # Model-Based Value: Q_MB = T * max(Q2)
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Value
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Stickiness
        if prev_action_1 != -1:
            q_net_1[prev_action_1] += stickiness

        exp_q1 = np.exp(beta_1 * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta_2 * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Stage 2 RPE
        delta_stage2 = r - q_stage2[state_idx, a2]
        alpha_eff = alpha_pos if delta_stage2 > 0 else alpha_neg
        q_stage2[state_idx, a2] = q_stage2[state_idx, a2] + alpha_eff * delta_stage2
        
        # Stage 1 MF Update (TD(1) style - direct reinforcement from reward)
        # Note: In hybrid models, MF part is often Sarsa(1).
        delta_mf = r - q_stage1_mf[a1]
        alpha_mf = alpha_pos if delta_mf > 0 else alpha_neg
        q_stage1_mf[a1] = q_stage1_mf[a1] + alpha_mf * delta_mf
        
        prev_action_1 = a1

    eps = 1e-10
    valid_mask = (p_choice_1 > 0) & (p_choice_2 > 0)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss
```