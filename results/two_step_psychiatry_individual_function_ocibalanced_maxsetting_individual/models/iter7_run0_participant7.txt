Here are the three proposed cognitive models.

### Model 1: Reward-Dependent Exploration (Win-Tighten, Lose-Loosen)
This model hypothesizes that the participant's exploration rate (inverse temperature $\beta_1$) in the first stage changes based on the previous trial's outcome. A win might lead to "tightening" the policy (higher $\beta$, exploitation), while a loss leads to "loosening" it (lower $\beta$, exploration). This captures a "Win-Exploit, Lose-Explore" dynamic distinct from simple stickiness.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Reward-Dependent Exploration Model.
    
    Modulates Stage 1 inverse temperature (beta) based on the previous trial's reward.
    Captures the tendency to exploit after wins and explore after losses.
    
    Parameters:
    - learning_rate: Rate at which Q-values are updated [0, 1]
    - beta_1_win: Inverse temperature for Stage 1 after a reward [0, 10]
    - beta_1_loss: Inverse temperature for Stage 1 after no reward [0, 10]
    - beta_2: Inverse temperature for Stage 2 [0, 10]
    - w: Mixing weight between Model-Based and Model-Free values [0, 1]
    - lam: Eligibility trace parameter [0, 1]
    - stick: Choice stickiness bonus for repeating the previous Stage 1 choice [0, 10]
    """
    learning_rate, beta_1_win, beta_1_loss, beta_2, w, lam, stick = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.full(2, 0.5)
    q_stage2_mf = np.full((2, 2), 0.5)

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Determine beta based on previous reward
        if trial > 0 and reward[trial-1] == 1:
            current_beta_1 = beta_1_win
        else:
            current_beta_1 = beta_1_loss
            
        logits_1 = current_beta_1 * q_net
        
        if trial > 0:
            logits_1[action_1[trial-1]] += stick

        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # policy for the second choice
        logits_2 = beta_2 * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Action value updating
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Eligibility trace update
        q_stage1_mf[action_1[trial]] += learning_rate * lam * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Transition-Dependent Learning Rates
This model applies different learning rates for the Model-Free Stage 1 update depending on whether the transition experienced was "common" or "rare". This tests the hypothesis that the participant might update their cached values differently when a surprising transition occurs (e.g., discounting rare transitions or over-weighting them).

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Transition-Dependent Learning Rate Model.
    
    Applies separate learning rates for Stage 1 MF updates depending on whether
    the transition was Common or Rare.
    
    Parameters:
    - lr_base: Base learning rate (used for Stage 2 and Common Stage 1 updates) [0, 1]
    - lr_rare: Learning rate for Stage 1 updates after a Rare transition [0, 1]
    - beta_1: Inverse temperature for Stage 1 [0, 10]
    - beta_2: Inverse temperature for Stage 2 [0, 10]
    - w: Mixing weight between Model-Based and Model-Free values [0, 1]
    - lam: Eligibility trace parameter [0, 1]
    - stick: Choice stickiness bonus [0, 10]
    """
    lr_base, lr_rare, beta_1, beta_2, w, lam, stick = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.full(2, 0.5)
    q_stage2_mf = np.full((2, 2), 0.5)

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta_1 * q_net
        if trial > 0:
            logits_1[action_1[trial-1]] += stick

        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # policy for the second choice
        logits_2 = beta_2 * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Determine if transition was common or rare
        # Common: Action 0 -> State 0, Action 1 -> State 1
        is_common = (action_1[trial] == state_idx)
        current_lr_s1 = lr_base if is_common else lr_rare

        # Action value updating
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += current_lr_s1 * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        # Use base learning rate for Stage 2 (bandit) updates
        q_stage2_mf[state_idx, action_2[trial]] += lr_base * delta_stage2
        
        # Eligibility trace update using the transition-specific rate
        q_stage1_mf[action_1[trial]] += current_lr_s1 * lam * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Dual Decay Model
This model extends the concept of passive decay to both stages of the task. It assumes that unchosen options in both the spaceship choice (Stage 1) and the alien choice (Stage 2) decay towards a neutral value (0.5) over time. This helps handle non-stationarity in the environment and potential forgetting of the Model-Free values for spaceships.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Dual Decay Model.
    
    Implements passive decay for unchosen options in BOTH Stage 1 and Stage 2.
    Allows the model to handle non-stationarity in both the spaceship preferences
    and alien reward probabilities.
    
    Parameters:
    - learning_rate: Rate at which Q-values are updated [0, 1]
    - beta_1: Inverse temperature for Stage 1 [0, 10]
    - beta_2: Inverse temperature for Stage 2 [0, 10]
    - w: Mixing weight between Model-Based and Model-Free values [0, 1]
    - lam: Eligibility trace parameter [0, 1]
    - decay_1: Decay rate for unchosen Stage 1 MF values [0, 1]
    - decay_2: Decay rate for unchosen Stage 2 values [0, 1]
    - stick: Choice stickiness bonus [0, 10]
    """
    learning_rate, beta_1, beta_2, w, lam, decay_1, decay_2, stick = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.full(2, 0.5)
    q_stage2_mf = np.full((2, 2), 0.5)

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta_1 * q_net
        if trial > 0:
            logits_1[action_1[trial-1]] += stick

        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # policy for the second choice
        logits_2 = beta_2 * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Action value updating
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        q_stage1_mf[action_1[trial]] += learning_rate * lam * delta_stage2
        
        # Decay Stage 1 unchosen options
        for a in range(2):
            if a != action_1[trial]:
                q_stage1_mf[a] = (1 - decay_1) * q_stage1_mf[a] + decay_1 * 0.5
                
        # Decay Stage 2 unchosen options
        for s in range(2):
            for a in range(2):
                if not (s == state_idx and a == action_2[trial]):
                    q_stage2_mf[s, a] = (1 - decay_2) * q_stage2_mf[s, a] + decay_2 * 0.5

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```