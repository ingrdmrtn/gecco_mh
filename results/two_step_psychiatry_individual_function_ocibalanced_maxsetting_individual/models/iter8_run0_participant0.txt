Here are the 3 new cognitive models based on the participant data and the provided template.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free RL with Reward-Dependent Perseveration, Eligibility Trace, and Value Decay.
    
    This model extends the standard MF model by splitting the perseveration parameter 
    into 'pers_win' and 'pers_loss'. This allows the model to capture the participant's 
    tendency to stick with a choice differently depending on the previous outcome 
    (Win-Stay vs Lose-Stay), which is relevant given the participant's long streaks 
    even after losses.
    
    Parameters:
    - alpha_1: [0, 1] Learning rate for Stage 1.
    - alpha_2: [0, 1] Learning rate for Stage 2.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - lambda_eligibility: [0, 1] Eligibility trace decay, linking Stage 2 error to Stage 1.
    - decay: [0, 1] Decay rate for unchosen Stage 2 values (forgetting).
    - pers_win: [0, 10] Perseveration bonus applied if the previous trial was a win.
    - pers_loss: [0, 10] Perseveration bonus applied if the previous trial was a loss.
    """
    alpha_1, alpha_2, beta_1, beta_2, lambda_eligibility, decay, pers_win, pers_loss = model_parameters
    n_trials = len(action_1)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    last_reward = 0
    eps = 1e-10

    for trial in range(n_trials):
        # Policy for the first choice
        logits_1 = beta_1 * q_stage1_mf
        
        # Apply reward-dependent perseveration
        if last_action_1 != -1:
            bonus = pers_win if last_reward == 1 else pers_loss
            logits_1[last_action_1] += bonus
            
        probs_1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 /= np.sum(probs_1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = int(action_1[trial])
        s2 = int(state[trial])

        # Policy for the second choice
        logits_2 = beta_2 * q_stage2_mf[s2]
        probs_2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 /= np.sum(probs_2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = int(action_2[trial])
        r = int(reward[trial])
        
        # Store pre-update Q-value for Stage 1 update calculation
        q2_pre = q_stage2_mf[s2, a2]
        
        # Stage 2 Update (TD Error)
        delta_stage2 = r - q2_pre
        q_stage2_mf[s2, a2] += alpha_2 * delta_stage2
        
        # Decay unchosen Stage 2 option to handle non-stationarity
        unchosen_a2 = 1 - a2
        q_stage2_mf[s2, unchosen_a2] *= (1.0 - decay)
        
        # Stage 1 Update (TD + Eligibility)
        delta_stage1 = q2_pre - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha_1 * (delta_stage1 + lambda_eligibility * delta_stage2)
        
        last_action_1 = a1
        last_reward = r

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model (MF + MB) with Value Decay and Simple Perseveration (No Eligibility).
    
    This model tests if a Model-Based component combined with Value Decay is sufficient 
    to explain behavior without an eligibility trace. The decay on Stage 2 values ensures 
    the MB system relies on fresh estimates of the changing alien reward probabilities.
    
    Parameters:
    - alpha_1: [0, 1] MF Learning rate for Stage 1.
    - alpha_2: [0, 1] Learning rate for Stage 2.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - w_mb: [0, 1] Weight of Model-Based control (0=Pure MF, 1=Pure MB).
    - decay: [0, 1] Decay rate for unchosen Stage 2 values.
    - perseveration: [0, 10] Stickiness bonus.
    """
    alpha_1, alpha_2, beta_1, beta_2, w_mb, decay, perseveration = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    eps = 1e-10

    for trial in range(n_trials):
        # Calculate Model-Based values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Calculate Net Q-values (Hybrid)
        q_net = w_mb * q_stage1_mb + (1 - w_mb) * q_stage1_mf
        
        # Policy for the first choice
        logits_1 = beta_1 * q_net
        if last_action_1 != -1:
            logits_1[last_action_1] += perseveration
            
        probs_1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 /= np.sum(probs_1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = int(action_1[trial])
        s2 = int(state[trial])

        # Policy for the second choice
        logits_2 = beta_2 * q_stage2_mf[s2]
        probs_2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 /= np.sum(probs_2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = int(action_2[trial])
        r = int(reward[trial])
        
        # Updates
        delta_stage2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += alpha_2 * delta_stage2
        
        # Decay unchosen Stage 2 option
        unchosen_a2 = 1 - a2
        q_stage2_mf[s2, unchosen_a2] *= (1.0 - decay)
        
        # Pure MF Update for Stage 1 (No eligibility trace)
        # Note: Standard Hybrid often uses pure MF for the MF component
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1] # Using updated Q2 is common in SARSA-like
        q_stage1_mf[a1] += alpha_1 * delta_stage1
        
        last_action_1 = a1

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free RL with Choice Kernel, Value Decay, and Separate Alphas (No Eligibility).
    
    Replaces simple perseveration with a Choice Kernel (CK) that accumulates choice history.
    This helps explain the long streaks (habits) observed in the participant data.
    The eligibility trace is removed to isolate the effect of the Choice Kernel mechanism.
    
    Parameters:
    - alpha_1: [0, 1] Learning rate for Stage 1.
    - alpha_2: [0, 1] Learning rate for Stage 2.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - decay: [0, 1] Decay rate for unchosen Stage 2 values.
    - ck_decay: [0, 1] Decay rate for the Choice Kernel (forgetting rate of habits).
    - ck_weight: [0, 10] Weight of the Choice Kernel in Stage 1 decision.
    """
    alpha_1, alpha_2, beta_1, beta_2, decay, ck_decay, ck_weight = model_parameters
    n_trials = len(action_1)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    ck = np.zeros(2) # Choice Kernel
    
    eps = 1e-10

    for trial in range(n_trials):
        # Policy for the first choice
        # Combine Q-values and Choice Kernel
        logits_1 = beta_1 * q_stage1_mf + ck_weight * ck
        
        probs_1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 /= np.sum(probs_1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = int(action_1[trial])
        s2 = int(state[trial])

        # Policy for the second choice
        logits_2 = beta_2 * q_stage2_mf[s2]
        probs_2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 /= np.sum(probs_2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = int(action_2[trial])
        r = int(reward[trial])
        
        # Choice Kernel Update
        # Decay all, then reinforce chosen
        ck *= (1.0 - ck_decay)
        ck[a1] += 1.0
        
        # Value Updates
        delta_stage2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += alpha_2 * delta_stage2
        
        # Decay unchosen Stage 2 option
        unchosen_a2 = 1 - a2
        q_stage2_mf[s2, unchosen_a2] *= (1.0 - decay)
        
        # Stage 1 Update (Simple TD, no eligibility)
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha_1 * delta_stage1

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```