Here are the three proposed cognitive models.

### Cognitive Model 1: TD($\lambda$) with Outcome-Dependent Perseveration
This model extends the best-performing TD($\lambda$) agent by distinguishing between "Win-Stay" and "Lose-Stay" behaviors. It posits that the participant's tendency to repeat a Stage 1 choice depends on whether the previous trial resulted in a reward or not, allowing for asymmetric stickiness (e.g., sticking to a choice even after a loss, or sticking more strongly after a win).

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    TD(lambda) with Outcome-Dependent Perseveration.
    
    This model uses Eligibility Traces to update Stage 1 values based on Stage 2 outcomes.
    It applies different perseveration bonuses depending on whether the previous trial
    was rewarded (Win-Stay) or unrewarded (Lose-Stay/Switch).
    
    Parameters:
    lr:          [0, 1] Learning rate for value updates.
    beta:        [0, 10] Inverse temperature (softmax sensitivity).
    lambda_param:[0, 1] Eligibility trace decay (0=TD(0), 1=Monte Carlo).
    persev_win:  [0, 5] Bonus added to previous Stage 1 action if Reward=1.
    persev_loss: [0, 5] Bonus added to previous Stage 1 action if Reward=0.
    """
    lr, beta, lambda_param, persev_win, persev_loss = model_parameters
    n_trials = len(action_1)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Q-values
    q_stage1 = np.zeros(2)      
    q_stage2 = np.zeros((2, 2)) # (State, Action)
    
    prev_action_1 = -1
    prev_reward = -1

    for trial in range(n_trials):
        # Handle missing data
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 0.5 
            p_choice_2[trial] = 0.5 
            prev_action_1 = -1 # Reset persev on missing data
            prev_reward = -1
            continue

        # Stage 1 Policy
        q_stage1_eff = q_stage1.copy()
        if prev_action_1 != -1 and prev_reward != -1:
            if prev_reward == 1:
                q_stage1_eff[prev_action_1] += persev_win
            else:
                q_stage1_eff[prev_action_1] += persev_loss

        exp_q1 = np.exp(beta * q_stage1_eff)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # Updates
        # TD Errors
        delta_stage1 = q_stage2[state_idx, action_2[trial]] - q_stage1[action_1[trial]]
        delta_stage2 = reward[trial] - q_stage2[state_idx, action_2[trial]]

        # Update Stage 2 (TD(0))
        q_stage2[state_idx, action_2[trial]] += lr * delta_stage2

        # Update Stage 1 (TD(lambda))
        combined_delta = delta_stage1 + lambda_param * delta_stage2
        q_stage1[action_1[trial]] += lr * combined_delta
        
        # Store history
        prev_action_1 = action_1[trial]
        prev_reward = reward[trial]

    eps = 1e-10
    valid_trials = (action_1 != -1) & (state != -1) & (action_2 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```

### Cognitive Model 2: TD($\lambda$) with Memory Decay
This model modifies the standard TD($\lambda$) learner by incorporating a forgetting mechanism. In dynamic environments, or when a participant's behavior exhibits switching after long blocks, it may be because the value of unchosen options decays over time (or uncertainty increases). Here, unchosen Q-values decay towards zero, which helps explain shifts in preference that aren't solely driven by negative prediction errors.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    TD(lambda) with Memory Decay.
    
    Standard TD(lambda) learning augmented with a decay parameter. 
    Unchosen action values in Stage 2 decay towards 0 on each trial, 
    simulating forgetting or a 'use-it-or-lose-it' heuristic.
    
    Parameters:
    lr:            [0, 1] Learning rate.
    beta:          [0, 10] Inverse temperature.
    lambda_param:  [0, 1] Eligibility trace decay.
    perseveration: [0, 5] Choice stickiness bonus.
    decay:         [0, 1] Decay rate for unchosen Stage 2 values (0 = no decay).
    """
    lr, beta, lambda_param, perseveration, decay = model_parameters
    n_trials = len(action_1)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1 = np.zeros(2)      
    q_stage2 = np.zeros((2, 2)) 
    
    prev_action_1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 0.5 
            p_choice_2[trial] = 0.5 
            continue

        # Stage 1 Policy
        q_stage1_eff = q_stage1.copy()
        if prev_action_1 != -1:
            q_stage1_eff[prev_action_1] += perseveration

        exp_q1 = np.exp(beta * q_stage1_eff)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # Updates
        delta_stage1 = q_stage2[state_idx, action_2[trial]] - q_stage1[action_1[trial]]
        delta_stage2 = reward[trial] - q_stage2[state_idx, action_2[trial]]

        # Update Stage 2 Chosen
        q_stage2[state_idx, action_2[trial]] += lr * delta_stage2
        
        # Decay Stage 2 Unchosen
        # Decay the unchosen action in the current state
        unchosen_a2 = 1 - action_2[trial]
        q_stage2[state_idx, unchosen_a2] *= (1 - decay)
        # Decay both actions in the unvisited state
        unchosen_state = 1 - state_idx
        q_stage2[unchosen_state, :] *= (1 - decay)

        # Update Stage 1
        combined_delta = delta_stage1 + lambda_param * delta_stage2
        q_stage1[action_1[trial]] += lr * combined_delta
        
        prev_action_1 = action_1[trial]

    eps = 1e-10
    valid_trials = (action_1 != -1) & (state != -1) & (action_2 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```

### Cognitive Model 3: Pure Model-Based with Transition Learning
This model tests the hypothesis that the participant behaves as a Model-Based agent who is actively learning the transition structure of the task (the probability of Spaceship A going to Planet X vs Y). Unlike the standard model which assumes fixed transition probabilities (0.7/0.3), this agent updates its belief about transitions based on observed outcomes, using these dynamic beliefs to compute Stage 1 values via the Bellman equation.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Based Reinforcement Learning with Dynamic Transition Learning.
    
    The agent maintains a transition probability matrix which is updated after
    every trial. Stage 1 values are computed on-the-fly using the Bellman equation
    (Model-Based), rather than cached (Model-Free).
    
    Parameters:
    lr_val:   [0, 1] Learning rate for Stage 2 (Alien) values.
    lr_trans: [0, 1] Learning rate for Transition probabilities.
    beta:     [0, 10] Inverse temperature.
    perseveration: [0, 5] Choice stickiness bonus for Stage 1.
    """
    lr_val, lr_trans, beta, perseveration = model_parameters
    n_trials = len(action_1)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Stage 2 values (Aliens)
    q_stage2 = np.zeros((2, 2)) 
    
    # Transition Probabilities: T[spaceship, planet]
    # Initialize with the common bias described in instructions (0.7 / 0.3)
    trans_probs = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    prev_action_1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 0.5 
            p_choice_2[trial] = 0.5 
            continue

        # Model-Based Stage 1 Value Calculation
        # Q_MB(s1, a) = Sum_s2 [ T(s1, a, s2) * max_a2 Q_stage2(s2, a2) ]
        max_q_stage2 = np.max(q_stage2, axis=1) # Max value of each planet
        q_stage1_mb = trans_probs @ max_q_stage2 # Expected value for each spaceship
        
        # Apply Perseveration
        q_stage1_eff = q_stage1_mb.copy()
        if prev_action_1 != -1:
            q_stage1_eff[prev_action_1] += perseveration

        exp_q1 = np.exp(beta * q_stage1_eff)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # Update Stage 2 Values (Model-Free update for the leaves)
        delta_stage2 = reward[trial] - q_stage2[state_idx, action_2[trial]]
        q_stage2[state_idx, action_2[trial]] += lr_val * delta_stage2
        
        # Update Transition Model
        # Increase prob of observed transition, decrease others
        # p(s'|s,a) <- p(s'|s,a) + alpha * (1 - p(s'|s,a))
        chosen_ship = action_1[trial]
        observed_planet = state[trial]
        
        trans_probs[chosen_ship, observed_planet] += lr_trans * (1 - trans_probs[chosen_ship, observed_planet])
        # Normalize (ensure sum is 1)
        trans_probs[chosen_ship, 1 - observed_planet] = 1.0 - trans_probs[chosen_ship, observed_planet]
        
        prev_action_1 = action_1[trial]

    eps = 1e-10
    valid_trials = (action_1 != -1) & (state != -1) & (action_2 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```