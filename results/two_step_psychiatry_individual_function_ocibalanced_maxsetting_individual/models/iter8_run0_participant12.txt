Here are three new cognitive models for the two-step task, expressed as Python functions.

### Model 1: Reference Point MB/MF Model
This model extends the standard hybrid learner by assuming that the participant evaluates rewards not in absolute terms (0 or 1), but relative to a dynamic "reference point" (average reward rate). If the reference point is high, a reward of 0 is perceived as a loss (negative prediction error). If low, 0 might be neutral. This captures context-dependent valuation.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Reference Point MB/MF Model.
    
    This model assumes that the participant evaluates rewards relative to a running 
    average of recent rewards (a reference point), rather than absolute values. 
    The effective reward is R_eff = Reward - AverageReward.
    This captures context-dependent valuation (e.g., 0 is a loss if average is high).
    
    Parameters:
    lr (float): Learning rate for Q-values [0, 1].
    beta (float): Inverse temperature [0, 10].
    w (float): MB/MF mixing weight [0, 1].
    lam (float): Eligibility trace parameter [0, 1].
    stick (float): Choice stickiness [0, 5].
    lr_avg (float): Learning rate for the average reward reference point [0, 1].
    """
    lr, beta, w, lam, stick, lr_avg = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    avg_reward = 0.5 # Initialize neutral
    
    last_action_1 = -1
    log_loss = 0
    eps = 1e-10
    
    for trial in range(n_trials):
        # --- Stage 1 ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        logits_1 = beta * q_net_1
        
        if last_action_1 != -1:
            logits_1[last_action_1] += stick
            
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = action_1[trial]
        if a1 != -1:
            log_loss -= np.log(probs_1[a1] + eps)
            last_action_1 = a1
        
        # --- Stage 2 ---
        s_idx = state[trial]
        if s_idx == -1: continue 
        
        logits_2 = beta * q_stage2_mf[s_idx]
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = action_2[trial]
        if a2 != -1:
            log_loss -= np.log(probs_2[a2] + eps)
            
            # --- Updates ---
            r = reward[trial]
            if r == -1: r = 0 
            
            # Calculate effective reward relative to reference point
            r_eff = r - avg_reward
            
            # Update Reference Point
            avg_reward += lr_avg * (r - avg_reward)
            
            # MB/MF Updates using effective reward
            delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
            delta_stage2 = r_eff - q_stage2_mf[s_idx, a2]
            
            q_stage1_mf[a1] += lr * delta_stage1 + lr * lam * delta_stage2
            q_stage2_mf[s_idx, a2] += lr * delta_stage2
            
    return log_loss
```

### Model 2: Choice Kernel MB/MF Model
This model replaces the simple 1-back stickiness parameter with a "Choice Kernel." The kernel tracks an exponentially moving average of past choices, allowing for the accumulation of habit strength over multiple trials. This can explain the long runs of identical choices seen in the participant data better than simple stickiness.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Choice Kernel MB/MF Model.
    
    Replaces simple 1-back stickiness with a 'Choice Kernel' that tracks the 
    history of choices. This captures stronger habit formation or perseveration 
    that builds up over repeated choices and decays slowly.
    Applies to both Stage 1 and Stage 2 choices.
    
    Parameters:
    lr (float): Learning rate [0, 1].
    beta (float): Inverse temperature [0, 10].
    w (float): MB/MF mixing weight [0, 1].
    lam (float): Eligibility trace [0, 1].
    lr_k (float): Learning/Decay rate for the choice kernel [0, 1].
    w_k (float): Weight of the choice kernel in decision making [0, 5].
    """
    lr, beta, w, lam, lr_k, w_k = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Choice kernels: smoothed history of choices
    kernel_s1 = np.zeros(2)
    kernel_s2 = np.zeros((2, 2)) # One per state
    
    log_loss = 0
    eps = 1e-10
    
    for trial in range(n_trials):
        # --- Stage 1 ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        # Add kernel bias
        logits_1 = beta * q_net_1 + w_k * kernel_s1
        
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = action_1[trial]
        if a1 != -1:
            log_loss -= np.log(probs_1[a1] + eps)
            # Update Kernel 1
            # Increase chosen, decay unchosen
            kernel_s1[a1] += lr_k * (1 - kernel_s1[a1])
            kernel_s1[1-a1] += lr_k * (0 - kernel_s1[1-a1])
        
        # --- Stage 2 ---
        s_idx = state[trial]
        if s_idx == -1: continue

        logits_2 = beta * q_stage2_mf[s_idx] + w_k * kernel_s2[s_idx]
        
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = action_2[trial]
        if a2 != -1:
            log_loss -= np.log(probs_2[a2] + eps)
            
            # Update Kernel 2
            kernel_s2[s_idx, a2] += lr_k * (1 - kernel_s2[s_idx, a2])
            kernel_s2[s_idx, 1-a2] += lr_k * (0 - kernel_s2[s_idx, 1-a2])
            
            # --- RL Updates ---
            r = reward[trial]
            if r == -1: r = 0
            
            delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
            delta_stage2 = r - q_stage2_mf[s_idx, a2]
            
            q_stage1_mf[a1] += lr * delta_stage1 + lr * lam * delta_stage2
            q_stage2_mf[s_idx, a2] += lr * delta_stage2
            
    return log_loss
```

### Model 3: Stage 1 Counterfactual MB/MF Model
This model applies the counterfactual updating logic specifically to the **Stage 1 (Spaceship)** choice. It hypothesizes that the participant views the two spaceships as anti-correlated; if the chosen spaceship yields a reward, they infer that the unchosen spaceship would have yielded no reward (and vice versa). This updates the Model-Free value of the unchosen spaceship towards the opposite outcome.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Stage 1 Counterfactual MB/MF Model.
    
    Assumes the participant believes the two spaceships are anti-correlated.
    When updating the chosen spaceship's value based on outcome, the unchosen 
    spaceship's value is updated towards the opposite outcome (1-R).
    This applies specifically to the Stage 1 Model-Free values.
    
    Parameters:
    lr (float): Learning rate [0, 1].
    beta (float): Inverse temperature [0, 10].
    w (float): MB/MF mixing weight [0, 1].
    lam (float): Eligibility trace [0, 1].
    stick (float): Stickiness [0, 5].
    lr_cf_s1 (float): Counterfactual learning rate for Stage 1 unchosen option [0, 1].
    """
    lr, beta, w, lam, stick, lr_cf_s1 = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    log_loss = 0
    eps = 1e-10
    
    for trial in range(n_trials):
        # --- Stage 1 ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        logits_1 = beta * q_net_1
        
        if last_action_1 != -1:
            logits_1[last_action_1] += stick
            
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = action_1[trial]
        if a1 != -1:
            log_loss -= np.log(probs_1[a1] + eps)
            last_action_1 = a1
        
        # --- Stage 2 ---
        s_idx = state[trial]
        if s_idx == -1: continue

        logits_2 = beta * q_stage2_mf[s_idx]
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = action_2[trial]
        if a2 != -1:
            log_loss -= np.log(probs_2[a2] + eps)
            
            # --- Updates ---
            r = reward[trial]
            if r == -1: r = 0
            
            # Standard updates
            delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
            delta_stage2 = r - q_stage2_mf[s_idx, a2]
            
            q_stage1_mf[a1] += lr * delta_stage1 + lr * lam * delta_stage2
            q_stage2_mf[s_idx, a2] += lr * delta_stage2
            
            # Counterfactual Update for Stage 1 Unchosen
            # If I got R on a1, assume I would have got 1-R on (1-a1)
            unchosen_a1 = 1 - a1
            # The 'prediction error' for unchosen is (1-r) - Q(unchosen)
            delta_cf = (1.0 - r) - q_stage1_mf[unchosen_a1]
            q_stage1_mf[unchosen_a1] += lr_cf_s1 * delta_cf
            
    return log_loss
```