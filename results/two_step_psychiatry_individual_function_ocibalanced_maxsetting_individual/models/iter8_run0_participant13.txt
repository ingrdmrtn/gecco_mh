Here are the 3 proposed cognitive models as Python functions. They are designed to capture the participant's behavior using variations of the Model-Based/Model-Free hybrid reinforcement learning framework, incorporating mechanisms like outcome-dependent noise, asymmetric learning, and stage-specific learning rates.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Model 1: Hybrid Outcome-Dependent Beta Controller.
    
    This model combines Model-Based (MB) and Model-Free (MF) reinforcement learning strategies.
    Crucially, it uses an outcome-dependent inverse temperature (beta), meaning the
    randomness of the participant's choices changes depending on whether the previous
    trial was rewarded or not. This captures the "win-stay, lose-shift" dynamic
    modulated by model-based planning.

    Parameters:
    learning_rate: [0, 1] - Learning rate for updating value estimates.
    beta_win: [0, 10] - Inverse temperature (exploration/exploitation) after a reward.
    beta_loss: [0, 10] - Inverse temperature after no reward.
    w: [0, 1] - Weight parameter mixing MB (1) and MF (0) values.
    perseveration: [-3, 3] - Stickiness to the previous Stage 1 choice.
    """
    learning_rate, beta_win, beta_loss, w, perseveration = model_parameters
    n_trials = len(action_1)
    
    # Initialize Q-values
    # Stage 1: 2 actions (Spaceships)
    q_stage1_mf = np.zeros(2) + 0.5
    # Stage 2: 2 states (Planets), 2 actions (Aliens) per state
    q_stage2_mf = np.zeros((2, 2)) + 0.5
    
    # Transition matrix for Model-Based calculation: 
    # State 0 is reached by Action 0 with p=0.7, Action 1 with p=0.3
    # State 1 is reached by Action 0 with p=0.3, Action 1 with p=0.7
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1
    prev_reward = 0 # Default to assuming a 'loss' state or neutral start
    
    for t in range(n_trials):
        # --- Stage 1 Decision ---
        
        # 1. Calculate Model-Based values for Stage 1
        # Max value achievable in each state (greedy over aliens)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        # Expected value of each spaceship based on transition probs
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Combine MB and MF values
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # 3. Select Beta based on previous outcome
        current_beta = beta_win if prev_reward == 1 else beta_loss
        
        # 4. Calculate probabilities (Softmax with Perseveration)
        logits_1 = current_beta * q_net_stage1
        if prev_a1 != -1:
            logits_1[prev_a1] += perseveration
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        p_choice_1[t] = probs_1[action_1[t]]
        
        # Record choice and state
        a1 = action_1[t]
        s_idx = state[t]
        
        # --- Stage 2 Decision ---
        
        # Standard Softmax on Stage 2 Q-values using the same beta
        logits_2 = current_beta * q_stage2_mf[s_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        p_choice_2[t] = probs_2[action_2[t]]
        
        a2 = action_2[t]
        r = reward[t]
        
        # --- Value Updates (Model-Free) ---
        
        # TD(0) update for Stage 2 (Alien value)
        delta_2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_2
        
        # TD(1) / SARSA-like update for Stage 1 (Spaceship value)
        # Using the value of the chosen Stage 2 action as the target
        delta_1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_1
        
        # Update history
        prev_a1 = a1
        prev_reward = r

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model 2: Hybrid Asymmetric Learning Controller.
    
    This model posits that the participant learns differently from positive prediction errors
    (better than expected) versus negative prediction errors (worse than expected).
    This asymmetry is applied within a Hybrid MB/MF framework.
    
    Parameters:
    alpha_pos: [0, 1] - Learning rate for positive prediction errors.
    alpha_neg: [0, 1] - Learning rate for negative prediction errors.
    beta: [0, 10] - Inverse temperature for action selection.
    w: [0, 1] - Weight parameter mixing MB (1) and MF (0) values.
    perseveration: [-3, 3] - Stickiness to the previous Stage 1 choice.
    """
    alpha_pos, alpha_neg, beta, w, perseveration = model_parameters
    n_trials = len(action_1)
    
    q_stage1_mf = np.zeros(2) + 0.5
    q_stage2_mf = np.zeros((2, 2)) + 0.5
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1
    
    for t in range(n_trials):
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net_stage1
        if prev_a1 != -1:
            logits_1[prev_a1] += perseveration
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[t] = probs_1[action_1[t]]
        
        a1 = action_1[t]
        s_idx = state[t]
        
        # --- Stage 2 Decision ---
        logits_2 = beta * q_stage2_mf[s_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[t] = probs_2[action_2[t]]
        
        a2 = action_2[t]
        r = reward[t]
        
        # --- Asymmetric Value Updates ---
        
        # Stage 2 Update
        delta_2 = r - q_stage2_mf[s_idx, a2]
        lr_2 = alpha_pos if delta_2 > 0 else alpha_neg
        q_stage2_mf[s_idx, a2] += lr_2 * delta_2
        
        # Stage 1 Update
        delta_1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        lr_1 = alpha_pos if delta_1 > 0 else alpha_neg
        q_stage1_mf[a1] += lr_1 * delta_1
        
        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model 3: Hybrid Dual Learning Rate Controller.
    
    This model separates the learning process into two distinct timescales:
    one for the first stage (Spaceship preference) and one for the second stage (Alien reward).
    This allows the agent to update their preferences for actions at different speeds depending
    on the stage of the task, integrated within a Hybrid MB/MF structure.
    
    Parameters:
    lr_stage1: [0, 1] - Learning rate for Stage 1 (Spaceships).
    lr_stage2: [0, 1] - Learning rate for Stage 2 (Aliens).
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - Weight parameter mixing MB (1) and MF (0) values.
    perseveration: [-3, 3] - Stickiness to the previous Stage 1 choice.
    """
    lr_stage1, lr_stage2, beta, w, perseveration = model_parameters
    n_trials = len(action_1)
    
    q_stage1_mf = np.zeros(2) + 0.5
    q_stage2_mf = np.zeros((2, 2)) + 0.5
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1
    
    for t in range(n_trials):
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net_stage1
        if prev_a1 != -1:
            logits_1[prev_a1] += perseveration
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[t] = probs_1[action_1[t]]
        
        a1 = action_1[t]
        s_idx = state[t]
        
        # --- Stage 2 Decision ---
        logits_2 = beta * q_stage2_mf[s_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[t] = probs_2[action_2[t]]
        
        a2 = action_2[t]
        r = reward[t]
        
        # --- Dual Rate Value Updates ---
        
        # Stage 2 Update (Alien)
        delta_2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr_stage2 * delta_2
        
        # Stage 1 Update (Spaceship)
        # Note: We use the updated Q2 value to drive Q1 update, or the reward directly?
        # Standard in this task is using Q2.
        delta_1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr_stage1 * delta_1
        
        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```