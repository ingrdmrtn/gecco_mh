Here are three new cognitive models for the two-step task, designed to capture specific behavioral nuances like outcome-dependent strategy switching, transition-dependent credit assignment, and loss aversion.

### Model 1: Outcome-Dependent Model-Based Weighting
This model hypothesizes that the participant shifts their reliance between Model-Based (planning) and Model-Free (habit) systems depending on the previous trial's outcome. For instance, a win might encourage sticking to habits (MF), while a loss might trigger a re-evaluation using the internal model (MB).

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF model with Outcome-Dependent Model-Based Weighting.
    
    The weight of the Model-Based system (w) is dynamic and depends on the 
    outcome of the previous trial. This allows the model to capture strategy 
    switching (e.g., relying more on planning after a loss).
    
    Parameters:
    - lr: [0, 1] Learning rate for Q-values.
    - beta: [0, 10] Inverse temperature (softmax).
    - w_win: [0, 1] Model-based weight applied after a rewarded trial (win).
    - w_loss: [0, 1] Model-based weight applied after an unrewarded trial (loss).
    - lam: [0, 1] Eligibility trace decay parameter.
    - stickiness: [0, 5] Choice perseveration bonus.
    """
    lr, beta, w_win, w_loss, lam, stickiness = model_parameters
    n_trials = len(action_1)
    
    # Transition matrix: 0->0 (70%), 0->1 (30%), 1->0 (30%), 1->1 (70%)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_mf = np.zeros(2)
    q2 = np.zeros((2, 2)) # State x Action
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1
    last_reward = 1 # Initialize assuming a win or neutral start
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])
        
        if a1 == -1:
            continue
            
        # Stage 1 Policy
        max_q2 = np.max(q2, axis=1)
        q_mb = transition_matrix @ max_q2
        
        # Select w based on the previous trial's reward
        w = w_win if last_reward == 1 else w_loss
        
        q_net = w * q_mb + (1 - w) * q_mf
        
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Stage 2 Policy
        exp_q2 = np.exp(beta * q2[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # Updates
        # Stage 1 RPE (TD(0))
        delta_1 = q2[s, a2] - q_mf[a1]
        q_mf[a1] += lr * delta_1
        
        # Stage 2 RPE
        delta_2 = r - q2[s, a2]
        q2[s, a2] += lr * delta_2
        
        # Eligibility trace to Stage 1
        q_mf[a1] += lr * lam * delta_2
        
        last_action_1 = a1
        last_reward = r

    eps = 1e-10
    mask = p_choice_1 > 0
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss
```

### Model 2: Transition-Dependent Eligibility Traces
This model proposes that the "credit assignment" from the final outcome back to the initial choice is modulated by the type of transition (Common vs. Rare). A rare transition might be perceived as a "glitch" or "luck," disrupting the eligibility trace ($\lambda$) and preventing the Model-Free system from reinforcing the spaceship choice.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF model with Transition-Dependent Eligibility Traces.
    
    The eligibility trace parameter (lambda) varies depending on whether the 
    observed transition was Common (expected) or Rare (unexpected). This 
    modulates how much credit the Stage 1 choice receives from the Stage 2 outcome.
    
    Parameters:
    - lr: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Model-based weight.
    - lam_common: [0, 1] Eligibility trace decay for common transitions.
    - lam_rare: [0, 1] Eligibility trace decay for rare transitions.
    - stickiness: [0, 5] Choice perseveration bonus.
    """
    lr, beta, w, lam_common, lam_rare, stickiness = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_mf = np.zeros(2)
    q2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])
        
        if a1 == -1:
            continue
            
        # Stage 1 Policy
        max_q2 = np.max(q2, axis=1)
        q_mb = transition_matrix @ max_q2
        q_net = w * q_mb + (1 - w) * q_mf
        
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Stage 2 Policy
        exp_q2 = np.exp(beta * q2[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # Determine if transition was common
        # 0->0 and 1->1 are common (0.7); 0->1 and 1->0 are rare (0.3)
        is_common = (a1 == s)
        current_lam = lam_common if is_common else lam_rare
        
        # Updates
        delta_1 = q2[s, a2] - q_mf[a1]
        q_mf[a1] += lr * delta_1
        
        delta_2 = r - q2[s, a2]
        q2[s, a2] += lr * delta_2
        
        # Eligibility trace update using transition-specific lambda
        q_mf[a1] += lr * current_lam * delta_2
        
        last_action_1 = a1

    eps = 1e-10
    mask = p_choice_1 > 0
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss
```

### Model 3: Loss Aversion (Subjective Reward Utility)
This model introduces the concept of subjective utility for outcomes. Instead of treating 0 coins as a neutral value (0), the participant may perceive it as a painful loss. The `loss_mag` parameter quantifies this aversion, treating 0 reward as a negative value (`-loss_mag`), which fundamentally alters the convergence points of the Q-values.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF model with Loss Aversion (Subjective Reward Utility).
    
    The model treats the absence of reward (0 coins) as a loss with negative utility,
    controlled by the 'loss_mag' parameter.
    Reward 1 has utility 1. Reward 0 has utility -loss_mag.
    
    Parameters:
    - lr: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Model-based weight.
    - lam: [0, 1] Eligibility trace.
    - stickiness: [0, 5] Choice perseveration bonus.
    - loss_mag: [0, 5] Magnitude of subjective loss for 0 reward (treated as -loss_mag).
    """
    lr, beta, w, lam, stickiness, loss_mag = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_mf = np.zeros(2)
    q2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r_raw = int(reward[trial])
        
        if a1 == -1:
            continue
            
        # Subjective utility of reward: 1 is +1, 0 is -loss_mag
        r_eff = 1.0 if r_raw == 1 else -loss_mag
            
        # Stage 1 Policy
        max_q2 = np.max(q2, axis=1)
        q_mb = transition_matrix @ max_q2
        q_net = w * q_mb + (1 - w) * q_mf
        
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Stage 2 Policy
        exp_q2 = np.exp(beta * q2[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # Updates
        delta_1 = q2[s, a2] - q_mf[a1]
        q_mf[a1] += lr * delta_1
        
        # Update Q2 using subjective utility
        delta_2 = r_eff - q2[s, a2]
        q2[s, a2] += lr * delta_2
        
        # Update MF using subjective utility error
        q_mf[a1] += lr * lam * delta_2
        
        last_action_1 = a1

    eps = 1e-10
    mask = p_choice_1 > 0
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss
```