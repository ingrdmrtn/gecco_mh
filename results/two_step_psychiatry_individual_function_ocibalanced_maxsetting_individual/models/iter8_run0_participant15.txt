Here are three cognitive models formulated as Python functions.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF with Counterfactual Updating for Stage 2.
    
    This model assumes the participant believes in an anticorrelation structure 
    between the aliens on a planet (e.g., if one is bad, the other is good). 
    It updates the chosen alien based on the reward received, and the unchosen 
    alien based on the counterfactual outcome (1 - reward).
    
    Parameters:
    - alpha: [0, 1] Learning rate for chosen options (Stage 1 and 2).
    - alpha_cf: [0, 1] Counterfactual learning rate for the unchosen alien.
    - beta_mb: [0, 10] Weight for Model-Based values in Stage 1.
    - beta_mf: [0, 10] Weight for Model-Free values in Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2 choices.
    - stickiness: [0, 10] Perseverance bonus for repeated Stage 1 choices.
    """
    alpha, alpha_cf, beta_mb, beta_mf, beta_2, stickiness = model_parameters
    n_trials = len(action_1)
    
    # Fixed transition belief: A(0)->X(0) 0.7, U(1)->Y(1) 0.7
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_mf1 = np.zeros(2)
    q_mf2 = np.zeros((2, 2)) + 0.5 # Initialize at chance
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1
    
    for t in range(n_trials):
        # Stage 1 Choice
        # MB Value: Expected max stage 2 value
        max_q2 = np.max(q_mf2, axis=1)
        q_mb1 = transition_matrix @ max_q2
        
        # Net logits combining MB, MF, and stickiness
        logits_1 = beta_mb * q_mb1 + beta_mf * q_mf1
        
        if last_action_1 != -1:
            logits_1[last_action_1] += stickiness
            
        # Softmax Stage 1
        logits_1 = logits_1 - np.max(logits_1) # Stability
        exp_1 = np.exp(logits_1)
        probs_1 = exp_1 / np.sum(exp_1)
        
        if action_1[t] != -1:
            p_choice_1[t] = probs_1[action_1[t]]
            a1 = action_1[t]
        else:
            last_action_1 = -1
            p_choice_1[t] = 1.0
            p_choice_2[t] = 1.0
            continue
            
        # Stage 2 Choice
        s_idx = state[t]
        logits_2 = beta_2 * q_mf2[s_idx]
        logits_2 = logits_2 - np.max(logits_2)
        exp_2 = np.exp(logits_2)
        probs_2 = exp_2 / np.sum(exp_2)
        
        if action_2[t] != -1:
            p_choice_2[t] = probs_2[action_2[t]]
            a2 = action_2[t]
            r = reward[t]
            
            # Update Chosen Stage 2 Value
            q_mf2[s_idx, a2] += alpha * (r - q_mf2[s_idx, a2])
            
            # Update Unchosen Stage 2 Value (Counterfactual)
            # Assume unchosen would have yielded 1 - r
            unc_a2 = 1 - a2
            q_mf2[s_idx, unc_a2] += alpha_cf * ((1 - r) - q_mf2[s_idx, unc_a2])
            
            # Update Stage 1 MF Value (Direct Reinforcement / TD(1))
            q_mf1[a1] += alpha * (r - q_mf1[a1])
            
        else:
            p_choice_2[t] = 1.0
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -np.sum(np.log(p_choice_1 + eps)) - np.sum(np.log(p_choice_2 + eps))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Dynamic Transition Learning Model.
    
    The agent learns the transition matrix between spaceships and planets
    online, rather than using fixed probabilities. This captures potential
    beliefs that the spaceship-planet mapping is changing or unreliable.
    
    Parameters:
    - alpha: [0, 1] Learning rate for value updates (Stage 1 & 2).
    - eta: [0, 1] Learning rate for transition matrix updates.
    - beta_mb: [0, 10] Weight for Model-Based values (using learned T).
    - beta_mf: [0, 10] Weight for Model-Free values.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - stickiness: [0, 10] Perseverance bonus.
    """
    alpha, eta, beta_mb, beta_mf, beta_2, stickiness = model_parameters
    n_trials = len(action_1)
    
    # Initialize Transition Matrix with standard task probabilities
    # Rows: Spaceship 0, Spaceship 1
    # Cols: Planet 0, Planet 1
    T = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_mf1 = np.zeros(2)
    q_mf2 = np.zeros((2, 2)) + 0.5
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1
    
    for t in range(n_trials):
        # Stage 1 Planning with current learned T
        max_q2 = np.max(q_mf2, axis=1)
        q_mb1 = T @ max_q2
        
        logits_1 = beta_mb * q_mb1 + beta_mf * q_mf1
        if last_action_1 != -1:
            logits_1[last_action_1] += stickiness
            
        logits_1 = logits_1 - np.max(logits_1)
        exp_1 = np.exp(logits_1)
        probs_1 = exp_1 / np.sum(exp_1)
        
        if action_1[t] == -1:
            last_action_1 = -1
            p_choice_1[t] = 1.0
            p_choice_2[t] = 1.0
            continue
            
        p_choice_1[t] = probs_1[action_1[t]]
        a1 = action_1[t]
        
        # State Transition Observed
        s_idx = state[t]
        
        # Update Transition Matrix based on observation
        # Move T[a1, s_idx] towards 1.0
        T[a1, s_idx] += eta * (1.0 - T[a1, s_idx])
        # Ensure row sums to 1
        T[a1, 1-s_idx] = 1.0 - T[a1, s_idx]
        
        # Stage 2 Choice
        logits_2 = beta_2 * q_mf2[s_idx]
        logits_2 = logits_2 - np.max(logits_2)
        exp_2 = np.exp(logits_2)
        probs_2 = exp_2 / np.sum(exp_2)
        
        if action_2[t] == -1:
            p_choice_2[t] = 1.0
            last_action_1 = a1
            continue
            
        p_choice_2[t] = probs_2[action_2[t]]
        a2 = action_2[t]
        r = reward[t]
        
        # Value Updates
        q_mf2[s_idx, a2] += alpha * (r - q_mf2[s_idx, a2])
        q_mf1[a1] += alpha * (r - q_mf1[a1])
        
        last_action_1 = a1
        
    eps = 1e-10
    log_loss = -np.sum(np.log(p_choice_1 + eps)) - np.sum(np.log(p_choice_2 + eps))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Decoupled Direct and Model-Based Learning with Asymmetric Stage 2 Updates.
    
    This model separates the "Habitual" system (Direct Stage 1 learning) from the 
    "Goal-Directed" system (MB) by using distinct learning rates. It also 
    incorporates asymmetric learning (pos/neg) for the Stage 2 values, which 
    feed into the MB system.
    
    Parameters:
    - alpha_dir: [0, 1] Learning rate for direct Stage 1 (Spaceship) values.
    - alpha_pos: [0, 1] Learning rate for Stage 2 (Alien) positive RPE.
    - alpha_neg: [0, 1] Learning rate for Stage 2 (Alien) negative RPE.
    - beta_dir: [0, 10] Weight for direct Stage 1 values.
    - beta_mb: [0, 10] Weight for Model-Based Stage 1 values.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - stickiness: [0, 10] Perseverance bonus.
    """
    alpha_dir, alpha_pos, alpha_neg, beta_dir, beta_mb, beta_2, stickiness = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_dir1 = np.zeros(2) # Direct values for spaceships (Model-Free)
    q_st2 = np.zeros((2, 2)) + 0.5 # Values for aliens
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1
    
    for t in range(n_trials):
        # Stage 1: Combination of Direct MF and MB
        max_q2 = np.max(q_st2, axis=1)
        q_mb1 = transition_matrix @ max_q2
        
        logits_1 = beta_dir * q_dir1 + beta_mb * q_mb1
        if last_action_1 != -1:
            logits_1[last_action_1] += stickiness
            
        logits_1 = logits_1 - np.max(logits_1)
        exp_1 = np.exp(logits_1)
        probs_1 = exp_1 / np.sum(exp_1)
        
        if action_1[t] == -1:
            last_action_1 = -1
            p_choice_1[t] = 1.0
            p_choice_2[t] = 1.0
            continue
            
        p_choice_1[t] = probs_1[action_1[t]]
        a1 = action_1[t]
        
        # Stage 2 Choice
        s_idx = state[t]
        logits_2 = beta_2 * q_st2[s_idx]
        logits_2 = logits_2 - np.max(logits_2)
        exp_2 = np.exp(logits_2)
        probs_2 = exp_2 / np.sum(exp_2)
        
        if action_2[t] == -1:
            p_choice_2[t] = 1.0
            last_action_1 = a1
            continue
            
        p_choice_2[t] = probs_2[action_2[t]]
        a2 = action_2[t]
        r = reward[t]
        
        # Updates
        
        # 1. Direct Stage 1 update (Driven purely by reward, ignoring state transitions)
        q_dir1[a1] += alpha_dir * (r - q_dir1[a1])
        
        # 2. Stage 2 update (Asymmetric learning for Aliens)
        pe = r - q_st2[s_idx, a2]
        if pe >= 0:
            q_st2[s_idx, a2] += alpha_pos * pe
        else:
            q_st2[s_idx, a2] += alpha_neg * pe
            
        last_action_1 = a1
        
    eps = 1e-10
    log_loss = -np.sum(np.log(p_choice_1 + eps)) - np.sum(np.log(p_choice_2 + eps))
    return log_loss
```