Here are three new cognitive models for the two-step decision task, designed based on the participant's behavior and the feedback provided.

### Cognitive Model 1: Asymmetric Learning with Passive Decay
This model builds on the success of the Asymmetric Learning (Model-Free) approach by adding a **decay** mechanism. The participant data shows reward probabilities drift over time. A simple learning rate might not capture the need to "forget" old values of unchosen options (especially for aliens on planets not visited recently). This model decays the Q-values of unchosen options towards 0, allowing the agent to re-explore them later.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free RL with Asymmetric Learning and Passive Decay.
    
    Standard MF updates for chosen options with separate learning rates for 
    positive/negative prediction errors. Unchosen options decay towards 0, 
    helping the agent adapt to drifting probabilities and preventing 
    stale values from inhibiting exploration.
    
    Parameters:
    - alpha_pos: [0, 1] Learning rate for positive RPE (Reward).
    - alpha_neg: [0, 1] Learning rate for negative RPE (No Reward).
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - decay: [0, 1] Decay rate for unchosen options (1 = no decay, 0 = full reset).
    """
    alpha_pos, alpha_neg, beta_1, beta_2, decay = model_parameters
    n_trials = len(action_1)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1 = np.zeros(2) # Spaceships 0, 1
    q_stage2 = np.zeros((2, 2)) # [Planet][Alien]
    
    for trial in range(n_trials):
        # Handle missing trials if any
        if action_1[trial] == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # Stage 1 Choice
        exp_q1 = np.exp(beta_1 * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Stage 2 Choice
        s_idx = state[trial]
        exp_q2 = np.exp(beta_2 * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # Updates
        r = reward[trial]
        
        # Stage 2 Update (Asymmetric)
        a2 = action_2[trial]
        delta2 = r - q_stage2[s_idx, a2]
        lr2 = alpha_pos if delta2 >= 0 else alpha_neg
        q_stage2[s_idx, a2] += lr2 * delta2
        
        # Decay unchosen Stage 2 option
        q_stage2[s_idx, 1 - a2] *= decay
        
        # Stage 1 Update (Direct Reinforcement, Asymmetric)
        a1 = action_1[trial]
        delta1 = r - q_stage1[a1]
        lr1 = alpha_pos if delta1 >= 0 else alpha_neg
        q_stage1[a1] += lr1 * delta1
        
        # Decay unchosen Stage 1 option
        q_stage1[1 - a1] *= decay

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2: Asymmetric Learning with Accumulating Habit
The participant data shows "blocky" choice behavior (sticking to one spaceship for many trials). While simple 1-step stickiness has been tried, this model implements an **Accumulating Habit** (or perseveration kernel). The habit trace builds up over repeated choices and decays slowly, creating a stronger, longer-lasting bias ("inertia") than simple stickiness.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free RL with Asymmetric Learning and Accumulating Habit.
    
    Incorporates a habit trace that accumulates with repeated choices, 
    biasing Stage 1 decisions towards repetition or switching over longer 
    timescales than simple 1-step stickiness.
    
    Parameters:
    - alpha_pos: [0, 1] Learning rate for positive RPE.
    - alpha_neg: [0, 1] Learning rate for negative RPE.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - habit_decay: [0, 1] Decay rate of the habit trace (persistence of habit).
    - habit_w: [0, 10] Weight of the habit trace in decision making.
    """
    alpha_pos, alpha_neg, beta_1, beta_2, habit_decay, habit_w = model_parameters
    n_trials = len(action_1)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    habit_trace = np.zeros(2)
    
    for trial in range(n_trials):
        if action_1[trial] == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # Stage 1 Choice with Habit
        # Combine Q-value and Habit
        logits_1 = beta_1 * q_stage1 + habit_w * habit_trace
        # Numerical stability
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Update Habit Trace
        # Decay both traces
        habit_trace *= habit_decay
        # Increment chosen trace
        habit_trace[action_1[trial]] += 1.0
        
        # Stage 2 Choice
        s_idx = state[trial]
        exp_q2 = np.exp(beta_2 * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # Updates
        r = reward[trial]
        
        # Stage 2 Update
        a2 = action_2[trial]
        delta2 = r - q_stage2[s_idx, a2]
        lr2 = alpha_pos if delta2 >= 0 else alpha_neg
        q_stage2[s_idx, a2] += lr2 * delta2
        
        # Stage 1 Update
        a1 = action_1[trial]
        delta1 = r - q_stage1[a1]
        lr1 = alpha_pos if delta1 >= 0 else alpha_neg
        q_stage1[a1] += lr1 * delta1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3: Stage-Dependent Asymmetric Learning
This model tests the hypothesis that the asymmetry in learning (sensitivity to wins vs losses) is specific to the **consumption** phase (Stage 2, choosing aliens for coins), while the **navigation** phase (Stage 1, choosing spaceships) follows a simpler symmetric update rule. This separates the learning mechanisms for the two distinct types of decisions.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Stage-Dependent Asymmetric Learning Model.
    
    Applies symmetric learning to Stage 1 (Navigation) and asymmetric learning
    to Stage 2 (Consumption). This tests if reward valence sensitivity is 
    specific to the proximal reward decision (Stage 2) while Stage 1 is 
    learned as a general value.
    
    Parameters:
    - alpha_s1: [0, 1] Learning rate for Stage 1 (Symmetric).
    - alpha_s2_pos: [0, 1] Learning rate for Stage 2 Positive RPE.
    - alpha_s2_neg: [0, 1] Learning rate for Stage 2 Negative RPE.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    """
    alpha_s1, alpha_s2_pos, alpha_s2_neg, beta_1, beta_2 = model_parameters
    n_trials = len(action_1)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    for trial in range(n_trials):
        if action_1[trial] == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # Stage 1 Choice
        exp_q1 = np.exp(beta_1 * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Stage 2 Choice
        s_idx = state[trial]
        exp_q2 = np.exp(beta_2 * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # Updates
        r = reward[trial]
        
        # Stage 2 Update (Asymmetric)
        a2 = action_2[trial]
        delta2 = r - q_stage2[s_idx, a2]
        lr2 = alpha_s2_pos if delta2 >= 0 else alpha_s2_neg
        q_stage2[s_idx, a2] += lr2 * delta2
        
        # Stage 1 Update (Symmetric)
        a1 = action_1[trial]
        delta1 = r - q_stage1[a1]
        q_stage1[a1] += alpha_s1 * delta1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```