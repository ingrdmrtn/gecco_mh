Here are three new cognitive models implemented as Python functions.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF model with Separate Decay Rates for Stage 1 and Stage 2.
    Allows the model to forget spaceship values (Stage 1) and alien values (Stage 2)
    at different rates, accommodating the different stability of transitions vs rewards.
    
    Parameters:
    lr (float): Learning rate [0,1].
    beta (float): Inverse temperature [0,10].
    w (float): MB/MF Weighting [0,1].
    decay_s1 (float): Decay rate for Stage 1 Q-values [0,1].
    decay_s2 (float): Decay rate for Stage 2 Q-values [0,1].
    pers (float): Perseveration bonus for Stage 1 choice [0,5].
    """
    lr, beta, w, decay_s1, decay_s2, pers = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_a1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue

        # Decay Q-values with separate rates for Stage 1 and Stage 2
        q_stage1_mf *= (1 - decay_s1)
        q_stage2_mf *= (1 - decay_s2)

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf

        logits_1 = beta * q_net_stage1
        if prev_a1 != -1:
            logits_1[prev_a1] += pers

        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # Stage 2 Policy
        logits_2 = beta * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # Updates
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr * delta_stage1

        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr * delta_stage2
        
        # Propagate Stage 2 RPE to Stage 1 MF (lambda=1)
        q_stage1_mf[action_1[trial]] += lr * delta_stage2
        
        prev_a1 = action_1[trial]

    eps = 1e-10
    valid_mask = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF model with Surprise-Modulated Learning Rate.
    The effective learning rate increases when the prediction error (surprise) is high,
    allowing faster adaptation to sudden changes in reward probabilities.
    
    Parameters:
    lr (float): Base learning rate [0,1].
    beta (float): Inverse temperature [0,10].
    w (float): MB/MF Weighting [0,1].
    decay (float): Decay rate for Q-values [0,1].
    pers (float): Perseveration bonus [0,5].
    surp (float): Surprise sensitivity scaling factor [0,10].
    """
    lr, beta, w, decay, pers, surp = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_a1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue

        q_stage1_mf *= (1 - decay)
        q_stage2_mf *= (1 - decay)

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf

        logits_1 = beta * q_net_stage1
        if prev_a1 != -1:
            logits_1[prev_a1] += pers

        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # Stage 2 Policy
        logits_2 = beta * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # Updates with Surprise Modulation
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]

        # Modulate LR based on absolute prediction error
        eff_lr_1 = lr * (1 + surp * np.abs(delta_stage1))
        eff_lr_1 = min(eff_lr_1, 1.0)
        
        eff_lr_2 = lr * (1 + surp * np.abs(delta_stage2))
        eff_lr_2 = min(eff_lr_2, 1.0)

        q_stage1_mf[action_1[trial]] += eff_lr_1 * delta_stage1
        q_stage2_mf[state_idx, action_2[trial]] += eff_lr_2 * delta_stage2
        
        # Propagate Stage 2 RPE to Stage 1
        q_stage1_mf[action_1[trial]] += eff_lr_2 * delta_stage2
        
        prev_a1 = action_1[trial]

    eps = 1e-10
    valid_mask = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF model with Decay, Perseveration, and Epsilon-Greedy Lapse.
    Incorporates a 'lapse rate' (epsilon) to account for random errors or exploration
    unrelated to value differences, improving robustness to outliers.
    
    Parameters:
    lr (float): Learning rate [0,1].
    beta (float): Inverse temperature [0,10].
    w (float): MB/MF Weighting [0,1].
    decay (float): Decay rate [0,1].
    pers (float): Perseveration bonus [0,5].
    epsilon (float): Lapse rate / random choice probability [0,1].
    """
    lr, beta, w, decay, pers, epsilon = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_a1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue

        q_stage1_mf *= (1 - decay)
        q_stage2_mf *= (1 - decay)

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf

        logits_1 = beta * q_net_stage1
        if prev_a1 != -1:
            logits_1[prev_a1] += pers

        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        # Apply Epsilon Mixture (Lapse)
        probs_1 = (1 - epsilon) * probs_1 + epsilon * 0.5
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # Stage 2 Policy
        logits_2 = beta * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        # Apply Epsilon Mixture (Lapse)
        probs_2 = (1 - epsilon) * probs_2 + epsilon * 0.5
        p_choice_2[trial] = probs_2[action_2[trial]]


        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr * delta_stage1

        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr * delta_stage2

        q_stage1_mf[action_1[trial]] += lr * delta_stage2
        
        prev_a1 = action_1[trial]

    eps = 1e-10
    valid_mask = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss
```