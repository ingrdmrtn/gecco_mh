Here are three new cognitive models for the two-step decision task, based on the participant data and the feedback provided.

### Model 1: Asymmetric Learning with Forgetting
This model builds on the success of the "Forgetting" mechanism but incorporates **asymmetric learning rates** for positive and negative prediction errors. Participant data shows long streaks of choosing the same option when rewarding (e.g., trials 11-26), but potentially slower or faster reactions to losses. This model allows for different updates when things go better than expected versus worse, while simultaneously decaying the values of unchosen options to handle the non-stationary environment.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Model 1: Asymmetric Learning Q-Learning with Forgetting.
    
    A Model-Free learner that distinguishes between positive and negative 
    reward prediction errors (RPE). It also decays the Q-values of unchosen 
    actions on every trial (forgetting), which helps in drifting environments.

    Parameters:
    - alpha_pos: [0, 1] Learning rate for positive RPE (better than expected).
    - alpha_neg: [0, 1] Learning rate for negative RPE (worse than expected).
    - beta: [0, 10] Inverse temperature (exploration/exploitation balance).
    - forget_rate: [0, 1] Decay rate for unchosen actions (Q_unchosen -> 0).
    """
    alpha_pos, alpha_neg, beta, forget_rate = model_parameters
    n_trials = len(action_1)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for t in range(n_trials):
        # -- Stage 1 Choice --
        logits_1 = beta * q_stage1
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]
        
        # -- Stage 2 Choice --
        s_idx = state[t]
        logits_2 = beta * q_stage2[s_idx]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]
        
        r = reward[t]
        
        # -- Update Stage 2 --
        pe2 = r - q_stage2[s_idx, a2]
        if pe2 >= 0:
            q_stage2[s_idx, a2] += alpha_pos * pe2
        else:
            q_stage2[s_idx, a2] += alpha_neg * pe2
            
        # Forgetting for Stage 2 unchosen
        unchosen_a2 = 1 - a2
        q_stage2[s_idx, unchosen_a2] *= (1.0 - forget_rate)
        
        # -- Update Stage 1 --
        # TD(0) update: target is value of chosen stage 2 state-action
        target_val = q_stage2[s_idx, a2]
        pe1 = target_val - q_stage1[a1]
        if pe1 >= 0:
            q_stage1[a1] += alpha_pos * pe1
        else:
            q_stage1[a1] += alpha_neg * pe1
            
        # Forgetting for Stage 1 unchosen
        unchosen_a1 = 1 - a1
        q_stage1[unchosen_a1] *= (1.0 - forget_rate)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Dual-Stage Stickiness Q-Learning
The participant exhibits strong perseverance (stickiness), repeating choices for long blocks. Previous models often apply stickiness only to the first decision (Spaceship). However, habits likely form at both levels: the choice of spaceship and the choice of alien within a planet. This model implements **separate stickiness parameters** for Stage 1 and Stage 2 to capture distinct habit strengths for the different types of decisions.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model 2: Q-Learning with Stage-Specific Stickiness.
    
    A Model-Free learner that applies distinct perseverance bonuses (stickiness) 
    for the spaceship choice (Stage 1) and the alien choice (Stage 2). 
    This allows the model to capture different levels of habit formation 
    at different stages of the task.

    Parameters:
    - alpha: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - stick_s1: [0, 10] Stickiness bonus for repeating the last Stage 1 choice.
    - stick_s2: [0, 10] Stickiness bonus for repeating the last Stage 2 choice (per state).
    """
    alpha, beta, stick_s1, stick_s2 = model_parameters
    n_trials = len(action_1)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1
    # Track previous action per state for Stage 2 (Planet 0 and Planet 1)
    prev_a2_per_state = [-1, -1] 
    
    for t in range(n_trials):
        # -- Stage 1 Choice --
        logits_1 = beta * q_stage1.copy()
        if prev_a1 != -1:
            logits_1[prev_a1] += stick_s1
            
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]
        
        # -- Stage 2 Choice --
        s_idx = state[t]
        logits_2 = beta * q_stage2[s_idx].copy()
        
        # Apply stickiness if we have visited this state before
        prev_a2 = prev_a2_per_state[s_idx]
        if prev_a2 != -1:
            logits_2[prev_a2] += stick_s2
            
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]
        
        # Update history
        prev_a1 = a1
        prev_a2_per_state[s_idx] = a2
        
        # -- Learning --
        r = reward[t]
        # Update Stage 2
        q_stage2[s_idx, a2] += alpha * (r - q_stage2[s_idx, a2])
        
        # Update Stage 1 (TD-0)
        target_val = q_stage2[s_idx, a2]
        q_stage1[a1] += alpha * (target_val - q_stage1[a1])

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Adaptive Model-Based RL
This model posits that the participant acts as a **Model-Based** agent but, instead of assuming a fixed transition structure (70/30), they **learn the transition matrix** over time. The participant data shows reactions to transitions (e.g., switching after rare transitions or persisting). This model updates its internal map of the world (`T_matrix`) based on observed transitions, and calculates Stage 1 values by planning over this learned map.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model 3: Adaptive Model-Based RL with Stickiness.
    
    A pure Model-Based agent that learns the transition structure of the task 
    (Adaptive MB) rather than using fixed probabilities. Stage 1 values are 
    computed by combining the learned transition matrix with Stage 2 values.

    Parameters:
    - alpha: [0, 1] Learning rate for Stage 2 Q-values (Alien reward values).
    - beta: [0, 10] Inverse temperature.
    - stickiness: [0, 10] Choice stickiness for Stage 1.
    - alpha_T: [0, 1] Learning rate for transition probabilities (State map).
    """
    alpha, beta, stickiness, alpha_T = model_parameters
    n_trials = len(action_1)
    
    # Initialize transition matrix: 2 actions (spaceships) -> 2 states (planets)
    # Start with uniform prior (0.5) to represent initial uncertainty or simple prior
    t_matrix = np.full((2, 2), 0.5)
    
    q_stage2 = np.zeros((2, 2)) # Model-Free values for Stage 2
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1
    
    for t in range(n_trials):
        # -- Stage 1 Choice (Model-Based Calculation) --
        # Q_MB(a1) = Sum_s' [ T(s'|a1) * Max_a2 Q(s', a2) ]
        max_q2 = np.max(q_stage2, axis=1) # Max value for each planet
        q_stage1_mb = t_matrix @ max_q2
        
        logits_1 = beta * q_stage1_mb.copy()
        if prev_a1 != -1:
            logits_1[prev_a1] += stickiness
            
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]
        
        # -- Stage 2 Choice --
        s_idx = state[t]
        logits_2 = beta * q_stage2[s_idx]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]
        
        # -- Learning --
        r = reward[t]
        
        # 1. Update Stage 2 Values (Reward Learning)
        q_stage2[s_idx, a2] += alpha * (r - q_stage2[s_idx, a2])
        
        # 2. Update Transition Matrix (Structure Learning)
        # T(s|a1) += alpha_T * (1 - T(s|a1))
        # T(other|a1) += alpha_T * (0 - T(other|a1))
        # This approximates counting transitions
        t_matrix[a1, s_idx] += alpha_T * (1.0 - t_matrix[a1, s_idx])
        t_matrix[a1, 1 - s_idx] += alpha_T * (0.0 - t_matrix[a1, 1 - s_idx])
        
        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```