Here are three new cognitive models for the two-step task.

### Model 1: MF with Separate Learning Rates and Decay
This model refines the previous best model by separating the learning rates for Stage 1 (spaceship choice) and Stage 2 (alien choice). It retains the decay mechanism for unchosen options to model forgetting and the stickiness parameter to capture the participant's tendency to repeat Stage 1 choices.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    MF with Separate Learning Rates and Decay.
    
    This model distinguishes between the learning processes for the first stage (spaceship choice)
    and the second stage (alien choice), allowing for different learning rates. It also includes
    a decay mechanism for unchosen options to model forgetting or return to baseline.
    
    Parameters:
    lr_s1: [0, 1] Learning rate for Stage 1 choice.
    lr_s2: [0, 1] Learning rate for Stage 2 choice.
    decay_rate: [0, 1] Decay rate for unchosen Q-values.
    beta: [0, 10] Inverse temperature for softmax.
    stickiness: [0, 5] Choice perseverance bonus for Stage 1.
    """
    lr_s1, lr_s2, decay_rate, beta, stickiness = model_parameters
    n_trials = len(action_1)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2)) # state x action
    
    log_likelihood = 0.0
    eps = 1e-10
    
    last_action_1 = -1 

    for trial in range(n_trials):
        a1 = action_1[trial]
        s2 = state[trial]
        a2 = action_2[trial]
        r = reward[trial]

        # Stage 1 Policy
        q_eff_1 = q_stage1.copy()
        if last_action_1 != -1:
            q_eff_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_eff_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_likelihood += np.log(probs_1[a1] + eps)
        
        last_action_1 = a1

        if a2 == -1:
            continue

        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_likelihood += np.log(probs_2[a2] + eps)

        # Update Stage 2
        # Chosen
        q_stage2[s2, a2] += lr_s2 * (r - q_stage2[s2, a2])
        # Unchosen in current state decays
        q_stage2[s2, 1-a2] *= (1.0 - decay_rate)
        # Unvisited state decays
        q_stage2[1-s2, :] *= (1.0 - decay_rate)

        # Update Stage 1
        q_stage1[a1] += lr_s1 * (q_stage2[s2, a2] - q_stage1[a1])
        # Unchosen Stage 1 decays
        q_stage1[1-a1] *= (1.0 - decay_rate)

    return -log_likelihood
```

### Model 2: Counterfactual Update Model
This model introduces a counterfactual updating mechanism in the second stage. When the participant observes the outcome of their chosen alien, they also update the value of the *unchosen* alien, assuming a complementary outcome (e.g., if the chosen alien gave 0 coins, the model assumes the other would have given 1).

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Counterfactual Update Model.
    
    Incorporates counterfactual learning in the second stage. When an outcome is observed,
    the model updates the unchosen option assuming it would have yielded the opposite outcome.
    This heuristic helps in environments with anticorrelated or binary rewards.
    
    Parameters:
    learning_rate: [0, 1] Standard learning rate.
    cf_learning_rate: [0, 1] Counterfactual learning rate for unchosen options.
    beta: [0, 10] Inverse temperature.
    stickiness: [0, 5] Choice perseverance for Stage 1.
    """
    learning_rate, cf_learning_rate, beta, stickiness = model_parameters
    n_trials = len(action_1)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    log_likelihood = 0.0
    eps = 1e-10
    
    last_action_1 = -1 

    for trial in range(n_trials):
        a1 = action_1[trial]
        s2 = state[trial]
        a2 = action_2[trial]
        r = reward[trial]

        # Stage 1
        q_eff_1 = q_stage1.copy()
        if last_action_1 != -1:
            q_eff_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_eff_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_likelihood += np.log(probs_1[a1] + eps)
        
        last_action_1 = a1

        if a2 == -1:
            continue

        # Stage 2
        exp_q2 = np.exp(beta * q_stage2[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_likelihood += np.log(probs_2[a2] + eps)

        # Update Stage 2
        # Chosen
        q_stage2[s2, a2] += learning_rate * (r - q_stage2[s2, a2])
        
        # Unchosen (Counterfactual)
        # Heuristic: if r <= 0 (bad), unchosen was good (1). If r > 0 (good), unchosen was bad (0).
        cf_reward = 1.0 if r <= 0 else 0.0
        q_stage2[s2, 1-a2] += cf_learning_rate * (cf_reward - q_stage2[s2, 1-a2])

        # Update Stage 1
        q_stage1[a1] += learning_rate * (q_stage2[s2, a2] - q_stage1[a1])

    return -log_likelihood
```

### Model 3: Adaptive Model-Based Hybrid
This model is a hybrid learner that does not assume fixed transition probabilities (0.7/0.3). Instead, it learns the transition matrix $T$ online based on observed transitions. This allows the model to adapt if the participant perceives the "rare" transitions as becoming more frequent or if they are learning the structure from scratch.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Adaptive Model-Based Hybrid.
    
    A hybrid Model-Based/Model-Free learner that dynamically updates its internal model
    of the transition matrix (Spaceship -> Planet probabilities) based on experience,
    rather than assuming fixed probabilities.
    
    Parameters:
    lr_reward: [0, 1] Learning rate for reward values.
    lr_trans: [0, 1] Learning rate for transition probabilities.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Weight for Model-Based values (0=MF, 1=MB).
    stickiness: [0, 5] Choice perseverance for Stage 1.
    """
    lr_reward, lr_trans, beta, w, stickiness = model_parameters
    n_trials = len(action_1)
    
    q_stage1_mf = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    # Initialize transitions with prior (0.7/0.3)
    T = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    log_likelihood = 0.0
    eps = 1e-10
    
    last_action_1 = -1 

    for trial in range(n_trials):
        a1 = action_1[trial]
        s2 = state[trial]
        a2 = action_2[trial]
        r = reward[trial]

        # Model-Based Value
        max_q2 = np.max(q_stage2, axis=1)
        q_stage1_mb = T @ max_q2
        
        # Hybrid Value
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Stickiness
        if last_action_1 != -1:
            q_net_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_likelihood += np.log(probs_1[a1] + eps)
        
        last_action_1 = a1

        if a2 == -1:
            continue

        # Stage 2
        exp_q2 = np.exp(beta * q_stage2[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_likelihood += np.log(probs_2[a2] + eps)

        # Update Values
        q_stage2[s2, a2] += lr_reward * (r - q_stage2[s2, a2])
        q_stage1_mf[a1] += lr_reward * (q_stage2[s2, a2] - q_stage1_mf[a1])
        
        # Update Transitions
        T[a1, s2] += lr_trans * (1.0 - T[a1, s2])
        T[a1, 1-s2] = 1.0 - T[a1, s2]

    return -log_likelihood
```