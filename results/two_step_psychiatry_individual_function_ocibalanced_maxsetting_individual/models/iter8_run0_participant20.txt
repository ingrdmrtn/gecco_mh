Here are the 3 new cognitive models based on your requirements and the participant data analysis.

### Model 1: Hybrid with Eligibility Trace and Dual Stickiness
This model builds on the successful "Dual Stickiness" approach by adding an eligibility trace parameter ($\lambda$). This allows the model to update the first-stage values based on a mixture of the second-stage value (TD(0)) and the final reward (Monte Carlo/TD(1)), capturing how the participant assigns credit to the spaceship choice based on the ultimate outcome.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Model-Based/Model-Free Hybrid with Eligibility Trace (Lambda) and Dual Stickiness.
    
    Incorporates separate stickiness parameters for spaceship (Stage 1) and alien (Stage 2) choices.
    Uses an eligibility trace parameter (lambda) to modulate how the second-stage reward 
    directly influences the first-stage Model-Free values.
    
    Parameters:
    - learning_rate: [0,1] Learning rate for value updates.
    - beta_1: [0,10] Inverse temperature for Stage 1 (Spaceship) choice.
    - beta_2: [0,10] Inverse temperature for Stage 2 (Alien) choice.
    - w: [0,1] Weighting between Model-Based (1) and Model-Free (0) values.
    - lambda_param: [0,1] Eligibility trace parameter (0 = TD(0), 1 = Monte Carlo).
    - stick_1: [0,10] Stickiness for Stage 1 (Spaceship).
    - stick_2: [0,10] Stickiness for Stage 2 (Alien).
    """
    learning_rate, beta_1, beta_2, w, lambda_param, stick_1, stick_2 = model_parameters
    n_trials = len(action_1)
    
    # Fixed transition probabilities as per task structure
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Initialize Q-values
    q_stage1_mf = np.zeros(2) # Values for Spaceship A (0) and U (1)
    q_stage2_mf = np.zeros((2, 2)) # Values for Aliens in Planet X (0) and Y (1)
    
    last_action_1 = -1
    last_action_2 = -1 # Global alien choice tracker
    
    for trial in range(n_trials):
        # Skip missing trials
        if action_1[trial] == -1:
            continue

        # --- Stage 1: Spaceship Choice ---
        # Model-Based Value Calculation: Q_MB = T * V_stage2
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Value: Weighted sum of MB and MF
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Logits with Stickiness
        logits_1 = beta_1 * q_net_1
        if last_action_1 != -1:
            logits_1[last_action_1] += stick_1
            
        # Softmax Probability
        exp_q1 = np.exp(logits_1 - np.max(logits_1)) # Subtract max for stability
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Record choice
        last_action_1 = action_1[trial]
        state_idx = state[trial] # Planet arrived at
        
        # --- Stage 2: Alien Choice ---
        # Logits with Stickiness (Global Stickiness)
        logits_2 = beta_2 * q_stage2_mf[state_idx]
        if last_action_2 != -1:
            logits_2[last_action_2] += stick_2
            
        # Softmax Probability
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        last_action_2 = action_2[trial]
        
        # --- Value Updating ---
        
        # Stage 1 Prediction Error (TD(0))
        # delta_1 = V(s') - Q(s,a)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        # Stage 2 Prediction Error
        # delta_2 = R - Q(s',a')
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Update Stage 1 MF Value: Q(s,a) += alpha * (delta_1 + lambda * delta_2)
        q_stage1_mf[action_1[trial]] += learning_rate * (delta_stage1 + lambda_param * delta_stage2)
        
        # Update Stage 2 MF Value: Q(s',a') += alpha * delta_2
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

    # Calculate Negative Log Likelihood
    eps = 1e-10
    # Only sum log-likelihoods for valid trials where a choice was made
    valid_idx = p_choice_1 > 0
    log_loss = -(np.sum(np.log(p_choice_1[valid_idx] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_idx] + eps)))
    return log_loss
```

### Model 2: Hybrid with Value Decay and Dual Stickiness
This model addresses the slowly changing reward probabilities in the task. It includes a `decay` parameter that causes the value of the *unchosen* alien to slowly revert towards zero (forgetting). This helps the model adapt when a previously good option becomes bad, or vice versa, without solely relying on the learning rate.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Based/Model-Free Hybrid with Value Decay and Dual Stickiness.
    
    Incorporates separate stickiness parameters for spaceship and alien choices.
    Includes a decay parameter for Stage 2 Q-values. Unchosen aliens' values decay 
    towards 0 on each trial, helping the model adapt to the slowly changing 
    reward probabilities of the task.
    
    Parameters:
    - learning_rate: [0,1] Update rate for chosen options.
    - beta_1: [0,10] Inverse temperature for Stage 1.
    - beta_2: [0,10] Inverse temperature for Stage 2.
    - w: [0,1] MB/MF weight.
    - stick_1: [0,10] Stickiness for Stage 1.
    - stick_2: [0,10] Stickiness for Stage 2.
    - decay: [0,1] Decay rate for unchosen Stage 2 options (0 = no decay, 1 = instant forget).
    """
    learning_rate, beta_1, beta_2, w, stick_1, stick_2, decay = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    last_action_2 = -1
    
    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue

        # --- Stage 1 ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta_1 * q_net_1
        if last_action_1 != -1:
            logits_1[last_action_1] += stick_1
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_action_1 = action_1[trial]
        state_idx = state[trial]
        
        # --- Stage 2 ---
        logits_2 = beta_2 * q_stage2_mf[state_idx]
        if last_action_2 != -1:
            logits_2[last_action_2] += stick_2
            
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        last_action_2 = action_2[trial]
        
        # --- Updates ---
        
        # Stage 1 Update (Standard TD(0))
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Stage 2 Update with Decay
        chosen_a2 = action_2[trial]
        unchosen_a2 = 1 - chosen_a2
        
        # Update chosen
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, chosen_a2]
        q_stage2_mf[state_idx, chosen_a2] += learning_rate * delta_stage2
        
        # Decay unchosen (in the current state)
        q_stage2_mf[state_idx, unchosen_a2] *= (1 - decay)
        
    eps = 1e-10
    valid_idx = p_choice_1 > 0
    log_loss = -(np.sum(np.log(p_choice_1[valid_idx] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_idx] + eps)))
    return log_loss
```

### Model 3: Hybrid with Asymmetric Learning and Dual Stickiness
This model tests the hypothesis that the participant learns differently from positive outcomes (rewards) versus negative outcomes (omissions). By splitting the learning rate into `lr_pos` and `lr_neg`, and combining this with the robust "Dual Stickiness" mechanism, the model can capture biases like "win-stay" being stronger than "lose-shift" (or vice versa).

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Based/Model-Free Hybrid with Asymmetric Learning and Dual Stickiness.
    
    Incorporates separate stickiness parameters for spaceship and alien choices.
    Uses separate learning rates for positive (lr_pos) and negative (lr_neg) prediction errors,
    allowing the model to weight wins and losses differently.
    
    Parameters:
    - lr_pos: [0,1] Learning rate for positive prediction errors (better than expected).
    - lr_neg: [0,1] Learning rate for negative prediction errors (worse than expected).
    - beta_1: [0,10] Inverse temperature for Stage 1.
    - beta_2: [0,10] Inverse temperature for Stage 2.
    - w: [0,1] MB/MF weight.
    - stick_1: [0,10] Stickiness for Stage 1.
    - stick_2: [0,10] Stickiness for Stage 2.
    """
    lr_pos, lr_neg, beta_1, beta_2, w, stick_1, stick_2 = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    last_action_2 = -1
    
    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue

        # --- Stage 1 ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta_1 * q_net_1
        if last_action_1 != -1:
            logits_1[last_action_1] += stick_1
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_action_1 = action_1[trial]
        state_idx = state[trial]
        
        # --- Stage 2 ---
        logits_2 = beta_2 * q_stage2_mf[state_idx]
        if last_action_2 != -1:
            logits_2[last_action_2] += stick_2
            
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        last_action_2 = action_2[trial]
        
        # --- Updates ---
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        if delta_stage1 >= 0:
            q_stage1_mf[action_1[trial]] += lr_pos * delta_stage1
        else:
            q_stage1_mf[action_1[trial]] += lr_neg * delta_stage1
        
        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        if delta_stage2 >= 0:
            q_stage2_mf[state_idx, action_2[trial]] += lr_pos * delta_stage2
        else:
            q_stage2_mf[state_idx, action_2[trial]] += lr_neg * delta_stage2

    eps = 1e-10
    valid_idx = p_choice_1 > 0
    log_loss = -(np.sum(np.log(p_choice_1[valid_idx] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_idx] + eps)))
    return log_loss
```