Here are three cognitive models based on the participant data and task structure.

### Model 1: Counterfactual Stage 2 Update
This model hypothesizes that the participant updates their beliefs about the *unchosen* alien in Stage 2 based on the outcome of the chosen alien. Given the "slowly changing probabilities" and the binary nature of the task (aliens often have anticorrelated reward probabilities in such bandit tasks), the participant might infer that if the chosen alien gives 0 coins, the other would likely have given 1 (and vice versa).

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Counterfactual Stage 2 Update Model.
    
    This model assumes the participant updates the unchosen alien in Stage 2 
    based on a counterfactual inference from the chosen alien's reward.
    If 'cf_weight' > 0, the agent updates the unchosen option towards (1 - reward).
    
    Parameters:
    lr: [0,1] - Learning rate for chosen options.
    beta_1: [0,10] - Inverse temperature for Stage 1.
    beta_2: [0,10] - Inverse temperature for Stage 2.
    w: [0,1] - Weighting of Model-Based values (0=MF, 1=MB).
    stick: [0,5] - Choice stickiness for Stage 1.
    cf_weight: [0,1] - Weight of counterfactual update (relative to lr).
                       0 = no counterfactual update.
    """
    lr, beta_1, beta_2, w, stick, cf_weight = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    
    prev_action_1 = -1

    for trial in range(n_trials):
        # Skip invalid trials
        if action_1[trial] == -1:
            continue

        # --- Stage 1 Policy ---
        # Model-Based Value Calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Logits and Stickiness
        logits_1 = beta_1 * q_net
        if prev_action_1 != -1:
            logits_1[prev_action_1] += stick
            
        # Softmax Stage 1
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        # --- Stage 2 Policy ---
        state_idx = state[trial]
        logits_2 = beta_2 * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        
        # Stage 1 MF Update (SARSA/TD)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr * delta_stage1

        # Stage 2 MF Update (Chosen)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr * delta_stage2
        
        # Stage 2 MF Update (Unchosen / Counterfactual)
        # Assume the unchosen alien would have yielded (1 - reward)
        unchosen_alien = 1 - action_2[trial]
        counterfactual_reward = 1.0 - reward[trial]
        delta_cf = counterfactual_reward - q_stage2_mf[state_idx, unchosen_alien]
        
        # Update unchosen with scaled learning rate
        q_stage2_mf[state_idx, unchosen_alien] += (lr * cf_weight) * delta_cf
        
        prev_action_1 = action_1[trial]

    eps = 1e-10
    # Filter out trials where probabilities were not calculated (invalid trials)
    valid_trials = (p_choice_1 > 0) & (p_choice_2 > 0)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```

### Model 2: Reference Point Dependence
This model introduces a "Reference Point" (`r_ref`) for reward evaluation. Instead of treating rewards purely as 0 or 1, the agent compares the outcome to a subjective standard. If `r_ref` is high (e.g., 0.5 or 0.8), receiving 0 coins is treated as a distinct loss (negative prediction error), whereas if `r_ref` is 0, 0 coins is neutral. This shifts the asymptotic values of the Q-functions and affects exploration dynamics.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Reference Point Dependence Model.
    
    The agent evaluates rewards relative to a reference point 'r_ref'.
    Effective Reward = Reward - r_ref.
    This allows the model to capture pessimism (high r_ref) or optimism,
    shifting the Q-values relative to the initialization of 0.
    
    Parameters:
    lr: [0,1] - Learning rate.
    beta_1: [0,10] - Inverse temperature for Stage 1.
    beta_2: [0,10] - Inverse temperature for Stage 2.
    w: [0,1] - MB/MF weighting.
    stick: [0,5] - Choice stickiness for Stage 1.
    r_ref: [0,1] - Reference reward value. 0 coins is perceived as (0 - r_ref).
    """
    lr, beta_1, beta_2, w, stick, r_ref = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta_1 * q_net
        if prev_action_1 != -1:
            logits_1[prev_action_1] += stick
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        # --- Stage 2 Policy ---
        state_idx = state[trial]
        logits_2 = beta_2 * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        # Calculate Effective Reward
        eff_reward = reward[trial] - r_ref
        
        # Stage 1 MF Update
        # Note: Q-values will converge to E[Reward] - r_ref
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr * delta_stage1

        # Stage 2 MF Update
        delta_stage2 = eff_reward - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr * delta_stage2
        
        prev_action_1 = action_1[trial]

    eps = 1e-10
    valid_trials = (p_choice_1 > 0) & (p_choice_2 > 0)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```

### Model 3: Action-Specific Learning Rates
The participant data shows a very strong, sustained preference for Spaceship 0 (A) followed by a switch to Spaceship 1 (U). This asymmetry suggests the participant might update value estimates for the two spaceships at different rates. This model assigns separate learning rates (`lr_A` for Spaceship 0, `lr_B` for Spaceship 1) for the Stage 1 Model-Free values.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Action-Specific Learning Rates Model.
    
    This model allows for different learning rates depending on which 
    Stage 1 spaceship (action) was chosen. This captures asymmetric 
    learning or attention allocation between the two primary options.
    
    Parameters:
    lr_A: [0,1] - Learning rate when Spaceship 0 (A) is chosen.
    lr_B: [0,1] - Learning rate when Spaceship 1 (B/U) is chosen.
    beta_1: [0,10] - Inverse temperature for Stage 1.
    beta_2: [0,10] - Inverse temperature for Stage 2.
    w: [0,1] - MB/MF weighting.
    stick: [0,5] - Choice stickiness for Stage 1.
    """
    lr_A, lr_B, beta_1, beta_2, w, stick = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta_1 * q_net
        if prev_action_1 != -1:
            logits_1[prev_action_1] += stick
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        # --- Stage 2 Policy ---
        state_idx = state[trial]
        logits_2 = beta_2 * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        
        # Select learning rate based on Stage 1 choice
        current_lr = lr_A if action_1[trial] == 0 else lr_B
        
        # Stage 1 MF Update
        # Note: We use the action-specific LR for the Stage 1 update
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += current_lr * delta_stage1

        # Stage 2 MF Update
        # We use the same current_lr for consistency within the trial, 
        # or it could be a shared parameter. Here we assume the 'attention' 
        # level (LR) persists for the trial.
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += current_lr * delta_stage2
        
        prev_action_1 = action_1[trial]

    eps = 1e-10
    valid_trials = (p_choice_1 > 0) & (p_choice_2 > 0)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```