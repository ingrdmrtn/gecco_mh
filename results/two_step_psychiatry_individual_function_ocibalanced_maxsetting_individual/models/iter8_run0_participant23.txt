Here are the 3 new cognitive models, designed to capture the specific patterns of perseveration and learning observed in the participant data.

### Model 1: Hybrid MB/MF with Stage-Specific Stickiness
This model integrates the structural knowledge of the task (Model-Based) with experience-driven learning (Model-Free), while maintaining the stage-specific perseveration mechanism that fits the participant's habit patterns well.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based/Model-Free with Stage-Specific Stickiness.
    
    Combines a weighted mixture of Model-Based (transition-matrix dependent) 
    and Model-Free (experience dependent) values for the first stage choice, 
    while applying distinct stickiness (perseveration) parameters to both 
    Stage 1 (Spaceship) and Stage 2 (Alien) choices.
    
    Parameters:
    learning_rate: [0, 1] Learning rate for value updates.
    beta:          [0, 10] Inverse temperature (exploration/exploitation).
    w:             [0, 1] Weight for Model-Based control (1=MB, 0=MF).
    stick_s1:      [-5, 5] Stickiness for Stage 1 choice.
    stick_s2:      [-5, 5] Stickiness for Stage 2 choice.
    """
    learning_rate, beta, w, stick_s1, stick_s2 = model_parameters
    n_trials = len(action_1)
    
    # Transition matrix assumed from task structure (0.7 common, 0.3 rare)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2) + 0.5
    q_stage2_mf = np.zeros((2, 2)) + 0.5
    
    log_loss = 0
    eps = 1e-10
    
    prev_a1 = -1
    prev_a2 = np.array([-1, -1])
    
    for trial in range(n_trials):
        a1 = action_1[trial]
        s_idx = state[trial]
        a2 = action_2[trial]
        r = reward[trial]
        
        # Handle missing data
        if a1 == -1 or s_idx == -1 or a2 == -1:
            continue
            
        # --- Stage 1 Policy ---
        # Model-Based Value: Expected max value of next stage weighted by transition probs
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Value: Weighted sum of MB and MF
        q_net_s1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net_s1
        if prev_a1 != -1:
            logits_1[prev_a1] += stick_s1
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + eps)
        
        # --- Stage 2 Policy ---
        logits_2 = beta * q_stage2_mf[s_idx]
        if prev_a2[s_idx] != -1:
            logits_2[prev_a2[s_idx]] += stick_s2
            
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[a2] + eps)
        
        # --- Updates ---
        # Stage 1 MF Update (SARSA-style)
        target_s1 = q_stage2_mf[s_idx, a2]
        q_stage1_mf[a1] += learning_rate * (target_s1 - q_stage1_mf[a1])
        
        # Stage 2 MF Update
        q_stage2_mf[s_idx, a2] += learning_rate * (r - q_stage2_mf[s_idx, a2])
        
        prev_a1 = a1
        prev_a2[s_idx] = a2
        
    return log_loss
```

### Model 2: Asymmetric Learning with Stage-Specific Stickiness
This model investigates whether the participant learns differently from better-than-expected vs. worse-than-expected outcomes (asymmetric learning rates), while maintaining the separation of habits (stickiness) for the two task stages.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Asymmetric Learning with Stage-Specific Stickiness.
    
    Incorporates separate learning rates for positive and negative prediction 
    errors (valence-dependent learning) to capture win-stay/lose-shift asymmetries, 
    alongside stage-specific stickiness to model habit formation in both 
    spaceship and alien choices independently.
    
    Parameters:
    lr_pos:   [0, 1] Learning rate for positive prediction errors (Reward > Q).
    lr_neg:   [0, 1] Learning rate for negative prediction errors (Reward < Q).
    beta:     [0, 10] Inverse temperature.
    stick_s1: [-5, 5] Stickiness for Stage 1.
    stick_s2: [-5, 5] Stickiness for Stage 2.
    """
    lr_pos, lr_neg, beta, stick_s1, stick_s2 = model_parameters
    n_trials = len(action_1)
    
    q_stage1 = np.zeros(2) + 0.5
    q_stage2 = np.zeros((2, 2)) + 0.5
    
    log_loss = 0
    eps = 1e-10
    
    prev_a1 = -1
    prev_a2 = np.array([-1, -1])
    
    for trial in range(n_trials):
        a1 = action_1[trial]
        s_idx = state[trial]
        a2 = action_2[trial]
        r = reward[trial]
        
        if a1 == -1 or s_idx == -1 or a2 == -1:
            continue
            
        # --- Stage 1 Choice ---
        logits_1 = beta * q_stage1
        if prev_a1 != -1:
            logits_1[prev_a1] += stick_s1
        
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + eps)
        
        # --- Stage 2 Choice ---
        logits_2 = beta * q_stage2[s_idx]
        if prev_a2[s_idx] != -1:
            logits_2[prev_a2[s_idx]] += stick_s2
            
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[a2] + eps)
        
        # --- Updates ---
        # Stage 1 Update
        target_s1 = q_stage2[s_idx, a2]
        pe_1 = target_s1 - q_stage1[a1]
        alpha_1 = lr_pos if pe_1 > 0 else lr_neg
        q_stage1[a1] += alpha_1 * pe_1
        
        # Stage 2 Update
        pe_2 = r - q_stage2[s_idx, a2]
        alpha_2 = lr_pos if pe_2 > 0 else lr_neg
        q_stage2[s_idx, a2] += alpha_2 * pe_2
        
        prev_a1 = a1
        prev_a2[s_idx] = a2
        
    return log_loss
```

### Model 3: Decoupled Stages with Dual Stickiness
This model treats the two stages as distinct cognitive processes with independent noise levels (`beta_s1` vs `beta_s2`) and habit strengths (`stick_s1` vs `stick_s2`). This allows for modeling a participant who might be very consistent in one stage (e.g., alien choice) but noisy in the other (e.g., spaceship choice).

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Decoupled Stages with Dual Stickiness.
    
    Treats the two stages as distinct decision processes with separate 
    noise levels (beta) and stickiness parameters. This allows the model 
    to capture different levels of exploration and habit strength for 
    navigation (Stage 1) versus harvesting (Stage 2).
    
    Parameters:
    learning_rate: [0, 1] Learning rate.
    beta_s1:       [0, 10] Inverse temperature for Stage 1.
    beta_s2:       [0, 10] Inverse temperature for Stage 2.
    stick_s1:      [-5, 5] Stickiness for Stage 1.
    stick_s2:      [-5, 5] Stickiness for Stage 2.
    """
    learning_rate, beta_s1, beta_s2, stick_s1, stick_s2 = model_parameters
    n_trials = len(action_1)
    
    q_stage1 = np.zeros(2) + 0.5
    q_stage2 = np.zeros((2, 2)) + 0.5
    
    log_loss = 0
    eps = 1e-10
    
    prev_a1 = -1
    prev_a2 = np.array([-1, -1])
    
    for trial in range(n_trials):
        a1 = action_1[trial]
        s_idx = state[trial]
        a2 = action_2[trial]
        r = reward[trial]
        
        if a1 == -1 or s_idx == -1 or a2 == -1:
            continue
            
        # --- Stage 1 ---
        logits_1 = beta_s1 * q_stage1
        if prev_a1 != -1:
            logits_1[prev_a1] += stick_s1
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + eps)
        
        # --- Stage 2 ---
        logits_2 = beta_s2 * q_stage2[s_idx]
        if prev_a2[s_idx] != -1:
            logits_2[prev_a2[s_idx]] += stick_s2
            
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[a2] + eps)
        
        # --- Updates ---
        target_s1 = q_stage2[s_idx, a2]
        q_stage1[a1] += learning_rate * (target_s1 - q_stage1[a1])
        
        q_stage2[s_idx, a2] += learning_rate * (r - q_stage2[s_idx, a2])
        
        prev_a1 = a1
        prev_a2[s_idx] = a2
        
    return log_loss
```