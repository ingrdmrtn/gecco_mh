Here are three new cognitive models for the two-step decision task.

### Model 1: Dual-Rate TD(lambda) with Stickiness
This model is a pure Model-Free learner (TD-learning) but distinguishes between the learning rates for the first stage (choosing a spaceship) and the second stage (choosing an alien). This allows the model to capture different rates of plasticity for spaceship preferences versus alien reward probabilities. It also includes an eligibility trace ($\lambda$) to modulate how much the second-stage outcome reinforces the first-stage choice.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Dual-Rate TD(lambda) with Choice Stickiness.
    
    A Model-Free learner that uses different learning rates for the first stage (spaceship choice)
    and the second stage (alien choice), allowing for differential plasticity.
    Includes eligibility traces and choice perseveration.
    
    Parameters:
    - lr_s1: [0, 1] Learning rate for Stage 1 (Spaceship) updates.
    - lr_s2: [0, 1] Learning rate for Stage 2 (Alien) updates.
    - beta: [0, 10] Inverse temperature.
    - lam: [0, 1] Eligibility trace decay parameter.
    - stickiness: [0, 10] Choice perseveration bonus for Stage 1.
    """
    lr_s1, lr_s2, beta, lam, stickiness = model_parameters
    n_trials = len(action_1)

    q_stage1 = np.zeros(2) + 0.5
    q_stage2 = np.zeros((2, 2)) + 0.5
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1

    for trial in range(n_trials):
        # Stage 1 Choice
        q_s1_mod = q_stage1.copy()
        if last_action_1 != -1:
            q_s1_mod[last_action_1] += stickiness
        
        exp_q1 = np.exp(beta * q_s1_mod)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        last_action_1 = a1
        
        # Stage 2 Choice
        s_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2[s_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        r = reward[trial]
        
        # Updates
        # Stage 1 TD Error (using Stage 2 value)
        pe_1 = q_stage2[s_idx, a2] - q_stage1[a1]
        q_stage1[a1] += lr_s1 * pe_1
        
        # Stage 2 TD Error (using Reward)
        pe_2 = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += lr_s2 * pe_2
        
        # Eligibility Trace Update for Stage 1
        # Updates Stage 1 value based on the Stage 2 outcome (Reward prediction error)
        q_stage1[a1] += lr_s1 * lam * pe_2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Dynamic Transition Hybrid with Stickiness
This model is a Hybrid (Model-Based + Model-Free) learner that does not assume fixed transition probabilities (e.g., 0.7/0.3). Instead, it estimates the transition matrix ($P(Planet|Spaceship)$) online based on observed transitions. This allows the model to adapt if the participant's belief about the spaceship structure changes or is being learned from scratch.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Dynamic Transition Hybrid Model with Stickiness.
    
    A Hybrid learner that estimates the transition matrix online rather than assuming it is fixed.
    Combines Model-Based values (using learned transitions) and Model-Free values.
    
    Parameters:
    - lr_val: [0, 1] Learning rate for Q-value updates (MF and Stage 2).
    - lr_trans: [0, 1] Learning rate for transition probability updates.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight (0 = Pure MF, 1 = Pure MB).
    - stickiness: [0, 10] Choice perseveration bonus.
    """
    lr_val, lr_trans, beta, w, stickiness = model_parameters
    n_trials = len(action_1)

    # Initialize Transition Matrix (subjective probabilities) with flat prior
    trans_probs = np.array([[0.5, 0.5], [0.5, 0.5]]) 
    
    q_mf = np.zeros(2) + 0.5
    q_s2 = np.zeros((2, 2)) + 0.5
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1

    for trial in range(n_trials):
        # MB Value Calculation
        # Q_MB(a) = sum(P(s'|a) * max(Q_s2(s', :)))
        max_q_s2 = np.max(q_s2, axis=1)
        q_mb = trans_probs @ max_q_s2
        
        # Net Value
        q_net = w * q_mb + (1 - w) * q_mf
        
        # Stickiness
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        # Stage 1 Choice
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        last_action_1 = a1
        
        # Stage 2 Choice
        s_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_s2[s_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        r = reward[trial]
        
        # Value Updates (TD)
        # MF Stage 1
        pe_1 = q_s2[s_idx, a2] - q_mf[a1]
        q_mf[a1] += lr_val * pe_1
        
        # Stage 2
        pe_2 = r - q_s2[s_idx, a2]
        q_s2[s_idx, a2] += lr_val * pe_2
        
        # MF Stage 1 Second Step (Implicit lambda=1 for MF component)
        q_mf[a1] += lr_val * pe_2
        
        # Transition Probability Update
        # P(s|a) <- P(s|a) + lr_trans * (1 - P(s|a))
        target = np.zeros(2)
        target[s_idx] = 1.0
        trans_probs[a1, :] += lr_trans * (target - trans_probs[a1, :])

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid with Lapse Rate
This is a standard Hybrid model (fixed transition matrix) that includes a "lapse" parameter ($\epsilon$). The lapse rate accounts for random behavioral noise, button presses, or attentional lapses that are not explained by the value-dependent softmax function. This mixture model helps robustly fit data where a participant generally follows a strategy but occasionally acts randomly.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Lapse Rate.
    
    A standard Hybrid (MB+MF) learner that includes a lapse parameter.
    The lapse parameter accounts for random errors or exploration not captured by softmax,
    mixing the softmax distribution with a uniform distribution.
    
    Parameters:
    - lr: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight (0 = Pure MF, 1 = Pure MB).
    - stickiness: [0, 10] Choice perseveration bonus.
    - epsilon: [0, 1] Lapse rate (probability of random choice).
    """
    lr, beta, w, stickiness, epsilon = model_parameters
    n_trials = len(action_1)

    # Fixed transition matrix for MB
    trans_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_mf = np.zeros(2) + 0.5
    q_s2 = np.zeros((2, 2)) + 0.5
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1

    for trial in range(n_trials):
        # MB Value
        max_q_s2 = np.max(q_s2, axis=1)
        q_mb = trans_matrix @ max_q_s2
        
        # Net Value
        q_net = w * q_mb + (1 - w) * q_mf
        
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        # Stage 1 Choice with Lapse
        exp_q1 = np.exp(beta * q_net)
        probs_1_soft = exp_q1 / np.sum(exp_q1)
        probs_1 = (1 - epsilon) * probs_1_soft + epsilon * 0.5
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        last_action_1 = a1
        
        # Stage 2 Choice with Lapse
        s_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_s2[s_idx, :])
        probs_2_soft = exp_q2 / np.sum(exp_q2)
        probs_2 = (1 - epsilon) * probs_2_soft + epsilon * 0.5
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        r = reward[trial]
        
        # Updates
        pe_1 = q_s2[s_idx, a2] - q_mf[a1]
        q_mf[a1] += lr * pe_1
        
        pe_2 = r - q_s2[s_idx, a2]
        q_s2[s_idx, a2] += lr * pe_2
        
        # MF Trace update (Implicit lambda=1)
        q_mf[a1] += lr * pe_2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```