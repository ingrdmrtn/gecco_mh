Here are three cognitive models implemented as Python functions, designed to explain the participant's behavior in the two-step task.

### Model 1: Model-Based with Transition-Dependent Stickiness
This model assumes the participant is a Model-Based learner who exhibits different perseveration biases depending on whether the previous transition was "Common" or "Rare". This captures the participant's reaction to the "surprise" of the transition, separate from the reward outcome.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Model-Based Learner with Transition-Dependent Stickiness.
    
    The participant uses a model-based strategy but their tendency to repeat 
    the previous Stage 1 choice depends on whether the previous trial's 
    transition was Common (0->0, 1->1) or Rare (0->1, 1->0).
    
    Parameters:
    - learning_rate: [0, 1] Update rate for Stage 2 alien values.
    - beta_1: [0, 10] Inverse temperature for Stage 1 choice.
    - beta_2: [0, 10] Inverse temperature for Stage 2 choice.
    - stick_common: [-5, 5] Bias to repeat Stage 1 choice after a Common transition.
    - stick_rare: [-5, 5] Bias to repeat Stage 1 choice after a Rare transition.
    """
    learning_rate, beta_1, beta_2, stick_common, stick_rare = model_parameters
    n_trials = len(action_1)
    
    # Fixed transition matrix: T[action, state]
    # A(0) -> X(0) is 0.7, A(0) -> Y(1) is 0.3
    # U(1) -> X(0) is 0.3, U(1) -> Y(1) is 0.7
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage2 = np.zeros((2, 2)) # Values for Aliens: [Planet, Alien]
    
    log_loss = 0.0
    eps = 1e-10
    
    last_action_1 = -1
    last_transition_type = -1 # 0 for Common, 1 for Rare
    
    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        # Stage 1 Policy (Model-Based)
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_stage1 = transition_matrix @ max_q_stage2
        
        # Add Stickiness based on previous transition type
        if last_action_1 != -1:
            if last_transition_type == 0: # Common
                q_stage1[last_action_1] += stick_common
            elif last_transition_type == 1: # Rare
                q_stage1[last_action_1] += stick_rare
                
        exp_q1 = np.exp(beta_1 * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        chosen_a1 = int(action_1[trial])
        log_loss -= np.log(probs_1[chosen_a1] + eps)
        
        # Determine current transition type for next trial
        current_state = int(state[trial])
        # Common: (0->0) or (1->1). Rare: (0->1) or (1->0).
        is_common = (chosen_a1 == current_state)
        current_transition_type = 0 if is_common else 1
        
        # Stage 2 Policy
        exp_q2 = np.exp(beta_2 * q_stage2[current_state])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        chosen_a2 = int(action_2[trial])
        log_loss -= np.log(probs_2[chosen_a2] + eps)
        
        # Learning Stage 2 Values
        r = reward[trial]
        delta = r - q_stage2[current_state, chosen_a2]
        q_stage2[current_state, chosen_a2] += learning_rate * delta
        
        # Update history
        last_action_1 = chosen_a1
        last_transition_type = current_transition_type
        
    return log_loss
```

### Model 2: Model-Based with Cumulative Stickiness Trace
This model addresses the rhythmic alternation behavior observed in the data. Instead of just "sticking" to the immediately previous choice, the participant maintains a decaying memory trace of past choices. This allows for stronger or more persistent alternation biases (if `stick_weight` is negative) that build up over trials.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Based Learner with Cumulative Stickiness Trace.
    
    Uses a decaying trace of past Stage 1 choices to bias current decisions.
    This can capture longer-term perseveration or alternation rhythms compared
    to simple 1-back stickiness.
    
    Parameters:
    - learning_rate: [0, 1] Update rate for Stage 2 alien values.
    - beta_1: [0, 10] Inverse temperature for Stage 1 choice.
    - beta_2: [0, 10] Inverse temperature for Stage 2 choice.
    - trace_decay: [0, 1] Decay rate of the choice trace (0 = only last choice, 1 = perfect memory).
    - stick_weight: [-5, 5] Weight of the trace in decision making (negative = alternation).
    """
    learning_rate, beta_1, beta_2, trace_decay, stick_weight = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage2 = np.zeros((2, 2))
    choice_trace = np.zeros(2)
    
    log_loss = 0.0
    eps = 1e-10
    
    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_stage1 = transition_matrix @ max_q_stage2
        
        # Apply cumulative stickiness bias
        q_stage1 += stick_weight * choice_trace
        
        exp_q1 = np.exp(beta_1 * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        chosen_a1 = int(action_1[trial])
        log_loss -= np.log(probs_1[chosen_a1] + eps)
        
        # Update Trace
        choice_trace *= trace_decay
        choice_trace[chosen_a1] += 1.0
        
        # Stage 2 Policy
        current_state = int(state[trial])
        exp_q2 = np.exp(beta_2 * q_stage2[current_state])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        chosen_a2 = int(action_2[trial])
        log_loss -= np.log(probs_2[chosen_a2] + eps)
        
        # Learning
        r = reward[trial]
        q_stage2[current_state, chosen_a2] += learning_rate * (r - q_stage2[current_state, chosen_a2])
        
    return log_loss
```

### Model 3: Hybrid Learner with Reward-Modulated Weighting
This model is a Hybrid (Model-Based + Model-Free) learner, but the weight $w$ determining the balance between the two systems changes dynamically based on the previous outcome. A win might make the participant rely more on their Model-Based plan (or vice versa), while a loss shifts the balance. It also includes a general stickiness parameter.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner with Reward-Modulated Mixing Weight.
    
    The participant combines Model-Based (MB) and Model-Free (MF) values for Stage 1.
    The mixing weight 'w' depends on whether the previous trial was rewarded.
    MF values for Stage 1 are updated via direct reinforcement (TD(1)-like).
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for both MF (Stage 1) and MB (Stage 2) updates.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - w_win: [0, 1] Weight of MB system after a Reward (1). (1 = Pure MB, 0 = Pure MF).
    - w_loss: [0, 1] Weight of MB system after a No-Reward (0).
    - stickiness: [-5, 5] General bias to repeat the previous Stage 1 choice.
    """
    learning_rate, beta_1, beta_2, w_win, w_loss, stickiness = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage2 = np.zeros((2, 2)) # MB components (Alien values)
    q_mf_stage1 = np.zeros(2)   # MF components (Spaceship values)
    
    log_loss = 0.0
    eps = 1e-10
    
    last_action_1 = -1
    last_reward = -1
    
    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        # Calculate MB values for Stage 1
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_mb_stage1 = transition_matrix @ max_q_stage2
        
        # Determine w based on previous reward
        if last_reward == 1:
            w = w_win
        elif last_reward == 0:
            w = w_loss
        else:
            w = 0.5 # Default for first trial
            
        # Combine MB and MF
        q_net_stage1 = w * q_mb_stage1 + (1 - w) * q_mf_stage1
        
        # Add Stickiness
        if last_action_1 != -1:
            q_net_stage1[last_action_1] += stickiness
            
        # Stage 1 Choice
        exp_q1 = np.exp(beta_1 * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        chosen_a1 = int(action_1[trial])
        log_loss -= np.log(probs_1[chosen_a1] + eps)
        
        # Stage 2 Choice
        current_state = int(state[trial])
        exp_q2 = np.exp(beta_2 * q_stage2[current_state])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        chosen_a2 = int(action_2[trial])
        log_loss -= np.log(probs_2[chosen_a2] + eps)
        
        # Learning
        r = reward[trial]
        
        # Update Stage 2 (MB component)
        q_stage2[current_state, chosen_a2] += learning_rate * (r - q_stage2[current_state, chosen_a2])
        
        # Update Stage 1 (MF component) - Direct reinforcement of spaceship choice
        q_mf_stage1[chosen_a1] += learning_rate * (r - q_mf_stage1[chosen_a1])
        
        last_action_1 = chosen_a1
        last_reward = r
        
    return log_loss
```