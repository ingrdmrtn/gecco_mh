Here are three new cognitive models for the two-step decision task, designed to capture the participant's behavior using mechanisms distinct from those already tested.

### Model 1: Accumulated Perseveration (Momentum) Model
This model hypothesizes that the participant's strong tendency to stick with a spaceship (as seen in the data streaks) is driven by a "choice trace" or momentum that builds up over repeated choices and decays slowly, rather than just a simple one-trial repetition bias.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Accumulated Perseveration (Momentum) Model.
    
    This model implements a 'choice trace' mechanism. Unlike simple stickiness which 
    only considers the previous trial, the choice trace accumulates when an action 
    is chosen and decays exponentially on every trial. This creates a longer-term 
    'momentum' or habit strength that biases the Stage 1 choice.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for Q-value updates.
    - beta_1: [0, 10] Inverse temperature for Stage 1 (Spaceships).
    - beta_2: [0, 10] Inverse temperature for Stage 2 (Aliens).
    - w: [0, 1] Weighting parameter (0 = Model-Free, 1 = Model-Based).
    - trace_decay: [0, 1] Decay rate of the choice trace (0 = instant decay, 1 = no decay).
    - trace_weight: [0, 5] Strength of the bias from the accumulated trace.
    """
    learning_rate, beta_1, beta_2, w, trace_decay, trace_weight = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    
    # Initialize choice trace (momentum) for the two spaceships
    choice_trace = np.zeros(2)
    
    for trial in range(n_trials):
        # Skip invalid trials
        if action_1[trial] == -1:
            continue

        # --- Stage 1: Spaceship Choice ---
        # Model-Based Value Calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add Accumulated Choice Trace Bias
        logits_1 = beta_1 * (q_net + trace_weight * choice_trace)
        
        exp_q1 = np.exp(logits_1 - np.max(logits_1)) # Subtract max for numerical stability
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        # Update Choice Trace
        # Decay both traces, then increment the chosen one
        choice_trace *= trace_decay
        choice_trace[action_1[trial]] += 1.0

        # --- Stage 2: Alien Choice ---
        state_idx = state[trial]
        logits_2 = beta_2 * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        # Stage 2 (TD)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Stage 1 (TD)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1

    eps = 1e-10
    # Only calculate loss for valid trials
    mask = (p_choice_1 > 0) & (p_choice_2 > 0)
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss
```

### Model 2: Dynamic Reference Point Model
This model incorporates the concept of "average reward reinforcement learning." The participant tracks a running average of rewards (a reference point) and updates values based on whether the outcome was better or worse than this average. This allows the model to adapt to blocks of high or low coin yield, normalizing the reward signal.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Dynamic Reference Point Model.
    
    This model assumes the participant evaluates rewards relative to a dynamic 
    reference point (running average of recent rewards). Instead of learning absolute 
    values (0 or 1), the participant learns values relative to their current 
    expectation of global reward rate.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for Q-values.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - w: [0, 1] Weighting between Model-Based and Model-Free.
    - stickiness: [0, 5] Standard one-trial perseveration bonus.
    - ref_lr: [0, 1] Learning rate for updating the reference point (average reward).
    """
    learning_rate, beta_1, beta_2, w, stickiness, ref_lr = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Initialize reference point at 0.5 (neutral expectation for binary 0/1)
    reference_point = 0.5 
    last_action_1 = -1
    
    for trial in range(n_trials):
        if action_1[trial] == -1:
            last_action_1 = -1
            continue

        # --- Stage 1 ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Apply Stickiness
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        logits_1 = beta_1 * q_net
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        # --- Stage 2 ---
        state_idx = state[trial]
        logits_2 = beta_2 * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Reward Transformation ---
        # Effective reward is the deviation from the reference point
        r_eff = reward[trial] - reference_point
        
        # --- Updates ---
        delta_stage2 = r_eff - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Update Reference Point
        reference_point += ref_lr * (reward[trial] - reference_point)
        
        last_action_1 = action_1[trial]

    eps = 1e-10
    mask = (p_choice_1 > 0) & (p_choice_2 > 0)
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss
```

### Model 3: Subjective Loss Aversion Model
This model proposes that the participant perceives the "0 coins" outcome not just as a lack of reward, but as a loss. By weighting the 0 outcome negatively, the model shifts the value scale, which can lead to different asymptotic Q-values and choice dynamics compared to standard 0/1 reinforcement, especially when combined with model-based planning.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Subjective Loss Aversion Model.
    
    This model transforms the binary reward signal into a subjective utility. 
    It posits that the participant treats '0 coins' as a loss rather than a neutral 
    outcome. The magnitude of this loss is controlled by 'loss_weight'.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta_1: [0, 10] Inverse temperature Stage 1.
    - beta_2: [0, 10] Inverse temperature Stage 2.
    - w: [0, 1] MB/MF weight.
    - stickiness: [0, 5] Perseveration bonus.
    - loss_weight: [0, 5] The negative weight applied to 0-reward outcomes (e.g., if 2.0, 0 coins = -2.0 utility).
    """
    learning_rate, beta_1, beta_2, w, stickiness, loss_weight = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        if action_1[trial] == -1:
            last_action_1 = -1
            continue

        # --- Stage 1 ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        logits_1 = beta_1 * q_net
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        # --- Stage 2 ---
        state_idx = state[trial]
        logits_2 = beta_2 * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Subjective Reward Transformation ---
        # 1 coin remains 1.0, but 0 coins becomes negative
        if reward[trial] == 1:
            r_subj = 1.0
        else:
            r_subj = -1.0 * loss_weight
        
        # --- Updates ---
        delta_stage2 = r_subj - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        last_action_1 = action_1[trial]

    eps = 1e-10
    mask = (p_choice_1 > 0) & (p_choice_2 > 0)
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss
```