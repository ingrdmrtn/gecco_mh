Here are three new cognitive models for the two-step task, designed to capture different potential mechanisms underlying the participant's behavior, such as separate learning stages, asymmetric reward processing, and persistent choice habits.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Separate Stage Learning Rates and Stickiness.
    
    This model posits that the participant learns the value of abstract spaceship 
    choices (Stage 1) at a different rate than concrete alien choices (Stage 2).
    It also includes a stickiness parameter to capture choice perseveration in Stage 1.
    
    Parameters:
    lr_1: [0, 1] Learning rate for Stage 1 (Spaceships).
    lr_2: [0, 1] Learning rate for Stage 2 (Aliens).
    beta_1: [0, 10] Inverse temperature for Stage 1.
    beta_2: [0, 10] Inverse temperature for Stage 2.
    w: [0, 1] Weight of model-based values in Stage 1.
    lam: [0, 1] Eligibility trace parameter connecting Stage 2 outcome to Stage 1.
    stick: [0, 5] Stickiness parameter (bonus for repeating the last Stage 1 choice).
    """
    lr_1, lr_2, beta_1, beta_2, w, lam, stick = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track the previous choice for stickiness
    last_action_1 = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Calculate logits and add stickiness to the previously chosen option
        logits_1 = beta_1 * q_net_1
        if last_action_1 != -1:
            logits_1[last_action_1] += stick
            
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Policy ---
        state_idx = state[trial]
        logits_2 = beta_2 * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        # Stage 1 TD update (using lr_1)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr_1 * delta_stage1
        
        # Stage 2 TD update (using lr_2)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr_2 * delta_stage2
        
        # Eligibility trace update for Stage 1 (using lr_1)
        q_stage1_mf[action_1[trial]] += lr_1 * lam * delta_stage2
        
        last_action_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Asymmetric Learning Rates and Counterfactual Updating.
    
    This model assumes the participant updates values differently depending on whether
    the prediction error is positive (better than expected) or negative (worse).
    It also updates the unchosen alien in Stage 2 assuming anti-correlated rewards.
    
    Parameters:
    lr_pos: [0, 1] Learning rate for positive prediction errors.
    lr_neg: [0, 1] Learning rate for negative prediction errors.
    lr_cf: [0, 1] Learning rate for counterfactual updates (unchosen option).
    beta_1: [0, 10] Inverse temperature for Stage 1.
    beta_2: [0, 10] Inverse temperature for Stage 2.
    w: [0, 1] Weight of model-based values in Stage 1.
    lam: [0, 1] Eligibility trace parameter.
    """
    lr_pos, lr_neg, lr_cf, beta_1, beta_2, w, lam = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_1 * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Policy ---
        state_idx = state[trial]
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        # Stage 1 Update (Asymmetric)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        lr_1 = lr_pos if delta_stage1 > 0 else lr_neg
        q_stage1_mf[action_1[trial]] += lr_1 * delta_stage1
        
        # Stage 2 Update (Asymmetric)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        lr_2 = lr_pos if delta_stage2 > 0 else lr_neg
        q_stage2_mf[state_idx, action_2[trial]] += lr_2 * delta_stage2
        
        # Eligibility Trace Update (Asymmetric based on outcome surprise)
        lr_1_trace = lr_pos if delta_stage2 > 0 else lr_neg
        q_stage1_mf[action_1[trial]] += lr_1_trace * lam * delta_stage2
        
        # Counterfactual Update (Unchosen Alien)
        # Assume if we got Reward R, the other alien would have given 1-R
        unchosen_a2 = 1 - action_2[trial]
        cf_reward = 1 - reward[trial] 
        delta_cf = cf_reward - q_stage2_mf[state_idx, unchosen_a2]
        q_stage2_mf[state_idx, unchosen_a2] += lr_cf * delta_cf

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Decaying Choice Trace (Persistent Stickiness).
    
    Instead of a binary stickiness (repeating the very last choice), this model 
    maintains a continuous 'choice trace' that accumulates and decays. This 
    simulates a stronger habit formation that persists over multiple trials.
    
    Parameters:
    lr: [0, 1] Learning rate.
    beta_1: [0, 10] Inverse temperature for Stage 1.
    beta_2: [0, 10] Inverse temperature for Stage 2.
    w: [0, 1] Weight of model-based values.
    lam: [0, 1] Eligibility trace.
    stick_weight: [0, 5] Importance of the choice trace in decision making.
    stick_decay: [0, 1] Decay rate of the choice trace (0 = last choice only, 1 = no forgetting).
    """
    lr, beta_1, beta_2, w, lam, stick_weight, stick_decay = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Initialize choice trace for Stage 1 options
    choice_trace = np.zeros(2)
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add the accumulated choice trace to the logits
        logits_1 = beta_1 * q_net_1 + stick_weight * choice_trace
        
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Policy ---
        state_idx = state[trial]
        logits_2 = beta_2 * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr * delta_stage2
        
        q_stage1_mf[action_1[trial]] += lr * lam * delta_stage2
        
        # --- Update Choice Trace ---
        # Decay the existing trace
        choice_trace *= stick_decay
        # Reinforce the chosen option
        choice_trace[action_1[trial]] += 1.0

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```