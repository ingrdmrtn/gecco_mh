Here are three new cognitive models, expressed as Python functions, designed to explain the participant's behavior in the two-step task.

### Model 1: Hybrid Model with Forgetting and Stickiness
This model extends the best-performing "Hybrid with Forgetting" model by adding a **Stickiness** (perseverance) parameter to Stage 1 choices. The participant data shows a strong tendency to repeat the spaceship choice (Action 0), even after failures. This model captures that habit formation explicitly, while the forgetting mechanism handles the drifting reward probabilities of the aliens.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Forgetting and Stage 1 Stickiness.
    Combines Model-Based and Model-Free learning with a forgetting mechanism 
    for unchosen options and a perseverance (stickiness) bonus for the 
    previously chosen spaceship.
    
    Parameters:
    - learning_rate: [0, 1] Update rate for Q-values.
    - beta: [0, 10] Inverse temperature for choices.
    - w: [0, 1] Weight for Model-Based values in Stage 1.
    - forget_rate: [0, 1] Decay rate for unchosen Q-values.
    - stickiness: [0, 10] Bonus added to the logits of the repeated Stage 1 choice.
    """
    learning_rate, beta, w, forget_rate, stickiness = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1
    
    log_loss = 0.0
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_s1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        logits_1 = beta * q_net_s1
        
        # Add stickiness to the previously chosen action
        if prev_action_1 != -1:
            logits_1[prev_action_1] += stickiness
            
        probs_1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 /= np.sum(probs_1)
        
        log_loss -= np.log(probs_1[action_1[trial]] + 1e-10)
        
        # --- Stage 2 Policy ---
        state_idx = state[trial]
        q_s2 = q_stage2_mf[state_idx]
        logits_2 = beta * q_s2
        probs_2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 /= np.sum(probs_2)
        
        log_loss -= np.log(probs_2[action_2[trial]] + 1e-10)
        
        # --- Updates ---
        r = reward[trial]
        a2 = action_2[trial]
        a1 = action_1[trial]
        
        # Stage 2 Update (Standard Q-Learning)
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Stage 2 Forgetting (Unchosen alien decays)
        unchosen_a2 = 1 - a2
        q_stage2_mf[state_idx, unchosen_a2] *= (1.0 - forget_rate)
        
        # Stage 1 Update (TD-Learning using Stage 2 value as target)
        target_stage1 = q_stage2_mf[state_idx, a2]
        delta_stage1 = target_stage1 - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Stage 1 Forgetting
        unchosen_a1 = 1 - a1
        q_stage1_mf[unchosen_a1] *= (1.0 - forget_rate)
        
        prev_action_1 = a1
        
    return log_loss
```

### Model 2: Dual-Learning-Rate Hybrid Model (Direct Reinforcement)
This model hypothesizes that the participant learns the value of spaceships (Stage 1) and aliens (Stage 2) at different rates and via different mechanisms. Specifically, it uses **Direct Reinforcement** for Stage 1, updating the spaceship value based directly on the reward received (ignoring the transition structure), which creates a stronger "Model-Free" habit than standard TD-learning. It also includes forgetting to manage the drifting environment.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Dual-Learning-Rate Hybrid Model with Forgetting.
    Separates the learning rates for Stage 1 (Spaceships) and Stage 2 (Aliens).
    Stage 1 MF values are updated directly from Reward (Direct Reinforcement),
    skipping the Stage 2 value dependency. This captures distinct timescales
    for learning high-level choices vs low-level details.
    
    Parameters:
    - lr_s1: [0, 1] Learning rate for Stage 1 MF values (Direct Reward update).
    - lr_s2: [0, 1] Learning rate for Stage 2 MF values.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weight for Model-Based values.
    - forget_rate: [0, 1] Decay rate for unchosen Q-values (both stages).
    """
    lr_s1, lr_s2, beta, w, forget_rate = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    log_loss = 0.0
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_s1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net_s1
        probs_1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 /= np.sum(probs_1)
        log_loss -= np.log(probs_1[action_1[trial]] + 1e-10)
        
        # --- Stage 2 Policy ---
        state_idx = state[trial]
        logits_2 = beta * q_stage2_mf[state_idx]
        probs_2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 /= np.sum(probs_2)
        log_loss -= np.log(probs_2[action_2[trial]] + 1e-10)
        
        # --- Updates ---
        r = reward[trial]
        a1 = action_1[trial]
        a2 = action_2[trial]
        
        # Stage 2 Update (Standard Q, uses lr_s2)
        q_stage2_mf[state_idx, a2] += lr_s2 * (r - q_stage2_mf[state_idx, a2])
        q_stage2_mf[state_idx, 1-a2] *= (1.0 - forget_rate)
        
        # Stage 1 Update (Direct Reward, uses lr_s1)
        # Updates spaceship value based on final outcome, ignoring the planet visited.
        q_stage1_mf[a1] += lr_s1 * (r - q_stage1_mf[a1])
        q_stage1_mf[1-a1] *= (1.0 - forget_rate)
        
    return log_loss
```

### Model 3: Independent Beta Hybrid Model
Instead of a mixing weight `w` that forces a trade-off between Model-Based (MB) and Model-Free (MF) values, this model assigns separate inverse temperatures (**Betas**) to the MB and MF components in Stage 1. This allows the model to capture states where the participant might be highly sensitive to habit (high `beta_mf`) while simultaneously being sensitive to structure (high `beta_mb`), or ignore one entirely.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Independent Beta Hybrid Model with Forgetting.
    Instead of a mixing weight 'w', this model uses separate inverse temperatures 
    (betas) for the Model-Based and Model-Free components in Stage 1. 
    This allows the model to independently scale the influence of MB structure 
    and MF experience. Stage 2 is driven by the MF beta.
    
    Parameters:
    - learning_rate: [0, 1] Update rate for Q-values.
    - beta_mb: [0, 10] Inverse temperature for Model-Based values (Stage 1).
    - beta_mf: [0, 10] Inverse temperature for Model-Free values (Stage 1 and 2).
    - forget_rate: [0, 1] Decay rate for unchosen Q-values.
    """
    learning_rate, beta_mb, beta_mf, forget_rate = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    log_loss = 0.0
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Independent scaling of MB and MF values in the logit
        logits_1 = (beta_mb * q_stage1_mb) + (beta_mf * q_stage1_mf)
        
        probs_1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 /= np.sum(probs_1)
        log_loss -= np.log(probs_1[action_1[trial]] + 1e-10)
        
        # --- Stage 2 Policy ---
        state_idx = state[trial]
        # Stage 2 is purely MF-driven
        logits_2 = beta_mf * q_stage2_mf[state_idx]
        probs_2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 /= np.sum(probs_2)
        log_loss -= np.log(probs_2[action_2[trial]] + 1e-10)
        
        # --- Updates ---
        r = reward[trial]
        a2 = action_2[trial]
        a1 = action_1[trial]
        
        # Stage 2 Update
        delta2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta2
        q_stage2_mf[state_idx, 1-a2] *= (1.0 - forget_rate)
        
        # Stage 1 Update (Standard TD)
        target_s1 = q_stage2_mf[state_idx, a2]
        delta1 = target_s1 - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta1
        q_stage1_mf[1-a1] *= (1.0 - forget_rate)
        
    return log_loss
```