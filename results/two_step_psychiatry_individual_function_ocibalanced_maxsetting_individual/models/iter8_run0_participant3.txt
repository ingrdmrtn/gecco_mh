Here are the 3 cognitive models based on the participant data and requirements.

### Cognitive Model 1: Hybrid Model with Static Side Bias
This model hypothesizes that the participant has an intrinsic, static preference for one spaceship over the other (e.g., due to color, position, or superstition), which acts as a bias on the Q-values regardless of the reward history. This is motivated by the participant's tendency to hold onto specific choices (like spaceship 1) for long durations.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Static Side Bias.
    
    Incorporates a fixed bias parameter that shifts the net Q-values for the 
    first-stage choice. This captures intrinsic preferences for one option 
    independent of learned value.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate for value updates.
    beta: [0, 10] - Inverse temperature for softmax choice.
    w: [0, 1] - Weight for Model-Based values (0 = pure MF, 1 = pure MB).
    bias: [-1, 1] - Static bias. Positive favors action 1, negative favors action 0.
    """
    learning_rate, beta, w, bias = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # state x action

    for trial in range(n_trials):
        # Skip missing data
        if action_1[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Mix MB and MF values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Apply static bias
        # We add bias to action 1. If bias > 0, action 1 is preferred.
        # If bias < 0, action 0 is preferred (relative to action 1).
        q_net_biased = q_net.copy()
        q_net_biased[1] += bias
        
        exp_q1 = np.exp(beta * q_net_biased)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        chosen_a1 = action_1[trial]
        # TD(0) update for Stage 1 MF
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[chosen_a1]
        q_stage1_mf[chosen_a1] += learning_rate * delta_stage1
        
        chosen_a2 = action_2[trial]
        # TD(0) update for Stage 2 MF
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, chosen_a2]
        q_stage2_mf[state_idx, chosen_a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2: Hybrid Model with Epsilon-Greedy Lapse
This model posits that the participant generally follows a softmax policy based on hybrid values but occasionally "lapses" (chooses randomly) with probability `epsilon`. This accounts for noise or exploratory choices that do not fit the value curve, which helps explain occasional deviations (like the switch in trial 36 or 40) that are not strictly value-driven.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Epsilon-Greedy Lapse.
    
    Assumes the participant mostly follows a softmax policy but occasionally 
    chooses uniformly at random (lapse). This mixture model helps account for 
    noise or unguided exploration.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate.
    beta: [0, 10] - Inverse temperature for the softmax component.
    w: [0, 1] - Weight for Model-Based values.
    epsilon: [0, 1] - Probability of a random choice (lapse rate).
    """
    learning_rate, beta, w, epsilon = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        if action_1[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax component
        exp_q1 = np.exp(beta * q_net)
        probs_1_softmax = exp_q1 / np.sum(exp_q1)
        
        # Mixture with uniform random (epsilon-greedy logic applied to probability)
        # Uniform probability is 0.5 for 2 options.
        probs_1 = (1 - epsilon) * probs_1_softmax + epsilon * 0.5
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2_softmax = exp_q2 / np.sum(exp_q2)
        
        # Apply lapse to stage 2 as well
        probs_2 = (1 - epsilon) * probs_2_softmax + epsilon * 0.5
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        chosen_a1 = action_1[trial]
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[chosen_a1]
        q_stage1_mf[chosen_a1] += learning_rate * delta_stage1
        
        chosen_a2 = action_2[trial]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, chosen_a2]
        q_stage2_mf[state_idx, chosen_a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3: Hybrid Model with Habitualization (Dynamic Weight)
This model assumes the participant shifts from a goal-directed (Model-Based) strategy to a habitual (Model-Free) strategy over time. The weight `w` starts at `w_init` and decays on each trial, simulating the process of "habitualization" where the computationally expensive planning is replaced by learned cache values as the task becomes familiar.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Habitualization (Dynamic Weight).
    
    The control weight 'w' decays exponentially over trials, modeling a 
    transition from Model-Based (planning) to Model-Free (habitual) control 
    as the participant gains experience.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate.
    beta: [0, 10] - Inverse temperature.
    w_init: [0, 1] - Initial weight for Model-Based values at trial 0.
    w_decay: [0, 1] - Rate of decay for w per trial (w_t = w_init * (1 - w_decay)^t).
    """
    learning_rate, beta, w_init, w_decay = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        if action_1[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Calculate dynamic w for this trial
        # w decays from w_init towards 0
        w_eff = w_init * ((1.0 - w_decay) ** trial)
        
        q_net = w_eff * q_stage1_mb + (1 - w_eff) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        chosen_a1 = action_1[trial]
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[chosen_a1]
        q_stage1_mf[chosen_a1] += learning_rate * delta_stage1
        
        chosen_a2 = action_2[trial]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, chosen_a2]
        q_stage2_mf[state_idx, chosen_a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```