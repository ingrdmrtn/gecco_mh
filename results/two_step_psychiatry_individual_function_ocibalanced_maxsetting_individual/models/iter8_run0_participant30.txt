Here are three new cognitive models for the two-step decision task.

### Model 1: Dual-Stage Fully Parameterized Q-Learning
This model posits that the participant employs distinct learning dynamics for the two stages of the task. Stage 1 (choosing a spaceship) is an abstract navigation task, while Stage 2 (choosing an alien) is a direct bandit task. This model allows for separate learning rates, noise levels (beta), and perseveration tendencies for each stage, connected by an eligibility trace.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Dual-Stage Fully Parameterized Q-Learning.
    Separates learning rates, inverse temperatures (betas), and perseveration
    for Stage 1 (Spaceships) and Stage 2 (Aliens) to capture distinct 
    plasticity and exploration profiles at each level.
    
    Parameters:
    lr_s1: Learning rate for Stage 1 [0,1]
    lr_s2: Learning rate for Stage 2 [0,1]
    beta_s1: Inverse temperature for Stage 1 [0,10]
    beta_s2: Inverse temperature for Stage 2 [0,10]
    lambda_eligibility: Eligibility trace decay [0,1]
    decay: Decay rate for Q-values [0,1]
    pers_s1: Perseveration for Stage 1 [0,5]
    pers_s2: Perseveration for Stage 2 [0,5]
    """
    lr_s1, lr_s2, beta_s1, beta_s2, lambda_eligibility, decay, pers_s1, pers_s2 = model_parameters
    n_trials = len(action_1)
    
    q_s1 = np.full(2, 0.5)
    q_s2 = np.full((2, 2), 0.5)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1
    last_action_2 = np.full(2, -1)
    
    for trial in range(n_trials):
        # Handle missing data
        if action_1[trial] == -1:
            last_action_1 = -1
            last_action_2 = np.full(2, -1)
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # Decay Q-values to baseline (0.5)
        q_s1 = q_s1 * (1 - decay) + 0.5 * decay
        q_s2 = q_s2 * (1 - decay) + 0.5 * decay

        # Stage 1 Policy
        q_net_s1 = q_s1.copy()
        if last_action_1 != -1:
            q_net_s1[last_action_1] += pers_s1
            
        exp_q1 = np.exp(beta_s1 * q_net_s1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        # Stage 2 Policy
        s_idx = state[trial]
        q_net_s2 = q_s2[s_idx].copy()
        if last_action_2[s_idx] != -1:
            q_net_s2[last_action_2[s_idx]] += pers_s2
            
        exp_q2 = np.exp(beta_s2 * q_net_s2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # Updates
        # TD Error 1: Q_s2(actual) - Q_s1(chosen)
        delta_1 = q_s2[s_idx, action_2[trial]] - q_s1[action_1[trial]]
        q_s1[action_1[trial]] += lr_s1 * delta_1
        
        # TD Error 2: Reward - Q_s2(chosen)
        delta_2 = reward[trial] - q_s2[s_idx, action_2[trial]]
        q_s2[s_idx, action_2[trial]] += lr_s2 * delta_2
        
        # Eligibility Trace: Update Q_s1 based on Stage 2 error
        q_s1[action_1[trial]] += lr_s1 * lambda_eligibility * delta_2
        
        last_action_1 = action_1[trial]
        last_action_2[s_idx] = action_2[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Counterfactual Hybrid Model
This model combines Model-Based and Model-Free learning with a specific "Counterfactual" update mechanism. It assumes the participant infers the outcome of the unchosen alien: if the chosen alien yielded a reward, the unchosen one likely would not have (and vice versa). This updates the unchosen action's value towards `1 - Reward`.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Counterfactual Hybrid Model.
    Combines Model-Based and Model-Free learning.
    Includes counterfactual updating for the unchosen alien in Stage 2,
    assuming anti-correlated rewards (if chosen=0, unchosen=1).
    
    Parameters:
    learning_rate: Learning rate for chosen actions [0,1]
    lr_cf: Counterfactual learning rate for unchosen actions [0,1]
    beta: Inverse temperature [0,10]
    w: Mixing weight (0=MF, 1=MB) [0,1]
    lambda_eligibility: Eligibility trace [0,1]
    pers_s1: Perseveration for Stage 1 [0,5]
    pers_s2: Perseveration for Stage 2 [0,5]
    """
    learning_rate, lr_cf, beta, w, lambda_eligibility, pers_s1, pers_s2 = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_s1_mf = np.full(2, 0.5)
    q_s2 = np.full((2, 2), 0.5)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1
    last_action_2 = np.full(2, -1)
    
    for trial in range(n_trials):
        if action_1[trial] == -1:
            last_action_1 = -1
            last_action_2 = np.full(2, -1)
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # Stage 1 Policy (Hybrid: w * MB + (1-w) * MF)
        max_q_s2 = np.max(q_s2, axis=1)
        q_s1_mb = transition_matrix @ max_q_s2
        
        q_net_s1 = w * q_s1_mb + (1 - w) * q_s1_mf
        if last_action_1 != -1:
            q_net_s1[last_action_1] += pers_s1
            
        exp_q1 = np.exp(beta * q_net_s1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Stage 2 Policy
        s_idx = state[trial]
        q_net_s2 = q_s2[s_idx].copy()
        if last_action_2[s_idx] != -1:
            q_net_s2[last_action_2[s_idx]] += pers_s2
            
        exp_q2 = np.exp(beta * q_net_s2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # Updates
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]
        
        # MF Stage 1 Update
        delta_1 = q_s2[s_idx, a2] - q_s1_mf[a1]
        q_s1_mf[a1] += learning_rate * delta_1
        
        # Stage 2 Update (Chosen)
        delta_2 = r - q_s2[s_idx, a2]
        q_s2[s_idx, a2] += learning_rate * delta_2
        
        # Stage 2 Update (Unchosen - Counterfactual)
        unchosen_a2 = 1 - a2
        r_cf = 1.0 - r # Assume anti-correlated outcome
        delta_cf = r_cf - q_s2[s_idx, unchosen_a2]
        q_s2[s_idx, unchosen_a2] += lr_cf * delta_cf
        
        # Eligibility Trace
        q_s1_mf[a1] += learning_rate * lambda_eligibility * delta_2
        
        last_action_1 = a1
        last_action_2[s_idx] = a2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Asymmetric Eligibility Trace Model
This model modifies the connection between stages (the eligibility trace) based on the outcome valence. It tests the hypothesis that the participant assigns credit to the spaceship choice differently when they get a reward (Win) versus when they get no reward (Loss), reflecting an attribution bias.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Asymmetric Eligibility Trace Model.
    A Model-Free learner where the eligibility trace (connection between Stage 2 outcome 
    and Stage 1 choice) depends on whether the outcome was a Reward or Omission.
    This allows for differential credit assignment for wins vs losses.
    
    Parameters:
    learning_rate: Learning rate [0,1]
    beta: Inverse temperature [0,10]
    lambda_pos: Eligibility trace for rewarded trials [0,1]
    lambda_neg: Eligibility trace for unrewarded trials [0,1]
    decay: Decay rate for Q-values [0,1]
    pers_s1: Perseveration for Stage 1 [0,5]
    pers_s2: Perseveration for Stage 2 [0,5]
    """
    learning_rate, beta, lambda_pos, lambda_neg, decay, pers_s1, pers_s2 = model_parameters
    n_trials = len(action_1)
    
    q_s1 = np.full(2, 0.5)
    q_s2 = np.full((2, 2), 0.5)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1
    last_action_2 = np.full(2, -1)
    
    for trial in range(n_trials):
        if action_1[trial] == -1:
            last_action_1 = -1
            last_action_2 = np.full(2, -1)
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        q_s1 = q_s1 * (1 - decay) + 0.5 * decay
        q_s2 = q_s2 * (1 - decay) + 0.5 * decay

        # Stage 1
        q_net_s1 = q_s1.copy()
        if last_action_1 != -1:
            q_net_s1[last_action_1] += pers_s1
        exp_q1 = np.exp(beta * q_net_s1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        # Stage 2
        s_idx = state[trial]
        q_net_s2 = q_s2[s_idx].copy()
        if last_action_2[s_idx] != -1:
            q_net_s2[last_action_2[s_idx]] += pers_s2
        exp_q2 = np.exp(beta * q_net_s2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # Updates
        delta_1 = q_s2[s_idx, action_2[trial]] - q_s1[action_1[trial]]
        q_s1[action_1[trial]] += learning_rate * delta_1
        
        delta_2 = reward[trial] - q_s2[s_idx, action_2[trial]]
        q_s2[s_idx, action_2[trial]] += learning_rate * delta_2
        
        # Asymmetric Eligibility Trace
        current_lambda = lambda_pos if reward[trial] == 1 else lambda_neg
        q_s1[action_1[trial]] += learning_rate * current_lambda * delta_2
        
        last_action_1 = action_1[trial]
        last_action_2[s_idx] = action_2[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```