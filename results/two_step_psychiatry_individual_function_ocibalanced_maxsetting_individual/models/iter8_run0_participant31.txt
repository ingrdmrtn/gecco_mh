Here are three cognitive models analyzing the participant's behavior in the two-step task.

### Cognitive Model 1: Curiosity-Driven Exploration
**Observation:** The participant exhibits a high rate of switching between spaceships (Action 1) across trials, often regardless of the previous outcome (e.g., Win-Switch behavior).
**Hypothesis:** The participant is driven by a "curiosity" or "boredom" mechanism that increases the value of an option as time passes since it was last chosen. This favors exploring the unchosen option, explaining the frequent switching.
**Mechanism:** A time-dependent exploration bonus `phi * (trial - last_chosen_trial)` is added to the Q-values (or logits) of each option in Stage 1.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Curiosity-Driven Exploration Model.
    
    Adds an exploration bonus to the Stage 1 choice values based on the time 
    elapsed since an option was last chosen. This captures strategies that 
    favor switching or sampling the environment (curiosity/boredom).
    
    Parameters:
    - lr: [0, 1] Learning rate for value updates.
    - beta: [0, 10] Inverse temperature (randomness) for choices.
    - w: [0, 1] Weight of Model-Based system (0=MF, 1=MB).
    - stick: [-5, 5] Standard 1-back stickiness (or alternation if negative).
    - phi: [0, 5] Exploration bonus scaling factor.
    """
    lr, beta, w, stick, phi = model_parameters
    n_trials = len(action_1)
    
    # Transition matrix: A(0)->X(0) 70%, U(1)->Y(1) 70%
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1
    # Track when each action was last chosen. Initialize to -1.
    last_chosen_time = np.array([-1.0, -1.0]) 
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Calculate exploration bonus
        # If never chosen, treat gap as current trial count (or 0 for first trial)
        time_gaps = np.zeros(2)
        for a in range(2):
            if last_chosen_time[a] == -1:
                time_gaps[a] = 0 # No bonus for initial state
            else:
                time_gaps[a] = trial - last_chosen_time[a]
        
        logits = q_net + phi * time_gaps
        
        # Add stickiness
        if last_action_1 != -1:
            logits[last_action_1] += stick
            
        exp_q1 = np.exp(beta * logits)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Update last chosen time
        last_chosen_time[action_1[trial]] = trial
        
        # --- Stage 2 Policy ---
        state_idx = state[trial]
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        # Stage 1 MF Update (TD Learning)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr * delta_stage1
        
        # Stage 2 MF Update (Reward Learning)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr * delta_stage2
        
        last_action_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2: Transition-Gated Model-Free Learning
**Observation:** Standard Model-Free (MF) learning updates Stage 1 values based on whatever outcome occurred. However, outcomes following "Rare" transitions are noisy predictors of the chosen spaceship's value.
**Hypothesis:** The participant might filter or weight MF updates differently depending on whether the transition was Common or Rare. For instance, they might suppress learning from Rare transitions ("flukes") or learn differently from them.
**Mechanism:** Two separate learning rates for Stage 1 MF updates: `lr_common` (used after Common transitions) and `lr_rare` (used after Rare transitions). A separate `lr_stage2` is used for the second stage.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Transition-Gated Model-Free Learning.
    
    Applies different learning rates to the Stage 1 Model-Free update depending 
    on whether the transition observed was Common or Rare. This allows the 
    agent to gate the influence of 'surprising' transitions on cached values.
    
    Parameters:
    - lr_common: [0, 1] Stage 1 MF learning rate after Common transitions.
    - lr_rare: [0, 1] Stage 1 MF learning rate after Rare transitions.
    - lr_s2: [0, 1] Learning rate for Stage 2 (reward learning).
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Model-Based weight.
    - stick: [-5, 5] Stickiness.
    """
    lr_common, lr_rare, lr_s2, beta, w, stick = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits = q_net.copy()
        if last_action_1 != -1:
            logits[last_action_1] += stick
            
        exp_q1 = np.exp(beta * logits)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Policy ---
        state_idx = state[trial]
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        # Determine transition type
        # 0->0 and 1->1 are Common. 0->1 and 1->0 are Rare.
        is_common = (action_1[trial] == state_idx)
        
        # Stage 1 Update: Select LR based on transition type
        current_lr_s1 = lr_common if is_common else lr_rare
        
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += current_lr_s1 * delta_stage1
        
        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr_s2 * delta_stage2
        
        last_action_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3: Action-Specific Stickiness
**Observation:** The participant may have different perseverative tendencies for the two spaceships. For example, they might be prone to "looping" on Spaceship A but switching away from Spaceship U, or vice versa.
**Hypothesis:** Stickiness is not a single global parameter but is specific to the action being repeated.
**Mechanism:** Separate stickiness parameters `stick_0` (for repeating Action 0) and `stick_1` (for repeating Action 1). This allows the model to capture asymmetric switching behavior.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Action-Specific Stickiness.
    
    Uses separate stickiness parameters for each Stage 1 action. This allows
    the model to capture if the participant is more likely to persevere on 
    one spaceship option than the other (e.g., sticky on A, switchy on U).
    
    Parameters:
    - lr: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Model-Based weight.
    - stick_0: [-5, 5] Stickiness bias for Action 0 (Spaceship A).
    - stick_1: [-5, 5] Stickiness bias for Action 1 (Spaceship U).
    """
    lr, beta, w, stick_0, stick_1 = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits = q_net.copy()
        
        # Apply specific stickiness based on which action was last taken
        if last_action_1 == 0:
            logits[0] += stick_0
        elif last_action_1 == 1:
            logits[1] += stick_1
            
        exp_q1 = np.exp(beta * logits)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Policy ---
        state_idx = state[trial]
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr * delta_stage2
        
        last_action_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```