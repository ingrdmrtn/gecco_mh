Here are three new cognitive models for the two-step task.

### Model 1: Hybrid Model with Asymmetric Learning and Split Perseveration
This model hypothesizes that the participant combines Model-Based and Model-Free strategies (`w`), but learns differently from positive versus negative prediction errors (`lr_pos`, `lr_neg`). It also distinguishes between perseveration after wins versus losses (`pers_win`, `pers_loss`), allowing for strategies like "Win-Stay, Lose-Shift" to be captured explicitly alongside value learning.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Asymmetric Learning Rates and Outcome-Dependent Perseveration.
    
    Combines Model-Based and Model-Free values (weighted by w).
    Uses separate learning rates for positive and negative prediction errors to capture 
    optimism/pessimism or confirmation bias.
    Applies outcome-dependent perseveration (Win-Stay, Lose-Stay/Shift).
    
    Parameters:
    - lr_pos: [0, 1] Learning rate for positive prediction errors.
    - lr_neg: [0, 1] Learning rate for negative prediction errors.
    - beta1: [0, 10] Inverse temperature for Stage 1 choice.
    - beta2: [0, 10] Inverse temperature for Stage 2 choice.
    - w: [0, 1] Weight for Model-Based values (0=MF, 1=MB).
    - pers_win: [0, 5] Perseveration bonus applied to the previous choice after a reward.
    - pers_loss: [0, 5] Perseveration bonus applied to the previous choice after no reward.
    """
    lr_pos, lr_neg, beta1, beta2, w, pers_win, pers_loss = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values initialization
    q_stage1_mf = np.ones(2) * 0.5
    q_stage2_mf = np.ones((2, 2)) * 0.5
    
    last_action_1 = -1
    last_reward = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based Value Calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value Combination
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta1 * q_net
        
        # Apply Perseveration
        if last_action_1 != -1:
            if last_reward == 1:
                logits_1[last_action_1] += pers_win
            else:
                logits_1[last_action_1] += pers_loss
        
        # Softmax for Stage 1
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        s_idx = state[trial]
        last_action_1 = a1
        
        # --- Stage 2 Policy ---
        logits_2 = beta2 * q_stage2_mf[s_idx]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]
        last_reward = r
        
        # --- Value Updates ---
        # Update Stage 2 Values (Asymmetric)
        pe2 = r - q_stage2_mf[s_idx, a2]
        lr2 = lr_pos if pe2 >= 0 else lr_neg
        q_stage2_mf[s_idx, a2] += lr2 * pe2
        
        # Update Stage 1 MF Values (Asymmetric, TD(1) from reward)
        # Using direct reward reinforcement for the MF component
        pe1 = r - q_stage1_mf[a1]
        lr1 = lr_pos if pe1 >= 0 else lr_neg
        q_stage1_mf[a1] += lr1 * pe1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Pure Model-Based with Asymmetric Learning and Split Perseveration
This model simplifies the previous one by assuming the participant is purely Model-Based (`w=1`). It tests the hypothesis that the observed behavior is best explained by a planning agent that updates its estimates of alien rewards asymmetrically (e.g., learning more from wins than losses) and has distinct "stickiness" biases depending on the outcome. This contrasts with models using passive decay.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Based with Asymmetric Learning and Outcome-Dependent Perseveration.
    
    Assumes pure Model-Based planning (w=1).
    Uses asymmetric learning rates for Stage 2 value updates.
    Replaces value decay with asymmetric updating (e.g. learning strongly from rewards 
    but ignoring non-rewards, or vice versa).
    
    Parameters:
    - lr_pos: [0, 1] Learning rate for positive PE.
    - lr_neg: [0, 1] Learning rate for negative PE.
    - beta1: [0, 10] Inverse temperature for Stage 1.
    - beta2: [0, 10] Inverse temperature for Stage 2.
    - pers_win: [0, 5] Perseveration bonus after reward.
    - pers_loss: [0, 5] Perseveration bonus after no reward.
    """
    lr_pos, lr_neg, beta1, beta2, pers_win, pers_loss = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage2_mf = np.ones((2, 2)) * 0.5
    
    last_action_1 = -1
    last_reward = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Policy (Pure MB) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        logits_1 = beta1 * q_stage1_mb
        
        if last_action_1 != -1:
            if last_reward == 1:
                logits_1[last_action_1] += pers_win
            else:
                logits_1[last_action_1] += pers_loss
        
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_action_1 = action_1[trial]
        s_idx = state[trial]
        
        # --- Stage 2 Policy ---
        logits_2 = beta2 * q_stage2_mf[s_idx]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]
        last_reward = r
        
        # --- Value Updates ---
        pe = r - q_stage2_mf[s_idx, a2]
        lr = lr_pos if pe >= 0 else lr_neg
        q_stage2_mf[s_idx, a2] += lr * pe
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Adaptive Model-Based with Transition Learning
This model assumes the participant is Model-Based but does not trust the fixed 0.7/0.3 transition probabilities. Instead, they learn the transition matrix (`lr_trans`) from experience. It incorporates the successful components of previous best models: passive value decay (`decay`) and split perseveration (`pers_win`, `pers_loss`).

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Adaptive Model-Based with Transition Learning, Value Decay, and Perseveration.
    
    The agent learns the transition matrix (Adaptive MB) rather than assuming fixed 0.7/0.3.
    Includes passive value decay for unchosen options and outcome-dependent perseveration.
    
    Parameters:
    - lr_trans: [0, 1] Learning rate for transition probabilities.
    - lr_val: [0, 1] Learning rate for Stage 2 values.
    - beta1: [0, 10] Inverse temperature for Stage 1.
    - beta2: [0, 10] Inverse temperature for Stage 2.
    - decay: [0, 1] Decay rate toward 0.5 for unchosen values.
    - pers_win: [0, 5] Perseveration bonus after reward.
    - pers_loss: [0, 5] Perseveration bonus after no reward.
    """
    lr_trans, lr_val, beta1, beta2, decay, pers_win, pers_loss = model_parameters
    n_trials = len(action_1)
    
    # Initialize transitions with prior belief (0.7/0.3)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage2_mf = np.ones((2, 2)) * 0.5
    
    last_action_1 = -1
    last_reward = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Policy (Adaptive MB) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        logits_1 = beta1 * q_stage1_mb
        
        if last_action_1 != -1:
            if last_reward == 1:
                logits_1[last_action_1] += pers_win
            else:
                logits_1[last_action_1] += pers_loss
        
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        s_idx = state[trial]
        last_action_1 = a1
        
        # --- Stage 2 Policy ---
        logits_2 = beta2 * q_stage2_mf[s_idx]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]
        last_reward = r
        
        # --- Value Updates ---
        # Update Stage 2 Value
        pe = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr_val * pe
        
        # Decay unchosen options toward 0.5
        unchosen_a2 = 1 - a2
        q_stage2_mf[s_idx, unchosen_a2] = (1 - decay) * q_stage2_mf[s_idx, unchosen_a2] + decay * 0.5
        other_state = 1 - s_idx
        q_stage2_mf[other_state, :] = (1 - decay) * q_stage2_mf[other_state, :] + decay * 0.5
        
        # Update Transition Matrix
        # Move the row corresponding to action_1 towards the observed state one-hot vector
        transition_matrix[a1, s_idx] += lr_trans * (1 - transition_matrix[a1, s_idx])
        transition_matrix[a1, 1-s_idx] = 1.0 - transition_matrix[a1, s_idx]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```