Here are three new cognitive models for the two-step task. These models introduce mechanisms such as stage-specific stickiness, independent system weighting, and random lapse rates (epsilon) to better capture the participant's behavior.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Dual Stickiness Hybrid Model with Decay.
    
    Extends the hybrid model by applying stickiness (perseveration) to both the 
    first-stage spaceship choice and the second-stage alien choice.
    Also includes decay for unchosen actions to allow for flexibility after long streaks.
    
    Parameters:
    - learning_rate: [0,1] Rate of value updating.
    - decay_rate: [0,1] Rate at which unchosen action values decay.
    - beta: [0,10] Inverse temperature (exploration/exploitation balance).
    - w: [0,1] Weighting of Model-Based (vs Model-Free) system.
    - stick_1: [0,5] Stickiness bonus for the previously chosen spaceship.
    - stick_2: [0,5] Stickiness bonus for the previously chosen alien (regardless of planet).
    """
    learning_rate, decay_rate, beta, w, stick_1, stick_2 = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    last_action_2 = -1
    
    log_loss = 0.0
    eps = 1e-10
    
    for trial in range(n_trials):
        a1 = action_1[trial]
        s_idx = state[trial]
        a2 = action_2[trial]
        r = reward[trial]
        
        # Handle missing data
        if a1 == -1 or a2 == -1:
            last_action_1 = -1
            last_action_2 = -1
            continue
            
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net_stage1
        if last_action_1 != -1:
            logits_1[last_action_1] += stick_1
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + eps)
        
        # --- Stage 2 Choice ---
        logits_2 = beta * q_stage2_mf[s_idx]
        if last_action_2 != -1:
            logits_2[last_action_2] += stick_2
            
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[a2] + eps)
        
        # --- Updates ---
        # Stage 1 (TD(0))
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        q_stage1_mf[1-a1] *= (1.0 - decay_rate)
        
        # Stage 2
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        q_stage2_mf[s_idx, 1-a2] *= (1.0 - decay_rate)
        
        last_action_1 = a1
        last_action_2 = a2

    return log_loss

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Independent Weights Hybrid Model with Decay.
    
    Instead of a convex combination (w, 1-w) scaled by a single beta, this model
    assigns independent weights (inverse temperatures) to the Model-Based and 
    Model-Free systems in Stage 1. This allows for cases where both systems are 
    weak or both are strong, independent of their relative balance.
    
    Parameters:
    - learning_rate: [0,1] Rate of value updating.
    - decay_rate: [0,1] Decay rate for unchosen actions.
    - beta_mb: [0,10] Weight/Inverse Temp for the Model-Based component in Stage 1.
    - beta_mf: [0,10] Weight/Inverse Temp for the Model-Free component in Stage 1.
    - beta_2: [0,10] Inverse temperature for Stage 2 choices.
    - stickiness: [0,5] Perseveration bonus for Stage 1.
    """
    learning_rate, decay_rate, beta_mb, beta_mf, beta_2, stickiness = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    
    log_loss = 0.0
    eps = 1e-10
    
    for trial in range(n_trials):
        a1 = action_1[trial]
        s_idx = state[trial]
        a2 = action_2[trial]
        r = reward[trial]
        
        if a1 == -1 or a2 == -1:
            last_action_1 = -1
            continue
            
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Independent weighting of components
        logits_1 = (beta_mb * q_stage1_mb) + (beta_mf * q_stage1_mf)
        
        if last_action_1 != -1:
            logits_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + eps)
        
        # --- Stage 2 Choice ---
        logits_2 = beta_2 * q_stage2_mf[s_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[a2] + eps)
        
        # --- Updates ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        q_stage1_mf[1-a1] *= (1.0 - decay_rate)
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        q_stage2_mf[s_idx, 1-a2] *= (1.0 - decay_rate)
        
        last_action_1 = a1

    return log_loss

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Epsilon-Hybrid Model with Decay.
    
    Adds an epsilon-greedy 'lapse' term to the standard hybrid model. This assumes 
    the participant chooses randomly with probability epsilon, and follows the 
    softmax policy otherwise. This makes the model robust to attentional lapses.
    
    Parameters:
    - learning_rate: [0,1] Rate of value updating.
    - decay_rate: [0,1] Decay rate for unchosen actions.
    - beta: [0,10] Inverse temperature.
    - w: [0,1] Model-Based weight.
    - stickiness: [0,5] Perseveration bonus for Stage 1.
    - epsilon: [0,1] Probability of making a random choice (lapse rate).
    """
    learning_rate, decay_rate, beta, w, stickiness, epsilon = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    
    log_loss = 0.0
    eps = 1e-10
    
    for trial in range(n_trials):
        a1 = action_1[trial]
        s_idx = state[trial]
        a2 = action_2[trial]
        r = reward[trial]
        
        if a1 == -1 or a2 == -1:
            last_action_1 = -1
            continue
            
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net
        if last_action_1 != -1:
            logits_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        # Apply epsilon mixture (lapse)
        probs_1 = (1 - epsilon) * probs_1 + epsilon * 0.5
        log_loss -= np.log(probs_1[a1] + eps)
        
        # --- Stage 2 Choice ---
        logits_2 = beta * q_stage2_mf[s_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        # Apply epsilon mixture
        probs_2 = (1 - epsilon) * probs_2 + epsilon * 0.5
        log_loss -= np.log(probs_2[a2] + eps)
        
        # --- Updates ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        q_stage1_mf[1-a1] *= (1.0 - decay_rate)
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        q_stage2_mf[s_idx, 1-a2] *= (1.0 - decay_rate)
        
        last_action_1 = a1

    return log_loss
```