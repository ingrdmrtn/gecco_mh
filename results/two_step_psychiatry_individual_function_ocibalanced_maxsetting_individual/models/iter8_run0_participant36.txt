Here are the three proposed cognitive models.

### Model 1: Choice Kernel with Asymmetric Learning
This model replaces the simple "stickiness" parameter with a **Choice Kernel** mechanism. The kernel tracks a decaying history of past choices, allowing for habit formation that strengthens with repetition (momentum). It also utilizes asymmetric learning rates for positive and negative prediction errors, allowing the model to weigh gains and losses differently.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Choice Kernel Model with Asymmetric Learning.
    
    Uses a choice kernel to track the frequency/recency of past choices (habit),
    combined with asymmetric learning rates for positive and negative prediction errors.
    
    Parameters:
    lr_pos: Learning rate for positive prediction errors [0,1]
    lr_neg: Learning rate for negative prediction errors [0,1]
    beta_1: Inverse temperature for Stage 1 [0,10]
    beta_2: Inverse temperature for Stage 2 [0,10]
    w: Weight mixing Model-Based and Model-Free values [0,1]
    lam: Eligibility trace parameter [0,1]
    ck_decay: Decay rate of the choice kernel [0,1]
    ck_weight: Weight of the choice kernel in Stage 1 decision [0,10]
    """
    lr_pos, lr_neg, beta_1, beta_2, w, lam, ck_decay, ck_weight = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Choice Kernel for Stage 1 actions (tracks history of choices)
    choice_kernel = np.zeros(2)
    
    for trial in range(n_trials):
        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add choice kernel influence (habit)
        q_net_stage1 += ck_weight * choice_kernel
        
        exp_q1 = np.exp(beta_1 * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        s1_choice = action_1[trial]
        state_idx = state[trial]
        
        # Update Choice Kernel
        choice_kernel *= ck_decay
        choice_kernel[s1_choice] += 1.0
        
        # Stage 2 Policy
        s2_choice = action_2[trial]
        
        if s2_choice != -1:
            qs_stage2 = q_stage2_mf[state_idx]
            exp_q2 = np.exp(beta_2 * qs_stage2)
            probs_2 = exp_q2 / np.sum(exp_q2)
            p_choice_2[trial] = probs_2[s2_choice]
            
            r = reward[trial]
            
            # Prediction Errors & Asymmetric Updating
            delta_stage1 = q_stage2_mf[state_idx, s2_choice] - q_stage1_mf[s1_choice]
            lr_s1 = lr_pos if delta_stage1 > 0 else lr_neg
            q_stage1_mf[s1_choice] += lr_s1 * delta_stage1
            
            delta_stage2 = r - q_stage2_mf[state_idx, s2_choice]
            lr_s2 = lr_pos if delta_stage2 > 0 else lr_neg
            q_stage2_mf[state_idx, s2_choice] += lr_s2 * delta_stage2
            
            # Eligibility Trace
            q_stage1_mf[s1_choice] += lr_s2 * lam * delta_stage2
        else:
            p_choice_2[trial] = 1.0

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Outcome-Dependent Perseveration with Bias
This model hypothesizes that the participant's tendency to repeat a choice depends on the previous outcome ("Win-Stay, Lose-Shift" dynamics). It separates perseveration into `p_win` and `p_loss`. Additionally, it includes a fixed `bias` parameter for Spaceship 0 (A), addressing the participant's observed initial preference for that option.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Outcome-Dependent Perseveration with Bias.
    
    Distinguishes between sticking to a choice after a reward (Win-Stay) vs 
    after no reward (Lose-Stay/Shift). Also includes a fixed bias for spaceship A.
    
    Parameters:
    lr: Learning rate [0,1]
    beta_1: Inverse temperature for Stage 1 [0,10]
    beta_2: Inverse temperature for Stage 2 [0,10]
    w: Weight mixing Model-Based and Model-Free values [0,1]
    lam: Eligibility trace parameter [0,1]
    p_win: Perseveration bonus after a rewarded trial [0,5]
    p_loss: Perseveration bonus after an unrewarded trial [0,5]
    bias: Fixed bias towards Spaceship 0 (A) [0,5]
    """
    lr, beta_1, beta_2, w, lam, p_win, p_loss, bias = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1
    prev_reward = 0
    
    for trial in range(n_trials):
        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add bias to Spaceship 0
        q_net_stage1[0] += bias
        
        # Add outcome-dependent perseveration
        if prev_action_1 != -1:
            if prev_reward == 1:
                q_net_stage1[prev_action_1] += p_win
            else:
                q_net_stage1[prev_action_1] += p_loss
                
        exp_q1 = np.exp(beta_1 * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        s1_choice = action_1[trial]
        state_idx = state[trial]
        prev_action_1 = s1_choice
        
        # Stage 2 Policy
        s2_choice = action_2[trial]
        r = reward[trial]
        prev_reward = r
        
        if s2_choice != -1:
            qs_stage2 = q_stage2_mf[state_idx]
            exp_q2 = np.exp(beta_2 * qs_stage2)
            probs_2 = exp_q2 / np.sum(exp_q2)
            p_choice_2[trial] = probs_2[s2_choice]
            
            # Updates
            delta_stage1 = q_stage2_mf[state_idx, s2_choice] - q_stage1_mf[s1_choice]
            q_stage1_mf[s1_choice] += lr * delta_stage1
            
            delta_stage2 = r - q_stage2_mf[state_idx, s2_choice]
            q_stage2_mf[state_idx, s2_choice] += lr * delta_stage2
            
            q_stage1_mf[s1_choice] += lr * lam * delta_stage2
        else:
            p_choice_2[trial] = 1.0
            prev_reward = 0 

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Dual-Stage Perseveration with Decay
Standard models only apply perseveration to the first stage (Spaceship choice). This model introduces a second perseveration parameter (`p_2`) for the second stage (Alien choice), accounting for the tendency to repeatedly query the same alien on a planet. It also includes global value decay to manage the drifting reward probabilities.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Dual-Stage Perseveration with Decay.
    
    Includes perseveration logic for both Stage 1 (Spaceship) and Stage 2 (Alien).
    Also includes value decay to handle drifting reward probabilities.
    
    Parameters:
    lr: Learning rate [0,1]
    beta_1: Inverse temperature for Stage 1 [0,10]
    beta_2: Inverse temperature for Stage 2 [0,10]
    w: Weight mixing Model-Based and Model-Free values [0,1]
    lam: Eligibility trace parameter [0,1]
    p_1: Perseveration for Stage 1 choice [0,5]
    p_2: Perseveration for Stage 2 choice [0,5]
    decay: Decay rate of Q-values toward 0.5 [0,1]
    """
    lr, beta_1, beta_2, w, lam, p_1, p_2, decay = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1
    prev_action_2 = [-1, -1] # Track last alien choice for each planet
    
    for trial in range(n_trials):
        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if prev_action_1 != -1:
            q_net_stage1[prev_action_1] += p_1
            
        exp_q1 = np.exp(beta_1 * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        s1_choice = action_1[trial]
        state_idx = state[trial]
        prev_action_1 = s1_choice
        
        # Stage 2 Policy
        s2_choice = action_2[trial]
        
        if s2_choice != -1:
            qs_stage2 = q_stage2_mf[state_idx].copy()
            
            # Add Stage 2 Perseveration
            if prev_action_2[state_idx] != -1:
                qs_stage2[prev_action_2[state_idx]] += p_2
            
            exp_q2 = np.exp(beta_2 * qs_stage2)
            probs_2 = exp_q2 / np.sum(exp_q2)
            p_choice_2[trial] = probs_2[s2_choice]
            
            r = reward[trial]
            prev_action_2[state_idx] = s2_choice
            
            # Updates
            delta_stage1 = q_stage2_mf[state_idx, s2_choice] - q_stage1_mf[s1_choice]
            q_stage1_mf[s1_choice] += lr * delta_stage1
            
            delta_stage2 = r - q_stage2_mf[state_idx, s2_choice]
            q_stage2_mf[state_idx, s2_choice] += lr * delta_stage2
            
            q_stage1_mf[s1_choice] += lr * lam * delta_stage2
        else:
            p_choice_2[trial] = 1.0
            
        # Decay
        q_stage1_mf = q_stage1_mf * (1 - decay) + 0.5 * decay
        q_stage2_mf = q_stage2_mf * (1 - decay) + 0.5 * decay

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```