Here are the three proposed cognitive models. They build upon the hybrid reinforcement learning framework by incorporating eligibility traces, stickiness, and unchosen value decay, while introducing specific mechanisms (dual learning rates, asymmetric updating, and separate decision noise) to better capture the participant's behavior.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Dual Learning Rates (Stage 1 vs Stage 2) model with Eligibility Trace, Stickiness, and Decay.
    Differentiates plasticity between the navigation step (Stage 1) and the harvesting step (Stage 2),
    allowing the model to capture different learning speeds for the two distinct sub-tasks.

    Parameters:
    lr_1: [0,1] - Learning rate for Stage 1 (Spaceship choice).
    lr_2: [0,1] - Learning rate for Stage 2 (Alien choice).
    beta: [0,10] - Inverse temperature (softmax sensitivity).
    w: [0,1] - Weighting parameter (0=Model-Free, 1=Model-Based).
    lam: [0,1] - Eligibility trace parameter (credit assignment to Stage 1).
    stickiness: [0,5] - Perseveration bonus for repeated Stage 1 choices.
    decay: [0,1] - Decay rate for unchosen action values (forgetting).
    """
    lr_1, lr_2, beta, w, lam, stickiness, decay = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        # Handle missing data
        if action_1[trial] == -1 or state[trial] == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Apply stickiness
        if last_action_1 != -1:
            q_net_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        if action_2[trial] == -1:
            p_choice_2[trial] = 1.0
            last_action_1 = action_1[trial]
            continue

        # --- Stage 2 Policy ---
        state_idx = state[trial]
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        if reward[trial] == -1:
            last_action_1 = action_1[trial]
            continue

        # --- Learning Updates ---
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]
        
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        
        # Update Stage 1 using lr_1
        q_stage1_mf[a1] += lr_1 * (delta_stage1 + lam * delta_stage2)
        
        # Update Stage 2 using lr_2
        q_stage2_mf[state_idx, a2] += lr_2 * delta_stage2

        # Decay unchosen options
        q_stage1_mf[1-a1] *= (1 - decay)
        q_stage2_mf[state_idx, 1-a2] *= (1 - decay)
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Asymmetric Learning Rates (Positive vs Negative RPE) model with Eligibility Trace, Stickiness, and Decay.
    Captures potential biases in learning from gains (positive RPE) versus losses/omissions (negative RPE).

    Parameters:
    alpha_pos: [0,1] - Learning rate for positive prediction errors.
    alpha_neg: [0,1] - Learning rate for negative prediction errors.
    beta: [0,10] - Inverse temperature.
    w: [0,1] - MB/MF weight.
    lam: [0,1] - Eligibility trace.
    stickiness: [0,5] - Perseveration bonus.
    decay: [0,1] - Decay rate for unchosen action values.
    """
    alpha_pos, alpha_neg, beta, w, lam, stickiness, decay = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # Stage 1 Choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_action_1 != -1:
            q_net_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        if action_2[trial] == -1:
            p_choice_2[trial] = 1.0
            last_action_1 = action_1[trial]
            continue

        # Stage 2 Choice
        state_idx = state[trial]
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        if reward[trial] == -1:
            last_action_1 = action_1[trial]
            continue

        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]
        
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        
        # Determine effective learning rates based on sign of error signals
        total_delta_1 = delta_stage1 + lam * delta_stage2
        lr_1 = alpha_pos if total_delta_1 > 0 else alpha_neg
        
        lr_2 = alpha_pos if delta_stage2 > 0 else alpha_neg
        
        q_stage1_mf[a1] += lr_1 * total_delta_1
        q_stage2_mf[state_idx, a2] += lr_2 * delta_stage2

        q_stage1_mf[1-a1] *= (1 - decay)
        q_stage2_mf[state_idx, 1-a2] *= (1 - decay)
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Separate Beta (Stage 1 vs Stage 2) model with Eligibility Trace, Stickiness, and Decay.
    Allows for different levels of exploration/exploitation noise in the high-level planning stage (Stage 1)
    versus the low-level bandit stage (Stage 2).

    Parameters:
    learning_rate: [0,1] - Learning rate.
    beta_1: [0,10] - Inverse temperature for Stage 1 choice.
    beta_2: [0,10] - Inverse temperature for Stage 2 choice.
    w: [0,1] - MB/MF weight.
    lam: [0,1] - Eligibility trace.
    stickiness: [0,5] - Perseveration bonus.
    decay: [0,1] - Decay rate for unchosen action values.
    """
    learning_rate, beta_1, beta_2, w, lam, stickiness, decay = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # Stage 1 Choice (using beta_1)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_action_1 != -1:
            q_net_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta_1 * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        if action_2[trial] == -1:
            p_choice_2[trial] = 1.0
            last_action_1 = action_1[trial]
            continue

        # Stage 2 Choice (using beta_2)
        state_idx = state[trial]
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        if reward[trial] == -1:
            last_action_1 = action_1[trial]
            continue

        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]
        
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        
        q_stage1_mf[a1] += learning_rate * (delta_stage1 + lam * delta_stage2)
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

        q_stage1_mf[1-a1] *= (1 - decay)
        q_stage2_mf[state_idx, 1-a2] *= (1 - decay)
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```