Here are the three cognitive models as Python functions.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF model with Uncertainty-Based Exploration in Stage 2.
    
    The agent adds an exploration bonus to the Stage 2 (Alien) options based on 
    how long since they were last chosen. This helps track the drifting reward 
    probabilities of the aliens, promoting exploration of neglected options.

    Parameters:
    - lr: Learning rate [0,1]
    - beta_1: Inverse temperature for stage 1 [0,10]
    - beta_2: Inverse temperature for stage 2 [0,10]
    - w: Mixing weight (0=MF, 1=MB) [0,1]
    - stick: Choice stickiness for stage 1 [0,5]
    - phi_2: Exploration bonus weight for stage 2 [0,5]
    """
    lr, beta_1, beta_2, w, stick, phi_2 = model_parameters
    n_trials = len(action_1)
    
    # Transition matrix: A(0)->X(0) 0.7, A(0)->Y(1) 0.3; U(1)->X(0) 0.3, U(1)->Y(1) 0.7
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # 2 planets, 2 aliens
    
    # Track time since last selection for each alien (2 planets x 2 aliens)
    time_since_choice_2 = np.zeros((2, 2))
    
    log_loss = 0.0
    eps = 1e-10
    prev_action_1 = -1
    
    for trial in range(n_trials):
        # Increment uncertainty for all aliens (drift happens everywhere)
        time_since_choice_2 += 1

        # --- Stage 1 Choice ---
        # Model-Based Value: Expected value of max stage 2 Q-values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Q-value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Apply Stickiness
        if prev_action_1 != -1:
            q_net[prev_action_1] += stick
            
        # Softmax Stage 1
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = action_1[trial]
        log_loss -= np.log(probs_1[a1] + eps)
        prev_action_1 = a1
        
        # --- Stage 2 Choice ---
        state_idx = state[trial]
        
        # Calculate Stage 2 Q-values with Exploration Bonus
        # Add bonus only to the available aliens on the current planet
        qs_2 = q_stage2_mf[state_idx] + phi_2 * time_since_choice_2[state_idx]
        
        # Softmax Stage 2
        exp_q2 = np.exp(beta_2 * qs_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = action_2[trial]
        if a2 == -1: 
            continue
            
        log_loss -= np.log(probs_2[a2] + eps)
        
        # Reset timer for chosen alien
        time_since_choice_2[state_idx, a2] = 0
        
        # --- Learning ---
        r = reward[trial]
        
        # Update Stage 2 MF (TD(0))
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += lr * delta_stage2
        
        # Update Stage 1 MF (TD(0) using Stage 2 Q-value of chosen option)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF model with Dynamic Weighting (Linear Trend).
    
    The mixing weight 'w' (MB vs MF) changes linearly over the course of the experiment,
    allowing the agent to shift strategy (e.g., from MF to MB or vice versa) as they
    gain more experience or as the task progresses.
    
    Parameters:
    - lr: Learning rate [0,1]
    - beta_1: Inverse temperature for stage 1 [0,10]
    - beta_2: Inverse temperature for stage 2 [0,10]
    - stick: Choice stickiness [0,5]
    - w_start: Initial mixing weight (trial 0) [0,1]
    - w_end: Final mixing weight (last trial) [0,1]
    """
    lr, beta_1, beta_2, stick, w_start, w_end = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    log_loss = 0.0
    eps = 1e-10
    prev_action_1 = -1
    
    for trial in range(n_trials):
        # Calculate dynamic w based on trial progress
        w = w_start + (w_end - w_start) * (trial / n_trials)
        
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if prev_action_1 != -1:
            q_net[prev_action_1] += stick
            
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = action_1[trial]
        log_loss -= np.log(probs_1[a1] + eps)
        prev_action_1 = a1
        
        # --- Stage 2 Choice ---
        state_idx = state[trial]
        qs_2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta_2 * qs_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = action_2[trial]
        if a2 == -1:
            continue
        log_loss -= np.log(probs_2[a2] + eps)
        
        # --- Learning ---
        r = reward[trial]
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += lr * delta_stage2
        
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF model with Subjective Loss Valuation.
    
    The agent perceives the lack of reward (0 coins) as a negative value (loss)
    rather than neutral. This effectively shifts the baseline of reinforcement,
    making 'loss' outcomes drive active avoidance (decreasing Q-values below 0).
    
    Parameters:
    - lr: Learning rate [0,1]
    - beta_1: Inverse temperature for stage 1 [0,10]
    - beta_2: Inverse temperature for stage 2 [0,10]
    - w: Mixing weight (0=MF, 1=MB) [0,1]
    - stick: Choice stickiness [0,5]
    - neg_val: Subjective magnitude of 0 coins [0,5]
               (The effective reward for 0 coins is -neg_val)
    """
    lr, beta_1, beta_2, w, stick, neg_val = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    log_loss = 0.0
    eps = 1e-10
    prev_action_1 = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if prev_action_1 != -1:
            q_net[prev_action_1] += stick
            
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = action_1[trial]
        log_loss -= np.log(probs_1[a1] + eps)
        prev_action_1 = a1
        
        # --- Stage 2 Choice ---
        state_idx = state[trial]
        qs_2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta_2 * qs_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = action_2[trial]
        if a2 == -1:
            continue
        log_loss -= np.log(probs_2[a2] + eps)
        
        # --- Learning ---
        r_raw = reward[trial]
        # Transform reward: 1 -> 1, 0 -> -neg_val
        r_effective = 1.0 if r_raw == 1 else -neg_val
        
        delta_stage2 = r_effective - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += lr * delta_stage2
        
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
    return log_loss
```