Here are the three proposed cognitive models.

### Model 1: Dual-Learning Rate Hybrid with Eligibility Traces
This model hypothesizes that the participant learns the value of spaceships (Stage 1) and aliens (Stage 2) at different rates (`alpha_1` vs `alpha_2`). Additionally, it includes an eligibility trace parameter (`lam`) which allows the outcome of Stage 2 (the reward) to directly influence the update of the Stage 1 choice, bridging the gap between the two stages more strongly than standard temporal difference learning.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Dual-Learning Rate Hybrid Model with Eligibility Traces.
    
    Distinguishes between Stage 1 (Spaceship) and Stage 2 (Alien) learning rates.
    Incorporates an eligibility trace (lambda) to allow Stage 2 reward prediction 
    errors to directly update Stage 1 Model-Free values.
    
    Parameters:
    alpha_1: [0, 1] Learning rate for Stage 1 MF values.
    alpha_2: [0, 1] Learning rate for Stage 2 MF values.
    beta_1: [0, 10] Inverse temperature for Stage 1 choice.
    beta_2: [0, 10] Inverse temperature for Stage 2 choice.
    w: [0, 1] Mixing weight (0=MF, 1=MB) for Stage 1.
    lam: [0, 1] Eligibility trace parameter (scaling of Stage 2 RPE to Stage 1).
    """
    alpha_1, alpha_2, beta_1, beta_2, w, lam = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta_1 * q_net_1
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = action_1[trial]
        p_choice_1[trial] = probs_1[a1]
        
        s_idx = state[trial]
        a2 = action_2[trial]
        
        # Stage 2 Policy & Updates
        if a2 != -1:
            # Policy
            logits_2 = beta_2 * q_stage2_mf[s_idx]
            exp_q2 = np.exp(logits_2 - np.max(logits_2))
            probs_2 = exp_q2 / np.sum(exp_q2)
            p_choice_2[trial] = probs_2[a2]
            
            r = reward[trial]
            
            # Prediction Errors
            # Stage 1 PE (SARSA-style): Q2(s, a2) - Q1(a1)
            delta_1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
            
            # Stage 2 PE: R - Q2(s, a2)
            delta_2 = r - q_stage2_mf[s_idx, a2]
            
            # Update Stage 1 MF
            # Q1 = Q1 + alpha1 * delta1 + alpha1 * lambda * delta2
            q_stage1_mf[a1] += alpha_1 * delta_1 + alpha_1 * lam * delta_2
            
            # Update Stage 2 MF
            q_stage2_mf[s_idx, a2] += alpha_2 * delta_2
            
        else:
            # Invalid trial handling
            p_choice_2[trial] = 1.0
            # If reward is negative (penalty), update Q1
            if reward[trial] < 0:
                delta_loss = reward[trial] - q_stage1_mf[a1]
                q_stage1_mf[a1] += alpha_1 * delta_loss

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model with Win-Stay Lose-Shift Stickiness
This model refines the concept of "stickiness" (perseveration). Instead of a constant tendency to repeat the last choice, this tendency is modulated by the outcome of that choice. The participant has separate stickiness parameters for when the previous choice was rewarded (`stick_win`) versus when it was unrewarded or punished (`stick_loss`). This captures a heuristic "Win-Stay, Lose-Shift" strategy running in parallel with reinforcement learning.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Outcome-Dependent Stickiness (Win-Stay, Lose-Shift).
    
    The stickiness bonus applied to the previously chosen spaceship depends on 
    whether the previous trial resulted in a reward (Win) or not (Lose).
    
    Parameters:
    learning_rate: [0, 1] Learning rate for Q-values.
    beta_1: [0, 10] Inverse temperature for Stage 1.
    beta_2: [0, 10] Inverse temperature for Stage 2.
    w: [0, 1] Mixing weight (0=MF, 1=MB).
    stick_win: [0, 10] Stickiness bonus after a rewarded trial.
    stick_loss: [0, 10] Stickiness bonus after a unrewarded/punished trial.
    """
    learning_rate, beta_1, beta_2, w, stick_win, stick_loss = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_a1 = -1
    prev_r = 0
    
    for trial in range(n_trials):
        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta_1 * q_net_1
        
        # Apply conditional stickiness based on previous outcome
        if prev_a1 != -1:
            if prev_r > 0:
                logits_1[prev_a1] += stick_win
            else:
                logits_1[prev_a1] += stick_loss
                
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = action_1[trial]
        p_choice_1[trial] = probs_1[a1]
        
        s_idx = state[trial]
        a2 = action_2[trial]
        
        if a2 != -1:
            logits_2 = beta_2 * q_stage2_mf[s_idx]
            exp_q2 = np.exp(logits_2 - np.max(logits_2))
            probs_2 = exp_q2 / np.sum(exp_q2)
            p_choice_2[trial] = probs_2[a2]
            
            r = reward[trial]
            
            delta_1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
            q_stage1_mf[a1] += learning_rate * delta_1
            
            delta_2 = r - q_stage2_mf[s_idx, a2]
            q_stage2_mf[s_idx, a2] += learning_rate * delta_2
            
            prev_a1 = a1
            prev_r = r
        else:
            p_choice_2[trial] = 1.0
            prev_a1 = a1
            prev_r = -1 # Treat invalid/timeout as a loss

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Pearce-Hall Hybrid Model (Dynamic Learning Rate)
This model implements a dynamic learning rate mechanism inspired by the Pearce-Hall theory. The agent tracks the "associability" (alpha) of the task, which increases when prediction errors are high (surprise) and decreases when outcomes are predictable. This allows the model to adapt quickly to the changes in reward probabilities mentioned in the task description, while stabilizing when the environment is consistent.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Pearce-Hall Hybrid Model (Dynamic Learning Rate).
    
    The learning rate is not fixed but adapts based on the magnitude of the 
    prediction error (associability). High surprise leads to faster learning.
    
    Parameters:
    init_alpha: [0, 1] Initial learning rate / associability.
    eta: [0, 1] Update rate for the associability (Pearce-Hall rate).
    beta_1: [0, 10] Inverse temperature for Stage 1.
    beta_2: [0, 10] Inverse temperature for Stage 2.
    w: [0, 1] Mixing weight (0=MF, 1=MB).
    stickiness: [0, 5] General stickiness bonus.
    """
    init_alpha, eta, beta_1, beta_2, w, stickiness = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    current_alpha = init_alpha
    prev_a1 = -1
    
    for trial in range(n_trials):
        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta_1 * q_net_1
        if prev_a1 != -1:
            logits_1[prev_a1] += stickiness
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = action_1[trial]
        p_choice_1[trial] = probs_1[a1]
        
        s_idx = state[trial]
        a2 = action_2[trial]
        
        if a2 != -1:
            logits_2 = beta_2 * q_stage2_mf[s_idx]
            exp_q2 = np.exp(logits_2 - np.max(logits_2))
            probs_2 = exp_q2 / np.sum(exp_q2)
            p_choice_2[trial] = probs_2[a2]
            
            r = reward[trial]
            
            # Use current_alpha for updates
            delta_1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
            q_stage1_mf[a1] += current_alpha * delta_1
            
            delta_2 = r - q_stage2_mf[s_idx, a2]
            q_stage2_mf[s_idx, a2] += current_alpha * delta_2
            
            # Update associability (alpha) based on absolute PE from Stage 2 (outcome)
            # This reflects how surprising the outcome was
            current_alpha = (1 - eta) * current_alpha + eta * np.abs(delta_2)
            
            prev_a1 = a1
        else:
            p_choice_2[trial] = 1.0
            prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```