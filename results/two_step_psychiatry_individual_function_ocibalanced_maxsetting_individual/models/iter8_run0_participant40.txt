Here are the 3 new cognitive models based on the participant data and feedback.

### Model 1: MF Q-Learning with Habit Trace and Q-Decay
This model extends the successful Model-Free with Decay approach by replacing simple binary perseverance with a continuous **Habit Trace**. The participant shows long streaks of choices ("stickiness"), which suggests a slowly building and decaying habit rather than just repeating the last choice.
- **Mechanism**: A habit trace accumulates for chosen actions and decays for all actions. This trace is added to the Q-values (weighted by `w_habit`) to bias selection. Q-values also decay to handle non-stationarity.
- **Parameters**: `learning_rate`, `beta`, `decay_q`, `decay_habit`, `w_habit`.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    MF Q-Learning with Habit Trace and Q-value Decay.
    
    Incorporates a dynamic 'habit' trace that builds up with repeated choices,
    explaining long streaks better than simple perseverance.
    
    Parameters:
    - learning_rate: [0, 1] Update rate for Q-values.
    - beta: [0, 10] Inverse temperature (exploration/exploitation).
    - decay_q: [0, 1] Decay rate for unchosen Q-values (forgetting).
    - decay_habit: [0, 1] Decay rate for the habit trace (1 = perfect memory, 0 = instant forget).
    - w_habit: [0, 10] Weight of the habit trace in decision making.
    """
    learning_rate, beta, decay_q, decay_habit, w_habit = model_parameters
    n_trials = len(action_1)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    # Habit trace for stage 1 actions
    habit_trace = np.zeros(2)
    
    # Initialize Q-values to 0.5 (center of reward range 0-1)
    q_stage1.fill(0.5)
    q_stage2.fill(0.5)

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # Combine Q-value and Habit Trace
        logits_1 = beta * q_stage1 + w_habit * habit_trace
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        s_idx = state[trial]
        
        # --- Stage 2 Decision ---
        # Standard Softmax on Stage 2 Q-values
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]
        
        # --- Updates ---
        
        # 1. Update Stage 1 Q-values (TD Learning)
        # Target is the max value of the next state (or value of chosen, here standard SARSA-like or Q)
        # Using Q-learning target: value of chosen option in stage 2
        target_stage1 = q_stage2[s_idx, a2]
        q_stage1[a1] += learning_rate * (target_stage1 - q_stage1[a1])
        
        # Decay unchosen Stage 1 Q-value
        q_stage1[1-a1] *= (1.0 - decay_q)
        
        # 2. Update Stage 2 Q-values
        q_stage2[s_idx, a2] += learning_rate * (r - q_stage2[s_idx, a2])
        
        # Decay unchosen Stage 2 Q-value (for the current state)
        q_stage2[s_idx, 1-a2] *= (1.0 - decay_q)
        
        # 3. Update Habit Trace
        # Chosen action trace moves towards 1, unchosen decays
        # Formulation: Trace(t+1) = Trace(t) * decay + I(choice) * (1-decay) ??
        # Or simple accumulation with decay: Trace = Trace * decay; Trace[choice] += 1 (Unbounded)
        # Using convex combination to keep in [0,1]
        habit_trace[a1] = habit_trace[a1] * decay_habit + 1.0 * (1.0 - decay_habit)
        habit_trace[1-a1] = habit_trace[1-a1] * decay_habit

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid (MB/MF) with Decay and Perseverance
This model tests if the participant uses Model-Based planning, but masked by memory decay and perseverance. It combines the full Hybrid architecture (Model-Based + Model-Free weighting) with the "Decay" and "Perseverance" mechanisms found to be effective in previous pure MF models.
- **Mechanism**: Computes a weighted average of Model-Based (planning) and Model-Free Q-values. Adds a perseverance bonus for the repeated choice. Unchosen Model-Free Q-values decay.
- **Parameters**: `learning_rate`, `beta`, `w` (mixing weight), `decay`, `perseverance`.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model (MB/MF) with Decay and Perseverance.
    
    Combines Model-Based planning with Model-Free learning, while accounting for
    memory decay and perseverance (stickiness).
    
    Parameters:
    - learning_rate: [0, 1] Update rate for MF Q-values.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weight for Model-Based values (0=Pure MF, 1=Pure MB).
    - decay: [0, 1] Decay rate for unchosen MF Q-values.
    - perseverance: [0, 5] Bonus added to the logits of the previously chosen action.
    """
    learning_rate, beta, w, decay, perseverance = model_parameters
    n_trials = len(action_1)
    
    # Transition matrix: A(0)->X(0) 0.7, U(1)->Y(1) 0.7
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    q_stage1_mf.fill(0.5)
    q_stage2_mf.fill(0.5)
    
    prev_action_1 = -1

    for trial in range(n_trials):
        
        # --- Stage 1 Decision ---
        # Model-Based Value Calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits = beta * q_net
        if prev_action_1 != -1:
            logits[prev_action_1] += perseverance
            
        exp_q1 = np.exp(logits)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        s_idx = state[trial]
        
        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]
        
        # --- Updates ---
        
        # Stage 1 MF Update
        target_stage1 = q_stage2_mf[s_idx, a2]
        q_stage1_mf[a1] += learning_rate * (target_stage1 - q_stage1_mf[a1])
        q_stage1_mf[1-a1] *= (1.0 - decay) # Decay unchosen
        
        # Stage 2 MF Update
        q_stage2_mf[s_idx, a2] += learning_rate * (r - q_stage2_mf[s_idx, a2])
        q_stage2_mf[s_idx, 1-a2] *= (1.0 - decay) # Decay unchosen
        
        prev_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: MF Q-Learning with Eligibility Traces and Decay
This model introduces **Eligibility Traces (Lambda)** to the Decay model. Eligibility traces allow the outcome at Stage 2 to directly update the Stage 1 choice, bridging the gap between action and reward more effectively than standard 1-step Q-learning.
- **Mechanism**: Uses TD(lambda) updates. Stage 1 value is updated by Stage 1 error + `lambda` * Stage 2 error. Unchosen values decay.
- **Parameters**: `learning_rate`, `beta`, `lam` (lambda), `decay`.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    MF Q-Learning with Eligibility Traces (Lambda) and Decay.
    
    Uses eligibility traces to credit Stage 2 reward back to Stage 1 choice,
    combined with forgetting (decay) for unchosen options.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - lam: [0, 1] Eligibility trace parameter (0 = pure TD(0), 1 = Monte Carlo).
    - decay: [0, 1] Decay rate for unchosen Q-values.
    """
    learning_rate, beta, lam, decay = model_parameters
    n_trials = len(action_1)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    q_stage1.fill(0.5)
    q_stage2.fill(0.5)

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        s_idx = state[trial]
        
        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]
        
        # --- Updates (TD Lambda) ---
        
        # Prediction Error Stage 1: (Q_s2 - Q_s1)
        # Note: We use the value of the state chosen in stage 2 (SARSA-style for stage 1-2 link)
        delta1 = q_stage2[s_idx, a2] - q_stage1[a1]
        
        # Prediction Error Stage 2: (Reward - Q_s2)
        delta2 = r - q_stage2[s_idx, a2]
        
        # Update Stage 1 Q-value
        # Accumulates immediate error (delta1) and weighted future error (lambda * delta2)
        q_stage1[a1] += learning_rate * (delta1 + lam * delta2)
        q_stage1[1-a1] *= (1.0 - decay) # Decay unchosen
        
        # Update Stage 2 Q-value
        q_stage2[s_idx, a2] += learning_rate * delta2
        q_stage2[s_idx, 1-a2] *= (1.0 - decay) # Decay unchosen

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```