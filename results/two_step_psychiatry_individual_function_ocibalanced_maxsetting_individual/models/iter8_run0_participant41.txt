Here are the 3 new cognitive models based on your specifications and the participant data analysis.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF Model with Stage-Specific Stickiness.
    
    This model assumes the participant utilizes both Model-Based (MB) and Model-Free (MF)
    strategies, combined via a weighting parameter `w`. Crucially, it differentiates 
    between perseveration (stickiness) at the high-level spaceship choice (Stage 1) 
    and the low-level alien choice (Stage 2).
    
    Parameters:
    learning_rate: [0,1] - Learning rate for value updates.
    beta_1: [0,10] - Inverse temperature for Stage 1 choice (exploration/exploitation).
    beta_2: [0,10] - Inverse temperature for Stage 2 choice.
    w: [0,1] - Weight of Model-Based control (0 = Pure MF, 1 = Pure MB).
    stick_1: [0,10] - Stickiness bonus for repeating the previous spaceship choice.
    stick_2: [0,10] - Stickiness bonus for repeating the previous alien choice (within a planet).
    """
    learning_rate, beta_1, beta_2, w, stick_1, stick_2 = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition matrix as per task structure
    # Row 0: Action 0 -> [Prob(State 0), Prob(State 1)]
    # Row 1: Action 1 -> [Prob(State 0), Prob(State 1)]
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)      # MF values for Stage 1 (Spaceships)
    q_stage2_mf = np.zeros((2, 2)) # Values for Stage 2 (Planets x Aliens)
    
    prev_a1 = -1
    # Track previous choice for Stage 2 separately for each state (planet)
    prev_a2_per_state = np.array([-1, -1]) 

    for trial in range(n_trials):
        
        # --- Stage 1 Policy ---
        # Model-Based Value: Expected max value of next stage weighted by transition probs
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combined Value
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Calculate logits with stickiness
        logits_1 = beta_1 * q_net_1
        if prev_a1 != -1:
            logits_1[prev_a1] += stick_1
            
        # Softmax for Stage 1
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        # Store probability of observed action
        if np.isnan(action_1[trial]):
            p_choice_1[trial] = 0.5 # Default for missing data
            prev_a1 = -1
            continue
        else:
            a1 = int(action_1[trial])
            p_choice_1[trial] = probs_1[a1]
            prev_a1 = a1
            
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        logits_2 = beta_2 * q_stage2_mf[state_idx]
        
        # Apply stickiness for Stage 2 specific to the current state
        if prev_a2_per_state[state_idx] != -1:
            logits_2[prev_a2_per_state[state_idx]] += stick_2
            
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        if np.isnan(action_2[trial]):
            p_choice_2[trial] = 0.5
            continue
        else:
            a2 = int(action_2[trial])
            p_choice_2[trial] = probs_2[a2]
            prev_a2_per_state[state_idx] = a2
        
        r = reward[trial]

        # --- Updating ---
        # TD(0) update for Stage 1 MF (SARSA style)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # TD update for Stage 2
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF Model with Directed Exploration (Exploration Bonus).
    
    This model implements a directed exploration strategy. Instead of relying solely
    on softmax noise (beta) for exploration, the agent adds a bonus to the values
    of options based on how long it has been since they were last chosen.
    
    Parameters:
    learning_rate: [0,1] - Value updating rate.
    beta_1: [0,10] - Inverse temperature for Stage 1.
    beta_2: [0,10] - Inverse temperature for Stage 2.
    w: [0,1] - Weight of Model-Based control.
    expl_bonus: [0,2] - Bonus added to Q-value per trial an option is unchosen.
    """
    learning_rate, beta_1, beta_2, w, expl_bonus = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track time since last choice (exploration counters)
    # Initialize with 0
    time_since_1 = np.zeros(2)
    time_since_2 = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add exploration bonus
        q_net_1_aug = q_net_1 + expl_bonus * time_since_1
        
        exp_q1 = np.exp(beta_1 * q_net_1_aug)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        if np.isnan(action_1[trial]):
            p_choice_1[trial] = 0.5
            continue # Cannot update if action unknown
        else:
            a1 = int(action_1[trial])
            p_choice_1[trial] = probs_1[a1]
            
            # Update exploration counters
            time_since_1 += 1
            time_since_1[a1] = 0
            
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        # Add exploration bonus to Stage 2 values
        q_stage2_aug = q_stage2_mf[state_idx] + expl_bonus * time_since_2[state_idx]
        
        exp_q2 = np.exp(beta_2 * q_stage2_aug)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        if np.isnan(action_2[trial]):
            p_choice_2[trial] = 0.5
            continue
        else:
            a2 = int(action_2[trial])
            p_choice_2[trial] = probs_2[a2]
            
            # Update exploration counters
            time_since_2[state_idx] += 1
            time_since_2[state_idx, a2] = 0
        
        r = reward[trial]

        # --- Updating ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF Model with Passive Value Decay.
    
    This model incorporates a decay mechanism where Q-values for both Model-Free
    stages 'leak' or decay towards zero at each trial. This simulates forgetting
    or a recency bias, where older experiences have less influence on current 
    decisions than recent ones.
    
    Parameters:
    learning_rate: [0,1] - Value updating rate for chosen options.
    decay_rate: [0,1] - Rate at which all Q-values decay per trial (0 = no decay).
    beta_1: [0,10] - Inverse temperature for Stage 1.
    beta_2: [0,10] - Inverse temperature for Stage 2.
    w: [0,1] - Weight of Model-Based control.
    """
    learning_rate, decay_rate, beta_1, beta_2, w = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        
        # Apply Passive Decay to all values at start of trial
        q_stage1_mf *= (1 - decay_rate)
        q_stage2_mf *= (1 - decay_rate)

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_1 * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        if np.isnan(action_1[trial]):
            p_choice_1[trial] = 0.5
            continue
        else:
            a1 = int(action_1[trial])
            p_choice_1[trial] = probs_1[a1]
            
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        if np.isnan(action_2[trial]):
            p_choice_2[trial] = 0.5
            continue
        else:
            a2 = int(action_2[trial])
            p_choice_2[trial] = probs_2[a2]
        
        r = reward[trial]

        # --- Updating ---
        # Note: We update based on the already decayed values from start of trial.
        
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```