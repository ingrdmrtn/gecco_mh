Here are three new cognitive models based on the participant's data and the two-step task structure.

### Model 1: Subjective Loss Sensitivity
This model hypothesizes that the participant perceives a reward of 0 not as a neutral outcome, but as a negative outcome (a subjective loss or frustration). This "loss magnitude" scales the negative prediction error, causing values to decrease more rapidly than they increase, which helps explain the rapid switching behavior observed after unrewarded trials.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Subjective Loss Model.
    
    Transforms the reward function such that 0 coins is perceived as a negative 
    value (loss), rather than neutral. This asymmetry captures the participant's 
    potential frustration or risk aversion, affecting both Stage 2 learning 
    and Stage 1 Model-Based value computation.
    
    Parameters:
    - lr: [0, 1] Learning rate for value updates.
    - beta: [0, 10] Inverse temperature (softmax sensitivity).
    - w: [0, 1] Mixing weight (0 = Model-Free, 1 = Model-Based).
    - loss_mag: [0, 5] Subjective magnitude of loss when reward is 0. 
                (Effective reward = -loss_mag).
    """
    lr, beta, w, loss_mag = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # state x action

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        s1_choice = action_1[trial]
        s2_state = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s2_state])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        s2_choice = action_2[trial]
        raw_reward = reward[trial]
        
        # --- Value Updating ---
        
        # Transform Reward: 0 becomes negative
        if raw_reward == 1:
            eff_reward = 1.0
        else:
            eff_reward = -loss_mag
            
        # Stage 1 MF Update
        delta_stage1 = q_stage2_mf[s2_state, s2_choice] - q_stage1_mf[s1_choice]
        q_stage1_mf[s1_choice] += lr * delta_stage1
        
        # Stage 2 MF Update
        delta_stage2 = eff_reward - q_stage2_mf[s2_state, s2_choice]
        q_stage2_mf[s2_state, s2_choice] += lr * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Dynamic Transition Learning
This model relaxes the assumption that the participant believes the transition probabilities are fixed at 0.7/0.3. Instead, the participant actively learns the transition matrix (`lr_trans`) based on observed transitions. This allows the Model-Based system to drift if the participant perceives "streaks" of rare transitions as a change in the environment's structure.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Dynamic Transition Learning Model.
    
    The participant does not assume fixed transition probabilities (70/30).
    Instead, they update their internal model of the spaceship-planet transitions
    trial-by-trial. This allows the Model-Based valuation to adapt to the 
    stochastic realization of transitions.
    
    Parameters:
    - lr: [0, 1] Learning rate for Q-values.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight (0=MF, 1=MB).
    - lr_trans: [0, 1] Learning rate for the transition matrix.
    """
    lr, beta, w, lr_trans = model_parameters
    n_trials = len(action_1)
  
    # Initialize transition belief with instructions (0.7/0.3)
    # Rows: Spaceship 0, Spaceship 1
    # Cols: Planet 0, Planet 1
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        s1_choice = action_1[trial]
        s2_state = state[trial] # Actual planet arrived at

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s2_state])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        s2_choice = action_2[trial]
        
        # --- Value Updating ---
        
        # 1. Update Transition Matrix Belief
        # Construct target vector: [1, 0] if Planet 0, [0, 1] if Planet 1
        target_trans = np.zeros(2)
        target_trans[s2_state] = 1.0
        
        # Delta rule update for the chosen spaceship's row
        # T_new = T_old + alpha * (Target - T_old)
        transition_matrix[s1_choice] += lr_trans * (target_trans - transition_matrix[s1_choice])
        
        # 2. Update Q-Values
        delta_stage1 = q_stage2_mf[s2_state, s2_choice] - q_stage1_mf[s1_choice]
        q_stage1_mf[s1_choice] += lr * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[s2_state, s2_choice]
        q_stage2_mf[s2_state, s2_choice] += lr * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Alien Generalization
This model proposes that the participant generalizes learning across planets. If they learn that "Alien 0" is generous on Planet X, they update their expectation for "Alien 0" on Planet Y as well. This explains synchronized shifts in alien preference observed in the data (e.g., switching from Alien 0 to Alien 1 globally).

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Alien Generalization Model.
    
    Hypothesizes that the participant generalizes value between identical aliens 
    on different planets. When updating the value of an alien on the current 
    planet, the model also updates the value of the same alien on the other 
    planet, scaled by a coupling parameter.
    
    Parameters:
    - lr: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight.
    - coupling: [0, 1] Strength of generalization (0=independent, 1=full transfer).
    """
    lr, beta, w, coupling = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # state (planet) x action (alien)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        s1_choice = action_1[trial]
        s2_state = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s2_state])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        s2_choice = action_2[trial]
        r = reward[trial]
        
        # --- Value Updating ---
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[s2_state, s2_choice] - q_stage1_mf[s1_choice]
        q_stage1_mf[s1_choice] += lr * delta_stage1
        
        # Stage 2 Update (Current Planet)
        delta_stage2 = r - q_stage2_mf[s2_state, s2_choice]
        q_stage2_mf[s2_state, s2_choice] += lr * delta_stage2
        
        # Stage 2 Update (Generalization to Other Planet)
        # Update the SAME alien on the OTHER planet
        other_planet = 1 - s2_state
        # Treat the reward as evidence for the other planet's alien as well
        delta_gen = r - q_stage2_mf[other_planet, s2_choice]
        q_stage2_mf[other_planet, s2_choice] += lr * coupling * delta_gen
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```