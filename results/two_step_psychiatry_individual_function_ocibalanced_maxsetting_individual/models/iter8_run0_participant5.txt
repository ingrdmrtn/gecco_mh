Here are three new cognitive models for the two-step decision task.

### Model 1: Gated Model-Free with Value Decay
This model assumes the participant uses a Model-Free strategy (learning from prediction errors) but incorporates a heuristic "gating" mechanism based on the transition structure. The agent only updates the value of the first-stage choice if the transition was "Common" (i.e., Spaceship A $\to$ Planet X or Spaceship U $\to$ Planet Y). If a "Rare" transition occurs, the agent attributes the outcome to chance and suppresses the learning signal for the first stage, effectively ignoring misleading data. It also includes value decay for unchosen options to handle drifting rewards.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Gated Model-Free with Value Decay.
    
    A Model-Free agent that only updates Stage 1 values after 'Common' transitions.
    Rare transitions are treated as noise/accidents for Stage 1 learning, preventing
    the reinforcement of actions based on outcomes from the 'wrong' state.
    Includes decay for unchosen actions.

    Parameters:
    - learning_rate: [0, 1] Learning rate for Q-value updates.
    - beta_1: [0, 10] Inverse temperature for Stage 1 softmax.
    - beta_2: [0, 10] Inverse temperature for Stage 2 softmax.
    - decay_rate: [0, 1] Decay rate for unchosen Q-values.
    """
    learning_rate, beta_1, beta_2, decay_rate = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1 = np.zeros(2)       # Q-values for Stage 1 (Spaceships)
    q_stage2 = np.zeros((2, 2))  # Q-values for Stage 2 (Planets, Aliens)

    for trial in range(n_trials):
        # Skip missing data
        if action_1[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta_1 * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        s_idx = state[trial]
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta_2 * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]

        # --- Updates ---
        
        # Update Stage 2 (Standard Q-Learning/Delta Rule)
        delta2 = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += learning_rate * delta2
        
        # Update Stage 1 (Gated)
        # Check if transition was common: (0->0) or (1->1)
        is_common = (a1 == s_idx)
        
        if is_common:
            # Update S1 based on value of chosen option in S2 (SARSA-like)
            delta1 = q_stage2[s_idx, a2] - q_stage1[a1]
            q_stage1[a1] += learning_rate * delta1
        else:
            # Rare transition: Do not update Stage 1 value (Gating)
            pass

        # --- Decay ---
        # Decay unchosen Stage 1
        q_stage1[1 - a1] *= (1.0 - decay_rate)
        
        # Decay unchosen Stage 2 (in the visited state)
        q_stage2[s_idx, 1 - a2] *= (1.0 - decay_rate)
        
        # Decay Stage 2 values in the unvisited state
        q_stage2[1 - s_idx, :] *= (1.0 - decay_rate)

    eps = 1e-10
    valid_mask = (p_choice_1 > 0) & (p_choice_2 > 0)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss
```

### Model 2: Model-Based with Dual Stickiness
This model posits that the participant makes Stage 1 decisions using a pure Model-Based calculation (planning over the transition matrix) but is heavily influenced by habit (stickiness) at *both* stages. Unlike standard models that often only include stickiness at Stage 1, this model allows for separate perseveration parameters for choosing spaceships and choosing aliens.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Based with Dual Stickiness.
    
    A pure Model-Based agent for Stage 1 planning, augmented with separate 
    stickiness (perseveration) parameters for Stage 1 (Spaceship) and 
    Stage 2 (Alien) choices.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for Stage 2 values.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - stick_1: [0, 10] Stickiness weight for Stage 1 choice.
    - stick_2: [0, 10] Stickiness weight for Stage 2 choice.
    """
    learning_rate, beta_1, beta_2, stick_1, stick_2 = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Transition matrix: A(0)->X(0) is 0.7, A(0)->Y(1) is 0.3, etc.
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage2 = np.zeros((2, 2)) # Learned MF values for aliens
    
    prev_a1 = -1
    prev_a2 = -1 # Assuming stickiness applies to the 'alien index' (0 or 1) regardless of planet

    for trial in range(n_trials):
        if action_1[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            prev_a1 = -1
            prev_a2 = -1
            continue

        # --- Stage 1 Choice (Model-Based) ---
        # Calculate MB values: T * max(Q_stage2)
        max_q2 = np.max(q_stage2, axis=1)
        q_mb = transition_matrix @ max_q2
        
        logits_1 = beta_1 * q_mb
        # Add stickiness
        if prev_a1 != -1:
            logits_1[prev_a1] += stick_1
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        s_idx = state[trial]

        # --- Stage 2 Choice ---
        logits_2 = beta_2 * q_stage2[s_idx]
        # Add stickiness (perseveration on left/right alien)
        if prev_a2 != -1:
            logits_2[prev_a2] += stick_2
            
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]

        # --- Update ---
        # Update Stage 2 values
        delta = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += learning_rate * delta
        
        # Store history
        prev_a1 = a1
        prev_a2 = a2

    eps = 1e-10
    valid_mask = (p_choice_1 > 0) & (p_choice_2 > 0)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss
```

### Model 3: Win-Stay Lose-Shift (WSLS) Augmented Model-Free
This model extends the standard Model-Free learning with a dynamic stickiness mechanism known as Win-Stay Lose-Shift. Instead of a single stickiness parameter, the tendency to repeat the previous Stage 1 choice is modulated by the previous reward. A baseline stickiness exists, but it is increased significantly if the previous trial was rewarded.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Win-Stay Lose-Shift (WSLS) Augmented Model-Free.
    
    A Model-Free agent where the probability of repeating the Stage 1 choice
    depends explicitly on the previous trial's outcome.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for Q-values.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - stick_base: [0, 10] Baseline tendency to repeat Stage 1 choice.
    - stick_win: [0, 10] Additional tendency to repeat if previous trial was rewarded.
    """
    learning_rate, beta_1, beta_2, stick_base, stick_win = model_parameters
    n_trials = len(action_1)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    prev_a1 = -1
    prev_reward = 0

    for trial in range(n_trials):
        if action_1[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            prev_a1 = -1
            prev_reward = 0
            continue

        # --- Stage 1 Choice ---
        logits_1 = beta_1 * q_stage1
        
        # Apply WSLS Stickiness
        if prev_a1 != -1:
            # Base stickiness applies always
            bonus = stick_base
            # Win stickiness applies if previous reward was 1
            if prev_reward == 1:
                bonus += stick_win
            
            logits_1[prev_a1] += bonus

        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        s_idx = state[trial]

        # --- Stage 2 Choice ---
        logits_2 = beta_2 * q_stage2[s_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]

        # --- Updates ---
        # Stage 2 update
        delta2 = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += learning_rate * delta2
        
        # Stage 1 update (SARSA: update towards value of chosen S2 option)
        delta1 = q_stage2[s_idx, a2] - q_stage1[a1]
        q_stage1[a1] += learning_rate * delta1
        
        # Store history
        prev_a1 = a1
        prev_reward = r

    eps = 1e-10
    valid_mask = (p_choice_1 > 0) & (p_choice_2 > 0)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss
```