Here are three new cognitive models for the two-step decision task, designed based on the participant data and previous feedback.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Value Decay and Dual Perseveration (Stage 1 & 2).
    
    This model extends the successful Hybrid + Decay architecture by adding 
    perseveration (stickiness) to the Stage 2 alien choice. This tests if the 
    participant exhibits inertia in their choice of aliens (e.g., repeating 
    a choice of Alien 0) independent of the spaceship choice.
    
    Parameters:
    - alpha: [0, 1] Shared learning rate for both stages.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - w_mb: [0, 1] Weight of Model-Based control.
    - decay: [0, 1] Decay rate for unchosen Stage 2 values.
    - pers_1: [0, 10] Perseveration bonus for Stage 1 (Spaceship).
    - pers_2: [0, 10] Perseveration bonus for Stage 2 (Alien).
    """
    alpha, beta_1, beta_2, w_mb, decay, pers_1, pers_2 = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    
    last_action_1 = -1
    # Track last action for each state (Planet X=0, Planet Y=1)
    last_action_2 = np.array([-1, -1]) 
    
    eps = 1e-10

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w_mb * q_stage1_mb + (1 - w_mb) * q_stage1_mf
        logits_1 = beta_1 * q_net_1
        
        if last_action_1 != -1:
            logits_1[last_action_1] += pers_1
            
        probs_1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 /= np.sum(probs_1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        
        # --- Stage 2 Policy ---
        logits_2 = beta_2 * q_stage2_mf[s2]
        
        # Apply perseveration specifically for the alien chosen in this state previously
        if last_action_2[s2] != -1:
            logits_2[last_action_2[s2]] += pers_2
            
        probs_2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 /= np.sum(probs_2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = int(action_2[trial])
        r = int(reward[trial])
        
        # --- Updates ---
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += alpha * delta_stage2
        
        # Value Decay for unchosen alien
        unchosen_a2 = 1 - a2
        q_stage2_mf[s2, unchosen_a2] *= (1.0 - decay)
        
        # Stage 1 Update (SARSA-style)
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * delta_stage1
        
        last_action_1 = a1
        last_action_2[s2] = a2

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Value Decay and Choice Kernel (Stage 1).
    
    This model combines the Hybrid architecture and Value Decay with a Choice Kernel 
    for Stage 1. Unlike simple perseveration (which only considers the previous trial), 
    the Choice Kernel integrates a history of past choices to model more persistent 
    habitual behavior.
    
    Parameters:
    - alpha: [0, 1] Shared learning rate.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - w_mb: [0, 1] Weight of Model-Based control.
    - val_decay: [0, 1] Decay rate for unchosen Stage 2 Q-values.
    - ck_decay: [0, 1] Decay rate for the Choice Kernel (Stage 1).
    - ck_weight: [0, 10] Weight of the Choice Kernel in Stage 1 decisions.
    """
    alpha, beta_1, beta_2, w_mb, val_decay, ck_decay, ck_weight = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Choice kernel for spaceship A/B
    ck_stage1 = np.zeros(2) 
    
    eps = 1e-10

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w_mb * q_stage1_mb + (1 - w_mb) * q_stage1_mf
        
        # Add Choice Kernel to logits
        logits_1 = beta_1 * q_net_1 + ck_weight * ck_stage1
        
        probs_1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 /= np.sum(probs_1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        
        # --- Stage 2 Policy ---
        logits_2 = beta_2 * q_stage2_mf[s2]
        probs_2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 /= np.sum(probs_2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = int(action_2[trial])
        r = int(reward[trial])
        
        # --- Updates ---
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += alpha * delta_stage2
        
        # Value Decay for unchosen alien
        unchosen_a2 = 1 - a2
        q_stage2_mf[s2, unchosen_a2] *= (1.0 - val_decay)
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * delta_stage1
        
        # Update Choice Kernel: Decay existing trace and reinforce chosen action
        ck_stage1 *= (1.0 - ck_decay)
        ck_stage1[a1] += 1.0 
        
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Asymmetric Learning Rates and Value Decay.
    
    This model investigates if the participant learns differently from positive 
    (wins) versus negative (losses) prediction errors. It incorporates this 
    asymmetry into the Hybrid + Decay framework, allowing for differential 
    updating speeds for better and worse outcomes.
    
    Parameters:
    - alpha_pos: [0, 1] Learning rate for positive prediction errors.
    - alpha_neg: [0, 1] Learning rate for negative prediction errors.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - w_mb: [0, 1] Weight of Model-Based control.
    - decay: [0, 1] Decay rate for unchosen Stage 2 values.
    - perseveration: [0, 10] Perseveration bonus for Stage 1.
    """
    alpha_pos, alpha_neg, beta_1, beta_2, w_mb, decay, perseveration = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    eps = 1e-10

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w_mb * q_stage1_mb + (1 - w_mb) * q_stage1_mf
        logits_1 = beta_1 * q_net_1
        
        if last_action_1 != -1:
            logits_1[last_action_1] += perseveration
            
        probs_1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 /= np.sum(probs_1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        
        # --- Stage 2 Policy ---
        logits_2 = beta_2 * q_stage2_mf[s2]
        probs_2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 /= np.sum(probs_2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = int(action_2[trial])
        r = int(reward[trial])
        
        # --- Updates ---
        # Stage 2 Update with Asymmetry
        delta_stage2 = r - q_stage2_mf[s2, a2]
        if delta_stage2 >= 0:
            q_stage2_mf[s2, a2] += alpha_pos * delta_stage2
        else:
            q_stage2_mf[s2, a2] += alpha_neg * delta_stage2
            
        # Value Decay
        unchosen_a2 = 1 - a2
        q_stage2_mf[s2, unchosen_a2] *= (1.0 - decay)
        
        # Stage 1 Update with Asymmetry
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        if delta_stage1 >= 0:
            q_stage1_mf[a1] += alpha_pos * delta_stage1
        else:
            q_stage1_mf[a1] += alpha_neg * delta_stage1
            
        last_action_1 = a1

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```