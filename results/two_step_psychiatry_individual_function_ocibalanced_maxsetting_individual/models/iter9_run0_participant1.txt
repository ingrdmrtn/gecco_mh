Here are the 3 new cognitive models as Python functions.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Independent Betas with Stage 2 Decay.
    
    This model separates Model-Based and Model-Free control using independent 
    inverse temperature parameters (betas). Additionally, it implements a 
    decay mechanism for the unchosen options in Stage 2. This allows the 
    model to 'forget' the value of unvisited aliens, which is useful in 
    environments where reward probabilities drift over time.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate for Q-value updates.
    decay: [0, 1] - Decay rate for unchosen Stage 2 actions (1 = full decay to 0).
    beta_mb: [0, 10] - Inverse temperature for Model-Based values (Stage 1).
    beta_mf: [0, 10] - Inverse temperature for Model-Free values (Stage 1 & 2).
    stickiness: [0, 5] - Choice perseverance bonus for Stage 1.
    """
    learning_rate, decay, beta_mb, beta_mf, stickiness = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_mf_stage1 = np.zeros(2)
    q_mf_stage2 = np.zeros((2, 2)) # State x Action
    
    last_action_1 = -1
    log_loss = 0.0
    eps = 1e-10
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Handle missing data
        if a1 == -1 or s_idx == -1 or a2 == -1:
            continue

        # --- Stage 1 Choice ---
        # Model-Based Value calculation
        max_q_stage2 = np.max(q_mf_stage2, axis=1)
        q_mb_stage1 = transition_matrix @ max_q_stage2
        
        # Combined Logits
        logits_stage1 = (beta_mb * q_mb_stage1) + (beta_mf * q_mf_stage1)
        
        # Stickiness
        if last_action_1 != -1:
            logits_stage1[last_action_1] += stickiness
            
        # Softmax Stage 1
        exp_q1 = np.exp(logits_stage1 - np.max(logits_stage1)) # Numerical stability
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        log_loss -= np.log(probs_1[a1] + eps)
        
        # --- Stage 2 Choice ---
        # Model-Free only for Stage 2
        logits_stage2 = beta_mf * q_mf_stage2[s_idx]
        exp_q2 = np.exp(logits_stage2 - np.max(logits_stage2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        log_loss -= np.log(probs_2[a2] + eps)
        
        # --- Learning ---
        # Update Stage 2 Chosen
        q_mf_stage2[s_idx, a2] += learning_rate * (r - q_mf_stage2[s_idx, a2])
        
        # Decay Stage 2 Unchosen
        unchosen_a2 = 1 - a2
        q_mf_stage2[s_idx, unchosen_a2] *= (1.0 - decay)
        
        # Update Stage 1 (Direct Reinforcement)
        q_mf_stage1[a1] += learning_rate * (r - q_mf_stage1[a1])
        
        last_action_1 = a1
        
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Independent Betas with Reward-Dependent Stickiness.
    
    This model separates MB and MF control strengths using independent betas. 
    It refines the stickiness mechanism by allowing the perseverance bonus 
    to differ based on the previous trial's outcome. This captures 'Win-Stay' 
    and 'Lose-Stay' (or shift) dynamics explicitly within the hybrid framework.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate for Q-value updates.
    beta_mb: [0, 10] - Inverse temperature for Model-Based values.
    beta_mf: [0, 10] - Inverse temperature for Model-Free values.
    stick_win: [0, 5] - Stickiness bonus applied if the previous trial was rewarded.
    stick_lose: [0, 5] - Stickiness bonus applied if the previous trial was unrewarded.
    """
    learning_rate, beta_mb, beta_mf, stick_win, stick_lose = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_mf_stage1 = np.zeros(2)
    q_mf_stage2 = np.zeros((2, 2))
    
    last_action_1 = -1
    last_reward = -1
    log_loss = 0.0
    eps = 1e-10
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        if a1 == -1 or s_idx == -1 or a2 == -1:
            continue

        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_mf_stage2, axis=1)
        q_mb_stage1 = transition_matrix @ max_q_stage2
        
        logits_stage1 = (beta_mb * q_mb_stage1) + (beta_mf * q_mf_stage1)
        
        # Reward-Dependent Stickiness
        if last_action_1 != -1:
            if last_reward == 1:
                logits_stage1[last_action_1] += stick_win
            else:
                logits_stage1[last_action_1] += stick_lose
                
        exp_q1 = np.exp(logits_stage1 - np.max(logits_stage1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        log_loss -= np.log(probs_1[a1] + eps)
        
        # --- Stage 2 Choice ---
        logits_stage2 = beta_mf * q_mf_stage2[s_idx]
        exp_q2 = np.exp(logits_stage2 - np.max(logits_stage2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        log_loss -= np.log(probs_2[a2] + eps)
        
        # --- Learning ---
        q_mf_stage2[s_idx, a2] += learning_rate * (r - q_mf_stage2[s_idx, a2])
        q_mf_stage1[a1] += learning_rate * (r - q_mf_stage1[a1])
        
        last_action_1 = a1
        last_reward = r 

    return log_loss

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Independent Betas with Separate Learning Rates.
    
    This model allows for different learning rates for the first-stage 
    (Spaceship choice) and second-stage (Alien choice) values, combined 
    with independent MB/MF control strengths. This accounts for potential 
    differences in how quickly the participant updates beliefs about 
    stable transitions versus drifting reward probabilities.
    
    Parameters:
    lr_s1: [0, 1] - Learning rate for Stage 1 (Spaceship) Q-values.
    lr_s2: [0, 1] - Learning rate for Stage 2 (Alien) Q-values.
    beta_mb: [0, 10] - Inverse temperature for Model-Based values.
    beta_mf: [0, 10] - Inverse temperature for Model-Free values.
    stickiness: [0, 5] - Choice perseverance bonus for Stage 1.
    """
    lr_s1, lr_s2, beta_mb, beta_mf, stickiness = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_mf_stage1 = np.zeros(2)
    q_mf_stage2 = np.zeros((2, 2))
    
    last_action_1 = -1
    log_loss = 0.0
    eps = 1e-10
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        if a1 == -1 or s_idx == -1 or a2 == -1:
            continue

        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_mf_stage2, axis=1)
        q_mb_stage1 = transition_matrix @ max_q_stage2
        
        logits_stage1 = (beta_mb * q_mb_stage1) + (beta_mf * q_mf_stage1)
        
        if last_action_1 != -1:
            logits_stage1[last_action_1] += stickiness
            
        exp_q1 = np.exp(logits_stage1 - np.max(logits_stage1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        log_loss -= np.log(probs_1[a1] + eps)
        
        # --- Stage 2 Choice ---
        logits_stage2 = beta_mf * q_mf_stage2[s_idx]
        exp_q2 = np.exp(logits_stage2 - np.max(logits_stage2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        log_loss -= np.log(probs_2[a2] + eps)
        
        # --- Learning ---
        # Stage 2 update uses lr_s2
        q_mf_stage2[s_idx, a2] += lr_s2 * (r - q_mf_stage2[s_idx, a2])
        
        # Stage 1 update uses lr_s1
        q_mf_stage1[a1] += lr_s1 * (r - q_mf_stage1[a1])
        
        last_action_1 = a1

    return log_loss
```