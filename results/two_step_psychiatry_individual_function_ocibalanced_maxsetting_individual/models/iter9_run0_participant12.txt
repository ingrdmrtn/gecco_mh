Here are three new cognitive models for the two-step task, designed to capture the participant's specific behavioral patterns such as long streaks of choices and switching behavior.

### Model 1: Surprise-Modulated Arbitration
This model hypothesizes that the balance between Model-Based (MB) and Model-Free (MF) control is not static but dynamic, modulated by the "surprise" (prediction error) experienced in the previous transition. High prediction errors (unexpected transitions) may reduce confidence in the Model-Based system (or the Model-Free system). Here, we model it such that high surprise reduces the Model-Based weight, causing a reversion to Model-Free habits, or vice versa.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Surprise-Modulated Arbitration Model.
    
    The weight `w` (MB vs MF) is dynamically adjusted based on the absolute 
    prediction error (surprise) from the previous Stage 1 transition.
    High surprise reduces the effective MB weight, shifting control towards MF.
    
    Parameters:
    lr (float): Learning rate [0, 1].
    beta (float): Inverse temperature [0, 10].
    w_base (float): Base Model-Based weight [0, 1].
    lam (float): Eligibility trace parameter [0, 1].
    stick (float): Choice stickiness for Stage 1 [0, 5].
    theta (float): Sensitivity to surprise (decay of w with error) [0, 10].
    """
    lr, beta, w_base, lam, stick, theta = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    last_delta_1 = 0.0
    log_loss = 0
    eps = 1e-10
    
    for trial in range(n_trials):
        # 1. Calculate effective w based on previous surprise
        # w_eff = w_base / (1 + theta * |delta|)
        w_eff = w_base / (1.0 + theta * np.abs(last_delta_1))

        # 2. Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w_eff * q_stage1_mb + (1 - w_eff) * q_stage1_mf
        logits_1 = beta * q_net_1
        
        if last_action_1 != -1:
            logits_1[last_action_1] += stick
            
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        if action_1[trial] != -1:
            log_loss -= np.log(probs_1[action_1[trial]] + eps)
            last_action_1 = action_1[trial]
        
        state_idx = state[trial]
        if state_idx == -1:
            continue

        # 3. Stage 2 Policy
        logits_2 = beta * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = action_2[trial]
        if a2 != -1:
            log_loss -= np.log(probs_2[a2] + eps)
            
            # 4. Updates
            r = reward[trial]
            if r == -1: r = 0
            
            a1 = action_1[trial]
            
            # Prediction error at transition
            delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
            last_delta_1 = delta_stage1 # Store for next trial's w modulation
            
            # Prediction error at reward
            delta_stage2 = r - q_stage2_mf[state_idx, a2]
            
            # Update Stage 1 MF
            q_stage1_mf[a1] += lr * delta_stage1 + lr * lam * delta_stage2
            
            # Update Stage 2 MF
            q_stage2_mf[state_idx, a2] += lr * delta_stage2

    return log_loss
```

### Model 2: Cumulative Perseveration
This model addresses the extremely long streaks of repeated choices (e.g., trials 107-151) by implementing a "cumulative stickiness" mechanism. Unlike standard stickiness which only considers the immediately preceding choice, this model builds up a stickiness trace that grows with repeated choices and decays slowly. This creates a strong "inertia" that makes switching difficult after a long streak.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Cumulative Perseveration Model.
    
    Stickiness is modeled as a trace that accumulates when an action is chosen 
    and decays over time. This captures the 'inertia' seen in long streaks 
    of identical choices.
    
    Parameters:
    lr (float): Learning rate [0, 1].
    beta (float): Inverse temperature [0, 10].
    w (float): MB/MF mixing weight [0, 1].
    lam (float): Eligibility trace parameter [0, 1].
    stick_decay (float): Decay rate of the stickiness trace [0, 1]. 
                         (1 = no decay, 0 = instant decay).
    stick_weight (float): Weight of the cumulative stickiness trace [0, 5].
    """
    lr, beta, w, lam, stick_decay, stick_weight = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Cumulative stickiness trace for Stage 1 actions
    stick_trace = np.zeros(2)
    
    log_loss = 0
    eps = 1e-10
    
    for trial in range(n_trials):

        # 1. Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add cumulative stickiness to logits
        logits_1 = beta * q_net_1 + stick_weight * stick_trace
        
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = action_1[trial]
        if a1 != -1:
            log_loss -= np.log(probs_1[a1] + eps)
            
            # Update stickiness trace
            stick_trace *= stick_decay
            stick_trace[a1] += 1.0
        else:
            # Even if no action recorded, trace decays
            stick_trace *= stick_decay
            
        state_idx = state[trial]
        if state_idx == -1:
            continue

        # 2. Stage 2 Policy
        logits_2 = beta * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = action_2[trial]
        if a2 != -1:
            log_loss -= np.log(probs_2[a2] + eps)
            
            r = reward[trial]
            if r == -1: r = 0

            delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
            delta_stage2 = r - q_stage2_mf[state_idx, a2]
            
            q_stage1_mf[a1] += lr * delta_stage1 + lr * lam * delta_stage2
            q_stage2_mf[state_idx, a2] += lr * delta_stage2

    return log_loss
```

### Model 3: Planet Stickiness (Outcome-Based Perseveration)
This model proposes that the participant's perseveration is not just on the *action* (spaceship) but on the *outcome* (planet). If they visited Planet X on the previous trial, they have a bias to visit Planet X again. This bias is incorporated into the Model-Based calculation: the agent prefers actions that transition to the previously visited planet, according to the transition matrix.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Planet Stickiness Model.
    
    Includes a specific perseveration bias for the previously visited Planet.
    The agent receives a value bonus for actions that are likely to lead 
    to the same planet as the previous trial. This is mediated by the 
    transition matrix.
    
    Parameters:
    lr (float): Learning rate [0, 1].
    beta (float): Inverse temperature [0, 10].
    w (float): MB/MF mixing weight [0, 1].
    lam (float): Eligibility trace parameter [0, 1].
    stick_action (float): Standard stickiness for the spaceship choice [0, 5].
    stick_planet (float): Stickiness for the previously visited planet [0, 5].
    """
    lr, beta, w, lam, stick_action, stick_planet = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    last_planet = -1
    log_loss = 0
    eps = 1e-10
    
    for trial in range(n_trials):

        # 1. Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        
        # Calculate MB values
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Add Planet Stickiness to MB component (or as a separate bias)
        # We calculate the probability of reaching the last planet for each action
        planet_stick_bonus = np.zeros(2)
        if last_planet != -1:
            # transition_matrix[a, s] gives P(s|a)
            # We want P(last_planet | a) for a=0,1
            planet_stick_bonus = transition_matrix[:, last_planet] * stick_planet
            
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        logits_1 = beta * q_net_1 + planet_stick_bonus
        
        if last_action_1 != -1:
            logits_1[last_action_1] += stick_action
            
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = action_1[trial]
        if a1 != -1:
            log_loss -= np.log(probs_1[a1] + eps)
            last_action_1 = a1
            
        state_idx = state[trial]
        if state_idx == -1:
            continue
            
        last_planet = state_idx # Update last planet

        # 2. Stage 2 Policy
        logits_2 = beta * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = action_2[trial]
        if a2 != -1:
            log_loss -= np.log(probs_2[a2] + eps)
            
            r = reward[trial]
            if r == -1: r = 0

            delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
            delta_stage2 = r - q_stage2_mf[state_idx, a2]
            
            q_stage1_mf[a1] += lr * delta_stage1 + lr * lam * delta_stage2
            q_stage2_mf[state_idx, a2] += lr * delta_stage2

    return log_loss
```