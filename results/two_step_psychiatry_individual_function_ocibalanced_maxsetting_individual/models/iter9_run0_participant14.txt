def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF model with Stage-Specific and Asymmetric Learning.
    
    This model posits that learning mechanisms differ between the transition-based first stage
    and the reward-based second stage. It uses a specific learning rate for the first stage
    (and eligibility traces), while the second stage updates are sensitive to reward valence
    (asymmetric learning for positive vs. negative prediction errors).

    Parameters:
    - lr_s1: [0, 1] Learning rate for Stage 1 updates (and eligibility trace).
    - lr_pos: [0, 1] Learning rate for Stage 2 positive prediction errors.
    - lr_neg: [0, 1] Learning rate for Stage 2 negative prediction errors.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weight of Model-Based system.
    - lam: [0, 1] Eligibility trace decay.
    - stickiness: [0, 5] Choice perseveration bonus.
    """
    lr_s1, lr_pos, lr_neg, beta, w, lam, stickiness = model_parameters
    n_trials = len(action_1)
    
    # Fixed transition matrix for MB system (Standard belief)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Initialize Q-values
    # q_mf: Stage 1 Model-Free values (2 actions: 0, 1)
    q_mf = np.zeros(2)
    # q2: Stage 2 values (2 states x 2 actions)
    q2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1
    eps = 1e-10
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])
        
        # Handle missing data
        if a1 == -1:
            continue
            
        # --- Stage 1 Choice ---
        # Model-Based Value calculation
        max_q2 = np.max(q2, axis=1)
        q_mb = transition_matrix @ max_q2
        
        # Net Value (Hybrid)
        q_net = w * q_mb + (1 - w) * q_mf
        
        # Apply Stickiness to Stage 1 choice
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        # Softmax for Stage 1
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Choice ---
        # Stage 2 depends only on Q2 values for the current state
        exp_q2 = np.exp(beta * q2[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        
        # Stage 1 PE (TD error)
        # Uses lr_s1 as this is a Stage 1 update
        delta_1 = q2[s, a2] - q_mf[a1]
        q_mf[a1] += lr_s1 * delta_1
        
        # Stage 2 PE (Reward prediction error)
        delta_2 = r - q2[s, a2]
        
        # Asymmetric Learning for Stage 2 (Valence dependent)
        if delta_2 > 0:
            current_lr2 = lr_pos
        else:
            current_lr2 = lr_neg
            
        q2[s, a2] += current_lr2 * delta_2
        
        # Eligibility Trace Update for Stage 1
        # Propagates Stage 2 error back to Stage 1 values
        # Uses lr_s1 to maintain consistency with Stage 1 plasticity
        q_mf[a1] += lr_s1 * lam * delta_2
        
        last_action_1 = a1

    # Calculate Negative Log Likelihood
    mask = p_choice_1 > 0
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF model with Subjective Transition Belief.
    
    This model assumes the participant does not use the objective transition probability (0.7)
    for model-based planning, but instead relies on a subjective belief parameter (p_belief).
    This allows the model to capture distorted perceptions of transition structure 
    (e.g., believing transitions are more random or more deterministic than reality).

    Parameters:
    - lr: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weight of Model-Based system.
    - lam: [0, 1] Eligibility trace.
    - stickiness: [0, 5] Choice perseveration bonus.
    - p_belief: [0, 1] Subjective probability of common transitions (A->X, U->Y).
    """
    lr, beta, w, lam, stickiness, p_belief = model_parameters
    n_trials = len(action_1)
    
    # Construct Subjective Transition Matrix based on parameter
    transition_matrix = np.array([[p_belief, 1.0 - p_belief], [1.0 - p_belief, p_belief]])
    
    q_mf = np.zeros(2)
    q2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1
    eps = 1e-10
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])
        
        if a1 == -1:
            continue
            
        # Stage 1 Policy (Model-Based)
        max_q2 = np.max(q2, axis=1)
        # Use the subjective transition matrix for planning
        q_mb = transition_matrix @ max_q2 
        
        q_net = w * q_mb + (1 - w) * q_mf
        
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Stage 2 Policy
        exp_q2 = np.exp(beta * q2[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # Learning (Standard Hybrid Updates)
        delta_1 = q2[s, a2] - q_mf[a1]
        q_mf[a1] += lr * delta_1
        
        delta_2 = r - q2[s, a2]
        q2[s, a2] += lr * delta_2
        
        q_mf[a1] += lr * lam * delta_2
        
        last_action_1 = a1
        
    mask = p_choice_1 > 0
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF model with Counterfactual Updating (Fictitious Play).
    
    This model implements a "counterfactual" or "fictitious play" mechanism where the 
    participant updates the value of the unchosen alien in the second stage.
    The update assumes an anti-correlated reward structure: if the chosen alien yields 
    a reward (1), the unchosen is updated as if it yielded 0, and vice versa.

    Parameters:
    - lr: [0, 1] Learning rate for the chosen option.
    - lr_cf: [0, 1] Counterfactual learning rate for the unchosen option.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weight of Model-Based system.
    - lam: [0, 1] Eligibility trace.
    - stickiness: [0, 5] Choice perseveration bonus.
    """
    lr, lr_cf, beta, w, lam, stickiness = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_mf = np.zeros(2)
    q2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1
    eps = 1e-10
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])
        
        if a1 == -1:
            continue
            
        # Stage 1 Policy
        max_q2 = np.max(q2, axis=1)
        q_mb = transition_matrix @ max_q2
        
        q_net = w * q_mb + (1 - w) * q_mf
        
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Stage 2 Policy
        exp_q2 = np.exp(beta * q2[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # Learning
        
        # 1. Update Chosen Option (Standard)
        delta_1 = q2[s, a2] - q_mf[a1]
        q_mf[a1] += lr * delta_1
        
        delta_2 = r - q2[s, a2]
        q2[s, a2] += lr * delta_2
        
        # 2. Update Unchosen Option (Counterfactual)
        # Assume rewards are binary 0/1. Infer opposite outcome.
        r_cf = 1 - r 
        unchosen_a2 = 1 - a2
        delta_cf = r_cf - q2[s, unchosen_a2]
        q2[s, unchosen_a2] += lr_cf * delta_cf
        
        # 3. Eligibility Trace (Driven by actual experience)
        q_mf[a1] += lr * lam * delta_2
        
        last_action_1 = a1
        
    mask = p_choice_1 > 0
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss