Here are three cognitive models based on the participant's data and the two-step task structure.

### Model 1: Hybrid MB/MF with Eligibility Traces (TD-$\lambda$)
This model hypothesizes that the participant does not rely solely on the "Step 2 Value" to update Step 1 (TD(0)), nor solely on the final "Reward" (TD(1)). Instead, they use an eligibility trace ($\lambda$) to blend these updates. This allows the Model-Free system to learn from the final outcome directly, potentially compensating for Model-Based deficits.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model-Based / Model-Free RL with Eligibility Traces (TD-Lambda).
    
    Combines Model-Based planning with a Model-Free system that updates via 
    an eligibility trace parameter (lambda). This interpolates between 
    updating Stage 1 based on Stage 2 values (TD(0)) and the final reward (TD(1)).
    
    Parameters:
    - alpha: [0, 1] Learning rate for value updates.
    - lambda_elig: [0, 1] Eligibility trace decay. 0 = TD(0) (chaining), 1 = TD(1) (direct reward).
    - beta_mb: [0, 10] Inverse temperature for Model-Based values in Stage 1.
    - beta_mf: [0, 10] Inverse temperature for Model-Free values in Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2 choice.
    - stickiness: [0, 10] Choice perseverance bonus for Stage 1.
    """
    alpha, lambda_elig, beta_mb, beta_mf, beta_2, stickiness = model_parameters
    n_trials = len(action_1)
  
    # Fixed transition structure for Model-Based system
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Initialize Q-values to 0.5 (chance expectation for 0/1 rewards)
    q_stage1_mf = np.zeros(2) + 0.5
    q_stage2_mf = np.zeros((2, 2)) + 0.5
    
    last_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based Value Calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combine MB and MF values
        logits_1 = (beta_mb * q_stage1_mb) + (beta_mf * q_stage1_mf)
        
        # Apply Stickiness
        if last_action_1 != -1:
            logits_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        # Handle missing data
        if action_1[trial] == -1:
            last_action_1 = -1
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue
            
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Choice ---
        state_idx = state[trial]
        
        logits_2 = beta_2 * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        if action_2[trial] == -1:
            # If stage 2 is missing, we cannot update values properly
            last_action_1 = action_1[trial]
            p_choice_2[trial] = 1.0
            continue
            
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        r = reward[trial]
        a1 = action_1[trial]
        a2 = action_2[trial]
        
        # Stage 2 RPE (Standard)
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += alpha * delta_stage2
        
        # Stage 1 Update with Eligibility Trace
        # TD(0) target: Value of the state we arrived at (q_stage2_mf[state_idx, a2])
        # TD(1) target: The actual reward (r)
        # Note: We use the value *before* the stage 2 update for the TD(0) component in standard SARSA,
        # but here we use the updated value or the value of the chosen option. 
        # We calculate the effective target blending the two.
        
        target_td0 = q_stage2_mf[state_idx, a2] # Using updated value (proxy for Sarsa)
        target_td1 = r
        
        mixed_target = (1 - lambda_elig) * target_td0 + lambda_elig * target_td1
        delta_stage1 = mixed_target - q_stage1_mf[a1]
        
        q_stage1_mf[a1] += alpha * delta_stage1
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid MB/MF with Asymmetric Learning and Stage 2 Forgetting
This model extends the "Best Model" (which was purely MB at Stage 1) by adding a Model-Free component to Stage 1. It posits that the participant uses a Hybrid strategy, but their habitual system (Stage 2) is sensitive to reward valence (learning more from positive or negative outcomes) and subject to forgetting due to the drifting environment.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF with Asymmetric Learning and Forgetting.
    
    Extends the previous best model by adding a Model-Free component to Stage 1.
    Features:
    1. Separate learning rates for positive/negative prediction errors (Stage 2).
    2. Passive forgetting of Stage 2 values (drift).
    3. Hybrid MB+MF control at Stage 1.
    
    Parameters:
    - alpha_pos: [0, 1] Learning rate for positive RPEs (Stage 2).
    - alpha_neg: [0, 1] Learning rate for negative RPEs (Stage 2).
    - forget: [0, 1] Decay rate of Stage 2 values towards chance (0.5).
    - beta_mb: [0, 10] Weight of Model-Based values in Stage 1.
    - beta_mf: [0, 10] Weight of Model-Free values in Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - stickiness: [0, 10] Perseverance bonus.
    """
    alpha_pos, alpha_neg, forget, beta_mb, beta_mf, beta_2, stickiness = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2) + 0.5
    q_stage2_mf = np.zeros((2, 2)) + 0.5
    
    last_action_1 = -1

    for trial in range(n_trials):
        # Decay Stage 2 values (simulate forgetting/drift)
        q_stage2_mf = (1 - forget) * q_stage2_mf + forget * 0.5
        
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid mixture
        logits_1 = (beta_mb * q_stage1_mb) + (beta_mf * q_stage1_mf)
        
        if last_action_1 != -1:
            logits_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        if action_1[trial] == -1:
            last_action_1 = -1
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue
            
        p_choice_1[trial] = probs_1[action_1[trial]]

        # --- Stage 2 Choice ---
        state_idx = state[trial]
        
        logits_2 = beta_2 * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        if action_2[trial] == -1:
            last_action_1 = action_1[trial]
            p_choice_2[trial] = 1.0
            continue
            
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        r = reward[trial]
        a1 = action_1[trial]
        a2 = action_2[trial]
        
        # Stage 2 Update (Asymmetric)
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        if delta_stage2 >= 0:
            q_stage2_mf[state_idx, a2] += alpha_pos * delta_stage2
        else:
            q_stage2_mf[state_idx, a2] += alpha_neg * delta_stage2
            
        # Stage 1 Update (Standard TD(0) using the updated S2 value)
        # Note: We use average alpha for Stage 1 or just alpha_pos? 
        # To keep params low, we reuse alpha_pos for Stage 1 (assuming general plasticity)
        # or we could assume Stage 1 is less sensitive. Let's use alpha_pos.
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha_pos * delta_stage1

        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid MB/MF with Cumulative Habit Trace
The participant exhibits long streaks of repeating the same spaceship choice. Simple "stickiness" (repeating the last action) may not capture the *strength* of this habit. This model implements a "Cumulative Habit Trace" where the tendency to repeat an action builds up over time if repeated, and decays slowly, acting as a third system alongside MB and MF.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF with Cumulative Habit Trace.
    
    Instead of binary stickiness (last trial), this model maintains a continuous
    'Habit Trace' that accumulates when actions are chosen and decays over time.
    This captures strong preservation behavior seen in the data.
    
    Parameters:
    - alpha: [0, 1] Learning rate.
    - habit_decay: [0, 1] Decay rate of the habit trace (1 = no decay, 0 = instant decay).
    - habit_strength: [0, 10] Weight of the habit trace in the decision.
    - beta_mb: [0, 10] Weight of Model-Based values.
    - beta_mf: [0, 10] Weight of Model-Free values.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    """
    alpha, habit_decay, habit_strength, beta_mb, beta_mf, beta_2 = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2) + 0.5
    q_stage2_mf = np.zeros((2, 2)) + 0.5
    
    # Habit trace for Stage 1 actions (A vs U)
    habit_trace = np.zeros(2)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Logits = MB + MF + Habit
        logits_1 = (beta_mb * q_stage1_mb) + \
                   (beta_mf * q_stage1_mf) + \
                   (habit_strength * habit_trace)
            
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        if action_1[trial] == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue
            
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Update Habit Trace: Decay existing, boost chosen
        habit_trace = habit_trace * habit_decay
        habit_trace[action_1[trial]] += 1.0 

        # --- Stage 2 Choice ---
        state_idx = state[trial]
        
        logits_2 = beta_2 * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        if action_2[trial] == -1:
            p_choice_2[trial] = 1.0
            continue
            
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        r = reward[trial]
        a1 = action_1[trial]
        a2 = action_2[trial]
        
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += alpha * delta_stage2
        
        # Stage 1 Update (TD(0))
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```