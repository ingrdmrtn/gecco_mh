Here are three new cognitive models for the two-step decision task, designed based on the participant's data and the exploration history.

### Cognitive Model 1
**Rationale:** This model combines Model-Based (MB) planning with Model-Free (MF) learning. It introduces **separate inverse temperatures (betas)** for Stage 1 and Stage 2, acknowledging that the participant's decision noise might differ between selecting a spaceship (strategic, stable) and an alien (drifting reward probabilities). Additionally, it includes **stickiness** (perseveration) on the Stage 1 choice, which helps explain the observed streaks of choosing the same spaceship.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF Model with Separate Betas and Choice Stickiness.
    
    Combines Model-Based planning (using known transitions) and Model-Free learning.
    Uses distinct noise parameters for the spaceship choice (Stage 1) and alien choice (Stage 2).
    Includes a stickiness parameter to account for choice repetition in Stage 1.
    
    Parameters:
    - alpha: [0, 1] Learning rate for Q-value updates.
    - beta_1: [0, 10] Inverse temperature for Stage 1 (Spaceship choice).
    - beta_2: [0, 10] Inverse temperature for Stage 2 (Alien choice).
    - w: [0, 1] Weighting parameter (0 = Pure MF, 1 = Pure MB).
    - stickiness: [0, 10] Choice perseveration bonus for Stage 1.
    """
    alpha, beta_1, beta_2, w, stickiness = model_parameters
    n_trials = len(action_1)
    
    # Transition matrix: A0->S0 (0.7), A1->S1 (0.7)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # Model-Based Value: Expected max Q_stage2 based on transition probabilities
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Q-value (Hybrid)
        q_net_s1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Calculate logits with stickiness
        logits_1 = beta_1 * q_net_s1
        if last_action_1 != -1:
            logits_1[last_action_1] += stickiness
            
        # Softmax for Stage 1
        exp_q1 = np.exp(logits_1 - np.max(logits_1)) # Subtract max for stability
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        
        # --- Stage 2 Decision ---
        logits_2 = beta_2 * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        # Stage 2 Prediction Error (Reward - Q_stage2)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += alpha * delta_stage2
        
        # Stage 1 Prediction Error (Q_stage2 - Q_stage1_MF)
        # Updates MF value of the chosen spaceship based on the value of the state reached
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += alpha * delta_stage1
        
        last_action_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2
**Rationale:** This model proposes that the participant uses different learning rates for the two stages (`alpha_1` and `alpha_2`). The value of spaceships (Stage 1) might be updated at a different rate than the rapidly drifting reward probabilities of aliens (Stage 2). This is combined with the Hybrid MB/MF architecture and separate betas to capture the full structural dynamics without the assumption of asymmetric positive/negative learning.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF Model with Stage-Specific Learning Rates.
    
    Differentiates the learning process between Stage 1 (Spaceships) and Stage 2 (Aliens).
    Allows the model to track the slowly changing spaceship values and the drifting alien 
    rewards with different sensitivities (alpha_1 vs alpha_2).
    
    Parameters:
    - alpha_1: [0, 1] Learning rate for Stage 1 (Spaceship value update).
    - alpha_2: [0, 1] Learning rate for Stage 2 (Alien reward update).
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - w: [0, 1] Weighting parameter (0 = Pure MF, 1 = Pure MB).
    """
    alpha_1, alpha_2, beta_1, beta_2, w = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_s1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta_1 * q_net_s1
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        
        # --- Stage 2 Decision ---
        logits_2 = beta_2 * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        # Update Stage 2 values with alpha_2 (driven by reward)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += alpha_2 * delta_stage2
        
        # Update Stage 1 MF values with alpha_1 (driven by Stage 2 value)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += alpha_1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3
**Rationale:** This is a Model-Free **TD(Lambda)** model. Instead of an explicit Model-Based planner, it uses an eligibility trace parameter (`lam`) to allow the reward outcome at Stage 2 to directly influence the value of the chosen spaceship at Stage 1. This captures the "credit assignment" logic without assuming the participant calculates transition matrices. It also includes separate betas and stickiness to handle the specific noise and repetition patterns in the data.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    TD(Lambda) Model-Free RL with Separate Betas and Stickiness.
    
    Uses eligibility traces (lambda) to connect Stage 2 rewards directly to Stage 1 choices.
    This allows for faster learning of Stage 1 values based on final outcomes.
    Includes separate noise levels (beta_1, beta_2) and choice stickiness.
    
    Parameters:
    - alpha: [0, 1] Learning rate.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - lam: [0, 1] Eligibility trace parameter (0 = TD(0), 1 = Monte Carlo).
    - stickiness: [0, 10] Choice perseveration bonus for Stage 1.
    """
    alpha, beta_1, beta_2, lam, stickiness = model_parameters
    n_trials = len(action_1)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        logits_1 = beta_1 * q_stage1
        if last_action_1 != -1:
            logits_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        
        # --- Stage 2 Decision ---
        logits_2 = beta_2 * q_stage2[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        # Stage 2 Prediction Error
        delta_stage2 = reward[trial] - q_stage2[state_idx, action_2[trial]]
        
        # Stage 1 Prediction Error (Standard Temporal Difference)
        delta_stage1 = q_stage2[state_idx, action_2[trial]] - q_stage1[action_1[trial]]
        
        # Update Stage 2 Q-values
        q_stage2[state_idx, action_2[trial]] += alpha * delta_stage2
        
        # Update Stage 1 Q-values
        # Combines TD update (delta_stage1) and eligibility trace update (lam * delta_stage2)
        q_stage1[action_1[trial]] += alpha * delta_stage1 + alpha * lam * delta_stage2
        
        last_action_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```