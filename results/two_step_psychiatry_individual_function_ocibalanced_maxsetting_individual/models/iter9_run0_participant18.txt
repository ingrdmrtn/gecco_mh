Here are three new cognitive models for the two-step task, designed to capture different psychological mechanisms such as selective memory, counterfactual reasoning, and state-based perseveration.

### Model 1: Selective Decay (Memory Interference)
This model hypothesizes that the participant actively maintains the value of chosen options (via reinforcement) but "forgets" the value of unchosen options over time. Unlike the standard decay model which decays *all* values, this model only applies decay to options that were *not* selected in the current trial, preserving the learned value of the active choice.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    MB/MF model with Selective Decay for Unchosen Options.
    Instead of decaying all Q-values, this model only applies decay to the 
    options that were NOT chosen in the current trial. Chosen options are 
    updated via standard RL. This simulates 'forgetting' of unvisited paths.

    Parameters:
    lr (float): Learning rate for chosen options [0,1].
    beta (float): Inverse temperature [0,10].
    w (float): MB/MF Weighting [0,1].
    decay_unc (float): Decay rate for UNCHOSEN options [0,1].
    pers (float): Perseveration bonus for Stage 1 choice [0,5].
    """
    lr, beta, w, decay_unc, pers = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Initialize Q-values
    q_stage1_mf = np.zeros(2)     # Model-free values for Stage 1 (Spaceships)
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for Stage 2 (Planets x Aliens)
    
    prev_a1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue

        # --- Stage 1 Policy (MB + MF) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net_stage1
        if prev_a1 != -1:
            logits_1[prev_a1] += pers
            
        # Softmax Stage 1
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        logits_2 = beta * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        
        # 1. Decay Unchosen Stage 1 Option
        unchosen_s1 = 1 - action_1[trial]
        q_stage1_mf[unchosen_s1] *= (1 - decay_unc)
        
        # 2. Decay Unchosen Stage 2 Options
        # Decay the unchosen alien in the current planet
        unchosen_s2 = 1 - action_2[trial]
        q_stage2_mf[state_idx, unchosen_s2] *= (1 - decay_unc)
        # Decay both aliens in the unvisited planet
        q_stage2_mf[1 - state_idx, :] *= (1 - decay_unc)

        # 3. Standard RL Update for Chosen Options
        # Stage 1 Update (Direct reinforcement from Stage 2 value)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr * delta_stage1
        
        # Stage 2 Update (Reward prediction error)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr * delta_stage2
        
        # Eligibility trace-like update for Stage 1 based on Stage 2 RPE
        q_stage1_mf[action_1[trial]] += lr * delta_stage2
        
        prev_a1 = action_1[trial]

    eps = 1e-10
    valid_mask = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss
```

### Model 2: Counterfactual Stage 2 Learning
This model posits that the participant infers the outcome of the *unchosen* alien in Stage 2. Specifically, it assumes an anti-correlation structure (common in such tasks): if the chosen alien gives a reward, the unchosen one likely wouldn't have, and vice versa. The unchosen option is updated towards `1 - Reward` with a separate learning rate.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    MB/MF model with Counterfactual Updating in Stage 2.
    Updates the unchosen alien in Stage 2 assuming it would have yielded 
    the opposite reward (1 - R). This captures 'fictitious play' or 
    inference about the unobserved counterfactual.

    Parameters:
    lr (float): Learning rate for chosen options [0,1].
    beta (float): Inverse temperature [0,10].
    w (float): MB/MF Weighting [0,1].
    decay (float): Global decay rate [0,1].
    pers (float): Perseveration bonus [0,5].
    lr_cf (float): Counterfactual learning rate for unchosen Stage 2 option [0,1].
    """
    lr, beta, w, decay, pers, lr_cf = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_a1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue

        # Global Decay
        q_stage1_mf *= (1 - decay)
        q_stage2_mf *= (1 - decay)

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net_stage1
        if prev_a1 != -1:
            logits_1[prev_a1] += pers

        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        logits_2 = beta * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        # Stage 1 MF Update
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr * delta_stage1

        # Stage 2 Chosen Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr * delta_stage2
        
        # Stage 2 Counterfactual Update (Unchosen)
        # Assume unchosen option yields (1 - reward)
        unchosen_s2 = 1 - action_2[trial]
        delta_cf = (1 - reward[trial]) - q_stage2_mf[state_idx, unchosen_s2]
        q_stage2_mf[state_idx, unchosen_s2] += lr_cf * delta_cf

        # Second Stage RPE effect on Stage 1
        q_stage1_mf[action_1[trial]] += lr * delta_stage2
        
        prev_a1 = action_1[trial]

    eps = 1e-10
    valid_mask = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss
```

### Model 3: Planet Stickiness (State Perseveration)
This model introduces a "Planet Stickiness" parameter. Instead of just repeating the previous motor action (Spaceship), the participant is motivated to return to the *previously visited Planet*. This bias is implemented by increasing the probability of choosing the spaceship that is most likely to lead to the previous planet, utilizing the transition matrix.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    MB/MF model with Planet Stickiness (State Perseveration).
    Includes a bonus for the spaceship choice that leads to the previously 
    visited planet, in addition to standard action perseveration. 
    This captures a desire to return to the same state (context).

    Parameters:
    lr (float): Learning rate [0,1].
    beta (float): Inverse temperature [0,10].
    w (float): MB/MF Weighting [0,1].
    decay (float): Value decay rate [0,1].
    pers_a (float): Action perseveration (Stickiness to Spaceship) [0,5].
    pers_p (float): Planet perseveration (Stickiness to Planet) [0,5].
    """
    lr, beta, w, decay, pers_a, pers_p = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_a1 = -1
    prev_planet = -1

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue

        # Decay
        q_stage1_mf *= (1 - decay)
        q_stage2_mf *= (1 - decay)

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net_stage1
        
        # Action Perseveration
        if prev_a1 != -1:
            logits_1[prev_a1] += pers_a
            
        # Planet Perseveration
        if prev_planet != -1:
            # Add bonus proportional to the probability of reaching prev_planet
            # If prev_planet was 0: Bonus to A is pers_p * 0.7, Bonus to U is pers_p * 0.3
            # If prev_planet was 1: Bonus to A is pers_p * 0.3, Bonus to U is pers_p * 0.7
            planet_probs_from_actions = transition_matrix[:, prev_planet]
            logits_1 += pers_p * planet_probs_from_actions

        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        logits_2 = beta * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr * delta_stage1

        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr * delta_stage2
        
        q_stage1_mf[action_1[trial]] += lr * delta_stage2
        
        prev_a1 = action_1[trial]
        prev_planet = state[trial]

    eps = 1e-10
    valid_mask = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss
```