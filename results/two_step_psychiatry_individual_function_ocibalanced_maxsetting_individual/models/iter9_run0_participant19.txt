Here are the 3 new cognitive models.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Model 1: Asymmetric TD(lambda) Q-Learning.
    
    A Model-Free learner that uses Eligibility Traces (lambda) to update Stage 1 
    values based on Stage 2 outcomes, allowing for more efficient credit assignment 
    than simple TD(0). It also incorporates asymmetric learning rates for positive 
    and negative prediction errors, capturing the participant's potential bias 
    towards gains or losses.

    Parameters:
    - alpha_pos: [0, 1] Learning rate for positive RPE.
    - alpha_neg: [0, 1] Learning rate for negative RPE.
    - beta: [0, 10] Inverse temperature (exploration/exploitation balance).
    - lambda_param: [0, 1] Eligibility trace decay parameter (0 = TD(0), 1 = Monte Carlo).
    """
    alpha_pos, alpha_neg, beta, lambda_param = model_parameters
    n_trials = len(action_1)
    
    q_stage1 = np.zeros(2)      # Q-values for Spaceships (A/U)
    q_stage2 = np.zeros((2, 2)) # Q-values for Aliens (State 0: W/S, State 1: P/H)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for t in range(n_trials):
        # --- Stage 1 Decision ---
        logits_1 = beta * q_stage1
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]
        
        # --- Stage 2 Decision ---
        s_idx = state[t]
        logits_2 = beta * q_stage2[s_idx]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]
        
        r = reward[t]
        
        # --- Updates ---
        # Stage 1 RPE (Prediction Error based on Stage 2 value)
        # delta_1 = Q_stage2(s, a2) - Q_stage1(a1)
        target_stage1 = q_stage2[s_idx, a2]
        delta_1 = target_stage1 - q_stage1[a1]
        
        # Apply asymmetric learning to Stage 1 update
        if delta_1 >= 0:
            q_stage1[a1] += alpha_pos * delta_1
        else:
            q_stage1[a1] += alpha_neg * delta_1
            
        # Stage 2 RPE (Prediction Error based on Reward)
        # delta_2 = r - Q_stage2(s, a2)
        delta_2 = r - q_stage2[s_idx, a2]
        
        # Update Stage 2 Value
        if delta_2 >= 0:
            q_stage2[s_idx, a2] += alpha_pos * delta_2
            # Eligibility Trace Update for Stage 1: Pass back delta_2 scaled by lambda
            q_stage1[a1] += alpha_pos * lambda_param * delta_2
        else:
            q_stage2[s_idx, a2] += alpha_neg * delta_2
            # Eligibility Trace Update for Stage 1
            q_stage1[a1] += alpha_neg * lambda_param * delta_2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model 2: Dual-Rate Model-Free Learning with Forgetting.
    
    A Model-Free learner that utilizes separate learning rates for Stage 1 
    (Spaceship choice) and Stage 2 (Alien choice). This allows the model to 
    capture different rates of adaptation for the stable transition structure 
    versus the drifting reward probabilities. It also includes a forgetting 
    parameter to decay unchosen action values.

    Parameters:
    - alpha_stage1: [0, 1] Learning rate for Stage 1 updates.
    - alpha_stage2: [0, 1] Learning rate for Stage 2 updates.
    - beta: [0, 10] Inverse temperature.
    - forget_rate: [0, 1] Decay rate for unchosen actions.
    """
    alpha_stage1, alpha_stage2, beta, forget_rate = model_parameters
    n_trials = len(action_1)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for t in range(n_trials):
        # --- Stage 1 Decision ---
        logits_1 = beta * q_stage1
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]
        
        # --- Stage 2 Decision ---
        s_idx = state[t]
        logits_2 = beta * q_stage2[s_idx]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]
        
        r = reward[t]
        
        # --- Updates ---
        # Update Stage 2 (Alien)
        pe2 = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += alpha_stage2 * pe2
        
        # Forgetting for unchosen Alien in the current state
        unchosen_a2 = 1 - a2
        q_stage2[s_idx, unchosen_a2] *= (1.0 - forget_rate)
        
        # Update Stage 1 (Spaceship)
        # Using TD(0): Target is the updated value of the state reached
        target_val = q_stage2[s_idx, a2]
        pe1 = target_val - q_stage1[a1]
        q_stage1[a1] += alpha_stage1 * pe1
        
        # Forgetting for unchosen Spaceship
        unchosen_a1 = 1 - a1
        q_stage1[unchosen_a1] *= (1.0 - forget_rate)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model 3: Asymmetric Hybrid Model with Forgetting.
    
    A Hybrid learner (mixing Model-Based and Model-Free values) that incorporates
    asymmetric learning rates for positive and negative outcomes and forgetting 
    for unchosen actions. This model tests if the combination of model-based 
    planning and valence-dependent maintenance (asymmetry + forgetting) explains 
    the behavior better than stickiness-based models.

    Parameters:
    - alpha_pos: [0, 1] Learning rate for positive RPE.
    - alpha_neg: [0, 1] Learning rate for negative RPE.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weight for Model-Based values (0 = Pure MF, 1 = Pure MB).
    - forget_rate: [0, 1] Decay rate for unchosen actions.
    """
    alpha_pos, alpha_neg, beta, w, forget_rate = model_parameters
    n_trials = len(action_1)
    
    # Transition matrix: T[action, next_state]
    # Action 0 (A) -> 0.7 to State 0 (X), 0.3 to State 1 (Y)
    # Action 1 (U) -> 0.3 to State 0 (X), 0.7 to State 1 (Y)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_mf_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for t in range(n_trials):
        # --- Stage 1 Decision (Hybrid) ---
        # Calculate Model-Based values
        # Q_MB(a1) = Sum_s' T(s'|a1) * max_a' Q_stage2(s', a')
        max_q_stage2 = np.max(q_stage2, axis=1) # Max value for each state [max(X), max(Y)]
        q_mb_stage1 = transition_matrix @ max_q_stage2
        
        # Mix MB and MF values
        q_net_stage1 = w * q_mb_stage1 + (1 - w) * q_mf_stage1
        
        logits_1 = beta * q_net_stage1
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]
        
        # --- Stage 2 Decision ---
        s_idx = state[t]
        logits_2 = beta * q_stage2[s_idx]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]
        
        r = reward[t]
        
        # --- Updates (Asymmetric + Forgetting) ---
        
        # Update Stage 2 (Alien)
        pe2 = r - q_stage2[s_idx, a2]
        if pe2 >= 0:
            q_stage2[s_idx, a2] += alpha_pos * pe2
        else:
            q_stage2[s_idx, a2] += alpha_neg * pe2
            
        unchosen_a2 = 1 - a2
        q_stage2[s_idx, unchosen_a2] *= (1.0 - forget_rate)
        
        # Update Stage 1 MF (Spaceship)
        target_val = q_stage2[s_idx, a2]
        pe1 = target_val - q_mf_stage1[a1]
        if pe1 >= 0:
            q_mf_stage1[a1] += alpha_pos * pe1
        else:
            q_mf_stage1[a1] += alpha_neg * pe1
            
        unchosen_a1 = 1 - a1
        q_mf_stage1[unchosen_a1] *= (1.0 - forget_rate)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```