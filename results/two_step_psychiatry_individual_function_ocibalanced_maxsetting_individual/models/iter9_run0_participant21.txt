Here are the three proposed cognitive models.

### Model 1: Asymmetric Transition Learning
This model combines **learned transition probabilities** (from the best-performing model) with **asymmetric learning rates** for positive and negative prediction errors. This allows the agent to update its internal model of the spaceship-planet mapping while also exhibiting different sensitivities to rewards (wins) versus omissions (losses), a common trait in clinical populations and reinforcement learning.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Asymmetric Transition Learning Model.
    
    Combines dynamic transition learning with separate learning rates for 
    positive and negative prediction errors.
    
    Parameters:
    lr_pos: [0, 1] Learning rate for positive prediction errors (better than expected).
    lr_neg: [0, 1] Learning rate for negative prediction errors (worse than expected).
    lr_trans: [0, 1] Learning rate for transition probabilities.
    beta_1: [0, 10] Inverse temperature for Stage 1 choice.
    beta_2: [0, 10] Inverse temperature for Stage 2 choice.
    w: [0, 1] Weight for Model-Based control (0=MF, 1=MB).
    stickiness: [0, 5] Choice stickiness / perseveration for Stage 1.
    """
    lr_pos, lr_neg, lr_trans, beta_1, beta_2, w, stickiness = model_parameters
    n_trials = len(action_1)

    # Initialize learned transitions to uniform (uncertainty)
    learned_transitions = np.array([[0.5, 0.5], [0.5, 0.5]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # 2 planets, 2 aliens

    last_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # Model-Based Value: Uses current learned transition matrix
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = learned_transitions @ max_q_stage2

        # Hybrid Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf

        # Apply Stickiness
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness

        # Softmax Choice 1
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        # --- Stage 2 Decision ---
        state_idx = state[trial] # Planet reached
        qs_current_state = q_stage2_mf[state_idx]
        
        # Softmax Choice 2
        exp_q2 = np.exp(beta_2 * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        act1 = action_1[trial]
        act2 = action_2[trial]
        
        # 1. Update Transition Matrix (State Prediction Error)
        # Update the row corresponding to the chosen spaceship
        learned_transitions[act1, state_idx] += lr_trans * (1 - learned_transitions[act1, state_idx])
        learned_transitions[act1, 1 - state_idx] += lr_trans * (0 - learned_transitions[act1, 1 - state_idx])

        # 2. Update Stage 1 MF Value
        delta_stage1 = q_stage2_mf[state_idx, act2] - q_stage1_mf[act1]
        eff_lr_1 = lr_pos if delta_stage1 > 0 else lr_neg
        q_stage1_mf[act1] += eff_lr_1 * delta_stage1

        # 3. Update Stage 2 MF Value
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, act2]
        eff_lr_2 = lr_pos if delta_stage2 > 0 else lr_neg
        q_stage2_mf[state_idx, act2] += eff_lr_2 * delta_stage2

        last_action_1 = act1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Coupled Counterfactual Updating
This model posits that the participant updates not just the chosen option, but also the **unchosen** option in the opposite direction. If the chosen spaceship yields a positive prediction error, the unchosen spaceship's value is decreased (and vice versa). This "coupling" mechanism exaggerates the contrast between options, potentially explaining the participant's tendency to switch decisively after losses or stay after wins.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Coupled Counterfactual Updating Model.
    
    The agent updates the unchosen Stage 1 action in the opposite direction 
    of the prediction error, scaled by a coupling parameter.
    
    Parameters:
    lr: [0, 1] Learning rate.
    beta_1: [0, 10] Inverse temperature for Stage 1.
    beta_2: [0, 10] Inverse temperature for Stage 2.
    w: [0, 1] Weight for Model-Based control.
    stickiness: [0, 5] Choice stickiness for Stage 1.
    coupling: [0, 1] Strength of counterfactual updating (0=none, 1=full opposite update).
    """
    lr, beta_1, beta_2, w, stickiness, coupling = model_parameters
    n_trials = len(action_1)

    # Fixed transition matrix as per task structure
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) 

    last_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2

        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf

        if last_action_1 != -1:
            q_net[last_action_1] += stickiness

        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        # --- Stage 2 Decision ---
        state_idx = state[trial]
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta_2 * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        act1 = action_1[trial]
        act2 = action_2[trial]
        unchosen_act1 = 1 - act1

        # Stage 1 Update with Coupling
        delta_stage1 = q_stage2_mf[state_idx, act2] - q_stage1_mf[act1]
        q_stage1_mf[act1] += lr * delta_stage1
        # Counterfactual update: if delta is positive, unchosen goes down; if negative, it goes up
        q_stage1_mf[unchosen_act1] -= lr * coupling * delta_stage1

        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, act2]
        q_stage2_mf[state_idx, act2] += lr * delta_stage2

        last_action_1 = act1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Dual-Time-Scale Learning (Split Learning Rates)
This model hypothesizes that the participant learns at different rates for the two stages of the task. The high-level choice (Spaceship) might be updated slowly (strategic stability), while the low-level choice (Alien) might be updated rapidly (bandit-like adaptation), or vice versa. This separation allows the model to capture the participant's behavior of long "blocks" of spaceship choices despite fluctuating rewards at the alien level.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Dual-Time-Scale Learning Model.
    
    Uses separate learning rates for Stage 1 (Spaceship choice) and 
    Stage 2 (Alien choice), allowing for different adaptation speeds.
    
    Parameters:
    lr_stage1: [0, 1] Learning rate for Stage 1 Model-Free values.
    lr_stage2: [0, 1] Learning rate for Stage 2 values (Alien reward estimates).
    beta_1: [0, 10] Inverse temperature for Stage 1.
    beta_2: [0, 10] Inverse temperature for Stage 2.
    w: [0, 1] Weight for Model-Based control.
    stickiness: [0, 5] Choice stickiness for Stage 1.
    """
    lr_stage1, lr_stage2, beta_1, beta_2, w, stickiness = model_parameters
    n_trials = len(action_1)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    last_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2

        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf

        if last_action_1 != -1:
            q_net[last_action_1] += stickiness

        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        # --- Stage 2 Decision ---
        state_idx = state[trial]
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta_2 * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        act1 = action_1[trial]
        act2 = action_2[trial]

        # Stage 1 Update using lr_stage1
        delta_stage1 = q_stage2_mf[state_idx, act2] - q_stage1_mf[act1]
        q_stage1_mf[act1] += lr_stage1 * delta_stage1

        # Stage 2 Update using lr_stage2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, act2]
        q_stage2_mf[state_idx, act2] += lr_stage2 * delta_stage2

        last_action_1 = act1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```