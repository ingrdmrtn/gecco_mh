Here are 3 new cognitive models based on the participant data and the provided template.

### Model 1: Dynamic Habit (Momentum) Model
This model hypothesizes that the participant's "stickiness" is not a fixed constant, but a dynamic state that builds up with repetition (momentum) and decays when an option is not chosen. This explains the long "runs" of identical choices observed in the data (e.g., trials 100-120).

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Dynamic Habit (Momentum) Model.
    
    Stickiness is modeled as a dynamic 'habit strength' state variable rather 
    than a fixed parameter. Choosing an option increases its habit strength, 
    while the unchosen option's habit strength decays. This creates choice 
    momentum.

    Parameters:
    lr: [0,1] - Learning rate for Q-values.
    beta_1: [0,10] - Inverse temperature for Stage 1.
    beta_2: [0,10] - Inverse temperature for Stage 2.
    w: [0,1] - Weighting between MB (1) and MF (0).
    habit_lr: [0,1] - Rate at which habit strength grows when chosen.
    habit_decay: [0,1] - Rate at which habit strength decays when unchosen.
    """
    lr, beta_1, beta_2, w, habit_lr, habit_decay = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Dynamic state for habit strength (Action 0, Action 1)
    habit_strength = np.zeros(2)

    for trial in range(n_trials):
        # Filter missing trials
        if action_1[trial] == -1:
            continue

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add dynamic habit strength to logits
        logits_1 = beta_1 * q_net + habit_strength
        
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        logits_2 = beta_2 * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Update Stage 1 MF
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr * delta_stage1
        
        # Update Habit Strength (Momentum)
        # Chosen option grows towards 1
        habit_strength[action_1[trial]] += habit_lr * (1.0 - habit_strength[action_1[trial]])
        # Unchosen option decays towards 0
        habit_strength[1 - action_1[trial]] *= (1.0 - habit_decay)

        # Update Stage 2 MF
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr * delta_stage2
        

    eps = 1e-10
    # Only sum log likelihoods for valid trials
    valid_mask = (p_choice_1 > 0) & (p_choice_2 > 0)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss
```

### Model 2: Counterfactual Stage 2 Learning
This model assumes the participant learns not just from what happened, but from what *didn't* happen. When they visit a planet and receive a specific outcome (e.g., 0 coins), they update the *unchosen* alien on that planet assuming it would have provided the opposite outcome (e.g., 1 coin), scaled by a counterfactual learning rate.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Counterfactual Stage 2 Learning Model.
    
    Updates the unchosen option in Stage 2 based on the outcome of the chosen 
    option. If the chosen alien gives 0, the agent infers the other alien 
    might have given 1 (and vice versa), creating a 'fictive' learning signal.
    
    Parameters:
    lr: [0,1] - Learning rate for chosen options.
    beta_1: [0,10] - Inverse temperature for Stage 1.
    beta_2: [0,10] - Inverse temperature for Stage 2.
    w: [0,1] - MB/MF weighting.
    stick: [0,5] - Basic choice stickiness.
    lr_cf: [0,1] - Learning rate for counterfactual (unchosen) options.
    """
    lr, beta_1, beta_2, w, stick, lr_cf = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta_1 * q_net
        if prev_action_1 != -1:
            logits_1[prev_action_1] += stick
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        logits_2 = beta_2 * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Update Stage 1
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr * delta_stage1
        
        # Update Stage 2 (Chosen)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr * delta_stage2
        
        # Update Stage 2 (Counterfactual / Unchosen)
        # Assume the unchosen option would have yielded (1 - reward)
        unchosen_action_2 = 1 - action_2[trial]
        fictive_reward = 1.0 - reward[trial]
        delta_cf = fictive_reward - q_stage2_mf[state_idx, unchosen_action_2]
        q_stage2_mf[state_idx, unchosen_action_2] += lr_cf * delta_cf
        
        prev_action_1 = action_1[trial]

    eps = 1e-10
    valid_mask = (p_choice_1 > 0) & (p_choice_2 > 0)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss
```

### Model 3: Subjective Loss Perception Model
Standard models treat "0 coins" as a numerical 0. However, participants often perceive the absence of a reward as a loss (pain). This model introduces a `loss_sensitivity` parameter that maps the 0-reward outcome to a negative value. This increases the contrast between winning and losing, potentially explaining rapid switching or strong avoidance behaviors.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Subjective Loss Perception Model.
    
    Treats the '0' reward outcome as a negative value (subjective loss) rather 
    than neutral zero. This amplifies the difference between getting a coin 
    and not getting one.
    
    Parameters:
    lr: [0,1] - Learning rate.
    beta_1: [0,10] - Inverse temperature for Stage 1.
    beta_2: [0,10] - Inverse temperature for Stage 2.
    w: [0,1] - MB/MF weighting.
    stick: [0,5] - Choice stickiness.
    loss_sens: [0,1] - Magnitude of negative utility for 0 reward. 
                       (0 reward is perceived as -loss_sens).
    """
    lr, beta_1, beta_2, w, stick, loss_sens = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta_1 * q_net
        if prev_action_1 != -1:
            logits_1[prev_action_1] += stick
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        logits_2 = beta_2 * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Update Stage 1
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr * delta_stage1
        
        # Update Stage 2 with Subjective Loss
        # If reward is 1, use 1. If reward is 0, use -loss_sens.
        val_outcome = 1.0 if reward[trial] == 1 else -loss_sens
        
        delta_stage2 = val_outcome - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr * delta_stage2
        
        prev_action_1 = action_1[trial]

    eps = 1e-10
    valid_mask = (p_choice_1 > 0) & (p_choice_2 > 0)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss
```