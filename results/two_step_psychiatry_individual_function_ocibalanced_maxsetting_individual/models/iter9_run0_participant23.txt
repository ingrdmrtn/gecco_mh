Here are three cognitive models defined as Python functions, designed to explain the participant's behavior in the two-step task.

### Cognitive Model 1: Subjective Belief Hybrid Model
This model hypothesizes that the participant uses a hybrid of Model-Based and Model-Free strategies, but their Model-Based system relies on a **subjective belief** about the transition probabilities (`p_belief`) which may differ from the true probability (0.7). This captures potential over-inference of structure or distrust of the common transitions.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Subjective Belief Hybrid Model.
    
    Combines Model-Based (MB) and Model-Free (MF) control. The MB system uses a 
    subjective transition probability matrix defined by `p_belief`. This allows 
    the model to capture if the participant treats transitions as more deterministic 
    or more random than they truly are.
    
    Parameters:
    learning_rate: [0, 1] Learning rate for MF values (Stage 1 and 2).
    beta:          [0, 10] Inverse temperature (softness of choice).
    w:             [0, 1] Weight of Model-Based control (0=MF, 1=MB).
    stickiness:    [-5, 5] Choice stickiness (perseveration) applied to both stages.
    p_belief:      [0, 1] Subjective probability of the 'common' transition (A->X, U->Y).
                          (True value is 0.7).
    """
    learning_rate, beta, w, stickiness, p_belief = model_parameters
    n_trials = len(action_1)
    
    # Initialize Q-values
    # Stage 1: 2 actions (Spaceships)
    q_stage1_mf = np.zeros(2) + 0.5
    # Stage 2: 2 states (Planets) x 2 actions (Aliens)
    q_stage2_mf = np.zeros((2, 2)) + 0.5
    
    # Subjective Transition Matrix based on parameter p_belief
    # row 0: Space 0 -> [Planet 0, Planet 1]
    # row 1: Space 1 -> [Planet 0, Planet 1]
    # Assuming Space 0 commonly goes to Planet 0, Space 1 to Planet 1.
    transition_matrix = np.array([[p_belief, 1.0 - p_belief], 
                                  [1.0 - p_belief, p_belief]])
    
    log_loss = 0
    eps = 1e-10
    
    prev_action_1 = -1
    prev_action_2 = np.array([-1, -1]) # Track last choice for each planet
    
    for trial in range(n_trials):
        a1 = action_1[trial]
        s_idx = state[trial]
        a2 = action_2[trial]
        r = reward[trial]
        
        # Handle missing data
        if a1 == -1 or s_idx == -1 or a2 == -1:
            continue
            
        # --- Stage 1 Decision ---
        # Model-Based Value: T * Max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Calculate Logits
        logits_1 = beta * q_net
        if prev_action_1 != -1:
            logits_1[prev_action_1] += stickiness
            
        # Softmax
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + eps)
        
        # --- Stage 2 Decision ---
        # Pure MF for Stage 2 (Aliens)
        logits_2 = beta * q_stage2_mf[s_idx]
        if prev_action_2[s_idx] != -1:
            logits_2[prev_action_2[s_idx]] += stickiness
            
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[a2] + eps)
        
        # --- Learning ---
        # Stage 2 Update (Reward Prediction Error)
        # Q2(s, a) += lr * (r - Q2(s, a))
        q_stage2_mf[s_idx, a2] += learning_rate * (r - q_stage2_mf[s_idx, a2])
        
        # Stage 1 Update (TD Prediction Error)
        # Using the value of the CHOSEN state-action pair (SARSA-like) 
        # as the target for Stage 1 MF value
        target_stage1 = q_stage2_mf[s_idx, a2]
        q_stage1_mf[a1] += learning_rate * (target_stage1 - q_stage1_mf[a1])
        
        # Update history
        prev_action_1 = a1
        prev_action_2[s_idx] = a2
        
    return log_loss
```

### Cognitive Model 2: Independent Stage Learning and Stickiness (ISLS)
This model assumes a pure Model-Free strategy but acknowledges that the learning dynamics for selecting a spaceship (Stage 1) and selecting an alien (Stage 2) are distinct. The participant data shows extremely long streaks for spaceship choices but shorter, more locally reactive streaks for alien choices. This implies separate learning rates and stickiness parameters for each stage.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Independent Stage Learning and Stickiness (ISLS).
    
    A pure Model-Free model that decouples the plasticity (learning rate) and 
    inertia (stickiness) of the two decision stages. This captures the observation 
    that the participant's habit formation and responsiveness to feedback differ 
    between the spaceship choice and the alien choice.
    
    Parameters:
    lr_s1:    [0, 1] Learning rate for Stage 1 (Spaceship) values.
    lr_s2:    [0, 1] Learning rate for Stage 2 (Alien) values.
    beta:     [0, 10] Inverse temperature (shared noise parameter).
    stick_s1: [-5, 5] Stickiness for Stage 1 choices.
    stick_s2: [-5, 5] Stickiness for Stage 2 choices.
    """
    lr_s1, lr_s2, beta, stick_s1, stick_s2 = model_parameters
    n_trials = len(action_1)
    
    q_stage1 = np.zeros(2) + 0.5
    q_stage2 = np.zeros((2, 2)) + 0.5
    
    log_loss = 0
    eps = 1e-10
    
    prev_action_1 = -1
    prev_action_2 = np.array([-1, -1])
    
    for trial in range(n_trials):
        a1 = action_1[trial]
        s_idx = state[trial]
        a2 = action_2[trial]
        r = reward[trial]
        
        if a1 == -1 or s_idx == -1 or a2 == -1:
            continue

        # --- Stage 1 Choice ---
        logits_1 = beta * q_stage1
        if prev_action_1 != -1:
            logits_1[prev_action_1] += stick_s1
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + eps)
        
        # --- Stage 2 Choice ---
        logits_2 = beta * q_stage2[s_idx]
        if prev_action_2[s_idx] != -1:
            logits_2[prev_action_2[s_idx]] += stick_s2
            
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[a2] + eps)
        
        # --- Learning ---
        # Stage 2: Reward driven
        q_stage2[s_idx, a2] += lr_s2 * (r - q_stage2[s_idx, a2])
        
        # Stage 1: TD driven (driven by updated Stage 2 value)
        target_s1 = q_stage2[s_idx, a2]
        q_stage1[a1] += lr_s1 * (target_s1 - q_stage1[a1])
        
        prev_action_1 = a1
        prev_action_2[s_idx] = a2
        
    return log_loss
```

### Cognitive Model 3: Decaying Habit Trace Model
This model replaces the standard "1-step stickiness" with a **habit trace** that accumulates over repeated choices and decays over time. This accounts for the "momentum" seen in the participant's data, where long streaks of choices (e.g., choosing Spaceship 1 for 20+ trials) create a strong habit that persists even through losses, which simple stickiness (based only on the previous trial) cannot fully capture.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Decaying Habit Trace Model.
    
    Instead of simple stickiness (which only considers the t-1 choice), this model 
    maintains a 'Habit Trace' for each action. The trace decays on every trial 
    and increments when an action is chosen. This models the 'buildup' of 
    perseveration over long streaks.
    
    Parameters:
    learning_rate: [0, 1] Learning rate for Q-values.
    beta:          [0, 10] Inverse temperature.
    habit_decay:   [0, 1] Decay rate of the habit trace (0 = instant forgetting, 1 = no decay).
    habit_amp:     [0, 10] Amplitude/Strength added to the trace upon choice.
    """
    learning_rate, beta, habit_decay, habit_amp = model_parameters
    n_trials = len(action_1)
    
    q_stage1 = np.zeros(2) + 0.5
    q_stage2 = np.zeros((2, 2)) + 0.5
    
    # Habit traces initialized to 0
    habit_trace_1 = np.zeros(2)
    habit_trace_2 = np.zeros((2, 2))
    
    log_loss = 0
    eps = 1e-10
    
    for trial in range(n_trials):
        a1 = action_1[trial]
        s_idx = state[trial]
        a2 = action_2[trial]
        r = reward[trial]
        
        if a1 == -1 or s_idx == -1 or a2 == -1:
            continue

        # --- Stage 1 Choice ---
        # Decision combines Q-values and accumulated Habit Trace
        logits_1 = beta * q_stage1 + habit_trace_1
        
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + eps)
        
        # --- Stage 2 Choice ---
        logits_2 = beta * q_stage2[s_idx] + habit_trace_2[s_idx]
        
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[a2] + eps)
        
        # --- Value Learning ---
        q_stage2[s_idx, a2] += learning_rate * (r - q_stage2[s_idx, a2])
        target_s1 = q_stage2[s_idx, a2]
        q_stage1[a1] += learning_rate * (target_s1 - q_stage1[a1])
        
        # --- Habit Trace Update ---
        # Decay all traces
        habit_trace_1 *= habit_decay
        habit_trace_2[s_idx] *= habit_decay # Only decay traces for current state? 
        # Usually habits decay over time regardless of state, but for simplicity 
        # and parsimony in multi-state tasks, we often decay all or just active.
        # Here we decay the active state's habit.
        
        # Increment chosen traces
        habit_trace_1[a1] += habit_amp
        habit_trace_2[s_idx, a2] += habit_amp
        
    return log_loss
```