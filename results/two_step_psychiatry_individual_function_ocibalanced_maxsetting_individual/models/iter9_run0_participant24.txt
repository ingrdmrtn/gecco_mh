Here are the three proposed cognitive models.

### Model 1: Model-Based with Coupled Transition and Rare Transition Pruning
This model extends the "Coupled Transition" framework by hypothesizing that the participant discounts information from "rare" or unexpected transitions when updating the values of the aliens (Stage 2). If a transition occurs that contradicts the current transition belief (e.g., observing a transition to Planet Y when Planet X is expected), the participant reduces the learning rate for the subsequent reward outcome, effectively "pruning" the rare branch to protect the value estimates from noise.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Model 1: Model-Based with Coupled Transition and Rare Transition Pruning.
    
    This model assumes the participant learns a coupled transition probability 
    (p_common) for both spaceships. Additionally, it implements a 'pruning' 
    mechanism: if a transition is deemed 'rare' according to the current 
    belief (p_common), the learning rate for the Stage 2 value update is 
    scaled down by a factor 'prune'. This reflects a heuristic where outcomes 
    from unexpected transitions are treated as noise or bad luck.
    
    Parameters:
    lr_val: [0, 1] - Base learning rate for Stage 2 Q-values.
    lr_common: [0, 1] - Learning rate for the common transition probability.
    beta: [0, 10] - Inverse temperature for softmax choice.
    pers: [0, 5] - Perseverance bonus for repeating the last Stage 1 choice.
    prune: [0, 1] - Scaling factor for learning rate on rare transitions (0 = ignore, 1 = full learning).
    """
    lr_val, lr_common, beta, pers, prune = model_parameters
    n_trials = len(action_1)
    
    # Q-values for Stage 2 (Aliens): 2 states (Planets) x 2 actions (Aliens)
    q_stage2 = np.zeros((2, 2))
    
    # Coupled transition probability: P(State=0|Action=0) and P(State=1|Action=1)
    p_common = 0.5
    
    last_action_1 = -1
    log_loss = 0.0
    eps = 1e-10
    
    for trial in range(n_trials):
        # Skip missing data
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1 or reward[trial] == -1:
            last_action_1 = -1
            continue
            
        a1 = int(action_1[trial])
        s_next = int(state[trial])
        a2 = int(action_2[trial])
        r = float(reward[trial])
        
        # --- Stage 1 Decision ---
        # Construct transition matrix based on current p_common
        # Row 0 (Action 0): [p_common, 1-p_common]
        # Row 1 (Action 1): [1-p_common, p_common]
        trans_probs = np.array([[p_common, 1.0 - p_common], 
                                [1.0 - p_common, p_common]])
        
        # Model-Based Value Calculation
        max_q_s2 = np.max(q_stage2, axis=1)
        q_mb = trans_probs @ max_q_s2
        
        # Add perseverance
        q_net_s1 = q_mb.copy()
        if last_action_1 != -1:
            q_net_s1[last_action_1] += pers
            
        # Softmax for Stage 1
        exp_q1 = np.exp(beta * q_net_s1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + eps)
        
        # --- Stage 2 Decision ---
        # Softmax for Stage 2
        exp_q2 = np.exp(beta * q_stage2[s_next])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[a2] + eps)
        
        # --- Updates ---
        
        # Determine if transition was "Subjectively Common" or "Subjectively Rare"
        # If p_common > 0.5, then matching indices (0->0, 1->1) are Common.
        # If p_common < 0.5, then mismatching indices (0->1, 1->0) are Common.
        observed_match = (a1 == s_next)
        belief_match_is_common = (p_common >= 0.5)
        
        is_subjectively_common = (observed_match == belief_match_is_common)
        
        # Select Learning Rate
        current_lr_val = lr_val if is_subjectively_common else (lr_val * prune)
        
        # Update Stage 2 Q-values
        delta_2 = r - q_stage2[s_next, a2]
        q_stage2[s_next, a2] += current_lr_val * delta_2
        
        # Update Transition Model (Coupled)
        # Update p_common towards 1 if observed match, else towards 0
        if observed_match:
            p_common += lr_common * (1.0 - p_common)
        else:
            p_common += lr_common * (0.0 - p_common)
            
        last_action_1 = a1

    return log_loss
```

### Model 2: Model-Based with Coupled Transition and Outcome-Dependent Temperature
This model proposes that the participant's exploration-exploitation balance (beta) is modulated by the previous trial's outcome. A reward (Win) induces a higher beta (more deterministic exploitation of the current model), while a lack of reward (Loss) induces a lower beta (more stochastic exploration). This captures a generalized "Win-Stay, Lose-Shift" dynamic within the softmax policy itself.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model 2: Model-Based with Coupled Transition and Outcome-Dependent Temperature.
    
    This model modifies the standard softmax policy by using two different 
    inverse temperature parameters (beta) depending on the outcome of the 
    previous trial. This allows the model to capture state-dependent 
    exploration/exploitation trade-offs (e.g., exploring more after a loss).
    
    Parameters:
    lr_val: [0, 1] - Learning rate for Stage 2 Q-values.
    lr_common: [0, 1] - Learning rate for the common transition probability.
    beta_win: [0, 10] - Inverse temperature after a rewarded trial.
    beta_loss: [0, 10] - Inverse temperature after an unrewarded trial.
    pers: [0, 5] - Perseverance bonus for Stage 1 choice.
    """
    lr_val, lr_common, beta_win, beta_loss, pers = model_parameters
    n_trials = len(action_1)
    
    q_stage2 = np.zeros((2, 2))
    p_common = 0.5
    
    last_action_1 = -1
    last_reward = 0.0 # Default to 0 (loss) for first trial or assume neutral
    
    log_loss = 0.0
    eps = 1e-10
    
    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1 or reward[trial] == -1:
            last_action_1 = -1
            continue
            
        a1 = int(action_1[trial])
        s_next = int(state[trial])
        a2 = int(action_2[trial])
        r = float(reward[trial])
        
        # Determine Beta based on previous reward
        # Assuming binary reward 0 or 1.
        current_beta = beta_win if last_reward > 0.5 else beta_loss
        
        # --- Stage 1 Decision ---
        trans_probs = np.array([[p_common, 1.0 - p_common], 
                                [1.0 - p_common, p_common]])
        
        max_q_s2 = np.max(q_stage2, axis=1)
        q_mb = trans_probs @ max_q_s2
        
        q_net_s1 = q_mb.copy()
        if last_action_1 != -1:
            q_net_s1[last_action_1] += pers
            
        exp_q1 = np.exp(current_beta * q_net_s1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + eps)
        
        # --- Stage 2 Decision ---
        # Note: Usually beta applies to both stages. 
        # Using the same outcome-dependent beta for consistency.
        exp_q2 = np.exp(current_beta * q_stage2[s_next])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[a2] + eps)
        
        # --- Updates ---
        delta_2 = r - q_stage2[s_next, a2]
        q_stage2[s_next, a2] += lr_val * delta_2
        
        if a1 == s_next:
            p_common += lr_common * (1.0 - p_common)
        else:
            p_common += lr_common * (0.0 - p_common)
            
        last_action_1 = a1
        last_reward = r

    return log_loss
```

### Model 3: Model-Based with Coupled Transition and Smoothed MB Values
This model assumes that the participant does not perfectly recalculate the Model-Based value tree from scratch on every trial. Instead, they maintain a running estimate of the Model-Based values (Stage 1 Q-values) which is updated incrementally towards the currently computed MB value. This introduces "planning inertia" or "smoothing" to the high-level decision-making, distinct from the learning of the low-level values.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model 3: Model-Based with Coupled Transition and Smoothed MB Values.
    
    This model posits that the participant maintains a cached estimate of the 
    Model-Based Q-values for Stage 1, rather than re-computing them perfectly 
    instantaneously. These cached values are updated via a 'smoothing' 
    parameter (alpha_mb) towards the current theoretical MB value derived 
    from the transition model and Stage 2 values. This adds inertia to the 
    planning process.
    
    Parameters:
    lr_val: [0, 1] - Learning rate for Stage 2 Q-values.
    lr_common: [0, 1] - Learning rate for the common transition probability.
    beta: [0, 10] - Inverse temperature.
    pers: [0, 5] - Perseverance bonus for Stage 1 choice.
    alpha_mb: [0, 1] - Smoothing rate for Stage 1 MB values (1 = instant update, 0 = no update).
    """
    lr_val, lr_common, beta, pers, alpha_mb = model_parameters
    n_trials = len(action_1)
    
    q_stage2 = np.zeros((2, 2))
    # Cached Stage 1 MB values, initialized to 0
    q_mb_cached = np.zeros(2)
    
    p_common = 0.5
    
    last_action_1 = -1
    log_loss = 0.0
    eps = 1e-10
    
    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1 or reward[trial] == -1:
            last_action_1 = -1
            continue
            
        a1 = int(action_1[trial])
        s_next = int(state[trial])
        a2 = int(action_2[trial])
        r = float(reward[trial])
        
        # --- Stage 1 Decision ---
        # 1. Calculate current theoretical MB value
        trans_probs = np.array([[p_common, 1.0 - p_common], 
                                [1.0 - p_common, p_common]])
        max_q_s2 = np.max(q_stage2, axis=1)
        q_mb_current = trans_probs @ max_q_s2
        
        # 2. Update cached MB value (Smoothing)
        q_mb_cached += alpha_mb * (q_mb_current - q_mb_cached)
        
        # 3. Use cached value for decision
        q_net_s1 = q_mb_cached.copy()
        if last_action_1 != -1:
            q_net_s1[last_action_1] += pers
            
        exp_q1 = np.exp(beta * q_net_s1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + eps)
        
        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2[s_next])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[a2] + eps)
        
        # --- Updates ---
        delta_2 = r - q_stage2[s_next, a2]
        q_stage2[s_next, a2] += lr_val * delta_2
        
        if a1 == s_next:
            p_common += lr_common * (1.0 - p_common)
        else:
            p_common += lr_common * (0.0 - p_common)
            
        last_action_1 = a1

    return log_loss
```