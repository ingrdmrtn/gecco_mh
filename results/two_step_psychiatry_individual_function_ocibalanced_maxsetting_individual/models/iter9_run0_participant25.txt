Here are the 3 new cognitive models based on the participant data and task structure.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Learner with Outcome-Dependent Stickiness.
    
    A Hybrid reinforcement learning model (combining Model-Based and Model-Free values)
    that applies different levels of choice stickiness (perseveration) depending on 
    whether the previous trial resulted in a reward (Win) or no reward (Loss).
    
    Parameters:
    - lr: [0, 1] Learning rate for value updates.
    - beta: [0, 10] Inverse temperature (softmax sensitivity).
    - w: [0, 1] Mixing weight between Model-Based and Model-Free values (0 = Pure MF, 1 = Pure MB).
    - stick_win: [0, 10] Stickiness bonus added to the previously chosen spaceship if it was rewarded.
    - stick_loss: [0, 10] Stickiness bonus added to the previously chosen spaceship if it was unrewarded.
    """
    lr, beta, w, stick_win, stick_loss = model_parameters
    n_trials = len(action_1)
    
    # Fixed transition matrix for MB: T[0,0]=0.7 (A->X), T[0,1]=0.3 (A->Y), etc.
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Q-values
    q_mf_s1 = np.zeros(2) + 0.5
    q_s2 = np.zeros((2, 2)) + 0.5  # State (Planet) x Action (Alien)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1
    last_reward = 0
    
    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        
        # Model-Based Value Calculation
        max_q_s2 = np.max(q_s2, axis=1) # Max value of aliens at each planet
        q_mb_s1 = transition_matrix @ max_q_s2
        
        # Hybrid Value
        q_net = w * q_mb_s1 + (1 - w) * q_mf_s1
        
        # Apply Outcome-Dependent Stickiness
        if last_action_1 != -1:
            if last_reward == 1:
                q_net[last_action_1] += stick_win
            else:
                q_net[last_action_1] += stick_loss
                
        # Softmax
        exp_q1 = np.exp(beta * q_net)
        if np.any(np.isinf(exp_q1)): exp_q1 = np.nan_to_num(exp_q1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Decision ---
        s_idx = int(state[trial])
        
        exp_q2 = np.exp(beta * q_s2[s_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        r = reward[trial]
        
        # --- Updates ---
        
        # TD(0) update for Stage 1 MF value
        # Note: Standard Hybrid models often use TD(0) for the MF part.
        pe_1 = q_s2[s_idx, a2] - q_mf_s1[a1]
        q_mf_s1[a1] += lr * pe_1
        
        # Update Stage 2 value
        pe_2 = r - q_s2[s_idx, a2]
        q_s2[s_idx, a2] += lr * pe_2
        
        last_action_1 = a1
        last_reward = r

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Transition-Gated Model-Free Learner.
    
    A pure Model-Free learner (no Model-Based planning) that modulates its 
    Stage 1 learning rate based on whether the transition to the second stage 
    was Common or Rare. This allows the agent to gate learning from "surprising" 
    transitions without a full world model.
    
    Parameters:
    - lr_common: [0, 1] Learning rate for Stage 1 updates after Common transitions.
    - lr_rare: [0, 1] Learning rate for Stage 1 updates after Rare transitions.
    - beta: [0, 10] Inverse temperature.
    - stickiness: [0, 10] Choice stickiness bonus for the previously chosen spaceship.
    """
    lr_common, lr_rare, beta, stickiness = model_parameters
    n_trials = len(action_1)
    
    q_s1 = np.zeros(2) + 0.5
    q_s2 = np.zeros((2, 2)) + 0.5
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        q_net = q_s1.copy()
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Decision ---
        s_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_s2[s_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        r = reward[trial]
        
        # --- Updates ---
        
        # Determine transition type
        # Spaceship 0 -> Planet 0 (Common), Spaceship 1 -> Planet 1 (Common)
        # Otherwise Rare.
        is_common = (a1 == s_idx)
        
        # Update Stage 1
        current_lr = lr_common if is_common else lr_rare
        pe_1 = q_s2[s_idx, a2] - q_s1[a1]
        q_s1[a1] += current_lr * pe_1
        
        # Update Stage 2 (using lr_common as the base rate for reward learning)
        pe_2 = r - q_s2[s_idx, a2]
        q_s2[s_idx, a2] += lr_common * pe_2
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Transition-Dependent Hybrid Learner.
    
    A Hybrid learner where the reliance on Model-Based vs Model-Free values (mixing weight w)
    changes dynamically based on the previous trial's transition type.
    The agent may rely more or less on the model after observing a Rare transition.
    
    Parameters:
    - lr: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w_common: [0, 1] Mixing weight (MB strength) if the previous transition was Common.
    - w_rare: [0, 1] Mixing weight (MB strength) if the previous transition was Rare.
    - stickiness: [0, 10] Choice stickiness bonus.
    """
    lr, beta, w_common, w_rare, stickiness = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_mf_s1 = np.zeros(2) + 0.5
    q_s2 = np.zeros((2, 2)) + 0.5
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1
    prev_transition_common = True # Assume common start
    
    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        max_q_s2 = np.max(q_s2, axis=1)
        q_mb_s1 = transition_matrix @ max_q_s2
        
        # Select mixing weight based on previous transition
        w = w_common if prev_transition_common else w_rare
        
        q_net = w * q_mb_s1 + (1 - w) * q_mf_s1
        
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_net)
        if np.any(np.isinf(exp_q1)): exp_q1 = np.nan_to_num(exp_q1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Decision ---
        s_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_s2[s_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        r = reward[trial]
        
        # --- Updates ---
        pe_1 = q_s2[s_idx, a2] - q_mf_s1[a1]
        q_mf_s1[a1] += lr * pe_1
        
        pe_2 = r - q_s2[s_idx, a2]
        q_s2[s_idx, a2] += lr * pe_2
        
        # Update history
        last_action_1 = a1
        # Check current transition type for next trial
        # 0->0 and 1->1 are Common
        prev_transition_common = (a1 == s_idx)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```