Here are three new cognitive models for the two-step task, implemented as Python functions.

### Model 1: Asymmetric Learning Model-Based Learner
This model extends the "Best Model" (Pure Model-Based) by hypothesizing that the participant learns differently from positive versus negative outcomes. In addition to outcome-dependent stickiness, it employs separate learning rates for rewarded (`lr_pos`) and unrewarded (`lr_neg`) trials when updating the values of the aliens (Stage 2). This captures potential biases like "optimism" (learning more from wins) or "pessimism" (learning more from losses).

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Based Learner with Asymmetric Learning Rates and Outcome-Dependent Stickiness.
    
    This model calculates Stage 1 values using the fixed transition matrix and Stage 2 values.
    It updates Stage 2 values with different learning rates depending on whether a reward was received.
    It also applies different stickiness biases to the Stage 1 choice based on the previous outcome.
    
    Parameters:
    - lr_pos: [0, 1] Learning rate for Stage 2 values when reward is 1.
    - lr_neg: [0, 1] Learning rate for Stage 2 values when reward is 0.
    - beta_1: [0, 10] Inverse temperature for Stage 1 choice.
    - beta_2: [0, 10] Inverse temperature for Stage 2 choice.
    - stick_win: [-5, 5] Stickiness bias added to previous choice if rewarded.
    - stick_loss: [-5, 5] Stickiness bias added to previous choice if unrewarded.
    """
    lr_pos, lr_neg, beta_1, beta_2, stick_win, stick_loss = model_parameters
    n_trials = len(action_1)
    
    # Fixed transition matrix as per task description
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Q-values for Stage 2 (2 states x 2 aliens)
    q_stage2 = np.zeros((2, 2))
    
    log_loss = 0.0
    eps = 1e-10
    
    last_action_1 = -1
    last_reward = -1
    
    for trial in range(n_trials):
        # Skip invalid data
        if action_1[trial] == -1:
            continue
            
        # --- Stage 1: Model-Based Choice ---
        # Calculate MB values: Transition * Max(Q_stage2)
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_stage1 = transition_matrix @ max_q_stage2
        
        # Add Stickiness
        if last_action_1 != -1:
            if last_reward == 1:
                q_stage1[last_action_1] += stick_win
            else:
                q_stage1[last_action_1] += stick_loss
        
        # Softmax for Stage 1
        exp_q1 = np.exp(beta_1 * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        chosen_a1 = int(action_1[trial])
        log_loss -= np.log(probs_1[chosen_a1] + eps)
        
        # --- Stage 2: Choice ---
        current_state = int(state[trial])
        
        # Softmax for Stage 2
        exp_q2 = np.exp(beta_2 * q_stage2[current_state])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        chosen_a2 = int(action_2[trial])
        log_loss -= np.log(probs_2[chosen_a2] + eps)
        
        # --- Value Update ---
        r = reward[trial]
        
        # Select learning rate based on outcome
        current_lr = lr_pos if r == 1 else lr_neg
        
        # Update Stage 2 value
        delta_stage2 = r - q_stage2[current_state, chosen_a2]
        q_stage2[current_state, chosen_a2] += current_lr * delta_stage2
        
        # Store history
        last_action_1 = chosen_a1
        last_reward = r

    return log_loss
```

### Model 2: Transition-Sensitive Model-Free Learner
This model tests the hypothesis that the participant uses a Model-Free strategy (TD learning) but updates their Stage 1 preferences differently depending on whether the transition was "Common" or "Rare". Unlike a standard Model-Free learner that ignores transition types, or a Model-Based learner that uses the full probability matrix, this agent uses a heuristic: it applies different learning rates (`lr_common` vs `lr_rare`) to backpropagate the value from Stage 2 to Stage 1.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Transition-Sensitive Model-Free Learner.
    
    This model is purely Model-Free (no transition matrix used for value calculation)
    but applies different learning rates for the Stage 1 update depending on whether
    the transition experienced was Common or Rare.
    
    Parameters:
    - lr_stage2: [0, 1] Learning rate for Stage 2 values.
    - lr_common: [0, 1] Learning rate for Stage 1 update after a Common transition.
    - lr_rare: [0, 1] Learning rate for Stage 1 update after a Rare transition.
    - beta_1: [0, 10] Inverse temperature for Stage 1 choice.
    - beta_2: [0, 10] Inverse temperature for Stage 2 choice.
    - stickiness: [-5, 5] General stickiness bias added to previous Stage 1 choice.
    """
    lr_stage2, lr_common, lr_rare, beta_1, beta_2, stickiness = model_parameters
    n_trials = len(action_1)
    
    # Q-values
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    log_loss = 0.0
    eps = 1e-10
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        # --- Stage 1: Choice ---
        # Add Stickiness to temporary Q-values for decision making
        q_stage1_choice = q_stage1.copy()
        if last_action_1 != -1:
            q_stage1_choice[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta_1 * q_stage1_choice)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        chosen_a1 = int(action_1[trial])
        log_loss -= np.log(probs_1[chosen_a1] + eps)
        
        # --- Stage 2: Choice ---
        current_state = int(state[trial])
        exp_q2 = np.exp(beta_2 * q_stage2[current_state])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        chosen_a2 = int(action_2[trial])
        log_loss -= np.log(probs_2[chosen_a2] + eps)
        
        # --- Updates ---
        r = reward[trial]
        
        # Stage 2 Update (Standard TD)
        delta_stage2 = r - q_stage2[current_state, chosen_a2]
        q_stage2[current_state, chosen_a2] += lr_stage2 * delta_stage2
        
        # Stage 1 Update (Transition-Sensitive)
        # Determine if transition was Common or Rare
        # Common: 0->0, 1->1. Rare: 0->1, 1->0
        is_common = (chosen_a1 == current_state)
        
        current_lr_stage1 = lr_common if is_common else lr_rare
        
        # SARSA-like update: target is the Q-value of the chosen state-action pair
        target = q_stage2[current_state, chosen_a2]
        delta_stage1 = target - q_stage1[chosen_a1]
        q_stage1[chosen_a1] += current_lr_stage1 * delta_stage1
        
        last_action_1 = chosen_a1

    return log_loss
```

### Model 3: Dynamic Transition Model-Based Learner
This model assumes the participant is Model-Based but does not trust the transition probabilities to be fixed. Instead, they continuously update their estimate of the transition matrix based on observed transitions. The `trans_decay` parameter allows for "forgetting" old observations, enabling the model to track perceived changes in spaceship reliability (even if the true probabilities are stable).

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Based Learner with Dynamic Transition Estimation.
    
    The participant estimates the transition matrix online. Counts of observed
    transitions decay over time, allowing the model to react to local streaks
    of rare or common transitions.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for Stage 2 alien values.
    - beta_1: [0, 10] Inverse temperature for Stage 1 choice.
    - beta_2: [0, 10] Inverse temperature for Stage 2 choice.
    - trans_decay: [0, 1] Decay rate for transition counts (1 = no decay, 0 = instant forgetting).
    - stickiness: [-5, 5] General stickiness bias added to previous Stage 1 choice.
    """
    learning_rate, beta_1, beta_2, trans_decay, stickiness = model_parameters
    n_trials = len(action_1)
    
    # Initialize transition counts with prior knowledge (70/30 split)
    # We use a weight of 2.0 to give it some initial stability
    trans_counts = np.array([[1.4, 0.6], [0.6, 1.4]])
    
    q_stage2 = np.zeros((2, 2))
    
    log_loss = 0.0
    eps = 1e-10
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        # --- Calculate Dynamic Transition Probabilities ---
        row_sums = np.sum(trans_counts, axis=1, keepdims=True)
        est_trans_matrix = trans_counts / (row_sums + eps)
        
        # --- Stage 1: Choice ---
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_stage1 = est_trans_matrix @ max_q_stage2
        
        if last_action_1 != -1:
            q_stage1[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta_1 * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        chosen_a1 = int(action_1[trial])
        log_loss -= np.log(probs_1[chosen_a1] + eps)
        
        # --- Stage 2: Choice ---
        current_state = int(state[trial])
        exp_q2 = np.exp(beta_2 * q_stage2[current_state])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        chosen_a2 = int(action_2[trial])
        log_loss -= np.log(probs_2[chosen_a2] + eps)
        
        # --- Updates ---
        r = reward[trial]
        
        # Update Stage 2 Values
        delta_stage2 = r - q_stage2[current_state, chosen_a2]
        q_stage2[current_state, chosen_a2] += learning_rate * delta_stage2
        
        # Update Transition Counts
        # Decay existing counts
        trans_counts *= trans_decay
        # Add current observation
        trans_counts[chosen_a1, current_state] += 1.0
        
        last_action_1 = chosen_a1
        
    return log_loss
```