Here are three new cognitive models designed to explain the participant's behavior, particularly the block-like perseveration and the specific learning dynamics suggested by previous analyses.

### Model 1: Separate Stage Learning Rates with Unchosen Decay
This model extends the successful "Unchosen Decay" mechanism by acknowledging that the participant likely learns at different rates for the two distinct stages of the task. The spaceship choice (Stage 1) involves stable, long-term structural knowledge (or habit), while the alien choice (Stage 2) involves tracking rapidly changing reward probabilities. By separating `lr_stage1` and `lr_stage2`, the model can capture the slow switching between spaceships alongside fast adaptation to alien rewards.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Separate Stage Learning Rates with Unchosen Decay.
    
    This model distinguishes between the learning rate for the first stage (spaceships)
    and the second stage (aliens). It incorporates the 'unchosen decay' mechanism 
    where Q-values of unselected aliens decay towards zero, simulating forgetting 
    or attention focusing.
    
    Parameters:
    - lr_stage1: [0, 1] Learning rate for Stage 1 (Spaceships).
    - lr_stage2: [0, 1] Learning rate for Stage 2 (Aliens).
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - w: [0, 1] Weighting parameter (0 = Model-Free, 1 = Model-Based).
    - stickiness: [0, 5] Perseveration bonus for repeating the Stage 1 choice.
    - decay_unchosen: [0, 1] Decay rate applied to unchosen Stage 2 options.
    """
    lr_stage1, lr_stage2, beta_1, beta_2, w, stickiness, decay_unchosen = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        # Handle missing data
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1 or reward[trial] == -1:
            last_action_1 = -1
            continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        # --- Stage 2 Policy ---
        state_idx = state[trial]
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        # Decay Unchosen Stage 2 Values
        mask = np.ones((2, 2), dtype=bool)
        mask[state_idx, action_2[trial]] = False
        q_stage2_mf[mask] *= (1 - decay_unchosen)

        # Update Chosen Stage 2 Value (RL)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr_stage2 * delta_stage2

        # Update Stage 1 Value (TD0)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr_stage1 * delta_stage1
        
        last_action_1 = action_1[trial]

    eps = 1e-10
    mask_valid = (p_choice_1 > 0) & (p_choice_2 > 0)
    log_loss = -(np.sum(np.log(p_choice_1[mask_valid] + eps)) + np.sum(np.log(p_choice_2[mask_valid] + eps)))
    return log_loss
```

### Model 2: Reward-Dependent Stickiness with Unchosen Decay
The participant shows strong persistence (stickiness) but eventually switches. This model posits that stickiness is not constant but depends on the outcome of the previous trial (Win-Stay, Lose-Shift logic). This outcome-dependency is combined with the `decay_unchosen` mechanism to manage value estimation. This combination allows the model to explain why the participant might stick to a spaceship despite fluctuating values, yet switch when specific conditions (unrewarded outcomes) accumulate.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Reward-Dependent Stickiness with Unchosen Decay.
    
    This model posits that the tendency to repeat a choice (stickiness) depends on 
    whether the previous trial was rewarded. It combines this with the unchosen 
    decay mechanism for value learning.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - w: [0, 1] Weighting parameter (Model-Based vs Model-Free).
    - stick_rew: [0, 5] Stickiness bonus if previous trial was rewarded.
    - stick_unrew: [0, 5] Stickiness bonus if previous trial was unrewarded.
    - decay_unchosen: [0, 1] Decay rate for unchosen Stage 2 options.
    """
    learning_rate, beta_1, beta_2, w, stick_rew, stick_unrew, decay_unchosen = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    last_reward = -1
    
    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1 or reward[trial] == -1:
            last_action_1 = -1
            last_reward = -1
            continue

        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Reward-Dependent Stickiness
        if last_action_1 != -1:
            if last_reward == 1:
                q_net[last_action_1] += stick_rew
            else:
                q_net[last_action_1] += stick_unrew
            
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        state_idx = state[trial]
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # Decay Unchosen
        mask = np.ones((2, 2), dtype=bool)
        mask[state_idx, action_2[trial]] = False
        q_stage2_mf[mask] *= (1 - decay_unchosen)

        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        last_action_1 = action_1[trial]
        last_reward = reward[trial]

    eps = 1e-10
    mask_valid = (p_choice_1 > 0) & (p_choice_2 > 0)
    log_loss = -(np.sum(np.log(p_choice_1[mask_valid] + eps)) + np.sum(np.log(p_choice_2[mask_valid] + eps)))
    return log_loss
```

### Model 3: Cumulative Stickiness with Unchosen Decay
The participant exhibits very long "runs" of choosing the same spaceship (e.g., trials 11-54). Standard stickiness applies a constant bonus. This model implements **cumulative stickiness**, where the urge to repeat an action grows the more it is chosen (momentum). This dynamic stickiness, combined with unchosen decay, aims to better fit the heavy perseveration blocks seen in the data.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Cumulative Stickiness with Unchosen Decay.
    
    This model implements a dynamic stickiness mechanism where the urge to repeat 
    an action grows with consecutive choices (momentum/inertia). It resets to a base 
    level upon switching. Combined with unchosen value decay.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - w: [0, 1] Weighting parameter.
    - stick_base: [0, 5] Base stickiness bonus.
    - stick_growth: [0, 5] Increment of stickiness per consecutive choice.
    - decay_unchosen: [0, 1] Decay rate for unchosen Stage 2 options.
    """
    learning_rate, beta_1, beta_2, w, stick_base, stick_growth, decay_unchosen = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track cumulative stickiness for each action
    stick_val = np.zeros(2)
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1 or reward[trial] == -1:
            last_action_1 = -1
            stick_val = np.zeros(2)
            continue

        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Apply cumulative stickiness
        q_net += stick_val
            
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        state_idx = state[trial]
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # Decay Unchosen
        mask = np.ones((2, 2), dtype=bool)
        mask[state_idx, action_2[trial]] = False
        q_stage2_mf[mask] *= (1 - decay_unchosen)

        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Update Cumulative Stickiness
        current_choice = action_1[trial]
        stick_val[current_choice] = max(stick_base, stick_val[current_choice] + stick_growth)
        stick_val[1 - current_choice] = 0 
        
        last_action_1 = action_1[trial]

    eps = 1e-10
    mask_valid = (p_choice_1 > 0) & (p_choice_2 > 0)
    log_loss = -(np.sum(np.log(p_choice_1[mask_valid] + eps)) + np.sum(np.log(p_choice_2[mask_valid] + eps)))
    return log_loss
```