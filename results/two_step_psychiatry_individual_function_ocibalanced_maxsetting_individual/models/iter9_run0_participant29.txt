Here are three new cognitive models based on the "Independent Beta" architecture, which appears to fit the participant well. These models explore different mechanisms for learning and persistence (Direct Reinforcement, Eligibility Traces, and Separate Learning Rates) to explain the participant's strong stickiness and model-based switching behavior.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Independent Beta Hybrid with Direct Reinforcement and Stickiness.
    
    This model splits the Stage 1 decision into a Model-Based component (structure-aware) 
    and a Direct Reinforcement component (structure-blind). Unlike standard Model-Free 
    learning which chains values via TD-learning, the Direct component updates Stage 1 
    values directly from the reward, ignoring the intermediate state value. 
    It also includes a stickiness parameter to capture the participant's strong perseverance.

    Parameters:
    - learning_rate: [0, 1] Update rate for Q-values.
    - beta_mb: [0, 10] Inverse temperature for Model-Based values (Stage 1).
    - beta_dr: [0, 10] Inverse temperature for Direct Reinforcement values (Stage 1).
    - stickiness: [0, 5] Bonus added to the logit of the previously chosen spaceship.
    """
    learning_rate, beta_mb, beta_dr, stickiness = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Q-values
    q_stage1_dr = np.zeros(2) # Direct reinforcement values for spaceships
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for aliens (Planet x Alien)
    
    log_loss = 0.0
    prev_action_1 = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # Model-Based Value: Expected max value of next stage
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combine MB and Direct Reinforcement
        logits_1 = (beta_mb * q_stage1_mb) + (beta_dr * q_stage1_dr)
        
        # Apply Stickiness
        if prev_action_1 != -1:
            logits_1[prev_action_1] += stickiness
            
        probs_1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 /= np.sum(probs_1)
        
        log_loss -= np.log(probs_1[action_1[trial]] + 1e-10)
        
        # --- Stage 2 Decision ---
        state_idx = state[trial]
        # Use beta_dr to scale stage 2 values (pure experience)
        logits_2 = beta_dr * q_stage2_mf[state_idx]
        probs_2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 /= np.sum(probs_2)
        
        log_loss -= np.log(probs_2[action_2[trial]] + 1e-10)
        
        # --- Learning Updates ---
        r = reward[trial]
        a1 = action_1[trial]
        a2 = action_2[trial]
        
        # Update Stage 2 (Standard Q-Learning)
        delta2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta2
        
        # Update Stage 1 (Direct Reinforcement)
        # Updates directly from reward, skipping the Stage 2 value connection
        delta1 = r - q_stage1_dr[a1]
        q_stage1_dr[a1] += learning_rate * delta1
        
        prev_action_1 = a1

    return log_loss

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Independent Beta Hybrid with Eligibility Traces (Lambda).
    
    This model uses separate inverse temperatures for Model-Based and Model-Free 
    control, allowing for independent scaling of their influence. 
    Crucially, it implements an eligibility trace (lambda) in the Model-Free update.
    This allows the Stage 1 update to be a blend of TD-learning (lambda=0) and 
    Monte-Carlo/Direct Reinforcement (lambda=1), capturing how deeply the 
    participant credits the outcome to the first choice.

    Parameters:
    - learning_rate: [0, 1] Update rate for Q-values.
    - beta_mb: [0, 10] Inverse temperature for Model-Based values.
    - beta_mf: [0, 10] Inverse temperature for Model-Free values.
    - lambd: [0, 1] Eligibility trace decay (0 = pure TD, 1 = pure Monte Carlo).
    """
    learning_rate, beta_mb, beta_mf, lambd = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    log_loss = 0.0
    
    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        logits_1 = (beta_mb * q_stage1_mb) + (beta_mf * q_stage1_mf)
        probs_1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 /= np.sum(probs_1)
        
        log_loss -= np.log(probs_1[action_1[trial]] + 1e-10)
        
        # --- Stage 2 Decision ---
        state_idx = state[trial]
        logits_2 = beta_mf * q_stage2_mf[state_idx]
        probs_2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 /= np.sum(probs_2)
        
        log_loss -= np.log(probs_2[action_2[trial]] + 1e-10)
        
        # --- Learning Updates ---
        r = reward[trial]
        a1 = action_1[trial]
        a2 = action_2[trial]
        
        # Stage 2 Prediction Error (R - Q_s2)
        delta2 = r - q_stage2_mf[state_idx, a2]
        
        # Stage 1 Prediction Error (Q_s2 - Q_s1)
        delta1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        
        # Update Stage 2
        q_stage2_mf[state_idx, a2] += learning_rate * delta2
        
        # Update Stage 1 with Eligibility Trace
        # Combines the immediate TD error (delta1) with the weighted Stage 2 error (lambda * delta2)
        q_stage1_mf[a1] += learning_rate * (delta1 + lambd * delta2)
        
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Independent Beta Hybrid with Dual Learning Rates.
    
    This model applies the Independent Beta architecture but separates the learning 
    rates for Stage 1 (Spaceship choice) and Stage 2 (Alien choice). This allows 
    the model to capture if the participant updates their spaceship preference 
    (structural/habitual) at a different speed than they learn the fluctuating 
    values of the aliens.

    Parameters:
    - lr_s1: [0, 1] Learning rate for Stage 1 Q-values.
    - lr_s2: [0, 1] Learning rate for Stage 2 Q-values.
    - beta_mb: [0, 10] Inverse temperature for Model-Based values.
    - beta_mf: [0, 10] Inverse temperature for Model-Free values.
    """
    lr_s1, lr_s2, beta_mb, beta_mf = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    log_loss = 0.0
    
    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        logits_1 = (beta_mb * q_stage1_mb) + (beta_mf * q_stage1_mf)
        probs_1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 /= np.sum(probs_1)
        
        log_loss -= np.log(probs_1[action_1[trial]] + 1e-10)
        
        # --- Stage 2 Decision ---
        state_idx = state[trial]
        logits_2 = beta_mf * q_stage2_mf[state_idx]
        probs_2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 /= np.sum(probs_2)
        
        log_loss -= np.log(probs_2[action_2[trial]] + 1e-10)
        
        # --- Learning Updates ---
        r = reward[trial]
        a1 = action_1[trial]
        a2 = action_2[trial]
        
        # Update Stage 2
        delta2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += lr_s2 * delta2
        
        # Update Stage 1 (TD-Learning)
        delta1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr_s1 * delta1
        
    return log_loss
```