Here are the three proposed cognitive models.

### Model 1: Transition-Gated Model-Free Learning
This model posits that the participant modulates their Model-Free learning based on the reliability of the transition. If a transition is "rare" (unexpected), the participant may discount the prediction error for the first-stage choice, treating the outcome as a fluke rather than a signal of the spaceship's value.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Transition-Gated Model-Free Learning.
    
    This model modulates the Model-Free update for the first-stage action based on 
    whether the transition to the second stage was common or rare. 
    Rare transitions might be discounted to prevent 'bad luck' or 'good luck' 
    from incorrectly updating the value of the chosen spaceship.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate for action values.
    beta: [0, 10] - Inverse temperature for softmax choice.
    w: [0, 1] - Weight for Model-Based values (0 = pure MF, 1 = pure MB).
    rare_factor: [0, 1] - Multiplier for the learning rate during rare transitions (0 = ignore rare, 1 = standard).
    """
    learning_rate, beta, w, rare_factor = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # state x action

    for trial in range(n_trials):
        if action_1[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # Stage 1 Choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # Stage 2 Choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # Updates
        chosen_a1 = action_1[trial]
        chosen_a2 = action_2[trial]
        
        # Determine if transition was common
        # Common: Action 0 -> State 0, Action 1 -> State 1
        is_common = (chosen_a1 == state_idx)
        
        # Scale learning rate if transition was rare
        current_lr = learning_rate if is_common else learning_rate * rare_factor
        
        # Stage 1 MF Update
        # Update Stage 1 MF value based on the value of the state reached
        delta_stage1 = q_stage2_mf[state_idx, chosen_a2] - q_stage1_mf[chosen_a1]
        q_stage1_mf[chosen_a1] += current_lr * delta_stage1

        # Stage 2 MF Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, chosen_a2]
        q_stage2_mf[state_idx, chosen_a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Dual-Stage Stickiness
This model captures perseveration (stickiness) in both stages of the task. While standard models often only account for stickiness in spaceship selection, participants may also form habits in alien selection (Stage 2), repeating the same motor action regardless of the planet or reward history.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Dual-Stage Stickiness Model.
    
    This model incorporates choice perseveration (stickiness) in both the first stage 
    (spaceship choice) and the second stage (alien choice). This accounts for 
    motor repetition or habit formation at both decision points.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate for action values.
    beta: [0, 10] - Inverse temperature for softmax choice.
    w: [0, 1] - Weight for Model-Based values.
    stick_1: [0, 10] - Stickiness bonus for repeating the previous Stage 1 choice.
    stick_2: [0, 10] - Stickiness bonus for repeating the previous Stage 2 choice.
    """
    learning_rate, beta, w, stick_1, stick_2 = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_a1 = -1
    prev_a2 = -1 

    for trial in range(n_trials):
        if action_1[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # Stage 1 Choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add Stickiness 1
        logits_1 = beta * q_net
        if prev_a1 != -1:
            logits_1[prev_a1] += stick_1
            
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # Stage 2 Choice
        # Add Stickiness 2 (Motor stickiness: repeating the same index)
        logits_2 = beta * q_stage2_mf[state_idx]
        if prev_a2 != -1:
            logits_2[prev_a2] += stick_2
            
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # Updates
        chosen_a1 = action_1[trial]
        chosen_a2 = action_2[trial]
        
        delta_stage1 = q_stage2_mf[state_idx, chosen_a2] - q_stage1_mf[chosen_a1]
        q_stage1_mf[chosen_a1] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, chosen_a2]
        q_stage2_mf[state_idx, chosen_a2] += learning_rate * delta_stage2
        
        prev_a1 = chosen_a1
        prev_a2 = chosen_a2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Planet Stickiness Model
This model introduces a form of state-based perseveration. Instead of simply repeating the previous *action* (spaceship), the participant is assumed to have a bias to return to the previously visited *planet* (state). They achieve this by boosting the value of the spaceship that commonly leads to that planet.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Planet Stickiness Model (State-Based Perseveration).
    
    This model assumes the participant has a tendency to return to the previously 
    visited planet (State), regardless of the specific spaceship chosen. 
    The model adds a bonus to the spaceship that commonly leads to the 
    planet visited in the previous trial.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate for action values.
    beta: [0, 10] - Inverse temperature for softmax choice.
    w: [0, 1] - Weight for Model-Based values.
    planet_stick: [0, 10] - Bonus added to the action commonly leading to the previous trial's planet.
    """
    learning_rate, beta, w, planet_stick = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_state = -1

    for trial in range(n_trials):
        if action_1[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # Stage 1 Choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add Planet Stickiness
        logits_1 = beta * q_net
        if prev_state != -1:
            # If prev_state was 0 (Planet X), bonus to Action 0 (A) (Common path)
            # If prev_state was 1 (Planet Y), bonus to Action 1 (U) (Common path)
            target_action = prev_state 
            logits_1[target_action] += planet_stick
            
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # Stage 2 Choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # Updates
        chosen_a1 = action_1[trial]
        chosen_a2 = action_2[trial]
        
        delta_stage1 = q_stage2_mf[state_idx, chosen_a2] - q_stage1_mf[chosen_a1]
        q_stage1_mf[chosen_a1] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, chosen_a2]
        q_stage2_mf[state_idx, chosen_a2] += learning_rate * delta_stage2
        
        prev_state = state_idx

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```