Here are three new cognitive models for the two-step decision task, designed to capture specific behavioral mechanisms such as stage-specific exploration, asymmetric learning from positive/negative outcomes, and differential plasticity between task stages.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model (MB/MF) with Stage-Specific Inverse Temperatures and Perseveration.
    
    Combines Model-Based (MB) and Model-Free (MF) values for the first stage choice.
    Crucially, it uses separate inverse temperature (beta) parameters for Stage 1 and Stage 2,
    allowing the model to capture different levels of decision noise or exploration/exploitation 
    trade-offs in the spaceship choice versus the alien choice. 
    It also includes stage-specific perseveration.
    
    Parameters:
    learning_rate: Learning rate for MF Q-values [0,1]
    beta_1: Inverse temperature for Stage 1 [0,10]
    beta_2: Inverse temperature for Stage 2 [0,10]
    w: Weighting parameter (0=MF, 1=MB) [0,1]
    pers_s1: Perseveration bonus for Stage 1 [0,5]
    pers_s2: Perseveration bonus for Stage 2 [0,5]
    """
    learning_rate, beta_1, beta_2, w, pers_s1, pers_s2 = model_parameters
    n_trials = len(action_1)
    
    # Transition matrix: Row=Action, Col=State. 0->0 (0.7), 1->1 (0.7)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_mf_1 = np.full(2, 0.5)
    q_mf_2 = np.full((2, 2), 0.5)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1
    # Track last action per state (planet) for Stage 2 perseveration
    last_action_2 = np.full(2, -1)
    
    for trial in range(n_trials):
        # Handle missing data
        if action_1[trial] == -1:
            last_action_1 = -1
            last_action_2[:] = -1
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue
            
        # --- Stage 1 Policy ---
        # Model-Based Value: T * max(Q_MF_2)
        max_q2 = np.max(q_mf_2, axis=1)
        q_mb_1 = transition_matrix @ max_q2
        
        # Net Value: Weighted sum of MB and MF
        q_net_1 = w * q_mb_1 + (1 - w) * q_mf_1
        
        # Add Perseveration
        if last_action_1 != -1:
            q_net_1[last_action_1] += pers_s1
            
        exp_q1 = np.exp(beta_1 * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Policy ---
        s_idx = state[trial]
        q_net_2 = q_mf_2[s_idx].copy()
        
        if last_action_2[s_idx] != -1:
            q_net_2[last_action_2[s_idx]] += pers_s2
            
        exp_q2 = np.exp(beta_2 * q_net_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        # Stage 1 MF Update (TD(0))
        delta_1 = q_mf_2[s_idx, action_2[trial]] - q_mf_1[action_1[trial]]
        q_mf_1[action_1[trial]] += learning_rate * delta_1
        
        # Stage 2 MF Update
        delta_2 = reward[trial] - q_mf_2[s_idx, action_2[trial]]
        q_mf_2[s_idx, action_2[trial]] += learning_rate * delta_2
        
        # Update history
        last_action_1 = action_1[trial]
        last_action_2[s_idx] = action_2[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Q-Learning with Asymmetric Learning Rates and Stage-Specific Perseveration.
    
    This model assumes the participant learns differently from positive outcomes (rewards)
    versus negative outcomes (omissions). It applies different learning rates based on the
    sign of the prediction error. It also includes eligibility traces to credit Stage 1
    choices for Stage 2 outcomes, and separate perseveration parameters for each stage.
    
    Parameters:
    lr_pos: Learning rate for positive prediction errors [0,1]
    lr_neg: Learning rate for negative prediction errors [0,1]
    beta: Inverse temperature [0,10]
    lambda_eligibility: Eligibility trace decay [0,1]
    pers_s1: Perseveration for Stage 1 [0,5]
    pers_s2: Perseveration for Stage 2 [0,5]
    """
    lr_pos, lr_neg, beta, lambda_eligibility, pers_s1, pers_s2 = model_parameters
    n_trials = len(action_1)
    
    q_s1 = np.full(2, 0.5)
    q_s2 = np.full((2, 2), 0.5)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1
    last_action_2 = np.full(2, -1)
    
    for trial in range(n_trials):
        if action_1[trial] == -1:
            last_action_1 = -1
            last_action_2[:] = -1
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # --- Stage 1 Choice ---
        q_net_1 = q_s1.copy()
        if last_action_1 != -1:
            q_net_1[last_action_1] += pers_s1
            
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        # --- Stage 2 Choice ---
        s_idx = state[trial]
        q_net_2 = q_s2[s_idx].copy()
        if last_action_2[s_idx] != -1:
            q_net_2[last_action_2[s_idx]] += pers_s2
            
        exp_q2 = np.exp(beta * q_net_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        # Stage 1 Prediction Error
        delta_1 = q_s2[s_idx, action_2[trial]] - q_s1[action_1[trial]]
        # Asymmetric learning rate for Stage 1
        lr_1 = lr_pos if delta_1 > 0 else lr_neg
        q_s1[action_1[trial]] += lr_1 * delta_1
        
        # Stage 2 Prediction Error
        delta_2 = reward[trial] - q_s2[s_idx, action_2[trial]]
        # Asymmetric learning rate for Stage 2
        lr_2 = lr_pos if delta_2 > 0 else lr_neg
        q_s2[s_idx, action_2[trial]] += lr_2 * delta_2
        
        # Eligibility Trace Update for Stage 1
        # We use the learning rate corresponding to the Stage 2 outcome
        lr_trace = lr_pos if delta_2 > 0 else lr_neg
        q_s1[action_1[trial]] += lr_trace * lambda_eligibility * delta_2
        
        last_action_1 = action_1[trial]
        last_action_2[s_idx] = action_2[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-Free Q-Learning with Stage-Specific Learning Rates and Perseveration.
    
    This model assigns different learning rates to Stage 1 (Spaceship choice) and 
    Stage 2 (Alien choice). This reflects the hypothesis that the participant might 
    update beliefs about the stable spaceship transitions (Stage 1) with different 
    plasticity than the drifting alien reward probabilities (Stage 2).
    
    Parameters:
    lr_s1: Learning rate for Stage 1 [0,1]
    lr_s2: Learning rate for Stage 2 [0,1]
    beta: Inverse temperature [0,10]
    lambda_eligibility: Eligibility trace decay [0,1]
    pers_s1: Perseveration for Stage 1 [0,5]
    pers_s2: Perseveration for Stage 2 [0,5]
    """
    lr_s1, lr_s2, beta, lambda_eligibility, pers_s1, pers_s2 = model_parameters
    n_trials = len(action_1)
    
    q_s1 = np.full(2, 0.5)
    q_s2 = np.full((2, 2), 0.5)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1
    last_action_2 = np.full(2, -1)
    
    for trial in range(n_trials):
        if action_1[trial] == -1:
            last_action_1 = -1
            last_action_2[:] = -1
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # --- Stage 1 Choice ---
        q_net_1 = q_s1.copy()
        if last_action_1 != -1:
            q_net_1[last_action_1] += pers_s1
            
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        # --- Stage 2 Choice ---
        s_idx = state[trial]
        q_net_2 = q_s2[s_idx].copy()
        if last_action_2[s_idx] != -1:
            q_net_2[last_action_2[s_idx]] += pers_s2
            
        exp_q2 = np.exp(beta * q_net_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        # Stage 1 Update using Stage 1 Learning Rate
        delta_1 = q_s2[s_idx, action_2[trial]] - q_s1[action_1[trial]]
        q_s1[action_1[trial]] += lr_s1 * delta_1
        
        # Stage 2 Update using Stage 2 Learning Rate
        delta_2 = reward[trial] - q_s2[s_idx, action_2[trial]]
        q_s2[s_idx, action_2[trial]] += lr_s2 * delta_2
        
        # Eligibility trace update for Stage 1 (driven by Stage 2 error)
        # Using lr_s1 as it updates Stage 1 weights
        q_s1[action_1[trial]] += lr_s1 * lambda_eligibility * delta_2
        
        last_action_1 = action_1[trial]
        last_action_2[s_idx] = action_2[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```