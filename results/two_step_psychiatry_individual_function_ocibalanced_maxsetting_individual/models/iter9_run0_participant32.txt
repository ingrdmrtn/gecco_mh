Here are the 3 new cognitive models based on the participant data and the feedback provided.

### Model 1: Adaptive Hybrid Agent
This model integrates the **learned transition matrix** (from the best-performing model so far) into a **Hybrid Model-Based / Model-Free** framework. It tests whether combining the adaptive world model with a Model-Free habit component (controlled by `w`) explains the data better than the pure Model-Based approach. It includes value decay and perseveration.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Adaptive Hybrid Agent with Learned Transitions, Decay, and Perseveration.
    
    Combines an adaptive Model-Based controller (which learns transition probabilities)
    with a Model-Free controller.
    
    Parameters:
    - lr_trans: [0, 1] Learning rate for transition probabilities.
    - lr_val: [0, 1] Learning rate for Q-values (both MF stages).
    - w: [0, 1] Mixing weight. 1 = Pure MB, 0 = Pure MF.
    - beta1: [0, 10] Inverse temperature for Stage 1.
    - beta2: [0, 10] Inverse temperature for Stage 2.
    - decay: [0, 1] Decay rate for unchosen Stage 2 values.
    - pers: [0, 5] Perseveration bonus for repeating Stage 1 choice.
    """
    lr_trans, lr_val, w, beta1, beta2, decay, pers = model_parameters
    n_trials = len(action_1)

    # Initialize transition matrix with prior knowledge but allow it to drift
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.ones((2, 2)) * 0.5 # Initialize at 0.5
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based Value
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta1 * q_net
        
        # Perseveration
        if last_action_1 != -1:
            logits_1[last_action_1] += pers
            
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        s_idx = state[trial]
        last_action_1 = a1
        
        # --- Stage 2 Choice ---
        logits_2 = beta2 * q_stage2_mf[s_idx]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]
        
        # --- Updates ---
        
        # 1. MF Stage 1 Update (TD(0))
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr_val * delta_stage1
        
        # 2. MF Stage 2 Update
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr_val * delta_stage2
        
        # 3. Decay Unchosen Stage 2
        unchosen_a2 = 1 - a2
        q_stage2_mf[s_idx, unchosen_a2] = (1 - decay) * q_stage2_mf[s_idx, unchosen_a2] + decay * 0.5
        other_state = 1 - s_idx
        q_stage2_mf[other_state, :] = (1 - decay) * q_stage2_mf[other_state, :] + decay * 0.5
        
        # 4. Transition Learning
        # Update probability of transition a1 -> s_idx
        transition_matrix[a1, s_idx] += lr_trans * (1 - transition_matrix[a1, s_idx])
        transition_matrix[a1, 1-s_idx] = 1.0 - transition_matrix[a1, s_idx]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Asymmetric Hybrid with Eligibility Traces
This model hypothesizes that the participant learns differently from positive prediction errors (better than expected) versus negative ones. It uses **asymmetric learning rates** (`lr_pos`, `lr_neg`) and an **eligibility trace** (`lam`) to connect the Stage 2 outcome directly to the Stage 1 MF value. It assumes fixed transitions but includes value decay.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Asymmetric Hybrid Model with Eligibility Traces and Decay.
    
    Uses separate learning rates for positive and negative prediction errors.
    Uses an eligibility trace (lambda) to allow Stage 2 reward to update Stage 1 MF values.
    Assumes fixed transition matrix.
    
    Parameters:
    - lr_pos: [0, 1] Learning rate for positive prediction errors.
    - lr_neg: [0, 1] Learning rate for negative prediction errors.
    - w: [0, 1] Mixing weight.
    - lam: [0, 1] Eligibility trace parameter (0=TD(0), 1=Monte Carlo like).
    - beta1: [0, 10] Inverse temperature Stage 1.
    - beta2: [0, 10] Inverse temperature Stage 2.
    - decay: [0, 1] Decay rate for unchosen Stage 2 values.
    """
    lr_pos, lr_neg, w, lam, beta1, beta2, decay = model_parameters
    n_trials = len(action_1)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.ones((2, 2)) * 0.5
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta1 * q_net
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        s_idx = state[trial]
        
        # --- Stage 2 Choice ---
        logits_2 = beta2 * q_stage2_mf[s_idx]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]
        
        # --- Updates ---
        
        # Stage 1 TD error
        delta1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        lr1 = lr_pos if delta1 > 0 else lr_neg
        q_stage1_mf[a1] += lr1 * delta1
        
        # Stage 2 TD error
        delta2 = r - q_stage2_mf[s_idx, a2]
        lr2 = lr_pos if delta2 > 0 else lr_neg
        q_stage2_mf[s_idx, a2] += lr2 * delta2
        
        # Eligibility Trace Update for Stage 1
        # Propagate delta2 back to q_stage1_mf scaled by lambda
        q_stage1_mf[a1] += lr2 * lam * delta2
        
        # Decay
        unchosen_a2 = 1 - a2
        q_stage2_mf[s_idx, unchosen_a2] = (1 - decay) * q_stage2_mf[s_idx, unchosen_a2] + decay * 0.5
        other_state = 1 - s_idx
        q_stage2_mf[other_state, :] = (1 - decay) * q_stage2_mf[other_state, :] + decay * 0.5

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Adaptive Asymmetric Model-Based Agent
This model combines the **Adaptive Transition Learning** (from the best model) with **Asymmetric Value Learning**. It is a pure Model-Based agent (no `w`, no `q_stage1_mf`) but assumes that the values used for planning are learned asymmetrically (gains vs losses). It allows testing if the improvement in the best model came from transition learning specifically combined with separate sensitivities to reward/no-reward.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Adaptive Asymmetric Model-Based Agent.
    
    Pure Model-Based agent that learns transition probabilities online.
    Updates Stage 2 values using asymmetric learning rates (Pos/Neg).
    Includes decay and perseveration.
    
    Parameters:
    - lr_trans: [0, 1] Learning rate for transition probabilities.
    - lr_pos: [0, 1] Learning rate for positive prediction errors.
    - lr_neg: [0, 1] Learning rate for negative prediction errors.
    - beta1: [0, 10] Inverse temperature Stage 1.
    - beta2: [0, 10] Inverse temperature Stage 2.
    - decay: [0, 1] Decay rate for unchosen Stage 2 values.
    - pers: [0, 5] Perseveration bonus for Stage 1.
    """
    lr_trans, lr_pos, lr_neg, beta1, beta2, decay, pers = model_parameters
    n_trials = len(action_1)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage2_mf = np.ones((2, 2)) * 0.5
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Choice (MB Only) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        logits_1 = beta1 * q_stage1_mb
        
        if last_action_1 != -1:
            logits_1[last_action_1] += pers
            
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        s_idx = state[trial]
        last_action_1 = a1
        
        # --- Stage 2 Choice ---
        logits_2 = beta2 * q_stage2_mf[s_idx]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]
        
        # --- Updates ---
        
        # Asymmetric Stage 2 Update
        delta2 = r - q_stage2_mf[s_idx, a2]
        lr = lr_pos if delta2 > 0 else lr_neg
        q_stage2_mf[s_idx, a2] += lr * delta2
        
        # Decay
        unchosen_a2 = 1 - a2
        q_stage2_mf[s_idx, unchosen_a2] = (1 - decay) * q_stage2_mf[s_idx, unchosen_a2] + decay * 0.5
        other_state = 1 - s_idx
        q_stage2_mf[other_state, :] = (1 - decay) * q_stage2_mf[other_state, :] + decay * 0.5
        
        # Transition Learning
        transition_matrix[a1, s_idx] += lr_trans * (1 - transition_matrix[a1, s_idx])
        transition_matrix[a1, 1-s_idx] = 1.0 - transition_matrix[a1, s_idx]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```