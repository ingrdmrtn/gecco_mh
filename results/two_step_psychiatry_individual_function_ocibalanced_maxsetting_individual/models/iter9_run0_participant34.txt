Here are three new cognitive models based on the participant's data and the two-step task structure.

### Model 1: Probability Distortion (CPT) Model
This model integrates principles from Cumulative Prospect Theory (CPT) into the Model-Based component. The participant may not perceive the transition probabilities (0.7/0.3) objectively. Instead, they might overweight rare transitions (treating them as more likely than they are) or underweight them (treating the system as deterministic). A probability weighting parameter `gamma` distorts the transition matrix used for Model-Based planning.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    CPT-RL Model (Probability Distortion).
    
    Applies a probability weighting function to the transition matrix in the 
    Model-Based calculation. This captures subjective biases in structural learning,
    such as ignoring rare transitions (gamma > 1) or overweighting them (gamma < 1).
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for Q-value updates.
    - beta_1: [0, 10] Inverse temperature for Stage 1 choice.
    - beta_2: [0, 10] Inverse temperature for Stage 2 choice.
    - w: [0, 1] Weight for Model-Based control (0=MF, 1=MB).
    - stickiness: [0, 5] Perseveration bonus for Stage 1.
    - gamma: [0.1, 5] Probability distortion parameter. 
             gamma=1 implies objective probability.
             gamma < 1 overweights rare events.
             gamma > 1 underweights rare events.
    """
    learning_rate, beta_1, beta_2, w, stickiness, gamma = model_parameters
    n_trials = len(action_1)
    
    # Objective transition matrix
    T_obj = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Apply CPT probability weighting function: w(p) = p^gamma / (p^gamma + (1-p)^gamma)^(1/gamma)
    # We apply this to the transition probabilities and re-normalize.
    T_subj_raw = (T_obj ** gamma) / ((T_obj ** gamma + (1 - T_obj) ** gamma) ** (1 / gamma))
    row_sums = T_subj_raw.sum(axis=1, keepdims=True)
    transition_matrix = T_subj_raw / row_sums
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # 2 planets, 2 aliens each
    
    last_action_1 = -1
    
    for t in range(n_trials):
        # --- Stage 1 Decision ---
        # Model-Based Value: Expected value of next stage using distorted transitions
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add Stickiness
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        # Softmax Stage 1
        q_net_stable = q_net - np.max(q_net)
        exp_q1 = np.exp(beta_1 * q_net_stable)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[t] = probs_1[action_1[t]]
        
        a1 = action_1[t]
        last_action_1 = a1
        s_next = state[t] # The planet we actually landed on
        
        # --- Stage 2 Decision ---
        q_s2_curr = q_stage2_mf[s_next]
        q_s2_stable = q_s2_curr - np.max(q_s2_curr)
        exp_q2 = np.exp(beta_2 * q_s2_stable)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[t] = probs_2[action_2[t]]
        
        a2 = action_2[t]
        r = reward[t]
        
        # --- Updates ---
        # Stage 1 MF Update (TD(1)-like, linking choice 1 to outcome)
        # Note: Standard two-step usually updates Q(S1, A1) towards Q(S2, A2) or Reward
        delta_stage1 = q_stage2_mf[s_next, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Stage 2 MF Update
        delta_stage2 = r - q_stage2_mf[s_next, a2]
        q_stage2_mf[s_next, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Stage 2 Win-Stay Lose-Shift (WSLS) Model
The participant data shows distinct "Win-Stay, Lose-Shift" behavior specifically at the second stage (choosing aliens), often switching aliens immediately after a loss (e.g., Trial 12 to 13), while exhibiting different habitual behavior at Stage 1. This model applies a standard hybrid RL to Stage 1, but augments Stage 2 choices with an explicit WSLS mechanism that biases the decision based on the immediate previous outcome at that specific planet.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Stage 2 WSLS Model.
    
    Separates the decision logic for Stage 1 (Spaceships) and Stage 2 (Aliens).
    Stage 1 uses Hybrid RL + Stickiness.
    Stage 2 uses RL + explicit Win-Stay Lose-Shift bias.
    This captures the rapid switching of aliens after failure, distinct from spaceship habits.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - w: [0, 1] Weight for Model-Based control.
    - stickiness_s1: [0, 5] Habitual stickiness for Stage 1 choices.
    - wsls_bias_s2: [0, 5] Bias added to Stage 2 Q-values based on WSLS logic.
                      (Adds to previous choice if Win, adds to other choice if Loss).
    """
    learning_rate, beta_1, beta_2, w, stickiness_s1, wsls_bias_s2 = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    
    # Store history for Stage 2 WSLS: [last_action, last_reward] for each planet
    # Initialize with -1 (no history)
    s2_history = np.full((2, 2), -1) # dim 0: planet, dim 1: [action, reward]
    
    for t in range(n_trials):
        # --- Stage 1 ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness_s1
            
        q_net_stable = q_net - np.max(q_net)
        exp_q1 = np.exp(beta_1 * q_net_stable)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[t] = probs_1[action_1[t]]
        
        a1 = action_1[t]
        last_action_1 = a1
        s_next = state[t]
        
        # --- Stage 2 ---
        q_s2_curr = q_stage2_mf[s_next].copy()
        
        # Apply Stage 2 WSLS Bias
        hist_action = int(s2_history[s_next, 0])
        hist_reward = int(s2_history[s_next, 1])
        
        if hist_action != -1:
            if hist_reward == 1:
                # Win-Stay: Boost previous action
                q_s2_curr[hist_action] += wsls_bias_s2
            else:
                # Lose-Shift: Boost the OTHER action
                q_s2_curr[1 - hist_action] += wsls_bias_s2
        
        q_s2_stable = q_s2_curr - np.max(q_s2_curr)
        exp_q2 = np.exp(beta_2 * q_s2_stable)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[t] = probs_2[action_2[t]]
        
        a2 = action_2[t]
        r = reward[t]
        
        # Update History for this planet
        s2_history[s_next, 0] = a2
        s2_history[s_next, 1] = r
        
        # --- Updates ---
        delta_stage1 = q_stage2_mf[s_next, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_next, a2]
        q_stage2_mf[s_next, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Streak-Based Habit Model
The participant exhibits very long "blocks" of choosing the same spaceship (Stage 1), suggesting a habit strength that grows with repetition. Unlike standard stickiness (which is binary based on the last trial), this model includes a `streak` counter. The stickiness bonus increases linearly with the number of consecutive times an action has been chosen, creating a "deepening habit" effect.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Streak-Based Habit Model.
    
    Stickiness is not constant but grows with the number of consecutive choices 
    of the same spaceship (Stage 1). This models the 'deepening' of habits 
    seen in the long blocks of identical Stage 1 choices.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - w: [0, 1] Weight for Model-Based control.
    - stick_base: [0, 5] Base stickiness for repeating the last action.
    - stick_growth: [0, 2] Additional stickiness per unit of streak length.
                    Total Bonus = stick_base + stick_growth * (streak - 1).
    """
    learning_rate, beta_1, beta_2, w, stick_base, stick_growth = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    streak_counter = 0
    
    for t in range(n_trials):
        # --- Stage 1 ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Apply Streak-Based Stickiness
        if last_action_1 != -1:
            streak_bonus = stick_base + stick_growth * (streak_counter - 1)
            q_net[last_action_1] += streak_bonus
            
        q_net_stable = q_net - np.max(q_net)
        exp_q1 = np.exp(beta_1 * q_net_stable)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[t] = probs_1[action_1[t]]
        
        a1 = action_1[t]
        
        # Update Streak
        if a1 == last_action_1:
            streak_counter += 1
        else:
            streak_counter = 1
        last_action_1 = a1
        
        s_next = state[t]
        
        # --- Stage 2 ---
        q_s2_curr = q_stage2_mf[s_next]
        q_s2_stable = q_s2_curr - np.max(q_s2_curr)
        exp_q2 = np.exp(beta_2 * q_s2_stable)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[t] = probs_2[action_2[t]]
        
        a2 = action_2[t]
        r = reward[t]
        
        # --- Updates ---
        delta_stage1 = q_stage2_mf[s_next, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_next, a2]
        q_stage2_mf[s_next, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```