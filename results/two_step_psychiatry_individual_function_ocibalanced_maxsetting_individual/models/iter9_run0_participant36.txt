Here are three new cognitive models for the two-step decision task, designed based on the participant's behavioral patterns (blocky choices, potential sensitivity to transition types, and switching behavior).

### Model 1: Transition-Weighted Eligibility Trace Model
This model modifies the eligibility trace mechanism. In standard models, the eligibility trace $\lambda$ credits the first-stage choice for the second-stage reward. This model scales that credit by the probability of the transition that occurred. If a "rare" transition leads to a reward, the model discounts the update to the first-stage choice, treating it as "lucky" noise rather than a reliable signal.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Transition-Weighted Eligibility Trace Model.
    
    The eligibility trace update for Stage 1 is scaled by the transition probability 
    of the observed transition. This means the agent credits the Stage 1 choice 
    more when the transition was expected (common) compared to unexpected (rare).
    Includes separate learning rates for positive/negative errors.

    Parameters:
    lr_pos: Learning rate for positive prediction errors [0,1]
    lr_neg: Learning rate for negative prediction errors [0,1]
    beta_1: Inverse temperature for Stage 1 [0,10]
    beta_2: Inverse temperature for Stage 2 [0,10]
    w: Weight mixing Model-Based and Model-Free values [0,1]
    p: Perseveration (stickiness) to previous Stage 1 choice [0,1]
    lam: Base eligibility trace parameter [0,1]
    """
    lr_pos, lr_neg, beta_1, beta_2, w, p, lam = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1
    
    for trial in range(n_trials):
        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if prev_action_1 != -1:
            q_net_stage1[prev_action_1] += p
            
        exp_q1 = np.exp(beta_1 * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        s1_choice = action_1[trial]
        state_idx = state[trial]
        prev_action_1 = s1_choice
        
        # Stage 2 Policy
        s2_choice = action_2[trial]
        
        if s2_choice != -1:
            exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
            probs_2 = exp_q2 / np.sum(exp_q2)
            p_choice_2[trial] = probs_2[s2_choice]
            
            # Updates
            r = reward[trial]
            
            # Stage 1 MF Update
            delta_stage1 = q_stage2_mf[state_idx, s2_choice] - q_stage1_mf[s1_choice]
            lr_s1 = lr_pos if delta_stage1 > 0 else lr_neg
            q_stage1_mf[s1_choice] += lr_s1 * delta_stage1
            
            # Stage 2 MF Update
            delta_stage2 = r - q_stage2_mf[state_idx, s2_choice]
            lr_s2 = lr_pos if delta_stage2 > 0 else lr_neg
            q_stage2_mf[state_idx, s2_choice] += lr_s2 * delta_stage2
            
            # Transition-Weighted Eligibility Trace
            # Common transitions (0->0 or 1->1) have prob 0.7
            # Rare transitions (0->1 or 1->0) have prob 0.3
            trans_prob = 0.7 if s1_choice == state_idx else 0.3
            
            q_stage1_mf[s1_choice] += lr_s2 * lam * trans_prob * delta_stage2
            
        else:
            p_choice_2[trial] = 1.0

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Probability Mixing Hybrid Model
Instead of mixing Q-values (which averages the values), this model mixes the *probabilities* (policies) of the Model-Based and Model-Free systems. This allows the system with lower entropy (higher confidence) to dominate the decision more effectively. It also includes separate inverse temperatures (`beta_mb`, `beta_mf`) to capture different levels of decision noise in each system.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Probability Mixing Hybrid Model with Decay.
    
    This model computes independent policies (probabilities) for the Model-Based 
    and Model-Free systems and mixes them linearly. This differs from standard 
    value mixing, allowing for independent strategy selection. Includes decay.
    
    Parameters:
    lr: Learning rate [0,1]
    beta_mb: Inverse temperature for Model-Based system [0,10]
    beta_mf: Inverse temperature for Model-Free system [0,10]
    beta_2: Inverse temperature for Stage 2 [0,10]
    w: Probability mixing weight (1 = pure MB, 0 = pure MF) [0,1]
    p: Stickiness [0,1]
    lam: Eligibility trace [0,1]
    decay: Decay rate of Q-values toward 0.5 [0,1]
    """
    lr, beta_mb, beta_mf, beta_2, w, p, lam, decay = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1
    
    for trial in range(n_trials):
        # Stage 1: Compute MB and MF probabilities separately
        
        # MB Values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # MF Values (with stickiness applied here)
        q_mf_aug = q_stage1_mf.copy()
        if prev_action_1 != -1:
            q_mf_aug[prev_action_1] += p
        
        # MB Probabilities
        exp_mb = np.exp(beta_mb * q_stage1_mb)
        probs_mb = exp_mb / np.sum(exp_mb)
        
        # MF Probabilities
        exp_mf = np.exp(beta_mf * q_mf_aug)
        probs_mf = exp_mf / np.sum(exp_mf)
        
        # Mixed Probabilities
        probs_1 = w * probs_mb + (1 - w) * probs_mf
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        s1_choice = action_1[trial]
        state_idx = state[trial]
        prev_action_1 = s1_choice
        
        # Stage 2 Policy
        s2_choice = action_2[trial]
        r = reward[trial]
        
        if s2_choice != -1:
            exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
            probs_2 = exp_q2 / np.sum(exp_q2)
            p_choice_2[trial] = probs_2[s2_choice]
            
            # Updates
            delta_stage1 = q_stage2_mf[state_idx, s2_choice] - q_stage1_mf[s1_choice]
            q_stage1_mf[s1_choice] += lr * delta_stage1
            
            delta_stage2 = r - q_stage2_mf[state_idx, s2_choice]
            q_stage2_mf[state_idx, s2_choice] += lr * delta_stage2
            
            q_stage1_mf[s1_choice] += lr * lam * delta_stage2
        else:
            p_choice_2[trial] = 1.0
            
        # Decay
        q_stage1_mf = q_stage1_mf * (1 - decay) + 0.5 * decay
        q_stage2_mf = q_stage2_mf * (1 - decay) + 0.5 * decay

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: WSLS-Augmented Hybrid Model
This model explicitly incorporates a "Win-Stay, Lose-Shift" (WSLS) heuristic alongside the standard MB/MF mechanisms. While `p` (stickiness) captures general perseveration (Win-Stay, Lose-Stay), the `theta` parameter specifically reinforces repeating a rewarded action and switching from an unrewarded one, capturing the participant's blocky but reactive behavior.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    WSLS-Augmented Hybrid Model.
    
    Incorporates a Win-Stay Lose-Shift (WSLS) heuristic explicitly into the 
    decision value. This is distinct from stickiness (which is Win-Stay Lose-Stay).
    The agent receives a bonus (theta) for repeating a rewarded action and 
    switching from an unrewarded one.
    
    Parameters:
    lr: Learning rate [0,1]
    beta_1: Inverse temperature Stage 1 [0,10]
    beta_2: Inverse temperature Stage 2 [0,10]
    w: MB/MF weight [0,1]
    p: Stickiness [0,1]
    theta: WSLS strength [0,10]
    lam: Eligibility trace [0,1]
    decay: Q-value decay [0,1]
    """
    lr, beta_1, beta_2, w, p, theta, lam, decay = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1
    prev_reward = -1 # Indicator for first trial
    
    for trial in range(n_trials):
        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Stickiness
        if prev_action_1 != -1:
            q_net_stage1[prev_action_1] += p
            
            # WSLS Logic
            if prev_reward != -1:
                # If prev reward > 0 (win), stay. Else (loss), shift.
                if prev_reward > 0:
                    q_net_stage1[prev_action_1] += theta
                else:
                    q_net_stage1[1 - prev_action_1] += theta

        exp_q1 = np.exp(beta_1 * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        s1_choice = action_1[trial]
        state_idx = state[trial]
        prev_action_1 = s1_choice
        
        # Stage 2 Policy
        s2_choice = action_2[trial]
        r = reward[trial]
        prev_reward = r
        
        if s2_choice != -1:
            exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
            probs_2 = exp_q2 / np.sum(exp_q2)
            p_choice_2[trial] = probs_2[s2_choice]
            
            # Updates
            delta_stage1 = q_stage2_mf[state_idx, s2_choice] - q_stage1_mf[s1_choice]
            q_stage1_mf[s1_choice] += lr * delta_stage1
            
            delta_stage2 = r - q_stage2_mf[state_idx, s2_choice]
            q_stage2_mf[state_idx, s2_choice] += lr * delta_stage2
            
            q_stage1_mf[s1_choice] += lr * lam * delta_stage2
        else:
            p_choice_2[trial] = 1.0
            
        # Decay
        q_stage1_mf = q_stage1_mf * (1 - decay) + 0.5 * decay
        q_stage2_mf = q_stage2_mf * (1 - decay) + 0.5 * decay

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```