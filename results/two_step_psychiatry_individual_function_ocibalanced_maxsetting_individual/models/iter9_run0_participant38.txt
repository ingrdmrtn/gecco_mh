Here are the three proposed cognitive models.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Asymmetric Learning with Dynamic Transitions.
    
    This model combines dynamic transition learning with asymmetric value learning rates.
    It hypothesizes that the participant learns the transition structure over time (rather 
    than knowing it a priori) and updates action values differently depending on whether 
    the outcome was better (positive RPE) or worse (negative RPE) than expected.
    
    Parameters:
    - lr_pos: [0, 1] Learning rate for positive prediction errors.
    - lr_neg: [0, 1] Learning rate for negative prediction errors.
    - lr_trans: [0, 1] Learning rate for transition probability updates.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - w: [0, 1] Mixing weight (0=MF, 1=MB).
    """
    lr_pos, lr_neg, lr_trans, beta_1, beta_2, w = model_parameters
    n_trials = len(action_1)

    # Initialize transition probabilities (flat prior)
    trans_probs = np.array([[0.5, 0.5], [0.5, 0.5]])

    # Initialize Q-values
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    q_stage2_mf.fill(0.5) # Initialize to 0.5 (middle of 0-1 reward)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # Handle missing data
        if action_1[trial] == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # --- Stage 1 Choice ---
        # Model-Based Value: Expected max Q-value of next stage based on learned transitions
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = trans_probs @ max_q_stage2
        
        # Hybrid Value
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax Probability
        exp_q1 = np.exp(beta_1 * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        s2 = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta_2 * q_stage2_mf[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]

        # --- Updates ---
        
        # Stage 2 Update (MF)
        delta_2 = r - q_stage2_mf[s2, a2]
        lr_2 = lr_pos if delta_2 > 0 else lr_neg
        q_stage2_mf[s2, a2] += lr_2 * delta_2

        # Stage 1 Update (MF)
        delta_1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        lr_1 = lr_pos if delta_1 > 0 else lr_neg
        q_stage1_mf[a1] += lr_1 * delta_1

        # Transition Probability Update
        # Update the transition probability for the chosen action and observed state towards 1
        trans_probs[a1, s2] += lr_trans * (1 - trans_probs[a1, s2])
        # Ensure probabilities sum to 1
        trans_probs[a1, 1 - s2] = 1.0 - trans_probs[a1, s2]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Dynamic Transitions with Dual Stickiness.
    
    This model extends the dynamic transition learning hypothesis by adding choice stickiness 
    (perseveration) to both Stage 1 (spaceships) and Stage 2 (aliens). This captures the 
    participant's tendency to repeat choices at both levels regardless of reward history, 
    which is a common heuristic in sequential tasks.
    
    Parameters:
    - lr_val: [0, 1] Learning rate for value updates.
    - lr_trans: [0, 1] Learning rate for transition probability updates.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - w: [0, 1] Mixing weight (0=MF, 1=MB).
    - stick_1: [0, 5] Stickiness parameter for Stage 1 choice.
    - stick_2: [0, 5] Stickiness parameter for Stage 2 choice.
    """
    lr_val, lr_trans, beta_1, beta_2, w, stick_1, stick_2 = model_parameters
    n_trials = len(action_1)

    trans_probs = np.array([[0.5, 0.5], [0.5, 0.5]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    q_stage2_mf.fill(0.5)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1
    prev_a2 = -1 # We track previous a2 generally, or could track per state. Here standard is last choice.

    for trial in range(n_trials):
        if action_1[trial] == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            prev_a1 = -1
            prev_a2 = -1
            continue

        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = trans_probs @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness bonus
        logits_1 = beta_1 * q_net_1
        if prev_a1 != -1:
            logits_1[prev_a1] += stick_1
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1)) # subtract max for numerical stability
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        s2 = state[trial]

        # --- Stage 2 Choice ---
        logits_2 = beta_2 * q_stage2_mf[s2]
        # Stickiness for stage 2: usually applied if we are in the same state as previous, 
        # or just general repetition. Given structure, we stick to the specific alien.
        # We only apply stickiness if the previous trial visited this state? 
        # Or simpler: just persistence on the button press (0/1). 
        # We will apply persistence on the action index 0/1.
        if prev_a2 != -1:
            logits_2[prev_a2] += stick_2

        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]

        # --- Updates ---
        delta_2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += lr_val * delta_2

        delta_1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr_val * delta_1

        trans_probs[a1, s2] += lr_trans * (1 - trans_probs[a1, s2])
        trans_probs[a1, 1 - s2] = 1.0 - trans_probs[a1, s2]
        
        prev_a1 = a1
        prev_a2 = a2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Dynamic Transitions with Decay and Eligibility Trace (Lambda).
    
    This model integrates three powerful mechanisms:
    1. Dynamic Transition Learning: The agent learns the transition matrix.
    2. Value Decay: Unchosen options in Stage 2 decay towards a neutral value (0.5), 
       helping to track drifting reward probabilities.
    3. Eligibility Trace (Lambda): The reward prediction error from Stage 2 is 
       propagated back to update Stage 1 values directly, allowing for faster 
       learning of the full chain.
    
    Parameters:
    - lr_val: [0, 1] Learning rate for value updates.
    - lr_trans: [0, 1] Learning rate for transition probability updates.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - w: [0, 1] Mixing weight (0=MF, 1=MB).
    - decay: [0, 1] Decay rate for unchosen Stage 2 options.
    - lam: [0, 1] Eligibility trace parameter (lambda).
    """
    lr_val, lr_trans, beta_1, beta_2, w, decay, lam = model_parameters
    n_trials = len(action_1)

    trans_probs = np.array([[0.5, 0.5], [0.5, 0.5]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    q_stage2_mf.fill(0.5)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        if action_1[trial] == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = trans_probs @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_1 * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        s2 = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta_2 * q_stage2_mf[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]

        # --- Updates ---
        
        # Stage 2 RPE
        delta_2 = r - q_stage2_mf[s2, a2]
        
        # Update Chosen Stage 2 Value
        q_stage2_mf[s2, a2] += lr_val * delta_2
        
        # Decay Unchosen Stage 2 Value
        unchosen_a2 = 1 - a2
        q_stage2_mf[s2, unchosen_a2] = (1 - decay) * q_stage2_mf[s2, unchosen_a2] + decay * 0.5

        # Stage 1 Update with Eligibility Trace
        # First, standard TD(0) update based on Q-values
        delta_1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1] # Note: using updated Q2 is common in some implementations, or old Q2 (SARSA). Here using updated approximates Q-learning.
        q_stage1_mf[a1] += lr_val * delta_1
        
        # Second, add the eligibility trace from Stage 2 RPE
        # This propagates the surprise at Stage 2 back to Stage 1
        q_stage1_mf[a1] += lr_val * lam * delta_2

        # Transition Probability Update
        trans_probs[a1, s2] += lr_trans * (1 - trans_probs[a1, s2])
        trans_probs[a1, 1 - s2] = 1.0 - trans_probs[a1, s2]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```