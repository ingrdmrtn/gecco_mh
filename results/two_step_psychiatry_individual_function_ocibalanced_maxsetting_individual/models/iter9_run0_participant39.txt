Here are three new cognitive models for the two-step task, designed to capture specific patterns in the participant's behavior such as strategy switching, risk sensitivity, and outcome-dependent reliability arbitration.

### Model 1: Probability Mixture (Strategy Switching) Model
This model posits that instead of integrating values into a single "net" Q-value (Value Mixture), the agent computes complete policies (probabilities) for the Model-Based and Model-Free systems separately and then mixes them. This represents a "Mixture of Experts" or strategy-switching approach where the agent probabilistically selects a strategy on each trial rather than averaging their value estimates. It includes stickiness and an exploration bonus (`phi`).

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Probability Mixture (Strategy Switching) Model.
    
    Computes separate choice probabilities for Model-Based (MB) and Model-Free (MF) 
    systems and mixes them linearly, rather than mixing the Q-values.
    
    Parameters:
    - lr: Learning rate [0,1]
    - beta_1: Inverse temperature for stage 1 [0,10]
    - beta_2: Inverse temperature for stage 2 [0,10]
    - w: Probability mixing weight (0=Pure MF strategy, 1=Pure MB strategy) [0,1]
    - stick: Choice stickiness [0,5]
    - phi: Exploration bonus weight [0,5]
    """
    lr, beta_1, beta_2, w, stick, phi = model_parameters
    n_trials = len(action_1)
    
    # Fixed transition matrix: T[0] = [0.7, 0.3], T[1] = [0.3, 0.7]
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # 2 planets (states), 2 aliens
    
    # Track time since last choice for exploration bonus
    time_since_choice_1 = np.zeros(2)
    
    log_loss = 0.0
    eps = 1e-10
    prev_action_1 = -1
    
    for trial in range(n_trials):
        time_since_choice_1 += 1
        
        # --- Stage 1 Policy Calculation ---
        
        # 1. Model-Based Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Apply Stickiness and Exploration to MB logits
        logits_mb = beta_1 * q_stage1_mb + phi * time_since_choice_1
        if prev_action_1 != -1:
            logits_mb[prev_action_1] += stick
            
        exp_mb = np.exp(logits_mb - np.max(logits_mb)) # safe exp
        probs_mb = exp_mb / np.sum(exp_mb)
        
        # 2. Model-Free Policy
        # Apply Stickiness and Exploration to MF logits
        logits_mf = beta_1 * q_stage1_mf + phi * time_since_choice_1
        if prev_action_1 != -1:
            logits_mf[prev_action_1] += stick
            
        exp_mf = np.exp(logits_mf - np.max(logits_mf))
        probs_mf = exp_mf / np.sum(exp_mf)
        
        # 3. Mixture of Probabilities
        probs_1 = w * probs_mb + (1 - w) * probs_mf
        
        # Accumulate Loss
        a1 = action_1[trial]
        log_loss -= np.log(probs_1[a1] + eps)
        
        # Update trace
        time_since_choice_1[a1] = 0
        prev_action_1 = a1
        
        # --- Stage 2 Policy & Updates ---
        a2 = action_2[trial]
        if a2 == -1: # Handle missing data
            continue
            
        state_idx = state[trial]
        
        # Stage 2 Policy (Standard Softmax)
        qs_2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta_2 * qs_2 - np.max(beta_2 * qs_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        log_loss -= np.log(probs_2[a2] + eps)
        
        # RL Updates
        r = reward[trial]
        
        # Stage 2 Update (TD)
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += lr * delta_stage2
        
        # Stage 1 MF Update (TD)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
    return log_loss
```

### Model 2: Risk-Sensitive Hybrid Model
This model incorporates risk sensitivity by tracking both the mean (`Q`) and the variance (`Var`) of the rewards associated with each alien in Stage 2. The Model-Based system uses these risk-adjusted values (Mean + RiskParam * StdDev) to evaluate Stage 1 options. This tests if the participant is risk-seeking (positive parameter) or risk-averse regarding the uncertain alien payoffs.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Risk-Sensitive Hybrid Model.
    
    Tracks reward variance in Stage 2. The MB system computes values based on 
    a Mean-Variance trade-off (Q + risk_param * sqrt(Variance)).
    
    Parameters:
    - lr: Learning rate [0,1]
    - beta_1: Inverse temperature for stage 1 [0,10]
    - beta_2: Inverse temperature for stage 2 [0,10]
    - w: Mixing weight [0,1]
    - stick: Choice stickiness [0,5]
    - risk_param: Risk sensitivity (positive=seeking, negative=averse) [0,5]
      (Note: Bounds are usually positive in this framework, effectively a 'bonus' for variability)
    """
    lr, beta_1, beta_2, w, stick, risk_param = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    var_stage2 = np.zeros((2, 2)) # Variance tracking for Stage 2
    
    log_loss = 0.0
    eps = 1e-10
    prev_action_1 = -1
    
    for trial in range(n_trials):
        
        # --- Stage 1 Choice ---
        
        # Calculate Risk-Adjusted Stage 2 Values
        # Q_risk = Q + theta * sqrt(Var)
        q_stage2_risk = q_stage2_mf + risk_param * np.sqrt(var_stage2)
        
        # MB Calculation using Risk-Adjusted values
        max_q_stage2 = np.max(q_stage2_risk, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if prev_action_1 != -1:
            q_net[prev_action_1] += stick
            
        exp_q1 = np.exp(beta_1 * q_net - np.max(beta_1 * q_net))
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = action_1[trial]
        log_loss -= np.log(probs_1[a1] + eps)
        prev_action_1 = a1
        
        # --- Stage 2 Choice ---
        a2 = action_2[trial]
        if a2 == -1:
            continue
            
        state_idx = state[trial]
        
        # Stage 2 Policy uses standard Q-values (or could use risk, but standard is safer baseline)
        qs_2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta_2 * qs_2 - np.max(beta_2 * qs_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        log_loss -= np.log(probs_2[a2] + eps)
        
        # --- Updates ---
        r = reward[trial]
        
        # Update Stage 2 Mean
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += lr * delta_stage2
        
        # Update Stage 2 Variance: Var <- Var + lr * ((R - Q)^2 - Var)
        # Note: We use the Q value *before* update for the error, or after? Standard is before.
        sq_error = delta_stage2 ** 2
        var_stage2[state_idx, a2] += lr * (sq_error - var_stage2[state_idx, a2])
        
        # Update Stage 1 MF
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
    return log_loss
```

### Model 3: Outcome-Dependent Arbitration Model
This model hypothesizes that the participant's reliance on Model-Based vs. Model-Free strategies shifts dynamically based on the previous trial's outcome. A "Win-Stay" heuristic might imply relying on MF (habit) after a reward, while a "Lose-Shift" heuristic might trigger re-evaluation using the MB (planning) system. Thus, `w` (MB weight) toggles between `w_win` and `w_loss`.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Outcome-Dependent Arbitration Model.
    
    The mixing weight 'w' depends on the previous trial's reward.
    Allows testing if the participant becomes more Model-Based (planning) 
    after losses or more Model-Free (habitual) after wins.
    
    Parameters:
    - lr: Learning rate [0,1]
    - beta_1: Inverse temperature for stage 1 [0,10]
    - beta_2: Inverse temperature for stage 2 [0,10]
    - w_win: MB weight after a rewarded trial [0,1]
    - w_loss: MB weight after an unrewarded trial [0,1]
    - stick: Choice stickiness [0,5]
    """
    lr, beta_1, beta_2, w_win, w_loss, stick = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    log_loss = 0.0
    eps = 1e-10
    prev_action_1 = -1
    prev_reward = 0 # Assume loss/neutral for first trial context
    
    for trial in range(n_trials):
        
        # Determine current mixing weight based on previous reward
        w = w_win if prev_reward == 1 else w_loss
        
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if prev_action_1 != -1:
            q_net[prev_action_1] += stick
            
        exp_q1 = np.exp(beta_1 * q_net - np.max(beta_1 * q_net))
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = action_1[trial]
        log_loss -= np.log(probs_1[a1] + eps)
        prev_action_1 = a1
        
        # --- Stage 2 Choice ---
        a2 = action_2[trial]
        if a2 == -1:
            # If trial is invalid, we still need to update prev_reward for next trial logic?
            # Usually invalid trials break the chain. We'll assume prev_reward=0.
            prev_reward = 0
            continue
            
        state_idx = state[trial]
        qs_2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta_2 * qs_2 - np.max(beta_2 * qs_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        log_loss -= np.log(probs_2[a2] + eps)
        
        # --- Updates ---
        r = reward[trial]
        prev_reward = r # Store for next trial's arbitration
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += lr * delta_stage2
        
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
    return log_loss
```