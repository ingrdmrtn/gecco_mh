Here are three cognitive models designed to explain the participant's behavior in the two-step decision task.

### Model 1: Dual-Learning Rate Hybrid Learner with Counterfactuals
This model extends the successful hybrid (Model-Based/Model-Free) approach by allowing different learning rates for the two stages. The participant shows distinct behavioral patterns in the first stage (strong perseveration/bias) versus the second stage (rapid adaptation to reward probabilities). Separating `lr_1` (Spaceship transitions) and `lr_2` (Alien rewards) allows the model to capture this varying plasticity. It also retains the counterfactual updating mechanism for the unchosen alien, which is crucial for binary outcome tasks.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Dual-Learning Rate Hybrid Learner with Counterfactuals.
    
    Distinguishes between learning rates for the stable first stage (transitions)
    and the drifting second stage (rewards). Includes counterfactual updating
    for the unchosen alien in Stage 2.
    
    Parameters:
    lr_1: [0, 1] Learning rate for Stage 1 (Spaceship choice).
    lr_2: [0, 1] Learning rate for Stage 2 (Alien choice).
    beta_1: [0, 10] Inverse temperature for Stage 1.
    beta_2: [0, 10] Inverse temperature for Stage 2.
    w: [0, 1] Mixing weight (0=MF, 1=MB).
    stickiness: [0, 5] Perseveration bonus for Stage 1.
    cf_weight: [0, 1] Weight of counterfactual update for unchosen alien.
    """
    lr_1, lr_2, beta_1, beta_2, w, stickiness, cf_weight = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta_1 * q_net_1
        if prev_action_1 != -1:
            logits_1[prev_action_1] += stickiness
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        prev_action_1 = action_1[trial]
        
        # --- Stage 2 Decision & Updates ---
        s_idx = state[trial]
        a2 = action_2[trial]
        
        if a2 != -1:
            logits_2 = beta_2 * q_stage2_mf[s_idx]
            exp_q2 = np.exp(logits_2 - np.max(logits_2))
            probs_2 = exp_q2 / np.sum(exp_q2)
            p_choice_2[trial] = probs_2[a2]
            
            r = reward[trial]
            
            # Update Stage 2 (Alien) - Chosen
            pe_2 = r - q_stage2_mf[s_idx, a2]
            q_stage2_mf[s_idx, a2] += lr_2 * pe_2
            
            # Update Stage 2 (Alien) - Unchosen (Counterfactual)
            # Assume anti-correlated outcomes: if r=0, unchosen implies 1.
            cf_target = 1.0 if r <= 0 else 0.0
            pe_cf = cf_target - q_stage2_mf[s_idx, 1-a2]
            q_stage2_mf[s_idx, 1-a2] += lr_2 * cf_weight * pe_cf
            
            # Update Stage 1 (Spaceship)
            pe_1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[action_1[trial]]
            q_stage1_mf[action_1[trial]] += lr_1 * pe_1
            
        else:
            p_choice_2[trial] = 1.0

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Biased Hybrid Learner with Counterfactuals
The participant exhibits an overwhelming preference for Spaceship 0 (choosing it in the vast majority of trials), regardless of recent outcomes. While `stickiness` captures the tendency to repeat the *previous* choice, it depends on history. This model adds a static `bias_0` parameter that represents an intrinsic, constant preference for Spaceship 0, independent of learning or history. This likely explains the data better than stickiness alone.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Biased Hybrid Learner with Counterfactuals.
    
    Incorporates a static bias towards Spaceship 0, alongside dynamic stickiness
    and hybrid Model-Based/Model-Free learning. This accounts for strong 
    intrinsic preferences independent of reward history.
    
    Parameters:
    learning_rate: [0, 1] Learning rate for value updates.
    beta_1: [0, 10] Inverse temperature for Stage 1.
    beta_2: [0, 10] Inverse temperature for Stage 2.
    w: [0, 1] Mixing weight (0=MF, 1=MB).
    stickiness: [0, 5] Perseveration bonus for Stage 1.
    bias_0: [0, 5] Static bias added to Spaceship 0 logit.
    cf_weight: [0, 1] Weight of counterfactual update for unchosen alien.
    """
    learning_rate, beta_1, beta_2, w, stickiness, bias_0, cf_weight = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    prev_action_1 = -1

    for trial in range(n_trials):
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta_1 * q_net_1
        
        # Add Stickiness
        if prev_action_1 != -1:
            logits_1[prev_action_1] += stickiness
            
        # Add Static Bias to Option 0
        logits_1[0] += bias_0
        
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        prev_action_1 = action_1[trial]
        
        s_idx = state[trial]
        a2 = action_2[trial]
        
        if a2 != -1:
            logits_2 = beta_2 * q_stage2_mf[s_idx]
            exp_q2 = np.exp(logits_2 - np.max(logits_2))
            probs_2 = exp_q2 / np.sum(exp_q2)
            p_choice_2[trial] = probs_2[a2]
            
            r = reward[trial]
            pe_2 = r - q_stage2_mf[s_idx, a2]
            q_stage2_mf[s_idx, a2] += learning_rate * pe_2
            
            cf_target = 1.0 if r <= 0 else 0.0
            pe_cf = cf_target - q_stage2_mf[s_idx, 1-a2]
            q_stage2_mf[s_idx, 1-a2] += learning_rate * cf_weight * pe_cf
            
            pe_1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[action_1[trial]]
            q_stage1_mf[action_1[trial]] += learning_rate * pe_1
        else:
            p_choice_2[trial] = 1.0

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Decaying Hybrid Learner with Counterfactuals
Since the reward probabilities of the aliens drift slowly over time, older information becomes less reliable. This model introduces a `decay_rate` parameter that pulls the Q-values of the aliens (Stage 2) towards a neutral prior (0.5) on every trial. This models a "forgetting" or uncertainty process, encouraging the agent to rely more on recent outcomes or re-explore neglected options.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Decaying Hybrid Learner with Counterfactuals.
    
    Q-values for aliens decay towards a neutral center (0.5) on every trial,
    representing forgetting or uncertainty about drifting reward probabilities.
    Includes counterfactual updating.
    
    Parameters:
    learning_rate: [0, 1] Update rate.
    beta_1: [0, 10] Inverse temperature Stage 1.
    beta_2: [0, 10] Inverse temperature Stage 2.
    w: [0, 1] Mixing weight.
    stickiness: [0, 5] Perseveration bonus.
    cf_weight: [0, 1] Counterfactual weight.
    decay_rate: [0, 1] Rate at which Q-values decay to 0.5.
    """
    learning_rate, beta_1, beta_2, w, stickiness, cf_weight, decay_rate = model_parameters
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    # Initialize Stage 2 values at 0.5 (neutral) since we decay towards 0.5
    q_stage2_mf = np.zeros((2, 2)) + 0.5 
    
    prev_action_1 = -1

    for trial in range(n_trials):
        # Decay Step: Stage 2 Q-values decay towards 0.5 (uncertainty)
        q_stage2_mf += decay_rate * (0.5 - q_stage2_mf)

        # Stage 1 Decision
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta_1 * q_net_1
        if prev_action_1 != -1:
            logits_1[prev_action_1] += stickiness
        
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        prev_action_1 = action_1[trial]
        
        # Stage 2 Decision
        s_idx = state[trial]
        a2 = action_2[trial]
        
        if a2 != -1:
            logits_2 = beta_2 * q_stage2_mf[s_idx]
            exp_q2 = np.exp(logits_2 - np.max(logits_2))
            probs_2 = exp_q2 / np.sum(exp_q2)
            p_choice_2[trial] = probs_2[a2]
            
            r = reward[trial]
            pe_2 = r - q_stage2_mf[s_idx, a2]
            q_stage2_mf[s_idx, a2] += learning_rate * pe_2
            
            cf_target = 1.0 if r <= 0 else 0.0
            pe_cf = cf_target - q_stage2_mf[s_idx, 1-a2]
            q_stage2_mf[s_idx, 1-a2] += learning_rate * cf_weight * pe_cf
            
            pe_1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[action_1[trial]]
            q_stage1_mf[action_1[trial]] += learning_rate * pe_1
        else:
            p_choice_2[trial] = 1.0

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```