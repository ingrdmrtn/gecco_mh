Here are the three proposed cognitive models.

### Model 1: MF Q-Learning with Dual Learning Rates, Decay, and Perseverance
This model extends the best-performing model by splitting the learning rate into two separate parameters for Stage 1 (Spaceship choice) and Stage 2 (Alien choice). This accounts for the fact that Stage 1 involves learning stable transition structures (which might be learned slowly or quickly) while Stage 2 involves tracking slowly drifting reward probabilities.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    MF Q-Learning with Dual Learning Rates, Decay, and Perseverance.
    
    Separates learning rates for the spaceship choice (Stage 1) and alien choice (Stage 2),
    acknowledging their different dynamics (stable transitions vs drifting reward probs).
    Includes decay for unchosen options and perseverance for the first stage choice.
    
    Parameters:
    - alpha1: [0, 1] Learning rate for Stage 1 Q-values.
    - alpha2: [0, 1] Learning rate for Stage 2 Q-values.
    - beta: [0, 10] Inverse temperature (softmax sensitivity).
    - decay: [0, 1] Decay rate for unchosen Q-values (0=no decay, 1=reset to 0).
    - perseverance: [0, 5] Additional bias added to the logits of the previously chosen action.
    """
    alpha1, alpha2, beta, decay, perseverance = model_parameters
    n_trials = len(action_1)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.full(2, 0.5)
    q_stage2 = np.full((2, 2), 0.5)
    
    prev_action_1 = -1

    for trial in range(n_trials):
        # Stage 1 Choice
        logits = beta * q_stage1
        if prev_action_1 != -1:
            logits[prev_action_1] += perseverance
        
        exp_q1 = np.exp(logits)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Stage 2 Choice
        s = state[trial]
        exp_q2 = np.exp(beta * q_stage2[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # Updates
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]
        
        # Stage 1 Update (using alpha1)
        target_stage1 = q_stage2[s, a2]
        q_stage1[a1] += alpha1 * (target_stage1 - q_stage1[a1])
        q_stage1[1-a1] *= (1.0 - decay)
        
        # Stage 2 Update (using alpha2)
        q_stage2[s, a2] += alpha2 * (r - q_stage2[s, a2])
        q_stage2[s, 1-a2] *= (1.0 - decay)
        
        prev_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: MF Q-Learning with Eligibility Traces (Lambda), Decay, and Perseverance
This model introduces an eligibility trace (`lam`) to the best-performing architecture. This allows the reward prediction error from Stage 2 to directly update the Stage 1 Q-values, bridging the temporal gap more effectively than simple TD(0) learning, while retaining the crucial decay and perseverance mechanisms.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    MF Q-Learning with Eligibility Traces (Lambda), Decay, and Perseverance.
    
    Incorporates eligibility traces (lambda) to allow the reward outcome to directly 
    influence the first-stage choice value, bridging the gap between stages.
    
    Parameters:
    - alpha: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - lam: [0, 1] Eligibility trace parameter (weight of stage 2 RPE on stage 1 update).
    - decay: [0, 1] Decay rate for unchosen Q-values.
    - perseverance: [0, 5] Bias for repeating the previous Stage 1 choice.
    """
    alpha, beta, lam, decay, perseverance = model_parameters
    n_trials = len(action_1)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.full(2, 0.5)
    q_stage2 = np.full((2, 2), 0.5)
    
    prev_action_1 = -1

    for trial in range(n_trials):
        # Stage 1 Choice
        logits = beta * q_stage1
        if prev_action_1 != -1:
            logits[prev_action_1] += perseverance
        
        exp_q1 = np.exp(logits)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Stage 2 Choice
        s = state[trial]
        exp_q2 = np.exp(beta * q_stage2[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # Updates
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]
        
        # Prediction Errors
        pe_1 = q_stage2[s, a2] - q_stage1[a1]
        pe_2 = r - q_stage2[s, a2]
        
        # Stage 1 Update (Direct + Eligibility Trace)
        q_stage1[a1] += alpha * pe_1 + alpha * lam * pe_2
        q_stage1[1-a1] *= (1.0 - decay)
        
        # Stage 2 Update
        q_stage2[s, a2] += alpha * pe_2
        q_stage2[s, 1-a2] *= (1.0 - decay)
        
        prev_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: MF Q-Learning with Decay and Asymmetric Perseverance
The participant data shows long streaks of repeating the same choice, even interspersed with losses. This model refines the perseverance mechanism by allowing different "stickiness" levels depending on whether the previous trial was rewarded (`p_win`) or unrewarded (`p_loss`). This captures "Win-Stay, Lose-Stay" or "Win-Stay, Lose-Switch" behaviors more granularly.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    MF Q-Learning with Decay and Asymmetric Perseverance (Win-Stay/Lose-Stay).
    
    Perseverance is modulated by the previous trial's reward outcome, allowing
    different degrees of stickiness after wins (p_win) vs losses (p_loss).
    
    Parameters:
    - alpha: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - decay: [0, 1] Decay rate for unchosen Q-values.
    - p_win: [0, 5] Perseverance bonus after a rewarded trial.
    - p_loss: [0, 5] Perseverance bonus after an unrewarded trial.
    """
    alpha, beta, decay, p_win, p_loss = model_parameters
    n_trials = len(action_1)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.full(2, 0.5)
    q_stage2 = np.full((2, 2), 0.5)
    
    prev_action_1 = -1
    prev_reward = 0

    for trial in range(n_trials):
        # Stage 1 Choice
        logits = beta * q_stage1
        if prev_action_1 != -1:
            bias = p_win if prev_reward == 1 else p_loss
            logits[prev_action_1] += bias
        
        exp_q1 = np.exp(logits)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Stage 2 Choice
        s = state[trial]
        exp_q2 = np.exp(beta * q_stage2[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # Updates
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]
        
        q_stage1[a1] += alpha * (q_stage2[s, a2] - q_stage1[a1])
        q_stage1[1-a1] *= (1.0 - decay)
        
        q_stage2[s, a2] += alpha * (r - q_stage2[s, a2])
        q_stage2[s, 1-a2] *= (1.0 - decay)
        
        prev_action_1 = a1
        prev_reward = r

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```