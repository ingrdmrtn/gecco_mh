Here are three new cognitive models for the two-step decision task.

### Model 1: Risk-Sensitive Hybrid Model
This model combines the "Hybrid" architecture (weighted mixture of Model-Based and Model-Free values) with "Risk-Sensitive" learning. The agent learns from prediction errors asymmetrically: positive errors (better than expected) and negative errors (worse than expected) are scaled by different learning rates (`alpha_pos`, `alpha_neg`). This allows the model to capture optimistic or pessimistic learning biases within a two-step planning framework.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Risk-Sensitive Hybrid Model.
    
    Combines Model-Based (MB) planning with Model-Free (MF) learning, where the 
    MF updates are risk-sensitive (asymmetric learning rates for positive/negative errors).
    
    Parameters:
    alpha_pos: [0,1] - Learning rate for positive prediction errors.
    alpha_neg: [0,1] - Learning rate for negative prediction errors.
    beta_1: [0,10] - Inverse temperature for Stage 1.
    beta_2: [0,10] - Inverse temperature for Stage 2.
    w: [0,1] - Weighting parameter (0 = Pure MF, 1 = Pure MB).
    stickiness: [0,10] - Choice stickiness (perseveration) bonus.
    """
    alpha_pos, alpha_neg, beta_1, beta_2, w, stickiness = model_parameters
    n_trials = len(action_1)
    
    # Transition matrix: T[action, state]
    # 0->0 (A->X) is common (0.7), 1->1 (U->Y) is common (0.7)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Q-values
    q_mf_1 = np.zeros(2)        # Stage 1 MF values
    q_mf_2 = np.zeros((2, 2))   # Stage 2 MF values (State, Alien)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # Model-Based Value: T * max(Q_stage2)
        max_q2 = np.max(q_mf_2, axis=1) # Max value for each state (Planet)
        q_mb_1 = transition_matrix @ max_q2
        
        # Hybrid Value
        q_net_1 = w * q_mb_1 + (1 - w) * q_mf_1
        
        # Action selection
        logits_1 = beta_1 * q_net_1
        if prev_a1 != -1:
            logits_1[prev_a1] += stickiness
            
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        if np.isnan(action_1[trial]):
            p_choice_1[trial] = 0.5
            prev_a1 = -1
            continue
            
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        prev_a1 = a1
        
        # --- Stage 2 Decision ---
        s_idx = int(state[trial]) # Planet reached
        
        logits_2 = beta_2 * q_mf_2[s_idx]
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        if np.isnan(action_2[trial]):
            p_choice_2[trial] = 0.5
            continue
            
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        r = reward[trial]
        
        # --- Updates ---
        # Stage 1 MF Update (TD(0): Update Q1 towards Q2)
        # Note: We use the value of the chosen option in stage 2 as the target
        delta_1 = q_mf_2[s_idx, a2] - q_mf_1[a1]
        lr_1 = alpha_pos if delta_1 > 0 else alpha_neg
        q_mf_1[a1] += lr_1 * delta_1
        
        # Stage 2 MF Update
        delta_2 = r - q_mf_2[s_idx, a2]
        lr_2 = alpha_pos if delta_2 > 0 else alpha_neg
        q_mf_2[s_idx, a2] += lr_2 * delta_2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Pure Model-Based with Decaying Choice Kernel
This model assumes the participant is purely Model-Based (planning using the transition matrix) but is influenced by a "Choice Kernel" that tracks the frequency of recent choices. Unlike simple stickiness (which only considers the very last choice), the kernel decays slowly, capturing a longer-term habit or trend in decision-making.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Based with Decaying Choice Kernel.
    
    The agent calculates values purely via Model-Based planning (using fixed transitions).
    However, choice probabilities are also influenced by a 'Choice Kernel' which accumulates 
    a trace of past choices, creating a habit strength that decays over time.
    
    Parameters:
    alpha: [0,1] - Learning rate for Stage 2 values.
    beta_1: [0,10] - Inverse temperature for Stage 1.
    beta_2: [0,10] - Inverse temperature for Stage 2.
    kernel_lr: [0,1] - Learning rate (decay rate) for the choice kernel.
    kernel_weight: [0,10] - Weight of the choice kernel in the decision.
    """
    alpha, beta_1, beta_2, kernel_lr, kernel_weight = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage2 = np.zeros((2, 2)) # State x Alien
    choice_kernel = np.zeros(2) # Trace for Stage 1 choices
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # MB Value Calculation
        max_q2 = np.max(q_stage2, axis=1)
        q_mb_1 = transition_matrix @ max_q2
        
        # Policy mixes Value and Habit (Kernel)
        logits_1 = beta_1 * q_mb_1 + kernel_weight * choice_kernel
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        if np.isnan(action_1[trial]):
            p_choice_1[trial] = 0.5
            # Decay kernel even if no choice? Assuming no update if missing.
            continue
            
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # Update Kernel: K_new = (1-lr)*K_old + lr*Ind(choice)
        choice_kernel *= (1 - kernel_lr)
        choice_kernel[a1] += kernel_lr * 1.0
        
        # --- Stage 2 Decision ---
        s_idx = int(state[trial])
        
        logits_2 = beta_2 * q_stage2[s_idx]
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        if np.isnan(action_2[trial]):
            p_choice_2[trial] = 0.5
            continue
            
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        r = reward[trial]
        
        # --- Update Stage 2 Values ---
        delta = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += alpha * delta
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Pure Model-Based with Transition Learning
This model assumes the participant is purely Model-Based but does not trust the fixed transition probabilities (70/30). Instead, the agent actively learns the transition matrix (`lr_trans`) based on observed transitions (Space A -> Planet X vs Y). This allows the agent to adapt if they perceive the "common" and "rare" paths are changing or different from instructions.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Pure Model-Based with Transition Learning.
    
    The agent is Model-Based but learns the transition matrix (Spaceship -> Planet)
    dynamically from experience, rather than using fixed probabilities.
    
    Parameters:
    lr_val: [0,1] - Learning rate for Stage 2 values (Aliens).
    lr_trans: [0,1] - Learning rate for the transition matrix probabilities.
    beta_1: [0,10] - Inverse temperature for Stage 1.
    beta_2: [0,10] - Inverse temperature for Stage 2.
    stickiness: [0,10] - Choice stickiness bonus for Stage 1.
    """
    lr_val, lr_trans, beta_1, beta_2, stickiness = model_parameters
    n_trials = len(action_1)
    
    # Initialize Transition Matrix with prior belief (e.g., 0.7 as instructed)
    # T[0,0] = P(Planet 0 | Spaceship 0)
    # T[1,1] = P(Planet 1 | Spaceship 1)
    # We maintain the diagonal probabilities and assume T[i,j] = 1 - T[i,i]
    p_trans = np.array([0.7, 0.7]) 
    
    q_stage2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1
    
    for trial in range(n_trials):
        # Construct current full transition matrix
        # T = [[p_trans[0], 1-p_trans[0]], [1-p_trans[1], p_trans[1]]]
        T = np.array([
            [p_trans[0], 1.0 - p_trans[0]],
            [1.0 - p_trans[1], p_trans[1]]
        ])
        
        # --- Stage 1 Decision ---
        max_q2 = np.max(q_stage2, axis=1)
        q_mb_1 = T @ max_q2
        
        logits_1 = beta_1 * q_mb_1
        if prev_a1 != -1:
            logits_1[prev_a1] += stickiness
            
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        if np.isnan(action_1[trial]):
            p_choice_1[trial] = 0.5
            prev_a1 = -1
            continue
            
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        prev_a1 = a1
        
        # --- Stage 2 Decision ---
        s_idx = int(state[trial])
        
        logits_2 = beta_2 * q_stage2[s_idx]
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        if np.isnan(action_2[trial]):
            p_choice_2[trial] = 0.5
            continue
            
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        r = reward[trial]
        
        # --- Updates ---
        
        # 1. Value Update (Stage 2)
        delta = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += lr_val * delta
        
        # 2. Transition Update
        # If a1=0 (Space A): Observed transition to s_idx. 
        # Target for p_trans[0] is 1 if s_idx=0, else 0.
        if a1 == 0:
            target = 1.0 if s_idx == 0 else 0.0
            p_trans[0] += lr_trans * (target - p_trans[0])
        elif a1 == 1:
            # If a1=1 (Space U): Observed transition to s_idx.
            # Target for p_trans[1] is 1 if s_idx=1, else 0.
            target = 1.0 if s_idx == 1 else 0.0
            p_trans[1] += lr_trans * (target - p_trans[1])
            
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```