Here are three new cognitive models for the two-step task, designed to capture different psychological mechanisms for learning and decision-making.

### Model 1: Coupled Alien Learning (Cross-State Generalization)
This model hypothesizes that the participant generalizes learning about aliens across planets. When the participant updates the value of an alien on the current planet, they also update the value of the **same alien** on the **other planet**, scaled by a `coupling_factor`. This reflects a belief that aliens of the same type (e.g., Alien 0) might share characteristics regardless of location.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Coupled Alien Learning Model.
    
    This model assumes that learning about an alien in one state (planet) 
    partially transfers to the same alien in the other state. This is controlled 
    by the 'coupling_factor'.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for Q-value updates.
    - beta: [0, 10] Inverse temperature for softmax choice policy.
    - w: [0, 1] Weighting parameter for Model-Based (1) vs Model-Free (0) control.
    - lambda_coef: [0, 1] Eligibility trace decay parameter.
    - stickiness: [0, 10] Choice perseverance bonus for Stage 1.
    - coupling_factor: [0, 1] Degree to which update in one state transfers to the other state.
    """
    learning_rate, beta, w, lambda_coef, stickiness, coupling_factor = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action (Alien)

    last_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_s1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net_s1
        if last_action_1 != -1:
            logits_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_action_1 = action_1[trial]
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        q_s2 = q_stage2_mf[state_idx]
        logits_2 = beta * q_s2
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        chosen_a2 = action_2[trial]
        
        # Stage 1 Update (TD)
        delta_stage1 = q_stage2_mf[state_idx, chosen_a2] - q_stage1_mf[action_1[trial]]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, chosen_a2]
        
        q_stage1_mf[action_1[trial]] += learning_rate * (delta_stage1 + lambda_coef * delta_stage2)
        
        # Stage 2 Update (Direct)
        q_stage2_mf[state_idx, chosen_a2] += learning_rate * delta_stage2
        
        # Coupled Update (Cross-State)
        # Simulate an update for the same alien in the OTHER state (1 - state_idx)
        # We assume the 'reward' would have been the same, driving values together.
        other_state = 1 - state_idx
        delta_coupled = reward[trial] - q_stage2_mf[other_state, chosen_a2]
        q_stage2_mf[other_state, chosen_a2] += learning_rate * coupling_factor * delta_coupled

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Rare Transition Discounting
This model captures the intuition that participants may discount the outcome of a trial if it occurred after a "rare" transition (e.g., choosing Spaceship A but landing on Planet Y). The parameter `rare_factor` scales the eligibility trace (`lambda`), effectively reducing the credit assignment to the Stage 1 choice when the transition was unexpected.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Rare Transition Discounting Model.
    
    This model modulates the eligibility trace (lambda) based on whether the 
    transition was common or rare. If the transition was rare, the impact of 
    the Stage 2 reward on the Stage 1 choice value is scaled by 'rare_factor'.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] MB/MF weight.
    - lambda_coef: [0, 1] Baseline eligibility trace decay.
    - stickiness: [0, 10] Stage 1 stickiness.
    - rare_factor: [0, 1] Multiplier for lambda during rare transitions (0 = ignore, 1 = same as common).
    """
    learning_rate, beta, w, lambda_coef, stickiness, rare_factor = model_parameters
    n_trials = len(action_1)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    last_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_s1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net_s1
        if last_action_1 != -1:
            logits_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_action_1 = action_1[trial]
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        logits_2 = beta * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        # Determine if transition was common
        # Action 0 -> State 0 (Common), Action 1 -> State 1 (Common)
        is_common = (action_1[trial] == 0 and state_idx == 0) or \
                    (action_1[trial] == 1 and state_idx == 1)
        
        # Modulate lambda based on transition type
        current_lambda = lambda_coef if is_common else lambda_coef * rare_factor

        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        q_stage1_mf[action_1[trial]] += learning_rate * (delta_stage1 + current_lambda * delta_stage2)
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Subjective Probability Distortion
This model incorporates concepts from Prospect Theory, suggesting that the participant may not use the objective transition probabilities (0.7/0.3) for Model-Based planning. Instead, they use a distorted version of the transition matrix, governed by `trans_gamma`.
- If `gamma < 1`, probabilities are compressed towards 0.5 (underweighting certainty).
- If `gamma > 1`, probabilities are pushed towards 0 and 1 (overweighting certainty).

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Subjective Probability Distortion Model.
    
    The Model-Based component uses a distorted transition matrix T^gamma (normalized),
    reflecting subjective weighting of transition probabilities.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] MB/MF weight.
    - lambda_coef: [0, 1] Eligibility trace.
    - stickiness: [0, 10] Perseverance.
    - trans_gamma: [0, 5] Probability distortion exponent. 
      (1=Objective, <1=Underweighting, >1=Overweighting).
    """
    learning_rate, beta, w, lambda_coef, stickiness, trans_gamma = model_parameters
    n_trials = len(action_1)
  
    # Objective transition matrix
    T_obj = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Apply distortion
    T_distorted = np.power(T_obj, trans_gamma)
    # Normalize rows to sum to 1
    row_sums = np.sum(T_distorted, axis=1, keepdims=True)
    T_subjective = T_distorted / row_sums

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    last_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        
        # Use subjective matrix for planning
        q_stage1_mb = T_subjective @ max_q_stage2
        
        q_net_s1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net_s1
        if last_action_1 != -1:
            logits_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_action_1 = action_1[trial]
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        logits_2 = beta * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        q_stage1_mf[action_1[trial]] += learning_rate * (delta_stage1 + lambda_coef * delta_stage2)
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```