Here are three cognitive models designed to explain the participant's behavior in the two-step task.

### Model 1: Hybrid Model with Dynamic Transition Learning
This model hypothesizes that the participant does not assume a fixed transition matrix (e.g., 70/30) but learns the transition probabilities between spaceships and planets over time. This allows the model-based component to adapt if the participant perceives the transition structure as volatile or different from instructions.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Dynamic Transition Learning.
    
    The agent updates the transition probabilities (Model-Based component) based on 
    observed transitions, rather than assuming a fixed transition matrix.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for value updates (MF).
    - trans_lr: [0, 1] Learning rate for transition probability updates.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - w: [0, 1] Weight for MB component (1=MB, 0=MF).
    - stickiness: [0, 5] Perseverance bonus for repeated Stage 1 choices.
    """
    learning_rate, trans_lr, beta_1, beta_2, w, stickiness = model_parameters
    n_trials = len(action_1)
    
    # Initialize Q-values
    q_mf1 = np.zeros(2)
    q_mf2 = np.zeros((2, 2))
    
    # Initialize belief about the "common" transition probability
    # Starts at 0.7 as per task instructions
    p_common = 0.7
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1
    
    for t in range(n_trials):
        a1 = action_1[t]
        s = state[t]
        a2 = action_2[t]
        r = reward[t]
        
        # Handle missing data
        if a1 == -1 or a2 == -1 or s == -1:
            p_choice_1[t] = 0.5
            p_choice_2[t] = 0.5
            continue
            
        # --- Stage 1 Choice ---
        # Model-Based Values
        v_stage2 = np.max(q_mf2, axis=1) # Max Q over aliens for each planet
        
        # Q_MB calculation using dynamic p_common
        # Action 0 (A) -> commonly State 0 (X)
        q_mb1_0 = p_common * v_stage2[0] + (1 - p_common) * v_stage2[1]
        # Action 1 (U) -> commonly State 1 (Y)
        q_mb1_1 = (1 - p_common) * v_stage2[0] + p_common * v_stage2[1]
        
        q_mb1 = np.array([q_mb1_0, q_mb1_1])
        
        # Net Q-values: Hybrid mixture
        q_net1 = w * q_mb1 + (1 - w) * q_mf1
        
        # Add stickiness to the previously chosen action
        if prev_a1 != -1:
            q_net1[prev_a1] += stickiness
            
        # Softmax Stage 1
        exp_q1 = np.exp(beta_1 * q_net1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[t] = probs_1[a1]
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta_2 * q_mf2[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[t] = probs_2[a2]
        
        # --- Updates ---
        # 1. Update Transition Probability
        # Determine if transition was "common" (A->X or U->Y)
        is_common = 1.0 if a1 == s else 0.0
        p_common += trans_lr * (is_common - p_common)
        
        # 2. Update Stage 2 Q-values (MF)
        delta2 = r - q_mf2[s, a2]
        q_mf2[s, a2] += learning_rate * delta2
        
        # 3. Update Stage 1 Q-values (MF) via SARSA(0)
        delta1 = q_mf2[s, a2] - q_mf1[a1]
        q_mf1[a1] += learning_rate * delta1
        
        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model with Loss Aversion
This model incorporates a specific sensitivity to negative rewards (-1). The participant may weigh losses differently than gains (0 or 1). This is implemented by scaling negative rewards by a `loss_mult` parameter before updating Q-values.

```python
def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid Model with Loss Aversion.
    
    Standard Hybrid model but with a specific multiplier for negative rewards.
    This captures if the participant is more sensitive to losses (-1 coins) 
    than gains, or treats them differently.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for Q-values.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - w: [0, 1] Weight for MB component.
    - loss_mult: [0, 10] Multiplier for negative rewards. >1 implies aversion.
    - stickiness: [0, 5] Perseverance bonus.
    """
    learning_rate, beta_1, beta_2, w, loss_mult, stickiness = model_parameters
    n_trials = len(action_1)
    
    q_mf1 = np.zeros(2)
    q_mf2 = np.zeros((2, 2))
    
    # Fixed transition matrix for standard MB
    trans_mat = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1
    
    for t in range(n_trials):
        a1 = action_1[t]
        s = state[t]
        a2 = action_2[t]
        r = reward[t]
        
        if a1 == -1 or a2 == -1 or s == -1:
            p_choice_1[t] = 0.5
            p_choice_2[t] = 0.5
            continue
            
        # MB Calculation
        v_stage2 = np.max(q_mf2, axis=1)
        q_mb1 = trans_mat @ v_stage2
        
        # Net Q
        q_net1 = w * q_mb1 + (1 - w) * q_mf1
        if prev_a1 != -1:
            q_net1[prev_a1] += stickiness
            
        exp_q1 = np.exp(beta_1 * q_net1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[t] = probs_1[a1]
        
        exp_q2 = np.exp(beta_2 * q_mf2[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[t] = probs_2[a2]
        
        # Reward Scaling for Loss Aversion
        r_eff = r
        if r < 0:
            r_eff = r * loss_mult
            
        # Updates
        delta2 = r_eff - q_mf2[s, a2]
        q_mf2[s, a2] += learning_rate * delta2
        
        delta1 = q_mf2[s, a2] - q_mf1[a1]
        q_mf1[a1] += learning_rate * delta1
        
        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: TD-Lambda Model with Decaying Choice Trace
This model extends the Model-Free TD-Lambda approach by implementing a "decaying choice trace" for stickiness. Instead of just repeating the very last choice, the agent builds up a habit strength (`choice_trace`) that accumulates with repeated choices and decays slowly, capturing longer-term perseverance or habit formation.

```python
def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    TD-Lambda Model with Decaying Choice Trace.
    
    Instead of a simple 1-trial stickiness, this model maintains a 'trace' of 
    past choices that decays over time. This captures longer-term perseverance 
    or habits.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for Q-values.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - lambd: [0, 1] Eligibility trace for credit assignment.
    - stick_weight: [0, 5] Weight of the choice trace in decision.
    - stick_decay: [0, 1] Decay rate of the choice trace (0=instant forgetting, 1=no decay).
    """
    learning_rate, beta_1, beta_2, lambd, stick_weight, stick_decay = model_parameters
    n_trials = len(action_1)
    
    q_mf1 = np.zeros(2)
    q_mf2 = np.zeros((2, 2))
    
    # Trace of past choices for Stage 1
    choice_trace = np.zeros(2)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for t in range(n_trials):
        a1 = action_1[t]
        s = state[t]
        a2 = action_2[t]
        r = reward[t]
        
        if a1 == -1 or a2 == -1 or s == -1:
            p_choice_1[t] = 0.5
            p_choice_2[t] = 0.5
            continue
            
        # Stage 1 Choice
        # Q_net includes the accumulated choice trace
        q_net1 = q_mf1 + stick_weight * choice_trace
        
        exp_q1 = np.exp(beta_1 * q_net1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[t] = probs_1[a1]
        
        # Update Choice Trace
        # Decay existing trace
        choice_trace *= stick_decay
        # Add current choice
        choice_trace[a1] += 1.0
        
        # Stage 2 Choice
        exp_q2 = np.exp(beta_2 * q_mf2[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[t] = probs_2[a2]
        
        # TD Updates
        # Stage 2 PE
        delta2 = r - q_mf2[s, a2]
        q_mf2[s, a2] += learning_rate * delta2
        
        # Stage 1 PE
        delta1 = q_mf2[s, a2] - q_mf1[a1]
        q_mf1[a1] += learning_rate * delta1
        
        # Eligibility Trace Update (TD-Lambda)
        # Update Stage 1 Q-value with Stage 2 PE scaled by lambda
        q_mf1[a1] += learning_rate * lambd * delta2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```