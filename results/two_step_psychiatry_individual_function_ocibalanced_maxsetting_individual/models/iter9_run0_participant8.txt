Here are the 3 new cognitive models as Python functions.

```python
def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Stage-Dependent Decay Model.
    
    This model assumes that the participant forgets (decays) the values of unchosen 
    options at different rates for Stage 1 (Spaceships) and Stage 2 (Aliens/Planets).
    This accounts for the difference between the stable transition structure of 
    spaceships and the drifting reward probabilities of aliens.
    
    Parameters:
    lr: [0,1] - Learning rate for value updates.
    decay_s1: [0,1] - Decay rate for unchosen Stage 1 options (Spaceships).
    decay_s2: [0,1] - Decay rate for unchosen Stage 2 options (Aliens).
    beta: [0,10] - Inverse temperature for softmax choice policy.
    w: [0,1] - Weighting parameter (0 = Pure MF, 1 = Pure MB).
    stickiness: [0,5] - Choice perseveration bonus for Stage 1.
    """
    lr, decay_s1, decay_s2, beta, w, stickiness = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_s1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if prev_a1 != -1:
            q_net_s1[prev_a1] += stickiness
            
        exp_q1 = np.exp(beta * q_net_s1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        chosen_a1 = action_1[trial]
        prev_a1 = chosen_a1
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        chosen_a2 = action_2[trial]
        r = reward[trial]

        # --- Updates ---
        # Stage 1 MF Update (TD(0) - no lambda in this model, relying on W and Decay)
        delta_stage1 = q_stage2_mf[state_idx, chosen_a2] - q_stage1_mf[chosen_a1]
        q_stage1_mf[chosen_a1] += lr * delta_stage1
        
        # Stage 2 MF Update
        delta_stage2 = r - q_stage2_mf[state_idx, chosen_a2]
        q_stage2_mf[state_idx, chosen_a2] += lr * delta_stage2

        # --- Decay ---
        # Decay unchosen Stage 1 option
        q_stage1_mf[1 - chosen_a1] *= (1 - decay_s1)
        
        # Decay unchosen Stage 2 options (all options not visited/chosen this trial)
        for s in range(2):
            for a in range(2):
                if not (s == state_idx and a == chosen_a2):
                    q_stage2_mf[s, a] *= (1 - decay_s2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Outcome-Dependent Eligibility Trace Model.
    
    This model allows the strength of the eligibility trace (lambda) to vary 
    depending on whether the trial resulted in a reward or not. This tests the 
    hypothesis that the participant may credit their Stage 1 choice differently 
    after a success (reinforcement) vs a failure (omission).
    
    Parameters:
    lr: [0,1] - Learning rate.
    beta: [0,10] - Inverse temperature.
    w: [0,1] - Weighting parameter (0 = Pure MF, 1 = Pure MB).
    lam_rew: [0,1] - Eligibility trace strength when Reward = 1.
    lam_unrew: [0,1] - Eligibility trace strength when Reward = 0.
    stickiness: [0,5] - Choice perseveration bonus for Stage 1.
    """
    lr, beta, w, lam_rew, lam_unrew, stickiness = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_s1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if prev_a1 != -1:
            q_net_s1[prev_a1] += stickiness
            
        exp_q1 = np.exp(beta * q_net_s1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        chosen_a1 = action_1[trial]
        prev_a1 = chosen_a1
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        chosen_a2 = action_2[trial]
        r = reward[trial]

        # --- Updates ---
        # Stage 1 MF Update
        delta_stage1 = q_stage2_mf[state_idx, chosen_a2] - q_stage1_mf[chosen_a1]
        q_stage1_mf[chosen_a1] += lr * delta_stage1
        
        # Stage 2 MF Update
        delta_stage2 = r - q_stage2_mf[state_idx, chosen_a2]
        q_stage2_mf[state_idx, chosen_a2] += lr * delta_stage2
        
        # Eligibility Trace Update (Stage 1 updated by Stage 2 RPE)
        # Select lambda based on outcome
        current_lam = lam_rew if r == 1 else lam_unrew
        q_stage1_mf[chosen_a1] += lr * current_lam * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Dual-Beta Hybrid Model with Decay and Eligibility Traces.
    
    A comprehensive model that includes separate exploration parameters (beta) 
    for Stage 1 and Stage 2, allowing for different levels of decision noise 
    in strategic (spaceship) vs tactical (alien) choices. Also includes decay 
    for handling drifting rewards and eligibility traces for credit assignment.
    
    Parameters:
    lr: [0,1] - Learning rate.
    decay: [0,1] - Decay rate for unchosen options.
    beta_1: [0,10] - Inverse temperature for Stage 1.
    beta_2: [0,10] - Inverse temperature for Stage 2.
    w: [0,1] - Weighting parameter (0 = Pure MF, 1 = Pure MB).
    lam: [0,1] - Eligibility trace parameter.
    stickiness: [0,5] - Choice perseveration bonus for Stage 1.
    """
    lr, decay, beta_1, beta_2, w, lam, stickiness = model_parameters
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Policy (using beta_1) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_s1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if prev_a1 != -1:
            q_net_s1[prev_a1] += stickiness
            
        exp_q1 = np.exp(beta_1 * q_net_s1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        chosen_a1 = action_1[trial]
        prev_a1 = chosen_a1
        state_idx = state[trial]

        # --- Stage 2 Policy (using beta_2) ---
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        chosen_a2 = action_2[trial]
        r = reward[trial]

        # --- Updates ---
        # Stage 1 MF Update
        delta_stage1 = q_stage2_mf[state_idx, chosen_a2] - q_stage1_mf[chosen_a1]
        q_stage1_mf[chosen_a1] += lr * delta_stage1
        
        # Stage 2 MF Update
        delta_stage2 = r - q_stage2_mf[state_idx, chosen_a2]
        q_stage2_mf[state_idx, chosen_a2] += lr * delta_stage2
        
        # Eligibility Trace
        q_stage1_mf[chosen_a1] += lr * lam * delta_stage2
        
        # --- Decay ---
        # Decay unchosen options
        q_stage1_mf[1 - chosen_a1] *= (1 - decay)
        for s in range(2):
            for a in range(2):
                if not (s == state_idx and a == chosen_a2):
                    q_stage2_mf[s, a] *= (1 - decay)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```