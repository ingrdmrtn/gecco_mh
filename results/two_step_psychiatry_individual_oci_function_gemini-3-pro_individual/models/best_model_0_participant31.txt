def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: Hybrid Model with OCI-Modulated Choice Kernel (Habit).
    
    This model combines Model-Based and Model-Free learning with a "Choice Kernel"
    representing habit strength. The influence of this habit is modulated by the OCI score.
    Unlike simple stickiness (which typically repeats only the very last choice), 
    the kernel integrates the history of past choices.
    
    Hypothesis: Higher OCI scores correlate with a stronger reliance on habitual patterns 
    (Choice Kernel) over value-based calculations.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for Q-values and Choice Kernel.
    - beta: [0, 10] Inverse temperature for softmax.
    - w: [0, 1] Weighting parameter (0 = Pure MF, 1 = Pure MB).
    - ck_weight_oci: [0, 10] Scaling factor for how much OCI enhances the Choice Kernel's influence.
      (Total weight = ck_weight_oci * OCI)
    """
    learning_rate, beta, w, ck_weight_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]



    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_mf = np.zeros((2, 2)) # Stage 2 values: [State, Action]
    q_mb = np.zeros(2)      # Stage 1 MB values
    q_stage1_mf = np.zeros(2) # Stage 1 MF values

    choice_kernel = np.zeros(2)
    
    for trial in range(n_trials):


        max_q_stage2 = np.max(q_mf, axis=1)
        q_mb = transition_matrix @ max_q_stage2

        q_net = w * q_mb + (1 - w) * q_stage1_mf

        habit_bonus = (ck_weight_oci * oci_score) * choice_kernel
        q_net += habit_bonus

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]

        chosen_a1 = int(action_1[trial])
        choice_kernel[chosen_a1] += learning_rate * (1 - choice_kernel[chosen_a1])
        choice_kernel[1 - chosen_a1] += learning_rate * (0 - choice_kernel[1 - chosen_a1])

        state_idx = int(state[trial])
        
        q_s2 = q_mf[state_idx]
        exp_q2 = np.exp(beta * q_s2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]


        delta_stage1 = q_mf[state_idx, int(action_2[trial])] - q_stage1_mf[chosen_a1]
        q_stage1_mf[chosen_a1] += learning_rate * delta_stage1

        delta_stage2 = reward[trial] - q_mf[state_idx, int(action_2[trial])]
        q_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss