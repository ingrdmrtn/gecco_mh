def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free RL with OCI-modulated choice perseveration (stickiness).
    
    This model adds a 'stickiness' bonus to the previously chosen action. 
    The magnitude of this stickiness is scaled by the OCI score.
    Higher OCI might lead to higher repetition of choices (compulsivity),
    regardless of value.

    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - stickiness_base: [0, 5] Base tendency to repeat the previous choice.
    - oci_stick_mod: [0, 5] How much OCI amplifies the stickiness.
    """
    learning_rate, beta, stickiness_base, oci_stick_mod = model_parameters
    n_trials = len(action_1)
    oci_val = oci[0]


    effective_stickiness = stickiness_base + (oci_val * oci_stick_mod)

    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1 # No previous action on trial 0

    for trial in range(n_trials):


        q_stage1_modified = q_stage1.copy()
        if last_action_1 != -1:
            q_stage1_modified[last_action_1] += effective_stickiness
            
        exp_q1 = np.exp(beta * q_stage1_modified)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        s1_choice = action_1[trial]
        last_action_1 = s1_choice
        s2_state = state[trial]


        exp_q2 = np.exp(beta * q_stage2[s2_state])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        s2_choice = action_2[trial]
        r = reward[trial]


        delta_stage2 = r - q_stage2[s2_state, s2_choice]
        q_stage2[s2_state, s2_choice] += learning_rate * delta_stage2


        delta_stage1 = q_stage2[s2_state, s2_choice] - q_stage1[s1_choice]
        q_stage1[s1_choice] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss