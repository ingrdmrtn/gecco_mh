Here are three cognitive models designed to explain the participant's behavior, incorporating their high OCI-R score (0.65) into the decision-making logic.

### Model 1: The "Habitual Anxiety" Model (MB/MF Trade-off modulated by OCI)
This model posits that individuals with high OCI scores rely more heavily on Model-Free (habitual) learning rather than Model-Based (goal-directed) planning, especially under uncertainty. The `oci` score directly modulates the mixing weight (`w`) between the model-based and model-free values in the first stage. Higher OCI scores push the agent towards pure model-free behavior (compulsivity/habit).

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: Habitual Anxiety Model.
    
    Hypothesis: High OCI scores reduce Model-Based control, increasing reliance on 
    Model-Free (habitual) strategies. The mixing parameter 'w' is dynamically 
    adjusted by the participant's OCI score.
    
    Parameters:
    learning_rate: [0,1] Rate at which Q-values are updated.
    beta: [0,10] Inverse temperature for softmax choice.
    w_base: [0,1] Baseline weighting for Model-Based control (0=MF, 1=MB).
    """
    learning_rate, beta, w_base = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0] # High OCI (0.65)
    
    # Transition matrix: A(0)->X(0) (0.7), U(1)->Y(1) (0.7)
    # Note: The template implies state 0 is planet 0, state 1 is planet 1.
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)       # Model-free values for spaceships
    q_stage2_mf = np.zeros((2, 2))  # Model-free values for aliens (state x alien)

    # Modulate mixing weight w based on OCI.
    # Higher OCI -> Lower w (more MF/Habit). 
    # We clamp w to be between 0 and 1.
    # If OCI is high, w decreases.
    w = w_base * (1.0 - oci_score) 
    w = np.clip(w, 0.0, 1.0)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based Value: Transition prob * Max Stage 2 Value
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Value: weighted sum of MB and MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        # Store probability of chosen action
        # Ensure index is integer
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        state_idx = int(state[trial])
        
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        r = reward[trial]
        
        # Stage 2 Update (TD)
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 Update (TD - Model Free)
        # Standard SARSA-like update using the value of the state actually reached
        # Value of state reached is the value of the action taken there (or max, here we use chosen)
        # Actually, standard Two-Step MF often uses the value of the chosen stage 2 option.
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: The "Perseveration" Model (Stickiness modulated by OCI)
This model hypothesizes that high OCI scores manifest as "stickiness" or perseverationâ€”a compulsion to repeat previous actions regardless of reward outcomes. The OCI score scales a choice autocorrelation parameter (`stickiness`), making the agent more likely to repeat the previous Stage 1 choice.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: Perseveration Model.
    
    Hypothesis: High OCI relates to behavioral rigidity. The OCI score acts as a 
    gain on a 'stickiness' parameter, increasing the probability of repeating the 
    previous Stage 1 choice regardless of the outcome.
    
    Parameters:
    learning_rate: [0,1]
    beta: [0,10]
    stickiness_base: [0,5] Base tendency to repeat choices.
    """
    learning_rate, beta, stickiness_base = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Calculate effective stickiness. 
    # High OCI amplifies the base stickiness.
    # We add 1.0 to OCI to make it a multiplier >= 1.
    effective_stickiness = stickiness_base * (1.0 + oci_score)

    prev_action_1 = -1 # Initialize with no previous action

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        # Pure Model-Free logic for value, but added stickiness bonus
        q_net = q_stage1_mf.copy()
        
        # Add stickiness bonus to the previously chosen action
        if prev_action_1 != -1:
            q_net[prev_action_1] += effective_stickiness
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # Update previous action for next trial
        prev_action_1 = a1
        
        # --- Stage 2 Policy ---
        state_idx = int(state[trial])
        
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        r = reward[trial]
        
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 Update (Simple Model Free TD(1))
        # Updating based on the final reward received
        delta_stage1 = r - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: The "Hyper-Learning" Model (Learning Rate modulated by OCI)
This model suggests that OCI is linked to an inability to ignore error signals (hyper-responsivity to prediction errors). High OCI participants might update their beliefs too drastically after every outcome (high learning rate), treating noise as signal. The OCI score acts as a floor or booster for the learning rate.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: Hyper-Learning Model.
    
    Hypothesis: OCI involves an over-sensitivity to prediction errors. 
    The learning rate is boosted by the OCI score, causing the agent to 
    update values more aggressively (volatility).
    
    Parameters:
    alpha_base: [0,1] Base learning rate.
    beta: [0,10] Inverse temperature.
    w: [0,1] Mixing weight (MB vs MF).
    """
    alpha_base, beta, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Effective Learning Rate
    # The OCI score increases the learning rate.
    # We blend the base alpha towards 1.0 based on OCI intensity.
    # Formula: alpha + (1-alpha) * OCI * scaling_factor
    # Simplified: alpha is boosted by OCI.
    effective_alpha = alpha_base + (1.0 - alpha_base) * oci_score * 0.5
    effective_alpha = np.clip(effective_alpha, 0.0, 1.0)

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        state_idx = int(state[trial])
        
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        r = reward[trial]
        
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += effective_alpha * delta_stage2
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += effective_alpha * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```