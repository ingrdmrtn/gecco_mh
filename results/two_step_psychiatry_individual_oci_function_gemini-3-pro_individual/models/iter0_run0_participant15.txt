Here are three cognitive models designed to explain the participant's behavior, incorporating the OCI-R score to modulate specific mechanisms.

### Model 1: Hybrid Learner with OCI-modulated Model-Based Weight (`w`)
This model assumes that individuals use a hybrid of Model-Free (MF) and Model-Based (MB) reinforcement learning. The core hypothesis here is that the trade-off parameter `w` (which balances MB and MF control) is influenced by the OCI score. While high compulsivity is often associated with rigid habits (MF), low OCI scores (like this participant's 0.075) might suggest a more flexible, goal-directed (MB) approach, or simply a baseline balance.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free learner where OCI score modulates the mixing weight 'w'.
    
    Hypothesis: The balance between goal-directed (MB) and habitual (MF) control is
    influenced by obsessive-compulsive traits.
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    w_base: [0, 1] (Base weight for Model-Based control)
    w_oci_slope: [-1, 1] (How strongly OCI affects w)
    """
    learning_rate, beta, w_base, w_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    # Transition matrix: A->X (0->0) is 0.7, A->Y (0->1) is 0.3, etc.
    # Assuming state 0 is X, state 1 is Y.
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)       # Model-free values for stage 1 (Spaceships)
    q_stage2_mf = np.zeros((2, 2))  # Model-free values for stage 2 (Aliens per Planet)

    # Calculate the mixing weight w based on OCI
    # Sigmoid transform to keep w within [0, 1]
    # w = 1 means pure Model-Based, w = 0 means pure Model-Free
    raw_w = w_base + w_oci_slope * oci_score
    w = 1.0 / (1.0 + np.exp(-raw_w)) 

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based valuation: V(state) = max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid valuation
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial]) # The planet arrived at

        # --- Stage 2 Policy ---
        # Standard Softmax on Stage 2 Q-values
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        # --- Learning / Updating ---
        
        # Prediction errors
        # Stage 1 PE (SARSA-style using the value of the state actually reached)
        # Note: Standard Daw et al. uses the max of next stage or the chosen value. 
        # Here we use the value of the chosen stage 2 action as the target.
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        
        # Update Stage 1 MF
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1
        
        # Stage 2 PE
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        
        # Update Stage 2 MF
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2
        
        # Note: We assume the transition matrix is fixed/known and does not update in this simplified model.

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-modulated Learning Rate Asymmetry
This model focuses on how OCI scores might affect sensitivity to rewards versus punishments (or lack of reward). A very low OCI score might indicate a different responsiveness to positive feedback compared to negative feedback. This model splits the learning rate into positive (`alpha_pos`) and negative (`alpha_neg`) components, where the balance is shifted by the OCI score.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model with Asymmetric Learning Rates modulated by OCI.
    
    Hypothesis: OCI scores influence the bias between learning from positive prediction errors
    (rewards) versus negative prediction errors (omissions).
    
    Bounds:
    lr_base: [0, 1]
    lr_bias_oci: [-1, 1] (Shifts bias towards pos or neg learning based on OCI)
    beta: [0, 10]
    eligibility_trace: [0, 1] (lambda parameter for stage 1 update)
    """
    lr_base, lr_bias_oci, beta, eligibility_trace = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    # Calculate specific learning rates
    # If lr_bias_oci is positive, high OCI increases learning from positive PE.
    # If negative, high OCI decreases it.
    # We use a tanh to keep the modifier bounded.
    modifier = np.tanh(lr_bias_oci * oci_score)
    
    # Ensure LRs stay in [0, 1] approx
    alpha_pos = np.clip(lr_base * (1 + modifier), 0, 1)
    alpha_neg = np.clip(lr_base * (1 - modifier), 0, 1)

    for trial in range(n_trials):

        # --- Stage 1 Policy (Pure Model-Free for simplicity in this variant) ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        # --- Updating ---
        
        # Stage 2 PE
        delta_stage2 = reward[trial] - q_stage2[state_idx, int(action_2[trial])]
        
        # Select learning rate based on sign of PE
        lr_2 = alpha_pos if delta_stage2 >= 0 else alpha_neg
        
        # Update Stage 2
        q_stage2[state_idx, int(action_2[trial])] += lr_2 * delta_stage2
        
        # Stage 1 Update (TD(lambda) / Eligibility Trace approach)
        # The update for Stage 1 is driven by Stage 2 value + Stage 2 PE * lambda
        # This effectively allows the reward at stage 2 to directly influence stage 1
        
        # Standard TD error for stage 1
        delta_stage1 = q_stage2[state_idx, int(action_2[trial])] - q_stage1[int(action_1[trial])]
        
        # Combined update for Stage 1:
        # V_new = V_old + alpha * (delta_1 + lambda * delta_2)
        
        lr_1 = alpha_pos if (delta_stage1 + eligibility_trace * delta_stage2) >= 0 else alpha_neg
        
        q_stage1[int(action_1[trial])] += lr_1 * (delta_stage1 + eligibility_trace * delta_stage2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-modulated Choice Stickiness (Perseveration)
This model investigates whether the tendency to repeat the previous choice (perseveration or "stickiness") is modulated by the OCI score. Compulsivity is often linked to repetitive behaviors. Even with a low score, modeling the *relationship* allows the data to determine if this specific participant (low OCI) shows lower stickiness than a theoretical high-OCI baseline.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model with Choice Stickiness (Perseveration) modulated by OCI.
    
    Hypothesis: The tendency to repeat the previous choice regardless of reward 
    (stickiness) is a function of OCI traits.
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    stickiness_base: [0, 5] (Base tendency to repeat choice)
    stickiness_oci_mod: [-5, 5] (How OCI amplifies or reduces stickiness)
    """
    learning_rate, beta, stickiness_base, stickiness_oci_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    # Calculate effective stickiness parameter
    # A positive value means they like to repeat the last action.
    eff_stickiness = stickiness_base + (stickiness_oci_mod * oci_score)

    last_action_1 = -1 # Initialize as -1 (no previous action)

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        # Add stickiness bonus to Q-values before softmax
        q_stage1_modified = q_stage1.copy()
        
        if last_action_1 != -1:
            q_stage1_modified[int(last_action_1)] += eff_stickiness

        exp_q1 = np.exp(beta * q_stage1_modified)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # Record this action for next trial's stickiness
        last_action_1 = action_1[trial]
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        # (Stickiness is usually modeled primarily on the first stage in this task, 
        # but could apply to stage 2. Here we apply only to stage 1 for simplicity).
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        # --- Value Updating (Simple Model-Free) ---
        
        delta_stage1 = q_stage2[state_idx, int(action_2[trial])] - q_stage1[int(action_1[trial])]
        q_stage1[int(action_1[trial])] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2[state_idx, int(action_2[trial])]
        q_stage2[state_idx, int(action_2[trial])] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```