Here are three cognitive models designed to explain the participant's behavior in the two-step task, incorporating their low OCI-R score (0.1125).

### Model 1: Hybrid Model with OCI-modulated Model-Based Weight
This model assumes that decision-making is a mix of Model-Based (MB) and Model-Free (MF) reinforcement learning. The core hypothesis here is that the trade-off between these two systems is influenced by the participant's OCI score. Since the participant has a low OCI score (indicative of low compulsivity), this model posits they might be more flexible and goal-directed (higher MB weight) compared to high-OCI individuals who might rely more on habitual (MF) control.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL model where the mixing weight (w) between Model-Based and Model-Free 
    values is modulated by the OCI score.
    
    Hypothesis: Lower OCI scores correlate with higher Model-Based control (higher w).
    
    Parameters:
    learning_rate: [0, 1] - Update rate for Q-values.
    beta: [0, 10] - Inverse temperature for softmax choice.
    w_base: [0, 1] - Baseline mixing weight (0=Pure MF, 1=Pure MB).
    oci_sensitivity: [0, 5] - How strongly OCI reduces MB control.
    """
    learning_rate, beta, w_base, oci_sensitivity = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0] # Extract scalar OCI
  
    # Fixed transition matrix for the task (Action 0 -> State 0 (70%), Action 1 -> State 1 (70%))
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)      # Model-free values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for stage 2 (State x Action)

    # Calculate effective mixing weight w. 
    # Logic: Higher OCI reduces w (pushes towards MF/habit). 
    # We clip to ensure it stays valid [0,1].
    w = w_base - (current_oci * oci_sensitivity)
    w = np.clip(w, 0.0, 1.0)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based value calculation: V(state) = max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid value: weighted sum of MB and MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax policy for stage 1
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # Observe state
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # --- Stage 2 Choice ---
        # Standard Model-Free Q-learning for the second stage
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning / Updating ---
        r = reward[trial]
        
        # Stage 2 Update (TD error)
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 Update (TD-lambda style, but simplified to standard MF here)
        # Using the value of the state actually reached
        v_state_reached = q_stage2_mf[state_idx, a2] # SARSA-like update for stage 1
        delta_stage1 = v_state_reached - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Driven Perseveration (Stickiness)
This model focuses on "stickiness" or choice perseveration (repeating the previous choice regardless of reward). OCI is often linked to repetitive behaviors. Even though this participant has a *low* OCI score, modeling the relationship allows us to see if their low score corresponds to low stickiness. This model adds a perseveration bonus to the previously chosen action, scaled by the OCI score.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free RL with OCI-modulated choice perseveration (stickiness).
    
    Hypothesis: OCI score scales the tendency to repeat the previous Stage 1 choice,
    regardless of outcome.
    
    Parameters:
    learning_rate: [0, 1]
    beta: [0, 10]
    stickiness_base: [0, 5] - General tendency to repeat choices.
    oci_sticky_factor: [0, 5] - How much OCI amplifies stickiness.
    """
    learning_rate, beta, stickiness_base, oci_sticky_factor = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    # Calculate effective stickiness
    # Low OCI -> Low stickiness bonus
    # High OCI -> High stickiness bonus
    perseveration_weight = stickiness_base + (current_oci * oci_sticky_factor)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1 # No previous action on trial 0

    for trial in range(n_trials):

        # --- Stage 1 Choice ---
        # Calculate values
        q_vals = q_stage1_mf.copy()
        
        # Add stickiness bonus if not the first trial
        if last_action_1 != -1:
            q_vals[last_action_1] += perseveration_weight

        # Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        # Note: This model uses a pure MF strategy for value, but adds stickiness
        # We ignore MB calculation here to isolate the stickiness effect
        
        exp_q1 = np.exp(beta * q_vals)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        state_idx = int(state[trial])

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        r = reward[trial]
        
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 Update (TD(0))
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Update history
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Learning Rate Asymmetry
This model investigates if the OCI score affects how participants learn from positive versus negative prediction errors. It posits that OCI might influence sensitivity to punishment (lack of reward) versus reward. Specifically, we define separate learning rates for positive and negative prediction errors, where the negative learning rate is scaled by the OCI score (reflecting potential anxiety or perfectionism often comorbid with OCI traits).

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free RL with asymmetric learning rates modulated by OCI.
    
    Hypothesis: OCI affects how strongly the participant updates values 
    following negative prediction errors (disappointments).
    
    Parameters:
    alpha_pos: [0, 1] - Learning rate for positive prediction errors (RPE > 0).
    alpha_neg_base: [0, 1] - Base learning rate for negative prediction errors (RPE < 0).
    beta: [0, 10] - Inverse temperature.
    oci_punish_mod: [0, 1] - Modulates alpha_neg based on OCI.
    """
    alpha_pos, alpha_neg_base, beta, oci_punish_mod = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
  
    # Calculate effective negative learning rate
    # We assume higher OCI might increase sensitivity to negative outcomes
    alpha_neg = alpha_neg_base + (current_oci * oci_punish_mod)
    alpha_neg = np.clip(alpha_neg, 0.0, 1.0) # Ensure bounds

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Simple Model-Free Q-values
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        state_idx = int(state[trial])

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
  
        # --- Updates with Asymmetric Learning Rates ---
        r = reward[trial]
        
        # Stage 2 Prediction Error
        delta_stage2 = r - q_stage2[state_idx, a2]
        
        # Apply specific learning rate based on sign of error
        lr_2 = alpha_pos if delta_stage2 >= 0 else alpha_neg
        q_stage2[state_idx, a2] += lr_2 * delta_stage2
        
        # Stage 1 Prediction Error
        # Using the value of the chosen stage 2 state as the target (SARSA-style)
        delta_stage1 = q_stage2[state_idx, a2] - q_stage1[a1]
        
        lr_1 = alpha_pos if delta_stage1 >= 0 else alpha_neg
        q_stage1[a1] += lr_1 * delta_stage1
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```