Here are three cognitive models designed to capture the behavior of a participant in the two-step task, incorporating their OCI score into the learning and decision-making process.

### Model 1: Hybrid Learner with OCI-modulated Model-Based Weight
This model assumes that the balance between Model-Based (planning) and Model-Free (habitual) control is influenced by the participant's OCI score. Specifically, it tests the hypothesis that even low OCI scores might slightly shift the balance towards habit (Model-Free) or planning (Model-Based). Since the participant has a low OCI, this model allows the data to determine if `w` (the mixing weight) is scaled by this trait.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free learner where the mixing weight 'w' is 
    modulated by the OCI score.
    
    Parameters:
    learning_rate: [0, 1] - Rate at which Q-values are updated.
    beta: [0, 10] - Inverse temperature for softmax choice.
    w_base: [0, 1] - Baseline weight for Model-Based control (1 = full MB, 0 = full MF).
    oci_sensitivity: [0, 5] - How strongly OCI affects the mixing weight 'w'.
    """
    learning_rate, beta, w_base, oci_sensitivity = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]
    
    # Calculate the effective mixing weight w.
    # We use a sigmoid-like transformation or simple clipping to keep w in [0,1].
    # Hypothesis: Higher OCI might reduce Model-Based control (more habitual).
    # Since this participant is low OCI, they might have a higher effective w.
    # We model w = w_base - (oci_sensitivity * participant_oci).
    w = w_base - (oci_sensitivity * participant_oci)
    w = np.clip(w, 0.0, 1.0)

    # Fixed transition matrix as described in the task (A->X common, U->Y common)
    # Row 0: Spaceship A (0), Row 1: Spaceship U (1)
    # Col 0: Planet X (0), Col 1: Planet Y (1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2) # MF values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # MF values for stage 2 (2 planets, 2 aliens)

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # Model-Based Value: V(s') = max_a Q(s', a)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Value: Q_net = w * Q_MB + (1-w) * Q_MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax Policy
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        # Store probability of the chosen action (0 or 1)
        # Input data is float, cast to int for indexing
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Transition ---
        s_prime = int(state[trial]) # Planet arrived at
        
        # --- Stage 2 Decision ---
        # Standard Model-Free Q-learning for the second stage
        exp_q2 = np.exp(beta * q_stage2_mf[s_prime])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning / Updates ---
        r = reward[trial]
        
        # Prediction errors
        # Stage 2 PE: r - Q(s', a2)
        delta_stage2 = r - q_stage2_mf[s_prime, a2]
        
        # Stage 1 PE: Q(s', a2) - Q(s, a1) (TD(0) update)
        # Note: Some models use max Q(s', a') instead of Q(s', a2) (SARSA vs Q-learning).
        # We use SARSA-style here (actual choice value) for consistency with standard 2-step models.
        delta_stage1 = q_stage2_mf[s_prime, a2] - q_stage1_mf[a1]
        
        # Update Q-values
        q_stage2_mf[s_prime, a2] += learning_rate * delta_stage2
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Driven Perseveration (Stickiness)
This model posits that OCI traits relate to behavioral rigidity or "stickiness" (perseveration). Even with a low score, the participant might exhibit a baseline tendency to repeat the last choice regardless of reward. This model modifies the choice policy to include a `stickiness` parameter which is modulated by the OCI score.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner with Choice Stickiness (Perseveration) modulated by OCI.
    
    Parameters:
    learning_rate: [0, 1]
    beta: [0, 10]
    stickiness_base: [0, 5] - Base tendency to repeat the previous choice.
    oci_stickiness_mod: [0, 5] - How much OCI amplifies stickiness.
    """
    learning_rate, beta, stickiness_base, oci_stickiness_mod = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]
    
    # Calculate effective stickiness
    # Hypothesis: Higher OCI leads to higher repetitive behavior.
    stickiness = stickiness_base + (oci_stickiness_mod * participant_oci)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Pure Model-Free Q-values for simplicity to isolate stickiness effect
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1 # No previous action on trial 0

    for trial in range(n_trials):
        
        # --- Stage 1 Decision ---
        # Calculate logits (values before exponentiation)
        logits_1 = beta * q_stage1_mf.copy()
        
        # Add stickiness bonus if not the first trial
        if last_action_1 != -1:
            logits_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # Store action for next trial's stickiness
        last_action_1 = a1
        
        s_prime = int(state[trial])
        
        # --- Stage 2 Decision ---
        # Standard softmax on Q-values
        exp_q2 = np.exp(beta * q_stage2_mf[s_prime])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        r = reward[trial]
        
        delta_stage2 = r - q_stage2_mf[s_prime, a2]
        delta_stage1 = q_stage2_mf[s_prime, a2] - q_stage1_mf[a1]
        
        q_stage2_mf[s_prime, a2] += learning_rate * delta_stage2
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Learning Rate Asymmetry
This model investigates if OCI scores affect how participants learn from positive versus negative prediction errors. It is often hypothesized that anxiety or compulsivity relates to altered sensitivity to punishment (or lack of reward). Here, we split the learning rate into `alpha_pos` and `alpha_neg`, where the balance is shifted by the OCI score.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Based learner where OCI modulates the asymmetry between learning from 
    positive vs negative prediction errors.
    
    Parameters:
    beta: [0, 10]
    alpha_base: [0, 1] - Base learning rate.
    asymmetry_param: [0, 1] - Bias towards positive (learning more from reward).
    oci_impact: [0, 5] - How much OCI distorts the learning rate based on error sign.
    """
    beta, alpha_base, asymmetry_param, oci_impact = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]
    
    # We define effective learning rates for positive (alpha_pos) and negative (alpha_neg) PEs.
    # OCI might increase sensitivity to negative outcomes (or lack of reward).
    # We use oci_impact to scale the negative learning rate relative to base.
    
    # Base split
    alpha_pos = np.clip(alpha_base + (asymmetry_param * 0.1), 0, 1)
    alpha_neg = np.clip(alpha_base - (asymmetry_param * 0.1), 0, 1)
    
    # OCI modification: Higher OCI -> Higher learning from negative PEs
    alpha_neg = np.clip(alpha_neg + (participant_oci * oci_impact * 0.5), 0, 1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Using a simplified MB approach where we learn Q-values for stage 2 and project back
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Decision (Pure Model-Based for this variation) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        s_prime = int(state[trial])
        
        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_prime])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        r = reward[trial]
        delta_stage2 = r - q_stage2_mf[s_prime, a2]
        
        # Apply asymmetric learning rates
        if delta_stage2 >= 0:
            q_stage2_mf[s_prime, a2] += alpha_pos * delta_stage2
        else:
            q_stage2_mf[s_prime, a2] += alpha_neg * delta_stage2
            
        # Note: In this pure MB formulation, we don't update a separate Stage 1 Q-table,
        # because choice 1 is derived entirely from the transition matrix and Stage 2 values.

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```