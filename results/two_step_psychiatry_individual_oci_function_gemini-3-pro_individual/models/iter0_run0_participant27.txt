Here are three cognitive models proposing different ways the OCI-R score (obsessive-compulsive symptoms) might influence decision-making in the two-step task.

### Model 1: OCI-Modulated Model-Based/Model-Free Balance (w)
This model hypothesizes that OCI scores affect the balance between goal-directed (Model-Based) and habitual (Model-Free) control. High OCI is often associated with rigid, habitual behaviors. Here, the `w` parameter (mixing weight) is modulated by the OCI score. A base `w` is modified such that higher OCI scores might push the participant towards more Model-Free (habitual) control or Model-Based (rigid rule-following) depending on the fit.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-modulated Model-Based/Model-Free weighting.
    Hypothesis: The balance between Model-Based (planning) and Model-Free (habit) systems 
    is influenced by the OCI score.
    
    Parameters:
    learning_rate: [0, 1] Rate at which values are updated.
    beta: [0, 10] Inverse temperature for softmax choice.
    w_base: [0, 1] Baseline weighting for Model-Based control (0=MF, 1=MB).
    w_oci_slope: [0, 1] Strength of OCI influence on the w parameter.
    """
    learning_rate, beta, w_base, w_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    # Calculate effective w based on OCI. We clip to ensure it stays in [0,1].
    # We assume OCI modulates the w parameter linearly.
    # If w_oci_slope is positive, higher OCI leads to higher w (more Model-Based).
    # If w_oci_slope is conceptually negative (handled by optimizer or interpretation), 
    # it would lead to more Model-Free. Here we model an additive effect.
    w = w_base + (w_oci_slope * (oci_score - 0.5)) # Center OCI around 0.5
    w = np.clip(w, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2) # Model-free values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for stage 2 (2 states, 2 actions)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based Value Calculation: V(S') = max_a Q(S', a)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Q-value: weighted sum of MB and MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # --- Stage 2 Policy ---
        # Standard softmax on Stage 2 Q-values
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Value Updating ---
        # Prediction errors
        # Stage 2 PE: Reward - Expectation
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        
        # Stage 1 PE: Value of state 2 - Value of action 1
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]

        # Update Stage 2 Q-values
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Update Stage 1 MF Q-values
        # Note: In full TD(lambda), there is an eligibility trace. 
        # Here we use a simplified update often used in these models.
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Choice Stickiness (Perseveration)
This model hypothesizes that OCI scores relate to "stickiness" or perseverationâ€”the tendency to repeat the previous choice regardless of reward history. A high OCI score might indicate a stronger compulsion to repeat actions (high stickiness). The `stickiness` parameter is dynamically adjusted by the OCI score.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-modulated Choice Stickiness.
    Hypothesis: OCI score influences the tendency to repeat the previous choice (perseveration),
    independent of reward learning.
    
    Parameters:
    learning_rate: [0, 1] Rate at which values are updated.
    beta: [0, 10] Inverse temperature for softmax choice.
    stickiness_base: [0, 5] Baseline tendency to repeat choice.
    stickiness_oci_mod: [0, 5] Additional stickiness scaling with OCI.
    """
    learning_rate, beta, stickiness_base, stickiness_oci_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective stickiness
    # Higher OCI -> Higher tendency to repeat previous action
    stickiness = stickiness_base + (stickiness_oci_mod * oci_score)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track previous choice for stickiness (initialize to -1 or handle first trial)
    prev_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Pure Model-Free with Stickiness
        # We define Q_net as Q_MF + Stickiness_Bonus
        q_net = q_stage1_mf.copy()
        
        if prev_action_1 != -1:
            q_net[prev_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        prev_action_1 = a1 # Update for next trial

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Value Updating ---
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Learning Rate (Rigidity)
This model hypothesizes that OCI impacts cognitive flexibility, represented here by the learning rate. Individuals with higher OCI scores might be more rigid in their beliefs, resulting in a lower learning rate (updating values more slowly in response to new evidence), or conversely, hyper-sensitive to error. This model scales the learning rate `alpha` based on the OCI score.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-modulated Learning Rate.
    Hypothesis: OCI score impacts the speed of updating value estimates.
    High OCI might lead to lower learning rates (rigidity/resistance to change).
    
    Parameters:
    alpha_base: [0, 1] Baseline learning rate.
    alpha_oci_factor: [0, 1] Factor to reduce/increase alpha based on OCI.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Model-based vs Model-free weight (fixed parameter for this model).
    """
    alpha_base, alpha_oci_factor, beta, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    # Calculate effective learning rate
    # We model this as: alpha = base * (1 - factor * OCI)
    # If factor is high and OCI is high, learning rate drops (rigidity).
    effective_alpha = alpha_base * (1.0 - (alpha_oci_factor * oci_score))
    effective_alpha = np.clip(effective_alpha, 0.01, 1.0) # Ensure it doesn't hit 0 or go negative

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid MB/MF choice
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Value Updating ---
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        
        # Use the OCI-modulated effective_alpha
        q_stage2_mf[state_idx, a2] += effective_alpha * delta_stage2
        q_stage1_mf[a1] += effective_alpha * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```