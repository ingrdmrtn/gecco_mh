Here are three cognitive models designed to explain the participant's behavior in the two-step task, incorporating the OCI-R score to modulate learning and decision-making processes.

### Model 1: Hybrid Learner with OCI-Modulated Mixing
This model assumes that decision-making is a mix of Model-Based (planning) and Model-Free (habitual) strategies. The core hypothesis here is that OCI symptoms influence the balance between these two systems. Specifically, the mixing parameter `w` is not static but is modulated by the participant's OCI score, testing the theory that obsessive-compulsive traits might bias an individual towards more habitual (Model-Free) or more goal-directed (Model-Based) control.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free learner where the balance (w) is modulated by OCI.
    
    This model posits that the trade-off between goal-directed planning (MB) and 
    habitual caching (MF) is influenced by the individual's OCI score.
    
    Parameters:
    learning_rate: [0,1] Rate at which Q-values are updated.
    beta_1: [0,10] Inverse temperature for Stage 1 choices.
    beta_2: [0,10] Inverse temperature for Stage 2 choices.
    w_base: [0,1] Base weighting parameter (0 = pure MF, 1 = pure MB).
    oci_sens: [0,1] Sensitivity of the weighting parameter to the OCI score.
    """
    learning_rate, beta_1, beta_2, w_base, oci_sens = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Modulate mixing weight w based on OCI. 
    # We clip to [0, 1] to ensure valid probability weighting.
    # Hypothesis: Higher OCI might shift balance (direction determined by fit).
    w = np.clip(w_base + (oci_sens * (oci_score - 0.5)), 0.0, 1.0)

    # Fixed transition matrix for the task (70% common, 30% rare)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values initialization
    q_stage1_mf = np.zeros(2) # Model-free values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for stage 2 (2 planets x 2 aliens)

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        
        # 1. Model-Based Value Calculation: V_MB = T * max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Hybrid Value Calculation: Q_net = w * Q_MB + (1-w) * Q_MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax for Stage 1
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Decision ---
        
        state_idx = int(state[trial]) # 0 or 1 (Planets)
        
        # Softmax for Stage 2 (purely model-free based on immediate reward history)
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning Updates ---
        
        # Prediction errors
        # Stage 2 PE: Difference between received reward and expected value of alien
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Stage 1 PE: Difference between Stage 2 value and Stage 1 expectation
        # Note: In standard TD(0), we use the value of the state arrived at.
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        # Update Stage 2 values
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Update Stage 1 MF values (using both PEs for eligibility trace effect, simplified)
        # Often modeled as Q1 += alpha * delta1 + alpha * lambda * delta2. 
        # Here we assume lambda=1 for simplicity in a 5-param limit model.
        q_stage1_mf[action_1[trial]] += learning_rate * (delta_stage1 + delta_stage2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Driven Perseveration
This model focuses on the "stickiness" or perseveration of choices. High OCI is often associated with repetitive behaviors or difficulty switching sets. This model introduces a perseveration parameter that biases the participant to repeat the previous Stage 1 choice, where the strength of this perseveration is directly scaled by the OCI score.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Based Learner with OCI-modulated Perseveration.
    
    This model assumes the participant is primarily Model-Based but suffers from
    choice "stickiness" (perseveration) which is scaled by their OCI score.
    
    Parameters:
    learning_rate: [0,1] Learning rate for Stage 2 values.
    beta: [0,10] Inverse temperature for both stages.
    persev_base: [0,5] Base tendency to repeat the previous Stage 1 choice.
    oci_persev_scale: [0,5] Additional perseveration added per unit of OCI.
    """
    learning_rate, beta, persev_base, oci_persev_scale = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective perseveration bonus
    # This value is added to the Q-value of the previously chosen action
    perseveration_bonus = persev_base + (oci_persev_scale * oci_score)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Only tracking Stage 2 values to compute MB values for Stage 1
    q_stage2_mf = np.zeros((2, 2)) 
    
    last_action_1 = -1 # Initialize with no previous action

    for trial in range(n_trials):
        
        # --- Stage 1 Decision (Model-Based + Perseveration) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Add perseveration bonus to the logits (not the Q-values themselves)
        logits_1 = beta * q_stage1_mb
        if last_action_1 != -1:
            logits_1[last_action_1] += perseveration_bonus
            
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Record action for next trial
        last_action_1 = action_1[trial]
        state_idx = int(state[trial])

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning ---
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Learning Rate Asymmetry
This model investigates if OCI scores correlate with how participants learn from positive versus negative outcomes. It implements separate learning rates for positive prediction errors (better than expected) and negative prediction errors (worse than expected), with the OCI score modulating the magnitude of the negative learning rate. This reflects the hypothesis that compulsive traits might be linked to hypersensitivity to negative feedback or errors.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free Learner with OCI-modulated Negative Learning Rate.
    
    This model tests if OCI affects sensitivity to negative outcomes.
    It uses separate learning rates for positive (alpha_pos) and negative (alpha_neg)
    prediction errors, where alpha_neg is scaled by the OCI score.
    
    Parameters:
    alpha_pos: [0,1] Learning rate for positive prediction errors.
    alpha_neg_base: [0,1] Base learning rate for negative prediction errors.
    oci_neg_mod: [0,1] Scaling factor for OCI's impact on negative learning rate.
    beta: [0,10] Inverse temperature.
    lambda_eligibility: [0,1] Eligibility trace decay (how much Stage 2 updates Stage 1).
    """
    alpha_pos, alpha_neg_base, oci_neg_mod, beta, lambda_eligibility = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective negative learning rate
    # We hypothesize OCI might increase sensitivity to negative feedback
    alpha_neg = np.clip(alpha_neg_base + (oci_neg_mod * oci_score), 0.0, 1.0)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Decision ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = int(state[trial])

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning Updates ---
        
        # Calculate Prediction Errors
        delta_stage2 = reward[trial] - q_stage2[state_idx, action_2[trial]]
        delta_stage1 = q_stage2[state_idx, action_2[trial]] - q_stage1[action_1[trial]]
        
        # Helper to select learning rate based on sign of PE
        def get_alpha(delta):
            return alpha_pos if delta >= 0 else alpha_neg

        # Update Stage 2
        lr_2 = get_alpha(delta_stage2)
        q_stage2[state_idx, action_2[trial]] += lr_2 * delta_stage2
        
        # Update Stage 1
        # TD(1)-like update: Stage 1 is updated by its own PE plus the Stage 2 PE (scaled by lambda)
        lr_1 = get_alpha(delta_stage1)
        q_stage1[action_1[trial]] += lr_1 * delta_stage1 + lambda_eligibility * lr_2 * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```