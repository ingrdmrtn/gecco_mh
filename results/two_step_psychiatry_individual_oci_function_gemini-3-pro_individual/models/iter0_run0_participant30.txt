Here are three cognitive models designed to explain the participant's behavior in the two-step task, incorporating their OCI-R score (0.35, medium symptoms) into the decision-making process.

### Model 1: Hybrid Learner with OCI-Modulated Mixing Weight
This model assumes that the balance between Model-Based (planning) and Model-Free (habitual) learning is directly influenced by the participant's OCI score. A common hypothesis in computational psychiatry is that compulsivity relates to an over-reliance on habitual (Model-Free) control. Here, the mixing parameter `w` is a function of the OCI score.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free learner where the trade-off parameter 'w' 
    is modulated by the OCI score.
    
    The model assumes that higher OCI scores might push the participant towards 
    more Model-Free (habitual) behavior or alter the balance.
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    w_base: [0, 1] (Base mixing weight)
    oci_sensitivity: [0, 5] (How strongly OCI affects the mixing weight)
    """
    learning_rate, beta, w_base, oci_sensitivity = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate mixing weight w based on OCI
    # We use a sigmoid-like transformation or simple linear clamping to keep w in [0,1].
    # Here: w = w_base - (oci_sensitivity * oci_score). 
    # Hypothesis: Higher OCI reduces Model-Based control (lower w).
    w = w_base - (oci_sensitivity * oci_score)
    if w < 0: w = 0
    if w > 1: w = 1

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)      # Model-free values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for stage 2 (State x Action)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based Value: Q_MB = Transition * max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value: Q_net = w * Q_MB + (1-w) * Q_MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Policy ---
        state_idx = state[trial]
        # Standard Softmax on Stage 2 Q-values
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning / Updating ---
        # Prediction error for Stage 1 (Model-Free only)
        # SARSA-style update often used in 2-step tasks for the MF component
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Prediction error for Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Note: In a pure hybrid model, we usually update the Q_stage2_mf which is used 
        # by both the MF system (directly) and the MB system (for planning).

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Perseveration Model with OCI-Driven Stickiness
This model posits that the participant is primarily a Model-Free learner, but their OCI score drives a "perseveration" or "stickiness" parameter. This reflects the repetitive nature of compulsive symptomsâ€”a tendency to repeat the previous action regardless of the outcome.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner with choice perseveration (stickiness).
    The strength of the stickiness is determined by the OCI score.
    
    Hypothesis: Higher OCI leads to higher choice repetition (stickiness).
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    stickiness_base: [0, 5]
    oci_boost: [0, 5]
    """
    learning_rate, beta, stickiness_base, oci_boost = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective stickiness
    # Stickiness increases with OCI score
    stickiness = stickiness_base + (oci_boost * oci_score)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2)) # State x Action
    
    last_action_1 = -1 # Indicator for previous choice

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Add stickiness bonus to the Q-value of the previously chosen action
        q_stage1_effective = q_stage1.copy()
        if last_action_1 != -1:
            q_stage1_effective[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_stage1_effective)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Record choice for next trial
        last_action_1 = action_1[trial]
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        # Assuming no stickiness at stage 2 for simplicity, just value-based
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning ---
        # TD(1) / Direct reinforcement for Stage 1 (ignoring transition structure)
        # This is a pure Model-Free approach
        delta_stage2 = reward[trial] - q_stage2[state_idx, action_2[trial]]
        q_stage2[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Update Stage 1 based on the reward received at the very end
        # (Standard Q-learning update for stage 1 based on stage 2 value)
        delta_stage1 = q_stage2[state_idx, action_2[trial]] - q_stage1[action_1[trial]]
        q_stage1[action_1[trial]] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Learning Rate Asymmetry
This model suggests that OCI symptoms affect how participants learn from positive versus negative prediction errors. Specifically, it tests if higher OCI makes a participant more sensitive to punishments (zero coins) or rewards (gold coins), creating separate learning rates derived from the OCI score.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Based learner with asymmetric learning rates for positive and negative 
    prediction errors, where the asymmetry is modulated by OCI.
    
    Hypothesis: OCI might correlate with hyper-sensitivity to negative outcomes 
    (or lack of reward), leading to a higher learning rate for negative prediction errors.
    
    Bounds:
    lr_base: [0, 1]
    beta: [0, 10]
    oci_neg_bias: [0, 2] (Multiplier for negative PE learning rate based on OCI)
    """
    lr_base, beta, oci_neg_bias = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Define learning rates
    # lr_pos is the base rate.
    # lr_neg is modified by OCI. If oci_neg_bias is high, high OCI leads to rapid unlearning.
    lr_pos = lr_base
    lr_neg = lr_base * (1.0 + (oci_neg_bias * oci_score))
    # Clamp to 1
    if lr_neg > 1: lr_neg = 1.0

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # We use a purely Model-Based structure here for Stage 1 evaluation
    # Q_stage2 stores the values of aliens (State, Action)
    q_stage2 = np.zeros((2, 2)) 

    for trial in range(n_trials):

        # --- Stage 1 Policy (Pure Model-Based) ---
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning ---
        # We only update the Stage 2 values (aliens) based on reward
        # The Stage 1 values are computed on the fly via the transition matrix
        
        pe = reward[trial] - q_stage2[state_idx, action_2[trial]]
        
        # Apply asymmetric learning rates
        if pe >= 0:
            q_stage2[state_idx, action_2[trial]] += lr_pos * pe
        else:
            q_stage2[state_idx, action_2[trial]] += lr_neg * pe
            
        # No explicit Stage 1 update needed for pure MB, as it uses max_q_stage2 directly.

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```