Here are three cognitive models designed to explain the participant's behavior, incorporating their low OCI-R score into the decision-making process.

### Model 1: Hybrid Learner with OCI-modulated Model-Based Control
This model assumes that individuals with lower OCI scores (indicating lower compulsivity) are more likely to rely on flexible, goal-directed (Model-Based) planning rather than rigid habits (Model-Free). The `w` parameter (mixing weight) is dynamically adjusted by the OCI score, where lower OCI increases the weight of the Model-Based system.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Reinforcement Learning model where the balance between Model-Based (MB) 
    and Model-Free (MF) control is modulated by the OCI score.
    
    Hypothesis: Lower OCI scores correlate with higher cognitive flexibility and 
    therefore a higher weight (w) on Model-Based planning.
    
    Parameters:
    learning_rate: [0, 1] - Rate of updating Q-values.
    beta: [0, 10] - Inverse temperature for softmax choice.
    w_base: [0, 1] - Baseline mixing weight for MB vs MF (1 = fully MB).
    oci_sensitivity: [0, 5] - How strongly OCI reduces the Model-Based weight.
    """
    learning_rate, beta, w_base, oci_sensitivity = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate mixing weight w based on OCI.
    # Higher OCI reduces w (more Model-Free/habitual). 
    # Lower OCI keeps w high (more Model-Based).
    # We clip to ensure w stays in [0, 1].
    w = np.clip(w_base - (current_oci * oci_sensitivity), 0.0, 1.0)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2) # Model-free values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Values for stage 2 (State x Action)

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # Model-Based Value Calculation: V(s') = max_a Q(s', a)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value: w * MB + (1-w) * MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Decision ---
        curr_state = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[curr_state])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning ---
        # Stage 2 Update (TD Learning)
        chosen_a2 = action_2[trial]
        prediction_error_2 = reward[trial] - q_stage2_mf[curr_state, chosen_a2]
        q_stage2_mf[curr_state, chosen_a2] += learning_rate * prediction_error_2
        
        # Stage 1 Update (TD Learning using Stage 2 value)
        chosen_a1 = action_1[trial]
        # Standard TD(0) update for Model-Free Stage 1
        # Note: We use the value of the state actually reached (q_stage2_mf[curr_state, chosen_a2])
        prediction_error_1 = q_stage2_mf[curr_state, chosen_a2] - q_stage1_mf[chosen_a1]
        q_stage1_mf[chosen_a1] += learning_rate * prediction_error_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Driven Stickiness (Perseveration)
This model posits that OCI scores relate to "stickiness" or choice perseverationâ€”the tendency to repeat the previous action regardless of reward. While high OCI might lead to repetitive checking (high stickiness), a low OCI participant might exhibit low or even negative stickiness (switching).

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner with choice perseveration (stickiness) modulated by OCI.
    
    Hypothesis: OCI acts as a scaling factor for the stickiness parameter. 
    Higher OCI leads to higher stickiness (compulsive repetition).
    
    Parameters:
    learning_rate: [0, 1] - Rate of updating Q-values.
    beta: [0, 10] - Inverse temperature.
    stickiness_base: [0, 5] - Baseline tendency to repeat the last choice.
    oci_mod: [0, 5] - How much OCI amplifies the stickiness.
    """
    learning_rate, beta, stickiness_base, oci_mod = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate effective stickiness
    # Effective stickiness increases with OCI score
    eff_stickiness = stickiness_base + (current_oci * oci_mod)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    last_action_1 = -1 # Initialize with an impossible action

    for trial in range(n_trials):

        # --- Stage 1 Decision ---
        # Add stickiness bonus to the Q-values if it matches the previous action
        q_stage1_modified = q_stage1.copy()
        if last_action_1 != -1:
            q_stage1_modified[last_action_1] += eff_stickiness
            
        exp_q1 = np.exp(beta * q_stage1_modified)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Record action for next trial's stickiness
        last_action_1 = action_1[trial]
        state_idx = int(state[trial])

        # --- Stage 2 Decision ---
        # Standard Softmax for stage 2 (no stickiness modeled here for simplicity)
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        # Update Stage 1 based on the value of the state reached (SARSA-like logic for stage 1-2 link)
        chosen_a2 = action_2[trial]
        chosen_a1 = action_1[trial]
        
        delta_stage1 = q_stage2[state_idx, chosen_a2] - q_stage1[chosen_a1]
        q_stage1[chosen_a1] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2[state_idx, chosen_a2]
        q_stage2[state_idx, chosen_a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Reward Sensitivity (Learning Rate Asymmetry)
This model suggests that OCI scores influence how individuals learn from positive versus negative outcomes. High OCI is often associated with anxiety and error-avoidance. Low OCI (like this participant) might suggest a more balanced learning rate or one less biased by negative prediction errors.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Dual Learning Rate model where the learning rate for negative prediction errors (punishment/omission)
    is modulated by OCI.
    
    Hypothesis: OCI affects sensitivity to "missed" rewards (0 reward).
    Higher OCI might make people over-correct for errors (higher alpha_neg).
    
    Parameters:
    alpha_pos: [0, 1] - Learning rate for positive prediction errors (RPE > 0).
    alpha_neg_base: [0, 1] - Base learning rate for negative prediction errors (RPE < 0).
    beta: [0, 10] - Inverse temperature.
    oci_anxiety_factor: [0, 2] - Multiplier for OCI to increase alpha_neg.
    """
    alpha_pos, alpha_neg_base, beta, oci_anxiety_factor = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
  
    # Effective negative learning rate scales with OCI
    # High OCI -> Higher learning rate from failures (hyper-correction)
    alpha_neg = np.clip(alpha_neg_base * (1 + current_oci * oci_anxiety_factor), 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # We use a purely Model-Based approach for Stage 1 value estimation here
    # to isolate the effect of learning rates on the Stage 2 values.
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Decision (Model-Based) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = int(state[trial])

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        chosen_a2 = action_2[trial]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, chosen_a2]
        
        # Apply asymmetric learning rates
        if delta_stage2 >= 0:
            eff_alpha = alpha_pos
        else:
            eff_alpha = alpha_neg
            
        q_stage2_mf[state_idx, chosen_a2] += eff_alpha * delta_stage2
        
        # Note: In this specific MB formulation, we don't update a separate Stage 1 Q-table,
        # because choice 1 is derived entirely from the transition matrix and Stage 2 values.

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```