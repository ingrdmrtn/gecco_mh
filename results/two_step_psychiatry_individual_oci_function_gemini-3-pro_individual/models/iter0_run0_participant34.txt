Here are three cognitive models designed to explain the participant's behavior, incorporating their OCI-R score (obsessive-compulsive inventory) as a factor influencing decision-making parameters.

### Model 1: OCI-Modulated Model-Based/Model-Free Hybrid
This model hypothesizes that the balance between Model-Based (planning) and Model-Free (habitual) control is influenced by OCI symptoms. Specifically, higher OCI scores might lead to a more rigid, habitual (Model-Free) strategy or, conversely, an over-reliance on planning (Model-Based) depending on the theoretical framework. Here, we model OCI as shifting the weight `w` towards Model-Based control.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free Reinforcement Learning with OCI-modulated mixing weight.
    
    This model assumes decision-making is a linear combination of Model-Based (MB) and 
    Model-Free (MF) values. The mixing weight 'w' is dynamically adjusted based on the 
    participant's OCI score, testing the hypothesis that OC traits influence the reliance 
    on goal-directed planning vs. habits.
    
    Parameters:
    - learning_rate: Rate at which Q-values are updated (alpha).
    - beta: Inverse temperature for the softmax choice rule.
    - w_base: Baseline mixing weight (0 = pure MF, 1 = pure MB).
    - w_oci_slope: How strongly OCI affects the mixing weight.
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    w_base: [0, 1]
    w_oci_slope: [-1, 1]
    """
    learning_rate, beta, w_base, w_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate the effective mixing weight based on OCI
    # We clip to ensure w stays within [0, 1]
    w = w_base + (w_oci_slope * oci_score)
    w = np.clip(w, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)      # Model-free values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for stage 2 (State x Action)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based Value Calculation: V(S') = max_a Q(S', a)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value: w * MB + (1-w) * MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax Policy Stage 1
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # --- Stage 2 Choice ---
        state_idx = int(state[trial])
        
        # Softmax Policy Stage 2 (Pure Model-Free)
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Learning Updates ---
        # Stage 2 Update (SARSA/Q-learning)
        # Prediction error: Reward - Current Estimate
        pe_2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * pe_2
        
        # Stage 1 Update (TD-learning)
        # Prediction error: Value of state reached - Current Estimate of chosen action
        pe_1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += learning_rate * pe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Driven Stickiness (Perseveration)
This model posits that obsessive-compulsive traits manifest as "stickiness" or perseverationâ€”a tendency to repeat the previous action regardless of reward or model-based inference. Here, the OCI score scales a `stickiness` parameter added to the choice logits.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Reinforcement Learning with OCI-modulated Choice Stickiness.
    
    This model assumes a standard Q-learning framework but adds a 'stickiness' bonus 
    to the previously chosen action. The magnitude of this stickiness is scaled by 
    the participant's OCI score, hypothesizing that higher OCI leads to more 
    repetitive motor behaviors or difficulty switching sets.
    
    Parameters:
    - learning_rate: Rate at which Q-values are updated.
    - beta: Inverse temperature for softmax.
    - stickiness_base: Baseline tendency to repeat previous choice.
    - stickiness_oci_mod: Additional stickiness per unit of OCI score.
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    stickiness_base: [0, 5]
    stickiness_oci_mod: [0, 5]
    """
    learning_rate, beta, stickiness_base, stickiness_oci_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective stickiness
    eff_stickiness = stickiness_base + (stickiness_oci_mod * oci_score)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values (Simple Model-Free for this example to isolate stickiness effect)
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    last_action_1 = -1 # No previous action for first trial

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Add stickiness bonus to Q-values before softmax
        logits_1 = beta * q_stage1.copy()
        if last_action_1 != -1:
            logits_1[last_action_1] += eff_stickiness
            
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # Update tracker
        last_action_1 = int(action_1[trial])
        
        # --- Stage 2 Choice ---
        state_idx = int(state[trial])
        # Standard softmax for stage 2 (no stickiness modeled here for simplicity)
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Learning Updates ---
        # Standard Q-learning updates
        pe_2 = reward[trial] - q_stage2[state_idx, int(action_2[trial])]
        q_stage2[state_idx, int(action_2[trial])] += learning_rate * pe_2
        
        # TD(0) update for Stage 1 using Stage 2 Q-value (common in MF models)
        pe_1 = q_stage2[state_idx, int(action_2[trial])] - q_stage1[int(action_1[trial])]
        q_stage1[int(action_1[trial])] += learning_rate * pe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Learning Rate Asymmetry
This model suggests that OCI traits affect how participants learn from positive versus negative prediction errors. It implements separate learning rates for positive and negative outcomes, where the balance between them is skewed by the OCI score (e.g., potentially higher sensitivity to negative outcomes/errors in high OCI).

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Asymmetric Learning Rate Model modulated by OCI.
    
    This model splits the learning rate into alpha_pos (for positive prediction errors)
    and alpha_neg (for negative prediction errors). The OCI score acts as a bias 
    factor, shifting the emphasis towards learning more from negative errors (or positive),
    reflecting potential anxiety-driven error sensitivity common in OC traits.
    
    Parameters:
    - alpha_base: The central tendency of the learning rate.
    - alpha_bias_oci: How much OCI skews learning towards negative PE. 
                      Positive value = OCI increases learning from negative errors.
    - beta: Inverse temperature.
    - w: Mixing weight for MB vs MF (fixed parameter here to focus on learning rates).
    
    Bounds:
    alpha_base: [0, 1]
    alpha_bias_oci: [-0.5, 0.5] (Limited to keep alphas in range)
    beta: [0, 10]
    w: [0, 1]
    """
    alpha_base, alpha_bias_oci, beta, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Define asymmetric learning rates based on OCI
    # If alpha_bias_oci is positive, high OCI increases alpha_neg and decreases alpha_pos
    alpha_pos = alpha_base - (alpha_bias_oci * oci_score)
    alpha_neg = alpha_base + (alpha_bias_oci * oci_score)
    
    # Clamp to valid range [0, 1]
    alpha_pos = np.clip(alpha_pos, 0.0, 1.0)
    alpha_neg = np.clip(alpha_neg, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # --- Stage 2 Choice ---
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Learning Updates with Asymmetry ---
        
        # Stage 2 PE
        pe_2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        alpha_2 = alpha_pos if pe_2 > 0 else alpha_neg
        q_stage2_mf[state_idx, int(action_2[trial])] += alpha_2 * pe_2
        
        # Stage 1 PE
        pe_1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        alpha_1 = alpha_pos if pe_1 > 0 else alpha_neg
        q_stage1_mf[int(action_1[trial])] += alpha_1 * pe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```