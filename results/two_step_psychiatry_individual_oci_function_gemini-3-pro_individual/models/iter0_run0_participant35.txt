Here are three cognitive models designed to capture the behavior of a participant with a medium-to-high OCI-R score (0.5) in a two-step decision task.

### Cognitive Model 1: The "Habitual Stickiness" Model
**Hypothesis:** Individuals with higher obsessive-compulsive traits often exhibit rigidity or "stickiness" in their choices, repeating previous actions regardless of recent outcomes. This model incorporates a choice perseveration (stickiness) parameter that is modulated by the OCI score. Specifically, higher OCI leads to a stronger tendency to repeat the last Stage 1 choice, interfering with standard Model-Based (MB) or Model-Free (MF) planning.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: Hybrid Model-Based/Model-Free Reinforcement Learning with OCI-modulated Stickiness.
    
    Hypothesis: The participant uses a mix of model-based and model-free learning. 
    However, their OCI score drives a 'stickiness' parameter, making them more likely 
    to repeat their previous Stage 1 choice regardless of value.
    
    Parameters:
    - learning_rate: [0, 1] Rate at which Q-values are updated.
    - beta: [0, 10] Inverse temperature for softmax choice (exploration/exploitation).
    - w: [0, 1] Weighting parameter (1 = pure Model-Based, 0 = pure Model-Free).
    - stickiness_base: [0, 5] Base tendency to repeat the previous choice.
    
    The effective stickiness is calculated as: stickiness_base * (1 + oci)
    """
    learning_rate, beta, w, stickiness_base = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Effective stickiness increases with OCI score
    eff_stickiness = stickiness_base * (1.0 + oci_score)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2) # Model-free values for stage 1 (Spaceships)
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for stage 2 (Aliens)
    
    last_choice_1 = -1 # Track previous choice

    for trial in range(n_trials):
        # --- STAGE 1 CHOICE ---
        # Model-Based Calculation: V(state) = max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combined Q-value: w * MB + (1-w) * MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness bonus to the previously chosen action
        if last_choice_1 != -1:
            q_net[last_choice_1] += eff_stickiness
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        # Record probability of the observed action
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- STAGE 2 CHOICE ---
        state_idx = state[trial] # Which planet we arrived at
        
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- LEARNING ---
        # Prediction errors
        # Stage 1 MF update (SARSA-like logic using Stage 2 value)
        # We use the value of the state chosen in stage 2 as the target
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Stage 2 MF update (Reward prediction error)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Update history
        last_choice_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2: The "Anxious Uncertainty" Model
**Hypothesis:** Medium-to-high OCI is often associated with intolerance of uncertainty. This model posits that the participant relies more heavily on Model-Based (planning) strategies when they are "anxious" (high OCI), attempting to control the outcome by using the transition structure. However, their learning rate is dampened by their OCI score, reflecting a rigid belief system that is slow to update even in the face of new evidence.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Modulated Weighting and Learning Rate.
    
    Hypothesis: 
    1. Higher OCI leads to higher reliance on Model-Based control (higher 'w') 
       in an attempt to reduce uncertainty.
    2. Higher OCI leads to lower learning rates (rigidity/resistance to change).
    
    Parameters:
    - base_lr: [0, 1] Base learning rate.
    - beta: [0, 10] Inverse temperature.
    - w_base: [0, 1] Base weighting for Model-Based system.
    - lambda_eligibility: [0, 1] Eligibility trace decay (how much stage 2 reward updates stage 1).
    
    Transformations:
    - effective_lr = base_lr / (1 + oci)  -> Higher OCI reduces learning speed.
    - effective_w = w_base + (1 - w_base) * oci -> Higher OCI pushes w towards 1 (Model-Based).
    """
    base_lr, beta, w_base, lambda_eligibility = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # OCI Modifications
    # Rigid beliefs: Learning rate decreases as OCI increases
    learning_rate = base_lr / (1.0 + oci_score)
    # Control seeking: Weight on Model-Based increases with OCI
    w = w_base + (1.0 - w_base) * oci_score 
    # Clamp w to max 1.0 just in case
    if w > 1.0: w = 1.0

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- STAGE 1 CHOICE ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- STAGE 2 CHOICE ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- LEARNING ---
        # Stage 2 RPE
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Stage 1 update
        # In this model, we use an eligibility trace (lambda) to allow the Stage 2 RPE 
        # to directly update Stage 1 MF values, alongside the standard TD(0) update.
        # This represents how credit is assigned.
        
        # 1. Standard TD update for Stage 1 (State 1 -> State 2)
        # Note: We use the max value of the next state (Q-learning style) for the TD target here
        # to differentiate from Model 1.
        delta_stage1 = np.max(q_stage2_mf[state_idx]) - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # 2. Eligibility trace update: The reward at the very end also updates the first choice
        q_stage1_mf[action_1[trial]] += learning_rate * lambda_eligibility * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3: The "Safe Bet" Model
**Hypothesis:** Participants with higher OCI scores may exhibit higher loss aversion or a stronger focus on avoiding negative outcomes (0 coins). This model modifies the reward function perception. Instead of maximizing expected value purely, the participant treats the reward signal differently based on their OCI, effectively amplifying the "relief" of getting a reward or the "pain" of missing it, leading to a distinct Inverse Temperature (beta) modulation.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: Pure Model-Free with OCI-driven Exploration Suppression.
    
    Hypothesis: The participant relies primarily on Model-Free (habitual) learning.
    However, the OCI score modulates the 'beta' (inverse temperature).
    Higher OCI leads to significantly higher beta values, meaning the participant 
    explores less and exploits more (compulsive repetition of what works), 
    fearing the switch to the unknown.
    
    Parameters:
    - learning_rate: [0, 1]
    - beta_base: [0, 10] Base inverse temperature.
    - decay: [0, 1] Forgetting rate for unchosen options (compulsive focus).
    
    Transformations:
    - effective_beta = beta_base * (1 + 2 * oci)
      (A high OCI score significantly sharpens the softmax curve).
    """
    learning_rate, beta_base, decay = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # OCI amplifies exploitation (reduces exploration noise)
    beta = beta_base * (1.0 + 2.0 * oci_score)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Pure Model-Free setup
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- STAGE 1 CHOICE ---
        # Pure MF policy
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- STAGE 2 CHOICE ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- LEARNING ---
        # SARSA(0) style updates
        
        # 1. Update Stage 1 based on Stage 2 value
        delta_stage1 = q_stage2[state_idx, action_2[trial]] - q_stage1[action_1[trial]]
        q_stage1[action_1[trial]] += learning_rate * delta_stage1
        
        # 2. Update Stage 2 based on Reward
        delta_stage2 = reward[trial] - q_stage2[state_idx, action_2[trial]]
        q_stage2[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # 3. Forgetting/Decay for unchosen options
        # High OCI might correlate with ignoring alternative possibilities (tunnel vision)
        # We decay the value of the unchosen spaceship towards 0
        unchosen_s1 = 1 - action_1[trial]
        q_stage1[unchosen_s1] *= (1 - decay)
        
        # We decay the value of the unchosen alien on the current planet
        unchosen_s2 = 1 - action_2[trial]
        q_stage2[state_idx, unchosen_s2] *= (1 - decay)
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```