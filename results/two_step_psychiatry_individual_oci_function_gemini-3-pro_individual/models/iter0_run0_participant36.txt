Here are three cognitive models designed to explain the participant's behavior, incorporating their OCI score into the learning and decision-making processes.

### Model 1: OCI-Modulated Model-Based/Model-Free Balance
This model hypothesizes that the balance between Model-Based (goal-directed, planning) and Model-Free (habitual) control is influenced by obsessive-compulsive traits. Specifically, it tests whether higher OCI scores lead to a reliance on habitual (Model-Free) control, making the `w` parameter (mixing weight) a function of the OCI score.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-modulated Model-Based vs. Model-Free weighting.
    
    Hypothesis: The balance between goal-directed (MB) and habitual (MF) control 
    is influenced by OCI symptoms. Higher OCI might drive more habitual behavior 
    (lower w) or rigid model-based behavior. Here, 'w' is derived from a base 
    parameter and the OCI score.

    Parameters:
    - learning_rate: [0, 1] Rate of updating Q-values.
    - beta: [0, 10] Inverse temperature for softmax choice.
    - w_base: [0, 1] Base weighting for Model-Based control (0=MF, 1=MB).
    - oci_sens: [0, 1] Sensitivity of 'w' to the OCI score.
    """
    learning_rate, beta, w_base, oci_sens = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate the mixing weight 'w' based on OCI.
    # We model this such that OCI modulates the base weight. 
    # A simple linear interaction constrained to [0, 1].
    # If oci_sens is high, the OCI score drags w towards 0 (more habitual).
    w = w_base * (1.0 - (oci_score * oci_sens))
    w = np.clip(w, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2) # Model-free Q-values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Model-free Q-values for stage 2

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based Value: Transition * Max(Stage 2 Q)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Value: Weighted sum of MB and MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial]) # The planet arrived at

        # --- Stage 2 Policy ---
        # Standard Softmax on Stage 2 Q-values
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # --- Updates ---
        # Prediction Error Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2
        
        # Prediction Error Stage 1 (TD-error using Stage 2 value)
        # Note: In pure MF, we update using the value of the state reached.
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Driven Perseveration (Stickiness)
This model posits that Obsessive-Compulsive traits manifest as behavioral rigidity or "stickiness." People with higher OCI scores might be more likely to repeat their previous choice regardless of reward outcomes. The `stickiness` parameter is directly scaled by the OCI score.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-driven Choice Perseveration (Stickiness).
    
    Hypothesis: OCI symptoms relate to repetitive behaviors. This model adds a 
    perseveration bonus to the previously chosen action in Stage 1. The magnitude
    of this bonus is determined by the OCI score.

    Parameters:
    - learning_rate: [0, 1] Rate of updating Q-values.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weight for Model-Based control.
    - stickiness_factor: [0, 5] Base magnitude of perseveration, scaled by OCI.
    """
    learning_rate, beta, w, stickiness_factor = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Previous choice tracker (initialized to -1 for none)
    prev_choice_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Calculate Net Q
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add Stickiness Bonus
        # If OCI is high, the bonus for repeating the previous action is larger.
        stickiness_bonus = np.zeros(2)
        if prev_choice_1 != -1:
            stickiness_bonus[int(prev_choice_1)] = stickiness_factor * oci_score
        
        # Softmax with stickiness
        exp_q1 = np.exp(beta * (q_net + stickiness_bonus))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # Update previous choice
        prev_choice_1 = action_1[trial]
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Learning Rate Asymmetry (Anxiety/Avoidance)
This model suggests that OCI, often comorbid with anxiety, leads to an asymmetry in how participants learn from positive versus negative outcomes. Specifically, this model tests if the OCI score amplifies the learning rate for negative prediction errors (punishment/lack of reward sensitivity) relative to positive ones.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-modulated Learning Rate Asymmetry (Pos/Neg).
    
    Hypothesis: Higher OCI scores might be associated with hyper-sensitivity to 
    negative outcomes (or lack of reward). This model splits learning rates into 
    alpha_pos and alpha_neg, where alpha_neg is boosted by the OCI score.

    Parameters:
    - alpha_base: [0, 1] Base learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weight for Model-Based control.
    - neg_bias: [0, 1] How much the OCI score amplifies learning from negative PEs.
    """
    alpha_base, beta, w, neg_bias = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        # --- Updates ---
        # Calculate PEs
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        
        # Determine Learning Rate based on PE sign and OCI
        # If PE is negative, learning rate is boosted by OCI * neg_bias
        # If PE is positive, use base alpha
        
        # Update Stage 1
        lr_1 = alpha_base
        if delta_stage1 < 0:
            lr_1 = alpha_base + (neg_bias * oci_score)
            lr_1 = np.clip(lr_1, 0, 1) # Ensure bounds
        q_stage1_mf[int(action_1[trial])] += lr_1 * delta_stage1
        
        # Update Stage 2
        lr_2 = alpha_base
        if delta_stage2 < 0:
            lr_2 = alpha_base + (neg_bias * oci_score)
            lr_2 = np.clip(lr_2, 0, 1)
        q_stage2_mf[state_idx, int(action_2[trial])] += lr_2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```