Here are three cognitive models based on the two-step task framework, incorporating the OCI score to modulate learning and decision-making parameters.

### Model 1: OCI-Modulated Model-Based/Model-Free Balance
This model hypothesizes that Obsessive-Compulsive symptoms (OCI) affect the balance between Model-Based (planning) and Model-Free (habitual) control. Specifically, higher OCI might correlate with a rigidity or over-reliance on habits (Model-Free) or, conversely, an obsessive tracking of transition structures (Model-Based). Given the low OCI score (0.125) of this participant, this model tests if `w` (the mixing weight) is a function of OCI.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: Hybrid Model-Based/Model-Free RL with OCI-modulated mixing weight.
    
    This model assumes decision-making is a mix of Model-Based (MB) and Model-Free (MF) systems.
    The mixing weight 'w' is modulated by the participant's OCI score.
    
    Parameters:
    - learning_rate: Update rate for Q-values [0, 1]
    - beta: Inverse temperature for softmax choice [0, 10]
    - w_base: Baseline weighting for Model-Based control [0, 1]
    - w_oci_slope: How strongly OCI affects the MB weighting [-1, 1] (scaled to stay in bounds)
    """
    learning_rate, beta, w_base, w_oci_slope = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0] # Extract scalar
    
    # Calculate the mixing weight w based on OCI
    # We use a sigmoid-like transformation or simple clamping to keep w in [0,1]
    # Here, we treat w_oci_slope as a modifier to w_base.
    # If slope is positive, higher OCI -> more Model-Based.
    raw_w = w_base + (w_oci_slope * (current_oci - 0.31)) # Centered roughly on "medium" cutoff
    w = np.clip(raw_w, 0.0, 1.0)
    
    # Fixed transition matrix (Common: 0.7, Rare: 0.3)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2) # MF values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # MF values for stage 2 (2 planets, 2 aliens)
    
    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based Value: V_MB(s1) = T * max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value: Q_net = w * Q_MB + (1-w) * Q_MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax Policy Stage 1
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        # Store likelihood of observed action
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # Transition
        state_idx = int(state[trial]) # The planet arrived at
        
        # --- Stage 2 Choice ---
        # Standard MF choice at stage 2
        qs_stage2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_stage2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        r = reward[trial]
        
        # Stage 2 Update (TD Error)
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 Update (TD Error, using stage 2 value as proxy for reward)
        # Note: In pure Daw 2011, Stage 1 MF updates from the value of the CHOSEN state in stage 2
        # or the max of stage 2. Here we use the standard SARSA-like update.
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        
        # Eligibility trace: Stage 1 also learns from Stage 2's RPE (lambda=1 assumption for simplification)
        q_stage1_mf[a1] += learning_rate * delta_stage1 + learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Driven Perseveration
This model hypothesizes that OCI relates to "stickiness" or perseveration. Individuals with different OCI levels might be more prone to simply repeating the previous action regardless of reward (motor perseveration). The parameter `perseveration` is added to the Q-values in the softmax step, and its magnitude is scaled by the OCI score.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: Model-Free RL with OCI-modulated Perseveration.
    
    This model assumes a simpler Model-Free architecture but adds a perseveration bonus.
    The strength of perseveration is scaled by the OCI score.
    
    Parameters:
    - learning_rate: Update rate for Q-values [0, 1]
    - beta: Inverse temperature for softmax choice [0, 10]
    - pers_base: Base perseveration parameter [0, 5]
    - pers_oci_mod: Modulation of perseveration by OCI [0, 5]
    """
    learning_rate, beta, pers_base, pers_oci_mod = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate effective perseveration bonus
    # High OCI -> Higher tendency to repeat previous choice
    pers_bonus = pers_base + (pers_oci_mod * current_oci)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1 # Initialize as invalid index
    
    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Add perseveration bonus to Q-values before softmax
        q_net_s1 = q_stage1_mf.copy()
        if last_action_1 != -1:
            q_net_s1[last_action_1] += pers_bonus
            
        exp_q1 = np.exp(beta * q_net_s1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # Update last action for next trial
        last_action_1 = a1
        
        state_idx = int(state[trial])
        
        # --- Stage 2 Choice ---
        # No perseveration modeled for stage 2 (aliens change per planet)
        qs_stage2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_stage2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        r = reward[trial]
        
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1 + learning_rate * delta_stage2 # TD(1) update
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Dependent Learning Rate Asymmetry
This model suggests that OCI symptoms might relate to how individuals learn from positive versus negative outcomes. Specifically, it implements separate learning rates for positive prediction errors (learning from success) and negative prediction errors (learning from failure), where the ratio or balance is influenced by the OCI score.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: Dual Learning Rate Model (Positive vs Negative) modulated by OCI.
    
    This model separates learning rates for positive and negative prediction errors.
    The OCI score shifts the bias towards learning more from negative outcomes (anxiety/avoidance).
    
    Parameters:
    - alpha_base: Base learning rate [0, 1]
    - alpha_neg_bias: Additional boost to negative learning rate based on OCI [0, 1]
    - beta: Inverse temperature [0, 10]
    """
    alpha_base, alpha_neg_bias, beta = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Define Learning Rates
    # Alpha_pos is the base rate
    alpha_pos = alpha_base
    
    # Alpha_neg is boosted by OCI. High OCI -> Higher sensitivity to negative RPEs.
    # We clip to ensure it stays <= 1.
    alpha_neg = np.clip(alpha_base + (alpha_neg_bias * current_oci), 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Pure Model-Based for Stage 1 to isolate learning rate effects on Stage 2 values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        state_idx = int(state[trial])

        # --- Stage 2 Choice ---
        qs_stage2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_stage2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning with Asymmetric Rates ---
        r = reward[trial]
        
        # Calculate RPE
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        
        # Choose alpha based on sign of RPE
        if delta_stage2 >= 0:
            eff_alpha = alpha_pos
        else:
            eff_alpha = alpha_neg
            
        q_stage2_mf[state_idx, a2] += eff_alpha * delta_stage2
        
        # We assume stage 1 values are derived MB, so we don't update q_stage1_mf here
        # (This keeps the model focused on how value estimation changes, not the mixing weight)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```