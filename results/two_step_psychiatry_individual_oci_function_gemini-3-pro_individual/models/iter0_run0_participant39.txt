Here are three cognitive models designed to explain the participant's behavior in the two-step task, incorporating their low OCI-R score (0.1125). Since the participant has low obsessive-compulsive symptoms, these models generally hypothesize standard reinforcement learning mechanisms (Model 1), a balance of model-based/model-free control (Model 2), and a specific sensitivity to rare transitions (Model 3), with the OCI score modulating key parameters like learning rates or mixing weights.

### Model 1: OCI-Modulated Model-Based Reinforcement Learning
This model assumes that individuals with lower OCI scores might be more flexible and goal-directed (Model-Based). It implements a pure Model-Based controller where the learning rate for the second-stage values is modulated by the OCI score. The hypothesis is that lower OCI scores allow for faster updating of the reward probabilities (higher learning rate), whereas higher OCI scores might lead to rigidity (slower updating).

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: Pure Model-Based RL with OCI-modulated learning rate.
    
    Hypothesis: Participants with low OCI scores (like this one) are more adaptable
    to changing reward contingencies. The base learning rate is modified by the OCI
    score such that lower OCI leads to higher effective learning rates.
    
    Parameters:
    base_lr: [0, 1] Base learning rate for stage 2 updates.
    beta: [0, 10] Inverse temperature for softmax choice.
    oci_sensitivity: [0, 1] How strongly OCI dampens the learning rate.
    """
    base_lr, beta, oci_sensitivity = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Effective learning rate: Lower OCI -> Higher learning rate (more adaptable)
    # If OCI is high, the learning rate shrinks.
    alpha = base_lr * (1.0 - (oci_score * oci_sensitivity))
    # Ensure alpha stays in bounds
    alpha = np.clip(alpha, 0.0, 1.0)

    # Fixed transition matrix (70% common, 30% rare)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values for Stage 2 (aliens): 2 planets x 2 aliens
    q_stage2_mf = np.zeros((2, 2)) # Initialized at 0 or 0.5 usually, here 0 per template

    for trial in range(n_trials):
        # --- Stage 1 Policy (Model-Based) ---
        # Calculate Model-Based values: Transition * Max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial]) # Planet reached (0 or 1)

        # --- Stage 2 Policy ---
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # --- Updating ---
        # No Stage 1 update in pure Model-Based (it relies on Stage 2 values + transition matrix)
        
        # Update Stage 2 Q-values based on reward
        chosen_alien = int(action_2[trial])
        prediction_error = reward[trial] - q_stage2_mf[state_idx, chosen_alien]
        q_stage2_mf[state_idx, chosen_alien] += alpha * prediction_error

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model-Based/Model-Free with OCI Mixing Weight
This is the classic "Daw two-step" hybrid model. It assumes behavior is a mix of Model-Based (goal-directed) and Model-Free (habitual) systems. The mixing parameter `w` determines the balance. Here, `w` is not a free parameter but is derived from the OCI score and a scaling factor. The hypothesis is that low OCI participants (low compulsivity) rely more on the flexible Model-Based system (higher `w`), while high OCI participants rely more on habits (Model-Free, lower `w`).

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: Hybrid MB/MF with OCI-determined mixing weight.
    
    Hypothesis: Low OCI correlates with higher Model-Based control.
    We model the mixing weight 'w' as a function of OCI.
    w = 1 means pure Model-Based, w = 0 means pure Model-Free.
    
    Parameters:
    lr: [0, 1] Learning rate for both stages.
    beta: [0, 10] Inverse temperature.
    w_intercept: [0, 1] Baseline model-basedness.
    w_slope: [0, 1] How much OCI reduces model-basedness.
    """
    lr, beta, w_intercept, w_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate mixing weight w based on OCI
    # Higher OCI -> Lower w (more habitual/MF)
    # Lower OCI -> Higher w (more goal-directed/MB)
    w = w_intercept - (w_slope * oci_score)
    w = np.clip(w, 0.0, 1.0) # Ensure w stays between 0 and 1

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)      # MF values for spaceships
    q_stage2_mf = np.zeros((2, 2)) # MF values for aliens

    for trial in range(n_trials):
        # --- Stage 1 Policy (Hybrid) ---
        # 1. Calculate Model-Based values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Mix MB and MF values
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # --- Updating ---
        # Update Stage 1 MF (TD(0))
        # Note: In full SARSA(lambda) models this is more complex, here we use simple TD
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += lr * delta_stage1
        
        # Update Stage 2 MF
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += lr * delta_stage2
        
        # Additional Update: TD(1) / Eligibility Trace for Stage 1
        # The reward at stage 2 also updates stage 1 choice in MF system
        q_stage1_mf[int(action_1[trial])] += lr * delta_stage2 # simplified lambda=1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Perseveration-Augmented RL with OCI Modulation
This model posits that OCI scores relate to "stickiness" or perseveration (repeating the same choice regardless of reward). Even low OCI participants might have some baseline stickiness, or perhaps they are *less* sticky. Here, the OCI score scales a `perseveration` parameter that adds a bonus to the previously chosen action.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: Model-Free RL with OCI-scaled Perseveration.
    
    Hypothesis: OCI relates to repetitive behavior (compulsivity).
    We add a 'stickiness' bonus to the Q-value of the previously chosen action.
    The magnitude of this stickiness is scaled by the OCI score.
    
    Parameters:
    lr: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    pers_weight: [0, 5] Base magnitude of perseveration bonus.
    """
    lr, beta, pers_weight = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Perseveration bonus is scaled by OCI.
    # High OCI -> Stronger tendency to repeat previous choice.
    # Low OCI -> Weaker tendency.
    effective_stickiness = pers_weight * oci_score

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1 # No previous action at start

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        q_stage1_effective = q_stage1_mf.copy()
        
        # Apply perseveration bonus if not the first trial
        if last_action_1 != -1:
            q_stage1_effective[last_action_1] += effective_stickiness
            
        exp_q1 = np.exp(beta * q_stage1_effective)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        current_action_1 = int(action_1[trial])

        # --- Stage 2 Policy ---
        # We assume perseveration primarily affects the high-level spaceship choice (stage 1)
        # so Stage 2 is standard softmax on Q-values.
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # --- Updating ---
        # Update Stage 1
        # Using the value of the state reached (max Q of next stage) as target (Q-learning style)
        max_next_q = np.max(q_stage2_mf[state_idx])
        delta_stage1 = max_next_q - q_stage1_mf[current_action_1]
        q_stage1_mf[current_action_1] += lr * delta_stage1
        
        # Update Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += lr * delta_stage2
        
        # Store action for next trial's perseveration
        last_action_1 = current_action_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```