Here are three cognitive models designed to explain the participant's behavior in the two-step task, incorporating the OCI-R score as a modulator of learning and decision-making.

### Model 1: OCI-Modulated Model-Based/Model-Free Trade-off
This model hypothesizes that the balance between Model-Based (planning-oriented) and Model-Free (habit-oriented) control in the first stage is influenced by the participant's obsessive-compulsive traits. Specifically, it tests whether higher OCI scores lead to a rigid reliance on habits (Model-Free) or an over-deliberative planning style (Model-Based). Since this participant has a very low OCI score (0.0125), this model allows the data to determine if low OCI correlates with a specific mixing weight (`w`).

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free Reinforcement Learning model.
    The mixing weight 'w' (trade-off between MB and MF systems) is modulated by OCI score.
    
    Parameters:
    learning_rate: [0, 1] Rate at which Q-values are updated.
    beta: [0, 10] Inverse temperature for softmax choice rule (exploration/exploitation).
    w_base: [0, 1] Baseline weight for Model-Based control (0 = pure MF, 1 = pure MB).
    w_oci_slope: [-1, 1] How OCI score shifts the balance. Positive = OCI increases MB control.
    """
    learning_rate, beta, w_base, w_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate the effective mixing weight w based on OCI
    # We use a sigmoid-like constraint or simple clipping to keep w in [0,1]
    raw_w = w_base + (w_oci_slope * oci_score)
    w = np.clip(raw_w, 0.0, 1.0)
    
    # Transition matrix (fixed as per task structure: A->X (0->0) is 0.7)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2) # Model-free values for stage 1 (Spaceships)
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for stage 2 (Aliens on Planets)
    
    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # Model-Based Value Calculation: V(state) = max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value: w * MB + (1-w) * MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax Policy Stage 1
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # Transition to State (Planet)
        state_idx = int(state[trial])
        
        # --- Stage 2 Decision ---
        # Standard Model-Free Q-learning for the second stage
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Learning / Updating ---
        # Prediction errors
        # Stage 1 MF update (SARSA-style using stage 2 value)
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1
        
        # Stage 2 MF update (Reward based)
        # Note: reward in data might be -1, 0, 1. We treat it directly.
        r = reward[trial]
        delta_stage2 = r - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Driven Perseveration (Stickiness)
This model posits that OCI symptoms relate to "cognitive stickiness" or perseverationâ€”the tendency to repeat the previous choice regardless of reward. While the participant has a low OCI score, this model allows us to test if low OCI corresponds to low stickiness (flexible switching) versus high OCI corresponding to high stickiness (repetitive behavior). The `stickiness` parameter is added to the Q-values in the softmax step.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free Q-Learning with OCI-modulated Choice Perseveration (Stickiness).
    
    Parameters:
    learning_rate: [0, 1] Update rate.
    beta: [0, 10] Inverse temperature.
    stickiness_base: [0, 5] Base tendency to repeat the last choice.
    stickiness_oci_mod: [-5, 5] Modulation of stickiness by OCI score.
    """
    learning_rate, beta, stickiness_base, stickiness_oci_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective stickiness
    eff_stickiness = stickiness_base + (stickiness_oci_mod * oci_score)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track previous choice for stickiness (initially -1 so no stickiness on trial 0)
    prev_action_1 = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # Add stickiness bonus to the Q-values before softmax
        q_stage1_boosted = q_stage1_mf.copy()
        if prev_action_1 != -1:
            q_stage1_boosted[prev_action_1] += eff_stickiness
            
        exp_q1 = np.exp(beta * q_stage1_boosted)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        
        # --- Stage 2 Decision ---
        # Simple MF choice at stage 2
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Learning ---
        # TD(0) updates
        # Stage 1 update
        # Using the value of the state chosen in stage 2 (max Q of stage 2) as target
        target_stage1 = np.max(q_stage2_mf[state_idx])
        delta_stage1 = target_stage1 - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1
        
        # Stage 2 update
        r = reward[trial]
        delta_stage2 = r - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2
        
        # Update history
        prev_action_1 = int(action_1[trial])

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Learning Rate Asymmetry
This model investigates if OCI scores affect how participants learn from positive versus negative outcomes. It is often hypothesized that anxiety or compulsive traits sensitize individuals to punishments (or lack of reward). Here, the learning rate for negative prediction errors (disappointment) is modulated by OCI, while the learning rate for positive errors is fixed to a base parameter.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Dual Learning Rate Model (Positive/Negative) with OCI modulation on Negative Learning.
    
    Parameters:
    alpha_pos: [0, 1] Learning rate for positive prediction errors.
    alpha_neg_base: [0, 1] Base learning rate for negative prediction errors.
    alpha_neg_oci_slope: [-1, 1] How OCI affects learning from negative outcomes.
    beta: [0, 10] Inverse temperature.
    """
    alpha_pos, alpha_neg_base, alpha_neg_oci_slope, beta = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective negative learning rate
    raw_alpha_neg = alpha_neg_base + (alpha_neg_oci_slope * oci_score)
    alpha_neg = np.clip(raw_alpha_neg, 0.0, 1.0)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        
        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Learning ---
        # Stage 1 Update
        # Using simple Q-learning: target is max of next state
        target_val = np.max(q_stage2_mf[state_idx])
        delta_stage1 = target_val - q_stage1_mf[int(action_1[trial])]
        
        if delta_stage1 >= 0:
            q_stage1_mf[int(action_1[trial])] += alpha_pos * delta_stage1
        else:
            q_stage1_mf[int(action_1[trial])] += alpha_neg * delta_stage1
            
        # Stage 2 Update
        r = reward[trial]
        delta_stage2 = r - q_stage2_mf[state_idx, int(action_2[trial])]
        
        if delta_stage2 >= 0:
            q_stage2_mf[state_idx, int(action_2[trial])] += alpha_pos * delta_stage2
        else:
            q_stage2_mf[state_idx, int(action_2[trial])] += alpha_neg * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```