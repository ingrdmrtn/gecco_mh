Here are three new cognitive models exploring different mechanisms for how OCI traits might influence decision-making in this two-step task.

### Model 1: Hybrid Model with OCI-modulated Weighting ($w$)
This model hypothesizes that the balance between Model-Based (planning) and Model-Free (habitual) control is influenced by the participant's OCI score. Specifically, it tests if obsessive-compulsive traits shift the reliance on the internal model of the task structure (transition matrix) versus simple reward history.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free reinforcement learning where the weighting parameter 'w'
    is modulated by the OCI score.
    
    Hypothesis: OCI score influences the trade-off between goal-directed (MB) and 
    habitual (MF) control. Higher OCI might correlate with more rigid, habitual behavior 
    (lower w) or potentially more extensive planning (higher w).
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    w_intercept: [0, 1] (Base mixing weight)
    w_slope: [-1, 1] (Effect of OCI on mixing weight)
    """
    learning_rate, beta, w_intercept, w_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective w and clamp it between 0 and 1
    w = w_intercept + (w_slope * oci_score)
    w = np.clip(w, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2) # Model-free Q-values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Model-free Q-values for stage 2 (same for MB)

    for trial in range(n_trials):
        
        # --- Stage 1 Policy ---
        # Model-Based Value Calculation: V_MB(s1) = T * max(Q(s2))
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value: Q_net = w * Q_MB + (1-w) * Q_MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])

        # --- Stage 2 Policy ---
        # Standard Softmax on Stage 2 Q-values
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        # Stage 2 Update (standard TD)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 Update (TD(0) / SARSA-like for MF)
        # Note: In standard hybrid models, MF is updated via TD(1) (reward) or TD(0) (next state value)
        # Here we use the standard TD(1) style update often used in Daw et al. 2011 for simplicity in this template context
        # Or more accurately, MF updates using the stage 2 value.
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Learning Rates (Separate Positive/Negative)
This model investigates if OCI traits affect sensitivity to rewards versus punishments differently. It posits that individuals with higher OCI scores might learn differently from positive outcomes (coins) compared to negative outcomes (no coins), reflecting altered reward processing or punishment sensitivity.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model with Asymmetric Learning Rates modulated by OCI.
    
    Hypothesis: OCI affects how strongly participants update values after a reward (positive prediction error)
    versus no reward (negative prediction error). High OCI might lead to hyper-sensitivity to 
    negative outcomes (avoidance) or rigidity.
    
    Bounds:
    lr_base: [0, 1] (Base learning rate)
    beta: [0, 10]
    oci_mod_pos: [-1, 1] (Modulation of learning rate for positive errors)
    oci_mod_neg: [-1, 1] (Modulation of learning rate for negative errors)
    """
    lr_base, beta, oci_mod_pos, oci_mod_neg = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate specific learning rates
    # We apply the modulation to the base rate. We use sigmoid or clipping to keep it in [0,1].
    # Simple linear modulation with clipping is used here.
    lr_pos = np.clip(lr_base + (oci_mod_pos * oci_score), 0.0, 1.0)
    lr_neg = np.clip(lr_base + (oci_mod_neg * oci_score), 0.0, 1.0)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Pure Model-Free Q-learning for simplicity to isolate LR effects
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        state_idx = int(state[trial])
        r = reward[trial]

        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        # Stage 2 Update
        pe2 = r - q_stage2[state_idx, a2]
        eff_lr2 = lr_pos if pe2 >= 0 else lr_neg
        q_stage2[state_idx, a2] += eff_lr2 * pe2
        
        # Stage 1 Update (using Stage 2 value as proxy for reward in TD chain)
        pe1 = q_stage2[state_idx, a2] - q_stage1[a1]
        eff_lr1 = lr_pos if pe1 >= 0 else lr_neg
        q_stage1[a1] += eff_lr1 * pe1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Inverse Temperature (Exploration) Modulated by OCI
This model proposes that the OCI score primarily impacts the "randomness" or exploration-exploitation trade-off in decision-making. High OCI might lead to more deterministic (lower temperature, higher beta) choices, sticking rigidly to the perceived best option, or conversely, more erratic behavior.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model with Inverse Temperature (Beta) modulated by OCI.
    
    Hypothesis: OCI traits influence the exploration-exploitation balance. 
    A higher OCI score might lead to a higher Beta (more deterministic/rigid choice selection)
    or lower Beta (more noise).
    
    Bounds:
    learning_rate: [0, 1]
    beta_base: [0, 10] (Base inverse temperature)
    beta_oci_slope: [-10, 10] (How OCI changes decision noise)
    """
    learning_rate, beta_base, beta_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective beta
    # Ensure beta stays non-negative
    beta_eff = beta_base + (beta_oci_slope * oci_score)
    beta_eff = np.maximum(beta_eff, 0.0)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Simple Q-learning setup
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        state_idx = int(state[trial])
        r = reward[trial]

        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta_eff * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta_eff * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        # Stage 2 Update
        delta2 = r - q_stage2[state_idx, a2]
        q_stage2[state_idx, a2] += learning_rate * delta2
        
        # Stage 1 Update
        delta1 = q_stage2[state_idx, a2] - q_stage1[a1]
        q_stage1[a1] += learning_rate * delta1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```