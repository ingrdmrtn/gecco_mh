Here are 3 new cognitive models that explore different mechanisms for how OCI scores might influence decision-making in the two-step task, specifically focusing on the balance between model-based and model-free control, learning rates, and exploration.

### Model 1: Hybrid Learner with OCI-modulated Model-Based Weight
This model tests the hypothesis that OCI symptoms relate to the balance between goal-directed (Model-Based) and habitual (Model-Free) control. Specifically, it posits that higher OCI scores might lead to a stronger reliance on rigid, habitual (Model-Free) strategies, or conversely, hyper-deliberative (Model-Based) strategies. Given the low score here, we parameterize the mixing weight $w$ to be a function of OCI.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free learner where the mixing weight 'w' is modulated by OCI.
    
    Hypothesis: The balance between Model-Based (planning) and Model-Free (habit) systems
    is influenced by OCI traits. The weight 'w' determines the contribution of the MB system.
    w = w_base + (oci_mod * oci_score).
    
    Parameters:
    lr: [0, 1] Learning rate for value updates.
    beta: [0, 10] Inverse temperature for softmax choice.
    w_base: [0, 1] Baseline weight for Model-Based control (0=Pure MF, 1=Pure MB).
    oci_mod: [-1, 1] Modulation of 'w' by OCI score. (Positive = OCI increases MB, Negative = OCI increases MF).
    """
    lr, beta, w_base, oci_mod = model_parameters
    n_trials = len(action_1)
    oci_val = oci[0]

    # Calculate the mixing weight based on OCI
    # We clip w to be between 0 and 1 to remain a valid probability weight
    w = w_base + (oci_mod * oci_val)
    w = np.clip(w, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2) # Model-Free Stage 1 values
    q_stage2_mf = np.zeros((2, 2)) # Model-Free Stage 2 values (also used for MB calculation)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based Value: V_MB(s1) = T * max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value: Q_net = w * Q_MB + (1-w) * Q_MF
        q_net_s1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_s1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        state_idx = int(state[trial])

        # --- Stage 2 Choice ---
        # Standard Model-Free choice at stage 2
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        r = reward[trial]
        
        # Stage 2 Update (TD)
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += lr * delta_stage2
        
        # Stage 1 Update (TD)
        # Using the value of the state actually reached (SARSA-like or Q-learning depending on implementation preference)
        # Here we use the value of the chosen action at stage 2 as the target
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Model-Free Learner with OCI-modulated Learning Rate Asymmetry
This model investigates if OCI scores affect how participants learn from positive versus negative feedback. It is often hypothesized that anxiety or compulsivity involves altered sensitivity to punishment or negative prediction errors.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner with asymmetric learning rates for positive and negative prediction errors,
    where the negative learning rate is modulated by OCI.
    
    Hypothesis: OCI scores relate to altered sensitivity to negative outcomes.
    lr_neg = lr_pos * (1 + oci_scale * oci)
    
    Parameters:
    lr_pos: [0, 1] Learning rate for positive prediction errors (PE > 0).
    beta: [0, 10] Inverse temperature.
    oci_scale: [-5, 5] Scaling factor. If positive, higher OCI leads to higher learning from negative PEs.
    """
    lr_pos, beta, oci_scale = model_parameters
    n_trials = len(action_1)
    oci_val = oci[0]
    
    # Calculate negative learning rate based on OCI
    # We ensure lr_neg stays within [0, 1]
    lr_neg_raw = lr_pos * (1.0 + oci_scale * oci_val)
    lr_neg = np.clip(lr_neg_raw, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Choice ---
        # Pure Model-Free policy for Stage 1 in this variant to isolate learning rate effects
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        state_idx = int(state[trial])

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        r = reward[trial]
        
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        
        # Apply asymmetric learning rate
        lr_s2 = lr_pos if delta_stage2 >= 0 else lr_neg
        q_stage2_mf[state_idx, a2] += lr_s2 * delta_stage2
        
        # Stage 1 Update
        # Using Q-learning update (max of next stage)
        max_q_next = np.max(q_stage2_mf[state_idx])
        delta_stage1 = max_q_next - q_stage1_mf[a1]
        
        # Apply asymmetric learning rate
        lr_s1 = lr_pos if delta_stage1 >= 0 else lr_neg
        q_stage1_mf[a1] += lr_s1 * delta_stage1
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Model-Based Learner with OCI-modulated Exploration (Inverse Temperature)
This model posits that OCI affects the "decision noise" or exploration parameter ($\beta$). High OCI might lead to more rigid, deterministic choices (higher $\beta$), while low OCI might imply more stochastic behavior. Alternatively, anxiety might increase noise (lower $\beta$).

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Pure Model-Based learner where the inverse temperature (beta) is modulated by OCI.
    
    Hypothesis: OCI score impacts the determinism of choice (exploration/exploitation balance).
    beta_effective = beta_base * (1 + oci_beta_mod * oci)
    
    Parameters:
    lr: [0, 1] Learning rate for state-value learning.
    beta_base: [0, 10] Baseline inverse temperature.
    oci_beta_mod: [-1, 1] Modulation factor. Positive = OCI makes choices more deterministic (higher beta).
    """
    lr, beta_base, oci_beta_mod = model_parameters
    n_trials = len(action_1)
    oci_val = oci[0]
  
    # Modulate beta based on OCI
    beta_effective = beta_base * (1.0 + oci_beta_mod * oci_val)
    # Ensure beta stays non-negative
    beta_effective = np.maximum(beta_effective, 0.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # We only track Stage 2 values, as a pure MB learner computes Stage 1 values on the fly
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Choice ---
        # Pure Model-Based calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        exp_q1 = np.exp(beta_effective * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        state_idx = int(state[trial])

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta_effective * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
  
        # --- Learning ---
        # Even a "Model-Based" agent usually learns the terminal state values via TD or simple averaging
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += lr * delta_stage2
        
        # Note: No separate Stage 1 Q-value update needed for pure MB, 
        # as it is derived from the transition matrix and Stage 2 values.

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```