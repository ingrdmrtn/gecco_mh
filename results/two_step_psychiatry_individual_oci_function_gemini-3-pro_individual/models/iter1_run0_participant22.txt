Here are three new cognitive models designed to explain the participant's behavior, incorporating the OCI score into the decision-making or learning process.

### Model 1: Hybrid Learner with OCI-Modulated Model-Based/Model-Free Balance
This model assumes that individuals use a mix of Model-Based (planning) and Model-Free (habitual) strategies. The core hypothesis here is that the balance between these two systems (`w`) is influenced by the OCI score. Lower OCI scores (like this participant's) might be associated with a more flexible, Model-Based approach, while higher scores might lean towards habitual, Model-Free control.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL (MB/MF) where the weighting parameter 'w' is modulated by OCI.
    
    Hypothesis: The balance between model-based and model-free control depends on OCI.
    w = w_base + w_mod * oci
    If w is closer to 1, behavior is Model-Based. Closer to 0 is Model-Free.
    
    Parameters:
    learning_rate: [0,1] - Update rate for Q-values.
    beta: [0,10] - Inverse temperature for softmax.
    w_base: [0,1] - Baseline weighting for Model-Based control.
    w_mod: [-1,1] - Modulation of weighting by OCI score.
    """
    learning_rate, beta, w_base, w_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective w and clamp between 0 and 1
    w = w_base + (w_mod * oci_score)
    w = np.clip(w, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # MF Q-values
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based Value calculation
        # Max value of next stage states
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        # Expected value based on transition matrix
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combined Value
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # --- Stage 2 Choice ---
        # Standard Model-Free choice at stage 2
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Learning ---
        # Update Stage 2 Q-values (TD learning)
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Update Stage 1 MF Q-values (TD learning)
        # Note: Using Q(s', a') as the target, consistent with SARSA-like updates often used in these models
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Learning Rate Asymmetry
This model hypothesizes that OCI traits affect how participants learn from positive versus negative outcomes. Specifically, it tests if the learning rate for "disappointing" outcomes (prediction errors < 0) differs from "rewarding" outcomes, and if this asymmetry scales with OCI. High OCI might be associated with hypersensitivity to errors or negative feedback.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner with separate learning rates for positive and negative prediction errors.
    The negative learning rate is modulated by OCI.
    
    Hypothesis: OCI affects sensitivity to negative prediction errors (punishment/lack of reward).
    alpha_pos: Base learning rate for positive PE.
    alpha_neg = alpha_pos * (1 + oci_sens * oci)
    
    Parameters:
    alpha_base: [0,1] - Base learning rate.
    beta: [0,10] - Inverse temperature.
    oci_sens: [0,5] - Sensitivity multiplier for OCI on negative learning rate.
    """
    alpha_base, beta, oci_sens = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2)) # state x action

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Learning ---
        # Stage 2 Update
        delta_stage2 = r - q_stage2[state_idx, a2]
        
        # Determine learning rate based on sign of PE
        if delta_stage2 >= 0:
            eff_alpha = alpha_base
        else:
            # Modulate negative learning rate by OCI
            # If oci_sens is high, negative events cause larger updates for higher OCI
            factor = 1.0 + (oci_sens * oci_score)
            eff_alpha = np.clip(alpha_base * factor, 0.0, 1.0)
            
        q_stage2[state_idx, a2] += eff_alpha * delta_stage2
        
        # Stage 1 Update
        delta_stage1 = q_stage2[state_idx, a2] - q_stage1[a1]
        
        # Apply same logic to Stage 1 PE
        if delta_stage1 >= 0:
            eff_alpha_s1 = alpha_base
        else:
            factor = 1.0 + (oci_sens * oci_score)
            eff_alpha_s1 = np.clip(alpha_base * factor, 0.0, 1.0)

        q_stage1[a1] += eff_alpha_s1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Eligibility Trace (Lambda)
This model introduces an eligibility trace parameter (`lambda`) typically found in TD(lambda) algorithms. This parameter controls how much credit the first-stage choice gets for the second-stage outcome. The hypothesis is that OCI impacts the "credit assignment" window. A higher OCI might lead to stronger or weaker linking of distal outcomes (rewards) to proximal choices (Stage 1 spaceship choice).

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    TD(lambda) learner where the eligibility trace decay (lambda) is modulated by OCI.
    
    Hypothesis: OCI affects how strongly the Stage 1 choice is reinforced by the Stage 2 reward directly.
    lambda = lambda_base + lambda_mod * oci
    
    Parameters:
    learning_rate: [0,1] - Learning rate.
    beta: [0,10] - Inverse temperature.
    lambda_base: [0,1] - Base eligibility trace decay.
    lambda_mod: [-1,1] - Modulation of lambda by OCI.
    """
    learning_rate, beta, lambda_base, lambda_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective lambda
    eff_lambda = lambda_base + (lambda_mod * oci_score)
    eff_lambda = np.clip(eff_lambda, 0.0, 1.0)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = int(action_1[trial])
        state_idx = int(state[trial])

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Learning ---
        # In simple TD(lambda) for this task structure:
        # 1. Update Stage 2 value based on Reward
        delta_2 = r - q_stage2[state_idx, a2]
        q_stage2[state_idx, a2] += learning_rate * delta_2
        
        # 2. Update Stage 1 value. 
        # It receives a direct TD error from the transition (delta_1)
        # AND a portion of the Stage 2 TD error (delta_2) scaled by lambda.
        # This effectively allows the reward 'r' to update q_stage1 directly.
        
        delta_1 = q_stage2[state_idx, a2] - q_stage1[a1] # Note: using updated q_stage2 is common in some variants, here using pre-update logic conceptually or simply summing deltas
        
        # Standard TD(1) or TD(lambda) update logic often combines these:
        # Q1_new = Q1 + alpha * (delta_1 + lambda * delta_2)
        
        total_update_s1 = delta_1 + (eff_lambda * delta_2)
        q_stage1[a1] += learning_rate * total_update_s1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```