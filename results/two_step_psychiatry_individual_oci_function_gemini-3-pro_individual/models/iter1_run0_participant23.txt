Here are three new cognitive models exploring different mechanisms for how OCI scores might influence decision-making in this two-step task, specifically focusing on the balance between model-based/model-free control and learning rates.

### Model 1: Hybrid Model with OCI-modulated Weighting (w)
This model posits that individuals with different OCI scores rely differently on Model-Based (planning) vs. Model-Free (habitual) systems. High OCI might be associated with rigid habits (Model-Free) or over-thinking (Model-Based). This model tests if the mixing weight `w` scales with OCI.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free learner where the mixing weight 'w' is modulated by OCI.
    
    The weight 'w' determines the balance between Model-Based (w=1) and Model-Free (w=0) control.
    Here, w is a function of a baseline and an OCI-dependent shift.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for value updates.
    - beta: [0, 10] Inverse temperature for softmax choice.
    - w_base: [0, 1] Baseline mixing weight (intercept).
    - w_oci_slope: [-1, 1] How much OCI score shifts the weight towards MB or MF.
    """
    learning_rate, beta, w_base, w_oci_slope = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
  
    # Calculate effective w, bounded between 0 and 1
    w = w_base + (w_oci_slope * current_oci)
    w = np.clip(w, 0.0, 1.0)

    # Fixed transition matrix for the task structure
    # A -> X (0.7), Y (0.3); U -> X (0.3), Y (0.7)
    # Indices: A=0, U=1; X=0, Y=1 (mapped from state inputs usually 0/1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Choice ---
        # Model-Based Value: V(s') = max_a Q(s', a)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Value: w * MB + (1-w) * MF
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Choice ---
        # Standard Model-Free Q-learning at stage 2
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # --- Learning ---
        # Stage 2 RPE
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 RPE (TD(0))
        # Note: In a full hybrid model, the MF system updates based on the state actually reached
        # or the value of the next state. Here we use the standard TD error form.
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Dual Learning Rates (Positive vs Negative) Modulated by OCI
This model investigates if OCI scores relate to an asymmetry in learning from positive versus negative outcomes. High OCI is often associated with anxiety and avoidant behavior, which might manifest as a heightened sensitivity (learning rate) to negative prediction errors (punishment/omission of reward) compared to positive ones.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner with separate learning rates for positive/negative prediction errors,
    where the negative learning rate is modulated by OCI.
    
    This tests the hypothesis that higher OCI leads to hyper-learning from failures/omissions.
    
    Parameters:
    - lr_pos: [0, 1] Learning rate for positive prediction errors (RPE > 0).
    - lr_neg_base: [0, 1] Base learning rate for negative prediction errors (RPE < 0).
    - lr_neg_oci_mod: [0, 1] Additional scaling of negative learning rate by OCI.
    - beta: [0, 10] Inverse temperature.
    """
    lr_pos, lr_neg_base, lr_neg_oci_mod, beta = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate effective negative learning rate
    lr_neg = lr_neg_base + (current_oci * lr_neg_oci_mod)
    lr_neg = np.clip(lr_neg, 0.0, 1.0)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2)) # 2 states (planets), 2 actions (aliens)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # --- Learning ---
        # Stage 2 Update
        delta_stage2 = r - q_stage2[s_idx, a2]
        alpha_2 = lr_pos if delta_stage2 > 0 else lr_neg
        q_stage2[s_idx, a2] += alpha_2 * delta_stage2
        
        # Stage 1 Update
        # Using the value of the chosen stage 2 option to drive stage 1 update (SARSA-like)
        delta_stage1 = q_stage2[s_idx, a2] - q_stage1[a1]
        alpha_1 = lr_pos if delta_stage1 > 0 else lr_neg
        q_stage1[a1] += alpha_1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Eligibility Trace (Lambda) Modulated by OCI
This model considers the temporal credit assignment problem. The parameter `lambda` controls how much the second-stage outcome updates the first-stage choice directly (eligibility trace). A higher lambda connects outcomes to distant choices more strongly. We test if OCI modulates this "binding" of distal rewards to proximal choices.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    TD(lambda) Model-Free learner where the eligibility trace decay (lambda) is modulated by OCI.
    
    Lambda determines how much credit the first-stage action receives from the second-stage reward.
    Lambda = 0 is pure TD(0) (Stage 1 updates only from Stage 2 value).
    Lambda = 1 is Monte Carlo (Stage 1 updates directly from Reward).
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - lambda_base: [0, 1] Base eligibility trace parameter.
    - lambda_oci_mod: [-1, 1] Modulation of lambda by OCI.
    """
    learning_rate, beta, lambda_base, lambda_oci_mod = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
  
    # Calculate effective lambda
    eff_lambda = lambda_base + (current_oci * lambda_oci_mod)
    eff_lambda = np.clip(eff_lambda, 0.0, 1.0)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # --- Learning ---
        # 1. Prediction error at Stage 2 (Reward - Q_stage2)
        delta_stage2 = r - q_stage2[s_idx, a2]
        
        # 2. Prediction error at Stage 1 (Q_stage2 - Q_stage1)
        delta_stage1 = q_stage2[s_idx, a2] - q_stage1[a1]
        
        # Update Stage 2
        q_stage2[s_idx, a2] += learning_rate * delta_stage2
        
        # Update Stage 1
        # The update combines the immediate TD error (delta_stage1) 
        # plus the eligibility trace of the second stage error (eff_lambda * delta_stage2)
        q_stage1[a1] += learning_rate * (delta_stage1 + eff_lambda * delta_stage2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```