Here are three new cognitive models that explore different mechanisms by which Obsessive-Compulsive Inventory (OCI) scores might influence decision-making in the two-step task.

### Model 1: OCI-Modulated Model-Based vs. Model-Free Weighting
This model hypothesizes that OCI scores influence the balance between goal-directed (Model-Based) and habitual (Model-Free) control. High OCI might be associated with a stronger reliance on habits (Model-Free) or a deficit in goal-directed planning. Here, the mixing parameter `w` is derived from a baseline and an OCI-dependent modulation.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated MB/MF Weighting.
    
    This model assumes OCI modulates the trade-off between Model-Based (MB) 
    and Model-Free (MF) control strategies.
    
    Parameters:
    - learning_rate: [0, 1] Update rate for Q-values.
    - beta: [0, 10] Inverse temperature (exploration/exploitation).
    - w_base: [0, 1] Baseline weighting for Model-Based control (0=MF, 1=MB).
    - oci_w_mod: [0, 1] Strength of OCI modulation on 'w'.
    """
    learning_rate, beta, w_base, oci_w_mod = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    # Calculate effective w, bounded between 0 and 1.
    # We assume higher OCI might reduce MB control (or increase it, the sign allows flexibility).
    # Here we model it as w = w_base * (1 - oci * oci_w_mod) to test if OCI reduces MB.
    w = w_base * (1.0 - (current_oci * oci_w_mod))
    w = np.clip(w, 0.0, 1.0)

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combined Q-value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        state_idx = int(state[trial])

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        # Update stage 1 MF
        # SARSA-like update for stage 1 using stage 2 Q-value
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1
        
        # Update stage 2 MF
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Driven Learning Rate Asymmetry
This model suggests that OCI affects how individuals learn from positive versus negative prediction errors. High OCI might be associated with hypersensitivity to errors (negative feedback) or rigid beliefs. This model splits the learning rate into a base rate and a penalty/boost derived from OCI, specifically affecting the update when the reward is zero (omission).

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Driven Learning Rate Asymmetry.
    
    This model assumes OCI modulates sensitivity to negative outcomes (reward = 0).
    It uses a purely Model-Based planner for Stage 1 choice to isolate the learning 
    effect in Stage 2 Q-values.
    
    Parameters:
    - lr_pos: [0, 1] Learning rate for positive rewards (reward = 1).
    - beta: [0, 10] Inverse temperature.
    - lr_neg_base: [0, 1] Baseline learning rate for negative outcomes (reward = 0).
    - oci_neg_mod: [0, 1] Modulation of negative learning rate by OCI.
    """
    lr_pos, beta, lr_neg_base, oci_neg_mod = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    # q_stage1_mf is unused in pure MB, but kept for structure consistency
    q_stage1_mf = np.zeros(2) 
    q_stage2_mf = np.zeros((2, 2))

    # Calculate effective negative learning rate
    # People with high OCI might over-update on failure.
    lr_neg = lr_neg_base + (current_oci * oci_neg_mod)
    lr_neg = np.clip(lr_neg, 0.0, 1.0)

    for trial in range(n_trials):

        # policy for the first choice: Pure Model-Based
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        state_idx = int(state[trial])

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        # Update stage 2 MF with asymmetric learning rates
        r = reward[trial]
        delta_stage2 = r - q_stage2_mf[state_idx, int(action_2[trial])]
        
        effective_lr = lr_pos if r > 0 else lr_neg
        q_stage2_mf[state_idx, int(action_2[trial])] += effective_lr * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Randomness (Inverse Temperature)
This model posits that OCI scores relate to decision noise or exploration/exploitation balance. It tests the hypothesis that higher OCI scores might lead to more deterministic (lower temperature, higher beta) behavior due to rigidity, or conversely, more erratic behavior.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Inverse Temperature (Beta).
    
    This model assumes OCI modulates the 'beta' parameter, making choices
    more deterministic (rigid) or more noisy. It uses a hybrid MB/MF structure
    with a fixed weighting 'w' to focus on the beta modulation.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta_base: [0, 10] Baseline inverse temperature.
    - w: [0, 1] Weight for Model-Based control.
    - oci_beta_slope: [0, 5] Slope of OCI effect on beta.
    """
    learning_rate, beta_base, w, oci_beta_slope = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    # Calculate effective beta
    # If oci_beta_slope is positive, high OCI -> higher beta -> more rigid/deterministic choice.
    # We add a small floor to prevent beta from being negative or 0.
    effective_beta = beta_base + (current_oci * oci_beta_slope)
    effective_beta = np.maximum(effective_beta, 0.001)

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(effective_beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        state_idx = int(state[trial])

        # policy for the second choice
        exp_q2 = np.exp(effective_beta * q_stage2_mf[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        # Update stage 1 MF
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1
        
        # Update stage 2 MF
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```