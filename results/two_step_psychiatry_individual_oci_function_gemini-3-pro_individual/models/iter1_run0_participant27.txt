def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-modulated Inverse Temperature (Hybrid Model).
    Hypothesis: OCI score modulates the exploration/exploitation balance (beta).
    High OCI might lead to more deterministic choices (higher beta) or noisier ones,
    independent of the balance between model-based and model-free systems.
    
    Parameters:
    learning_rate: [0, 1] Value update rate.
    beta_base: [0, 10] Baseline inverse temperature.
    beta_oci_slope: [-5, 5] Effect of OCI on beta (beta = base + slope * oci).
    w: [0, 1] Weight for Model-Based control (0=MF, 1=MB).
    """
    learning_rate, beta_base, beta_oci_slope, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Modulate beta
    beta = beta_base + (beta_oci_slope * oci_score)
    # Ensure beta is within reasonable non-negative bounds for softmax
    beta = max(0.0, min(20.0, beta)) 

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based Value calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net_s1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax
        exp_q1 = np.exp(beta * q_net_s1)
        if np.sum(exp_q1) == 0: probs_1 = np.array([0.5, 0.5])
        else: probs_1 = exp_q1 / np.sum(exp_q1)
        
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        s1_choice = int(action_1[trial])
        s2_state = int(state[trial])
        
        # --- Stage 2 Choice ---
        # Stage 2 is purely model-free (terminal state)
        exp_q2 = np.exp(beta * q_stage2_mf[s2_state])
        if np.sum(exp_q2) == 0: probs_2 = np.array([0.5, 0.5])
        else: probs_2 = exp_q2 / np.sum(exp_q2)
        
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        s2_choice = int(action_2[trial])
        r = reward[trial]
        
        # --- Updates ---
        # Stage 2 RPE
        delta_stage2 = r - q_stage2_mf[s2_state, s2_choice]
        q_stage2_mf[s2_state, s2_choice] += learning_rate * delta_stage2
        
        # Stage 1 RPE (TD(0))
        # Note: In this task, Stage 1 MF value is updated towards Stage 2 Value (SARSA-style)
        delta_stage1 = q_stage2_mf[s2_state, s2_choice] - q_stage1_mf[s1_choice]
        q_stage1_mf[s1_choice] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-modulated Eligibility Trace (Pure Model-Free).
    Hypothesis: OCI modulates the 'eligibility trace' (lambda) in a pure model-free learner.
    This determines how strongly the final reward reinforces the first-stage choice directly,
    bypassing the second stage value. High OCI might imply stronger direct reinforcement chains.
    
    Parameters:
    learning_rate: [0, 1] Value update rate.
    beta: [0, 10] Inverse temperature.
    lambda_base: [0, 1] Baseline eligibility trace decay.
    lambda_oci_slope: [-1, 1] Effect of OCI on lambda.
    """
    learning_rate, beta, lambda_base, lambda_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Modulate lambda
    lam = lambda_base + (lambda_oci_slope * oci_score)
    lam = max(0.0, min(1.0, lam))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        if np.sum(exp_q1) == 0: probs_1 = np.array([0.5, 0.5])
        else: probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        s1_choice = int(action_1[trial])
        s2_state = int(state[trial])
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[s2_state])
        if np.sum(exp_q2) == 0: probs_2 = np.array([0.5, 0.5])
        else: probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        s2_choice = int(action_2[trial])
        r = reward[trial]
        
        # --- Updates (TD(lambda)) ---
        # 1. Prediction error at stage 2 (reward - V_stage2)
        delta2 = r - q_stage2[s2_state, s2_choice]
        
        # 2. Prediction error at stage 1 (V_stage2 - V_stage1)
        delta1 = q_stage2[s2_state, s2_choice] - q_stage1[s1_choice]
        
        # Update Stage 2 value
        q_stage2[s2_state, s2_choice] += learning_rate * delta2
        
        # Update Stage 1 value
        # Direct update from next state (TD(0) part)
        q_stage1[s1_choice] += learning_rate * delta1
        # Eligibility trace update from reward (TD(1) part scaled by lambda)
        q_stage1[s1_choice] += learning_rate * lam * delta2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-modulated Asymmetric Learning (Hybrid Model).
    Hypothesis: OCI affects the ratio of learning from negative prediction errors 
    (disappointments/misses) versus positive ones (gains). 
    
    Parameters:
    alpha: [0, 1] Base learning rate (used for positive prediction errors).
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Model-based weight.
    asym_oci_slope: [-5, 5] Modulation of negative learning rate relative to positive.
                    alpha_neg = alpha * (1 + slope * OCI).
    """
    alpha, beta, w, asym_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate negative learning rate
    # Multiplier centered at 1. If slope is positive, high OCI -> higher alpha_neg (oversensitivity to loss).
    neg_multiplier = 1.0 + (asym_oci_slope * oci_score)
    neg_multiplier = max(0.0, min(5.0, neg_multiplier)) 
    
    alpha_pos = alpha
    alpha_neg = alpha * neg_multiplier
    # Clip alpha_neg to [0, 1]
    alpha_neg = max(0.0, min(1.0, alpha_neg))

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_s1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_s1)
        if np.sum(exp_q1) == 0: probs_1 = np.array([0.5, 0.5])
        else: probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        s1_choice = int(action_1[trial])
        s2_state = int(state[trial])
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[s2_state])
        if np.sum(exp_q2) == 0: probs_2 = np.array([0.5, 0.5])
        else: probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        s2_choice = int(action_2[trial])
        r = reward[trial]
        
        # --- Updates ---
        delta_stage2 = r - q_stage2_mf[s2_state, s2_choice]
        lr2 = alpha_pos if delta_stage2 >= 0 else alpha_neg
        q_stage2_mf[s2_state, s2_choice] += lr2 * delta_stage2
        
        delta_stage1 = q_stage2_mf[s2_state, s2_choice] - q_stage1_mf[s1_choice]
        lr1 = alpha_pos if delta_stage1 >= 0 else alpha_neg
        q_stage1_mf[s1_choice] += lr1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss