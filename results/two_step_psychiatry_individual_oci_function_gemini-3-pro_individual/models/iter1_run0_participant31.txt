Here are three new cognitive models exploring different mechanisms for how OCI scores might influence decision-making in the two-step task, specifically focusing on the balance between model-based and model-free control, as well as distinct learning rates for positive and negative outcomes.

### Model 1: Hybrid Model with OCI-modulated Weighting ($w$)
This model hypothesizes that OCI scores influence the balance between Model-Based (planning) and Model-Free (habitual) systems. Specifically, it tests if higher OCI scores lead to a more rigid, habit-driven strategy (lower $w$) or a more goal-directed strategy. Given the participant's low OCI, this model allows the data to determine if low OCI correlates with higher model-based control.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: Hybrid Model-Based/Model-Free with OCI-modulated Weighting.
    
    Hypothesis: The balance between Model-Based (MB) and Model-Free (MF) control
    is modulated by the OCI score. The parameter `w` (weight) is determined by
    a base weight plus an OCI-dependent shift.
    
    Bounds:
    learning_rate: [0,1]
    beta: [0,10]
    w_base: [0,1] - Base weight for Model-Based system.
    w_oci_mod: [-1,1] - How strongly OCI shifts the weight.
    """
    learning_rate, beta, w_base, w_oci_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate mixing weight w, constrained to [0, 1]
    # If w is 1, purely Model-Based. If w is 0, purely Model-Free.
    w = w_base + (w_oci_mod * oci_score)
    w = np.clip(w, 0.0, 1.0)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2) # MF Q-values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # MF Q-values for stage 2 (aliens)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based calculation: V(state) = max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Q-value
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # --- Stage 2 Policy ---
        q_s2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * q_s2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Updates ---
        # SARSA(0) update for Stage 1 MF
        # Note: In standard hybrid models, Stage 1 MF is often updated by the Stage 2 Q-value
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Q-learning update for Stage 2 MF
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Dual Learning Rates (Positive/Negative) Modulated by OCI
This model investigates if OCI scores affect sensitivity to punishment versus reward. It proposes that the learning rate for negative prediction errors (disappointments) is modulated differently by OCI than positive ones, potentially reflecting altered error processing common in obsessive-compulsive traits.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: Asymmetric Learning Rates with OCI modulation on Punishment Sensitivity.
    
    Hypothesis: OCI scores specifically modulate how strongly participants learn 
    from negative prediction errors (when outcomes are worse than expected).
    This is a purely Model-Free approach.
    
    Bounds:
    alpha_pos: [0,1] - Learning rate for positive prediction errors.
    alpha_neg_base: [0,1] - Base learning rate for negative prediction errors.
    beta: [0,10]
    oci_neg_mod: [0, 2] - Multiplier for OCI's effect on negative learning rate.
    """
    alpha_pos, alpha_neg_base, beta, oci_neg_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    # Calculate effective negative learning rate
    # Higher OCI -> Higher sensitivity to negative errors (if mod > 0)
    alpha_neg = alpha_neg_base * (1 + (oci_neg_mod * oci_score))
    alpha_neg = np.clip(alpha_neg, 0.0, 1.0)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        # Stage 1 Choice
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # Stage 2 Choice
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = int(action_2[trial])
        r = reward[trial]
  
        # Update Stage 1
        delta_stage1 = q_stage2[state_idx, a2] - q_stage1[a1]
        lr_1 = alpha_pos if delta_stage1 > 0 else alpha_neg
        q_stage1[a1] += lr_1 * delta_stage1
        
        # Update Stage 2
        delta_stage2 = r - q_stage2[state_idx, a2]
        lr_2 = alpha_pos if delta_stage2 > 0 else alpha_neg
        q_stage2[state_idx, a2] += lr_2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Eligibility Traces with OCI-modulated Decay ($\lambda$)
This model introduces eligibility traces ($\lambda$), which allow reward information to propagate back to the first-stage choice more efficiently than standard one-step updates. The hypothesis is that OCI scores influence the decay rate $\lambda$. A higher $\lambda$ implies treating the two stages as a single complex event (chaining credit assignment), while a lower $\lambda$ treats them as separate events.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: TD(lambda) with OCI-modulated Eligibility Trace Decay.
    
    Hypothesis: The parameter lambda, which controls how much credit is assigned 
    to the first stage choice based on the second stage outcome, varies with OCI.
    This reflects how 'connected' the sequential decisions feel to the participant.
    
    Bounds:
    learning_rate: [0,1]
    beta: [0,10]
    lambda_base: [0,1] - Base eligibility trace decay parameter.
    lambda_oci_slope: [-1,1] - How OCI changes lambda.
    """
    learning_rate, beta, lambda_base, lambda_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective lambda
    lambda_param = lambda_base + (lambda_oci_slope * oci_score)
    lambda_param = np.clip(lambda_param, 0.0, 1.0)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = int(action_2[trial])
        r = reward[trial]
  
        # --- Updates using Eligibility Traces logic (simplified for 2-step) ---
        
        # 1. Prediction error at stage 1 (based on stage 2 value)
        delta1 = q_stage2[state_idx, a2] - q_stage1[a1]
        
        # 2. Prediction error at stage 2 (based on reward)
        delta2 = r - q_stage2[state_idx, a2]
        
        # Update Stage 1:
        # It gets the direct update from delta1
        # PLUS it gets a discounted portion of delta2 (controlled by lambda)
        q_stage1[a1] += learning_rate * delta1 + (learning_rate * lambda_param * delta2)
        
        # Update Stage 2:
        # Standard TD update
        q_stage2[state_idx, a2] += learning_rate * delta2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```