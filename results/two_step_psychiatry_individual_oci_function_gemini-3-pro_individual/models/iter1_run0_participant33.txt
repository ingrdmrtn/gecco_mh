Here are three new cognitive models that explore different mechanistic roles for the OCI score in reinforcement learning, specifically focusing on the balance between model-based and model-free control, learning rates, and noise.

### Model 1: Hybrid Learner with OCI-modulated Model-Based Weight
This model assumes that individuals use a mixture of Model-Based (planning) and Model-Free (habitual) strategies. The core hypothesis here is that OCI symptoms might relate to a rigidity or reliance on habits (Model-Free) versus goal-directed planning. This model tests if the mixing weight `w` is a function of the OCI score.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based / Model-Free Learner with OCI-modulated Mixing Weight.
    
    The weight 'w' determines the balance between Model-Based (w=1) and Model-Free (w=0)
    control. The effective w is a linear function of the OCI score.
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    w_intercept: [0, 1] (Base mixing weight for OCI=0)
    w_slope: [-1, 1] (How OCI changes the reliance on Model-Based control)
    """
    learning_rate, beta, w_intercept, w_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate mixing weight w, clamped between 0 and 1
    w = w_intercept + (w_slope * oci_score)
    w = np.clip(w, 0.0, 1.0)

    # Fixed transition matrix as per task description: A->X (0.7), U->Y (0.7)
    # actions: 0=A, 1=U; states: 0=X, 1=Y
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2) # Model-free Q-values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Model-free Q-values for stage 2 (State x Action)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # 1. Calculate Model-Based values
        # Max Q-value for each state in stage 2
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        # Bellman equation: Transition prob * max future value
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Integrate MB and MF values using weight w
        q_net_stage1 = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # 3. Softmax choice
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # Observe state
        s_idx = int(state[trial])

        # --- Stage 2 Policy ---
        # Standard softmax on stage 2 Q-values
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # --- Learning / Updating ---
        r = reward[trial]
        
        # TD Update for Stage 2
        # Prediction error: Reward - expected value
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # TD Update for Stage 1 (SARSA-like update using stage 2 value as proxy for reward)
        # Note: In standard Daw 2-step, Stage 1 MF is often updated by the Q-value of the chosen stage 2 option
        # or the raw reward (TD(1)). Here we use a simple TD(0) to the value of the state entered.
        # Using the value of the state chosen in step 2 (q_stage2_mf[s_idx, a2]) as the target.
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Dual Learning Rates (Positive vs Negative) Modulated by OCI
This model hypothesizes that OCI affects sensitivity to feedback differently for rewards (positive prediction errors) versus omissions (negative prediction errors). High OCI might correlate with "perfectionism" or high sensitivity to failure (negative errors).

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free Learner with Asymmetric Learning Rates modulated by OCI.
    
    This model separates learning from positive prediction errors (alpha_pos)
    and negative prediction errors (alpha_neg). The OCI score scales the 
    sensitivity to negative prediction errors specifically.
    
    Bounds:
    alpha_pos: [0, 1] (Learning rate for positive errors)
    alpha_neg_base: [0, 1] (Base learning rate for negative errors)
    alpha_neg_oci_scale: [0, 2] (Multiplier for OCI effect on negative learning rate)
    beta: [0, 10]
    """
    alpha_pos, alpha_neg_base, alpha_neg_oci_scale, beta = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective negative learning rate
    # If OCI is high, this rate increases/decreases based on the scale param
    alpha_neg = alpha_neg_base * (1 + (alpha_neg_oci_scale * oci_score))
    alpha_neg = np.clip(alpha_neg, 0.0, 1.0)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Simple Model-Free Q-values
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        s_idx = int(state[trial])
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        r = reward[trial]
        
        # --- Updating Stage 2 ---
        pe2 = r - q_stage2[s_idx, a2]
        lr2 = alpha_pos if pe2 >= 0 else alpha_neg
        q_stage2[s_idx, a2] += lr2 * pe2
        
        # --- Updating Stage 1 ---
        # Using the updated stage 2 value as the target (TD-like)
        pe1 = q_stage2[s_idx, a2] - q_stage1[a1]
        lr1 = alpha_pos if pe1 >= 0 else alpha_neg
        q_stage1[a1] += lr1 * pe1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Inverse Temperature (Decision Noise)
This model tests the hypothesis that OCI relates to the consistency of choices (exploration vs. exploitation). A higher OCI might lead to more deterministic (lower noise/higher beta) behavior, or conversely, more uncertainty. Here, the inverse temperature `beta` is a function of the OCI score.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Pure Model-Based Learner with OCI-modulated Decision Noise (Beta).
    
    This model assumes the participant is fully Model-Based (planning using the 
    transition matrix) but their ability to deterministically select the best 
    option (beta) is modulated by their OCI score.
    
    Bounds:
    learning_rate: [0, 1]
    beta_base: [0, 10] (Base inverse temperature)
    beta_oci_coeff: [-5, 5] (How OCI adds/subtracts from beta)
    """
    learning_rate, beta_base, beta_oci_coeff = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective beta
    # beta = beta_base + coeff * OCI
    # We must ensure beta stays non-negative.
    beta_eff = beta_base + (beta_oci_coeff * oci_score)
    beta_eff = np.maximum(beta_eff, 0.0) # Ensure non-negative
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Only need Stage 2 values for a pure MB model, as Stage 1 is derived
    q_stage2 = np.zeros((2, 2)) 

    for trial in range(n_trials):
        # --- Stage 1 (Model-Based) ---
        # Compute MB values: T * max(Q2)
        max_q2 = np.max(q_stage2, axis=1)
        q_stage1_mb = transition_matrix @ max_q2
        
        exp_q1 = np.exp(beta_eff * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        s_idx = int(state[trial])
        
        # --- Stage 2 ---
        exp_q2 = np.exp(beta_eff * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        r = reward[trial]
        
        # --- Update ---
        # Only Stage 2 values need direct updating from reward
        delta = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += learning_rate * delta
        
        # Stage 1 values are computed on the fly in the next trial based on updated Q2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```