Here are three new cognitive models exploring different mechanisms for how OCI scores might influence decision-making in this two-step task.

### Model 1: Hybrid RL with OCI-modulated Model-Based Weighting
This model hypothesizes that OCI symptoms influence the balance between Model-Based (planning) and Model-Free (habitual) control. High OCI might be associated with a reliance on habits (Model-Free) or rigidity, effectively reducing the weight parameter `w` that mixes the two systems.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL with OCI-modulated Model-Based Weighting.
    
    This model combines Model-Based (MB) and Model-Free (MF) reinforcement learning.
    The mixing weight 'w' determines the balance: w=1 is pure MB, w=0 is pure MF.
    We hypothesize that OCI score modulates this weight. High OCI might lead to 
    reduced model-based control (more habitual/compulsive behavior), so the 
    effective weight is reduced by the OCI score.

    Parameters:
    - learning_rate: Rate at which Q-values are updated.
    - beta: Inverse temperature for softmax.
    - w_base: Baseline weighting for Model-Based system (0 to 1).
    - w_oci_penalty: Reduction in MB weighting per unit of OCI.
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    w_base: [0, 1]
    w_oci_penalty: [0, 1]
    """
    learning_rate, beta, w_base, w_oci_penalty = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective weight w, clamped between 0 and 1
    w = w_base - (w_oci_penalty * oci_score)
    w = np.clip(w, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_mf_stage1 = np.zeros(2) # Model-free stage 1 values
    q_mb_stage1 = np.zeros(2) # Model-based stage 1 values
    q_stage2 = np.zeros((2, 2)) # Stage 2 values (shared by MB and MF logic)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based valuation: V(state) = max(Q_stage2(state, action))
        max_q_stage2 = np.max(q_stage2, axis=1)
        # Bellman equation using known transition matrix
        q_mb_stage1 = transition_matrix @ max_q_stage2
        
        # Hybrid valuation
        q_net_stage1 = w * q_mb_stage1 + (1 - w) * q_mf_stage1
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]

        # --- Stage 2 Policy ---
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # --- Learning ---
        # Update Stage 2 Q-values (Standard TD)
        a2 = int(action_2[trial])
        pe_2 = reward[trial] - q_stage2[state_idx, a2]
        q_stage2[state_idx, a2] += learning_rate * pe_2
        
        # Update Stage 1 Model-Free Q-values (TD(0))
        # Note: In full Daw task, eligibility traces are often used, but simple TD(0) 
        # is often sufficient for basic MF representation in simplified models.
        a1 = int(action_1[trial])
        pe_1 = q_stage2[state_idx, a2] - q_mf_stage1[a1]
        q_mf_stage1[a1] += learning_rate * pe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Learning Rates (Positive/Negative asymmetry)
This model posits that OCI scores affect how participants learn from positive versus negative outcomes. Specifically, individuals with higher compulsivity might be hyper-sensitive to errors (negative prediction errors) or less sensitive to rewards, altering their learning dynamics.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Asymmetric Learning Rate Model modulated by OCI.
    
    This model separates learning rates for positive prediction errors (better than expected)
    and negative prediction errors (worse than expected). The OCI score modulates the 
    learning rate for negative prediction errors specifically, hypothesizing that 
    compulsivity is linked to error sensitivity or avoidance learning.
    
    Parameters:
    - alpha_pos: Learning rate for positive prediction errors.
    - alpha_neg_base: Baseline learning rate for negative prediction errors.
    - alpha_neg_oci_scale: Scaling factor for OCI effect on negative learning rate.
    - beta: Inverse temperature.
    
    Bounds:
    alpha_pos: [0, 1]
    alpha_neg_base: [0, 1]
    alpha_neg_oci_scale: [0, 1]
    beta: [0, 10]
    """
    alpha_pos, alpha_neg_base, alpha_neg_oci_scale, beta = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective negative learning rate
    alpha_neg = alpha_neg_base + (alpha_neg_oci_scale * oci_score)
    alpha_neg = np.clip(alpha_neg, 0.0, 1.0)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Pure Model-Free Q-values for simplicity to isolate learning rate effects
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        # Stage 1 Choice
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]

        # Stage 2 Choice
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # Update Stage 2
        a2 = int(action_2[trial])
        pe_2 = reward[trial] - q_stage2[state_idx, a2]
        if pe_2 >= 0:
            q_stage2[state_idx, a2] += alpha_pos * pe_2
        else:
            q_stage2[state_idx, a2] += alpha_neg * pe_2

        # Update Stage 1
        a1 = int(action_1[trial])
        pe_1 = q_stage2[state_idx, a2] - q_stage1[a1]
        if pe_1 >= 0:
            q_stage1[a1] += alpha_pos * pe_1
        else:
            q_stage1[a1] += alpha_neg * pe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Uncertainty-Weighted Exploration with OCI Modulation
This model suggests that OCI relates to intolerance of uncertainty. Instead of just tracking value, the agent tracks uncertainty (variance) of the options. OCI modulates the 'uncertainty bonus' (or penalty) in the decision rule. High OCI might lead to risk aversion (avoiding uncertain options) or excessive checking (seeking uncertain options to resolve doubt).

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Uncertainty-Weighted Exploration with OCI Modulation.
    
    This model tracks both the Q-value and the uncertainty (variance) of the Q-value estimates.
    The decision policy includes an uncertainty term (Kalman Filter-like approximation).
    The parameter 'phi' controls whether uncertainty is sought (exploration) or avoided.
    Here, 'phi' is a function of OCI, testing if compulsivity relates to uncertainty intolerance.
    
    Parameters:
    - learning_rate: Rate at which Q-values are updated.
    - beta: Inverse temperature.
    - phi_base: Baseline uncertainty weight.
    - phi_oci_mod: Modulation of uncertainty weight by OCI.
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    phi_base: [-1, 1] (Negative = avoid uncertainty, Positive = seek it)
    phi_oci_mod: [-1, 1]
    """
    learning_rate, beta, phi_base, phi_oci_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Effective phi
    phi = phi_base + (phi_oci_mod * oci_score)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2) + 0.5 # Initialize at 0.5
    u_stage1 = np.ones(2)        # Initial high uncertainty
    
    q_stage2 = np.zeros((2, 2)) + 0.5
    u_stage2 = np.ones((2, 2))

    for trial in range(n_trials):
        # Stage 1 Choice: Q + phi * Uncertainty
        # We use a simple decay heuristic for uncertainty similar to Pearce-Hall or Kalman gain proxy
        
        vals_1 = q_stage1 + phi * u_stage1
        exp_q1 = np.exp(beta * vals_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]

        # Stage 2 Choice
        state_idx = int(state[trial])
        vals_2 = q_stage2[state_idx] + phi * u_stage2[state_idx]
        exp_q2 = np.exp(beta * vals_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # Learning
        a2 = int(action_2[trial])
        a1 = int(action_1[trial])

        # Update Stage 2
        pe_2 = reward[trial] - q_stage2[state_idx, a2]
        q_stage2[state_idx, a2] += learning_rate * pe_2
        # Uncertainty decreases for chosen option, increases slightly for unchosen (forgetting)
        u_stage2[state_idx, a2] *= (1 - learning_rate) 
        u_stage2[state_idx, 1-a2] = min(1.0, u_stage2[state_idx, 1-a2] + 0.01)

        # Update Stage 1
        pe_1 = q_stage2[state_idx, a2] - q_stage1[a1]
        q_stage1[a1] += learning_rate * pe_1
        u_stage1[a1] *= (1 - learning_rate)
        u_stage1[1-a1] = min(1.0, u_stage1[1-a1] + 0.01)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```