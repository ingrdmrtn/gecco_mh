Here are three new cognitive models exploring different mechanisms for how Obsessive-Compulsive symptoms (OCI) might influence decision-making in the two-step task.

### Model 1: OCI-Driven Stickiness and Beta Modulation
This model hypothesizes that higher OCI leads to "stickiness" (repetitive choice behavior regardless of reward) and potentially higher decision noise (lower beta) due to anxiety or doubt. Instead of modulating the MB/MF trade-off directly, OCI here acts as a constraint on switching behavior.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Driven Choice Stickiness and Beta Modulation.
    
    Hypothesis:
    1. Higher OCI increases choice 'stickiness' (perseveration), making it harder to switch 
       from the previous action regardless of value.
    2. Higher OCI increases decision noise (lowers inverse temperature beta) due to doubt/anxiety.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for Q-values.
    - beta_base: [0, 10] Base inverse temperature.
    - w: [0, 1] Weighting for Model-Based system (fixed, not modulated by OCI here).
    - stick_base: [0, 5] Base stickiness parameter.
    
    Transformations:
    - effective_stickiness = stick_base * (1 + oci) -> Higher OCI amplifies repetition.
    - effective_beta = beta_base / (1 + oci) -> Higher OCI increases randomness/doubt.
    """
    learning_rate, beta_base, w, stick_base = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # OCI modulations
    effective_stickiness = stick_base * (1.0 + oci_score)
    effective_beta = beta_base / (1.0 + oci_score) # Higher OCI -> Lower Beta (more noise)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Store previous action for stickiness
    prev_action = -1 

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness bonus to the Q-value of the previously chosen action
        if prev_action != -1:
            q_net[prev_action] += effective_stickiness
            
        exp_q1 = np.exp(effective_beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(effective_beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        # Stage 2 update (TD)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Stage 1 update (TD)
        # Using simple SARSA-like update for MF stage 1 based on stage 2 value
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Update previous action for next trial
        prev_action = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Transition Learning (Distorted Model)
This model assumes that OCI affects how the "Model" in Model-Based RL is learned or perceived. Specifically, high OCI might be associated with a distrust of the probabilistic nature of the task, leading the participant to treat the transition matrix as more deterministic or more uncertain than it is, effectively distorting the `transition_matrix` used for planning.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Modulated Transition Probability Distortion.
    
    Hypothesis:
    Participants with higher OCI might perceive the transition structure differently.
    This model assumes OCI distorts the perceived transition matrix used for MB planning.
    Specifically, high OCI might lead to 'over-interpreting' the common transition 
    (making the model more deterministic).
    
    Parameters:
    - learning_rate: [0, 1]
    - beta: [0, 10]
    - w: [0, 1] Weight for MB.
    - distortion_factor: [0, 1] How much OCI distorts the transition matrix.
    
    Transformations:
    - perceived_prob = 0.7 + (distortion_factor * oci * 0.3)
      If distortion is high and OCI is high, 0.7 becomes closer to 1.0 (deterministic belief).
    """
    learning_rate, beta, w, distortion_factor = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate distorted transition matrix
    # Standard is 0.7. distortion pushes this towards 1.0 based on OCI.
    # Max OCI is approx 1 usually, so we scale carefully.
    base_prob = 0.7
    # If oci=1 and distortion=1, new_prob = 0.7 + 0.3 = 1.0
    new_prob = base_prob + (distortion_factor * oci_score * (1.0 - base_prob))
    if new_prob > 0.99: new_prob = 0.99
    
    # Symmetric transition matrix based on distorted probability
    transition_matrix = np.array([[new_prob, 1-new_prob], [1-new_prob, new_prob]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        
        # MB calculation uses the DISTORTED matrix
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Standard TD(0) update for Stage 1 MF
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Dependent Asymmetric Learning Rates
This model suggests that OCI is related to how individuals process positive versus negative prediction errors. Individuals with high OCI (often comorbid with anxiety) might be hyper-sensitive to negative outcomes (losses or lack of reward) or less sensitive to gains.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Dependent Asymmetric Learning (Pos/Neg split).
    
    Hypothesis:
    OCI affects the balance between learning from positive rewards vs negative outcomes (0 reward).
    High OCI participants might have a heightened learning rate for negative prediction errors 
    (fear of failure/checking behavior) compared to positive ones.
    
    Parameters:
    - lr_pos: [0, 1] Learning rate for positive prediction errors (RPE > 0).
    - lr_neg_base: [0, 1] Base learning rate for negative prediction errors (RPE < 0).
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] MB/MF weight.
    - oci_sens: [0, 5] Sensitivity of negative learning rate to OCI.
    
    Transformations:
    - lr_neg = lr_neg_base * (1 + oci_sens * oci)
      Higher OCI amplifies the learning rate when things go wrong (or don't yield reward).
    """
    lr_pos, lr_neg_base, beta, w, oci_sens = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate OCI-modulated negative learning rate
    lr_neg = lr_neg_base * (1.0 + oci_sens * oci_score)
    if lr_neg > 1.0: lr_neg = 1.0

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        # Stage 2 Update with Asymmetric Learning
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        if delta_stage2 >= 0:
            current_lr = lr_pos
        else:
            current_lr = lr_neg
            
        q_stage2_mf[state_idx, action_2[trial]] += current_lr * delta_stage2
        
        # Stage 1 Update with Asymmetric Learning
        # Note: Using stage 2 value to drive stage 1 MF update
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        if delta_stage1 >= 0:
            current_lr_s1 = lr_pos
        else:
            current_lr_s1 = lr_neg
            
        q_stage1_mf[action_1[trial]] += current_lr_s1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```