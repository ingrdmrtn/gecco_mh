Here are three new cognitive models exploring different mechanisms by which Obsessive-Compulsive symptoms (OCI) might influence decision-making in the two-step task.

### Model 1: OCI-Modulated Model-Based Control
This model hypothesizes that individuals with higher OCI scores might rely less on flexible, model-based planning (mental simulation of the task structure) and more on habitual, model-free learning. This aligns with theories of compulsivity as a dominance of habit over goal-directed control. Here, the weight parameter `w` (balancing Model-Based vs. Model-Free) is not fixed but is a function of the OCI score.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Model-Based Control.
    
    This model posits that OCI symptoms interfere with goal-directed (model-based)
    planning. The mixing weight 'w' between model-based and model-free values
    decreases as OCI increases.
    
    w_effective = w_max * (1 - oci_penalty * oci)
    
    Parameters:
    learning_rate: [0, 1] Update rate for value learning.
    beta: [0, 10] Inverse temperature (softness of choice).
    w_max: [0, 1] The maximum model-based weight possible (at OCI=0).
    oci_penalty: [0, 1] How strongly OCI reduces the model-based weight.
    
    Bounds:
    learning_rate: [0,1]
    beta: [0,10]
    w_max: [0,1]
    oci_penalty: [0,1]
    """
    learning_rate, beta, w_max, oci_penalty = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    # Calculate effective weight w based on OCI
    # As OCI increases, w decreases (more model-free/habitual)
    w = w_max * (1.0 - oci_penalty * current_oci)
    # Ensure w stays within [0, 1]
    w = np.clip(w, 0.0, 1.0)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Weighted mixture of MB and MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # Update Stage 1 MF values (TD(1) logic simplified)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] = q_stage1_mf[a1] + learning_rate * delta_stage1
        
        # Update Stage 2 MF values
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] = q_stage2_mf[s_idx, a2] + learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Driven Learning Rate Asymmetry
This model suggests that OCI is related to an over-sensitivity to negative outcomes (or lack of reward), leading to different learning rates for positive vs. negative prediction errors. Specifically, high OCI might amplify learning from "disappointment" (negative prediction errors) relative to positive ones, reflecting a perfectionistic or anxiety-driven learning style.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Driven Learning Rate Asymmetry.
    
    This model assumes OCI affects how participants update values based on 
    prediction errors (PE). Specifically, OCI scales the learning rate 
    for negative PEs (punishment/omission), potentially making the agent 
    more sensitive to failure.
    
    alpha_neg = alpha_pos * (1 + oci_scale * oci)
    
    Parameters:
    alpha_pos: [0, 1] Base learning rate for positive prediction errors.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Model-based weight (fixed).
    oci_scale: [0, 5] Multiplier for OCI impact on negative learning rate.
    
    Bounds:
    alpha_pos: [0,1]
    beta: [0,10]
    w: [0,1]
    oci_scale: [0,5]
    """
    alpha_pos, beta, w, oci_scale = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Calculate negative learning rate based on OCI
    # Higher OCI -> higher learning rate for negative outcomes
    alpha_neg = alpha_pos * (1.0 + oci_scale * current_oci)
    alpha_neg = np.clip(alpha_neg, 0.0, 1.0) # Ensure it doesn't exceed 1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Stage 1 Choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2 Choice
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # Update Stage 1
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        lr_1 = alpha_pos if delta_stage1 >= 0 else alpha_neg
        q_stage1_mf[a1] = q_stage1_mf[a1] + lr_1 * delta_stage1
        
        # Update Stage 2
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        lr_2 = alpha_pos if delta_stage2 >= 0 else alpha_neg
        q_stage2_mf[s_idx, a2] = q_stage2_mf[s_idx, a2] + lr_2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Scaled Exploration (Inverse Temperature)
This model tests the hypothesis that OCI relates to rigidity in choice stochasticity. Instead of affecting learning or planning weights, OCI might increase the `beta` (inverse temperature) parameter. A higher beta means decisions are more deterministic (exploitative) and less exploratory. High OCI individuals might be less willing to explore uncertain options, sticking rigidly to the currently perceived best option.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Scaled Exploration (Inverse Temperature).
    
    This model proposes that OCI reduces exploration. The inverse temperature
    (beta) is scaled by the OCI score. Higher OCI leads to higher beta,
    resulting in more deterministic (rigid) choices and less random exploration.
    
    beta_effective = beta_base * (1 + oci_stiffness * oci)
    
    Parameters:
    learning_rate: [0, 1] Update rate.
    beta_base: [0, 10] Base inverse temperature.
    w: [0, 1] Model-based weight.
    oci_stiffness: [0, 5] How much OCI increases beta (rigidity).
    
    Bounds:
    learning_rate: [0,1]
    beta_base: [0,10]
    w: [0,1]
    oci_stiffness: [0,5]
    """
    learning_rate, beta_base, w, oci_stiffness = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    # Calculate effective beta
    # Higher OCI -> Higher Beta -> More deterministic/rigid behavior
    beta_eff = beta_base * (1.0 + oci_stiffness * current_oci)
    # Cap beta to prevent numerical overflow in exp()
    beta_eff = np.clip(beta_eff, 0.0, 20.0) 

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Stage 1 Choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_eff * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2 Choice
        exp_q2 = np.exp(beta_eff * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # Update Stage 1
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] = q_stage1_mf[a1] + learning_rate * delta_stage1
        
        # Update Stage 2
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] = q_stage2_mf[s_idx, a2] + learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```