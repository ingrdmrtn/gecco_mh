Here are three new cognitive models that explore different mechanisms for how OCI scores might interact with decision-making in the two-step task.

### Model 1: Hybrid Learner with OCI-modulated Model-Based Weighting
This model tests the hypothesis that OCI symptoms relate to the balance between Model-Based (planning) and Model-Free (habitual) control. It proposes that higher OCI scores might lead to a greater reliance on the Model-Based system (or potentially Model-Free, depending on the fitted parameter sign), modulating the mixing weight `w`.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: Hybrid Learner with OCI-modulated Model-Based Weighting.
    
    This model combines Model-Based (MB) and Model-Free (MF) reinforcement learning.
    The mixing parameter 'w' determines the relative contribution of the MB system.
    Here, 'w' is linear function of the OCI score, allowing OCI to shift the
    balance between goal-directed planning and habitual responding.
    
    Parameters:
    - learning_rate: Update rate for Q-values [0, 1]
    - beta: Inverse temperature for softmax choice [0, 10]
    - w_intercept: Base weight for MB system when OCI is 0 [0, 1]
    - w_slope: Effect of OCI on MB weight [-1, 1] (Bounded to ensure w stays in [0,1])
    """
    learning_rate, beta, w_intercept, w_slope = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate mixing weight w, clamped between 0 and 1
    w = w_intercept + (w_slope * current_oci)
    w = np.clip(w, 0.0, 1.0)
    
    # Transition matrix (fixed for this task structure)
    # A -> 70% X (0), 30% Y (1); U -> 30% X (0), 70% Y (1)
    # Rows are actions (A, U), Columns are states (X, Y)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2) # Model-free Q-values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Q-values for stage 2 (2 states, 2 actions)
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        # Model-Based Value Calculation
        # Max Q value for each state in stage 2
        max_q_stage2 = np.max(q_stage2_mf, axis=1) 
        # Bellman equation using known transition structure
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        qs_stage2 = q_stage2_mf[s_idx]
        exp_q2 = np.exp(beta * qs_stage2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        # Stage 2 RPE
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 RPE (Model-Free only)
        # Using the value of the state actually reached (SARSA-like logic for the transition)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        
        # Update Stage 1 MF Q-value
        # Note: Standard hybrid models often use eligibility traces (lambda). 
        # Here we simplify to direct TD(1) style update for the MF component or just TD(0)
        # We will use a simple accumulation of stage 1 and stage 2 errors for MF update
        q_stage1_mf[a1] += learning_rate * delta_stage1 + learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-modulated Learning Rate Asymmetry
This model investigates if OCI scores relate to how participants learn from positive versus negative outcomes. It posits that individuals with different OCI levels might exhibit a bias in learning rate, potentially being more sensitive to punishments (lack of reward) or rewards.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-modulated Learning Rate Asymmetry.
    
    This model is a pure Model-Free learner (TD(1)) but splits the learning rate
    into positive (alpha_pos) and negative (alpha_neg) components.
    Crucially, the asymmetry is modulated by OCI.
    
    Parameters:
    - alpha_base: Base learning rate [0, 1]
    - alpha_asym_oci: OCI-dependent asymmetry parameter [-1, 1].
                      If positive, higher OCI increases learning from positive RPEs relative to negative.
                      If negative, higher OCI increases learning from negative RPEs.
    - beta: Inverse temperature [0, 10]
    - eligibility: Eligibility trace decay parameter (lambda) [0, 1]
    """
    alpha_base, alpha_asym_oci, beta, eligibility = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        # Calculate RPEs
        delta1 = q_stage2[s_idx, a2] - q_stage1[a1]
        delta2 = r - q_stage2[s_idx, a2]
        
        # Determine effective learning rates based on sign of final outcome (delta2)
        # This is a simplification: often asymmetry is applied to each delta separately.
        # Here we apply it to the final reward outcome signal strength.
        
        # Calculate alpha_pos and alpha_neg based on OCI
        # We model this as: alpha_pos = base * (1 + mod*OCI), alpha_neg = base * (1 - mod*OCI)
        # Clamped to [0, 1]
        
        mod = alpha_asym_oci * current_oci
        alpha_pos = np.clip(alpha_base * (1 + mod), 0, 1)
        alpha_neg = np.clip(alpha_base * (1 - mod), 0, 1)

        # Update Stage 2
        lr_2 = alpha_pos if delta2 > 0 else alpha_neg
        q_stage2[s_idx, a2] += lr_2 * delta2
        
        # Update Stage 1
        # TD(1) update logic: Stage 1 value accumulates both prediction errors
        # We apply the learning rate corresponding to the specific error sign for precision,
        # or generally based on the trial outcome. Let's use trial outcome for consistency.
        lr_1 = alpha_pos if (delta1 + delta2) > 0 else alpha_neg
        
        q_stage1[a1] += lr_1 * (delta1 + (eligibility * delta2))

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-modulated Inverse Temperature (Exploration/Exploitation)
This model posits that OCI symptoms affect the randomness of choice (exploration vs. exploitation). High OCI might be associated with more rigid, deterministic behavior (higher beta), or conversely, more uncertainty-driven exploration.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-modulated Inverse Temperature (Exploration/Exploitation).
    
    This model uses a standard hybrid MB/MF architecture (fixed weight w),
    but the inverse temperature parameter (beta) is a function of the OCI score.
    This tests if OCI affects the 'noise' or determinism in decision making.
    
    Parameters:
    - learning_rate: Learning rate [0, 1]
    - w: Fixed mixing weight for MB vs MF [0, 1]
    - beta_intercept: Base inverse temperature [0, 10]
    - beta_slope: Effect of OCI on beta [-5, 5]
    """
    learning_rate, w, beta_intercept, beta_slope = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate beta based on OCI
    # We use an exponential transform or simple linear clipping to ensure beta is positive
    # Linear clipping is safer for stability
    beta = beta_intercept + (beta_slope * current_oci)
    beta = np.clip(beta, 0.0, 20.0) # Allow slightly higher upper bound for very deterministic
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        qs_stage2 = q_stage2_mf[s_idx]
        exp_q2 = np.exp(beta * qs_stage2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1 + learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```