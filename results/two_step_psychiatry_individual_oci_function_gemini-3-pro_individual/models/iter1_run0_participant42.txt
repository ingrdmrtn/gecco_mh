Here are three new cognitive models. These models investigate how Obsessive-Compulsive Inventory (OCI) scores might modulate different computational mechanisms: the rate of belief updating (Learning Rate), the randomness of choice (Inverse Temperature), and the temporal credit assignment (Eligibility Trace).

### Model 1: OCI Modulates Learning Rate
This model hypothesizes that OCI scores affect the "volatility" or "rigidity" of value updates. High OCI might lead to faster updating (hypersensitivity to recent errors) or slower updating (rigidity/habit).

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid learner where OCI modulates the Learning Rate.
    
    Hypothesis: OCI scores correlate with the speed of value updating (rigidity or volatility).
    
    Parameters:
    - lr_base: [0, 1] Base learning rate.
    - lr_oci: [-1, 1] Change in learning rate per unit of OCI.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight for Model-Based (1) vs Model-Free (0).
    - stickiness: [0, 5] Choice stickiness (perseveration).
    """
    lr_base, lr_oci, beta, w, stickiness = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective learning rate based on OCI
    learning_rate = lr_base + (lr_oci * oci_score)
    learning_rate = np.clip(learning_rate, 0.0, 1.0) 

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # state x action

    last_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net
        if last_action_1 != -1:
            logits_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_action_1 = action_1[trial]
        state_idx = int(state[trial])
        
        # --- Stage 2 Policy ---
        logits_2 = beta * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        # Prediction error at transition (Stage 1 -> Stage 2)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Prediction error at outcome (Stage 2 -> Reward)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI Modulates Inverse Temperature (Beta)
This model hypothesizes that OCI scores affect the determinism of choices. High OCI might relate to anxiety-driven randomness (low beta) or compulsive rigidity (high beta).

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid learner where OCI modulates the Inverse Temperature (Beta).
    
    Hypothesis: OCI scores correlate with the determinism of choices (exploration vs exploitation).
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta_base: [0, 10] Base inverse temperature.
    - beta_oci: [-5, 5] Change in beta per unit of OCI.
    - w: [0, 1] Mixing weight for Model-Based (1) vs Model-Free (0).
    - stickiness: [0, 5] Choice stickiness.
    """
    learning_rate, beta_base, beta_oci, w, stickiness = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective beta based on OCI
    beta = beta_base + (beta_oci * oci_score)
    beta = np.clip(beta, 0.0, 20.0) # Upper bound to prevent overflow

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    last_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net
        if last_action_1 != -1:
            logits_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_action_1 = action_1[trial]
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        logits_2 = beta * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI Modulates Eligibility Trace (Lambda)
This model hypothesizes that OCI affects how far back the reward credit is assigned. It modulates the $\lambda$ parameter in a TD($\lambda$) update, controlling how much the final reward reinforces the first-stage choice directly.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid learner where OCI modulates the Eligibility Trace (Lambda).
    
    Hypothesis: OCI scores correlate with how strongly distal rewards (stage 2 outcome) 
    reinforce proximal choices (stage 1), representing temporal credit assignment.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight for Model-Based (1) vs Model-Free (0).
    - lambda_base: [0, 1] Base eligibility trace parameter.
    - lambda_oci: [-1, 1] Change in lambda per unit of OCI.
    """
    learning_rate, beta, w, lambda_base, lambda_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective lambda based on OCI
    lam = lambda_base + (lambda_oci * oci_score)
    lam = np.clip(lam, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    # Note: Stickiness is omitted to keep parameter count to 5 and focus on lambda.

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net
            
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        logits_2 = beta * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        # TD error at transition
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        # TD error at outcome
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Update Stage 1: Standard TD(0) part + Eligibility Trace part (TD(lambda))
        # Q1 = Q1 + alpha * (delta1 + lambda * delta2)
        q_stage1_mf[action_1[trial]] += learning_rate * (delta_stage1 + lam * delta_stage2)
        
        # Update Stage 2: Standard TD(0)
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```