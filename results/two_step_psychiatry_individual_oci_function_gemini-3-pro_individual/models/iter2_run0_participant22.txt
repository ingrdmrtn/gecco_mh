Here are three new cognitive models exploring different mechanisms for how OCI scores might interact with reinforcement learning in a two-step task.

### Model 1: Hybrid Learner with OCI-modulated Model-Based Weight
This model hypothesizes that the balance between Model-Based (planning) and Model-Free (habitual) control is influenced by OCI traits. Specifically, it tests if the mixing parameter `w` (which determines the weight of model-based values) is modulated by the OCI score. A base `w` is modified by the OCI score, allowing the degree of goal-directed planning to vary with obsessive-compulsive tendencies.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL model where the Model-Based weight (w) is modulated by OCI.
    
    Parameters:
    learning_rate: [0,1] - Value update rate.
    beta: [0,10] - Inverse temperature for softmax.
    w_base: [0,1] - Base weight for Model-Based control (0=MF, 1=MB).
    w_oci_sens: [0,1] - Sensitivity of w to OCI score.
    
    Hypothesis: The balance between goal-directed (MB) and habitual (MF) control
    is shifted by OCI symptoms. w = clip(w_base + w_oci_sens * oci, 0, 1).
    """
    learning_rate, beta, w_base, w_oci_sens = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate the effective mixing weight w
    # We allow OCI to either increase or decrease MB control depending on the sign/magnitude
    # Here we model it as an additive modulation.
    w = w_base + (w_oci_sens * oci_score)
    w = np.clip(w, 0.0, 1.0) # Ensure w stays valid

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2) # Model-free values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for stage 2 (same as MB terminal values)

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        
        # Model-Based calculation for Stage 1
        # MB values are expected max Q-value of next stage weighted by transition probs
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Q-value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # --- Stage 2 Decision ---
        state_idx = int(state[trial])
        
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Learning Updates ---
        
        # SARSA(0) / TD update for Stage 1 MF
        # Note: Standard two-step task usually updates Stage 1 MF using Stage 2 Q-value (TD)
        dt_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += learning_rate * dt_stage1
        
        # Reward Prediction Error for Stage 2
        dt_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * dt_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-modulated Learning Rate Asymmetry (Pos/Neg)
This model investigates if OCI scores relate to an asymmetry in how participants learn from positive versus negative prediction errors. High OCI might lead to hypersensitivity to negative outcomes (avoidance learning) or positive outcomes. Here, we define a base learning rate and an asymmetry parameter that is scaled by the OCI score.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free RL with OCI-modulated learning rate asymmetry.
    
    Parameters:
    alpha_base: [0,1] - Base learning rate.
    beta: [0,10] - Inverse temperature.
    asymmetry_factor: [0,1] - Degree to which OCI distorts pos/neg learning.
    
    Hypothesis: OCI affects the ratio of learning from positive vs negative errors.
    alpha_pos = alpha_base
    alpha_neg = alpha_base * (1 + asymmetry_factor * oci)
    (If OCI is high, alpha_neg might be significantly larger than alpha_pos).
    """
    alpha_base, beta, asymmetry_factor = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate separate learning rates
    # We hypothesize OCI might amplify learning from negative prediction errors (or lack of reward)
    alpha_pos = alpha_base
    # Ensure alpha_neg doesn't exceed 1
    alpha_neg = np.clip(alpha_base * (1.0 + asymmetry_factor * oci_score), 0.0, 1.0) 

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # --- Updates ---
        
        # Stage 1 Update (TD)
        # We use the max Q of stage 2 as the target for stage 1 (Q-learning style)
        # or the chosen Q (SARSA style). Let's stick to chosen Q for consistency with standard 2-step.
        delta_stage1 = q_stage2[state_idx, int(action_2[trial])] - q_stage1[int(action_1[trial])]
        
        # Apply asymmetric learning rate
        eff_alpha1 = alpha_pos if delta_stage1 >= 0 else alpha_neg
        q_stage1[int(action_1[trial])] += eff_alpha1 * delta_stage1

        # Stage 2 Update (Reward)
        delta_stage2 = reward[trial] - q_stage2[state_idx, int(action_2[trial])]
        
        eff_alpha2 = alpha_pos if delta_stage2 >= 0 else alpha_neg
        q_stage2[state_idx, int(action_2[trial])] += eff_alpha2 * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-modulated Eligibility Trace (Lambda)
This model introduces an eligibility trace parameter ($\lambda$) that allows the reward at the second stage to directly update the first-stage choice, bridging the gap between actions. The hypothesis is that OCI scores might correlate with how strongly distant rewards reinforce initial choices (credit assignment), perhaps reflecting a tendency to over-attribute outcomes to distal causes or a specific learning style.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    RL with Eligibility Traces (TD(lambda)) where lambda is modulated by OCI.
    
    Parameters:
    learning_rate: [0,1] - Alpha.
    beta: [0,10] - Inverse temperature.
    lambda_scale: [0,1] - Scaling factor for lambda based on OCI.
    
    Hypothesis: The eligibility trace decay (lambda) is a function of OCI.
    lambda = lambda_scale * oci. 
    This controls how much the Stage 2 reward directly updates Stage 1 values.
    """
    learning_rate, beta, lambda_scale = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective lambda
    # If OCI is high, maybe credit assignment is broader (higher lambda)
    lam = np.clip(lambda_scale * oci_score * 5.0, 0.0, 1.0) # scaling up to allow full range

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])

        # --- Updates with Eligibility Trace ---
        
        # 1. TD error at Stage 1 (transition from Stage 1 to Stage 2)
        # The value of the state arrived at (q_stage2) minus value of chosen action (q_stage1)
        delta1 = q_stage2[state_idx, a2] - q_stage1[a1]
        
        # Update Stage 1 immediately based on transition surprise
        q_stage1[a1] += learning_rate * delta1
        
        # 2. TD error at Stage 2 (Reward)
        delta2 = reward[trial] - q_stage2[state_idx, a2]
        
        # Update Stage 2
        q_stage2[state_idx, a2] += learning_rate * delta2
        
        # Update Stage 1 again based on Stage 2 error, scaled by lambda
        # This is the eligibility trace mechanism: Stage 1 gets credit for Stage 2's RPE
        q_stage1[a1] += learning_rate * lam * delta2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```