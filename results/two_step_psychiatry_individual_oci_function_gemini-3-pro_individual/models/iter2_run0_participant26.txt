Here are three cognitive models that incorporate the OCI score into the decision-making process, distinct from the previously tried parameter combinations.

### Model 1: OCI-Modulated Learning Rate
This model tests the hypothesis that OCI symptoms relate to the *speed* of value updating. Individuals with higher compulsivity might form associations more rapidly (or slowly) than controls, affecting how quickly they lock onto a high-reward alien or switch away from a low-reward one.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Learning Rate.
    
    This model posits that OCI scores influence the learning rate (alpha).
    A standard Hybrid Model (MB/MF) is used, but the rate at which 
    Model-Free Q-values are updated is a function of the participant's OCI.
    
    Parameters:
    - beta: [0, 10] Inverse temperature (choice consistency).
    - w: [0, 1] Mixing weight (1 = pure Model-Based, 0 = pure Model-Free).
    - lr_base: [0, 1] Baseline learning rate.
    - oci_lr_mod: [-1, 1] Modulation of learning rate by OCI. 
                  (Effective LR is clipped between 0 and 1).
    """
    beta, w, lr_base, oci_lr_mod = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate effective learning rate based on OCI
    # We clip to ensure it stays within valid bounds [0, 1]
    learning_rate = lr_base + (current_oci * oci_lr_mod)
    learning_rate = np.clip(learning_rate, 0.0, 1.0)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Model-Free Q-values
    q_stage1_mf = np.zeros(2)     # Values for Spaceships A(0) and U(1)
    q_stage2_mf = np.zeros((2, 2)) # Values for Aliens (Planet X:0,1; Planet Y:0,1)

    for trial in range(n_trials):

        # --- Stage 1 Policy (Hybrid) ---
        # 1. Model-Based Value: Expected value of best option at next stage
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Hybrid Value: Weighted sum of MB and MF
        q_net_stage1 = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # 3. Softmax Choice
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial]) # Planet X(0) or Y(1)

        # --- Stage 2 Policy (Model-Free) ---
        # Standard softmax on the aliens available at the current planet
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        # --- Updating ---
        # Prediction Error 1: Value of chosen state (planet) vs chosen spaceship
        # Note: In standard Daw task, Stage 1 MF update usually uses the value of the *chosen* 
        # option in Stage 2, or the max of Stage 2. Here we use chosen (SARSA-like).
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1
        
        # Prediction Error 2: Reward vs value of chosen alien
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Reward Sensitivity
This model suggests that OCI affects how subjectively "valuable" a reward feels. Rather than changing the learning rate (how fast we update), it changes the magnitude of the signal being learned. A high OCI score might amplify the perception of success (hypersensitivity to reward) or dampen it.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Modulated Reward Sensitivity.
    
    This model assumes OCI modulates the subjective valuation of the reward.
    Instead of learning directly from the objective reward (0 or 1), the agent 
    learns from an 'effective reward'.
    
    Parameters:
    - learning_rate: [0, 1] Standard learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight (MB vs MF).
    - rew_sens_base: [0, 2] Baseline sensitivity to reward.
    - oci_sens_mod: [-1, 1] How OCI modifies reward sensitivity.
    """
    learning_rate, beta, w, rew_sens_base, oci_sens_mod = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate effective reward multiplier
    # Effective Reward = Reward * (Base + OCI * Mod)
    sensitivity = rew_sens_base + (current_oci * oci_sens_mod)
    sensitivity = np.maximum(sensitivity, 0.0) # Sensitivity cannot be negative
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        # --- Updating ---
        # Calculate Effective Reward
        effective_reward = reward[trial] * sensitivity
        
        # Standard TD updates using the Effective Reward
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1
        
        delta_stage2 = effective_reward - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Stage 2 Precision
This model hypothesizes that OCI specifically impacts the "checking" or "collecting" phase (Stage 2) differently than the planning phase (Stage 1). It allows for different levels of choice stochasticity (beta) for the spaceship choice versus the alien choice, with the latter being driven by OCI. This reflects the idea that compulsivity might manifest as rigid behavior specifically at the point of outcome receipt.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Stage 2 Precision.
    
    This model separates the inverse temperature (beta) for Stage 1 and Stage 2.
    It posits that OCI specifically modulates the precision/determinism of the 
    second-step choice (choosing the alien), while the first step uses a fixed beta.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - w: [0, 1] Mixing weight.
    - beta_1: [0, 10] Inverse temperature for Stage 1 (Spaceship).
    - beta_2_base: [0, 10] Baseline inverse temperature for Stage 2 (Alien).
    - oci_beta2_mod: [-5, 5] Modulation of Stage 2 beta by OCI.
    """
    learning_rate, w, beta_1, beta_2_base, oci_beta2_mod = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate Stage 2 Beta
    beta_2 = beta_2_base + (current_oci * oci_beta2_mod)
    beta_2 = np.maximum(beta_2, 0.0) # Ensure beta is non-negative
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy (Uses beta_1) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta_1 * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])

        # --- Stage 2 Policy (Uses OCI-modulated beta_2) ---
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        # --- Updating ---
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```