Here are three new cognitive models exploring different mechanisms for how OCI might influence decision-making in the two-step task.

### Model 1: Hybrid Learner with OCI-Modulated Model-Based Weight
This model assumes that all participants use a mixture of model-free (habitual) and model-based (goal-directed) strategies. The core hypothesis here is that the Obsessive-Compulsive Inventory (OCI) score directly modulates the balance between these two systems. A higher OCI might lead to a more rigid, habitual (model-free) strategy, or conversely, a more deliberative (model-based) one.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: Hybrid Learner with OCI-Modulated Model-Based Weight.
    Hypothesis: The balance between model-based (planning) and model-free (habit) systems
    is determined by a weighting parameter 'w'. This model tests if OCI score linearly
    shifts this balance.
    
    Parameters:
    learning_rate: [0, 1] Update rate for Q-values.
    beta: [0, 10] Inverse temperature (softmax sensitivity).
    w_base: [0, 1] Baseline weighting for model-based system (0=pure MF, 1=pure MB).
    w_oci_slope: [-1, 1] How much OCI score changes the weighting 'w'.
    """
    learning_rate, beta, w_base, w_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate the mixing weight based on OCI
    w = w_base + (w_oci_slope * oci_score)
    w = np.clip(w, 0.0, 1.0) # Ensure w stays within valid bounds [0, 1]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2) # Model-free values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for stage 2

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based Value Calculation: Bellman equation using transition matrix
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value: Weighted sum of MB and MF values
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Softmax selection
        exp_q1 = np.exp(beta * q_net)
        if np.sum(exp_q1) == 0: probs_1 = np.array([0.5, 0.5])
        else: probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        s_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        if np.sum(exp_q2) == 0: probs_2 = np.array([0.5, 0.5])
        else: probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Value Updating ---
        r = reward[trial]
        
        # TD Error Stage 2
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # TD Error Stage 1 (SARSA-style update for MF value)
        # Note: Standard hybrid models often use Q(s2, a2) for the stage 1 update
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Eligibility trace: The stage 2 RPE also updates stage 1 MF value (lambda=1 implicitly here for simplicity in this hybrid formulation)
        q_stage1_mf[a1] += learning_rate * delta_stage2 

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Dependent Perseveration (Sticky-Choice)
This model investigates "stickiness" or choice perseverationâ€”the tendency to repeat the previous action regardless of reward. High OCI scores are often associated with repetitive behaviors or compulsions. This model hypothesizes that OCI modulates the `stickiness` parameter, making participants with higher scores more likely to repeat their Stage 1 choice.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Dependent Perseveration (Sticky-Choice).
    Hypothesis: OCI modulates choice perseveration (stickiness). Higher OCI might lead
    to a higher tendency to repeat the previous Stage 1 choice regardless of outcome.
    
    Parameters:
    learning_rate: [0, 1] Update rate.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Weight for Model-Based system (fixed across OCI, unlike Model 1).
    stickiness_oci_slope: [-5, 5] Effect of OCI on the stickiness bonus.
    """
    learning_rate, beta, w, stickiness_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate stickiness based on OCI
    # Base stickiness is assumed 0 for simplicity, slope determines magnitude and direction
    stickiness = stickiness_oci_slope * oci_score
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1 # Initialize with invalid action

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Add stickiness bonus to the Q-value of the previously chosen action
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness

        exp_q1 = np.exp(beta * q_net)
        if np.sum(exp_q1) == 0: probs_1 = np.array([0.5, 0.5])
        else: probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        last_action_1 = a1 # Store for next trial
        
        # --- Stage 2 Policy ---
        s_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        if np.sum(exp_q2) == 0: probs_2 = np.array([0.5, 0.5])
        else: probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Value Updating ---
        r = reward[trial]
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Eligibility trace update
        q_stage1_mf[a1] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Second-Stage Learning Rate
This model posits that OCI affects how sensitive participants are to the final reward outcome (the coin). Specifically, it modifies the learning rate at the second stage (the direct interaction with aliens). A high OCI might imply hyper-sensitivity to errors or rewards (high learning rate) or rigidity/insensitivity (low learning rate).

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Second-Stage Learning Rate.
    Hypothesis: OCI scores impact the learning rate (alpha) specifically for the second stage.
    This reflects how quickly participants update their estimates of the aliens' reward probabilities
    based on their OCI profile.
    
    Parameters:
    alpha_base: [0, 1] Baseline learning rate.
    alpha_oci_slope: [-1, 1] Effect of OCI on learning rate.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Weight for Model-Based system.
    """
    alpha_base, alpha_oci_slope, beta, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective learning rate
    lr = alpha_base + (alpha_oci_slope * oci_score)
    lr = np.clip(lr, 0.0, 1.0) # Bound between 0 and 1

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        if np.sum(exp_q1) == 0: probs_1 = np.array([0.5, 0.5])
        else: probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        s_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        if np.sum(exp_q2) == 0: probs_2 = np.array([0.5, 0.5])
        else: probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Value Updating ---
        r = reward[trial]
        
        # Update Stage 2 values using the OCI-modulated learning rate
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_stage2
        
        # Update Stage 1 MF values
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        # Eligibility trace update
        q_stage1_mf[a1] += lr * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```