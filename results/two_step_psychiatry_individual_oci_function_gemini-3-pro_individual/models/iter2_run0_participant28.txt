Here are three new cognitive models based on the two-step task, incorporating OCI scores into different mechanisms of the reinforcement learning process.

### Model 1: OCI-Modulated Pessimism in Planning
This model hypothesizes that high OCI scores are associated with "catastrophic thinking" or risk aversion during the model-based planning phase. Instead of assuming the optimal future outcome (max Q-value) at the second stage, the agent mixes in the worst-case scenario (min Q-value) proportional to their OCI score.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model with OCI-modulated Pessimism in Planning.
    
    Hypothesis: OCI is associated with risk aversion and doubt. 
    In the Model-Based planning step, instead of purely assuming the optimal 
    future action (max Q), high OCI participants dampen this expectation 
    towards the worst case (min Q), reflecting a 'pessimistic' evaluation 
    of future states.

    Parameters:
    learning_rate: [0,1] - Learning rate for MF values.
    beta: [0,10] - Inverse temperature for softmax.
    w: [0,1] - Weight for Model-Based control.
    pessimism_factor: [0, 5] - Scaling factor for OCI-induced pessimism.
    """
    learning_rate, beta, w, pessimism_factor = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate pessimism coefficient (bounded [0, 1])
    # If OCI is high, rho approaches 1 (considering min Q).
    rho = current_oci * pessimism_factor
    if rho > 1.0: rho = 1.0
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based valuation:
        # Standard MB uses max(Q2). Pessimistic MB mixes max(Q2) and min(Q2).
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        min_q_stage2 = np.min(q_stage2_mf, axis=1)
        
        # Effective estimated value of the state
        expected_val_stage2 = (1 - rho) * max_q_stage2 + rho * min_q_stage2
        
        q_stage1_mb = transition_matrix @ expected_val_stage2
        
        # Hybrid value
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # --- Stage 2 Policy ---
        s_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Updates ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Standard TD updates
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Second-Stage Rigidity
This model suggests that OCI manifests specifically as "checking rigidity" at the moment of the final action (Stage 2). While Stage 1 might be governed by a baseline exploration parameter, high OCI leads to a higher inverse temperature (beta) at Stage 2, causing more deterministic/compulsive behavior once the planet is reached.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model with OCI-modulated Stage 2 Rigidity.
    
    Hypothesis: OCI affects decision noise differentially across stages.
    While Stage 1 represents abstract planning, Stage 2 represents the 
    concrete 'checking' action. High OCI leads to increased rigidity 
    (higher Beta) specifically at Stage 2, reducing exploration 
    regardless of the learned values.

    Parameters:
    learning_rate: [0,1] - Learning rate.
    beta_base: [0,10] - Baseline inverse temperature (Stage 1).
    w: [0,1] - Weight for Model-Based control.
    oci_rigidity: [0, 10] - Additional beta added to Stage 2 based on OCI.
    """
    learning_rate, beta_base, w, oci_rigidity = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Stage 2 beta is boosted by OCI
    beta_stage2 = beta_base + (current_oci * oci_rigidity)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Policy (Base Beta) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_base * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # --- Stage 2 Policy (Rigid Beta) ---
        s_idx = int(state[trial])
        
        # Use the OCI-modified beta here
        exp_q2 = np.exp(beta_stage2 * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Updates ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Subjective Reward Valuation
This model tests the hypothesis that OCI alters the subjective magnitude of rewards. Rather than changing the learning rate (speed), this scales the reward itself. A high OCI score might lead to "hyper-valuation" (relief is intense) or "devaluation" (anhedonia). This scaling affects the asymptotic Q-values, which in turn interacts with the temperature parameter to change the distinctness of the preferred option.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model with OCI-modulated Subjective Reward Valuation.
    
    Hypothesis: OCI scales the perceived magnitude of the reward (coin).
    If OCI is linked to hyper-sensitivity, the effective reward might be 
    > 1, leading to Q-values that grow larger than the standard range, 
    making the agent more exploitative for the same Beta. 

    Parameters:
    learning_rate: [0,1] - Learning rate.
    beta: [0,10] - Inverse temperature.
    w: [0,1] - Weight for Model-Based control.
    oci_rew_scale: [-1, 5] - Scaling factor for reward. 
                             Effective R = R * (1 + OCI * scale).
    """
    learning_rate, beta, w, oci_rew_scale = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate effective reward multiplier
    # If scale is positive, rewards feel larger. If negative, smaller.
    reward_multiplier = 1.0 + (current_oci * oci_rew_scale)
    if reward_multiplier < 0: reward_multiplier = 0.0
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # --- Stage 2 Policy ---
        s_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Updates ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        
        # Subjective reward scaling
        r_effective = reward[trial] * reward_multiplier
        
        delta_stage2 = r_effective - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```