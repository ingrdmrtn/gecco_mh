Here are three new cognitive models exploring different mechanisms for how Obsessive-Compulsive Inventory (OCI) scores might influence decision-making in this two-step task.

### Model 1: Hybrid Learner with OCI-modulated Model-Based Weight
This model posits that individuals with higher OCI scores might rely more on habitual (model-free) control rather than goal-directed (model-based) planning. It uses a mixing parameter `w` to blend model-based and model-free values, where `w` is modulated by the OCI score. A lower OCI (like this participant's) would retain more model-based control, whereas higher OCI shifts towards model-free.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free learner where OCI modulates the balance (w).
    
    Hypothesis: High OCI is associated with deficits in goal-directed control, leading to 
    lower model-based weighting (w).
    
    w_effective = w_base / (1 + oci * oci_impact)
    
    Parameters:
    learning_rate: [0, 1] - Learning rate for Q-values.
    beta: [0, 10] - Inverse temperature for softmax.
    w_base: [0, 1] - Baseline model-based weight (0=MF, 1=MB).
    oci_impact: [0, 5] - Strength of OCI reduction on model-based weight.
    """
    learning_rate, beta, w_base, oci_impact = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate effective w. Higher OCI reduces effective w.
    w_effective = w_base / (1.0 + current_oci * oci_impact)
    # Ensure w stays in bounds [0, 1]
    w_effective = np.clip(w_effective, 0.0, 1.0)

    # Fixed transition matrix for the task (MB component)
    # A->X (0.7), U->Y (0.7) usually.
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Initialize Q-values
    q_stage1_mf = np.zeros(2) # Model-free Q(s1, a1)
    q_stage2 = np.zeros((2, 2)) # Q(s2, a2) - used for both MB and MF
    
    for trial in range(n_trials):
        
        # --- Stage 1 Choice ---
        # Model-Based Value: T * max(Q_stage2)
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = w_effective * q_stage1_mb + (1 - w_effective) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Choice ---
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning ---
        chosen_a1 = action_1[trial]
        chosen_a2 = action_2[trial]
        r = reward[trial]
        
        # TD Update for Stage 2
        # Q_s2(s, a) = Q_s2(s, a) + lr * (r - Q_s2(s, a))
        delta_stage2 = r - q_stage2[state_idx, chosen_a2]
        q_stage2[state_idx, chosen_a2] += learning_rate * delta_stage2
        
        # TD Update for Stage 1 (Model-Free)
        # Q_s1(a) = Q_s1(a) + lr * (Q_s2(s, a) - Q_s1(a))
        # Note: We use the updated stage 2 value (SARSA-like or Q-learning depending on implementation, 
        # here using the value of the state actually reached)
        delta_stage1 = q_stage2[state_idx, chosen_a2] - q_stage1_mf[chosen_a1]
        q_stage1_mf[chosen_a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Eligibility Trace (Lambda)
This model assumes the participant uses a purely Model-Free strategy but varies in how effectively they credit the first-stage choice for the second-stage reward. The eligibility trace parameter ($\lambda$) determines this credit assignment. The hypothesis is that OCI affects the temporal linking of events (perhaps due to over-focus on immediate outcomes or perseveration), modulating $\lambda$.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner with Eligibility Trace (lambda) modulated by OCI.
    
    Hypothesis: OCI affects the eligibility trace parameter (lambda).
    Low lambda implies updates are driven mostly by immediate next state.
    High lambda connects stage 1 directly to final reward.
    
    lambda_eff = lambda_base + (oci * oci_lambda_mod)
    
    Parameters:
    learning_rate: [0, 1]
    beta: [0, 10]
    lambda_base: [0, 1] - Baseline eligibility trace.
    oci_lambda_mod: [-1, 1] - How OCI shifts lambda (can be positive or negative).
    """
    learning_rate, beta, lambda_base, oci_lambda_mod = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    # Calculate effective lambda
    lambda_eff = lambda_base + (current_oci * oci_lambda_mod)
    lambda_eff = np.clip(lambda_eff, 0.0, 1.0)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    for trial in range(n_trials):
        
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Choice ---
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning with Eligibility Trace ---
        chosen_a1 = action_1[trial]
        chosen_a2 = action_2[trial]
        r = reward[trial]
        
        # Prediction error at stage 2
        # delta2 = r - Q2(s2, a2)
        pe_2 = r - q_stage2[state_idx, chosen_a2]
        
        # Prediction error at stage 1
        # delta1 = Q2(s2, a2) - Q1(a1)
        pe_1 = q_stage2[state_idx, chosen_a2] - q_stage1[chosen_a1]
        
        # Update Stage 2
        q_stage2[state_idx, chosen_a2] += learning_rate * pe_2
        
        # Update Stage 1
        # Q1 gets updated by its own PE, plus a portion of Stage 2's PE (via lambda)
        q_stage1[chosen_a1] += learning_rate * pe_1 + learning_rate * lambda_eff * pe_2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Asymmetric Learning Rates Modulated by OCI
This model investigates if OCI is related to differential sensitivity to positive versus negative prediction errors. It proposes that higher OCI might lead to an increased learning rate for negative outcomes (fear of failure/loss aversion) or positive outcomes, creating an asymmetry.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner with Asymmetric Learning Rates modulated by OCI.
    
    Hypothesis: OCI modulates sensitivity to negative prediction errors (alpha_neg).
    Individuals with high OCI might be hyper-sensitive to errors/missing rewards.
    
    alpha_pos = base_alpha
    alpha_neg = base_alpha + (oci * oci_neg_boost)
    
    Parameters:
    base_alpha: [0, 1] - Base learning rate.
    beta: [0, 10]
    oci_neg_boost: [0, 2] - Additional learning rate for negative PEs based on OCI.
    stickiness: [0, 5] - Choice perseveration.
    """
    base_alpha, beta, oci_neg_boost, stickiness = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        
        # --- Stage 1 Choice with Stickiness ---
        q_stage1_mod = q_stage1.copy()
        if last_action_1 != -1:
            q_stage1_mod[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_stage1_mod)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_action_1 = action_1[trial]
        
        # --- Stage 2 Choice ---
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning ---
        chosen_a1 = action_1[trial]
        chosen_a2 = action_2[trial]
        r = reward[trial]
        
        # Calculate PEs
        pe_1 = q_stage2[state_idx, chosen_a2] - q_stage1[chosen_a1]
        pe_2 = r - q_stage2[state_idx, chosen_a2]
        
        # Determine alpha for Stage 1 update
        alpha_eff_1 = base_alpha
        if pe_1 < 0:
             alpha_neg = base_alpha + (current_oci * oci_neg_boost)
             alpha_eff_1 = np.clip(alpha_neg, 0, 1)
        
        q_stage1[chosen_a1] += alpha_eff_1 * pe_1
        
        # Determine alpha for Stage 2 update
        alpha_eff_2 = base_alpha
        if pe_2 < 0:
             alpha_neg = base_alpha + (current_oci * oci_neg_boost)
             alpha_eff_2 = np.clip(alpha_neg, 0, 1)

        q_stage2[state_idx, chosen_a2] += alpha_eff_2 * pe_2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```