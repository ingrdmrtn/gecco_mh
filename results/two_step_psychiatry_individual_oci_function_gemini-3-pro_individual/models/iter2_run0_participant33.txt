Here are three new cognitive models that incorporate the OCI score into the decision-making process for the two-step task.

### Model 1: Hybrid Model with OCI-Modulated Model-Based Weight
This model assumes that individuals use a mix of Model-Free (MF) and Model-Based (MB) strategies. The core hypothesis here is that OCI symptoms might relate to a deficit in goal-directed (MB) control, or an over-reliance on habitual (MF) control. We model the mixing weight `w` as a function of the OCI score.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL model where the balance between Model-Based and Model-Free control (w)
    is modulated by the OCI score.
    
    Hypothesis: Higher OCI scores might lead to more habitual (Model-Free) behavior,
    so we model 'w' (MB weight) as decreasing with OCI.
    
    Parameters:
    - learning_rate: [0, 1] - Update rate for Q-values
    - beta: [0, 10] - Inverse temperature for softmax
    - w_max: [0, 1] - Maximum Model-Based weight (at OCI=0)
    - w_decay: [0, 5] - Rate at which MB weight decays as OCI increases
    """
    learning_rate, beta, w_max, w_decay = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate the effective weight w based on OCI
    # As OCI increases, w decreases (more Model-Free)
    # Using an exponential decay or linear clipping logic
    w = w_max * np.exp(-w_decay * oci_score)
    # Ensure w stays in [0, 1]
    w = np.clip(w, 0.0, 1.0)

    # Transition matrix (fixed for this task structure)
    # 0 -> 70% to 0, 30% to 1
    # 1 -> 30% to 0, 70% to 1
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2) # MF values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # MF values for stage 2 (also used for MB calc)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        # 1. Model-Based Value Calculation
        # V(state') = max_a Q(state', a)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        # Q_MB(a1) = sum P(s'|a1) * V(s')
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Hybrid Value Calculation
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # 3. Choice Probability
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        # Standard Model-Free choice at stage 2
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning Updates ---
        
        # Stage 2 Update (TD error)
        # Q2(s, a2) <- Q2(s, a2) + alpha * (r - Q2(s, a2))
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 Update (TD(1) / Direct Reinforcement)
        # Note: In standard hybrid models, MF Q1 is often updated by the reward directly (SARSA/Q-learning)
        # Here we use the observed reward to update Q1 MF directly for simplicity in this template
        delta_stage1 = r - q_stage1_mf[a1] # Using r instead of Q2 to keep it purely MF-outcome based
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Eligibility Trace Decay OCI Model
This model introduces an eligibility trace parameter `lambda` that determines how much the Stage 1 choice is reinforced by the final reward. The hypothesis is that OCI affects the efficiency of credit assignmentâ€”specifically, how well the reward at the end of the trial propagates back to the first decision.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Reinforcement Learning with Eligibility Traces (TD(lambda)), where lambda is modulated by OCI.
    
    Hypothesis: OCI might affect the temporal credit assignment. Higher OCI could mean
    'stickier' traces or difficulty distinguishing relevant causal chains (higher lambda),
    or conversely, disconnected learning (lower lambda).
    
    Parameters:
    - learning_rate: [0, 1]
    - beta: [0, 10]
    - lambda_base: [0, 1] - Base eligibility trace decay parameter
    - lambda_oci_mod: [-1, 1] - Modulation of lambda by OCI score
    """
    learning_rate, beta, lambda_base, lambda_oci_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective lambda
    eff_lambda = lambda_base + (lambda_oci_mod * oci_score)
    eff_lambda = np.clip(eff_lambda, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        # Pure Model-Free for Stage 1 here to isolate lambda effect
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning Updates (TD(lambda) logic) ---
        
        # 1. Prediction Error at Stage 2 (Reward prediction error)
        # delta2 = r - Q2(s, a2)
        delta2 = r - q_stage2_mf[s_idx, a2]
        
        # 2. Prediction Error at Stage 1
        # delta1 = Q2(s, a2) - Q1(a1)
        delta1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        
        # Update Stage 2 Q-value
        q_stage2_mf[s_idx, a2] += learning_rate * delta2
        
        # Update Stage 1 Q-value
        # In TD(lambda), the update for Q1 includes its own error plus discounted error from stage 2
        # Q1(a1) <- Q1(a1) + alpha * (delta1 + lambda * delta2)
        q_stage1_mf[a1] += learning_rate * (delta1 + eff_lambda * delta2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Asymmetric Learning Rate Model (Pos/Neg OCI Modulation)
This model posits that OCI scores specifically impact how individuals learn from negative outcomes (punishment/lack of reward) versus positive outcomes. It splits the learning rate into `alpha_pos` and `alpha_neg`, and applies OCI modulation specifically to the negative learning rate.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Dual Learning Rate Model where OCI modulates sensitivity to negative prediction errors.
    
    Hypothesis: Obsessive-compulsive traits may be linked to altered processing of 
    negative feedback (e.g., hyper-sensitivity to failure).
    
    Parameters:
    - alpha_pos: [0, 1] - Learning rate for positive prediction errors
    - alpha_neg_base: [0, 1] - Base learning rate for negative prediction errors
    - alpha_neg_oci_slope: [0, 1] - How much OCI increases learning from negative errors
    - beta: [0, 10]
    """
    alpha_pos, alpha_neg_base, alpha_neg_oci_slope, beta = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective negative learning rate
    alpha_neg = alpha_neg_base + (alpha_neg_oci_slope * oci_score)
    alpha_neg = np.clip(alpha_neg, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Using simple MF Q-values
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        # Using a simple Model-Based calculation to inform Stage 1
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Mixing MB and MF (fixed 50/50 assumption to focus on learning rates)
        q_net = 0.5 * q_stage1_mb + 0.5 * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning Updates ---
        
        # Stage 2 Prediction Error
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        # Apply asymmetric learning rates
        eff_alpha_2 = alpha_pos if delta_stage2 > 0 else alpha_neg
        q_stage2_mf[s_idx, a2] += eff_alpha_2 * delta_stage2
        
        # Stage 1 Prediction Error (using MB-like update from Q2)
        # Note: We update MF Q1 using the difference between current Q1 and the Q2 value of the chosen path
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        
        eff_alpha_1 = alpha_pos if delta_stage1 > 0 else alpha_neg
        q_stage1_mf[a1] += eff_alpha_1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```