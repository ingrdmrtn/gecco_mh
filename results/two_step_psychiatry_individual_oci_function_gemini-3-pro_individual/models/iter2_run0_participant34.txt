Here are the 3 proposed cognitive models.

### Model 1: OCI-Modulated Response Rigidity (Beta-Modulation)
This model hypothesizes that OCI symptoms relate to the rigidity of choice behavior. Instead of a fixed inverse temperature ($\beta$), the OCI score modulates the trade-off between exploration and exploitation. High OCI leads to higher $\beta$ (determinism/rigidity), while low OCI leads to lower $\beta$ (more stochasticity).

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF Model with OCI-Modulated Inverse Temperature (Response Rigidity).
    
    This model assumes a hybrid Model-Based (MB) and Model-Free (MF) architecture.
    The Obsessive-Compulsive Inventory (OCI) score modulates the inverse temperature (beta).
    Higher OCI scores increase beta, leading to more rigid, deterministic choices (exploitation),
    while lower scores allow for more noise/exploration.
    
    Parameters:
    - learning_rate: Rate at which MF Q-values are updated [0, 1].
    - w: Mixing weight between MB (1) and MF (0) systems [0, 1].
    - beta_base: Baseline inverse temperature [0, 10].
    - beta_oci_slope: How much OCI increases rigidity [0, 10]. 
      (Effective beta = beta_base + beta_oci_slope * oci)
    
    Bounds:
    learning_rate: [0, 1]
    w: [0, 1]
    beta_base: [0, 10]
    beta_oci_slope: [0, 10]
    """
    learning_rate, w, beta_base, beta_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective beta based on OCI
    # We clip to ensure it stays within reasonable numerical bounds
    beta_eff = beta_base + (beta_oci_slope * oci_score)
    beta_eff = np.clip(beta_eff, 0, 20) 

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based value calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid value: Mix MB and MF
        q_net_stage1 = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Softmax with OCI-modulated beta
        exp_q1 = np.exp(beta_eff * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        # Standard Softmax for second stage
        exp_q2 = np.exp(beta_eff * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Updates ---
        # Stage 2 RPE
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2
        
        # Stage 1 RPE (TD-0)
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Memory Distrust (Forgetting)
This model posits that OCI symptoms are linked to "memory distrust" or the need to check. It implements a decay mechanism where Q-values for unchosen options degrade over time. The rate of this forgetting is proportional to the OCI score: higher OCI subjects "forget" the value of unvisited states faster, potentially driving checking behaviors.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF Model with OCI-Modulated Value Decay (Memory Distrust).
    
    This model introduces a forgetting parameter. On every trial, the Q-values
    of the unchosen options decay towards 0. The rate of decay is modulated by OCI.
    High OCI implies faster decay (distrust of memory/past learning), necessitating
    more frequent 'checking' or resampling.
    
    Parameters:
    - learning_rate: Rate at which Q-values are updated [0, 1].
    - beta: Inverse temperature [0, 10].
    - w: Mixing weight between MB and MF [0, 1].
    - decay_base: Baseline forgetting rate [0, 1].
    - decay_oci_mod: Additional decay per unit of OCI [0, 1].
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    w: [0, 1]
    decay_base: [0, 1]
    decay_oci_mod: [0, 1]
    """
    learning_rate, beta, w, decay_base, decay_oci_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective decay rate
    decay_rate = decay_base + (decay_oci_mod * oci_score)
    decay_rate = np.clip(decay_rate, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Updates ---
        # Identify chosen/unchosen indices
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        unchosen_a2 = 1 - a2
        
        # Update Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Decay unchosen option in the current state (Memory Distrust)
        q_stage2_mf[state_idx, unchosen_a2] *= (1.0 - decay_rate)
        
        # Update Stage 1
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Decay unchosen option in Stage 1
        q_stage1_mf[1 - a1] *= (1.0 - decay_rate)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Transition Uncertainty
This model suggests that OCI affects the Model-Based component specifically by distorting the internal belief about the transition matrix. While the true transition is 0.7/0.3, high OCI participants may perceive the world as more volatile or uncertain (e.g., believing the transitions are closer to 0.5/0.5). This degrades the utility of Model-Based planning.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF Model with OCI-Modulated Transition Uncertainty.
    
    This model assumes that OCI symptoms correlate with a distrust in the 
    structure of the environment. The Model-Based system relies on a transition 
    matrix (typically 0.7 probability of common transition). This model distorts 
    that probability based on OCI: High OCI reduces the subjective probability 
    of the common transition towards 0.5 (maximum entropy/uncertainty), making 
    the MB system less decisive.
    
    Parameters:
    - learning_rate: Rate at which MF Q-values are updated [0, 1].
    - beta: Inverse temperature [0, 10].
    - w: Mixing weight [0, 1].
    - uncertainty_oci: Scale of distortion applied to transition matrix [0, 1].
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    w: [0, 1]
    uncertainty_oci: [0, 1]
    """
    learning_rate, beta, w, uncertainty_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Distort the internal model of the transition matrix
    # True common prob is 0.7. High OCI pulls this down towards 0.5.
    # Distortion is capped so prob doesn't drop below 0.5.
    subjective_common_prob = 0.7 - (uncertainty_oci * oci_score)
    subjective_common_prob = np.maximum(subjective_common_prob, 0.5)
    
    # Construct the subjective transition matrix used for MB planning
    subjective_trans_matrix = np.array([
        [subjective_common_prob, 1.0 - subjective_common_prob],
        [1.0 - subjective_common_prob, subjective_common_prob]
    ])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # MB calculation uses the DISTORTED/SUBJECTIVE matrix
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = subjective_trans_matrix @ max_q_stage2
        
        q_net_stage1 = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Updates ---
        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```