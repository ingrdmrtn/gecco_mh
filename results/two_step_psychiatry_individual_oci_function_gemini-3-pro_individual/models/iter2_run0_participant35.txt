Here are three new cognitive models that explore different mechanisms for how Obsessive-Compulsive symptoms (OCI) might influence decision-making in the two-step task. These models introduce variations in perseveration, uncertainty-driven exploration, and transition learning.

### Model 1: OCI-Driven Perseveration (Choice Stickiness)
This model hypothesizes that high OCI scores are associated with behavioral rigidity, specifically "stickiness" or perseveration. Individuals with higher OCI might be more likely to repeat their previous Stage 1 choice regardless of reward history, reflecting a compulsion to maintain a routine or a resistance to switching.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Driven Perseveration (Choice Stickiness).
    
    Hypothesis: 
    Higher OCI leads to increased choice stickiness (perseveration) at Stage 1.
    High OCI individuals may feel a compulsion to repeat actions, making them 
    less sensitive to value differences and more driven by repetition.
    
    Parameters:
    - learning_rate: [0, 1] Rate of updating Q-values.
    - beta: [0, 10] Inverse temperature for softmax.
    - w: [0, 1] Weighting between Model-Based (1) and Model-Free (0).
    - stick_sensitivity: [0, 5] How strongly OCI scales the perseveration bonus.
    
    Mechanism:
    - A 'stickiness' bonus is added to the Q-value of the previously chosen action at Stage 1.
    - The magnitude of this bonus is determined by `stick_sensitivity * oci`.
    """
    learning_rate, beta, w, stick_sensitivity = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Fixed transition matrix (70-30 structure)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track previous choice for stickiness
    last_action_1 = -1 

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based Value
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Value (Weighted Average)
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add Stickiness Bonus
        # If it's not the first trial, add bonus to the previously chosen action
        stickiness_bonus = np.zeros(2)
        if last_action_1 != -1:
            stickiness_bonus[last_action_1] = stick_sensitivity * oci_score
        
        # Softmax selection
        exp_q1 = np.exp(beta * (q_net + stickiness_bonus))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Update last action
        last_action_1 = action_1[trial]
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Value Updating ---
        # Stage 2 Update (SARSA/Q-learning)
        # Prediction error at outcome
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Stage 1 Update (TD-learning)
        # Prediction error at transition (using max Q of next stage)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Eligibility trace (Stage 2 reward influencing Stage 1 choice directly)
        # Standard lambda=1 assumption for pure MF often used in these simplifications
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Transition Learning
This model posits that OCI symptoms interfere with the ability to learn or utilize the transition structure of the task. While standard models assume a fixed 70-30 transition matrix, this model assumes the participant *learns* the transition matrix, but the rate at which they update their internal model of the world (state transitions) is modulated by their OCI score. A high OCI might lead to "hyper-learning" of transitions (over-sensitivity to rare events) or rigidity (under-learning). Here, we model it as OCI affecting the transition learning rate.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Modulated Transition Learning.
    
    Hypothesis: 
    Participants learn the transition matrix (T) dynamically rather than assuming it is fixed.
    OCI modulates the 'transition learning rate' (lr_T). 
    Higher OCI might lead to faster updating of structural beliefs (e.g., believing the 
    world has changed after a single rare transition due to anxiety/uncertainty).
    
    Parameters:
    - learning_rate_reward: [0, 1] Rate of updating Reward Q-values.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weighting for Model-Based control.
    - lr_T_base: [0, 1] Base learning rate for the transition matrix.
    - oci_T_mod: [0, 5] How strongly OCI amplifies transition learning.
    
    Mechanism:
    - effective_lr_T = lr_T_base * (1 + oci_T_mod * oci)
    - The internal transition matrix is updated every trial based on the observed state transition.
    """
    learning_rate_reward, beta, w, lr_T_base, oci_T_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Effective transition learning rate
    effective_lr_T = lr_T_base * (1.0 + oci_T_mod * oci_score)
    # Clip to ensure stability [0, 1]
    if effective_lr_T > 1.0: effective_lr_T = 1.0
    
    # Initialize internal belief of transitions: Start with uniform priors (0.5, 0.5)
    # or slight bias towards truth. Let's start neutral/flat.
    # Rows: Action 1 (0 or 1), Cols: State (0 or 1)
    # T_counts represents the participant's estimated probability of reaching state X given action A
    T_matrix = np.array([[0.5, 0.5], [0.5, 0.5]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based Calculation using DYNAMIC T_matrix
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = T_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Observe State ---
        state_idx = state[trial]
        chosen_a1 = action_1[trial]

        # --- Update Transition Beliefs ---
        # State_idx is 0 or 1. Create a one-hot vector of the observed outcome.
        outcome_vector = np.zeros(2)
        outcome_vector[state_idx] = 1.0
        
        # Update the row corresponding to the chosen action
        # T_new = T_old + lr * (Observed - T_old)
        T_matrix[chosen_a1] += effective_lr_T * (outcome_vector - T_matrix[chosen_a1])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Reward Value Updating ---
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate_reward * delta_stage2
        
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[chosen_a1]
        q_stage1_mf[chosen_a1] += learning_rate_reward * delta_stage1
        
        # Eligibility trace
        q_stage1_mf[chosen_a1] += learning_rate_reward * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Based Inverse Temperature (Exploration/Exploitation)
This model suggests that OCI affects the balance between exploration and exploitation. High OCI is often associated with intolerance of uncertainty. This might manifest as a very high `beta` (low temperature), causing the participant to deterministically exploit small differences in value (rigidity) rather than exploring options that currently seem slightly worse but might yield information.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Based Inverse Temperature (Exploration/Exploitation).
    
    Hypothesis: 
    OCI directly impacts the 'beta' (inverse temperature) parameter.
    High OCI individuals are intolerant of uncertainty and likely to exploit 
    perceived best options rigidly. 
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta_base: [0, 10] Baseline inverse temperature.
    - w: [0, 1] Weighting for Model-Based control.
    - oci_beta_scale: [0, 5] Scaling factor for OCI's effect on beta.
    
    Mechanism:
    - effective_beta = beta_base * (1 + oci_beta_scale * oci)
    - Higher OCI results in higher beta, leading to more deterministic (greedy) choices
      and reduced random exploration.
    """
    learning_rate, beta_base, w, oci_beta_scale = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate individual beta
    effective_beta = beta_base * (1.0 + oci_beta_scale * oci_score)
    # Cap beta to prevent numerical overflow in exp()
    if effective_beta > 20.0: effective_beta = 20.0

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Use effective_beta
        exp_q1 = np.exp(effective_beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        # Use effective_beta here as well
        exp_q2 = np.exp(effective_beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Value Updating ---
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Eligibility trace
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```