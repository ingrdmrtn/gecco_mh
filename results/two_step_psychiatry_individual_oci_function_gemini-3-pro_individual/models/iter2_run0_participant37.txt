Here are three new cognitive models that incorporate the OCI score into the decision-making process in distinct ways, avoiding the specific parameter combinations listed in the feedback.

### Model 1: Valence-Dependent Learning Asymmetry
This model hypothesizes that OCI symptoms are associated with an increased sensitivity to negative outcomes (or "better than expected" outcomes depending on the sign). Specifically, it tests if higher OCI scores lead to stronger learning (higher learning rate) when prediction errors are negative (disappointment/punishment), reflecting a "harm avoidance" or "error correction" bias often seen in obsessive-compulsive traits.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: Valence-Dependent Learning Asymmetry.
    
    This model assumes that OCI severity modulates how strongly participants 
    learn from negative prediction errors compared to positive ones. 
    High OCI acts as a multiplier on the learning rate specifically when
    outcomes are worse than expected (negative RPE), reflecting 
    hyper-sensitivity to errors or missing rewards.

    Parameters:
    learning_rate: [0, 1] Base learning rate for positive updates.
    beta: [0, 10] Inverse temperature (exploration/exploitation).
    w: [0, 1] Weight for Model-Based control.
    neg_lr_boost: [0, 5] How much OCI amplifies learning from negative RPEs.
    
    Bounds:
    learning_rate: [0,1]
    beta: [0,10]
    w: [0,1]
    neg_lr_boost: [0,5]
    """
    learning_rate, beta, w, neg_lr_boost = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Policy for Choice 1
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Policy for Choice 2
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Updating
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        
        # Calculate effective learning rate based on sign of prediction error
        if delta_stage1 < 0:
            lr_eff_1 = learning_rate * (1 + current_oci * neg_lr_boost)
            lr_eff_1 = min(lr_eff_1, 1.0) # Cap at 1.0
        else:
            lr_eff_1 = learning_rate
            
        q_stage1_mf[a1] = q_stage1_mf[a1] + lr_eff_1 * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        # Apply asymmetry to second stage update as well
        if delta_stage2 < 0:
            lr_eff_2 = learning_rate * (1 + current_oci * neg_lr_boost)
            lr_eff_2 = min(lr_eff_2, 1.0)
        else:
            lr_eff_2 = learning_rate

        q_stage2_mf[s_idx, a2] = q_stage2_mf[s_idx, a2] + lr_eff_2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Distorted Transition Beliefs
This model proposes that high OCI scores correlate with "black-and-white" thinking or an over-estimation of structure. While the true transition probability is 0.7, this model allows the participant's *internal model* of the transition matrix to become more deterministic (closer to 1.0) as OCI increases. This affects the Model-Based value calculation, making the participant overly confident in the spaceship-planet link.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Distorted Transition Beliefs.
    
    This model assumes OCI affects the internal model of the environment's structure.
    While the real transition probability is 0.7, high OCI participants may 
    perceive the transitions as more deterministic (rigid thinking).
    The parameter `belief_rigidity` scales the transition probability from 0.7
    towards 1.0 based on the OCI score.
    
    Parameters:
    learning_rate: [0, 1] Update rate for MF values.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Weight for Model-Based control.
    belief_rigidity: [0, 1] How much OCI distorts beliefs toward determinism.
    
    Bounds:
    learning_rate: [0,1]
    beta: [0,10]
    w: [0,1]
    belief_rigidity: [0,1]
    """
    learning_rate, beta, w, belief_rigidity = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
  
    # Base transition probability
    base_prob = 0.7
    
    # Distort probability: OCI pushes 0.7 towards 1.0
    # If belief_rigidity is 0, prob stays 0.7. If 1 and OCI is high, prob -> 1.0.
    # Max possible distortion is (1.0 - 0.7) = 0.3
    distorted_prob = base_prob + (0.3 * current_oci * belief_rigidity)
    
    # Internal model used for planning (MB)
    mb_transition_matrix = np.array([
        [distorted_prob, 1 - distorted_prob], 
        [1 - distorted_prob, distorted_prob]
    ])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Policy for Choice 1
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        
        # Use the distorted matrix for calculation
        q_stage1_mb = mb_transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Policy for Choice 2
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Updating
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] = q_stage1_mf[a1] + learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] = q_stage2_mf[s_idx, a2] + learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Stage-Specific Learning Modulation
This model posits that OCI affects learning rates differentially between the abstract planning stage (Stage 1) and the concrete reward stage (Stage 2). It tests the hypothesis that high OCI individuals are hyper-focused on the immediate, tangible outcomes (aliens/coins) rather than the predictive cues (spaceships), resulting in a boosted learning rate specifically for the second stage Q-values.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: Stage-Specific Learning Modulation.
    
    This model separates learning into a base rate (for Stage 1) and a 
    modulated rate for Stage 2. It hypothesizes that OCI drives a 
    hyper-focus on immediate, concrete rewards (Stage 2 outcomes) 
    over abstract transition cues.
    
    Parameters:
    lr_base: [0, 1] Base learning rate (applied to Stage 1).
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Weight for Model-Based control.
    s2_focus: [0, 5] Multiplier for Stage 2 learning rate based on OCI.
    
    Bounds:
    lr_base: [0,1]
    beta: [0,10]
    w: [0,1]
    s2_focus: [0,5]
    """
    lr_base, beta, w, s2_focus = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    # Calculate the stage 2 specific learning rate once
    # OCI increases the learning rate for the immediate reward stage
    lr_stage2 = lr_base * (1 + current_oci * s2_focus)
    lr_stage2 = min(lr_stage2, 1.0) # Ensure it doesn't exceed 1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Policy for Choice 1
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Policy for Choice 2
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Updating
        
        # Stage 1 uses base learning rate
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] = q_stage1_mf[a1] + lr_base * delta_stage1
        
        # Stage 2 uses OCI-boosted learning rate
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] = q_stage2_mf[s_idx, a2] + lr_stage2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```