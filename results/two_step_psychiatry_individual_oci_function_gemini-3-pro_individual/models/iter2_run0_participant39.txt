Here are three new cognitive models for the two-step task, incorporating OCI scores into the decision-making and learning processes.

### Model 1: Hybrid Model with OCI-Modulated Memory Decay
This model hypothesizes that OCI symptoms correlate with anxiety-driven "forgetting" or instability in value representations. While the agent learns from rewards, the values of unchosen actions decay over time, with the rate of decay scaled by the OCI score. Higher OCI leads to faster forgetting of alternative options, potentially promoting repetitive behavior (since unchosen options lose value).

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: Hybrid MB/MF with OCI-Modulated Memory Decay.
    
    Hypothesis: OCI relates to memory instability or anxiety-driven forgetting.
    Unchosen Q-values decay towards 0 on each trial. The decay rate is proportional 
    to the OCI score.
    
    Parameters:
    lr: [0, 1] Learning rate for reward values.
    beta: [0, 10] Inverse temperature (softmax sensitivity).
    w: [0, 1] Mixing weight (0=Pure MF, 1=Pure MB).
    decay_scale: [0, 1] Scaling factor for OCI-dependent decay. 
                 Decay rate = decay_scale * OCI.
    """
    lr, beta, w, decay_scale = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Decay rate depends on OCI
    decay_rate = decay_scale * oci_score
    # Clamp decay to [0, 1] to prevent instability
    if decay_rate > 1.0: decay_rate = 1.0

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # Model-Based Value
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax Choice 1
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        
        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Learning ---
        
        # 1. Update Stage 1 MF
        # TD(0) update for Stage 1
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        # 2. OCI-Dependent Decay for Unchosen Stage 1 MF
        unchosen_a1 = 1 - a1
        q_stage1_mf[unchosen_a1] *= (1.0 - decay_rate)

        # 3. Update Stage 2 MF
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_stage2
        
        # (Optional: Could also decay unchosen stage 2, but keeping it simple to Stage 1)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model with OCI-Scaled Choice Trace
This model posits that OCI relates to the formation of strong habits. Instead of simple "stickiness" (repeating the last action), this model uses an exponentially decaying **Choice Trace**. The influence of this trace on the decision is scaled by the OCI score. This captures the idea that compulsivity involves a "buildup" of urge to repeat actions over time.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: Hybrid Model with OCI-Scaled Choice Trace (Habit).
    
    Hypothesis: OCI relates to the strength of habits formed by repetition.
    We maintain a 'choice trace' that accumulates when an action is chosen and 
    decays over time. The OCI score determines the weight of this trace 
    in the final decision value.
    
    Parameters:
    lr: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Mixing weight (MB vs MF).
    tau: [0, 1] Decay rate of the choice trace (0 = 1-back sticky, 1 = perfect memory).
    habit_scale: [0, 5] Weight of the trace, scaled by OCI.
    """
    lr, beta, w, tau, habit_scale = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    habit_weight = habit_scale * oci_score

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Choice trace for Stage 1 actions
    trace = np.zeros(2)

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combine MB, MF, and Habit (Trace)
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf + habit_weight * trace
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        
        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Learning ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_stage2
        
        # Update Trace
        # Decay existing trace
        trace *= tau
        # Increment trace for chosen action
        trace[a1] += 1.0

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Dynamic Model-Based Learning with OCI Sensitivity
Standard models assume the transition matrix (probabilities of going to Planet X vs Y) is fixed and known. This model assumes the agent *learns* the transition structure over time. The OCI score modulates the **Structure Learning Rate**. This tests the hypothesis that OCI symptoms relate to how rigidly or fluidly an individual updates their internal model of the environment's causal structure.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: Dynamic Model-Based Learning with OCI Sensitivity.
    
    Hypothesis: The agent learns the transition matrix (World Model) rather than 
    assuming it is fixed. OCI modulates the learning rate for this structure learning.
    High OCI might imply over-sensitivity to transition noise (high lr) or 
    rigidity (low lr).
    
    Parameters:
    lr_val: [0, 1] Learning rate for reward values (Q-values).
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Mixing weight (MB vs MF).
    lr_struct_scale: [0, 1] Scaling factor for structure learning rate.
                     lr_structure = lr_struct_scale * OCI.
    """
    lr_val, beta, w, lr_struct_scale = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    lr_struct = lr_struct_scale * oci_score
    if lr_struct > 1.0: lr_struct = 1.0

    # Initialize dynamic transition matrix (Rows: Action, Cols: State)
    # We start with the true priors, but allow them to drift based on experience/OCI
    trans_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Decision (Using Dynamic Transition Matrix) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = trans_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        
        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Value Learning ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr_val * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr_val * delta_stage2
        
        # --- Structure Learning (OCI Modulated) ---
        # Update probability of reaching state s_idx given action a1
        # Target is 1.0 for the observed state
        curr_prob = trans_matrix[a1, s_idx]
        trans_matrix[a1, s_idx] += lr_struct * (1.0 - curr_prob)
        
        # The other state probability must be 1 - new_prob
        trans_matrix[a1, 1 - s_idx] = 1.0 - trans_matrix[a1, s_idx]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```