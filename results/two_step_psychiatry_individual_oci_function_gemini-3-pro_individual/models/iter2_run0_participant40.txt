Here are 3 new cognitive models based on the two-step task structure, incorporating the OCI score in novel ways compared to the previous attempts.

### Model 1: Hybrid Model with OCI-modulated Model-Based Weight (`w`)
This model implements the classic "Hybrid" reinforcement learning model (Daw et al., 2011) which combines model-free (MF) and model-based (MB) value estimation. The key hypothesis here is that the trade-off parameter `w` (which governs the balance between MB and MF control) is modulated by the participant's OCI score. Compulsivity is often theorized to relate to habitual (MF) vs. goal-directed (MB) control.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free RL with OCI-modulated mixing weight (w).
    
    The parameter 'w' determines the balance between Model-Based (goal-directed)
    and Model-Free (habitual) control. We hypothesize OCI score affects this balance.
    
    Parameters:
    learning_rate: [0, 1] Update rate for Q-values.
    beta: [0, 10] Inverse temperature (softmax sensitivity).
    w_base: [0, 1] Base mixing weight (0=Pure MF, 1=Pure MB).
    w_oci_mod: [-1, 1] Modulation of w by OCI score.
    """
    learning_rate, beta, w_base, w_oci_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective mixing weight w, bounded between 0 and 1
    w = w_base + (w_oci_mod * oci_score)
    w = np.clip(w, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)      # Model-free values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for stage 2 (terminal)

    for trial in range(n_trials):
        # --- STAGE 1 CHOICE ---
        
        # 1. Model-Based Value Calculation (Bellman equation using transition matrix)
        # Max value of stage 2 states
        max_q_stage2 = np.max(q_stage2_mf, axis=1) 
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Hybrid Value Calculation
        q_stage1_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax Policy Stage 1
        exp_q1 = np.exp(beta * q_stage1_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # --- STAGE 2 CHOICE ---
        
        # Softmax Policy Stage 2 (Purely Model-Free as it's the terminal step)
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- UPDATES ---
        
        # SARSA(0) / TD(0) update for Stage 1 MF
        # Note: In Daw 2011, Stage 1 MF is often updated by Q(s2, a2) rather than max Q(s2).
        # We use the observed state's value.
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # TD(0) update for Stage 2 MF
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Eligibility trace: Stage 1 MF is typically also updated by the Stage 2 RPE (lambda=1)
        # This connects the final reward back to the first choice in the MF system.
        q_stage1_mf[a1] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Asymmetric Learning Rates Modulated by OCI
This model hypothesizes that individuals with higher OCI scores might learn differently from positive versus negative outcomes. Specifically, compulsivity might be associated with an over-sensitivity to punishment (or lack of reward) or a rigidity in updating values. Here, we split the learning rate into `alpha_pos` and `alpha_neg`, and let OCI modulate the `alpha_neg` parameter specifically, testing if high OCI leads to different updating on "disappointment".

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free Q-Learning with Asymmetric Learning Rates (Pos/Neg),
    where OCI modulates the Negative Learning Rate.
    
    Parameters:
    alpha_pos: [0, 1] Learning rate for positive prediction errors (RPE > 0).
    alpha_neg_base: [0, 1] Base learning rate for negative prediction errors.
    alpha_neg_oci_mod: [-1, 1] Modulation of negative learning rate by OCI.
    beta: [0, 10] Inverse temperature.
    """
    alpha_pos, alpha_neg_base, alpha_neg_oci_mod, beta = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective negative learning rate
    alpha_neg = alpha_neg_base + (alpha_neg_oci_mod * oci_score)
    alpha_neg = np.clip(alpha_neg, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]]) # Not used in pure MF, but part of template context
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- STAGE 1 CHOICE ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # --- STAGE 2 CHOICE ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- UPDATES ---
        
        # Stage 1 Update
        # Using Max Q for Stage 1 update (Q-learning)
        target_stage1 = np.max(q_stage2_mf[state_idx])
        delta_stage1 = target_stage1 - q_stage1_mf[a1]
        
        if delta_stage1 >= 0:
            q_stage1_mf[a1] += alpha_pos * delta_stage1
        else:
            q_stage1_mf[a1] += alpha_neg * delta_stage1
            
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        
        if delta_stage2 >= 0:
            q_stage2_mf[state_idx, a2] += alpha_pos * delta_stage2
        else:
            q_stage2_mf[state_idx, a2] += alpha_neg * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-modulated Inverse Temperature (Exploration/Exploitation)
This model tests the hypothesis that OCI scores relate to the randomness or determinism of choices (exploration vs. exploitation). High OCI might be linked to more rigid, deterministic responding (higher beta), or conversely, more uncertainty-driven exploration (lower beta). We modify the pure Hybrid model structure but fix `w` (mixing weight) to a constant or remove it (pure MF/MB) to focus parameters on Beta. Here, we assume a basic Model-Free framework but allow Beta to vary with OCI.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free Q-Learning with OCI-modulated Inverse Temperature (Beta).
    
    This tests if OCI affects the exploration-exploitation trade-off.
    
    Parameters:
    learning_rate: [0, 1] Update rate.
    beta_base: [0, 10] Base inverse temperature.
    beta_oci_mod: [-5, 5] Modulation of beta by OCI score.
    stickiness: [0, 5] Choice perseveration (constant).
    """
    learning_rate, beta_base, beta_oci_mod, stickiness = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective beta
    # We clip to ensure it stays within reasonable bounds and non-negative
    beta = beta_base + (beta_oci_mod * oci_score)
    beta = np.clip(beta, 0.0, 20.0) # Allow slightly higher upper bound for high rigidity
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1

    for trial in range(n_trials):
        # --- STAGE 1 CHOICE ---
        
        # Add stickiness bonus to Q-values for choice probability calculation
        q_stage1_net = q_stage1_mf.copy()
        if prev_action_1 != -1:
            q_stage1_net[prev_action_1] += stickiness

        exp_q1 = np.exp(beta * q_stage1_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # --- STAGE 2 CHOICE ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- UPDATES ---
        
        # SARSA(1) / TD(1) style update logic often used in 2-step tasks
        # Update Stage 1 based on Stage 2 value
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Update Stage 2 based on reward
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Direct reinforcement of Stage 1 by reward (eligibility trace)
        q_stage1_mf[a1] += learning_rate * delta_stage2
        
        prev_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```