Here are three new cognitive models, expressed as Python functions, that hypothesize different mechanisms for how Obsessive-Compulsive Inventory (OCI) scores influence decision-making.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid learner with Asymmetric Learning Rates modulated by OCI.
    
    Hypothesis: OCI scores correlate with altered sensitivity to negative prediction errors 
    (disappointment/punishment). High OCI individuals may learn more aggressively from 
    negative outcomes (errors) than positive ones, leading to an asymmetric update rule.
    
    Parameters:
    - lr_pos: [0, 1] Learning rate for positive prediction errors.
    - lr_neg_base: [0, 1] Base learning rate for negative prediction errors.
    - lr_neg_oci: [0, 1] Additional learning rate for negative errors per unit of OCI.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight for Model-Based (1) vs Model-Free (0).
    """
    lr_pos, lr_neg_base, lr_neg_oci, beta, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective negative learning rate, bounded between 0 and 1
    # Hypothesis: Higher OCI -> Higher sensitivity to "bad" outcomes
    lr_neg_effective = lr_neg_base + (lr_neg_oci * oci_score)
    lr_neg_effective = np.clip(lr_neg_effective, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # Policy for the first choice (Hybrid MB/MF)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = int(state[trial])

        # Policy for the second choice (Model-Free)
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Action value updating for Choice 1 (TD-SARSA style)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        # Use asymmetric learning rate based on sign of delta
        alpha_1 = lr_pos if delta_stage1 >= 0 else lr_neg_effective
        q_stage1_mf[action_1[trial]] += alpha_1 * delta_stage1
        
        # Action value updating for Choice 2 (Reward Prediction Error)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        # Use asymmetric learning rate based on sign of delta
        alpha_2 = lr_pos if delta_stage2 >= 0 else lr_neg_effective
        q_stage2_mf[state_idx, action_2[trial]] += alpha_2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid learner with Stage-Specific Inverse Temperatures, Stage 2 modulated by OCI.
    
    Hypothesis: OCI affects decision rigidity (determinism) differently at the immediate 
    reward stage (Stage 2) compared to the planning stage (Stage 1). High OCI may lead 
    to more rigid/deterministic choices (higher beta) when the reward is imminent, reflecting 
    a "need for control" in the concrete environment.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta_1: [0, 10] Inverse temperature for Stage 1 choices.
    - beta_2_base: [0, 10] Base inverse temperature for Stage 2 choices.
    - beta_2_oci: [0, 5] Increase in Stage 2 beta per unit of OCI.
    - w: [0, 1] Mixing weight for Model-Based (1) vs Model-Free (0).
    """
    learning_rate, beta_1, beta_2_base, beta_2_oci, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Effective Beta for Stage 2
    beta_2_effective = beta_2_base + (beta_2_oci * oci_score)
    # Ensure beta stays within reasonable bounds
    beta_2_effective = np.clip(beta_2_effective, 0.0, 20.0) 

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # Policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = int(state[trial])

        # Policy for the second choice (Uses OCI-modulated Beta)
        exp_q2 = np.exp(beta_2_effective * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Action value updating for Choice 1
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Action value updating for Choice 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid learner with Reward Sensitivity modulated by OCI.
    
    Hypothesis: OCI scores correlate with the subjective valuation of the reward (gold coins).
    High OCI might lead to hyper-valuation (e.g., hoarding instinct or perfectionism),
    effectively scaling the reward signal before it enters the prediction error calculation.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight for Model-Based (1) vs Model-Free (0).
    - rho_base: [0, 5] Base reward sensitivity (scaling factor).
    - rho_oci: [0, 5] Additional reward sensitivity per unit of OCI.
    """
    learning_rate, beta, w, rho_base, rho_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Effective Reward Sensitivity (rho)
    rho_effective = rho_base + (rho_oci * oci_score)
    rho_effective = np.maximum(rho_effective, 0.0) # Sensitivity cannot be negative

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # Policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = int(state[trial])

        # Policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Action value updating for Choice 1
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Action value updating for Choice 2 (Scaled Reward)
        effective_reward = reward[trial] * rho_effective
        delta_stage2 = effective_reward - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```