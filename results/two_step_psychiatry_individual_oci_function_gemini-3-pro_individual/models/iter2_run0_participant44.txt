Here are three new cognitive models exploring different mechanisms for how OCI scores might influence decision-making in this two-step task.

### Model 1: Hybrid Model with OCI-modulated Model-Based Weighting
This model tests the hypothesis that higher OCI scores relate to a reliance on habit (Model-Free) over goal-directed planning (Model-Based). It uses a mixing parameter `w` that determines the balance between MB and MF values in the first stage. This `w` is modulated by the participant's OCI score.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free with OCI-modulated mixing weight.
    
    Hypothesis: OCI score modulates the trade-off between Model-Based (planning)
    and Model-Free (habitual) control. High OCI might reduce Model-Based control
    (lower w).

    Parameters:
    - learning_rate: [0, 1] Learning rate for value updates.
    - beta: [0, 10] Inverse temperature (softmax sensitivity).
    - w_intercept: [0, 1] Baseline weight for Model-Based control (when OCI is 0).
    - w_slope: [-1, 1] How OCI changes the MB weight. (w = w_int + w_slope * oci).
      (Bounded to [0,1] internally).
    """
    learning_rate, beta, w_intercept, w_slope = model_parameters
    n_trials = len(action_1)
    oci_val = oci[0]
    
    # Calculate effective w (mixing weight) based on OCI
    # We clip it to ensure it stays a valid probability weight [0, 1]
    w = w_intercept + (w_slope * oci_val)
    if w < 0: w = 0
    if w > 1: w = 1

    # Fixed transition matrix for the task structure
    # A -> X (0.7), Y (0.3); U -> X (0.3), Y (0.7)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)      # Model-free Q-values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Model-free Q-values for stage 2

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based Value Calculation: V_MB = T * max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value: Q_net = w * Q_MB + (1-w) * Q_MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Policy ---
        state_idx = state[trial]
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning Updates ---
        # Reward Prediction Error at Stage 2
        r = reward[trial]
        delta_stage2 = r - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # TD Prediction Error at Stage 1 (SARSA-style update using Q-value of chosen stage 2 state)
        # Note: Standard Daw et al. 2011 uses the value of the state arrived at, 
        # but here we follow the template structure implying MF update.
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Eligibility Trace (Lambda)
This model implements a TD(lambda) learning rule where the eligibility trace parameter `lambda` is modulated by OCI. The eligibility trace determines how much credit the first-stage choice gets for the second-stage reward directly. A high `lambda` links the final reward strongly back to the first choice (mimicking model-based-like behavior without an explicit model), while a low `lambda` relies on the chain of prediction errors.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    TD(lambda) Model with OCI-modulated eligibility trace.
    
    Hypothesis: OCI affects the credit assignment process. Specifically, 
    it modulates 'lambda', the eligibility trace parameter. 
    High lambda allows direct updating of Stage 1 values from Stage 2 rewards.

    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - lambda_base: [0, 1] Base eligibility trace parameter.
    - lambda_oci_mod: [-1, 1] Modulation of lambda by OCI.
    """
    learning_rate, beta, lambda_base, lambda_oci_mod = model_parameters
    n_trials = len(action_1)
    oci_val = oci[0]
    
    # Calculate effective lambda
    eff_lambda = lambda_base + (lambda_oci_mod * oci_val)
    if eff_lambda < 0: eff_lambda = 0
    if eff_lambda > 1: eff_lambda = 1
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Chosen action 1
        a1 = action_1[trial]
        state_idx = state[trial]
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # Chosen action 2
        a2 = action_2[trial]
        r = reward[trial]
        
        # --- Updates ---
        # Stage 2 Prediction Error
        # The value of the second stage state-action pair is updated by the reward
        delta2 = r - q_stage2[state_idx, a2]
        q_stage2[state_idx, a2] += learning_rate * delta2
        
        # Stage 1 Prediction Error
        # The value of the first stage choice is updated by the value of the second stage choice
        delta1 = q_stage2[state_idx, a2] - q_stage1[a1]
        
        # Direct update from Stage 1 PE
        q_stage1[a1] += learning_rate * delta1
        
        # Eligibility Trace update: Stage 1 also learns from Stage 2 PE scaled by lambda
        q_stage1[a1] += learning_rate * eff_lambda * delta2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Learning Rate Asymmetry (Positive vs Negative)
This model posits that OCI scores relate to an imbalance in learning from positive versus negative prediction errors. For example, individuals with high OCI might be hyper-sensitive to errors (negative PE) or less sensitive to rewards (positive PE).

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free RL with OCI-modulated asymmetric learning rates.
    
    Hypothesis: OCI affects sensitivity to positive vs negative prediction errors.
    The model uses separate learning rates for positive and negative PEs.
    The ratio or magnitude of the negative learning rate is scaled by OCI.

    Parameters:
    - lr_pos: [0, 1] Learning rate for positive prediction errors.
    - lr_neg_base: [0, 1] Base learning rate for negative prediction errors.
    - beta: [0, 10] Inverse temperature.
    - oci_neg_scale: [0, 5] Scaling factor for negative learning rate based on OCI.
      (lr_neg = lr_neg_base * (1 + oci_neg_scale * OCI))
    """
    lr_pos, lr_neg_base, beta, oci_neg_scale = model_parameters
    n_trials = len(action_1)
    oci_val = oci[0]
    
    # Calculate effective negative learning rate
    # This implies high OCI might increase learning from disappointment/failure
    lr_neg = lr_neg_base * (1.0 + oci_neg_scale * oci_val)
    if lr_neg > 1: lr_neg = 1.0 # Bound it
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        state_idx = state[trial]
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]
        
        # --- Updates ---
        
        # Stage 2 Update
        delta2 = r - q_stage2[state_idx, a2]
        if delta2 >= 0:
            q_stage2[state_idx, a2] += lr_pos * delta2
        else:
            q_stage2[state_idx, a2] += lr_neg * delta2
            
        # Stage 1 Update
        delta1 = q_stage2[state_idx, a2] - q_stage1[a1]
        if delta1 >= 0:
            q_stage1[a1] += lr_pos * delta1
        else:
            q_stage1[a1] += lr_neg * delta1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```