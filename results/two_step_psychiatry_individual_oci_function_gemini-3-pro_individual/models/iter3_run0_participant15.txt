Here are three new cognitive models for the two-step task, incorporating OCI scores in distinct ways.

### Model 1: Differential Learning Rates
This model hypothesizes that OCI traits affect the granularity of learning. Specifically, it proposes that OCI modulates the learning rate for immediate outcomes (Stage 2: Alien rewards) differently than for abstract state values (Stage 1: Spaceship values).

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Differential Learning Rate Model.
    
    Hypothesis: OCI scores differentially impact the learning rate for the second stage 
    (concrete alien rewards) compared to the first stage (abstract spaceship values).
    
    Bounds:
    lr_s1: [0, 1] (Learning rate for Stage 1)
    lr_s2_base: [0, 1] (Base learning rate for Stage 2)
    lr_s2_oci_slope: [-1, 1] (Effect of OCI on Stage 2 learning rate)
    beta: [0, 10] (Inverse temperature)
    w: [0, 1] (Mixing weight for MB vs MF)
    """
    lr_s1, lr_s2_base, lr_s2_oci_slope, beta, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate stage 2 learning rate with OCI modulation
    lr_s2 = lr_s2_base + (lr_s2_oci_slope * oci_score)
    lr_s2 = np.clip(lr_s2, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        
        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # Updating
        # Stage 1 Update (Fixed LR)
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += lr_s1 * delta_stage1
        
        # Stage 2 Update (OCI-modulated LR)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += lr_s2 * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Distorted Transition Beliefs
This model suggests that OCI affects the internal model of the environment's structure. It allows the participant's belief about the transition probabilities (how likely Spaceship A goes to Planet X) to deviate from the true value (0.7) as a function of their OCI score.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Distorted Transition Belief Model.
    
    Hypothesis: OCI modulates the internal model of the transition probabilities used
    in Model-Based planning. High OCI might lead to believing transitions are more 
    deterministic (rigid structure) or more random (uncertainty) than reality.
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    w: [0, 1]
    trans_prob_base: [0.5, 1.0] (Base belief about common transition probability)
    trans_prob_oci_mod: [-0.5, 0.5] (How OCI distorts this belief)
    """
    learning_rate, beta, w, trans_prob_base, trans_prob_oci_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate subjective transition probability
    p_trans = trans_prob_base + (trans_prob_oci_mod * oci_score)
    p_trans = np.clip(p_trans, 0.0, 1.0)
    
    # Subjective transition matrix used for MB planning
    transition_matrix = np.array([[p_trans, 1-p_trans], [1-p_trans, p_trans]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        # MB values derived from subjective transition matrix
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        
        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # Updating
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Independent Controller Strengths
Instead of a single inverse temperature (beta) and a mixing weight (w), this model assigns independent "strengths" to the Model-Based and Model-Free systems. It hypothesizes that OCI specifically modulates the strength (or "gain") of the Model-Free system, allowing for states where both systems are strong (high conflict) or both are weak (randomness), which a simple mixing weight cannot capture.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Independent Controller Strength Model.
    
    Hypothesis: The Model-Based and Model-Free systems have independent 'strengths' 
    (inverse temperatures). The strength of the Model-Free system is modulated by OCI,
    reflecting how compulsivity might drive habitual value sensitivity independent of planning.
    
    Bounds:
    learning_rate: [0, 1]
    beta_mb: [0, 10] (Strength of MB system at Stage 1)
    beta_mf_base: [0, 10] (Base strength of MF system at Stage 1)
    beta_mf_oci_slope: [-5, 5] (OCI modulation of MF strength)
    beta_s2: [0, 10] (Inverse temperature for Stage 2 choices)
    """
    learning_rate, beta_mb, beta_mf_base, beta_mf_oci_slope, beta_s2 = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate OCI-modulated MF strength
    beta_mf = beta_mf_base + (beta_mf_oci_slope * oci_score)
    beta_mf = np.maximum(beta_mf, 0.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combined logits: Independent contributions from MB and MF systems
        logits = (beta_mb * q_stage1_mb) + (beta_mf * q_stage1_mf)
        
        # Softmax on logits
        logits = logits - np.max(logits) # for numerical stability
        exp_logits = np.exp(logits)
        probs_1 = exp_logits / np.sum(exp_logits)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        
        # Stage 2 Policy
        exp_q2 = np.exp(beta_s2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # Updating
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```