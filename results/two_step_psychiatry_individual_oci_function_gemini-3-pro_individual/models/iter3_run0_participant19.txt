Here are 3 new cognitive models that introduce different mechanisms for how OCI scores might influence decision-making in this two-step task.

### Model 1: Hybrid Learner with OCI-modulated Model-Based Weight (`w`)
This model hypothesizes that individuals with higher OCI scores rely more on habitual (model-free) control rather than goal-directed (model-based) planning. The parameter `w` controls the balance between these two systems. Here, the OCI score negatively modulates `w`, meaning higher OCI leads to less model-based influence (more habits).

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free learner where OCI modulates the mixing weight 'w'.
    
    Hypothesis: Higher OCI scores are associated with reduced model-based control (goal-directedness)
    and increased reliance on model-free (habitual) systems.
    
    Parameters:
    learning_rate: [0, 1] Learning rate for value updates.
    beta: [0, 10] Inverse temperature for softmax choice.
    w_intercept: [0, 1] Baseline weight for model-based control (at OCI=0).
    oci_slope: [0, 1] How strongly OCI reduces the model-based weight.
    """
    learning_rate, beta, w_intercept, oci_slope = model_parameters
    n_trials = len(action_1)
    oci_val = oci[0]
    
    # Calculate mixing weight w, bounded between 0 and 1.
    # Higher OCI -> Lower w (less Model-Based, more Model-Free)
    w = w_intercept - (oci_slope * oci_val)
    w = np.clip(w, 0.0, 1.0)

    # Fixed transition matrix for Model-Based calculation
    # Row 0: Space 0 -> [Planet 0 (0.7), Planet 1 (0.3)]
    # Row 1: Space 1 -> [Planet 0 (0.3), Planet 1 (0.7)]
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)      # Model-Free values for Stage 1
    q_stage2_mf = np.zeros((2, 2)) # Model-Free values for Stage 2 (Planets)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based Value: V_MB(s1) = T * max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value: Q_net = w * Q_MB + (1-w) * Q_MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        r = reward[trial]
        
        # TD Error Stage 2
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # TD Error Stage 1 (using SARSA-style update with Stage 2 value)
        # Note: In standard MB/MF models, MF update often uses Q(s2, a2)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Loss Aversion Learner with OCI-modulated Loss Sensitivity
This model explores the idea that OCI is related to enhanced sensitivity to punishment or negative outcomes. It splits the learning rate (or value update magnitude) based on whether the reward is positive or negative. The OCI score specifically amplifies the impact of negative rewards (losses).

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner with separate processing for gains and losses.
    
    Hypothesis: OCI score modulates 'loss sensitivity'. High OCI individuals 
    react more strongly to negative rewards (losses) than low OCI individuals.
    
    Parameters:
    lr_base: [0, 1] Base learning rate for positive outcomes.
    beta: [0, 10] Inverse temperature.
    loss_mult_base: [0, 5] Baseline multiplier for negative outcomes (loss aversion).
    oci_sens: [0, 5] How much OCI increases the loss multiplier.
    """
    lr_base, beta, loss_mult_base, oci_sens = model_parameters
    n_trials = len(action_1)
    oci_val = oci[0]

    # Calculate the effective multiplier for losses
    # If loss_multiplier > 1, losses drive learning faster/stronger than gains
    loss_multiplier = loss_mult_base + (oci_sens * oci_val)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy (Pure Model-Free for simplicity to isolate loss mechanism) ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        r = reward[trial]
        
        # Determine effective learning rate based on outcome valence
        # Assuming rewards are roughly -1 or 1 (or 0)
        if r < 0:
            eff_lr = lr_base * loss_multiplier
        else:
            eff_lr = lr_base
            
        # Clip learning rate to keep stability
        eff_lr = np.clip(eff_lr, 0.0, 1.0)
  
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += eff_lr * delta_stage2
        
        # Stage 1 update
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += eff_lr * delta_stage1 # Use same modulated rate for consistency

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Uncertainty-Driven Exploration with OCI Modulation
This model posits that OCI relates to intolerance of uncertainty. Instead of just tracking values (Q), the model tracks the variance/uncertainty of those values. The OCI score modulates an "uncertainty penalty" (or bonus) parameter `kappa`. High OCI might lead to avoiding uncertain options (penalty) or obsessively checking them (bonus); this model allows the data to fit the direction via the base and modulation parameters.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner with Uncertainty Modulation (Kalman Filter-like approximation).
    
    Hypothesis: OCI modulates how uncertainty affects choice. 
    High OCI might lead to uncertainty avoidance (negative kappa).
    
    Parameters:
    lr: [0, 1] Learning rate for value mean.
    beta: [0, 10] Inverse temperature.
    kappa_base: [0, 1] Base weight for uncertainty (variance) in choice.
    oci_uncert: [0, 1] OCI modulation of the uncertainty weight.
    phi: [0, 1] Decay rate of uncertainty (how fast uncertainty grows when unchosen).
    """
    lr, beta, kappa_base, oci_uncert, phi = model_parameters
    n_trials = len(action_1)
    oci_val = oci[0]
    
    # Effective kappa: how much variance adds to value
    # Can represent exploration bonus or uncertainty avoidance
    # We map parameters to allow a range. Here we assume it acts as a bonus/penalty.
    # To allow sign flexibility within [0,1] bounds, we can treat kappa_base as an offset
    # But sticking to simple additive logic:
    kappa_eff = kappa_base + (oci_uncert * oci_val) 

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Means
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    # Variances (uncertainty tracking) - Initialize with some uncertainty
    v_stage1 = np.ones(2) 
    v_stage2 = np.ones((2, 2))

    for trial in range(n_trials):
        
        # --- Stage 1 Choice ---
        # Value = Mean + Kappa * sqrt(Variance)
        # Note: If kappa is interpreted as 'intolerance', we might expect negative fitting weights
        # but here we model it as a bias term added to Q.
        
        total_val_1 = q_stage1 + (kappa_eff * np.sqrt(v_stage1))
        
        exp_q1 = np.exp(beta * total_val_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Choice ---
        state_idx = int(state[trial])
        
        total_val_2 = q_stage2[state_idx] + (kappa_eff * np.sqrt(v_stage2[state_idx]))
        
        exp_q2 = np.exp(beta * total_val_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        r = reward[trial]
        
        # Update Mean (Standard Rescorla-Wagner)
        delta2 = r - q_stage2[state_idx, a2]
        q_stage2[state_idx, a2] += lr * delta2
        
        delta1 = q_stage2[state_idx, a2] - q_stage1[a1]
        q_stage1[a1] += lr * delta1
        
        # Update Variance (Approximate logic: chosen decreases, unchosen increases)
        # Decrease variance of chosen option
        v_stage2[state_idx, a2] *= (1 - lr) 
        v_stage1[a1] *= (1 - lr)
        
        # Increase variance of ALL options (passive forgetting/uncertainty growth)
        # This prevents variance from just going to zero forever
        v_stage2 += phi
        v_stage1 += phi
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```