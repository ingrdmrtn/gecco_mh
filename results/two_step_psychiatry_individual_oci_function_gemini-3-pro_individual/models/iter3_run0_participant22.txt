Here are three new cognitive models expressed as Python functions. These models explore different mechanisms by which Obsessive-Compulsive symptoms (approximated by the OCI score) might influence decision-making in the two-step task, specifically focusing on the balance between Model-Based (planning) and Model-Free (habit) systems, as well as learning rates.

### Model 1: Hybrid RL with OCI-Modulated Mixing Weight
This model posits that the OCI score influences the balance between Model-Based (goal-directed) and Model-Free (habitual) control. High OCI scores might be associated with a stronger reliance on habitual (Model-Free) systems, leading to a lower mixing weight `w` for the Model-Based component.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL model where the balance between Model-Based (MB) and Model-Free (MF) 
    control is modulated by the OCI score.
    
    Hypothesis: Higher OCI scores lead to reduced Model-Based control (lower w).
    w = w_max * (1 - oci_sensitivity * oci)
    
    Bounds:
    learning_rate: [0,1]
    beta: [0,10]
    w_max: [0,1] - Maximum weight for MB system.
    oci_sensitivity: [0,1] - How much OCI reduces MB weight.
    """
    learning_rate, beta, w_max, oci_sensitivity = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective mixing weight w
    # Ensure w stays within [0, 1]
    w_effective = w_max * (1.0 - oci_sensitivity * oci_score)
    w_effective = np.clip(w_effective, 0.0, 1.0)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        # Model-Based Value: V(s') = max(Q_MF(s', a'))
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value: w * MB + (1-w) * MF
        q_hybrid = w_effective * q_stage1_mb + (1 - w_effective) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_hybrid)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])

        # --- Stage 2 Policy ---
        # Standard Softmax on Stage 2 Q-values
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # --- Learning ---
        # SARSA-style update for Stage 1 MF
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Standard Q-learning update for Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Learning Rate Asymmetry (Positive vs. Negative)
This model tests the hypothesis that OCI traits affect how individuals learn from positive versus negative prediction errors. Specifically, it proposes that the OCI score scales the learning rate for negative prediction errors (punishment sensitivity/avoidance), potentially making the agent more reactive to missed rewards or losses.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free RL with asymmetric learning rates modulated by OCI.
    
    Hypothesis: OCI score specifically amplifies learning from negative prediction errors 
    (disappointments), reflecting a potential 'fear of failure' or perfectionism.
    
    alpha_pos = base_alpha
    alpha_neg = base_alpha * (1 + oci_scale * oci)
    
    Bounds:
    base_alpha: [0,1]
    beta: [0,10]
    oci_scale: [0,5] - Multiplier for OCI effect on negative alpha.
    """
    base_alpha, beta, oci_scale = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    # Define learning rates
    alpha_pos = base_alpha
    alpha_neg = base_alpha * (1.0 + oci_scale * oci_score)
    # Clip alpha_neg to prevent instability, though bounds usually handle this
    alpha_neg = np.clip(alpha_neg, 0.0, 1.0) 

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Pure Model-Free Q-values
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])

        # --- Stage 1 Policy ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # --- Learning ---
        
        # Stage 1 Update
        delta_stage1 = q_stage2[state_idx, a2] - q_stage1[a1]
        if delta_stage1 >= 0:
            q_stage1[a1] += alpha_pos * delta_stage1
        else:
            q_stage1[a1] += alpha_neg * delta_stage1
            
        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2[state_idx, a2]
        if delta_stage2 >= 0:
            q_stage2[state_idx, a2] += alpha_pos * delta_stage2
        else:
            q_stage2[state_idx, a2] += alpha_neg * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Dependent Eligibility Trace (Lambda)
This model investigates if OCI scores influence the "eligibility trace" parameter ($\lambda$). In reinforcement learning, $\lambda$ controls how much credit for a reward is assigned to the first-stage choice versus the second-stage choice. A higher $\lambda$ links the reward more strongly back to the initial decision (Stage 1). We hypothesize that OCI might correlate with a higher $\lambda$, representing a tendency to over-attribute outcomes to initial causes or an inability to "let go" of the previous state.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    TD(lambda) Model-Free RL where the eligibility trace parameter (lambda)
    is modulated by the OCI score.
    
    Hypothesis: Higher OCI leads to higher lambda, meaning credit for the reward 
    is propagated more strongly back to the first stage choice.
    
    lambda = lambda_base + (lambda_oci_slope * oci)
    
    Bounds:
    learning_rate: [0,1]
    beta: [0,10]
    lambda_base: [0,1] - Baseline eligibility trace.
    lambda_oci_slope: [0,1] - Sensitivity of lambda to OCI.
    """
    learning_rate, beta, lambda_base, lambda_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective lambda
    lambda_eff = lambda_base + (lambda_oci_slope * oci_score)
    lambda_eff = np.clip(lambda_eff, 0.0, 1.0)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])

        # --- Stage 1 Policy ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # --- Learning with Eligibility Traces ---
        
        # Prediction error at Stage 1 (TD error)
        # Note: In standard TD(lambda) for this task, we often view the transition
        # to stage 2 as having 0 immediate reward.
        # delta1 = r1 + gamma * V(s') - V(s) -> here r1=0, gamma=1
        delta_stage1 = q_stage2[state_idx, a2] - q_stage1[a1]
        
        # Prediction error at Stage 2
        # delta2 = r2 - V(s')
        delta_stage2 = reward[trial] - q_stage2[state_idx, a2]
        
        # Update Stage 2 Q-values (standard)
        q_stage2[state_idx, a2] += learning_rate * delta_stage2
        
        # Update Stage 1 Q-values
        # The update includes the direct error from stage 1 transition (delta_stage1)
        # PLUS the trace of the error from stage 2 (lambda * delta_stage2)
        q_stage1[a1] += learning_rate * (delta_stage1 + lambda_eff * delta_stage2)
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```