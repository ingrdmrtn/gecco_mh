Here are three new cognitive models based on the provided template and task description.

### Model 1: OCI-Modulated Eligibility Trace
This model assumes that the Obsessive-Compulsive Inventory (OCI) score modulates the **eligibility trace** ($\lambda$). The eligibility trace determines how much the outcome at the second stage (reward) reinforces the choice made at the first stage. A higher eligibility trace implies a stronger "binding" between the final outcome and the initial decision, potentially reflecting a compulsion to attribute outcomes to distant antecedents.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Eligibility Trace.
    
    This model assumes OCI modulates the eligibility trace parameter (lambda).
    Lambda controls how much the Stage 2 reward prediction error updates the 
    Stage 1 Model-Free Q-values.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for Q-value updates.
    - beta: [0, 10] Inverse temperature (softmax sensitivity) for both stages.
    - w: [0, 1] Weighting parameter (0 = pure MF, 1 = pure MB).
    - lambda_base: [0, 1] Baseline eligibility trace.
    - oci_lambda_mod: [-1, 1] Modulation of lambda by OCI score.
    """
    learning_rate, beta, w, lambda_base, oci_lambda_mod = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
  
    # Calculate OCI-modulated eligibility trace
    lambda_val = lambda_base + (current_oci * oci_lambda_mod)
    lambda_val = np.clip(lambda_val, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Q-value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        # Update Stage 1 (TD-0 part)
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1
        
        # Update Stage 2 and Eligibility Trace part for Stage 1
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2
        
        # Apply eligibility trace update to Stage 1 choice based on Stage 2 error
        q_stage1_mf[int(action_1[trial])] += learning_rate * lambda_val * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Stage 1 Exploration
This model hypothesizes that OCI specifically affects the exploration-exploitation trade-off at the **first stage** (choosing a spaceship), while the second stage (choosing an alien) remains governed by a baseline temperature. High OCI might lead to more rigid (high beta) or more chaotic (low beta) planning choices, distinct from the immediate reaction to aliens.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Modulated Stage 1 Exploration.
    
    This model posits that OCI affects the inverse temperature (beta) specifically
    for the first stage decision (planning/spaceship choice), allowing for 
    different levels of randomness in planning vs. immediate reaction.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - w: [0, 1] MB/MF mixing weight.
    - beta_2: [0, 10] Inverse temperature for Stage 2 (Alien choice).
    - beta_1_base: [0, 10] Baseline inverse temperature for Stage 1.
    - oci_beta1_mod: [-5, 5] Modulation of Stage 1 beta by OCI.
    """
    learning_rate, w, beta_2, beta_1_base, oci_beta1_mod = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
  
    # Calculate OCI-modulated Stage 1 beta
    beta_1 = beta_1_base + (current_oci * oci_beta1_mod)
    beta_1 = np.clip(beta_1, 0.0, 10.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Use beta_1 here
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])

        # policy for the second choice
        # Use beta_2 here
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Memory Decay
This model introduces a **decay parameter** for unchosen options. It suggests that OCI modulates how quickly the value of unchosen actions decays (is forgotten). High OCI might be associated with "stickier" memory (low decay), where old values persist longer, or conversely, a need to constantly refresh memory.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Memory Decay.
    
    This model assumes that Q-values for unchosen options decay over time,
    and the rate of this decay is modulated by OCI.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for chosen options.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] MB/MF mixing weight.
    - decay_base: [0, 1] Baseline decay rate for unchosen options (0=no decay, 1=instant forget).
    - oci_decay_mod: [-1, 1] Modulation of decay rate by OCI.
    """
    learning_rate, beta, w, decay_base, oci_decay_mod = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
  
    # Calculate OCI-modulated decay
    decay_rate = decay_base + (current_oci * oci_decay_mod)
    decay_rate = np.clip(decay_rate, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2
        
        # Apply decay to unchosen options
        # Stage 1 unchosen
        unchosen_1 = 1 - int(action_1[trial])
        q_stage1_mf[unchosen_1] *= (1.0 - decay_rate)
        
        # Stage 2 unchosen (only for the visited state)
        unchosen_2 = 1 - int(action_2[trial])
        q_stage2_mf[state_idx, unchosen_2] *= (1.0 - decay_rate)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```