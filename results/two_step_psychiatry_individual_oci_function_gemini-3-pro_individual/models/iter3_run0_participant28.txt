Here are three new cognitive models implemented as Python functions, designed to explain individual variability in the two-step task using OCI scores.

### Model 1: OCI-modulated Transition Belief (Intolerance of Uncertainty)
This model hypothesizes that high OCI scores, often associated with an intolerance of uncertainty, lead participants to rely on a distorted Model-Based transition matrix. Specifically, high OCI participants may perceive the "common" transitions as more deterministic than they actually are (e.g., treating 70% as 90% or 100%), effectively ignoring the stochastic nature of the state transitions in their planning.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model with OCI-modulated Transition Belief.
    
    Hypothesis: High OCI individuals (Intolerance of Uncertainty) may rely on a 
    distorted Model-Based transition matrix that exaggerates the probability of 
    common transitions, perceiving the world as more deterministic than it is.
    
    Parameters:
    learning_rate: [0,1] - Learning rate for MF values.
    beta: [0,10] - Inverse temperature.
    w: [0,1] - Weight for Model-Based control.
    oci_belief_skew: [0,1] - How much OCI distorts the transition probability towards 1.0.
    """
    learning_rate, beta, w, oci_belief_skew = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    # Distort the transition matrix based on OCI
    # Base is 0.7. Max distortion makes it 1.0.
    # We scale the 0.3 gap by the OCI factor.
    p_common = 0.7 + (0.3 * oci_belief_skew * current_oci) 
    # Clamp to ensure valid probability
    if p_common > 0.999: p_common = 0.999
    
    transition_matrix = np.array([[p_common, 1-p_common], [1-p_common, p_common]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        s_idx = int(state[trial])

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
  
        # Update
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-modulated Counterfactual Updating (Regret)
This model posits that high OCI is associated with increased counterfactual thinking ("What if I had chosen the other spaceship?"). In this model, OCI drives an update to the *unchosen* Stage 1 action. If the chosen action results in a positive prediction error (better than expected), the unchosen action is devalued. If the chosen action results in a negative prediction error (worse than expected), the unchosen action is valued up (regret).

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model with OCI-modulated Counterfactual Updating.
    
    Hypothesis: High OCI is associated with doubt and "what if" thinking. 
    This model posits that OCI drives updates to the *unchosen* Stage 1 action 
    in the opposite direction of the prediction error (Fictitious Play/Regret).
    
    Parameters:
    learning_rate: [0,1] - Learning rate for chosen action.
    beta: [0,10] - Inverse temperature.
    w: [0,1] - Weight for Model-Based control.
    oci_cf_scale: [0,1] - Scaling factor for counterfactual updating based on OCI.
    """
    learning_rate, beta, w, oci_cf_scale = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        s_idx = int(state[trial])

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
  
        # Update
        # TD(0) for Stage 1
        v_stage2 = q_stage2_mf[s_idx, a2]
        delta_stage1 = v_stage2 - q_stage1_mf[a1]
        
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Counterfactual update for unchosen action
        unc_a1 = 1 - a1
        cf_lr = learning_rate * oci_cf_scale * current_oci
        # Inverse update: if delta > 0 (good surprise), unchosen gets worse. 
        # If delta < 0 (bad surprise), unchosen gets better (regret).
        q_stage1_mf[unc_a1] -= cf_lr * delta_stage1 
        
        delta_stage2 = reward[trial] - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-modulated Sensitivity to Rare Transitions
This model hypothesizes that high OCI individuals are hypersensitive to violations of expectation (surprises). When a "Rare" transition occurs (Spaceship A -> Planet Y, or U -> X), high OCI participants treat this as a significant signal and boost their learning rate for that trial, updating their values more aggressively than they would after a Common transition.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model with OCI-modulated Sensitivity to Rare Transitions.
    
    Hypothesis: High OCI individuals are hypersensitive to violations of expectation.
    When a 'Rare' transition occurs (0->1 or 1->0), they treat this as a significant 
    event and increase their learning rate for that trial.
    
    Parameters:
    learning_rate: [0,1] - Base learning rate.
    beta: [0,10] - Inverse temperature.
    w: [0,1] - Weight for Model-Based control.
    oci_rare_boost: [0,10] - Multiplier for learning rate on Rare trials (scaled by OCI).
    """
    learning_rate, beta, w, oci_rare_boost = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        s_idx = int(state[trial])

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
  
        # Determine if transition was Rare
        # Common: A(0)->X(0) or U(1)->Y(1). Rare: 0->1 or 1->0.
        is_rare = (a1 != s_idx)
        
        # Modulate Learning Rate
        lr_eff = learning_rate
        if is_rare:
            # Boost LR based on OCI
            boost_factor = 1.0 + (oci_rare_boost * current_oci)
            lr_eff = learning_rate * boost_factor
            if lr_eff > 1.0: lr_eff = 1.0
        
        # Update
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr_eff * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr_eff * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```