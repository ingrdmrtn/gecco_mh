Here are three new cognitive models based on the two-step task, incorporating the OCI score into different mechanisms of learning and decision-making.

### Model 1: Hybrid Learner with OCI-Distorted Transition Beliefs
**Hypothesis:** Individuals with high OCI symptoms may mistrust the stability of the environment or overestimate the probability of rare events (catastrophizing/uncertainty). In this model, OCI distorts the Model-Based system's understanding of the transition matrix. While a standard agent knows the 70/30 transition probability, a high-OCI agent perceives the transitions as more random (closer to 50/50), reducing the effectiveness of their planning.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model where OCI distorts the Model-Based transition matrix.
    
    High OCI reduces the perceived probability of the common transition,
    making the agent's planning phase less confident/accurate.
    
    perceived_prob = 0.7 - (oci * distortion_param)
    (Clipped so it doesn't go below 0.5).
    
    Parameters:
    learning_rate: [0, 1] - Learning rate for MF values.
    beta: [0, 10] - Inverse temperature (softmax).
    w: [0, 1] - Weighting parameter (0=Pure MF, 1=Pure MB).
    oci_distortion: [0, 0.4] - How much OCI flattens the transition matrix.
    """
    learning_rate, beta, w, oci_distortion = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Apply OCI distortion to the transition belief
    # If distortion is high, 0.7 drops towards 0.5 (max uncertainty)
    perceived_common = 0.7 - (current_oci * oci_distortion)
    # Clip to ensure valid probability and maintain common > rare assumption
    perceived_common = np.clip(perceived_common, 0.5, 0.99)
    perceived_rare = 1.0 - perceived_common
    
    # Custom transition matrix based on OCI
    transition_matrix = np.array([[perceived_common, perceived_rare], 
                                  [perceived_rare, perceived_common]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Policy for Choice 1 ---
        # Model-Based Value: Expected value of next stage weighted by transition probs
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Value
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = int(state[trial])

        # --- Policy for Choice 2 ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updating ---
        chosen_a1 = action_1[trial]
        chosen_a2 = action_2[trial]
        r = reward[trial]

        # Stage 1 MF Update (TD(0))
        # Note: In a full hybrid model, MF Q1 is updated by the value of the state reached (or Q2)
        delta_stage1 = q_stage2_mf[state_idx, chosen_a2] - q_stage1_mf[chosen_a1]
        q_stage1_mf[chosen_a1] += learning_rate * delta_stage1
        
        # Stage 2 MF Update
        delta_stage2 = r - q_stage2_mf[state_idx, chosen_a2]
        q_stage2_mf[state_idx, chosen_a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Learner with OCI-Driven Perseveration
**Hypothesis:** Compulsivity is characterized by repetitive behaviors and "stickiness" to previous actions regardless of outcomes. Instead of OCI modulating a learning rate, this model posits that OCI directly drives a *perseveration bias*. High OCI participants will artificially inflate the value of the action they just took, making them resistant to switching even after negative feedback.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model with OCI-modulated Stickiness (Perseveration).
    
    The agent combines MB and MF values, but adds a 'stickiness' bonus
    to the previously chosen action. The magnitude of this stickiness
    is determined by the OCI score.
    
    stickiness_bonus = oci * oci_stick_strength
    
    Parameters:
    learning_rate: [0, 1]
    beta: [0, 10]
    w: [0, 1] - MB/MF mixing weight.
    oci_stick_strength: [0, 5] - How strongly OCI induces repetition.
    """
    learning_rate, beta, w, oci_stick_strength = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate stickiness magnitude based on OCI
    stickiness_val = current_oci * oci_stick_strength
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):

        # --- Policy for Choice 1 ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Base Net Value
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Add Stickiness to the previously chosen action
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness_val
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Update tracker
        last_action_1 = action_1[trial]
        state_idx = int(state[trial])

        # --- Policy for Choice 2 ---
        # (Standard MF for stage 2)
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updating ---
        chosen_a1 = action_1[trial]
        chosen_a2 = action_2[trial]
        r = reward[trial]

        delta_stage1 = q_stage2_mf[state_idx, chosen_a2] - q_stage1_mf[chosen_a1]
        q_stage1_mf[chosen_a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[state_idx, chosen_a2]
        q_stage2_mf[state_idx, chosen_a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Learner with OCI-Decoupled Stage 2 Learning
**Hypothesis:** OCI might differentially impact abstract planning (Stage 1) versus concrete reward processing (Stage 2). This model proposes that high OCI individuals are hyper-responsive to the immediate, concrete feedback from the aliens (Stage 2), potentially due to heightened error monitoring, while their base learning rate for the abstract spaceship choice remains standard. This creates a disconnect between the two stages.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model with Differential Learning Rates modulated by OCI.
    
    The learning rate for Stage 2 (aliens/concrete reward) is scaled by OCI,
    making high-OCI participants update immediate outcome values more aggressively.
    Stage 1 learning uses the base rate.
    
    lr_stage2 = lr_base * (1 + (oci * oci_lr_scale))
    
    Parameters:
    lr_base: [0, 1] - Base learning rate (used for Stage 1).
    beta: [0, 10]
    w: [0, 1] - MB/MF mixing.
    oci_lr_scale: [0, 5] - Multiplier for OCI impact on Stage 2 learning.
    """
    lr_base, beta, w, oci_lr_scale = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate Stage 2 specific learning rate
    # We clip to 1.0 to ensure stability
    lr_stage2 = lr_base * (1.0 + (current_oci * oci_lr_scale))
    lr_stage2 = np.clip(lr_stage2, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Policy for Choice 1 ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = int(state[trial])

        # --- Policy for Choice 2 ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updating ---
        chosen_a1 = action_1[trial]
        chosen_a2 = action_2[trial]
        r = reward[trial]

        # Stage 1 update uses base learning rate
        delta_stage1 = q_stage2_mf[state_idx, chosen_a2] - q_stage1_mf[chosen_a1]
        q_stage1_mf[chosen_a1] += lr_base * delta_stage1
        
        # Stage 2 update uses OCI-boosted learning rate
        delta_stage2 = r - q_stage2_mf[state_idx, chosen_a2]
        q_stage2_mf[state_idx, chosen_a2] += lr_stage2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```