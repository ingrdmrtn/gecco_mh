Here are three new cognitive models that incorporate the OCI score into the decision-making process in the two-step task.

### Model 1: OCI-Modulated Model-Based/Model-Free Balance
This model hypothesizes that high OCI scores are associated with a rigid reliance on habit (Model-Free) over goal-directed planning (Model-Based), or potentially the inverse depending on the fit. Here, the weighting parameter `w` (which balances MB and MF control) is directly modulated by the OCI score.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Model-Based/Model-Free Balance.
    
    Hypothesis: 
    OCI score shifts the balance between Model-Based (planning) and Model-Free (habit) control.
    Instead of a fixed 'w', the weight is a logistic function of a base weight and the OCI score,
    allowing OCI to push the participant towards more habitual (w -> 0) or goal-directed (w -> 1) behavior.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for Q-value updates.
    - beta: [0, 10] Inverse temperature for softmax.
    - w_base: [0, 1] Baseline mixing weight (before OCI modulation).
    - oci_mod: [0, 5] Strength of OCI modulation on the mixing weight.
      (Positive values imply OCI increases MB, negative logic handled via transformation).
      
    Mechanism:
    - w_effective = 1 / (1 + exp(-(logit(w_base) + oci_mod * (oci - 0.5))))
    - This keeps w bounded [0,1] while allowing OCI to shift it.
    """
    learning_rate, beta, w_base, oci_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Transform w_base to logit space to apply linear modulation safely
    # Clip to avoid log(0)
    w_base = np.clip(w_base, 0.01, 0.99)
    logit_w = np.log(w_base / (1 - w_base))
    
    # Calculate effective w based on OCI
    # Centering OCI around 0.5 (typical mean) for stability
    logit_w_eff = logit_w + oci_mod * (oci_score - 0.5)
    w = 1 / (1 + np.exp(-logit_w_eff))

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Q-value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Update Stage 1 MF
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Update Stage 2 MF
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Eligibility trace update for Stage 1 (TD(1) logic common in these models)
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Driven Asymmetric Learning Rates
This model suggests that high OCI scores might relate to an asymmetry in how positive versus negative prediction errors are processed. For example, individuals with high OCI might be hyper-sensitive to negative outcomes (errors) or failures to receive a reward, leading to "checking" behaviors or rapid shifting.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Driven Asymmetric Learning Rates.
    
    Hypothesis: 
    OCI scores modulate the ratio between learning from positive prediction errors (rewards)
    vs negative prediction errors (omissions). High OCI might lead to over-weighting 
    negative feedback (loss aversion/anxiety).
    
    Parameters:
    - lr_base: [0, 1] Base learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] MB/MF weight.
    - oci_asym: [0, 1] Factor by which OCI distorts the learning rate for negative PEs.
    
    Mechanism:
    - lr_pos = lr_base
    - lr_neg = lr_base * (1 + oci_asym * oci_score) (or similar scaling)
    - If PE > 0, use lr_pos. If PE < 0, use lr_neg.
    """
    lr_base, beta, w, oci_asym = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Define asymmetric learning rates based on OCI
    # If oci_asym is high, high OCI people learn much faster from disappointment
    # We clip to ensure LR doesn't exceed 1.0 arbitrarily
    lr_pos = lr_base
    lr_neg = np.clip(lr_base * (1.0 + oci_asym * (oci_score * 5)), 0, 1) 

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Update Stage 1
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        lr_1 = lr_pos if delta_stage1 >= 0 else lr_neg
        q_stage1_mf[action_1[trial]] += lr_1 * delta_stage1
        
        # Update Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        lr_2 = lr_pos if delta_stage2 >= 0 else lr_neg
        q_stage2_mf[state_idx, action_2[trial]] += lr_2 * delta_stage2
        
        # Eligibility trace for Stage 1
        q_stage1_mf[action_1[trial]] += lr_1 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Scaled Exploration (Inverse Temperature)
This model posits that OCI affects decision noise or exploration. Obsessive-compulsive traits might manifest as a rigid adherence to the highest value option (low temperature/high beta) due to anxiety about uncertainty, or conversely, high noise if the participant is obsessively checking alternatives.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Scaled Exploration (Inverse Temperature).
    
    Hypothesis: 
    OCI score modulates the 'beta' (inverse temperature) parameter.
    High OCI might lead to 'determinism' (high beta) where the participant 
    rigidly picks the max Q-value, or 'uncertainty' (low beta).
    
    Parameters:
    - learning_rate: [0, 1]
    - beta_base: [0, 10] Baseline inverse temperature.
    - w: [0, 1] MB/MF weight.
    - oci_beta_slope: [0, 5] How much OCI adds/subtracts from beta.
      (We model: beta = beta_base + oci_beta_slope * OCI).
    
    Mechanism:
    - The softmax temperature is directly modified by the trait OCI score.
    """
    learning_rate, beta_base, w, oci_beta_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective beta
    # If oci_beta_slope is positive, higher OCI = more exploitation/rigidity
    beta_eff = beta_base + (oci_beta_slope * oci_score)
    # Ensure beta stays within reasonable bounds (e.g., non-negative)
    beta_eff = np.maximum(beta_eff, 0.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Use effective beta
        exp_q1 = np.exp(beta_eff * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta_eff * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Update Stage 1
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Update Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Eligibility trace for Stage 1
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```