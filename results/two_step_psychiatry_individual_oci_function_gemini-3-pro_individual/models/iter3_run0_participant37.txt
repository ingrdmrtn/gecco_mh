Here are three new cognitive models for the two-step task, incorporating the OCI score into different mechanisms of learning and decision-making.

### Model 1: OCI-Modulated Model-Based Control (`cognitive_model1`)
This model hypothesizes that individuals with higher OCI scores rely more heavily on habit (model-free) rather than goal-directed (model-based) planning. Here, the weighting parameter $w$ (which balances MB and MF systems) is dynamically reduced by the OCI score. A base $w$ is defined, and OCI acts as a penalty, shifting control towards the Model-Free system.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Model-Based Control.
    
    Hypothesis: High OCI is associated with a deficit in goal-directed (Model-Based) control.
    The mixing weight 'w' is reduced by the OCI score, shifting behavior towards
    habitual (Model-Free) control.
    
    Parameters:
    learning_rate: [0, 1] Learning rate for value updates.
    beta: [0, 10] Inverse temperature for softmax choice.
    w_base: [0, 1] Baseline weight for Model-Based system (before OCI adjustment).
    oci_impairment: [0, 1] Scaling factor for how much OCI reduces MB control.
    
    Bounds:
    learning_rate: [0,1]
    beta: [0,10]
    w_base: [0,1]
    oci_impairment: [0,1]
    """
    learning_rate, beta, w_base, oci_impairment = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    # Calculate effective w: w_base reduced by OCI score
    # We clip to ensure w remains in [0, 1]
    w_eff = w_base * (1.0 - (oci_impairment * current_oci))
    w_eff = np.clip(w_eff, 0.0, 1.0)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Policy for the first choice
        # Calculate Model-Based values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combine MF and MB values using effective w
        q_net = w_eff * q_stage1_mb + (1 - w_eff) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Action value updating for Choice 1 (Model-Free)
        # SARSA-style update using the value of the state reached
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] = q_stage1_mf[a1] + learning_rate * delta_stage1
        
        # Action value updating for Choice 2
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] = q_stage2_mf[s_idx, a2] + learning_rate * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Driven Asymmetric Learning (`cognitive_model2`)
This model tests the hypothesis that OCI is related to an over-sensitivity to negative outcomes (or lack of reward). Instead of a single learning rate, the model uses separate learning rates for positive and negative prediction errors. The OCI score specifically amplifies the learning rate for negative prediction errors (`alpha_neg`), making the participant quicker to abandon actions that yield zero reward.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Driven Asymmetric Learning (Negative Bias).
    
    Hypothesis: High OCI participants are hypersensitive to failure/omission of reward.
    The OCI score increases the learning rate specifically for negative prediction errors
    (when reward is 0), leading to faster abandonment of non-rewarded options.
    
    Parameters:
    alpha_base: [0, 1] Base learning rate for positive prediction errors.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Weight for Model-Based control.
    oci_neg_sens: [0, 1] How much OCI adds to the learning rate for negative PEs.
    
    Bounds:
    alpha_base: [0,1]
    beta: [0,10]
    w: [0,1]
    oci_neg_sens: [0,1]
    """
    alpha_base, beta, w, oci_neg_sens = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    # Calculate negative learning rate based on OCI
    # alpha_neg = alpha_base + (oci_score * sensitivity)
    alpha_neg = alpha_base + (current_oci * oci_neg_sens)
    alpha_neg = np.clip(alpha_neg, 0.0, 1.0)
    
    # Positive learning rate is just the base rate
    alpha_pos = alpha_base

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Action value updating for Choice 1
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        lr_1 = alpha_pos if delta_stage1 >= 0 else alpha_neg
        q_stage1_mf[a1] = q_stage1_mf[a1] + lr_1 * delta_stage1
        
        # Action value updating for Choice 2
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        lr_2 = alpha_pos if delta_stage2 >= 0 else alpha_neg
        q_stage2_mf[s_idx, a2] = q_stage2_mf[s_idx, a2] + lr_2 * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Scaled Eligibility Trace (`cognitive_model3`)
This model explores the interaction between OCI and the credit assignment process. In standard temporal difference learning, an eligibility trace ($\lambda$) determines how much the Stage 1 choice is updated by the final reward (Stage 2 outcome). Here, OCI modulates this eligibility trace. High OCI might lead to "over-attribution" or hyper-connection of distal outcomes to initial choices, effectively increasing $\lambda$.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Scaled Eligibility Trace.
    
    Hypothesis: OCI affects credit assignment. High OCI leads to stronger
    direct reinforcement of the first-stage choice by the second-stage reward,
    bypassing the state-value intermediate. This is modeled by scaling the
    eligibility trace parameter (lambda) with OCI.
    
    Parameters:
    learning_rate: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Weight for Model-Based control.
    lambda_scale: [0, 1] Scaling factor for eligibility trace based on OCI.
    
    Bounds:
    learning_rate: [0,1]
    beta: [0,10]
    w: [0,1]
    lambda_scale: [0,1]
    """
    learning_rate, beta, w, lambda_scale = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    # Calculate eligibility trace lambda based on OCI
    # A higher OCI results in a higher lambda (up to 1.0)
    # This means the Stage 1 value is updated more strongly by the final reward directly.
    lambda_eff = current_oci * lambda_scale
    lambda_eff = np.clip(lambda_eff, 0.0, 1.0)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Action value updating
        # Standard TD(lambda) logic for two-step task
        
        # Stage 2 PE
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        # Stage 1 PE (prediction of Stage 2 value)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        
        # Update Stage 2
        q_stage2_mf[s_idx, a2] = q_stage2_mf[s_idx, a2] + learning_rate * delta_stage2
        
        # Update Stage 1:
        # 1. Direct update from Stage 1 PE
        # 2. Eligibility trace update from Stage 2 PE
        q_stage1_mf[a1] = q_stage1_mf[a1] + learning_rate * delta_stage1 + (learning_rate * lambda_eff * delta_stage2)
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```