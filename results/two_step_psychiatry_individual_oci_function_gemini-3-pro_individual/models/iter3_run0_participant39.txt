Here are three new cognitive models exploring different mechanisms for how OCI scores might influence decision-making in the two-step task.

### Model 1: OCI-Modulated Model-Based/Model-Free Trade-off
This model hypothesizes that OCI symptoms relate to the balance between goal-directed (Model-Based) and habitual (Model-Free) control. Specifically, it tests if higher OCI scores lead to a stronger reliance on the habitual system (lower `w`) or the goal-directed system, by making the mixing weight `w` a logistic function of the OCI score.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Mixing Weight (w).
    
    Hypothesis: The balance between Model-Based (MB) and Model-Free (MF) control
    is directly modulated by OCI severity. Instead of a linear slope, we use a 
    logistic function to bound w between 0 and 1 naturally.
    
    Parameters:
    lr: [0, 1] Learning rate for value updates.
    beta: [0, 10] Inverse temperature for softmax.
    w_base: [0, 1] Baseline mixing weight (MB vs MF) for an average participant.
    w_oci_strength: [-5, 5] How strongly OCI shifts w. Positive = more MB with high OCI.
    """
    lr, beta, w_base, w_oci_strength = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Map parameters to a logistic function to keep w in [0, 1]
    # We center the OCI effect around a typical score (e.g., 0.3) or just raw.
    # Here we use the raw score.
    # Logit transform the base to get the intercept
    eps = 1e-5
    # Inverse logit logic: w = 1 / (1 + exp(-(logit_base + strength * oci)))
    # To make w_base interpretable as the w at OCI=0, we derive the intercept:
    logit_base = np.log(w_base / (1 - w_base + eps))
    
    current_logit = logit_base + w_oci_strength * oci_score
    w = 1.0 / (1.0 + np.exp(-current_logit))
    
    # Ensure w stays strictly in bounds
    w = np.clip(w, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        
        # Model-Based Value: V(s') = max(Q(s', a'))
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax Choice 1
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        a1 = int(action_1[trial])
        s_idx = int(state[trial])

        # --- Stage 2 Decision ---
        
        # Softmax Choice 2
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Learning ---
        
        # TD Error Stage 1 (SARSA-like for MF)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        # TD Error Stage 2
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Dependent Learning Rates (Positive vs Negative)
This model investigates if OCI is associated with an asymmetry in how participants learn from positive versus negative prediction errors. High OCI might correlate with "hyper-learning" from failure (negative prediction errors) or safety signals, potentially driving avoidance or repetitive checking behaviors.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: Asymmetric Learning Rates Modulated by OCI.
    
    Hypothesis: OCI affects sensitivity to negative feedback differently than positive feedback.
    We separate learning rates into lr_pos and lr_neg. We allow OCI to scale 
    the negative learning rate (lr_neg), testing if high OCI participants over-update 
    on errors (or lack of reward).
    
    Parameters:
    lr_pos: [0, 1] Learning rate for positive prediction errors (RPE > 0).
    lr_neg_base: [0, 1] Baseline learning rate for negative prediction errors (RPE < 0).
    neg_oci_scale: [0, 5] Multiplier for how OCI affects lr_neg. 
                  lr_neg_effective = lr_neg_base * (1 + neg_oci_scale * OCI).
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Mixing weight.
    """
    lr_pos, lr_neg_base, neg_oci_scale, beta, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective negative learning rate based on OCI
    # We clip it to ensure stability.
    lr_neg_eff = lr_neg_base * (1.0 + neg_oci_scale * oci_score)
    lr_neg_eff = np.clip(lr_neg_eff, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        a1 = int(action_1[trial])
        s_idx = int(state[trial])

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Update Stage 1 ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        
        if delta_stage1 >= 0:
            q_stage1_mf[a1] += lr_pos * delta_stage1
        else:
            q_stage1_mf[a1] += lr_neg_eff * delta_stage1
            
        # --- Update Stage 2 ---
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        if delta_stage2 >= 0:
            q_stage2_mf[s_idx, a2] += lr_pos * delta_stage2
        else:
            q_stage2_mf[s_idx, a2] += lr_neg_eff * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Driven Choice Stickiness (Perseveration)
This model proposes that the primary effect of OCI in this task is not strictly learning-based, but rather a simple motor or cognitive perseveration (stickiness). High OCI scores might lead to a compulsion to repeat the previous action regardless of the outcome or model-based inference.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Driven Choice Perseveration (Stickiness).
    
    Hypothesis: OCI relates to a 'stickiness' parameter. Higher OCI leads to 
    a higher tendency to repeat the immediately preceding Stage 1 choice, 
    independent of reward history.
    
    Parameters:
    lr: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Mixing weight (MB vs MF).
    stickiness_base: [-2, 2] Baseline tendency to repeat choice (can be negative for alternation).
    stickiness_oci_slope: [0, 5] Additional stickiness added per unit of OCI score.
    """
    lr, beta, w, stickiness_base, stickiness_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate total stickiness bonus
    # This is added to the Q-value of the previously chosen action.
    stickiness_val = stickiness_base + stickiness_oci_slope * oci_score

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Variable to track previous choice (initialize to None or handle first trial)
    prev_a1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Base net value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness bonus if not the first trial
        q_net_sticky = q_net.copy()
        if prev_a1 != -1:
            q_net_sticky[prev_a1] += stickiness_val
        
        exp_q1 = np.exp(beta * q_net_sticky)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        a1 = int(action_1[trial])
        prev_a1 = a1 # Update previous choice
        
        s_idx = int(state[trial])

        # --- Stage 2 Choice ---
        # Note: Stickiness usually modeled at Stage 1 in this task, 
        # but could apply to Stage 2. Here we apply standard softmax.
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Learning ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```