Here are the three proposed cognitive models.

### Model 1: Hybrid RL with OCI-Modulated Disappointment
This model hypothesizes that OCI scores correlate with "loss aversion" or a sense of disappointment when a reward is not received. Instead of treating a 0 reward as neutral, high-OCI participants might perceive it as a punishment (negative reward).

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF RL where OCI modulates the perception of non-reward (0) as punishment.
    
    High OCI scores increase the 'disappointment' penalty when reward is 0.
    This effectively shifts the reward function for high OCI individuals, 
    making them more avoidant of non-rewarding outcomes than standard RL.

    Parameters:
    - learning_rate: [0, 1] Learning rate for value updates.
    - beta: [0, 10] Inverse temperature (softness of choice).
    - w: [0, 1] Mixing weight (0=Pure MF, 1=Pure MB).
    - oci_disappointment: [0, 5] Scaling factor for the penalty applied when reward is 0.
      Effective Reward = Reward - (OCI * oci_disappointment) if Reward == 0.
    """
    learning_rate, beta, w, oci_disappointment = model_parameters
    n_trials = len(action_1)
    oci_val = oci[0]

    # Initialize Q-values
    # Stage 1: 2 actions (Spaceships)
    q_stage1_mf = np.zeros(2) 
    # Stage 2: 2 states (Planets) x 2 actions (Aliens)
    q_stage2_mf = np.zeros((2, 2)) 
    
    # Fixed transition matrix: T[action, state]
    # Action 0 -> 0.7 prob State 0, 0.3 prob State 1
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based Value: Expected value of next stage states
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        s2_state = state[trial] # Observed state (planet)

        # --- Stage 2 Choice ---
        # Standard MF choice at Stage 2
        exp_q2 = np.exp(beta * q_stage2_mf[s2_state])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]
        
        # --- Reward Modulation ---
        # If reward is 0, interpret as negative based on OCI
        effective_reward = r
        if r == 0:
            effective_reward = -1.0 * (oci_val * oci_disappointment)

        # --- Updates ---
        # Stage 2 Update (TD Error)
        delta_stage2 = effective_reward - q_stage2_mf[s2_state, a2]
        q_stage2_mf[s2_state, a2] += learning_rate * delta_stage2
        
        # Stage 1 Update (TD Error with Eligibility Trace = 1 for simplicity/standard MF)
        # Using the value of the chosen stage 2 option as the target
        delta_stage1 = q_stage2_mf[s2_state, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid RL with OCI-Modulated Memory Decay
This model suggests that OCI relates to the persistence of value representations. Specifically, it tests if OCI scores modulate the rate at which the values of *unchosen* options decay (are forgotten).

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF RL where OCI modulates the decay rate of unchosen options.
    
    Standard RL often assumes values stay constant until visited. 
    This model assumes values of unchosen actions decay toward 0.
    The rate of this decay is modulated by OCI.
    High OCI could lead to rigid maintenance of old values (low decay) 
    or rapid forgetting of alternatives (high decay).

    Parameters:
    - learning_rate: [0, 1] Learning rate for chosen options.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight (MB vs MF).
    - decay_base: [0, 1] Base decay rate for unchosen options.
    - decay_oci_mod: [-1, 1] How OCI modifies the decay rate. 
      Decay = decay_base + (OCI * decay_oci_mod), clamped [0, 1].
    """
    learning_rate, beta, w, decay_base, decay_oci_mod = model_parameters
    n_trials = len(action_1)
    oci_val = oci[0]

    # Calculate effective decay rate
    decay_rate = decay_base + (oci_val * decay_oci_mod)
    # Clamp to [0, 1]
    if decay_rate < 0: decay_rate = 0.0
    if decay_rate > 1: decay_rate = 1.0

    q_stage1_mf = np.zeros(2) 
    q_stage2_mf = np.zeros((2, 2)) 
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        s2_state = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[s2_state])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]

        # --- Updates ---
        # Stage 2 Chosen
        delta_stage2 = r - q_stage2_mf[s2_state, a2]
        q_stage2_mf[s2_state, a2] += learning_rate * delta_stage2
        
        # Stage 2 Unchosen Decay
        unchosen_a2 = 1 - a2
        q_stage2_mf[s2_state, unchosen_a2] *= (1.0 - decay_rate)

        # Stage 1 Chosen
        delta_stage1 = q_stage2_mf[s2_state, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Stage 1 Unchosen Decay
        unchosen_a1 = 1 - a1
        q_stage1_mf[unchosen_a1] *= (1.0 - decay_rate)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid RL with OCI-Modulated Second-Stage Precision
This model hypothesizes that OCI affects the exploration/exploitation balance (noise) differently depending on the stage. Specifically, it tests if OCI modulates the inverse temperature ($\beta$) at the second stage (immediate reward context) while leaving the first stage (planning context) baseline.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF RL with stage-specific noise, where OCI modulates Stage 2 precision.
    
    This model separates the inverse temperature (beta) for Stage 1 and Stage 2.
    It posits that OCI specifically affects decision noise at the terminal stage 
    (where immediate reward is received), potentially making choices more deterministic 
    (compulsive/rigid) or more noisy, distinct from the planning stage.

    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - w: [0, 1] Mixing weight (MB vs MF).
    - beta_stage1: [0, 10] Fixed inverse temperature for Stage 1.
    - beta_stage2_base: [0, 10] Base inverse temperature for Stage 2.
    - beta_stage2_oci_slope: [-5, 5] Modulation of Stage 2 beta by OCI.
      Beta2 = beta_stage2_base + (OCI * beta_stage2_oci_slope), clamped > 0.
    """
    learning_rate, w, beta_stage1, beta_stage2_base, beta_stage2_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_val = oci[0]

    # Calculate effective Stage 2 beta
    beta_stage2 = beta_stage2_base + (oci_val * beta_stage2_oci_slope)
    if beta_stage2 < 0: beta_stage2 = 0.0

    q_stage1_mf = np.zeros(2) 
    q_stage2_mf = np.zeros((2, 2)) 
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice (Uses beta_stage1) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_stage1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        s2_state = state[trial]

        # --- Stage 2 Choice (Uses OCI-modulated beta_stage2) ---
        exp_q2 = np.exp(beta_stage2 * q_stage2_mf[s2_state])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]

        # --- Updates ---
        delta_stage2 = r - q_stage2_mf[s2_state, a2]
        q_stage2_mf[s2_state, a2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[s2_state, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```