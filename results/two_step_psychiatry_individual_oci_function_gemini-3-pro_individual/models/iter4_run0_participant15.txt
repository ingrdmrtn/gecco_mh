def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model with OCI-modulated Model-Based Learning Rate.
    
    Hypothesis: OCI scores influence how rapidly participants update their internal 
    model of the transition probabilities (spaceship -> planet) upon observing transitions. 
    High OCI may lead to over-updating (chasing noise) or rigidity (ignoring updates).
    
    Bounds:
    lr_mf: [0, 1] (Model-Free learning rate)
    beta: [0, 10] (Inverse temperature)
    w: [0, 1] (Mixing weight: 1=Pure MB, 0=Pure MF)
    lr_mb_base: [0, 1] (Base learning rate for transition matrix)
    lr_mb_oci_mod: [-1, 1] (Modulation of MB learning rate by OCI)
    """
    lr_mf, beta, w, lr_mb_base, lr_mb_oci_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    # Calculate effective Model-Based learning rate
    lr_mb = lr_mb_base + (lr_mb_oci_mod * oci_score)
    # Clamp to [0, 1] to ensure valid probabilities
    lr_mb = min(max(lr_mb, 0.0), 1.0)

    # Initialize transition matrix with instructed knowledge (0.7 common, 0.3 rare)
    # Row 0: Spaceship A, Row 1: Spaceship U
    # Col 0: Planet X, Col 1: Planet Y
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        # Model-Based Value: Expected max value of next stage weighted by transition matrix
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax Choice 1
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        # Standard Softmax on Stage 2 Q-values
        exp_q2 = np.exp(beta * q_stage2_mf[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # --- Updates ---
        
        # 1. Update Model-Free Values (SARSA style for Stage 1)
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr_mf * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += lr_mf * delta_stage2
        
        # 2. Update Model-Based Transition Matrix
        # Update the probability of reaching state s2 given action a1
        # If we observed s2, the target probability is 1.0.
        transition_matrix[a1, s2] += lr_mb * (1.0 - transition_matrix[a1, s2])
        # Ensure the other probability sums to 1
        transition_matrix[a1, 1-s2] = 1.0 - transition_matrix[a1, s2]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model with OCI-modulated Subjective Loss Sensitivity.
    
    Hypothesis: Participants with high OCI perceive the absence of a reward (0 coins)
    not as neutral, but as a loss (negative utility). OCI modulates the magnitude 
    of this subjective loss.
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    w: [0, 1]
    loss_base: [0, 5] (Base magnitude of negative utility for 0 reward)
    loss_oci_mod: [-5, 5] (Modulation of loss sensitivity by OCI)
    """
    learning_rate, beta, w, loss_base, loss_oci_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    # Calculate subjective value for "no reward"
    loss_magnitude = loss_base + (loss_oci_mod * oci_score)
    # Loss magnitude should generally be positive (making the utility negative)
    # but we allow flexibility.
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r_observed = reward[trial]
        
        # Transform Reward to Subjective Utility
        if r_observed == 1:
            r_effective = 1.0
        else:
            r_effective = -1.0 * loss_magnitude

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # --- Updates ---
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Update using effective reward (utility)
        delta_stage2 = r_effective - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model with OCI-modulated Stage 1 Choice Precision (Beta).
    
    Hypothesis: OCI affects the exploration/exploitation balance specifically at 
    the first stage (planning stage), distinct from the second stage. High OCI 
    might lead to more rigid (deterministic) or more anxious (random) choices 
    at the spaceship level.
    
    Bounds:
    learning_rate: [0, 1]
    w: [0, 1]
    beta_2: [0, 10] (Inverse temperature for Stage 2)
    beta_1_base: [0, 10] (Base inverse temperature for Stage 1)
    beta_1_oci_slope: [-5, 5] (Modulation of Stage 1 beta by OCI)
    """
    learning_rate, w, beta_2, beta_1_base, beta_1_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    # Calculate effective Beta for Stage 1
    beta_1 = beta_1_base + (beta_1_oci_slope * oci_score)
    # Ensure beta is non-negative
    beta_1 = max(beta_1, 0.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy (Uses beta_1) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy (Uses beta_2) ---
        exp_q2 = np.exp(beta_2 * q_stage2_mf[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # --- Updates ---
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss