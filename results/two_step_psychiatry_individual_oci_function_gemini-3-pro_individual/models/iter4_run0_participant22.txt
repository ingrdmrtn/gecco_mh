Here are three new cognitive models exploring different mechanisms for how OCI scores might influence decision-making in this two-step task, specifically focusing on the balance between model-based/model-free control and learning rates.

### Model 1: Hybrid MB/MF with OCI-modulated Learning Rate
This model hypothesizes that OCI symptoms might relate to cognitive rigidity or flexibility in updating values. Specifically, it tests if higher OCI scores lead to different learning rates (e.g., faster or slower updating of value expectations). Here, the learning rate is a function of the OCI score. It uses a standard hybrid Model-Based/Model-Free architecture where `w` controls the mixing weight.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free RL with OCI-modulated Learning Rate.

    Parameters:
    alpha_base: [0,1] - Baseline learning rate.
    beta: [0,10] - Inverse temperature.
    w: [0,1] - Weight for Model-Based control (0=MF, 1=MB).
    alpha_mod: [0, 5] - Sensitivity of learning rate to OCI.

    Hypothesis: 
    The effective learning rate is modulated by OCI.
    effective_alpha = sigmoid(alpha_base + alpha_mod * (oci - 0.31))
    (Using 0.31 as the 'low/med' cut-off point for centering).
    """
    alpha_base, beta, w, alpha_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Modulate learning rate based on OCI
    # We use a logistic transform to keep alpha within [0,1]
    # Centering around 0.31 (clinical cutoff)
    logit_alpha = alpha_base + alpha_mod * (oci_score - 0.31)
    learning_rate = 1 / (1 + np.exp(-logit_alpha))

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based Value Calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # TD Error Stage 1 (Model-Free)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # TD Error Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Model-Free RL with OCI-modulated Eligibility Traces
This model hypothesizes that OCI affects the "credit assignment" process. Specifically, it modulates the eligibility trace parameter `lambda`. An eligibility trace determines how much the Stage 1 choice is reinforced by the Stage 2 reward directly. A high `lambda` means the agent connects the final reward strongly back to the first spaceship choice (characteristic of MF), while a low `lambda` treats the stages more independently.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free RL with OCI-modulated Eligibility Traces (Lambda).

    Parameters:
    learning_rate: [0,1] - Update rate.
    beta: [0,10] - Inverse temperature.
    lambda_base: [0,1] - Baseline eligibility trace.
    lambda_oci_factor: [0, 2] - How strongly OCI affects lambda.

    Hypothesis:
    OCI affects the strength of the eligibility trace (memory of the first action).
    effective_lambda = lambda_base + lambda_oci_factor * oci
    (Clipped to [0,1]).
    """
    learning_rate, beta, lambda_base, lambda_oci_factor = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective lambda
    eff_lambda = lambda_base + lambda_oci_factor * oci_score
    eff_lambda = np.clip(eff_lambda, 0.0, 1.0)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Stage 2 Prediction Error
        delta_stage2 = reward[trial] - q_stage2[state_idx, a2]
        
        # Update Stage 2 value
        q_stage2[state_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 Prediction Error (TD(0))
        # Note: In standard TD(lambda), the update for stage 1 includes both
        # the immediate transition error and the discounted stage 2 error.
        # Here we implement a simplified TD(lambda) often used in this task:
        # Q1 updates based on Q2 (transition) + lambda * (Reward - Q2)
        
        delta_stage1 = q_stage2[state_idx, a2] - q_stage1[a1]
        
        # Update Stage 1 value:
        # 1. Update based on transition to stage 2 state
        q_stage1[a1] += learning_rate * delta_stage1
        
        # 2. Update based on eligibility trace from stage 2 reward
        q_stage1[a1] += learning_rate * eff_lambda * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid MB/MF with OCI-modulated Inverse Temperature (Beta)
This model tests the hypothesis that OCI scores relate to decision noise or exploration/exploitation balance. It posits that the weighting `w` between MB and MF is constant, but the precision of choices (`beta`) varies with OCI. For example, high OCI might lead to more deterministic (higher beta) or more erratic (lower beta) responding regardless of the strategy used.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF with OCI-modulated Inverse Temperature (Beta).

    Parameters:
    learning_rate: [0,1] - Update rate.
    w: [0,1] - Weight for Model-Based control.
    beta_base: [0,10] - Baseline inverse temperature.
    beta_oci_slope: [-5, 5] - Slope of OCI effect on beta.

    Hypothesis:
    OCI modulates the randomness of choice (exploration).
    effective_beta = beta_base + beta_oci_slope * oci
    (Clipped to [0, 20] to prevent overflow, though bounds say 0-10 usually,
     we allow dynamic range but keep it positive).
    """
    learning_rate, w, beta_base, beta_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective beta
    eff_beta = beta_base + beta_oci_slope * oci_score
    # Ensure beta stays non-negative and within reasonable bounds
    eff_beta = np.clip(eff_beta, 0.0, 15.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(eff_beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])

        # --- Stage 2 Choice ---
        # Stage 2 is purely model-free in this paradigm usually
        exp_q2 = np.exp(eff_beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```