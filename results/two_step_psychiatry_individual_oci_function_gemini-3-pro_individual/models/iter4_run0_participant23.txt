Here are three new cognitive models exploring different mechanisms for how OCI scores might influence reinforcement learning in this two-step task.

### Model 1: Hybrid Learner with OCI-modulated Transition Learning
This model hypothesizes that individuals with varying OCI levels might update their internal model of the environment (the transition probabilities) differently. Specifically, it tests if higher OCI scores lead to a more rigid or "sticky" transition matrix, making it harder to learn rare transitions or changes in spaceship-planet mappings. The OCI score modulates the learning rate for the transition matrix itself, separate from the value learning rate.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free learner where OCI modulates the learning rate 
    of the state transition matrix (Model-Based component).
    
    Hypothesis: Higher OCI might lead to more rigid structural learning (lower transition 
    learning rate) or hyper-sensitivity to structural changes.
    
    Parameters:
    - lr_value: [0, 1] Learning rate for action values (Q-values).
    - lr_transition_base: [0, 1] Base learning rate for updating the transition matrix.
    - lr_transition_oci_slope: [-1, 1] How OCI affects transition learning rate.
    - w: [0, 1] Weighting between Model-Based (1) and Model-Free (0) control.
    - beta: [0, 10] Inverse temperature for softmax.
    """
    lr_value, lr_transition_base, lr_transition_oci_slope, w, beta = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    # Calculate transition learning rate modulated by OCI
    lr_transition = lr_transition_base + (current_oci * lr_transition_oci_slope)
    lr_transition = np.clip(lr_transition, 0.0, 1.0)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # 2 states (planets), 2 actions (aliens)
    
    # Initialize Transition Matrix (Spaceship -> Planet)
    # Rows: Action 1 (Spaceship A, U), Cols: State (Planet X, Y)
    # Start with uniform prior or standard task structure guess (0.5)
    transition_matrix = np.full((2, 2), 0.5)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Choice ---
        # Model-Based Value: T * max(Q2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix[a1] @ max_q_stage2 # Expected value given chosen action
        
        # For selection, we need values for both actions
        q_mb_all = transition_matrix @ max_q_stage2
        
        # Integrated Value
        q_net = w * q_mb_all + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        
        # 1. Update Transition Matrix (Model-Based)
        # Create a one-hot vector for the observed state
        state_one_hot = np.zeros(2)
        state_one_hot[s_idx] = 1.0
        
        # Delta for transition probability
        trans_err = state_one_hot - transition_matrix[a1]
        transition_matrix[a1] += lr_transition * trans_err
        
        # 2. Update Q-values (Model-Free)
        # Stage 2 update
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr_value * delta_stage2
        
        # Stage 1 update (TD-0)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr_value * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Choice Stickiness (Perseveration)
This model tests the hypothesis that OCI relates to behavioral perseveration or "stickiness." Instead of affecting learning rates, the OCI score directly modulates a choice autocorrelation parameter (`stickiness`). This parameter encourages (or discourages) repeating the exact same spaceship choice made in the previous trial, regardless of the reward outcome. This captures the compulsive aspect of repeating actions.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner with Choice Stickiness (Perseveration) modulated by OCI.
    
    Hypothesis: OCI scores correlate with a tendency to repeat previous choices 
    (compulsivity/habit), independent of reward value.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for Q-values.
    - beta: [0, 10] Inverse temperature.
    - stick_base: [-5, 5] Base level of choice stickiness (positive = repeat, negative = switch).
    - stick_oci_slope: [-5, 5] How OCI modulates the stickiness parameter.
    """
    learning_rate, beta, stick_base, stick_oci_slope = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    # Calculate effective stickiness
    stickiness = stick_base + (current_oci * stick_oci_slope)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    last_action_1 = -1 # No previous action for the first trial

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Choice with Stickiness ---
        # Calculate logits (Q * beta + stickiness bonus)
        logits_1 = beta * q_stage1.copy()
        
        if last_action_1 != -1:
            logits_1[last_action_1] += stickiness
            
        # Softmax
        # Subtract max for numerical stability
        c = np.max(logits_1)
        exp_q1 = np.exp(logits_1 - c)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        # Stage 2
        delta_stage2 = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += learning_rate * delta_stage2

        # Stage 1
        # Using the value of the state reached (s_idx) and the max Q of that state
        # Standard TD(0) typically uses V(s') which is max_a' Q(s', a')
        v_stage2 = np.max(q_stage2[s_idx])
        delta_stage1 = v_stage2 - q_stage1[a1]
        q_stage1[a1] += learning_rate * delta_stage1
        
        # Update history
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Reward Sensitivity (Subjective Utility)
This model proposes that high OCI scores might blunt or enhance the subjective experience of the reward itself. Rather than changing how fast one learns (learning rate) or how they select actions (beta/stickiness), this model scales the reward input `r` before it enters the prediction error calculation. A parameter `rho` (reward sensitivity) is modulated by OCI. If OCI leads to anhedonia or reduced sensitivity to positive feedback, `rho` might decrease as OCI increases.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Learner where OCI modulates Reward Sensitivity (Rho).
    
    Hypothesis: OCI affects the subjective utility of the reward. High OCI might
    dampen the impact of rewards (anhedonia-like) or heighten it.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Model-based weight.
    - rho_base: [0, 5] Base reward sensitivity scalar.
    - rho_oci_mod: [-2, 2] Modulation of reward sensitivity by OCI.
    """
    learning_rate, beta, w, rho_base, rho_oci_mod = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    # Calculate effective reward sensitivity
    # Ensure rho stays non-negative
    rho = rho_base + (current_oci * rho_oci_mod)
    rho = max(0.0, rho)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) 
    
    # Fixed transition matrix for simple hybrid model
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        
        # Apply subjective utility scaling to reward
        r_subjective = reward[trial] * rho

        # --- Stage 1 Choice ---
        # Model-Based Component
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Weighted combination
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        # Stage 2 update with subjective reward
        delta_stage2 = r_subjective - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 update
        # Note: In standard hybrid models, MF Q1 is often updated via TD(1) (eligibility trace)
        # or TD(0). Using TD(0) here for simplicity consistent with template.
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```