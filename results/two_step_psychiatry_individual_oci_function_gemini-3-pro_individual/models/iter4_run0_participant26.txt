Here are three new cognitive models that incorporate the OCI score to explain individual variability in the two-step task.

### Model 1: OCI-Distorted Transition Beliefs
This model hypothesizes that OCI symptoms distort the participant's internal model of the environment's structure. High OCI participants may perceive the "common" transitions as more deterministic (closer to 1.0) than they really are, reflecting a need for certainty or control. This affects the Model-Based value calculation, as the agent places higher confidence in the expected state transitions.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Distorted Transition Beliefs.
    
    This model hypothesizes that OCI symptoms distort the participant's internal model 
    of the environment's transition probabilities. High OCI participants may perceive 
    the "common" transitions as more deterministic (closer to 1.0) or more random 
    (closer to 0.5) than reality, affecting their Model-Based value calculation.
    
    Parameters:
    - learning_rate: [0, 1] Weight for prediction error updates.
    - beta: [0, 10] Inverse temperature for softmax choice.
    - w: [0, 1] Weighting between Model-Based (1) and Model-Free (0) control.
    - distortion: [0, 1] Magnitude of OCI-driven distortion on transition probabilities.
      A positive effect increases the perceived probability of common transitions 
      (making the world seem more deterministic).
    """
    learning_rate, beta, w, distortion = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
  
    # Calculate subjective transition probability
    # Base is 0.7. distortion * oci adds to this, effectively increasing confidence in structure.
    # We clip to max 0.99 to keep probabilities valid.
    subjective_p = 0.7 + (distortion * current_oci * 0.29) 
    subjective_p = np.clip(subjective_p, 0.5, 0.99)
    
    # Subjective transition matrix used for MB planning
    transition_matrix = np.array([[subjective_p, 1-subjective_p], 
                                  [1-subjective_p, subjective_p]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based Value calculation uses the distorted transition matrix
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax Policy
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        
        # --- Updates ---
        # TD(0) update for Stage 1 MF (Transition Prediction Error)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # TD(0) update for Stage 2 MF (Reward Prediction Error)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Surprise Penalty
This model assumes that individuals with high OCI symptoms are averse to uncertainty or "rare" events. When a rare transition occurs (Action A -> Planet Y or Action B -> Planet X), a penalty is applied to the Stage 1 value update. This effectively discourages the choice that led to the unexpected outcome, even if the subsequent reward was high. This models a "fear of the unexpected" mechanism.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Modulated Surprise Penalty.
    
    This model assumes that individuals with high OCI symptoms are averse to 
    uncertainty or "rare" events. When a rare transition occurs (Action A -> Planet Y 
    or Action B -> Planet X), a penalty is applied to the Stage 1 value update, 
    effectively discouraging the choice that led to the unexpected outcome, 
    regardless of the subsequent reward.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for Q-values.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weighting for Model-Based control.
    - surprise_penalty: [0, 5] Magnitude of the penalty applied upon encountering a rare transition, scaled by OCI.
    """
    learning_rate, beta, w, surprise_penalty = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        
        # --- Updates ---
        # Determine if transition was rare
        # Common: 0->0, 1->1. Rare: 0->1, 1->0.
        is_rare = (a1 != state_idx) 
        
        # Calculate penalty based on OCI and rareness
        current_penalty = surprise_penalty * current_oci * float(is_rare)
        
        # Update Stage 1: The value of the chosen action is penalized if the transition was rare
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1] - current_penalty
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Driven Learning Rigidity (Annealing)
This model proposes that high OCI correlates with cognitive rigidity that develops over time. The learning rate is not constant but "anneals" (decays) as trials progress. Higher OCI scores accelerate this decay, leading the participant to "freeze" on their early beliefs (forming rigid habits) and become increasingly unresponsive to new reward information as the task continues.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Driven Learning Rigidity (Annealing).
    
    This model proposes that high OCI correlates with cognitive rigidity that develops 
    over time. The learning rate is not constant but "anneals" (decays) as trials 
    progress. Higher OCI scores accelerate this decay, leading the participant to 
    "freeze" on their early beliefs and become unresponsive to new reward information.
    
    Parameters:
    - lr_init: [0, 1] Initial learning rate at trial 0.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weighting for Model-Based control.
    - rigidity_rate: [0, 1] Coefficient determining how fast the learning rate decays with trial number, modulated by OCI.
    """
    lr_init, beta, w, rigidity_rate = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # Dynamic Learning Rate
        # Formula: lr = lr_init / (1 + decay * trial)
        # OCI modulates the decay speed.
        effective_decay = rigidity_rate * current_oci
        current_lr = lr_init / (1 + effective_decay * trial)

        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
  
        # --- Updates ---
        # Use the dynamic current_lr for updates
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += current_lr * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += current_lr * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```