Here are three new cognitive models incorporating OCI-R scores into the two-step decision task.

### Model 1: Subjective Transition Probability Distortion
This model hypothesizes that OCI symptoms, often linked to intolerance of uncertainty or a need for control, distort the participant's internal model of the environment's transition structure. While the true common transition probability is 0.7, high OCI participants may perceive this relationship as more deterministic (closer to 1.0) or, conversely, doubt the structure (closer to 0.5), affecting their Model-Based value calculation.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    MB/MF Hybrid with OCI-modulated Subjective Transition Probability.
    
    Hypothesis: OCI modulates the 'belief' in the transition matrix probabilities used 
    by the Model-Based system. High OCI might lead to an over-estimation of the 
    deterministic nature of transitions (rigidity/certainty seeking).
    
    Parameters:
    learning_rate: [0,1] - value update rate.
    beta: [0,10] - softmax inverse temperature.
    w: [0,1] - weight of Model-Based system (0=MF, 1=MB).
    trans_prob_base: [0.5, 1.0] - baseline belief in common transition probability.
    oci_trans_sens: [-0.5, 0.5] - how much OCI shifts this belief.
    """
    learning_rate, beta, w, trans_prob_base, oci_trans_sens = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate subjective transition probability
    # Base is typically around 0.7, but we allow it to vary and be shifted by OCI
    p_common = trans_prob_base + (current_oci * oci_trans_sens)
    
    # Clamp probability to reasonable bounds [0, 1]
    if p_common > 1.0: p_common = 1.0
    if p_common < 0.0: p_common = 0.0
    
    # Subjective transition matrix used for MB planning
    # [[P(X|A), P(Y|A)], [P(X|U), P(Y|U)]]
    # Assuming A->X and U->Y are the common paths
    transition_matrix = np.array([[p_common, 1.0 - p_common], 
                                  [1.0 - p_common, p_common]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        
        # Model-Based Value calculation using the OCI-distorted matrix
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        r = reward[trial]
        
        # Prediction errors
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        
        # Update Stage 1 (TD(0))
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Update Stage 2
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-modulated Value Leak (Generalization)
This model hypothesizes that high OCI scores are associated with a failure to inhibit irrelevant associations or "over-connectivity." When a participant updates the value of a specific alien (state-action pair), a fraction of this learning "leaks" to the unchosen alien on the same planet.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    MB/MF Hybrid with OCI-modulated Value Leak (Generalization).
    
    Hypothesis: High OCI is associated with 'hyper-connectivity' or poor inhibition.
    When an outcome is received for one alien (Action A), the value of the 
    unchosen alien (Action B) on the same planet is also updated partially 
    (leaky generalization), modulated by OCI.
    
    Parameters:
    learning_rate: [0,1] - value update rate.
    beta: [0,10] - softmax inverse temperature.
    w: [0,1] - weight of Model-Based system.
    leak_base: [0,1] - baseline generalization to unchosen option.
    oci_leak_sens: [-1, 1] - sensitivity of leak to OCI.
    """
    learning_rate, beta, w, leak_base, oci_leak_sens = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate effective leak rate
    leak_rate = leak_base + (current_oci * oci_leak_sens)
    if leak_rate > 1.0: leak_rate = 1.0
    if leak_rate < 0.0: leak_rate = 0.0

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        r = reward[trial]
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        
        # Update Stage 1
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Update Stage 2 with LEAK
        # Update chosen option
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Update unchosen option (Generalization)
        unchosen_a2 = 1 - a2
        # The unchosen option moves in the same direction as the chosen one, scaled by leak
        q_stage2_mf[state_idx, unchosen_a2] += learning_rate * leak_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Post-Loss Reactive Rigidity
This model hypothesizes that OCI symptoms manifest as a specific reaction to failure (0 coins). High OCI participants may exhibit "reactive rigidity," where the inverse temperature (beta) increases specifically for the trial immediately following a loss, reflecting a compulsive tightening of choice stochasticity in response to negative feedback.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    MB/MF Hybrid with OCI-modulated Post-Loss Rigidity (Dynamic Beta).
    
    Hypothesis: OCI affects the reaction to non-rewarded trials (losses).
    After receiving 0 coins, high OCI participants become more rigid (higher beta)
    on the subsequent trial, reducing exploration.
    
    Parameters:
    learning_rate: [0,1] - value update rate.
    beta_base: [0,10] - baseline inverse temperature.
    w: [0,1] - weight of Model-Based system.
    oci_loss_rigidity: [0, 5] - increase in beta after a loss, scaled by OCI.
    """
    learning_rate, beta_base, w, oci_loss_rigidity = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_reward = 1.0 # Assume baseline state (no loss) before first trial

    for trial in range(n_trials):
        
        # Calculate dynamic beta based on previous outcome
        current_beta = beta_base
        if prev_reward == 0.0:
            current_beta += (current_oci * oci_loss_rigidity)
            
        # Ensure beta stays within reasonable calculation bounds
        if current_beta > 20.0: current_beta = 20.0

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(current_beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        # Note: We apply the same dynamic beta to stage 2
        exp_q2 = np.exp(current_beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        r = reward[trial]
        prev_reward = r # Store for next trial's beta calculation
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        
        q_stage1_mf[a1] += learning_rate * delta_stage1
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```