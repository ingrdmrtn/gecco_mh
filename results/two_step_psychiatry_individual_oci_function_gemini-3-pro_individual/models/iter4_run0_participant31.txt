Here are three new cognitive models exploring different mechanisms for how OCI scores might influence decision-making in the two-step task, specifically focusing on exploration-exploitation trade-offs, punishment sensitivity, and model-based/model-free weighting.

### Model 1: Hybrid Model with OCI-modulated Inverse Temperature (Exploration/Exploitation)
This model hypothesizes that OCI levels influence the balance between exploration and exploitation. High OCI might be associated with more rigid, deterministic responding (higher beta/lower temperature) due to anxiety or a desire for control, or conversely, more random responding if overwhelmed. Here, we model `beta` as a function of OCI.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: Hybrid Model with OCI-modulated Inverse Temperature.
    
    Hypothesis: OCI scores modulate the 'beta' (inverse temperature) parameter.
    Individuals with higher OCI might exhibit more deterministic choice patterns (higher beta)
    or more chaotic ones (lower beta). Here, beta is a linear function of OCI.
    
    Bounds:
    learning_rate: [0,1]
    w: [0,1] - Weighting parameter (0 = pure Model-Free, 1 = pure Model-Based)
    beta_base: [0,10] - Baseline inverse temperature
    beta_oci_slope: [-5, 5] - How OCI changes beta (can be positive or negative)
    """
    learning_rate, w, beta_base, beta_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate individual beta based on OCI
    # We clip beta to be non-negative to avoid computational issues with exp()
    beta = max(0.0, beta_base + beta_oci_slope * oci_score)

    # Fixed transition matrix for Model-Based component
    # State 0 (Planet X) is usually reached by Action 0 (Spaceship A) -> 0.7
    # State 1 (Planet Y) is usually reached by Action 1 (Spaceship U) -> 0.7
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based Value Calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax Policy
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        curr_state = int(state[trial])
        curr_action_1 = int(action_1[trial])

        # --- Stage 2 Choice ---
        # Standard Model-Free Q-learning at second stage
        q_s2 = q_stage2_mf[curr_state]
        exp_q2 = np.exp(beta * q_s2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        curr_action_2 = int(action_2[trial])

        # --- Learning Updates ---
        # Update Stage 1 Model-Free value (SARSA-like logic for stage 1)
        # Note: In the two-step task, standard TD(0) or eligibility traces are used.
        # Here we use the standard TD error based on the stage 2 value of the *chosen* option.
        delta_stage1 = q_stage2_mf[curr_state, curr_action_2] - q_stage1_mf[curr_action_1]
        q_stage1_mf[curr_action_1] += learning_rate * delta_stage1
        
        # Update Stage 2 Model-Free value based on reward
        delta_stage2 = reward[trial] - q_stage2_mf[curr_state, curr_action_2]
        q_stage2_mf[curr_state, curr_action_2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Model-Free Learning with OCI-Modulated Punishment Sensitivity
This model hypothesizes that high OCI scores relate to an altered sensitivity to negative outcomes (lack of reward). Instead of a single learning rate, this model splits learning into positive (reward) and negative (no reward) updates, where the "negative" learning rate is modulated by OCI. This reflects the theory that OCD symptomatology involves hyper-responsiveness to error or failure signals.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: Model-Free Learning with OCI-Modulated Punishment Sensitivity.
    
    Hypothesis: OCI scores specifically affect how strongly the agent learns from 
    non-rewarded trials (punishment/omission). High OCI might lead to over-correction 
    after failure.
    
    Bounds:
    alpha_pos: [0,1] - Learning rate for rewarded trials
    alpha_neg_base: [0,1] - Baseline learning rate for unrewarded trials
    alpha_neg_oci_slope: [-1, 1] - How OCI modulates negative learning rate
    beta: [0,10]
    """
    alpha_pos, alpha_neg_base, alpha_neg_oci_slope, beta = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate negative learning rate based on OCI
    # Constrain to [0, 1]
    alpha_neg = alpha_neg_base + alpha_neg_oci_slope * oci_score
    alpha_neg = min(max(alpha_neg, 0.0), 1.0)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        curr_state = int(state[trial])
        curr_action_1 = int(action_1[trial])

        # --- Stage 2 Choice ---
        q_s2 = q_stage2[curr_state]
        exp_q2 = np.exp(beta * q_s2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        curr_action_2 = int(action_2[trial])
        r = reward[trial]

        # --- Learning Updates ---
        # Determine which learning rate to use based on outcome
        if r > 0:
            current_lr = alpha_pos
        else:
            current_lr = alpha_neg

        # Update Stage 1
        delta1 = q_stage2[curr_state, curr_action_2] - q_stage1[curr_action_1]
        q_stage1[curr_action_1] += current_lr * delta1

        # Update Stage 2
        delta2 = r - q_stage2[curr_state, curr_action_2]
        q_stage2[curr_state, curr_action_2] += current_lr * delta2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model with OCI-Modulated Eligibility Trace (Decay)
This model introduces an eligibility trace parameter (`lambda`) which controls how much credit the first-stage action receives for the second-stage outcome. It hypothesizes that OCI affects the ability to link distal outcomes to initial choices. A high `lambda` means the agent strongly connects the reward back to the spaceship choice (Model-Free TD(1)), while a low `lambda` relies more on the immediate transition value (TD(0)).

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: Hybrid Model with OCI-Modulated Eligibility Trace.
    
    Hypothesis: The parameter lambda (eligibility trace) controls the update of Stage 1 values 
    directly from the reward, bypassing the Stage 2 value estimate. OCI modulates this, 
    potentially reflecting an 'over-thinking' or 'direct-association' bias where outcomes 
    are attributed directly to the initial choice regardless of the intermediate state.
    
    Bounds:
    learning_rate: [0,1]
    beta: [0,10]
    w: [0,1] - Weight for Model-Based control
    lambda_base: [0,1] - Baseline eligibility trace
    lambda_oci_slope: [-1, 1] - OCI modulation of lambda
    """
    learning_rate, beta, w, lambda_base, lambda_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate lambda based on OCI
    lamb = lambda_base + lambda_oci_slope * oci_score
    lamb = min(max(lamb, 0.0), 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based Component
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        curr_state = int(state[trial])
        curr_action_1 = int(action_1[trial])

        # --- Stage 2 Choice ---
        q_s2 = q_stage2_mf[curr_state]
        exp_q2 = np.exp(beta * q_s2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        curr_action_2 = int(action_2[trial])
        r = reward[trial]

        # --- Learning Updates with Eligibility Trace ---
        
        # Stage 2 Prediction Error
        delta2 = r - q_stage2_mf[curr_state, curr_action_2]
        
        # Stage 1 Prediction Error (standard TD(0) part)
        delta1 = q_stage2_mf[curr_state, curr_action_2] - q_stage1_mf[curr_action_1]
        
        # Update Stage 2 (Standard Q-learning)
        q_stage2_mf[curr_state, curr_action_2] += learning_rate * delta2
        
        # Update Stage 1 (TD(lambda))
        # The update is a mix of the immediate error (delta1) and the stage 2 error (delta2)
        # scaled by lambda. If lambda=1, Stage 1 is updated by the full reward error.
        q_stage1_mf[curr_action_1] += learning_rate * (delta1 + lamb * delta2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```