Here are 3 new cognitive models that incorporate the OCI score into the decision-making process for the two-step task.

### Model 1: Hybrid Learner with OCI-modulated Exploration (Temperature)
This model hypothesizes that OCI levels influence the balance between exploitation and exploration (randomness). High OCI might correlate with more rigid (lower temperature) or more erratic (higher temperature) behavior depending on the specific symptom profile, but here we model it as modulating the inverse temperature `beta`.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free learner where OCI modulates the softmax temperature (beta).
    
    Hypothesis: OCI affects decision noise/exploration. High OCI might lead to 
    more deterministic (rigid) choices or more noise. Here, effective beta is 
    modulated by OCI.
    
    beta_eff = beta_base * (1 + oci * oci_beta_mod)
    
    Parameters:
    learning_rate: [0, 1] - Learning rate for value updates.
    beta_base: [0, 10] - Baseline inverse temperature.
    w: [0, 1] - Weighting parameter (0=Model-Free, 1=Model-Based).
    oci_beta_mod: [-1, 1] - How strongly OCI scales the beta parameter.
    """
    learning_rate, beta_base, w, oci_beta_mod = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate effective beta
    # We clip to ensure beta remains non-negative and within reasonable bounds
    beta_eff = beta_base * (1 + current_oci * oci_beta_mod)
    beta_eff = np.clip(beta_eff, 0, 20) 

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based Value
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_eff * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta_eff * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        chosen_a1 = action_1[trial]
        chosen_a2 = action_2[trial]
        r = reward[trial]

        # TD errors
        delta_stage1 = q_stage2_mf[state_idx, chosen_a2] - q_stage1_mf[chosen_a1]
        delta_stage2 = r - q_stage2_mf[state_idx, chosen_a2]
        
        # Value updates
        q_stage1_mf[chosen_a1] += learning_rate * delta_stage1
        q_stage2_mf[state_idx, chosen_a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Model-Based/Model-Free Hybrid with OCI-modulated Eligibility Traces
This model introduces an eligibility trace parameter `lambda` that controls how much the second-stage reward updates the first-stage value directly. The hypothesis is that OCI affects how credit is assigned to past actions (the "causal chain"), potentially making high-OCI individuals link outcomes more strongly to the initial choice (high lambda) or treat stages as disconnected (low lambda).

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid learner with OCI-modulated Eligibility Trace (lambda).
    
    Hypothesis: OCI impacts the 'credit assignment' across stages. 
    High OCI might involve over-attribution of outcomes to initial choices 
    (higher lambda) or fragmentation (lower lambda).
    
    lambda_eff = lambda_base + (oci * oci_lambda_scale)
    
    Parameters:
    learning_rate: [0, 1] - Learning rate.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - MB/MF weight.
    lambda_base: [0, 1] - Base eligibility trace decay.
    oci_lambda_scale: [-1, 1] - Modulation of lambda by OCI.
    """
    learning_rate, beta, w, lambda_base, oci_lambda_scale = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate effective lambda
    lambda_eff = lambda_base + (current_oci * oci_lambda_scale)
    lambda_eff = np.clip(lambda_eff, 0, 1)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        chosen_a1 = action_1[trial]
        chosen_a2 = action_2[trial]
        r = reward[trial]

        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[state_idx, chosen_a2]
        q_stage2_mf[state_idx, chosen_a2] += learning_rate * delta_stage2
        
        # Stage 1 Update with Eligibility Trace
        # The Stage 1 value is updated by the Stage 1 prediction error...
        delta_stage1 = q_stage2_mf[state_idx, chosen_a2] - q_stage1_mf[chosen_a1] # Note: using updated Q2 is SARSA-like/TD(1) approx
        
        # ...PLUS a portion of the Stage 2 prediction error, scaled by lambda
        total_update = delta_stage1 + (lambda_eff * delta_stage2)
        q_stage1_mf[chosen_a1] += learning_rate * total_update

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Pure Model-Based with OCI-modulated Transition Learning Rate
This model assumes a purely Model-Based strategy where the participant learns the transition matrix (instead of assuming it is fixed at 0.7/0.3). The OCI score modulates how quickly they update their beliefs about the spaceship-planet transitions. High OCI might be associated with hyper-vigilance to transition changes (high learning rate) or rigidity (low learning rate).

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Pure Model-Based learner that learns the transition matrix dynamically.
    OCI modulates the transition learning rate.
    
    Hypothesis: OCI relates to uncertainty monitoring. High OCI individuals may 
    update their internal model of the world (transition probabilities) differently 
    than low OCI individuals.
    
    lr_trans_eff = lr_trans_base * (1 + oci * oci_lr_mod)
    
    Parameters:
    lr_reward: [0, 1] - Learning rate for the reward values at the second stage.
    beta: [0, 10] - Inverse temperature.
    lr_trans_base: [0, 1] - Base learning rate for transition probabilities.
    oci_lr_mod: [-1, 1] - Modulation of transition learning rate by OCI.
    """
    lr_reward, beta, lr_trans_base, oci_lr_mod = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Effective transition learning rate
    lr_trans_eff = lr_trans_base * (1 + current_oci * oci_lr_mod)
    lr_trans_eff = np.clip(lr_trans_eff, 0, 1)

    # Initialize transition matrix (flat prior or starting assumption)
    # Rows: Action 1 (0 or 1), Cols: State (0 or 1)
    # Start with assumption of 0.5 or slight bias. Let's start neutral 0.5
    trans_probs = np.array([[0.5, 0.5], [0.5, 0.5]]) 
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Only Q-values for stage 2 are needed for pure MB
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy (Model-Based) ---
        # Calculate MB values: Transition_Matrix * Max_Q_Stage2
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_stage1_mb = trans_probs @ max_q_stage2
        
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        chosen_a1 = action_1[trial]
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        chosen_a2 = action_2[trial]
        r = reward[trial]

        # 1. Update Reward Values (Model-Free Stage 2)
        delta = r - q_stage2[state_idx, chosen_a2]
        q_stage2[state_idx, chosen_a2] += lr_reward * delta
        
        # 2. Update Transition Probabilities
        # State prediction error: 1 for the state we arrived at, 0 for the other
        # We update the row corresponding to the chosen action
        target = np.zeros(2)
        target[state_idx] = 1.0
        
        # Update row for chosen_a1
        trans_probs[chosen_a1] += lr_trans_eff * (target - trans_probs[chosen_a1])
        
        # Normalize to ensure proper probabilities (though standard delta rule usually keeps sum~1 if started at 1)
        trans_probs[chosen_a1] /= np.sum(trans_probs[chosen_a1])

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```