Here are three new cognitive models exploring different mechanisms for how OCI scores might influence decision-making in the two-step task, specifically focusing on perseveration, model-based control, and uncertainty-driven exploration.

### Cognitive Model 1: OCI-Modulated Perseveration (Stickiness)
This model hypothesizes that high OCI scores are associated with increased behavioral rigidity or "stickiness." Instead of altering learning rates or the balance between model-based/model-free systems directly, OCI acts as a multiplier on a choice autocorrelation term (perseveration). Participants with high OCI are more likely to repeat their previous Stage 1 choice regardless of the outcome, reflecting compulsive repetition.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Perseveration (Stickiness).

    Hypothesis:
    High OCI scores lead to increased "stickiness" or choice perseveration.
    Individuals with higher compulsivity are more prone to repeating the previous
    action regardless of reward history, reflecting a rigid behavioral policy.

    Parameters:
    - learning_rate: [0, 1] Learning rate for value updates.
    - beta: [0, 10] Inverse temperature (softness of softmax).
    - w: [0, 1] Weighting parameter (0 = pure MF, 1 = pure MB).
    - stick_base: [0, 5] Base level of choice stickiness (perseveration).
    - oci_stick_mod: [0, 5] How strongly OCI amplifies stickiness.

    Mechanism:
    - A 'stickiness' bonus is added to the Q-value of the previously chosen action at Stage 1.
    - The magnitude of this bonus is determined by: stick_base + (oci_stick_mod * oci_score).
    """
    learning_rate, beta, w, stick_base, oci_stick_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective stickiness based on OCI
    effective_stickiness = stick_base + (oci_stick_mod * oci_score)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track previous action for stickiness
    prev_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combined Model-Based and Model-Free values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness bonus to the previously chosen action
        q_net_stick = q_net.copy()
        if prev_action_1 != -1:
            q_net_stick[prev_action_1] += effective_stickiness

        exp_q1 = np.exp(beta * q_net_stick)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        chosen_a1 = action_1[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Value Updating ---
        # Stage 1 MF update (TD(0))
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[chosen_a1]
        q_stage1_mf[chosen_a1] += learning_rate * delta_stage1

        # Stage 2 MF update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

        # Eligibility trace update for Stage 1 (TD(1) / Sarsa(lambda=1))
        q_stage1_mf[chosen_a1] += learning_rate * delta_stage2
        
        # Update previous action
        prev_action_1 = chosen_a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2: OCI-Driven Model-Based Suppression
This model tests the theory that high OCI scores (compulsivity) are associated with a degradation of goal-directed (Model-Based) control in favor of habitual (Model-Free) control. Here, the mixing weight `w` is not a static parameter but is dynamically reduced by the OCI score.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Driven Model-Based Suppression.

    Hypothesis:
    Compulsivity is linked to a deficit in goal-directed (Model-Based) control.
    Higher OCI scores reduce the weight 'w' placed on the Model-Based system,
    making behavior more reliant on the Model-Free (habitual) system.

    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w_max: [0, 1] The maximum possible Model-Based weight (for OCI=0).
    - oci_suppression: [0, 2] Factor by which OCI reduces 'w'.

    Mechanism:
    - w_effective = w_max * (1 / (1 + oci_suppression * oci_score))
    - As OCI increases, w_effective decreases, shifting control to MF.
    """
    learning_rate, beta, w_max, oci_suppression = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective w based on OCI suppression
    # Using a decay-like function to keep w bounded between 0 and w_max
    w_effective = w_max * (1.0 / (1.0 + oci_suppression * oci_score))

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2

        # Use the OCI-adjusted effective weight
        q_net = w_effective * q_stage1_mb + (1 - w_effective) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        chosen_a1 = action_1[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Value Updating ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[chosen_a1]
        q_stage1_mf[chosen_a1] += learning_rate * delta_stage1

        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

        # Eligibility trace
        q_stage1_mf[chosen_a1] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3: OCI-Dependent Exploration (Inverse Temperature)
This model posits that OCI affects the exploration-exploitation trade-off. High anxiety or compulsivity might lead to intolerance of uncertainty, manifesting as a very high inverse temperature (`beta`), meaning the participant chooses the option with the slightly higher value deterministically (low exploration). Conversely, it could lead to erratic behavior (high exploration). This model allows the `beta` parameter to scale linearly with OCI.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Dependent Exploration (Beta Modulation).

    Hypothesis:
    OCI scores influence the randomness of choice (exploration vs. exploitation).
    Higher OCI might lead to more deterministic choices (high beta) due to 
    intolerance of uncertainty, or more erratic choices (low beta).
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta_intercept: [0, 10] Baseline inverse temperature when OCI is 0.
    - beta_slope: [-5, 5] How beta changes per unit of OCI. 
      (Note: we will use absolute value or bounding to keep beta positive).
    - w: [0, 1] MB/MF weight.

    Mechanism:
    - beta_effective = beta_intercept + (beta_slope * oci_score)
    - We clip beta_effective to be >= 0 to ensure mathematical validity.
    """
    learning_rate, beta_intercept, beta_slope, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective beta. 
    # If slope is positive, high OCI -> deterministic (rigid).
    # If slope is negative, high OCI -> random (noisy).
    beta_effective = np.clip(beta_intercept + (beta_slope * oci_score), 0.0, 20.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Use OCI-modulated beta
        exp_q1 = np.exp(beta_effective * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        chosen_a1 = action_1[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta_effective * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Value Updating ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[chosen_a1]
        q_stage1_mf[chosen_a1] += learning_rate * delta_stage1

        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

        # Eligibility trace
        q_stage1_mf[chosen_a1] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```