Here are three new cognitive models implemented as Python functions, exploring different mechanisms for how OCI scores might influence decision-making in the two-step task.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Stage 2 Exploration (Beta Asymmetry).
    
    This model hypothesizes that OCI symptoms differentially affect exploration/exploitation 
    trade-offs at the terminal stage (choosing aliens) compared to the planning stage (choosing spaceships).
    High OCI might lead to more rigid exploitation of the immediate option (Stage 2) 
    relative to the first stage choice, reflecting a compulsion to maximize immediate outcomes.
    
    Parameters:
    learning_rate: [0, 1] Value update rate.
    beta_1: [0, 10] Inverse temperature for Stage 1 (Spaceships).
    w: [0, 1] Weight for Model-Based control.
    oci_beta_2_scale: [0, 5] Scaling factor for Stage 2 beta based on OCI.
                             beta_2 = beta_1 * (1 + oci_beta_2_scale * oci)
    
    Bounds:
    learning_rate: [0,1]
    beta_1: [0,10]
    w: [0,1]
    oci_beta_2_scale: [0,5]
    """
    learning_rate, beta_1, w, oci_beta_2_scale = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
  
    # Calculate Stage 2 beta: OCI increases (or decreases) precision at the second stage
    beta_2 = beta_1 * (1.0 + oci_beta_2_scale * current_oci)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Stage 1 Policy (using beta_1)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2 Policy (using beta_2)
        exp_q2 = np.exp(beta_2 * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Updates
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] = q_stage1_mf[a1] + learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] = q_stage2_mf[s_idx, a2] + learning_rate * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Driven Value Decay (Forgetting).
    
    This model posits that OCI symptoms (e.g., intrusive thoughts) interfere with 
    working memory maintenance of value estimates. Unchosen options decay towards 
    neutrality (0) faster for individuals with higher OCI scores.
    
    Parameters:
    learning_rate: [0, 1] Value update rate for chosen options.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Weight for Model-Based control.
    oci_decay_scale: [0, 1] Scaling factor for decay rate. 
                            decay = oci * oci_decay_scale.
                            Q(unchosen) *= (1 - decay).
    
    Bounds:
    learning_rate: [0,1]
    beta: [0,10]
    w: [0,1]
    oci_decay_scale: [0,1]
    """
    learning_rate, beta, w, oci_decay_scale = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
  
    decay_rate = current_oci * oci_decay_scale
    if decay_rate > 1.0: decay_rate = 1.0
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Updates
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] = q_stage1_mf[a1] + learning_rate * delta_stage1
        
        # Stage 1 Decay (Unchosen)
        unchosen_a1 = 1 - a1
        q_stage1_mf[unchosen_a1] *= (1.0 - decay_rate)

        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] = q_stage2_mf[s_idx, a2] + learning_rate * delta_stage2
        
        # Stage 2 Decay (Unchosen in current state only)
        unchosen_a2 = 1 - a2
        q_stage2_mf[s_idx, unchosen_a2] *= (1.0 - decay_rate)
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Driven Punishment Sensitivity (Regret).
    
    This model assumes that individuals with high OCI scores process the absence 
    of a reward (0 coins) as an active punishment rather than a neutral outcome.
    The effective reward for 0 coins becomes negative, scaled by OCI.
    
    Parameters:
    learning_rate: [0, 1] Value update rate.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Weight for Model-Based control.
    oci_punish_scale: [0, 5] Scales the negative utility of 0 reward.
                             If reward=0, effective_reward = -1 * oci * oci_punish_scale.
    
    Bounds:
    learning_rate: [0,1]
    beta: [0,10]
    w: [0,1]
    oci_punish_scale: [0,5]
    """
    learning_rate, beta, w, oci_punish_scale = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Transform Reward: 0 becomes negative (punishment) based on OCI
        effective_r = float(r)
        if r == 0:
            effective_r = -1.0 * current_oci * oci_punish_scale

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Updates
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] = q_stage1_mf[a1] + learning_rate * delta_stage1
        
        delta_stage2 = effective_r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] = q_stage2_mf[s_idx, a2] + learning_rate * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```