Here are 3 new cognitive models based on the two-step task structure and the OCI score.

### Model 1: OCI-Modulated Subjective Transition Belief
This model hypothesizes that OCI symptoms distort the Model-Based (MB) system's understanding of the environment's probabilistic structure. While the true transition probability of the "common" path is 0.7, individuals with varying OCI levels might subjectively perceive this probability as more deterministic (closer to 1.0, reflecting rigidity) or more chaotic (closer to 0.5, reflecting uncertainty), regardless of the actual experience. This "subjective belief" replaces the true matrix in the MB calculation.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Subjective Transition Belief.
    
    Hypothesis: OCI distorts the internal model of transition probabilities used 
    for Model-Based planning. High OCI may lead to 'black and white' thinking 
    (perceiving transitions as deterministic) or doubt (perceiving them as random), 
    distinct from the objective 0.7 probability.
    
    Parameters:
    lr: [0, 1] Learning rate for MF values.
    beta: [0, 10] Inverse temperature (softmax sensitivity).
    w: [0, 1] Weighting between MB (1) and MF (0) control.
    p_belief_base: [0, 1] Baseline subjective probability of the common transition.
    p_belief_oci_slope: [-1, 1] How OCI score shifts this probability belief.
    """
    lr, beta, w, p_belief_base, p_belief_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate subjective transition probability
    # We clip it between 0.5 (random) and 1.0 (deterministic) for stability
    raw_p = p_belief_base + p_belief_oci_slope * oci_score
    subjective_p = np.clip(raw_p, 0.5, 0.99)
    
    # Construct the subjective transition matrix used for planning
    # If subjective_p is high, agent assumes Space A -> Planet X is nearly guaranteed
    subjective_matrix = np.array([
        [subjective_p, 1 - subjective_p], 
        [1 - subjective_p, subjective_p]
    ])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)     # MF values for spaceships
    q_stage2_mf = np.zeros((2, 2)) # MF values for aliens (2 planets x 2 aliens)

    for trial in range(n_trials):
        # --- STAGE 1 CHOICE ---
        # Model-Based Value Calculation using Subjective Matrix
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = subjective_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net_s1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_s1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        a1 = int(action_1[trial])
        s_idx = int(state[trial]) # Planet arrived at

        # --- STAGE 2 CHOICE ---
        # Simple MF choice at stage 2
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- UPDATES ---
        # Update Stage 1 MF (TD0)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        # Update Stage 2 MF
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Stage-Learning Asymmetry
This model hypothesizes that OCI affects the balance of learning between the "Planning" stage (Stage 1, choice of spaceship) and the "Outcome" stage (Stage 2, choice of alien). High OCI might be associated with hyper-focus on the immediate outcome (Stage 2) at the expense of learning the predictive structure of the first stage, or vice versa. This is modeled by scaling the Stage 1 learning rate relative to Stage 2 based on OCI.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Modulated Stage-Learning Asymmetry.
    
    Hypothesis: OCI modulates the ratio of learning rates between Stage 1 
    (Transition/Planning update) and Stage 2 (Reward/Outcome update).
    High OCI might drive strong learning from direct rewards (Stage 2) but 
    weaker learning from temporal difference errors in Stage 1.
    
    Parameters:
    lr_stage2: [0, 1] Base learning rate for Stage 2 (Aliens).
    beta: [0, 10] Inverse temperature.
    w: [0, 1] MB/MF weight.
    lr1_ratio_base: [0, 2] Base ratio of LR1 to LR2.
    lr1_ratio_oci_slope: [-2, 2] Effect of OCI on this ratio.
    """
    lr_stage2, beta, w, lr1_ratio_base, lr1_ratio_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate Stage 1 Learning Rate
    ratio = lr1_ratio_base + lr1_ratio_oci_slope * oci_score
    # Ensure ratio is non-negative
    ratio = max(0.0, ratio)
    
    lr_stage1 = lr_stage2 * ratio
    # Clip to valid learning rate range [0, 1]
    lr_stage1 = min(1.0, lr_stage1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- STAGE 1 CHOICE ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_s1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_s1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        a1 = int(action_1[trial])
        s_idx = int(state[trial])

        # --- STAGE 2 CHOICE ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- UPDATES ---
        # Use OCI-modulated lr_stage1
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr_stage1 * delta_stage1
        
        # Use base lr_stage2
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr_stage2 * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Second-Stage Response Stickiness
Previous models explored general choice perseveration (stickiness). This model specifically targets **Stage 2 Response Stickiness**. It hypothesizes that compulsive behavior (OCI) manifests as a repetitive motor response at the final stage (e.g., always pressing the left button for the Alien), regardless of the planet reached or the path taken. This distinguishes "planning rigidity" from "motor/response rigidity."

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Second-Stage Response Stickiness.
    
    Hypothesis: OCI relates to motor perseveration specifically at the outcome stage 
    (Stage 2). High OCI participants may tend to repeat the same button press 
    (Alien 0 or Alien 1) from the previous trial, regardless of which planet 
    they are currently on.
    
    Parameters:
    lr: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] MB/MF weight.
    st2_stick_base: [-2, 2] Base tendency to repeat Stage 2 action.
    st2_stick_oci: [-2, 2] OCI modulation of Stage 2 stickiness.
    """
    lr, beta, w, st2_stick_base, st2_stick_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate stickiness bonus for Stage 2
    stickiness_val = st2_stick_base + st2_stick_oci * oci_score
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_a2 = -1 # Track previous Stage 2 action

    for trial in range(n_trials):
        # --- STAGE 1 CHOICE ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_s1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_s1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        a1 = int(action_1[trial])
        s_idx = int(state[trial])

        # --- STAGE 2 CHOICE ---
        # Apply stickiness to Stage 2 values locally
        q_s2_local = q_stage2_mf[s_idx].copy()
        
        if prev_a2 != -1:
            q_s2_local[prev_a2] += stickiness_val
            
        exp_q2 = np.exp(beta * q_s2_local)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        prev_a2 = a2 # Update previous Stage 2 action
        
        r = reward[trial]
        
        # --- UPDATES ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```