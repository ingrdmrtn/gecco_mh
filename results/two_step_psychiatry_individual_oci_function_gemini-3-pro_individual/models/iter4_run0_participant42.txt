Here are 3 new cognitive models exploring different mechanisms by which OCI scores might influence reinforcement learning in the two-step task, specifically focusing on how negative outcomes are processed, how habit strength is modulated, and how exploration is regulated.

### Model 1: Asymmetric Learning Rates Modulated by OCI
This model hypothesizes that individuals with higher OCI scores might exhibit a "negativity bias" or heightened sensitivity to negative outcomes (punishment/omission of reward). Instead of a single learning rate, we split learning into positive (`lr_pos`) and negative (`lr_neg`) updates. The negative learning rate is modulated by the OCI score.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Asymmetric Learning Rate model where OCI modulates sensitivity to negative prediction errors.
    
    Hypothesis: High OCI is associated with increased sensitivity to negative outcomes 
    (or lack of reward), leading to faster updating when expectations are not met.
    
    Parameters:
    - lr_pos: [0, 1] Learning rate for positive prediction errors (Reward = 1).
    - lr_neg_base: [0, 1] Base learning rate for negative prediction errors (Reward = 0).
    - lr_neg_oci: [0, 1] Additional negative learning rate scaling per unit of OCI.
    - beta: [0, 10] Inverse temperature (softmax).
    - w: [0, 1] Mixing weight for Model-Based (1) vs Model-Free (0).
    """
    lr_pos, lr_neg_base, lr_neg_oci, beta, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective negative learning rate, bounded at 1.0
    lr_neg_effective = min(1.0, lr_neg_base + (lr_neg_oci * oci_score))
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = int(state[trial])
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        # Determine which learning rate to use based on the reward (outcome)
        # Note: We use the final reward to determine the "valence" of the update for both stages
        current_lr = lr_pos if r > 0 else lr_neg_effective

        # Stage 2 Update (Model-Free)
        rpe_2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += current_lr * rpe_2

        # Stage 1 Update (Model-Free TD(1))
        # Using the direct reward for simplicity in this TD(1) formulation
        rpe_1 = r - q_stage1_mf[a1]
        q_stage1_mf[a1] += current_lr * rpe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Decay (Forgetting) of Unchosen Options
This model introduces a memory decay parameter. In standard RL, unchosen options usually retain their values. This model posits that high OCI scores might correlate with "obsessive" maintenance of value (low decay) or, conversely, anxiety-driven forgetting. Here, we model the decay rate of *unchosen* actions as a function of OCI.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model with value decay for unchosen options, modulated by OCI.
    
    Hypothesis: OCI affects how well values of unchosen options are maintained 
    in working memory.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for chosen options.
    - decay_base: [0, 1] Base decay rate for unchosen options (1 = no decay, 0 = instant forgetting).
    - decay_oci_slope: [-1, 1] Modulation of decay by OCI.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight for MB vs MF.
    """
    learning_rate, decay_base, decay_oci_slope, beta, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective decay, bounded [0, 1]
    # If slope is negative, high OCI leads to faster forgetting (lower decay factor).
    decay_effective = decay_base + (decay_oci_slope * oci_score)
    decay_effective = max(0.0, min(1.0, decay_effective))

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = int(state[trial])
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Update Chosen Options ---
        # Stage 2
        rpe_2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * rpe_2
        
        # Stage 1
        rpe_1 = r - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * rpe_1
        
        # --- Decay Unchosen Options ---
        # Decay Stage 1 unchosen
        unchosen_a1 = 1 - a1
        q_stage1_mf[unchosen_a1] *= decay_effective
        
        # Decay Stage 2 unchosen (only in the visited state)
        unchosen_a2 = 1 - a2
        q_stage2_mf[state_idx, unchosen_a2] *= decay_effective
        
        # Note: We do not decay the unvisited state's values in this specific model variant
        # to keep the focus on the active decision path.

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Random Exploration (Epsilon-Greedy Hybrid)
Instead of modulating the learning process or the softmax temperature, this model hypothesizes that OCI affects "directed" vs "random" exploration. High OCI might lead to rigid adherence to the calculated policy (low random noise/lapses), or conversely, high anxiety might lead to more random "checking" behaviors. We model this as an epsilon-greedy mixture where the epsilon (lapse rate) depends on OCI.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Softmax model with OCI-modulated 'lapse' or random exploration rate.
    
    Hypothesis: The decision policy is a mixture of the softmax distribution and 
    a uniform random distribution. The weight of the random component (epsilon)
    is determined by OCI.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature for the softmax component.
    - w: [0, 1] Mixing weight for MB vs MF.
    - epsilon_base: [0, 1] Base probability of choosing randomly.
    - epsilon_oci: [-1, 1] Modulation of epsilon by OCI.
    """
    learning_rate, beta, w, epsilon_base, epsilon_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective epsilon, bounded [0, 1]
    epsilon_effective = epsilon_base + (epsilon_oci * oci_score)
    epsilon_effective = max(0.0, min(1.0, epsilon_effective))

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax component
        exp_q1 = np.exp(beta * q_net)
        softmax_probs_1 = exp_q1 / np.sum(exp_q1)
        
        # Mixture with uniform random (0.5 for binary choice)
        # P(choice) = (1 - eps) * Softmax + eps * 0.5
        final_prob_1 = (1 - epsilon_effective) * softmax_probs_1 + (epsilon_effective * 0.5)
        p_choice_1[trial] = final_prob_1[action_1[trial]]
        
        state_idx = int(state[trial])
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        softmax_probs_2 = exp_q2 / np.sum(exp_q2)
        
        final_prob_2 = (1 - epsilon_effective) * softmax_probs_2 + (epsilon_effective * 0.5)
        p_choice_2[trial] = final_prob_2[a2]

        # --- Learning ---
        rpe_2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * rpe_2

        rpe_1 = r - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * rpe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```