Here are the three proposed cognitive models.

### Model 1: Subjective Transition Belief Model
This model hypothesizes that OCI symptoms modulate the participant's internal model of the spaceship-planet transition probabilities. While the true probability is 0.7, participants with varying OCI levels might perceive the world as more deterministic (closer to 1.0) or more chaotic (closer to 0.5), affecting their Model-Based value calculation.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL with OCI-modulated Subjective Transition Belief.
    
    The Model-Based component relies on a transition matrix. This model proposes
    that OCI modulates the perceived probability of the 'common' transition.
    High OCI might lead to a belief in more deterministic transitions (rigidity)
    or less deterministic ones (uncertainty), altering the MB value calculation.

    Parameters:
    - learning_rate: [0, 1] Learning rate for MF values.
    - beta: [0, 10] Inverse temperature for softmax.
    - w: [0, 1] Weighting between MB (1) and MF (0) systems.
    - trans_belief_base: [0, 1] Baseline belief in the common transition probability (e.g., 0.7).
    - trans_belief_oci_mod: [0, 1] Magnitude of OCI modulation on transition belief.
      (The effective probability is clipped between 0.5 and 1.0).
    """
    learning_rate, beta, w, trans_belief_base, trans_belief_oci_mod = model_parameters
    n_trials = len(action_1)
    oci_val = oci[0]
  
    # Calculate subjective transition probability
    # We assume the modulation increases/decreases belief in the common transition
    # We center the modulation such that it modifies the base belief
    # Note: We clip to ensure it remains a valid probability > 0.5 (common)
    subjective_p = trans_belief_base + (oci_val * trans_belief_oci_mod)
    subjective_p = np.clip(subjective_p, 0.5, 0.99)
    
    # Construct the subjective transition matrix
    # Row 0: Space A -> [Prob X, Prob Y]
    # Row 1: Space B -> [Prob X, Prob Y] (Symmetric flip)
    transition_matrix = np.array([[subjective_p, 1 - subjective_p], 
                                  [1 - subjective_p, subjective_p]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        
        # Model-Based Value: computed using the subjective transition matrix
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Update Stage 1 MF
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Update Stage 2 MF
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Win-Stay Stickiness
This model differentiates between general perseveration (repeating any choice) and "Win-Stay" behavior (repeating a choice only if it was rewarded). It hypothesizes that OCI symptoms specifically modulate the compulsion to repeat a successful action, potentially reflecting a "safe" strategy or enhanced reinforcement sensitivity.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL with OCI-modulated Reward-Dependent Stickiness (Win-Stay).
    
    Unlike general stickiness, this model adds a bonus to the previous Stage 1 choice
    ONLY if the previous trial resulted in a reward (coin). The magnitude of this
    'Win-Stay' bonus is modulated by the OCI score.

    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weighting between MB and MF.
    - win_stay_base: [0, 5] Base bonus added to the previously rewarded action.
    - oci_win_stay_mod: [0, 5] Modulation of the Win-Stay bonus by OCI.
    """
    learning_rate, beta, w, win_stay_base, oci_win_stay_mod = model_parameters
    n_trials = len(action_1)
    oci_val = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    last_reward = 0

    # Calculate effective win-stay bonus
    effective_win_stay = win_stay_base + (oci_val * oci_win_stay_mod)

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Apply Win-Stay Bonus
        if last_action_1 != -1 and last_reward == 1:
            q_net[last_action_1] += effective_win_stay
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Record for next trial
        last_action_1 = action_1[trial]
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # Record reward for next trial's bonus
        last_reward = reward[trial]
  
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Dissociated Stage 1 Learning Rate Model
This model tests the hypothesis that OCI affects learning at different levels of abstraction. It proposes that while Stage 2 (direct bandit task) learning is constant, the learning rate for Stage 1 (the more abstract choice leading to a state transition) is modulated by OCI. This could reflect how compulsivity affects credit assignment for distal vs. proximal outcomes.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL with OCI-modulated Stage 1 Learning Rate.
    
    This model allows for different learning rates for the first stage (spaceship choice)
    and the second stage (alien choice). Specifically, the Stage 1 learning rate
    is modulated by the OCI score, testing if OCI impacts the updating of 
    distal predictors (Stage 1) differently than proximal ones (Stage 2).

    Parameters:
    - lr_stage2: [0, 1] Fixed learning rate for Stage 2 (Aliens).
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weighting between MB and MF.
    - lr_stage1_base: [0, 1] Base learning rate for Stage 1.
    - lr_stage1_oci_mod: [0, 1] OCI modulation for Stage 1 learning rate. 
      (Result is clipped to [0,1]).
    """
    lr_stage2, beta, w, lr_stage1_base, lr_stage1_oci_mod = model_parameters
    n_trials = len(action_1)
    oci_val = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Calculate effective Stage 1 learning rate
    lr_stage1 = lr_stage1_base + (oci_val * lr_stage1_oci_mod)
    lr_stage1 = np.clip(lr_stage1, 0.0, 1.0)

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Update Stage 1 using the OCI-modulated rate
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr_stage1 * delta_stage1
        
        # Update Stage 2 using the fixed rate
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr_stage2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```