Here are three new cognitive models that introduce different mechanisms for how OCI scores might influence decision-making in the two-step task, specifically focusing on mixing parameters, learning rate modulation, and inverse temperature modulation in ways not yet fully explored in the previous attempts.

### Model 1: OCI-Modulated Model-Based Weighting (Hybrid Learner)
This model tests the hypothesis that OCI symptoms influence the balance between Model-Based (planning) and Model-Free (habitual) control. Specifically, it proposes that higher OCI scores might lead to either a rigid reliance on habits (lower $w$) or an over-reliance on explicit rules (higher $w$).

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free learner where the mixing weight 'w' is modulated by OCI.
    
    Hypothesis: OCI traits alter the balance between goal-directed (MB) and habitual (MF) control.
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    w_base: [0, 1] (Base weighting for Model-Based system)
    w_oci_mod: [-1, 1] (Modulation of w by OCI score)
    """
    learning_rate, beta, w_base, w_oci_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective w and clip to [0, 1]
    w = w_base + (w_oci_mod * oci_score)
    if w > 1.0: w = 1.0
    if w < 0.0: w = 0.0

    # Initialize Q-values
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Fixed transition matrix for MB calculation (70/30 structure)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based value: expected value of best stage 2 options weighted by transition probs
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Q-value
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # --- Stage 2 Policy ---
        # Standard softmax on Stage 2 Q-values
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Updates ---
        # Stage 2 TD Error
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 TD Error (SARSA-style update for MF)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Second-Stage Learning Rate (Asymmetric Updating)
This model investigates if OCI scores affect how quickly participants update their beliefs about the reward probabilities at the second stage (the aliens). It tests the idea that high OCI might correlate with "hyper-learning" (rapidly changing beliefs based on recent outcomes) or rigidity (ignoring new evidence).

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model with OCI-modulated learning rate specifically for the second stage (reward estimation).
    
    Hypothesis: OCI traits influence the volatility of reward value estimation (Stage 2),
    while Stage 1 (transition structure) learning remains constant or implicit.
    
    Bounds:
    lr_base: [0, 1]
    lr_oci_slope: [-1, 1]
    beta: [0, 10]
    w: [0, 1]
    """
    lr_base, lr_oci_slope, beta, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective learning rate for stage 2
    lr_stage2 = lr_base + (lr_oci_slope * oci_score)
    if lr_stage2 > 1.0: lr_stage2 = 1.0
    if lr_stage2 < 0.0: lr_stage2 = 0.0
    
    # We use the base lr for stage 1 updates in this specific formulation
    lr_stage1 = lr_base 

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # Values of aliens
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Updates ---
        # Update Stage 2 values using the OCI-modulated rate
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += lr_stage2 * delta_stage2
        
        # Update Stage 1 MF values using base rate
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr_stage1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Randomness (Noise)
This model posits that OCI symptoms might relate to the "noise" or consistency of decision-making. Instead of altering the balance of systems or learning rates, OCI might simply modulate the inverse temperature ($\beta$), making choices more deterministic (high $\beta$, obsessive rigidity) or more stochastic (low $\beta$).

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model where the inverse temperature (beta) is modulated by OCI.
    
    Hypothesis: OCI traits affect the exploration/exploitation trade-off or decision noise.
    High OCI might lead to more deterministic (rigid) choices (higher beta), 
    or potentially more noisy behavior depending on the slope.
    
    Bounds:
    learning_rate: [0, 1]
    beta_base: [0, 10]
    beta_oci_slope: [-10, 10]
    w: [0, 1]
    """
    learning_rate, beta_base, beta_oci_slope, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective beta
    beta_eff = beta_base + (beta_oci_slope * oci_score)
    # Ensure beta stays within reasonable positive bounds
    if beta_eff < 0.0: beta_eff = 0.0
    if beta_eff > 20.0: beta_eff = 20.0 # Upper clamp to prevent overflow

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Use OCI-modulated beta here
        exp_q1 = np.exp(beta_eff * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # --- Stage 2 Policy ---
        # Use OCI-modulated beta here as well
        exp_q2 = np.exp(beta_eff * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Updates ---
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```