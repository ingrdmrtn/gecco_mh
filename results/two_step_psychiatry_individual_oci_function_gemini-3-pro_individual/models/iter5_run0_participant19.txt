Here are three new cognitive models implemented as Python functions, exploring different mechanisms for how OCI might influence decision-making in this task.

### Model 1: Model-Free Learning with OCI-Modulated Forgetting
This model tests the hypothesis that OCI scores relate to the persistence of memory for action values. Specifically, it proposes that while chosen actions are updated via reinforcement learning, **unchosen** actions decay (are forgotten) at a rate determined by the participant's OCI score. A high OCI might lead to rigid retention of values (low forgetting) or excessive doubting (high forgetting).

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner with Forgetting Rate modulated by OCI.
    
    Hypothesis: OCI affects how quickly values for UNCHOSEN options decay.
    
    Parameters:
    lr: [0, 1] Learning rate for chosen options.
    beta: [0, 10] Inverse temperature (softmax sensitivity).
    decay_base: [0, 1] Baseline decay rate for unchosen options.
    oci_decay_mod: [-1, 1] Modulation of decay rate by OCI.
                   (decay = base + mod * oci). Clipped to [0, 1].
    """
    lr, beta, decay_base, oci_decay_mod = model_parameters
    n_trials = len(action_1)
    oci_val = oci[0]

    # Calculate effective decay rate constrained to [0, 1]
    decay_rate = decay_base + (oci_decay_mod * oci_val)
    decay_rate = np.clip(decay_rate, 0.0, 1.0)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1 = np.zeros(2) # Values for Spaceships A(0), U(1)
    q_stage2 = np.zeros((2, 2)) # Values for Aliens (Planet X: 0,1; Planet Y: 0,1)

    for trial in range(n_trials):
        
        # --- Stage 1 Policy ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        r = reward[trial]

        # Stage 2 Update (TD)
        delta_stage2 = r - q_stage2[state_idx, a2]
        q_stage2[state_idx, a2] += lr * delta_stage2
        
        # Stage 2 Forgetting (Unchosen Alien)
        unchosen_a2 = 1 - a2
        q_stage2[state_idx, unchosen_a2] *= (1.0 - decay_rate)

        # Stage 1 Update (TD using max Q from stage 2)
        max_q2_next = np.max(q_stage2[state_idx])
        delta_stage1 = max_q2_next - q_stage1[a1]
        q_stage1[a1] += lr * delta_stage1
        
        # Stage 1 Forgetting (Unchosen Spaceship)
        unchosen_a1 = 1 - a1
        q_stage1[unchosen_a1] *= (1.0 - decay_rate)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Model-Free Learning with OCI-Modulated Punishment Sensitivity
This model tests the hypothesis that OCI relates to "Loss Aversion" or sensitivity to negative feedback. It posits that individuals with higher OCI scores might perceive negative rewards (punishments) more intensely than those with lower scores.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner with Punishment Sensitivity modulated by OCI.
    
    Hypothesis: OCI scores scale the subjective magnitude of negative rewards.
    
    Parameters:
    lr: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    punish_mult_base: [0, 5] Baseline multiplier for negative rewards.
    oci_punish_mod: [-5, 5] How OCI modifies punishment multiplier.
                    (effective_mult = base + mod * oci).
    """
    lr, beta, punish_mult_base, oci_punish_mod = model_parameters
    n_trials = len(action_1)
    oci_val = oci[0]

    # Calculate effective punishment multiplier
    # Ensure multiplier is non-negative to maintain direction of punishment
    p_mult = punish_mult_base + (oci_punish_mod * oci_val)
    p_mult = max(0.0, p_mult) 

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        
        # --- Stage 1 Policy ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        r = reward[trial]
        
        # Apply Punishment Sensitivity
        effective_r = r
        if r < 0:
            effective_r = r * p_mult

        # Stage 2 Update
        delta_stage2 = effective_r - q_stage2[state_idx, a2]
        q_stage2[state_idx, a2] += lr * delta_stage2

        # Stage 1 Update
        max_q2_next = np.max(q_stage2[state_idx])
        delta_stage1 = max_q2_next - q_stage1[a1]
        q_stage1[a1] += lr * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated "Win-Stay" Strategy
This model differentiates between general habit (simple perseveration) and reinforcement-driven repetition. It hypothesizes that OCI specifically modulates the tendency to repeat a choice **if and only if** it was previously rewarded (Win-Stay), rather than just repeating any previous choice.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner with Win-Stay Bonus modulated by OCI.
    
    Hypothesis: OCI modulates the specific tendency to repeat REWARDED actions,
    distinct from general stickiness.
    
    Parameters:
    lr: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    stick_base: [0, 5] General choice stickiness (applied every trial).
    winstay_oci_mod: [-5, 5] Extra stickiness applied ONLY after a reward,
                             scaled by OCI.
    """
    lr, beta, stick_base, winstay_oci_mod = model_parameters
    n_trials = len(action_1)
    oci_val = oci[0]

    # Calculate Win-Stay bonus factor
    winstay_bonus_factor = winstay_oci_mod * oci_val

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    last_a1 = -1
    last_reward = 0

    for trial in range(n_trials):
        
        # --- Stage 1 Policy ---
        # Base Q-values
        logits = beta * q_stage1.copy()
        
        # Add Stickiness Bonuses
        if last_a1 != -1:
            # General stickiness (habit)
            logits[last_a1] += stick_base
            
            # Win-Stay bonus (conditional on previous reward)
            if last_reward > 0:
                logits[last_a1] += winstay_bonus_factor
        
        # Softmax
        # Subtract max for numerical stability
        logits = logits - np.max(logits)
        exp_q1 = np.exp(logits)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        r = reward[trial]

        # Stage 2 Update
        delta_stage2 = r - q_stage2[state_idx, a2]
        q_stage2[state_idx, a2] += lr * delta_stage2

        # Stage 1 Update
        max_q2_next = np.max(q_stage2[state_idx])
        delta_stage1 = max_q2_next - q_stage1[a1]
        q_stage1[a1] += lr * delta_stage1
        
        # Store history
        last_a1 = a1
        last_reward = r

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```