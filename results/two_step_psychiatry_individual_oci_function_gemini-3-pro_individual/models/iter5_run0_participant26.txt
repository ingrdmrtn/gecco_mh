Here are 3 new cognitive models based on the provided template and participant data.

### Model 1: OCI-Modulated Loss Aversion
This model hypothesizes that OCI scores correlate with "loss aversion" or sensitivity to negative outcomes. In this task, getting 0 coins is the negative outcome. Instead of treating 0 coins as a neutral 0 value, this model treats it as a punishment (negative value), the magnitude of which is scaled by the participant's OCI score.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Loss Aversion.
    
    This model assumes that OCI modulates the subjective value of non-reward (0 coins).
    Rather than 0 being neutral, it is experienced as a loss.
    The effective reward is: Reward - (Loss_Base + OCI * Loss_Mod) if Reward is 0.
    It uses a standard hybrid Model-Based/Model-Free architecture.

    Parameters:
    - learning_rate: [0, 1] Rate at which Q-values are updated.
    - beta: [0, 10] Inverse temperature (softmax randomness).
    - w: [0, 1] Weighting parameter (0=Pure MF, 1=Pure MB).
    - loss_base: [0, 2] Baseline negative value assigned to missing a reward.
    - oci_loss_mod: [0, 5] How strongly OCI increases this feeling of loss.
    """
    learning_rate, beta, w, loss_base, oci_loss_mod = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
  
    # Standard transition matrix for the MB component
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    # Calculate the subjective penalty for getting 0 coins
    subjective_loss = loss_base + (current_oci * oci_loss_mod)

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        # Model-Based Value
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Updates ---
        # Calculate effective reward (subjective)
        r_actual = reward[trial]
        if r_actual == 0:
            r_effective = -subjective_loss
        else:
            r_effective = r_actual

        # Stage 1 MF Update (TD(0))
        # Note: In standard hybrid models, Stage 1 MF is often updated by the Stage 2 MF value 
        # (SARSA-style) or the best Stage 2 value. Here we use the chosen Stage 2 Q-value.
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1
        
        # Stage 2 MF Update
        delta_stage2 = r_effective - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Biased Transition Beliefs
This model suggests that OCI affects the Model-Based component specifically by distorting the internal model of the world structure. While the true transition probabilities are 0.7/0.3, this model posits that participants with different OCI scores might perceive the "common" transition as more deterministic (closer to 1.0) or more uncertain (closer to 0.5).

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Biased Transition Beliefs.
    
    This model assumes OCI distorts the participant's internal model of the 
    transition matrix used for Model-Based planning. 
    A base belief is modified by OCI.
    
    Parameters:
    - learning_rate: [0, 1] Update rate for MF values.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weighting between MB and MF.
    - trans_belief_base: [0.5, 1.0] The baseline belief in the probability of the common transition.
    - oci_trans_mod: [-1, 1] How OCI shifts this belief. 
      (Positive = OCI makes them think structure is more rigid/deterministic).
      (Negative = OCI makes them doubt the structure/think it's random).
    """
    learning_rate, beta, w, trans_belief_base, oci_trans_mod = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
  
    # Calculate subjective transition probability
    # We clip it to ensure it stays a valid probability [0, 1]
    p_common_subjective = trans_belief_base + (current_oci * oci_trans_mod)
    if p_common_subjective > 1.0: p_common_subjective = 1.0
    if p_common_subjective < 0.0: p_common_subjective = 0.0
    
    # Construct the subjective transition matrix
    # If p_common is for A->X, then p_rare is 1-p_common.
    # Matrix: [[A->X, A->Y], [U->X, U->Y]] (assuming standard mapping logic)
    # Actually, usually defined as [[Common, Rare], [Rare, Common]] 
    # to align with state indices 0 and 1 relative to choice.
    transition_matrix = np.array([
        [p_common_subjective, 1.0 - p_common_subjective], 
        [1.0 - p_common_subjective, p_common_subjective]
    ])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        
        # The MB calculation uses the DISTORTED transition matrix
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Initial Optimism (Priors)
This model investigates if OCI scores relate to the initial expectations (priors) participants bring to the task. Standard Q-learning initializes values at 0. This model initializes Q-values based on OCI, testing if higher OCI leads to initial "optimism" (expecting high rewards) or "pessimism" (expecting low rewards/failure), which significantly impacts exploration in the early trials.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Initial Optimism (Priors).
    
    This model assumes OCI determines the starting Q-values (priors).
    Instead of starting at 0, values start at a level defined by OCI.
    This affects how quickly they settle on a choice or how much they explore
    early on.
    
    Parameters:
    - learning_rate: [0, 1]
    - beta: [0, 10]
    - w: [0, 1]
    - q_init_base: [0, 1] Baseline initial Q-value.
    - oci_init_mod: [-1, 1] Modulation of initial value by OCI.
    """
    learning_rate, beta, w, q_init_base, oci_init_mod = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Calculate initial Q-value
    q_start_val = q_init_base + (current_oci * oci_init_mod)
    
    # Initialize Q-tables with this value
    q_stage1_mf = np.full(2, q_start_val)
    q_stage2_mf = np.full((2, 2), q_start_val)

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```