Here are three new cognitive models for the two-step decision task, incorporating the OCI-R score to explain individual variability.

### Model 1: OCI-Modulated Stage 2 Precision
This model hypothesizes that OCI symptoms specifically affect the decision noise (inverse temperature) in the second stage (Goal-Directed execution) while leaving the first stage (Planning) parameter distinct. High OCI might lead to more rigid or deterministic choices once the goal state (planet) is reached, or conversely, more noise.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Split Beta Model with OCI-modulated Stage 2 Precision.
    
    Hypothesis: OCI differentially impacts the exploration/exploitation trade-off 
    at the second stage (immediate reward) compared to the first stage (planning).
    
    Parameters:
    learning_rate: [0,1] - Value learning rate.
    w: [0,1] - Mixing weight (0=MF, 1=MB).
    beta_stage1: [0,10] - Inverse temperature for Stage 1 (Spaceship choice).
    beta_stage2_base: [0,10] - Baseline inverse temperature for Stage 2 (Alien choice).
    oci_beta2_sens: [0,10] - Sensitivity of Stage 2 beta to OCI.
    """
    learning_rate, w, beta_stage1, beta_stage2_base, oci_beta2_sens = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    # Calculate effective Stage 2 beta based on OCI
    # We assume OCI modulates rigidity (higher beta) or noise (lower beta) additively.
    # To allow for flexibility, we start with base and add the OCI effect.
    beta_stage2 = beta_stage2_base + (current_oci * oci_beta2_sens)
    if beta_stage2 < 0: beta_stage2 = 0.0
    if beta_stage2 > 20: beta_stage2 = 20.0 # Cap to prevent overflow

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_stage1 * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        s_idx = int(state[trial])
        
        # Use the OCI-modulated beta for Stage 2
        exp_q2 = np.exp(beta_stage2 * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # --- Updating ---
        r = reward[trial]
        
        # Stage 2 RPE
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 RPE (Model-Free)
        # Using simple TD(1) logic (updating Stage 1 with Stage 2 outcome)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        # Note: Standard Daw implementation often uses lambda=1 implicitly for simple MF
        # or updates with the final reward. Here we stick to simple SARSA-like structure
        # or direct reward update. Let's use the reward driven update for habit:
        q_stage1_mf[a1] += learning_rate * (r - q_stage1_mf[a1])

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Spatial Bias
This model hypothesizes that OCI correlates with a fixed "compulsive" side bias (specifically towards Spaceship A/Option 0), representing a motor stereotypy or safety behavior that persists regardless of value.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model with OCI-modulated Spatial Bias.
    
    Hypothesis: OCI scores predict a static bias towards Option 0 (Spaceship A),
    representing a compulsive motor repetition or side-preference.
    
    Parameters:
    learning_rate: [0,1] - Learning rate.
    beta: [0,10] - Inverse temperature.
    w: [0,1] - Mixing weight.
    bias_base: [0,1] - Baseline bias towards Option 0.
    oci_bias_sens: [0,5] - Sensitivity of bias to OCI.
    """
    learning_rate, beta, w, bias_base, oci_bias_sens = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    # Calculate effective bias
    # Bias is added to the Q-value of Option 0
    bias_eff = bias_base + (current_oci * oci_bias_sens)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Apply OCI-modulated bias to Option 0
        q_net_stage1[0] += bias_eff
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        s_idx = int(state[trial])
        
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # --- Updating ---
        r = reward[trial]
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Simple MF update for Stage 1
        q_stage1_mf[a1] += learning_rate * (r - q_stage1_mf[a1])

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Transition Learning
This model hypothesizes that participants do not assume fixed transition probabilities (0.7/0.3) but learn them online. OCI modulates the *learning rate* of this structural learning. High OCI might lead to "over-updating" beliefs about the world structure based on recent rare transitions (high transition learning rate).

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model with OCI-modulated Transition Learning Rate.
    
    Hypothesis: The agent learns the transition matrix online. OCI affects 
    how rapidly the agent updates their belief about spaceship-planet transitions.
    High OCI may result in instability in the internal model of the environment.
    
    Parameters:
    lr_reward: [0,1] - Learning rate for reward values (Q-values).
    beta: [0,10] - Inverse temperature.
    w: [0,1] - Mixing weight.
    lr_trans_base: [0,1] - Baseline learning rate for transition probabilities.
    oci_lr_trans_sens: [0,5] - Sensitivity of transition learning rate to OCI.
    """
    lr_reward, beta, w, lr_trans_base, oci_lr_trans_sens = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    # Calculate effective transition learning rate
    lr_trans = lr_trans_base + (current_oci * oci_lr_trans_sens)
    if lr_trans > 1.0: lr_trans = 1.0
    if lr_trans < 0.0: lr_trans = 0.0

    # Initialize transition matrix (Agent starts with correct knowledge)
    # But will update it based on experience
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Use current belief of transition matrix for MB planning
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        s_idx = int(state[trial]) # The planet we actually arrived at
        
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # --- Updating Values ---
        r = reward[trial]
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr_reward * delta_stage2
        
        q_stage1_mf[a1] += lr_reward * (r - q_stage1_mf[a1])
        
        # --- Updating Transition Matrix ---
        # We observed a transition from a1 to s_idx.
        # Create a target vector: 1 for the observed state, 0 for the other.
        target_trans = np.zeros(2)
        target_trans[s_idx] = 1.0
        
        # Update the row corresponding to the chosen spaceship (a1)
        # Row must sum to 1, so we update and normalize or use delta rule on probabilities directly
        # Standard delta rule: P_new = P_old + alpha * (Outcome - P_old)
        transition_matrix[a1] += lr_trans * (target_trans - transition_matrix[a1])

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```