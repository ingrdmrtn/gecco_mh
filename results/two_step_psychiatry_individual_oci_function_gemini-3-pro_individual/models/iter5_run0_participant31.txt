Here are three new cognitive models that incorporate OCI scores into the decision-making process, using mechanisms distinct from previous attempts.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: Hybrid Model with OCI-Modulated Choice Kernel (Habit).
    
    This model combines Model-Based and Model-Free learning with a "Choice Kernel"
    representing habit strength. The influence of this habit is modulated by the OCI score.
    Unlike simple stickiness (which typically repeats only the very last choice), 
    the kernel integrates the history of past choices.
    
    Hypothesis: Higher OCI scores correlate with a stronger reliance on habitual patterns 
    (Choice Kernel) over value-based calculations.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for Q-values and Choice Kernel.
    - beta: [0, 10] Inverse temperature for softmax.
    - w: [0, 1] Weighting parameter (0 = Pure MF, 1 = Pure MB).
    - ck_weight_oci: [0, 10] Scaling factor for how much OCI enhances the Choice Kernel's influence.
      (Total weight = ck_weight_oci * OCI)
    """
    learning_rate, beta, w, ck_weight_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Transition matrix for Model-Based (Fixed)
    # Action 0 -> 0.7 to State 0, 0.3 to State 1
    # Action 1 -> 0.3 to State 0, 0.7 to State 1
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_mf = np.zeros((2, 2)) # Stage 2 values: [State, Action]
    q_mb = np.zeros(2)      # Stage 1 MB values
    q_stage1_mf = np.zeros(2) # Stage 1 MF values
    
    # Choice Kernel (Action 0, Action 1)
    choice_kernel = np.zeros(2)
    
    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        
        # 1. Model-Based Value Calculation
        max_q_stage2 = np.max(q_mf, axis=1)
        q_mb = transition_matrix @ max_q_stage2
        
        # 2. Hybrid Value
        q_net = w * q_mb + (1 - w) * q_stage1_mf
        
        # 3. Add Choice Kernel Habit (Modulated by OCI)
        habit_bonus = (ck_weight_oci * oci_score) * choice_kernel
        q_net += habit_bonus
        
        # 4. Softmax
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # Update Choice Kernel (using same LR for simplicity)
        chosen_a1 = int(action_1[trial])
        choice_kernel[chosen_a1] += learning_rate * (1 - choice_kernel[chosen_a1])
        choice_kernel[1 - chosen_a1] += learning_rate * (0 - choice_kernel[1 - chosen_a1])
        
        # --- Stage 2 Choice ---
        state_idx = int(state[trial])
        
        q_s2 = q_mf[state_idx]
        exp_q2 = np.exp(beta * q_s2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Updates ---
        
        # Stage 1 MF Update (TD(0))
        delta_stage1 = q_mf[state_idx, int(action_2[trial])] - q_stage1_mf[chosen_a1]
        q_stage1_mf[chosen_a1] += learning_rate * delta_stage1
        
        # Stage 2 MF Update
        delta_stage2 = reward[trial] - q_mf[state_idx, int(action_2[trial])]
        q_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: Model-Free Q-Learning with OCI-Modulated Decay.
    
    This model introduces memory decay for unchosen actions. The rate of decay
    is determined by the participant's OCI score.
    
    Hypothesis: Participants with different OCI levels may have different memory 
    retention for values of options they didn't just choose. High OCI might imply 
    "tunnel vision" (high decay of unchosen) or rigid retention (low decay).
    
    Parameters:
    - learning_rate: [0, 1] Standard learning rate.
    - beta: [0, 10] Inverse temperature.
    - decay_oci_slope: [0, 5] Determines decay rate based on OCI.
      decay_rate = decay_oci_slope * OCI. (Clipped to [0,1]).
    """
    learning_rate, beta, decay_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Decay rate calculation
    decay_rate = decay_oci_slope * oci_score
    if decay_rate > 1.0: decay_rate = 1.0
    if decay_rate < 0.0: decay_rate = 0.0

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    # Stage 1: 2 actions
    q_stage1 = np.zeros(2)
    # Stage 2: 2 states * 2 actions
    q_stage2 = np.zeros((2, 2))
    
    for trial in range(n_trials):
        # --- Stage 1 ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        a1 = int(action_1[trial])
        curr_state = int(state[trial])
        
        # --- Stage 2 ---
        q_s2 = q_stage2[curr_state]
        exp_q2 = np.exp(beta * q_s2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        
        # --- Updates ---
        
        # Stage 1 Update (TD using Stage 2 Q-value as target)
        target_val = q_stage2[curr_state, a2]
        delta1 = target_val - q_stage1[a1]
        q_stage1[a1] += learning_rate * delta1
        
        # Decay unchosen Stage 1
        q_stage1[1-a1] *= (1.0 - decay_rate)
        
        # Stage 2 Update
        delta2 = reward[trial] - q_stage2[curr_state, a2]
        q_stage2[curr_state, a2] += learning_rate * delta2
        
        # Decay unchosen Stage 2 (in the current state)
        q_stage2[curr_state, 1-a2] *= (1.0 - decay_rate)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: Hybrid Model with OCI-Modulated Surprise Sensitivity.
    
    This model posits that OCI affects how participants learn from "Surprising" 
    (Rare) transitions vs Common transitions.
    
    Hypothesis: High OCI might lead to over-reaction or under-reaction to 
    rare transition events when updating the Stage 1 Model-Free values.
    
    Parameters:
    - learning_rate: [0, 1] Base learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] MB/MF mixing weight.
    - rare_mod_oci: [0, 5] Modifier for learning rate on Rare transitions.
      Effective LR (Rare) = learning_rate * (rare_mod_oci * OCI).
      If rare_mod_oci * OCI > 1, they learn faster from rare events.
    """
    learning_rate, beta, w, rare_mod_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_mf = np.zeros((2, 2)) # Stage 2
    q_stage1_mf = np.zeros(2) # Stage 1
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_mf, axis=1)
        q_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        a1 = int(action_1[trial])
        curr_state = int(state[trial])
        
        # --- Stage 2 Policy ---
        q_s2 = q_mf[curr_state]
        exp_q2 = np.exp(beta * q_s2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        
        # --- Updates ---
        
        # Determine if transition was Common or Rare
        # Common: (A0->S0), (A1->S1). Rare: (A0->S1), (A1->S0)
        is_common = (a1 == curr_state) 
        
        # Calculate Effective Learning Rate for Stage 1
        if is_common:
            eff_lr = learning_rate
        else:
            # Rare transition
            mod = rare_mod_oci * oci_score
            eff_lr = learning_rate * mod
            # Cap at 1.0 to prevent instability
            if eff_lr > 1.0: eff_lr = 1.0
            
        # Stage 1 MF Update
        delta_stage1 = q_mf[curr_state, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += eff_lr * delta_stage1
        
        # Stage 2 Update (Standard)
        delta_stage2 = reward[trial] - q_mf[curr_state, a2]
        q_mf[curr_state, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```