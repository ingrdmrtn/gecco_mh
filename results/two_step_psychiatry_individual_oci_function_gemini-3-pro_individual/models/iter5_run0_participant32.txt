def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Learner with OCI-Modulated Passive Decay (Forgetting).

    Hypothesis: OCI scores correlate with the inability to maintain value representations 
    for unchosen options over time. High OCI leads to faster decay (forgetting) 
    of action values that are not currently being reinforced, potentially driving 
    repetitive checking or instability in choice.

    Parameters:
    learning_rate: [0, 1] - MF learning rate.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - Weighting between MB (1) and MF (0).
    oci_decay_scale: [0, 1] - Scales the decay rate based on OCI. 
                              Decay rate = oci * oci_decay_scale.
                              Unchosen Q-values are multiplied by (1 - decay_rate).
    """
    learning_rate, beta, w, oci_decay_scale = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate decay rate specific to this participant
    decay_rate = current_oci * oci_decay_scale
    decay_rate = np.clip(decay_rate, 0.0, 1.0)
    decay_factor = 1.0 - decay_rate

    # Fixed transition matrix for MB step (standard task structure)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based Value Calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax Choice 1
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        chosen_a1 = action_1[trial]
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        # Stage 2 is purely Model-Free in this task structure
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        chosen_a2 = action_2[trial]
        r = reward[trial]

        # --- Updates ---
        
        # 1. Update Stage 2 MF (Standard Q-learning)
        pe_2 = r - q_stage2_mf[state_idx, chosen_a2]
        q_stage2_mf[state_idx, chosen_a2] += learning_rate * pe_2
        
        # 2. Update Stage 1 MF (TD-learning)
        # Using the value of the state actually reached
        v_stage2 = np.max(q_stage2_mf[state_idx])
        pe_1 = v_stage2 - q_stage1_mf[chosen_a1]
        q_stage1_mf[chosen_a1] += learning_rate * pe_1
        
        # 3. Decay Unchosen Stage 1 MF Value
        unchosen_a1 = 1 - chosen_a1
        q_stage1_mf[unchosen_a1] *= decay_factor

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Learner with OCI-Modulated Post-Error Anxiety (Beta Reduction).

    Hypothesis: High OCI participants experience higher anxiety or "panic" after 
    missing a reward (getting 0 coins), leading to a temporary reduction in 
    decision consistency (lower Beta/Exploration) on the subsequent trial. 
    They become "noisier" or more frantic after failure.

    Parameters:
    learning_rate: [0, 1] - Learning rate.
    beta_base: [0, 10] - Baseline inverse temperature (consistency).
    w: [0, 1] - Weighting between MB and MF.
    oci_panic_theta: [0, 5] - Scaling factor for beta reduction after loss.
                              Beta_effective = Beta_base / (1 + oci * theta) if prev_reward=0.
    """
    learning_rate, beta_base, w, oci_panic_theta = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_reward = 1.0 # Initialize as if rewarded to not trigger panic on trial 0

    for trial in range(n_trials):
        # Determine current beta based on previous outcome
        if prev_reward == 0.0:
            # Panic/Anxiety reduces beta (increases noise)
            denominator = 1.0 + (current_oci * oci_panic_theta)
            current_beta = beta_base / denominator
        else:
            current_beta = beta_base

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(current_beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        chosen_a1 = action_1[trial]
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(current_beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        chosen_a2 = action_2[trial]
        r = reward[trial]

        # --- Updates ---
        pe_2 = r - q_stage2_mf[state_idx, chosen_a2]
        q_stage2_mf[state_idx, chosen_a2] += learning_rate * pe_2
        
        v_stage2 = np.max(q_stage2_mf[state_idx])
        pe_1 = v_stage2 - q_stage1_mf[chosen_a1]
        q_stage1_mf[chosen_a1] += learning_rate * pe_1
        
        prev_reward = r

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Learner with OCI-Modulated Structural Rigidity.

    Hypothesis: High OCI is associated with "Black and White" thinking or 
    an over-reliance on rules. This model suggests OCI modulates the 
    subjective transition matrix used by the Model-Based system. 
    High OCI participants perceive the common transitions (70%) as being 
    more deterministic (closer to 100%) than they actually are, making 
    the MB system more rigid and biased towards common paths.

    Parameters:
    learning_rate: [0, 1] - Learning rate.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - Weighting between MB and MF.
    oci_rigidity: [0, 1] - Distortion factor for transition probabilities.
                           Subjective P(Common) = 0.7 + (0.3 * oci * rigidity).
    """
    learning_rate, beta, w, oci_rigidity = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Distort the transition matrix based on OCI
    # Base probability is 0.7. Max distortion pushes it to 1.0.
    # Logic: p_subj = 0.7 + (distance_to_1 * oci_factor)
    p_common_base = 0.7
    distortion = 0.3 * current_oci * oci_rigidity
    p_common_subj = np.clip(p_common_base + distortion, 0.0, 1.0)
    
    # Subjective Transition Matrix
    # Row 0: A -> [X, Y], Row 1: U -> [X, Y] (Assuming A->X is common)
    transition_matrix = np.array([
        [p_common_subj, 1.0 - p_common_subj], 
        [1.0 - p_common_subj, p_common_subj]
    ])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # MB Values calculated using the DISTORTED transition matrix
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        chosen_a1 = action_1[trial]
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        chosen_a2 = action_2[trial]
        r = reward[trial]

        # --- Updates ---
        pe_2 = r - q_stage2_mf[state_idx, chosen_a2]
        q_stage2_mf[state_idx, chosen_a2] += learning_rate * pe_2
        
        v_stage2 = np.max(q_stage2_mf[state_idx])
        pe_1 = v_stage2 - q_stage1_mf[chosen_a1]
        q_stage1_mf[chosen_a1] += learning_rate * pe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss