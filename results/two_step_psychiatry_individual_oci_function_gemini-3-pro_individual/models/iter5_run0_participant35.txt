Here are three new cognitive models that incorporate the OCI score into the decision-making process using different mechanisms.

### Model 1: OCI-Modulated Memory Decay
This model hypothesizes that OCI symptoms are related to the persistence of value representations. High OCI might be associated with "anxious forgetting" or a focus on immediate, recent history, leading to faster decay of unchosen options (forgetting the value of the path not taken).

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Memory Decay.

    Hypothesis:
    OCI scores modulate the decay rate of Q-values for unchosen options.
    High OCI leads to faster forgetting (higher decay) of values not recently experienced,
    potentially due to cognitive load or anxiety-driven focus on the immediate present.

    Parameters:
    - learning_rate: [0, 1] Standard learning rate for chosen options.
    - beta: [0, 10] Inverse temperature for softmax.
    - w: [0, 1] Weighting between Model-Based (1) and Model-Free (0).
    - decay_base: [0, 1] Baseline decay rate for unchosen options.
    - oci_decay_sens: [0, 1] Sensitivity of decay rate to OCI score.

    Mechanism:
    - decay = clip(decay_base + oci_decay_sens * oci_score, 0, 1)
    - Chosen Q-values update normally.
    - Unchosen Q-values decay towards 0 (or neutral) by factor (1 - decay).
    """
    learning_rate, beta, w, decay_base, oci_decay_sens = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate decay rate specific to this participant
    # We allow OCI to increase decay.
    decay_rate = np.clip(decay_base + (oci_decay_sens * oci_score), 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # Policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # Policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # Action Value Updating
        # Standard TD updates for chosen options
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Eligibility trace for second stage reward to first stage choice
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage2

        # Decay mechanism for UNCHOSEN options
        # Decay stage 1 unchosen
        unchosen_a1 = 1 - action_1[trial]
        q_stage1_mf[unchosen_a1] *= (1.0 - decay_rate)
        
        # Decay stage 2 unchosen (only for the state visited)
        unchosen_a2 = 1 - action_2[trial]
        q_stage2_mf[state_idx, unchosen_a2] *= (1.0 - decay_rate)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Eligibility Trace (Habit Strength)
This model tests if OCI scores influence the strength of "eligibility traces" ($\lambda$). A high $\lambda$ means the second-stage outcome directly reinforces the first-stage choice, bypassing the model-based structure. This corresponds to stronger habit formation.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Modulated Eligibility Trace.

    Hypothesis:
    OCI scores modulate the eligibility trace parameter (lambda).
    High OCI (compulsivity) is hypothesized to lead to stronger habit formation,
    represented by a higher lambda (closer to TD(1)), connecting outcomes directly 
    to past actions regardless of state structure.

    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] MB/MF weight.
    - lambda_base: [0, 1] Base eligibility trace value.
    - oci_lambda_mod: [0, 1] Modulation of lambda by OCI.

    Mechanism:
    - lambda_val = clip(lambda_base + oci_lambda_mod * oci_score, 0, 1)
    - If lambda is high, the Stage 2 prediction error strongly updates Stage 1 values.
    """
    learning_rate, beta, w, lambda_base, oci_lambda_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate lambda based on OCI
    # We assume OCI increases the eligibility trace (more habit-like/direct reinforcement)
    lambda_val = np.clip(lambda_base + (oci_lambda_mod * oci_score), 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # Policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # Policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # Action Value Updating
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Eligibility trace update: 
        # The Stage 2 RPE (delta_stage2) updates Stage 1 value scaled by lambda
        q_stage1_mf[action_1[trial]] += learning_rate * lambda_val * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Structural Confidence
This model proposes that high OCI leads to a distrust of the environmental structure (the transition probabilities). While the "true" matrix is 0.7/0.3, high OCI participants might act as if the transitions are more random (closer to 0.5/0.5), effectively dampening the Model-Based system's effectiveness.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Structural Confidence.

    Hypothesis:
    OCI scores modulate the participant's confidence in the transition matrix structure.
    High OCI (anxiety/uncertainty) reduces confidence, flattening the perceived 
    transition probabilities towards 0.5 (randomness), thereby degrading the 
    quality of Model-Based planning.

    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] MB/MF weight.
    - conf_base: [0, 1] Base confidence in the 0.7/0.3 structure.
    - oci_conf_decay: [0, 1] Reduction in confidence due to OCI.

    Mechanism:
    - effective_conf = clip(conf_base - oci_conf_decay * oci_score, 0, 1)
    - prob_transition = 0.5 + 0.2 * effective_conf
    - If effective_conf is 1, prob is 0.7 (True structure).
    - If effective_conf is 0, prob is 0.5 (No structure).
    """
    learning_rate, beta, w, conf_base, oci_conf_decay = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective confidence in the transition structure
    # High OCI reduces confidence (subtracts from base)
    effective_conf = np.clip(conf_base - (oci_conf_decay * oci_score), 0.0, 1.0)
    
    # Map confidence to probability: 0 -> 0.5, 1 -> 0.7
    p_trans = 0.5 + (0.2 * effective_conf)
    
    # Construct the subjective transition matrix used for MB planning
    # Note: We assume symmetry (P(X|A) = P(Y|U) = p_trans)
    subjective_transition_matrix = np.array([[p_trans, 1.0 - p_trans], 
                                             [1.0 - p_trans, p_trans]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # Policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        
        # Use the SUBJECTIVE matrix for planning
        q_stage1_mb = subjective_transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # Policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # Action Value Updating
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Standard hybrid update (lambda=1 implicitly in standard MB/MF hybrid usually, 
        # but here we stick to the template's minimal MF update plus the MB calculation above)
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```