Here are 3 new cognitive models that incorporate the OCI score into the decision-making process in the two-step task.

### Model 1: OCI-Modulated Model-Based Control
This model hypothesizes that high OCI scores are associated with a reliance on habit formation (Model-Free) over goal-directed planning (Model-Based). The weighting parameter `w`, which balances Model-Based vs. Model-Free control, is directly modulated by the OCI score. Specifically, higher OCI reduces `w`, pushing the participant towards Model-Free behavior.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Model-Based Control.
    
    Hypothesis: High OCI is associated with reduced Model-Based (goal-directed) control
    and increased reliance on Model-Free (habitual) control.
    The mixing weight 'w' is modeled as a base value minus an OCI-dependent penalty.
    
    Parameters:
    learning_rate: [0, 1] Learning rate for value updates.
    beta: [0, 10] Inverse temperature for softmax choice.
    w_base: [0, 1] Baseline weight for Model-Based control (before OCI adjustment).
    oci_mb_penalty: [0, 1] How strongly OCI reduces the Model-Based weight.
    
    Bounds:
    learning_rate: [0,1]
    beta: [0,10]
    w_base: [0,1]
    oci_mb_penalty: [0,1]
    """
    learning_rate, beta, w_base, oci_mb_penalty = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Calculate effective w. Clip to ensure it stays in [0, 1].
    # Higher OCI -> Lower w (less model-based)
    w_eff = w_base - (oci_mb_penalty * current_oci)
    w_eff = np.clip(w_eff, 0.0, 1.0)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combine MF and MB values using the OCI-adjusted weight
        q_net = w_eff * q_stage1_mb + (1 - w_eff) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # Action value updating
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] = q_stage1_mf[a1] + learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] = q_stage2_mf[s_idx, a2] + learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Driven Asymmetric Learning Rates
This model suggests that OCI is linked to an asymmetry in how positive versus negative outcomes are processed. Specifically, this model tests if OCI scales the learning rate specifically for *negative* prediction errors (disappointments), reflecting a potential hyper-sensitivity to failure or error correction often seen in compulsive phenotypes.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Driven Asymmetric Learning Rates.
    
    Hypothesis: OCI affects how strongly participants update values after negative outcomes.
    The learning rate is split into a base rate and a specific boost/penalty for negative
    prediction errors, scaled by OCI.
    
    Parameters:
    lr_base: [0, 1] Base learning rate for positive prediction errors.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Weight for Model-Based control.
    oci_neg_lr_scale: [0, 2] Multiplier for negative PE learning rate based on OCI.
    
    Bounds:
    lr_base: [0,1]
    beta: [0,10]
    w: [0,1]
    oci_neg_lr_scale: [0,2]
    """
    lr_base, beta, w, oci_neg_lr_scale = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    # Calculate learning rate for negative prediction errors
    # If oci_neg_lr_scale > 1, high OCI learns faster from errors.
    lr_neg = lr_base * (1.0 + oci_neg_lr_scale * current_oci)
    lr_neg = np.clip(lr_neg, 0.0, 1.0)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # Update Stage 1
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        alpha_1 = lr_base if delta_stage1 >= 0 else lr_neg
        q_stage1_mf[a1] = q_stage1_mf[a1] + alpha_1 * delta_stage1
        
        # Update Stage 2
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        alpha_2 = lr_base if delta_stage2 >= 0 else lr_neg
        q_stage2_mf[s_idx, a2] = q_stage2_mf[s_idx, a2] + alpha_2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Scaled Exploration (Inverse Temperature)
This model posits that OCI affects the exploration-exploitation trade-off. Specifically, high OCI might lead to more deterministic (rigid) choice behavior, represented by a higher inverse temperature (`beta`). The model defines `beta` as a baseline value plus an OCI-dependent increase.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Scaled Exploration (Inverse Temperature).
    
    Hypothesis: OCI correlates with choice rigidity (lower exploration).
    The inverse temperature parameter (beta) is scaled up by the OCI score,
    making choices more deterministic (steeper softmax) as OCI increases.
    
    Parameters:
    learning_rate: [0, 1] Learning rate.
    beta_base: [0, 10] Baseline inverse temperature (randomness).
    w: [0, 1] Weight for Model-Based control.
    oci_beta_stiffness: [0, 5] How much OCI increases beta (reducing randomness).
    
    Bounds:
    learning_rate: [0,1]
    beta_base: [0,10]
    w: [0,1]
    oci_beta_stiffness: [0,5]
    """
    learning_rate, beta_base, w, oci_beta_stiffness = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Calculate effective beta
    # Higher OCI -> Higher beta -> More deterministic/rigid
    beta_eff = beta_base + (oci_beta_stiffness * current_oci)
    # Cap beta to prevent numerical overflow in exp()
    beta_eff = np.minimum(beta_eff, 20.0) 

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_eff * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Policy for the second choice
        exp_q2 = np.exp(beta_eff * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # Action value updating
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] = q_stage1_mf[a1] + learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] = q_stage2_mf[s_idx, a2] + learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```