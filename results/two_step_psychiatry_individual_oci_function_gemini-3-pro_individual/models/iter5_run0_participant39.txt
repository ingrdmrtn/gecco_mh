Here are the 3 new cognitive models as Python functions.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Off-Policy Mixture (SARSA vs. Q-Learning).
    
    Hypothesis: OCI affects the optimism/conservatism of the Stage 1 update target.
    Standard SARSA uses the value of the action actually taken in Stage 2 (conservative/realistic).
    Q-Learning uses the max value of Stage 2 (optimistic/greedy).
    High OCI might drive a shift towards one strategy (e.g., more conservative/SARSA-like due to risk aversion, 
    or more Q-learning like if trying to maximize theoretical control).
    
    Parameters:
    lr: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Model-based / Model-free mixing weight.
    mix_base: [0, 1] Baseline mixture (0 = Pure SARSA, 1 = Pure Q-Learning).
    mix_oci_slope: [-1, 1] Effect of OCI on the mixture.
    """
    lr, beta, w, mix_base, mix_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate mixture parameter and bound it [0, 1]
    mix_param = mix_base + mix_oci_slope * oci_score
    mix_param = np.clip(mix_param, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_s1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_s1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        a1 = int(action_1[trial])
        s_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Updates ---
        
        # Calculate Stage 1 Target: Mixture of Q-Learning (Max) and SARSA (Actual)
        val_sarsa = q_stage2_mf[s_idx, a2]
        val_qlearn = np.max(q_stage2_mf[s_idx])
        
        target_stage1 = mix_param * val_qlearn + (1 - mix_param) * val_sarsa
        
        delta_stage1 = target_stage1 - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Modulated Novelty Bonus (Exploration).
    
    Hypothesis: OCI scores relate to how participants treat unchosen options (novelty).
    High OCI might correlate with neophobia (avoiding the unknown, negative bonus) 
    or excessive checking (seeking the unknown, positive bonus).
    This adds a bias to the Q-values based on how long ago an option was last chosen.
    
    Parameters:
    lr: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Model-based / Model-free mixing weight.
    nov_base: [-0.5, 0.5] Baseline novelty bonus per trial unchosen.
    nov_oci_slope: [-1, 1] Effect of OCI on novelty bonus.
    """
    lr, beta, w, nov_base, nov_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate novelty scaler
    novelty_scaler = nov_base + nov_oci_slope * oci_score
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track time since last selection for Stage 1 choices
    time_since_chosen = np.zeros(2) 

    for trial in range(n_trials):
        
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_s1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add Novelty Bonus
        # We use log(time + 1) to prevent the bonus from exploding in long tasks
        bonus = novelty_scaler * np.log(time_since_chosen + 1.0)
        q_net_s1_biased = q_net_s1 + bonus
        
        exp_q1 = np.exp(beta * q_net_s1_biased)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        a1 = int(action_1[trial])
        
        # Update novelty trackers
        time_since_chosen += 1
        time_since_chosen[a1] = 0 # Reset chosen option
        
        s_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Subjective Punishment Sensitivity.
    
    Hypothesis: OCI relates to the subjective magnitude of failure (omission of reward).
    When reward is 0, it is not processed as '0' but as a negative value (punishment).
    High OCI individuals may perceive the lack of gold as a more severe loss.
    
    Parameters:
    lr: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Model-based / Model-free mixing weight.
    loss_base: [0, 5] Baseline magnitude of perceived loss when Reward=0.
    loss_oci_slope: [0, 5] Additional loss magnitude per unit of OCI.
    """
    lr, beta, w, loss_base, loss_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate subjective loss magnitude
    # If slope is positive, higher OCI = more negative reaction to 0 reward
    loss_magnitude = loss_base + loss_oci_slope * oci_score
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_s1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_s1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        a1 = int(action_1[trial])
        s_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Calculate Effective Reward (Subjective)
        # If r=1, effective_r = 1. If r=0, effective_r = -loss_magnitude
        r_effective = r if r > 0.5 else -loss_magnitude

        # --- Updates ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        delta_stage2 = r_effective - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```