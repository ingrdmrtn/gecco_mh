Here are three new cognitive models implemented as Python functions, designed to explain participant behavior in the two-step task with modulation by OCI-R scores.

### Cognitive Model 1: Structural Distrust Model
This model tests the hypothesis that high OCI scores correlate with a reduced confidence in the "known" transition structure of the world. While a standard Model-Based learner assumes the transition matrix is fixed (e.g., 0.7/0.3), this model posits that high OCI participants perceive the transitions as more uncertain (closer to 0.5/0.5), effectively dampening the Model-Based contribution to value estimation.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Structural Distrust Model.
    
    Hypothesis: OCI modulates the perceived reliability of the transition structure.
    High OCI leads to a 'flattening' of the transition matrix (distrust in the 
    rule that A->X commonly), pushing perceived probabilities towards chance (0.5).
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for Model-Free Q-values.
    - beta: [0, 10] Inverse temperature (choice consistency).
    - w: [0, 1] Weight for Model-Based values (0=MF, 1=MB).
    - distrust_base: [0, 1] Base level of transition distrust.
    - distrust_oci: [0, 1] Additional distrust per unit of OCI.
      (Total distrust reduces the diagonal of the transition matrix from 0.7).
    """
    learning_rate, beta, w, distrust_base, distrust_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate perceived transition probability
    # Standard is 0.7. Distrust reduces this towards 0.5.
    # We clip the reduction so the probability doesn't drop below 0.5.
    raw_distrust = distrust_base + (distrust_oci * oci_score)
    # Ensure distrust doesn't flip the matrix (max reduction of 0.2 makes it 0.5)
    effective_distrust = np.clip(raw_distrust, 0.0, 0.2)
    
    p_common = 0.7 - effective_distrust
    p_rare = 1.0 - p_common
    
    # Modified transition matrix based on OCI
    transition_matrix = np.array([[p_common, p_rare], [p_rare, p_common]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # -- Stage 1 Choice --
        # Model-Based Value calculation using OCI-distorted matrix
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Policy
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = int(state[trial])

        # -- Stage 2 Choice --
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # -- Updates --
        # 1. Update Stage 2 MF values based on Reward
        # Prediction error for second stage
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # 2. Update Stage 1 MF values (SARSA-like / TD(0))
        # Note: We use the value of the state we actually landed in
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2: Stage 1 Compulsivity Model
This model investigates whether OCI specifically affects the decision noise (randomness vs. rigidity) at the planning stage (Stage 1), independent of the harvesting stage (Stage 2). It proposes that high OCI participants may be more "compulsive" (higher beta, more deterministic) or "anxious" (lower beta, more random) specifically when choosing between Spaceships, represented by modulating `beta_stage1`.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Stage 1 Compulsivity Model.
    
    Hypothesis: OCI modulates the Inverse Temperature (Beta) specifically for 
    the first-stage choice. This separates 'planning rigidity' from 'harvesting 
    consistency'. 
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta_stage2: [0, 10] Inverse temperature for Stage 2 (Planet -> Alien).
    - w: [0, 1] Mixing weight (MB vs MF).
    - beta1_base: [0, 10] Base inverse temperature for Stage 1.
    - beta1_oci_slope: [-5, 5] Modulation of Stage 1 beta by OCI.
      (Resulting beta1 is clipped to be non-negative).
    """
    learning_rate, beta_stage2, w, beta1_base, beta1_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate Stage 1 Beta dependent on OCI
    # We allow slope to be negative (anxiety/confusion) or positive (rigidity)
    beta_stage1 = beta1_base + (beta1_oci_slope * oci_score)
    beta_stage1 = np.maximum(0.0, beta_stage1) # Ensure non-negative

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # -- Stage 1 Choice --
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Use OCI-modulated Beta for Stage 1
        exp_q1 = np.exp(beta_stage1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = int(state[trial])

        # -- Stage 2 Choice --
        # Use fixed Beta for Stage 2
        exp_q2 = np.exp(beta_stage2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # -- Updates --
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3: Surprise Learning Sensitivity Model
This model tests if OCI scores influence how participants update their beliefs following "Rare" transitions (e.g., choosing Spaceship A but landing on Planet Y). It allows for a separate learning rate for these surprising trials, modulated by OCI, reflecting an altered sensitivity to prediction errors arising from structural uncertainty.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Surprise Learning Sensitivity Model.
    
    Hypothesis: OCI modulates the learning rate specifically on trials where 
    a 'Rare' transition occurred. High OCI participants may over-weigh (or ignore)
    outcomes from rare transitions when updating their Stage 1 values.
    
    Parameters:
    - lr_common: [0, 1] Learning rate for Common transition trials.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight.
    - lr_rare_base: [0, 1] Base learning rate for Rare transition trials.
    - lr_rare_oci: [-1, 1] Modulation of Rare learning rate by OCI.
    """
    lr_common, beta, w, lr_rare_base, lr_rare_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective learning rate for rare trials
    lr_rare_eff = lr_rare_base + (lr_rare_oci * oci_score)
    lr_rare_eff = np.clip(lr_rare_eff, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # -- Stage 1 Choice --
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = int(state[trial])
        chosen_action_1 = int(action_1[trial])

        # Determine if this was a Rare or Common transition
        # Action 0 (A) -> State 0 (X) is Common
        # Action 1 (U) -> State 1 (Y) is Common
        is_common = (chosen_action_1 == state_idx)
        
        # Select Learning Rate for this trial
        current_lr = lr_common if is_common else lr_rare_eff

        # -- Stage 2 Choice --
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # -- Updates --
        # Note: We apply the selected learning rate to BOTH updates in this trial
        # as the 'surprise' context colors the whole learning event.
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += current_lr * delta_stage2
        
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[chosen_action_1]
        q_stage1_mf[chosen_action_1] += current_lr * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```