Here are three new cognitive models based on the two-step task, incorporating OCI scores to explain individual variability in decision-making.

### Model 1: Hybrid Model with OCI-Modulated Initial Beliefs (Priors)
This model hypothesizes that OCI levels influence a participant's initial expectations of the environment. High OCI might correlate with "checking" behavior (high initial priors driving exploration) or pessimism (low initial priors). Instead of starting with neutral values (e.g., 0 or 0.5), the Q-values are initialized based on the OCI score.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model where OCI modulates the initialization of Q-values (Priors).
    
    This model captures whether OCI symptoms lead to optimistic (high priors) or 
    pessimistic (low priors) starting assumptions, which significantly affects 
    early exploration and policy formation in the task.

    Parameters:
    - learning_rate: [0, 1] Rate at which Q-values are updated.
    - beta: [0, 10] Inverse temperature for softmax (choice consistency).
    - w: [0, 1] Weighting parameter (0 = pure MF, 1 = pure MB).
    - q_init_base: [0, 1] Baseline initial Q-value.
    - q_init_oci_mod: [0, 1] How much OCI scales the initial Q-value (additive).
    """
    learning_rate, beta, w, q_init_base, q_init_oci_mod = model_parameters
    n_trials = len(action_1)
    oci_val = oci[0]
    
    # Initialize Q-values with OCI modulation
    # We clip to ensure priors stay within logical bounds of the reward scale [0, 1]
    initial_q = np.clip(q_init_base + (oci_val * q_init_oci_mod), 0.0, 1.0)
    
    q_stage1_mf = np.full(2, initial_q)
    q_stage2_mf = np.full((2, 2), initial_q)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based Value Calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value Calculation
        q_net_stage1 = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Softmax Policy Stage 1
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        s1_choice = action_1[trial]
        s2_state = state[trial] # 0 or 1 (Planet X or Y)

        # --- Stage 2 Choice ---
        # Only Model-Free values exist for Stage 2
        exp_q2 = np.exp(beta * q_stage2_mf[s2_state])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        s2_choice = action_2[trial]
        r = reward[trial]

        # --- Updates ---
        # Stage 1 Update (TD-0 / SARSA style)
        delta_stage1 = q_stage2_mf[s2_state, s2_choice] - q_stage1_mf[s1_choice]
        q_stage1_mf[s1_choice] += learning_rate * delta_stage1
        
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[s2_state, s2_choice]
        q_stage2_mf[s2_state, s2_choice] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model with OCI-Modulated Spatial Generalization
This model hypothesizes that OCI relates to a "spread of effect" or contamination fear. When a participant learns about an alien on one planet, they may generalize that learning to the equivalent alien on the *other* planet. OCI modulates the strength of this generalization ($\kappa$).

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model with OCI-modulated value generalization (contamination).
    
    When updating the value of an alien on the current planet, this model 
    also updates the value of the equivalent alien on the OTHER planet 
    by a factor 'kappa', which is determined by the OCI score.
    
    Parameters:
    - learning_rate: [0, 1] Primary learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weight for MB vs MF.
    - kappa_base: [0, 1] Base generalization rate.
    - kappa_oci_mod: [0, 1] OCI modulation of generalization magnitude.
    """
    learning_rate, beta, w, kappa_base, kappa_oci_mod = model_parameters
    n_trials = len(action_1)
    oci_val = oci[0]
    
    # Calculate Generalization Parameter kappa
    kappa = np.clip(kappa_base + (oci_val * kappa_oci_mod), 0.0, 1.0)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_stage1 = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        s1_choice = action_1[trial]
        s2_state = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[s2_state])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        s2_choice = action_2[trial]
        r = reward[trial]

        # --- Updates ---
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[s2_state, s2_choice] - q_stage1_mf[s1_choice]
        q_stage1_mf[s1_choice] += learning_rate * delta_stage1
        
        # Stage 2 Update (Direct)
        delta_stage2 = r - q_stage2_mf[s2_state, s2_choice]
        q_stage2_mf[s2_state, s2_choice] += learning_rate * delta_stage2
        
        # Stage 2 Update (Generalization to the other planet)
        # If we are at state 0, generalized state is 1, and vice versa.
        other_state = 1 - s2_state
        # We assume spatial correspondence (Action 0 on Planet A ~ Action 0 on Planet B)
        q_stage2_mf[other_state, s2_choice] += learning_rate * kappa * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model with OCI-Modulated Lapse Rate
This model hypothesizes that OCI symptoms manifest as "lapses" in goal-directed attention, or random "checking" behaviors that appear as noise. Instead of OCI affecting value learning, it affects the noise floor ($\epsilon$) in the decision policy, mixing the softmax distribution with a uniform distribution.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model with OCI-modulated Lapse Rate (Epsilon-Greedy mixture).
    
    This model assumes that OCI affects the probability of making a random 
    'lapse' or 'checking' error, independent of value. The final choice probability 
    is a mixture of the softmax policy and a uniform random guess, weighted by epsilon.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature for the non-random component.
    - w: [0, 1] Weight for MB vs MF.
    - epsilon_base: [0, 1] Base lapse rate.
    - epsilon_oci_mod: [0, 1] How much OCI increases the lapse rate.
    """
    learning_rate, beta, w, epsilon_base, epsilon_oci_mod = model_parameters
    n_trials = len(action_1)
    oci_val = oci[0]
    
    # Calculate Epsilon (Lapse Rate)
    epsilon = np.clip(epsilon_base + (oci_val * epsilon_oci_mod), 0.0, 1.0)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_stage1 = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Mixture Policy: (1-eps)*Softmax + eps*Random
        exp_q1 = np.exp(beta * q_net_stage1)
        softmax_probs_1 = exp_q1 / np.sum(exp_q1)
        final_probs_1 = (softmax_probs_1 * (1 - epsilon)) + (epsilon / 2.0)
        
        p_choice_1[trial] = final_probs_1[action_1[trial]]
        
        s1_choice = action_1[trial]
        s2_state = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[s2_state])
        softmax_probs_2 = exp_q2 / np.sum(exp_q2)
        final_probs_2 = (softmax_probs_2 * (1 - epsilon)) + (epsilon / 2.0)
        
        p_choice_2[trial] = final_probs_2[action_2[trial]]
        
        s2_choice = action_2[trial]
        r = reward[trial]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[s2_state, s2_choice] - q_stage1_mf[s1_choice]
        q_stage1_mf[s1_choice] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s2_state, s2_choice]
        q_stage2_mf[s2_state, s2_choice] += learning_rate * delta_stage2

    eps_safe = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps_safe)) + np.sum(np.log(p_choice_2 + eps_safe)))
    return log_loss
```