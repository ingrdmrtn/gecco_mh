Here are 3 new cognitive models that incorporate OCI scores into the decision-making process using mechanisms distinct from those previously tested.

### Model 1: Cognitive Model with OCI-Modulated Reward Sensitivity
This model hypothesizes that OCI traits alter the subjective valuation of the reward outcome. Rather than changing how quickly the participant learns (learning rate) or how they arbitrate between systems (w), this model suggests OCI affects the magnitude of the reward signal itself (e.g., hyper-sensitivity to success or anhedonia).

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model where OCI modulates Reward Sensitivity.
    
    Hypothesis: OCI score scales the effective reward perceived by the agent. 
    High OCI might lead to hypersensitivity (larger prediction errors for same reward) 
    or blunting.
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    w: [0, 1] (Weighting between MB and MF)
    r_sens_base: [0, 2] (Base reward sensitivity)
    r_sens_oci_slope: [-2, 2] (How OCI changes reward sensitivity)
    """
    learning_rate, beta, w, r_sens_base, r_sens_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    # Calculate effective reward scalar based on OCI
    # We clip to ensure reward doesn't invert negatively unless intended, 
    # though slope allows for reduction.
    eff_r_sens = r_sens_base + (r_sens_oci_slope * oci_score)
    if eff_r_sens < 0: eff_r_sens = 0

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        # Model-Based Value calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Value
        q_net_stage1 = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        # --- Updating ---
        # Modify the reward by the sensitivity parameter
        perceived_reward = reward[trial] * eff_r_sens

        # Update Stage 1 MF
        # TD(0) update for stage 1 based on stage 2 value
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1
        
        # Update Stage 2 MF
        # TD update using the perceived (scaled) reward
        delta_stage2 = perceived_reward - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Cognitive Model with OCI-Driven Learning Suppression
This model hypothesizes that OCI is linked to cognitive rigidity or anxiety-induced interference, resulting in a global suppression of learning. Unlike linear slope models, this uses an inverse function, modeling a "resistance" to updating beliefs that scales with OCI.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model with OCI-Driven Learning Suppression (Inverse Scaling).
    
    Hypothesis: Higher OCI scores increase 'resistance' to learning, 
    scaling the learning rate down non-linearly.
    Formula: Effective_LR = LR_Max / (1 + (suppression * OCI))
    
    Bounds:
    lr_max: [0, 1] (Maximum possible learning rate when OCI is 0)
    beta: [0, 10]
    w: [0, 1]
    suppression_oci: [0, 10] (Strength of OCI interference)
    """
    lr_max, beta, w, suppression_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    # Calculate effective learning rate
    # As OCI increases, the denominator grows, suppressing the learning rate.
    eff_learning_rate = lr_max / (1.0 + (suppression_oci * oci_score))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        # --- Updating ---
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += eff_learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += eff_learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Cognitive Model with OCI-Modulated Spatial Bias
This model differentiates between *dynamic* choice stickiness (repeating the last action) and *static* spatial bias (a fixed preference for one option, e.g., "always check left"). It hypothesizes that OCI manifests as a fixed compulsion or bias towards specific stimuli (Spaceship A vs U) regardless of their changing values.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model with OCI-Modulated Static Spatial Bias.
    
    Hypothesis: OCI participants exhibit a fixed bias towards one of the 
    Stage 1 options (e.g., Spaceship A) that is independent of reward history.
    This represents a 'compulsion' or fixed routine.
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    w: [0, 1]
    bias_base: [-2, 2] (General population bias towards option 0)
    bias_oci_slope: [-2, 2] (How OCI exacerbates or flips this bias)
    """
    learning_rate, beta, w, bias_base, bias_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    # Calculate static bias added to Option 0 (Spaceship A)
    # Positive value = bias towards 0, Negative = bias towards 1
    static_bias = bias_base + (bias_oci_slope * oci_score)

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Apply the static bias to the first option (index 0)
        # We add it to the Q-value before softmax
        q_net_stage1[0] += static_bias

        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        # --- Updating ---
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```