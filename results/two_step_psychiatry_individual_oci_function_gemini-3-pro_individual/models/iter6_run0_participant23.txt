Here are three new cognitive models that explain individual variability in the two-step task using OCI scores.

### Model 1: Hybrid Learner with OCI-Modulated Forgetting
This model tests the hypothesis that OCI scores relate to the persistence of value representations (memory). High OCI might be associated with "stickier" memories (low forgetting) or, conversely, a need for constant refreshment due to high decay of unvisited states.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free learner with OCI-modulated memory decay (forgetting).
    
    Unchosen Q-values decay towards 0 on each trial. The rate of decay is modulated by OCI.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for value updates (both stages).
    - beta: [0, 10] Inverse temperature (exploration/exploitation).
    - w: [0, 1] Weighting parameter (0=Pure MF, 1=Pure MB).
    - decay_base: [0, 1] Base decay rate for unchosen options.
    - decay_oci_slope: [-1, 1] Modulation of decay rate by OCI.
    """
    learning_rate, beta, w, decay_base, decay_oci_slope = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate OCI-modulated decay rate
    decay = decay_base + (current_oci * decay_oci_slope)
    decay = np.clip(decay, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # 2 states (planets), 2 actions (aliens)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        # Model-Based Value
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # --- Updates ---
        # Stage 2 Update (MF)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Forgetting for unchosen Stage 2 action
        unchosen_a2 = 1 - a2
        q_stage2_mf[s_idx, unchosen_a2] *= (1.0 - decay)

        # Stage 1 Update (MF)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Forgetting for unchosen Stage 1 action
        unchosen_a1 = 1 - a1
        q_stage1_mf[unchosen_a1] *= (1.0 - decay)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Learner with OCI-Modulated Subjective Belief
This model tests the hypothesis that OCI affects the *perceived* stability of the environment. While the true transition probability is 0.7, high OCI participants might act as if the world is more chaotic (lower belief probability) or more rigid (higher belief probability), affecting their Model-Based planning.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid learner where OCI modulates the subjective belief of transition probabilities.
    
    The transition matrix used for Model-Based planning is derived from OCI, rather than being fixed at 0.7.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for MF values.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weighting parameter (0=Pure MF, 1=Pure MB).
    - belief_base: [0, 1] Base subjective probability of the 'common' transition.
    - belief_oci_slope: [-1, 1] Modulation of belief by OCI.
    """
    learning_rate, beta, w, belief_base, belief_oci_slope = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    # Calculate Subjective Transition Probability
    p_common = belief_base + (current_oci * belief_oci_slope)
    p_common = np.clip(p_common, 0.0, 1.0)
    
    # Subjective Transition Matrix
    # We assume symmetry: P(X|A) = P(Y|U) = p_common
    transition_matrix = np.array([[p_common, 1-p_common], [1-p_common, p_common]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        # Model-Based Value using Subjective Matrix
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # --- Updates ---
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Learner with OCI-Modulated Surprise Sensitivity
This model tests if OCI modulates how participants learn from "Rare" vs "Common" transitions. It posits that OCI might lead to hyper-learning (or under-learning) specifically after surprising (Rare) state transitions, affecting the Model-Free update at Stage 1.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid learner with transition-dependent learning rates modulated by OCI.
    
    The learning rate for the Stage 1 Model-Free update depends on whether the 
    transition was Common or Rare. OCI specifically modulates the 'Rare' learning rate.
    
    Parameters:
    - lr_common: [0, 1] Learning rate for Stage 2 and Common Stage 1 updates.
    - lr_rare_base: [0, 1] Base learning rate for Rare Stage 1 updates.
    - lr_rare_oci_slope: [-1, 1] Modulation of Rare LR by OCI.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weighting parameter (0=Pure MF, 1=Pure MB).
    """
    lr_common, lr_rare_base, lr_rare_oci_slope, beta, w = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    # Calculate Rare Transition Learning Rate
    lr_rare = lr_rare_base + (current_oci * lr_rare_oci_slope)
    lr_rare = np.clip(lr_rare, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # --- Updates ---
        # Stage 2 uses the standard (common) learning rate
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr_common * delta_stage2

        # Stage 1 Update depends on transition type
        # Common: A(0)->X(0) or U(1)->Y(1). Rare: A(0)->Y(1) or U(1)->X(0).
        is_common = (a1 == s_idx) 
        
        current_lr = lr_common if is_common else lr_rare
        
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += current_lr * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```