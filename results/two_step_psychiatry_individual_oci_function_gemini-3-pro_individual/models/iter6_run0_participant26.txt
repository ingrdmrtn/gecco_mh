Here are three new cognitive models that incorporate the OCI score to explain individual variability in the two-step task.

### Model 1: OCI-Modulated Win-Stay-Lose-Shift (WSLS)
This model hypothesizes that OCI symptoms modulate a heuristic "Win-Stay" bias. While the standard model integrates reward history over time (Reinforcement Learning), this model adds an immediate bonus to the previously chosen spaceship *if and only if* it resulted in a reward on the last trial. High OCI might correlate with a stronger compulsion to repeat immediately rewarded actions (or a rigidity in strategy). This is distinct from "stickiness," which is a tendency to repeat regardless of the outcome.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Win-Stay-Lose-Shift (WSLS).
    
    This model combines standard MB/MF reinforcement learning with a heuristic 
    WSLS bias. If the previous trial was rewarded, a bonus is added to the 
    logit of repeating that Stage 1 choice. The magnitude of this bonus 
    is modulated by the OCI score.
    
    Parameters:
    - learning_rate: [0, 1] Update rate for Q-values.
    - beta: [0, 10] Inverse temperature (softmax slope).
    - w: [0, 1] Weighting between Model-Based (1) and Model-Free (0).
    - wsls_base: [0, 5] Baseline Win-Stay bonus.
    - oci_wsls_mod: [0, 5] Modulation of Win-Stay bonus by OCI score.
    """
    learning_rate, beta, w, wsls_base, oci_wsls_mod = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action

    # Win-Stay Bonus Calculation
    wsls_bonus = wsls_base + (current_oci * oci_wsls_mod)

    last_choice_1 = -1
    last_reward = 0

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        # Model-Based Value: Transition * Max(Stage 2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Value (Hybrid)
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Calculate Logits
        logits = beta * q_net
        
        # Apply Win-Stay Bonus
        if last_choice_1 != -1 and last_reward == 1.0:
            logits[last_choice_1] += wsls_bonus
            
        exp_q1 = np.exp(logits)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # Record choice for next trial
        last_choice_1 = int(action_1[trial])
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # --- Updates ---
        # Prediction errors
        # Stage 2 PE
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        
        # Stage 1 PE (SARSA-style for MF)
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        
        # Update Values
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2
        
        # Record reward for next trial's WSLS logic
        last_reward = reward[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Epsilon-Greedy "Lapse"
This model proposes that OCI modulates the rate of "random" responding (lapses) in the first stage. While `beta` controls the slope of the value-probability curve (consistency), `epsilon` sets a floor on randomness. High OCI participants might engage in "checking" behaviors or experience attentional lapses that look like random guesses, uncorrelated with the calculated value of the spaceships.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Modulated Epsilon-Greedy "Lapse".
    
    This model assumes OCI affects the 'lapse rate' or random exploration 
    (epsilon) in Stage 1 choices. The final choice probability is a mixture 
    of the softmax policy and a uniform random distribution.
    
    Parameters:
    - learning_rate: [0, 1] Update rate for Q-values.
    - beta: [0, 10] Inverse temperature for the Softmax component.
    - w: [0, 1] Weighting between Model-Based and Model-Free.
    - epsilon_base: [0, 1] Baseline random lapse rate.
    - oci_eps_mod: [0, 1] Modulation of lapse rate by OCI score.
    """
    learning_rate, beta, w, epsilon_base, oci_eps_mod = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    # Calculate Epsilon (bounded [0, 1])
    epsilon = epsilon_base + (current_oci * oci_eps_mod)
    epsilon = np.clip(epsilon, 0.0, 1.0)

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Softmax component
        exp_q1 = np.exp(beta * q_net)
        softmax_probs = exp_q1 / np.sum(exp_q1)
        
        # Mixture: (1-eps)*Softmax + eps*Random
        # Random probability for 2 choices is 0.5
        probs_1 = (1 - epsilon) * softmax_probs + (epsilon * 0.5)
        
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        # We apply standard softmax here (assuming lapses are primarily a Stage 1 planning failure)
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # --- Updates ---
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Stage 1 Learning Rate
This model allows for different learning rates for Stage 1 (Spaceships) and Stage 2 (Aliens), with OCI specifically modulating the Stage 1 learning rate. High OCI might lead to differential plasticity in updating high-level choices (spaceships) versus concrete outcomes (aliens). For example, a high OCI participant might be very sensitive to immediate alien rewards (fixed LR 2) but very slow to update their belief about the spaceships (modulated LR 1), or vice versa.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Stage 1 Learning Rate.
    
    This model separates the learning rates for Stage 1 (Spaceship choice) 
    and Stage 2 (Alien choice). The OCI score modulates only the Stage 1 
    learning rate, allowing the model to capture individual differences 
    in how quickly participants update values for the high-level decision 
    independently of the low-level outcome values.
    
    Parameters:
    - lr_stage2: [0, 1] Fixed learning rate for Stage 2 (Aliens).
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weighting between Model-Based and Model-Free.
    - lr_stage1_base: [0, 1] Baseline learning rate for Stage 1.
    - oci_lr1_mod: [-1, 1] Modulation of Stage 1 LR by OCI. 
      (Note: Logic ensures LR stays in [0,1]).
    """
    lr_stage2, beta, w, lr_stage1_base, oci_lr1_mod = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    # Calculate Modulated Learning Rate for Stage 1
    # We use a tanh-like or clipping approach to keep it in bounds [0,1]
    # Here we use simple linear + clip
    lr_stage1 = lr_stage1_base + (current_oci * oci_lr1_mod)
    lr_stage1 = np.clip(lr_stage1, 0.0, 1.0)

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # --- Updates ---
        # Stage 2 Update (uses lr_stage2)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += lr_stage2 * delta_stage2
        
        # Stage 1 Update (uses OCI-modulated lr_stage1)
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += lr_stage1 * delta_stage1
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```