Here are three new cognitive models that incorporate the OCI score into the decision-making process, using mechanisms distinct from those previously explored.

### Model 1: Stage 1 Indecisiveness Model
This model hypothesizes that high OCI scores are associated with increased "noise" or "indecisiveness" specifically during the planning phase (Stage 1), while the response to immediate stimuli (Stage 2) remains stable. High OCI dampens the inverse temperature (beta) for the first choice, flattening the probability distribution and making choices more random regardless of value differences.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Stage 1 Indecisiveness Model.
    
    Hypothesis: Higher OCI scores lead to reduced precision (lower inverse temperature) 
    specifically at the first stage (Spaceship choice), reflecting anxiety-induced 
    indecisiveness or planning noise, while Stage 2 precision remains constant.
    
    Parameters:
    learning_rate: [0,1] - Value learning rate.
    w: [0,1] - Mixing weight (0=MF, 1=MB).
    beta_stage2: [0,10] - Fixed inverse temperature for Stage 2 (Alien choice).
    beta_stage1_base: [0,10] - Baseline inverse temperature for Stage 1.
    oci_beta1_damp: [0,10] - Scaling factor for OCI-driven dampening of Stage 1 beta.
    """
    learning_rate, w, beta_stage2, beta_stage1_base, oci_beta1_damp = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    # OCI reduces the beta for stage 1. 
    # If OCI is high, beta_stage1 becomes smaller (more random/indecisive).
    # Using division ensures it scales down smoothly.
    beta_stage1 = beta_stage1_base / (1.0 + (current_oci * oci_beta1_damp))

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_stage1 * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        s_idx = int(state[trial])

        # --- Stage 2 Policy ---
        # Stage 2 uses the fixed beta_stage2 parameter
        exp_q2 = np.exp(beta_stage2 * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        r = reward[trial]

        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

        # Stage 1 Update
        q_stage1_mf[a1] += learning_rate * (q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]) # TD(0)
        # Note: Standard TD(1) or lambda models might use reward directly, 
        # but here we follow the template's implication of standard Q-learning structure.
        # However, to be consistent with the template's "delta_stage1" logic:
        # The template had: delta_stage1 = q_stage2_mf[...] - q_stage1_mf[...]
        # So we stick to that TD(0)-like update for MF.

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Learning Rate Annealing (Hyper-vigilance).
    
    Hypothesis: Healthy learning often involves "annealing" (decaying) the learning rate 
    over time as the environment is mapped. High OCI participants (hyper-vigilant) 
    resist this decay, maintaining a high learning rate throughout the task.
    
    Parameters:
    lr_start: [0,1] - Initial learning rate at trial 0.
    beta: [0,10] - Inverse temperature (applied to both stages).
    w: [0,1] - Mixing weight.
    decay_rate: [0,1] - Baseline rate at which LR decays over trials.
    oci_decay_resist: [0,1] - How strongly OCI prevents this decay (1 = full resistance).
    """
    lr_start, beta, w, decay_rate, oci_decay_resist = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    # Calculate effective decay rate.
    # If OCI is high and resist is high, effective_decay approaches 0.
    # This keeps LR high (hyper-vigilance).
    effective_decay = decay_rate * (1.0 - (current_oci * oci_decay_resist))
    if effective_decay < 0.0:
        effective_decay = 0.0

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # Dynamic Learning Rate for this trial
        current_lr = lr_start / (1.0 + (trial * effective_decay))

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        s_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        r = reward[trial]

        # --- Updates using current_lr ---
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += current_lr * delta_stage2

        q_stage1_mf[a1] += current_lr * (q_stage2_mf[s_idx, a2] - q_stage1_mf[a1])

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Stage 2 Compulsive Perseveration.
    
    Hypothesis: OCI drives compulsive repetition of actions specifically at the 
    immediate outcome stage (Stage 2/Alien choice), independent of reward history. 
    This models a "checking" compulsion where the participant sticks to the same alien.
    
    Parameters:
    learning_rate: [0,1] - Value learning rate.
    beta: [0,10] - Inverse temperature.
    w: [0,1] - Mixing weight.
    pers_2_base: [0,5] - Baseline perseveration bonus for Stage 2.
    oci_pers_2_sens: [0,5] - Sensitivity of Stage 2 perseveration to OCI.
    """
    learning_rate, beta, w, pers_2_base, oci_pers_2_sens = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    # Calculate perseveration bonus magnitude
    pers_bonus = pers_2_base + (current_oci * oci_pers_2_sens)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track last action taken in each state (Planet X=0, Planet Y=1)
    # Initialize with -1 (no previous action)
    last_action_2_in_state = [-1, -1]
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Policy (Standard) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        s_idx = int(state[trial])

        # --- Stage 2 Policy (With Perseveration) ---
        # Calculate logits (values before softmax)
        logits_stage2 = beta * q_stage2_mf[s_idx].copy()
        
        # Add stickiness bonus if there was a previous action in this state
        prev_a2 = last_action_2_in_state[s_idx]
        if prev_a2 != -1:
            logits_stage2[prev_a2] += pers_bonus
            
        exp_q2 = np.exp(logits_stage2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # Update history
        last_action_2_in_state[s_idx] = a2

        # --- Updates ---
        r = reward[trial]
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

        q_stage1_mf[a1] += learning_rate * (q_stage2_mf[s_idx, a2] - q_stage1_mf[a1])

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```