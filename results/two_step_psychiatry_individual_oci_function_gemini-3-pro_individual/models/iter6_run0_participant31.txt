Here are three new cognitive models for the two-step task, incorporating OCI-R scores to explain individual variability.

### Model 1: Hybrid Model with OCI-Modulated Loss Aversion
This model hypothesizes that individuals with higher OCI scores perceive the absence of a reward (0 coins) not merely as neutral, but as an aversive "loss" or punishment. This "fear of failure" drives their Q-values into negative territory, altering their trade-off between exploration and exploitation compared to a standard agent.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: Hybrid Model with OCI-Modulated Loss Aversion.
    
    Hypothesis: High OCI participants exhibit a distorted perception of neutral 
    outcomes (0 coins), treating them as active punishments (negative value) 
    rather than just the absence of reward. This reflects the "fear of failure" 
    or perfectionism often seen in OCD.
    
    Parameters:
    - learning_rate: [0, 1] Standard learning rate.
    - beta: [0, 10] Inverse temperature for softmax.
    - w: [0, 1] Weighting between Model-Based (1) and Model-Free (0).
    - loss_sensitivity_oci: [0, 5] Scales the negative reward signal for 0 outcomes.
      (Effective Reward = -1 * loss_sensitivity_oci * OCI, if reward is 0).
    """
    learning_rate, beta, w, loss_sensitivity_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # [State, Action]

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based Value Calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net_s1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax Policy Stage 1
        exp_q1 = np.exp(beta * q_net_s1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # --- Stage 2 Choice ---
        # Softmax Policy Stage 2
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        
        # --- Learning ---
        # Calculate Effective Reward (Loss Aversion Mechanism)
        r_eff = reward[trial]
        if r_eff == 0:
            r_eff = -1.0 * loss_sensitivity_oci * oci_score

        # Stage 1 Update (TD Learning)
        # Note: Using standard Q-learning TD error
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Stage 2 Update
        delta_stage2 = r_eff - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model with OCI-Modulated Memory Decay
This model hypothesizes that OCD symptoms correlate with "memory distrust." High OCI participants are modeled as having a higher rate of value decay for unchosen options. While they learn from current experiences, the values of options they *didn't* choose fade back to zero more rapidly, potentially driving redundant checking behaviors.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: Hybrid Model with OCI-Modulated Memory Decay.
    
    Hypothesis: High OCI participants have higher rates of value decay (forgetting) 
    for unchosen options, reflecting "memory distrust" or a need to constantly 
    refresh information. This leads to values of unvisited states/actions 
    eroding faster than for low OCI participants.
    
    Parameters:
    - learning_rate: [0, 1] Standard learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] MB/MF weight.
    - decay_factor_oci: [0, 1] Scaling factor for decay rate.
      (Decay Rate = decay_factor_oci * OCI).
    """
    learning_rate, beta, w, decay_factor_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) 

    # Calculate specific decay rate for this participant
    decay_rate = decay_factor_oci * oci_score
    # Clamp decay to be safe [0, 1]
    if decay_rate > 1.0: decay_rate = 1.0

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_s1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_s1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        
        # --- Learning ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # --- Memory Decay Mechanism ---
        # Decay unchosen action in Stage 1
        unchosen_a1 = 1 - a1
        q_stage1_mf[unchosen_a1] *= (1.0 - decay_rate)
        
        # Decay unchosen action in the VISITED Stage 2 state
        unchosen_a2 = 1 - a2
        q_stage2_mf[state_idx, unchosen_a2] *= (1.0 - decay_rate)
        
        # (Optional: One could also decay the unvisited state, but standard 
        # decay usually applies to local unchosen options).

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model with OCI-Modulated Win-Stay Bias
This model proposes that high OCI scores are associated with a compulsive "Win-Stay" heuristic. Unlike general habit/stickiness (which repeats any choice), this mechanism specifically adds a bonus to an action *only if* it was immediately rewarded on the previous trial, overriding model-based deliberations.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: Hybrid Model with OCI-Modulated Win-Stay Bias.
    
    Hypothesis: High OCI participants rely more on a heuristic "Win-Stay" strategy, 
    compulsively repeating actions that immediately preceded a reward, regardless 
    of the underlying model-based structure. This is distinct from general 
    stickiness (which repeats regardless of outcome).
    
    Parameters:
    - learning_rate: [0, 1] Standard learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] MB/MF weight.
    - win_stay_oci: [0, 5] Magnitude of the bonus applied to a previously 
      rewarded action. (Bonus = win_stay_oci * OCI).
    """
    learning_rate, beta, w, win_stay_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track previous trial info
    prev_a1 = None
    prev_reward = 0.0

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_s1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Win-Stay Mechanism
        # If previous trial was rewarded, boost the value of the action chosen then
        if prev_a1 is not None and prev_reward == 1.0:
            bonus = win_stay_oci * oci_score
            q_net_s1[prev_a1] += bonus
        
        exp_q1 = np.exp(beta * q_net_s1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        
        # --- Learning ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Store for next trial
        prev_a1 = a1
        prev_reward = reward[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```