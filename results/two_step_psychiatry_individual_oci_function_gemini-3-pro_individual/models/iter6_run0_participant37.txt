Here are the three proposed cognitive models.

### Model 1: OCI-Distorted Transition Beliefs
This model hypothesizes that OCI symptoms (often characterized by "the doubting disease") degrade the accuracy of the Model-Based system's internal map. While the objective transition probability is 0.7, high-OCI participants may subjectively perceive the environment as more unpredictable (higher entropy), pulling their internal transition belief towards 0.5. This reduces the effectiveness of Model-Based planning without changing the weight $w$ directly.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Distorted Transition Beliefs.

    This model assumes that OCI symptoms introduce 'doubt' or uncertainty into the 
    Model-Based system's internal map. High OCI scores distort the perceived 
    transition matrix towards randomness (0.5/0.5), making the Model-Based 
    planning less distinct from random noise, even if the mixing weight 'w' is high.

    Parameters:
    learning_rate: [0, 1] Update rate for value estimation.
    beta: [0, 10] Inverse temperature (choice consistency).
    w: [0, 1] Balance between Model-Based (1) and Model-Free (0) control.
    oci_distortion: [0, 1] Scaling factor. At 0, belief is accurate (0.7). 
                             At 1 (with max OCI), belief approaches random (0.5).

    Bounds:
    learning_rate: [0,1]
    beta: [0,10]
    w: [0,1]
    oci_distortion: [0,1]
    """
    learning_rate, beta, w, oci_distortion = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
  
    # Calculate subjective transition probability
    # Objective is 0.7. Distortion pulls it down towards 0.5 based on OCI.
    # We scale by 0.2 so that even with max OCI and max distortion, it doesn't go below 0.5.
    subjective_p_common = 0.7 - (oci_distortion * current_oci * 0.2)
    
    # Construct the distorted transition matrix used by the MB system
    transition_matrix = np.array([
        [subjective_p_common, 1 - subjective_p_common], 
        [1 - subjective_p_common, subjective_p_common]
    ])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        # Model-Based value calculation uses the DISTORTED matrix
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Mix MB and MF values
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # --- Updates ---
        # Update Stage 1 MF
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] = q_stage1_mf[a1] + learning_rate * delta_stage1
        
        # Update Stage 2 MF
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] = q_stage2_mf[s_idx, a2] + learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Amplified Model-Free Valuation
This model suggests that OCI is linked to an over-valuation of outcomes within the habitual (Model-Free) system. Instead of changing the learning rate, this model posits that OCI amplifies the *subjective magnitude* of the reward signal specifically when updating Model-Free values. This causes Model-Free Q-values to grow larger than Model-Based values (which rely on unscaled expectations), effectively biasing the agent towards habitual control through value magnitude rather than the mixing weight.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Amplified Model-Free Valuation.

    This model hypothesizes that OCI amplifies the reward signal processed by the 
    Model-Free (habit) system. High OCI leads to a subjective inflation of the 
    reward outcome, causing MF Q-values to grow larger in magnitude than MB values.
    This effectively biases the final choice towards the MF preference and increases 
    choice determinism for MF-favored options.

    Parameters:
    learning_rate: [0, 1] Standard update rate.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] MB/MF mixing weight.
    oci_mf_amp: [0, 5] Amplification factor. Reward entering MF update is multiplied 
                       by (1 + oci_mf_amp * OCI).

    Bounds:
    learning_rate: [0,1]
    beta: [0,10]
    w: [0,1]
    oci_mf_amp: [0,5]
    """
    learning_rate, beta, w, oci_mf_amp = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    # Calculate the subjective reward scalar for the MF system
    mf_reward_scalar = 1.0 + (oci_mf_amp * current_oci)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # --- Updates ---
        # Note: The amplification applies to the REWARD signal in the MF update
        # This propagates to Stage 1 MF values via the TD error
        
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] = q_stage1_mf[a1] + learning_rate * delta_stage1
        
        # Effective reward for MF system is amplified by OCI
        effective_r = r * mf_reward_scalar
        
        delta_stage2 = effective_r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] = q_stage2_mf[s_idx, a2] + learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Dampened Stage 1 Precision
This model proposes that OCI manifests as specific anxiety or "noise" at the initial decision stage (Stage 1), where uncertainty is highest. While the participant may learn and execute Stage 2 choices (concrete outcomes) with normal precision, OCI dampens the inverse temperature ($\beta$) specifically for Stage 1. This results in more stochastic/exploratory behavior in the spaceship choice, reflecting a difficulty in committing to a plan, while preserving precise execution at the alien choice level.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Dampened Stage 1 Precision.

    This model posits that OCI is associated with decision anxiety that specifically 
    affects the high-level planning stage (Stage 1). OCI acts to 'dampen' the 
    inverse temperature (beta) for the first choice, making spaceship selection 
    more stochastic/noisy compared to the alien selection (Stage 2), which remains 
    under the control of the base beta.

    Parameters:
    learning_rate: [0, 1] Update rate.
    beta_base: [0, 10] Base inverse temperature (used for Stage 2).
    w: [0, 1] MB/MF mixing weight.
    oci_s1_damp: [0, 1] Dampening factor for Stage 1. 
                        Beta_stage1 = Beta_base * (1 - oci_s1_damp * OCI).

    Bounds:
    learning_rate: [0,1]
    beta_base: [0,10]
    w: [0,1]
    oci_s1_damp: [0,1]
    """
    learning_rate, beta_base, w, oci_s1_damp = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    # Calculate Stage 1 specific beta (dampened by OCI)
    # If OCI is high, beta_1 decreases (more random).
    # We clip to ensure it doesn't go negative.
    dampening = 1.0 - (oci_s1_damp * current_oci)
    dampening = max(0.0, dampening) 
    beta_stage1 = beta_base * dampening

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Use the OCI-dampened beta for Stage 1
        exp_q1 = np.exp(beta_stage1 * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        # Use the base beta for Stage 2 (unaffected by OCI anxiety)
        exp_q2 = np.exp(beta_base * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # --- Updates ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] = q_stage1_mf[a1] + learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] = q_stage2_mf[s_idx, a2] + learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```