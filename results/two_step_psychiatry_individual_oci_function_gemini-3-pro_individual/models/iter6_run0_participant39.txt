Here are three new cognitive models implemented as Python functions.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Post-Error Beta Adjustment.
    
    Hypothesis: OCI scores relate to how participants react to failure (0 reward).
    High OCI might lead to 'freezing' or rigidity (increased inverse temperature)
    or conversely anxiety-induced randomness (decreased inverse temperature) 
    specifically after a loss, compared to their baseline behavior.
    
    Parameters:
    lr: [0, 1] Learning rate.
    w: [0, 1] Mixing weight (MB vs MF).
    beta_base: [0, 10] Baseline inverse temperature.
    beta_loss_base: [-5, 5] Baseline change in beta after a loss (0 reward).
    beta_loss_oci: [-5, 5] Modulation of the post-loss beta change by OCI score.
    """
    lr, w, beta_base, beta_loss_base, beta_loss_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track previous reward to adjust beta. Initialize as if previous was a win (no adjustment)
    prev_reward = 1.0 
    
    # Calculate the shift factor based on OCI
    loss_shift = beta_loss_base + beta_loss_oci * oci_score

    for trial in range(n_trials):
        
        # Determine current beta based on previous outcome
        current_beta = beta_base
        if prev_reward == 0.0:
            current_beta += loss_shift
        
        # Ensure beta stays non-negative
        current_beta = np.maximum(0.0, current_beta)

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(current_beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # policy for the second choice
        exp_q2 = np.exp(current_beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += lr * delta_stage2
        
        prev_reward = r

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Modulated Outcome-Based Perseveration.
    
    Hypothesis: High OCI participants may exhibit a "return to context" bias.
    Instead of repeating the previous motor action (Action Stickiness), they 
    prefer the action that leads to the previously visited Planet (Outcome Stickiness).
    This implies using the transition structure to repeat the state, regardless of reward.
    
    Parameters:
    lr: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Mixing weight.
    ostick_base: [-2, 2] Baseline outcome stickiness (bonus added to Q-value).
    ostick_oci: [-2, 2] Modulation of outcome stickiness by OCI.
    """
    lr, beta, w, ostick_base, ostick_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    stickiness_magnitude = ostick_base + ostick_oci * oci_score
    prev_state = -1

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Apply Outcome Stickiness
        # If prev_state was 0 (Planet X), boost Action 0 (A) which leads to X with 0.7 prob.
        # If prev_state was 1 (Planet Y), boost Action 1 (U) which leads to Y with 0.7 prob.
        q_net_sticky = q_net.copy()
        if prev_state != -1:
            # Since matrix is symmetric [[.7, .3], [.3, .7]], 
            # State 0 corresponds to Action 0, State 1 to Action 1
            q_net_sticky[prev_state] += stickiness_magnitude

        exp_q1 = np.exp(beta * q_net_sticky)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += lr * delta_stage2
        
        prev_state = state_idx

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Initial Beliefs (Pessimism/Optimism).
    
    Hypothesis: OCI scores correlate with the initial expectations of reward 
    (priors) before learning occurs. High OCI might be associated with 
    pessimism (low initial Q-values) or perfectionism/high expectations 
    (high initial Q-values), affecting early exploration and value convergence.
    
    Parameters:
    lr: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Mixing weight.
    q_init_base: [0, 1] Baseline initial Q-value.
    q_init_oci: [-1, 1] Modulation of initial Q-value by OCI.
    """
    lr, beta, w, q_init_base, q_init_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Calculate initial Q-value
    q0 = q_init_base + q_init_oci * oci_score
    # Clip to valid reward range [0, 1] to prevent numerical instability or illogical values
    q0 = np.clip(q0, 0.0, 1.0)
    
    # Initialize Q-tables with this value
    q_stage1_mf = np.full(2, q0)
    q_stage2_mf = np.full((2, 2), q0)

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += lr * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```