Here are three new cognitive models that explore different mechanisms for how Obsessive-Compulsive symptoms (OCI) might influence decision-making in this task.

### Model 1: Hybrid Model with OCI-modulated Model-Based Weighting (`w`)
This model hypothesizes that individuals with higher OCI scores might rely more on habitual (model-free) control rather than goal-directed (model-based) planning, or vice versa. It implements a standard hybrid reinforcement learning model where the balance between model-based and model-free values is controlled by a weighting parameter $w$, which is modulated by the OCI score.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free RL with OCI-modulated weighting (w).
    
    This model tests if OCI score shifts the balance between goal-directed (MB)
    and habitual (MF) control.
    
    Parameters:
    learning_rate: [0, 1] Learning rate for value updates.
    beta: [0, 10] Inverse temperature for softmax choice.
    w_base: [0, 1] Baseline weight for Model-Based control (0=MF, 1=MB).
    w_oci_mod: [-1, 1] Modulation of w by OCI score.
    """
    learning_rate, beta, w_base, w_oci_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate w based on OCI, clamping between 0 and 1
    w = w_base + (w_oci_mod * oci_score)
    w = np.clip(w, 0.0, 1.0)
    
    # Fixed transition matrix as per task description (A->X=0.7, U->Y=0.7)
    # Indices: 0=A, 1=U; 0=X, 1=Y
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # state x action

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based Value Calculation: V(s') = max_a Q(s', a)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value: Q_net = w * Q_mb + (1-w) * Q_mf
        q_stage1_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_stage1_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # --- Stage 2 Choice ---
        state_idx = int(state[trial])
        
        # Standard Q-learning for second stage
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Learning Updates ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Update Stage 2 MF values
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Update Stage 1 MF values (SARSA-like TD(1) update often used in 2-step)
        # Note: In standard hybrid models, MF Q1 is often updated via TD error from stage 2 state value
        # Or via direct reward. Here we use the standard TD(1) approach for MF.
        q_stage1_mf[a1] += learning_rate * delta_stage2 # Eligibility trace style update
        
        # TD(0) update for Stage 1 (optional but common)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Asymmetric Learning Rates Modulated by OCI
This model investigates if OCI symptoms relate to a bias in learning from positive versus negative outcomes. It posits that OCI might specifically modulate the learning rate for negative prediction errors (`alpha_neg`), reflecting a potential sensitivity to punishment or failure often associated with anxiety and compulsion.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free Q-learning with Asymmetric Learning Rates (Pos/Neg),
    where OCI modulates the Negative Learning Rate.
    
    Hypothesis: High OCI leads to hypersensitivity (or hyposensitivity) to negative outcomes.
    
    Parameters:
    alpha_pos: [0, 1] Learning rate for positive prediction errors.
    alpha_neg_base: [0, 1] Base learning rate for negative prediction errors.
    alpha_neg_oci_mod: [-1, 1] Modulation of negative learning rate by OCI.
    beta: [0, 10] Inverse temperature.
    stickiness: [0, 5] Choice perseveration.
    """
    alpha_pos, alpha_neg_base, alpha_neg_oci_mod, beta, stickiness = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate OCI-modulated negative learning rate
    alpha_neg = alpha_neg_base + (alpha_neg_oci_mod * oci_score)
    alpha_neg = np.clip(alpha_neg, 0.0, 1.0)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 ---
        q_net_s1 = q_stage1_mf.copy()
        if prev_action_1 != -1:
            q_net_s1[prev_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_net_s1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # --- Stage 2 ---
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Updates ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Stage 2 RPE
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        
        # Apply asymmetric learning rates for Stage 2
        lr_s2 = alpha_pos if delta_stage2 > 0 else alpha_neg
        q_stage2_mf[state_idx, a2] += lr_s2 * delta_stage2
        
        # Stage 1 update (using TD(1) logic - propagating the reward RPE back)
        # Note: We apply the same asymmetric logic to the propagated error
        lr_s1 = alpha_pos if delta_stage2 > 0 else alpha_neg
        q_stage1_mf[a1] += lr_s1 * delta_stage2
        
        prev_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Eligibility Trace Decay (Lambda)
This model explores the idea that OCI might affect how credit is assigned to past actions. In reinforcement learning, the eligibility trace parameter ($\lambda$) controls how much the second-stage outcome updates the first-stage choice. A higher $\lambda$ means the first-stage choice is held more responsible for the final reward. This model tests if OCI is associated with stronger or weaker credit assignment between distal actions and outcomes.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free Q-learning with OCI-modulated Eligibility Trace (Lambda).
    
    Hypothesis: OCI affects the strength of the link between stage 1 choices
    and stage 2 outcomes (credit assignment).
    
    Parameters:
    learning_rate: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    lambda_base: [0, 1] Base eligibility trace decay parameter.
    lambda_oci_mod: [-1, 1] Modulation of lambda by OCI.
    stickiness: [0, 5] Choice perseveration.
    """
    learning_rate, beta, lambda_base, lambda_oci_mod, stickiness = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate OCI-modulated lambda
    lam = lambda_base + (lambda_oci_mod * oci_score)
    lam = np.clip(lam, 0.0, 1.0)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    prev_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 ---
        q_net_s1 = q_stage1.copy()
        if prev_action_1 != -1:
            q_net_s1[prev_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_net_s1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # --- Stage 2 ---
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Updates ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # TD Error Stage 1 (Predicting Stage 2 value from Stage 1)
        # Note: Standard Q(lambda) or SARSA(lambda) logic adapted for 2-step
        # Prediction error at stage 1 transition
        delta_1 = q_stage2[state_idx, a2] - q_stage1[a1]
        
        # TD Error Stage 2 (Predicting Reward from Stage 2)
        delta_2 = r - q_stage2[state_idx, a2]
        
        # Update Stage 2
        q_stage2[state_idx, a2] += learning_rate * delta_2
        
        # Update Stage 1
        # The update is composed of the immediate TD error (delta_1) 
        # plus the eligibility trace of the second stage error (lambda * delta_2)
        q_stage1[a1] += learning_rate * (delta_1 + (lam * delta_2))
        
        prev_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```