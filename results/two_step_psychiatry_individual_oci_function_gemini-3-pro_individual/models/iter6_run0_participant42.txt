Here are three new cognitive models that incorporate the OCI-R score to explain individual variability in the two-step task.

### Cognitive Model 1: Subjective Transition Bias
This model hypothesizes that OCI scores correlate with a distorted perception of the transition probabilities between spaceships and planets. High OCI individuals may perceive the "common" transition as more deterministic than it actually is (ignoring rare transitions), or conversely, less reliable. This affects the Model-Based value calculation.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid learner with Subjective Transition Bias modulated by OCI.
    
    Hypothesis: OCI scores alter the subjective probability of the 'common' transition
    used in Model-Based planning. High OCI might lead to a belief in higher determinism
    (overestimating the common transition probability).

    Parameters:
    - learning_rate: [0, 1] Learning rate for MF values.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight for Model-Based (1) vs Model-Free (0).
    - trans_bias_base: [0, 1] Base bias towards determinism.
    - trans_bias_oci: [0, 1] Additional bias contributed per unit of OCI.
    """
    learning_rate, beta, w, trans_bias_base, trans_bias_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate subjective probability of the common transition (bounded 0.5 to 1.0)
    # Using tanh to ensure the bias stays within a reasonable range, scaling 0.5 to 1.0.
    raw_bias = trans_bias_base + (trans_bias_oci * oci_score)
    p_common = 0.5 + 0.5 * np.tanh(raw_bias * 2) 
    
    # Subjective transition matrix used for MB planning
    # [[P(X|A), P(Y|A)], [P(X|U), P(Y|U)]]
    transition_matrix = np.array([[p_common, 1 - p_common], [1 - p_common, p_common]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        
        # MB value uses the subjective transition matrix
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = int(state[trial])

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Update Stage 1 (TD-0 style as per template structure, using Stage 2 Q-value)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Update Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2: Intolerance of Uncertainty
This model hypothesizes that OCI scores correlate with "Intolerance of Uncertainty." High OCI participants may penalize options that have not been visited recently (uncertainty aversion), preferring to stick to known paths regardless of reward history.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid learner with Uncertainty Penalty modulated by OCI.
    
    Hypothesis: OCI scores correlate with Intolerance of Uncertainty.
    The model applies a penalty to Stage 1 options proportional to how long 
    it has been since they were last chosen.

    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight (MB vs MF).
    - unc_pen_base: [0, 1] Base penalty for uncertainty (time unvisited).
    - unc_pen_oci: [0, 1] Additional penalty scaling with OCI.
    """
    learning_rate, beta, w, unc_pen_base, unc_pen_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track when each spaceship was last chosen
    last_chosen_trial = np.array([-1.0, -1.0])

    penalty_factor = unc_pen_base + (unc_pen_oci * oci_score)

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Calculate uncertainty penalty based on trials since last choice
        # Using log to dampen the effect for very long unvisited times
        trials_since = trial - last_chosen_trial
        uncertainty_penalty = penalty_factor * np.log(trials_since + 1.0) # +1 to avoid log(0) initially
        
        # Apply penalty (subtract from Q-values before softmax)
        logits = beta * (q_net - uncertainty_penalty)
        
        exp_q1 = np.exp(logits)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Update tracking
        last_chosen_trial[action_1[trial]] = trial
        
        state_idx = int(state[trial])

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3: Stage 2 Stickiness (Outcome Perseveration)
This model hypothesizes that OCI-related compulsivity manifests as "stickiness" or perseveration specifically at the outcome stage (Stage 2: Choice of Alien). While Stage 1 involves planning, Stage 2 is a direct selection of an agent, and high OCI individuals may be prone to repetitively selecting the same alien on a planet, regardless of reward.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid learner with Stage 2 Stickiness modulated by OCI.
    
    Hypothesis: OCI scores correlate with perseveration at the second stage 
    (Alien choice). High OCI leads to repeating the last chosen alien for a 
    given planet, independent of the reward received.

    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight.
    - st2_stick_base: [0, 5] Base stickiness for Stage 2 choices.
    - st2_stick_oci: [0, 5] Additional Stage 2 stickiness per unit OCI.
    """
    learning_rate, beta, w, st2_stick_base, st2_stick_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track last action taken in each state (Planet X and Planet Y)
    # Initialize with -1 (no previous choice)
    last_action_stage2 = np.array([-1, -1]) 
    
    stickiness_mag = st2_stick_base + (st2_stick_oci * oci_score)

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = int(state[trial])

        # policy for the second choice
        logits_2 = beta * q_stage2_mf[state_idx]
        
        # Add stickiness bonus if there was a previous choice in this state
        prev_act = last_action_stage2[state_idx]
        if prev_act != -1:
            logits_2[prev_act] += stickiness_mag
            
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # Update tracking
        last_action_stage2[state_idx] = action_2[trial]
  
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```