Here are three new cognitive models that incorporate the OCI score to explain individual differences in the two-step task. These models introduce mechanisms related to uncertainty, dual-learning rates for different stages, and forgetting, all modulated by the OCI score.

### Model 1: Hybrid RL with OCI-modulated Transition Learning Rate
This model posits that individuals with higher OCI scores might be more sensitive to state transitions, potentially over-updating their internal model of the environment (the transition matrix) based on recent outcomes, reflecting a form of hyper-vigilance or checking behavior regarding the task structure.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL with OCI-modulated Transition Learning Rate.
    
    This model assumes the participant learns the transition matrix (Model-Based)
    rather than using a fixed one. The rate at which they update this transition
    matrix is modulated by their OCI score. High OCI might lead to faster,
    more volatile updating of structural beliefs (hyper-vigilance).

    Parameters:
    - learning_rate: [0, 1] Learning rate for reward values (Q-values).
    - beta: [0, 10] Inverse temperature for softmax.
    - w: [0, 1] Weighting parameter (0 = pure MF, 1 = pure MB).
    - lr_trans_base: [0, 1] Base learning rate for the transition matrix.
    - lr_trans_oci_mod: [0, 1] Modulation of transition learning rate by OCI.
    """
    learning_rate, beta, w, lr_trans_base, lr_trans_oci_mod = model_parameters
    n_trials = len(action_1)
    oci_val = oci[0]

    # Calculate effective transition learning rate, bounded [0, 1]
    # High OCI increases the transition learning rate
    lr_trans = lr_trans_base + (lr_trans_oci_mod * oci_val)
    if lr_trans > 1.0: lr_trans = 1.0
    if lr_trans < 0.0: lr_trans = 0.0

    # Initialize transition matrix (start with uniform or slight prior)
    # Rows: Action 1 (0 or 1), Cols: State (0 or 1)
    # Initial belief: 0.5/0.5 or could be the true 0.7/0.3 if instructed.
    # Let's assume they start with a flat prior 0.5.
    trans_matrix = np.array([[0.5, 0.5], [0.5, 0.5]])

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based Value: Q_MB = T * max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = trans_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        s1_choice = action_1[trial]
        s2_state = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s2_state])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        s2_choice = action_2[trial]
        r = reward[trial]

        # --- Updates ---
        
        # 1. Update Transition Matrix (State Prediction Error)
        # We observed transition s1_choice -> s2_state.
        # Create a target vector (1 for observed state, 0 for other)
        target = np.zeros(2)
        target[s2_state] = 1.0
        
        # Update row corresponding to chosen action
        trans_matrix[s1_choice] += lr_trans * (target - trans_matrix[s1_choice])
        # Normalize to ensure probabilities sum to 1
        trans_matrix[s1_choice] /= np.sum(trans_matrix[s1_choice])

        # 2. Update Stage 2 Values (TD)
        delta_stage2 = r - q_stage2_mf[s2_state, s2_choice]
        q_stage2_mf[s2_state, s2_choice] += learning_rate * delta_stage2
        
        # 3. Update Stage 1 Values (TD - MF only here for the MF component)
        delta_stage1 = q_stage2_mf[s2_state, s2_choice] - q_stage1_mf[s1_choice]
        q_stage1_mf[s1_choice] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid RL with OCI-modulated Forgetting
This model suggests that high OCI scores might be associated with a "flushing" or forgetting of learned values, perhaps due to cognitive interference or an inability to maintain stable value representations over time.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL with OCI-modulated Forgetting (Decay).
    
    This model implements value decay (forgetting) for unchosen options.
    The rate of decay is modulated by OCI. High OCI implies faster forgetting
    of values for options not currently engaged with, potentially leading
    to more exploration or erratic behavior.

    Parameters:
    - learning_rate: [0, 1] Learning rate for chosen options.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weighting parameter.
    - decay_rate_base: [0, 1] Base decay rate towards 0 for unchosen options.
    - decay_oci_mod: [0, 1] Modulation of decay by OCI.
    """
    learning_rate, beta, w, decay_rate_base, decay_oci_mod = model_parameters
    n_trials = len(action_1)
    oci_val = oci[0]

    # Calculate effective decay rate
    # decay = 0 means perfect memory, decay = 1 means instant forgetting
    eff_decay = decay_rate_base + (decay_oci_mod * oci_val)
    if eff_decay > 1.0: eff_decay = 1.0
    if eff_decay < 0.0: eff_decay = 0.0

    # Fixed transition matrix for MB calculation
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        s1_choice = action_1[trial]
        s2_state = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s2_state])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        s2_choice = action_2[trial]
        r = reward[trial]

        # --- Updates ---
        
        # Decay unchosen Stage 1 MF values
        # The chosen one gets updated by RL, the unchosen one decays
        unchosen_s1 = 1 - s1_choice
        q_stage1_mf[unchosen_s1] *= (1.0 - eff_decay)

        # Decay unchosen Stage 2 values (for BOTH states? Usually just the current state's unchosen)
        # Here we decay the unchosen action in the current state
        unchosen_s2 = 1 - s2_choice
        q_stage2_mf[s2_state, unchosen_s2] *= (1.0 - eff_decay)
        # Optionally, one could also decay the values of the unvisited state, 
        # but let's stick to the visited state for simplicity.

        # Standard RL updates
        delta_stage2 = r - q_stage2_mf[s2_state, s2_choice]
        q_stage2_mf[s2_state, s2_choice] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[s2_state, s2_choice] - q_stage1_mf[s1_choice]
        q_stage1_mf[s1_choice] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid RL with OCI-modulated Stage-Specific Learning Rates
This model hypothesizes that OCI affects the balance of learning between the immediate, concrete outcomes (Stage 2) and the abstract, predictive values (Stage 1). Compulsivity might manifest as an over-focus on immediate feedback (Stage 2) versus the preparatory stage.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL with OCI-modulated Stage-Specific Learning Rates.
    
    This model splits the learning rate into two: one for Stage 1 (abstract/preparatory)
    and one for Stage 2 (concrete/consummatory). The ratio or balance between these
    is modulated by OCI. Specifically, we test if OCI modulates the Stage 2 learning rate
    relative to a base rate, reflecting altered sensitivity to immediate outcomes.

    Parameters:
    - lr_base: [0, 1] Base learning rate (applied to Stage 1).
    - lr_s2_ratio_base: [0, 5] Base ratio of Stage 2 LR to Stage 1 LR.
    - lr_s2_oci_slope: [-5, 5] How OCI changes the Stage 2 LR ratio.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weighting parameter.
    """
    lr_base, lr_s2_ratio_base, lr_s2_oci_slope, beta, w = model_parameters
    n_trials = len(action_1)
    oci_val = oci[0]

    # Calculate Stage 1 Learning Rate
    lr_stage1 = lr_base
    
    # Calculate Stage 2 Learning Rate
    # It is a multiplier of the base rate, modulated by OCI
    ratio = lr_s2_ratio_base + (lr_s2_oci_slope * oci_val)
    # Ensure ratio is non-negative
    if ratio < 0: ratio = 0
    
    lr_stage2 = lr_base * ratio
    
    # Bound learning rates
    if lr_stage2 > 1.0: lr_stage2 = 1.0
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        s1_choice = action_1[trial]
        s2_state = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s2_state])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        s2_choice = action_2[trial]
        r = reward[trial]

        # --- Updates ---
        
        # Update Stage 2 with its specific rate
        delta_stage2 = r - q_stage2_mf[s2_state, s2_choice]
        q_stage2_mf[s2_state, s2_choice] += lr_stage2 * delta_stage2
        
        # Update Stage 1 with base rate
        delta_stage1 = q_stage2_mf[s2_state, s2_choice] - q_stage1_mf[s1_choice]
        q_stage1_mf[s1_choice] += lr_stage1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```