Here are 3 new cognitive models that introduce mechanisms distinct from the previously attempted parameter combinations.

### Model 1: Valence-Dependent Learning Asymmetry
**Hypothesis:** OCI relates to altered processing of negative feedback. This model posits that participants learn differently from positive prediction errors (better than expected) versus negative prediction errors (worse than expected), and that the learning rate for **negative** prediction errors is specifically modulated by their OCI score (e.g., high OCI might result in over-sensitivity to "missed" rewards).

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: Valence-Dependent Learning Asymmetry.
    
    Splits learning rate into positive (lr_pos) and negative (lr_neg) components.
    OCI modulates the learning rate specifically for negative prediction errors.
    
    Bounds:
    lr_pos: [0, 1]
    lr_neg_base: [0, 1]
    lr_neg_oci_mod: [-1, 1]
    beta: [0, 10]
    w: [0, 1]
    """
    lr_pos, lr_neg_base, lr_neg_oci_mod, beta, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)  # Model-free values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for stage 2 (Aliens)

    # Calculate effective negative learning rate bounded [0, 1]
    lr_neg_eff = lr_neg_base + (lr_neg_oci_mod * oci_score)
    lr_neg_eff = min(max(lr_neg_eff, 0.0), 1.0)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based Value Calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # --- Updating ---
        # Stage 1 Update (TD-0 for MF)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        
        # Apply valence-dependent learning rate
        lr_s1 = lr_pos if delta_stage1 >= 0 else lr_neg_eff
        q_stage1_mf[a1] += lr_s1 * delta_stage1
        
        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        
        # Apply valence-dependent learning rate
        lr_s2 = lr_pos if delta_stage2 >= 0 else lr_neg_eff
        q_stage2_mf[state_idx, a2] += lr_s2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: State-Target Perseveration
**Hypothesis:** Standard "stickiness" models assume participants repeat motor actions (Spaceships). This model hypothesizes that high OCI participants, driven by a need for certainty or safety, exhibit **State Stickiness**. They try to return to the *Planet* (State) they visited previously, regardless of the spaceship required to get there. OCI modulates the strength of this drive to return to the previous context.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: State-Target Perseveration.
    
    Instead of repeating the previous Action (Spaceship), the agent is biased 
    towards the Action that leads to the previous State (Planet).
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    w: [0, 1]
    state_stick_base: [0, 5]
    state_stick_oci_mod: [-5, 5]
    """
    learning_rate, beta, w, state_stick_base, state_stick_oci_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_state_idx = -1 # Initialize
    
    # Calculate effective state stickiness
    state_stick_eff = state_stick_base + (state_stick_oci_mod * oci_score)

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Apply State Stickiness Bonus
        # If we visited State 0 last time, we boost actions that lead to State 0.
        # Action 0 leads to State 0 with p=0.7. Action 1 leads to State 0 with p=0.3.
        if last_state_idx != -1:
            # Add bonus proportional to the probability of reaching the previous state
            # Bonus for Action 0 = Stickiness * P(LastState | Action 0)
            q_net[0] += state_stick_eff * transition_matrix[0, int(last_state_idx)]
            # Bonus for Action 1 = Stickiness * P(LastState | Action 1)
            q_net[1] += state_stick_eff * transition_matrix[1, int(last_state_idx)]

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # --- Updating ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        last_state_idx = state_idx

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Dynamic Post-Loss Rigidity
**Hypothesis:** High OCI is associated with anxiety and "freezing" or rigidity after negative outcomes. This model implements a dynamic inverse temperature (`beta`) that changes based on the previous trial's outcome. Specifically, OCI modulates how "rigid" (high beta) or "noisy" (low beta) the participant becomes immediately following a loss (0 reward) compared to a win.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: Dynamic Post-Loss Rigidity.
    
    The inverse temperature (beta) is not static. It changes based on the 
    previous trial's reward outcome. OCI modulates the beta specifically 
    after a 'Loss' (0 reward) trial.
    
    Bounds:
    learning_rate: [0, 1]
    beta_win: [0, 10] (Beta used after a reward=1)
    beta_loss_base: [0, 10] (Base beta used after a reward=0)
    beta_loss_oci_mod: [-5, 5] (How OCI shifts beta after a loss)
    w: [0, 1]
    """
    learning_rate, beta_win, beta_loss_base, beta_loss_oci_mod, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_reward = 1.0 # Initialize assuming a win (start neutral/optimistic)

    for trial in range(n_trials):
        
        # Determine Beta for this trial
        if last_reward == 1.0:
            current_beta = beta_win
        else:
            # After a loss, beta is modulated by OCI
            current_beta = beta_loss_base + (beta_loss_oci_mod * oci_score)
            # Ensure beta stays positive
            current_beta = max(current_beta, 0.0)

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(current_beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])

        # --- Stage 2 Policy ---
        # Note: We apply the same dynamic beta to stage 2
        exp_q2 = np.exp(current_beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # --- Updating ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        last_reward = reward[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```