Here are the three proposed cognitive models.

### Model 1: OCI-Modulated Stage 2 Stickiness
This model posits that obsessive-compulsive symptoms specifically increase perseveration at the level of concrete targets (Stage 2 Alien choice), rather than abstract planning (Stage 1 Spaceship choice). High OCI individuals may form stronger habits regarding specific objects (aliens) within specific contexts (planets).

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Stage 2 Stickiness.

    This model assumes OCI modulates stickiness (perseveration) specifically 
    during the second stage (Alien choice). While Stage 1 involves abstract 
    planning, Stage 2 involves concrete selection. High OCI may lead to 
    rigid repetition of choices within specific contexts (Planets).

    Parameters:
    - learning_rate: [0, 1] Value updating rate.
    - beta: [0, 10] Inverse temperature (softmax sensitivity).
    - w: [0, 1] Weighting between MB (1) and MF (0).
    - st2_stick_base: [0, 5] Baseline stickiness for Stage 2 choices.
    - oci_st2_stick_mod: [0, 5] Increase in Stage 2 stickiness per unit of OCI.
    """
    learning_rate, beta, w, st2_stick_base, oci_st2_stick_mod = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    # Calculate total Stage 2 stickiness
    st2_stickiness = st2_stick_base + (current_oci * oci_st2_stick_mod)

    # Fixed transition matrix for MB planning
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Q-values
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # 2 planets, 2 aliens each

    # Track last choice per planet for Stage 2 stickiness
    # Initialize with -1 (no previous choice)
    last_choice_stage2 = np.array([-1, -1]) 

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax Stage 1
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # --- Transition ---
        state_idx = int(state[trial]) # 0 for Planet X, 1 for Planet Y
        
        # --- Stage 2 Decision ---
        # Apply context-specific stickiness
        logits_stage2 = beta * q_stage2_mf[state_idx, :]
        
        if last_choice_stage2[state_idx] != -1:
            logits_stage2[last_choice_stage2[state_idx]] += st2_stickiness
            
        exp_q2 = np.exp(logits_stage2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # Update last choice for this planet
        last_choice_stage2[state_idx] = int(action_2[trial])

        # --- Learning ---
        # Stage 1 MF Update (TD(0))
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1
        
        # Stage 2 MF Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Fictive Learning
This model investigates if OCI is associated with counterfactual processing ("fictive learning"). High OCI individuals might be more prone to updating the value of the option they *didn't* choose, based on the outcome of the option they *did* choose (e.g., assuming a global "good/bad luck" state, or regret minimization).

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Modulated Fictive Learning.

    This model incorporates 'Fictive Learning' (counterfactual updating)
    for the Stage 1 choice. It updates the unchosen spaceship's value 
    based on the received reward, scaled by a fictive learning rate.
    OCI modulates the magnitude of this fictive update.

    Parameters:
    - learning_rate: [0, 1] Standard learning rate for chosen options.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weighting between MB and MF.
    - fictive_base: [0, 1] Baseline ratio of learning for unchosen options.
    - oci_fictive_mod: [0, 1] Modulation of fictive learning by OCI.
    """
    learning_rate, beta, w, fictive_base, oci_fictive_mod = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    # Calculate effective fictive learning rate scaling
    # We clip to ensure it stays within reasonable bounds [0, 1]
    fictive_scale = fictive_base + (current_oci * oci_fictive_mod)
    fictive_scale = np.clip(fictive_scale, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # --- Stage 2 Decision ---
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # --- Learning ---
        chosen_s1 = int(action_1[trial])
        unchosen_s1 = 1 - chosen_s1
        chosen_s2 = int(action_2[trial])

        # Standard Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, chosen_s2] - q_stage1_mf[chosen_s1]
        q_stage1_mf[chosen_s1] += learning_rate * delta_stage1
        
        # Fictive Stage 1 Update (Counterfactual)
        # Update unchosen option as if it had led to the same Stage 2 value
        # This represents a belief in global reward availability or state generalization
        delta_fictive = q_stage2_mf[state_idx, chosen_s2] - q_stage1_mf[unchosen_s1]
        q_stage1_mf[unchosen_s1] += (learning_rate * fictive_scale) * delta_fictive

        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, chosen_s2]
        q_stage2_mf[state_idx, chosen_s2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Transition Learning
This model relaxes the assumption that participants use the fixed, true transition matrix. Instead, they learn the transition probabilities over time. OCI modulates the *learning rate* of these transition probabilities, reflecting how quickly (or rigidly) participants adapt their internal model of the environment's structure.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Transition Learning Rate.

    This model assumes the participant learns the transition matrix 
    (A->X, A->Y, etc.) dynamically rather than using a fixed prior.
    OCI modulates the 'Transition Learning Rate', determining how fast
    the participant updates their belief about the spaceship-planet links.
    High OCI might imply over-sensitivity to rare transitions or rigidity.

    Parameters:
    - learning_rate: [0, 1] Reward value learning rate (MF).
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weighting between MB and MF.
    - tlr_base: [0, 1] Baseline Transition Learning Rate.
    - oci_tlr_mod: [0, 1] Modulation of transition learning rate by OCI.
    """
    learning_rate, beta, w, tlr_base, oci_tlr_mod = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    # Calculate Transition Learning Rate
    trans_lr = tlr_base + (current_oci * oci_tlr_mod)
    trans_lr = np.clip(trans_lr, 0.0, 1.0)

    # Initialize estimated transition matrix (start with uniform or slight bias)
    # Rows: Spaceship A (0), Spaceship U (1)
    # Cols: Planet X (0), Planet Y (1)
    # We maintain probabilities directly
    est_transition_matrix = np.array([[0.5, 0.5], [0.5, 0.5]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # Use the DYNAMIC estimated transition matrix for MB planning
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = est_transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # --- Transition & Structure Learning ---
        spaceship_idx = int(action_1[trial])
        planet_idx = int(state[trial])
        
        # Update Transition Probability estimates
        # Move probability towards 1.0 for the observed transition
        # and towards 0.0 for the unobserved transition
        est_transition_matrix[spaceship_idx, planet_idx] += trans_lr * (1.0 - est_transition_matrix[spaceship_idx, planet_idx])
        est_transition_matrix[spaceship_idx, 1 - planet_idx] += trans_lr * (0.0 - est_transition_matrix[spaceship_idx, 1 - planet_idx])
        
        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[planet_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # --- Value Learning ---
        delta_stage1 = q_stage2_mf[planet_idx, int(action_2[trial])] - q_stage1_mf[spaceship_idx]
        q_stage1_mf[spaceship_idx] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[planet_idx, int(action_2[trial])]
        q_stage2_mf[planet_idx, int(action_2[trial])] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```