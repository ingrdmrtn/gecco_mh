Here are three new cognitive models for the two-step task, incorporating the OCI score into different mechanisms of learning and decision-making.

### Model 1: OCI-Modulated Transition Learning (Distorted World Model)
This model posits that individuals with high OCI symptoms might develop or update their internal model of the world (the transition matrix) differently. Specifically, they might be more rigid or sensitive to transition uncertainty, effectively updating their belief about which spaceship goes to which planet (Model-Based component) at a rate modulated by their OCI score, distinct from the reward learning rate.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Transition Learning Rate.
    
    This model assumes that OCI affects how quickly participants update their internal 
    model of the transition structure (MB learning), while reward learning (MF) remains constant.
    High OCI might lead to hyper-vigilance about structure changes or rigidity.
    
    Parameters:
    - lr_reward: [0, 1] Learning rate for reward values (Q-values).
    - lr_transition_base: [0, 1] Base learning rate for the transition matrix.
    - lr_transition_oci_mod: [0, 1] How much OCI scales the transition learning rate.
      (Effective lr_trans = lr_transition_base * (1 + lr_transition_oci_mod * OCI))
      Bounded to [0, 1] internally.
    - beta: [0, 10] Inverse temperature for softmax.
    - w: [0, 1] Weighting parameter (0 = Pure MF, 1 = Pure MB).
    """
    lr_reward, lr_transition_base, lr_transition_oci_mod, beta, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective transition learning rate, bounded at 1.0
    lr_trans = lr_transition_base * (1 + lr_transition_oci_mod * oci_score)
    if lr_trans > 1.0:
        lr_trans = 1.0
    
    # Initialize transition matrix (subjective belief)
    # Starts at the true probabilities but updates based on experience
    trans_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based Value
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = trans_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        
        # --- Stage 2 Choice ---
        q_s2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * q_s2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Learning ---
        
        # 1. Update Transition Matrix (Model-Based Learning)
        # If we chose spaceship 0 (A) and went to planet 0 (X), that reinforces row 0, col 0
        # If we chose spaceship 0 (A) and went to planet 1 (Y), that reinforces row 0, col 1
        # Only update the row corresponding to the chosen spaceship
        
        # Create a one-hot vector for the observed state outcome
        outcome_vector = np.zeros(2)
        outcome_vector[state_idx] = 1.0
        
        # Update belief: Old belief + alpha * (Observed - Old belief)
        trans_matrix[a1] += lr_trans * (outcome_vector - trans_matrix[a1])
        
        # 2. Update Action Values (Model-Free Learning)
        # Stage 1 MF update (TD(0))
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr_reward * delta_stage1
        
        # Stage 2 MF update
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += lr_reward * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Second-Step Learning Rate (Perseveration on Outcomes)
This model tests the hypothesis that OCI affects how strongly proximal outcomes (getting the coin) reinforce behavior compared to distal predictions. Specifically, the learning rate for the second stage (direct reward receipt) is modulated by OCI, while the first stage learning rate is fixed. This reflects a potential focus on immediate control or checking behavior at the outcome stage.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Modulated Stage 2 Learning Rate.
    
    This model hypothesizes that OCI symptoms impact the learning rate specifically 
    at the second stage (direct reward interaction), reflecting altered sensitivity 
    to immediate feedback or "checking" outcomes.
    
    Parameters:
    - lr_stage1: [0, 1] Learning rate for the first stage choice.
    - lr_stage2_base: [0, 1] Base learning rate for the second stage choice.
    - lr_stage2_oci_slope: [0, 1] Slope for OCI effect on Stage 2 learning.
       (lr_s2 = lr_stage2_base + lr_stage2_oci_slope * OCI, bounded [0,1])
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] MB/MF weight.
    """
    lr_stage1, lr_stage2_base, lr_stage2_oci_slope, beta, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective stage 2 learning rate
    lr_stage2 = lr_stage2_base + (lr_stage2_oci_slope * oci_score)
    if lr_stage2 > 1.0: lr_stage2 = 1.0
    if lr_stage2 < 0.0: lr_stage2 = 0.0
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # --- Stage 2 Policy ---
        q_s2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * q_s2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]
  
        # --- Updates ---
        
        # Stage 1 Update (uses fixed lr_stage1)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr_stage1 * delta_stage1
        
        # Stage 2 Update (uses OCI-modulated lr_stage2)
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += lr_stage2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated "Uncertainty Penalty" (Ambiguity Aversion)
This model introduces a mechanism where the agent penalizes the Model-Based values based on the uncertainty of the transition structure. High OCI is hypothesized to correlate with intolerance of uncertainty. If the transition happened to be "rare" (unexpected) in the recent history, a high OCI agent might penalize the MB system's confidence or value, effectively reducing the Q-value of options that led to unexpected states.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Uncertainty Penalty.
    
    This model assumes high OCI is linked to intolerance of uncertainty/unexpected transitions.
    If a rare transition occurs, the model applies a penalty to the Model-Based Q-value 
    for the next trial, proportional to OCI. This acts like a dynamic "hesitation" 
    after surprise.
    
    Parameters:
    - learning_rate: [0, 1] Standard learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] MB/MF weight.
    - penalty_base: [0, 1] Base penalty subtracted from MB value after a rare transition.
    - penalty_oci_slope: [0, 1] Additional penalty scaled by OCI.
      (Total Penalty = penalty_base + penalty_oci_slope * OCI)
    """
    learning_rate, beta, w, penalty_base, penalty_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Variable to track if the LAST trial was a rare transition
    last_was_rare = False

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Apply Uncertainty Penalty if previous trial was rare
        if last_was_rare:
            penalty = penalty_base + (penalty_oci_slope * oci_score)
            # We penalize the MB values because the 'world model' felt unreliable
            q_stage1_mb -= penalty
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # --- Check for Rare Transition ---
        # Common transitions: A(0)->X(0) and U(1)->Y(1)
        # Rare transitions: A(0)->Y(1) and U(1)->X(0)
        if a1 == state_idx:
            last_was_rare = False # Common
        else:
            last_was_rare = True  # Rare

        # --- Stage 2 Policy ---
        q_s2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * q_s2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]
  
        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```