Here are three new cognitive models, expressed as Python functions, that incorporate the OCI score to explain individual variability in the two-step task.

### Model 1: Hybrid Learner with OCI-Mediated Transition Uncertainty
**Hypothesis**: Individuals with high OCI scores may exhibit "doubt" or uncertainty regarding the environmental structure. Even if the transition probabilities are stable (70/30), high OCI participants may perceive them as less reliable (flatter, closer to 50/50), reducing the effectiveness of their Model-Based (goal-directed) planning.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Learner where OCI flattens the Model-Based transition matrix (Transition Uncertainty).
    
    Hypothesis: High OCI leads to distrust in the 'common' transition structure,
    flattening the subjective transition matrix towards 50/50.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate for MF values.
    beta: [0, 10] - Inverse temperature (softmax).
    w: [0, 1] - Weighting between MB (1) and MF (0) systems.
    oci_doubt: [0, 0.5] - How much OCI reduces the subjective probability of common transitions.
    """
    learning_rate, beta, w, oci_doubt = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    # Modulate transition matrix based on OCI
    # Base common probability is 0.7. OCI reduces this towards 0.5.
    p_common = 0.7 - (current_oci * oci_doubt)
    p_common = np.clip(p_common, 0.5, 0.7) # Ensure it stays between random (0.5) and true (0.7)
    
    # Subjective transition matrix
    transition_matrix = np.array([[p_common, 1-p_common], [1-p_common, p_common]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice (Hybrid: w * MB + (1-w) * MF)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = int(state[trial])

        # policy for the second choice (Purely MF at stage 2)
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Update Stage 1 MF values (SARSA-like)
        # Using the value of the chosen state-action pair in stage 2
        chosen_a1 = action_1[trial]
        chosen_a2 = action_2[trial]
        
        delta_stage1 = q_stage2_mf[state_idx, chosen_a2] - q_stage1_mf[chosen_a1]
        q_stage1_mf[chosen_a1] += learning_rate * delta_stage1
        
        # Update Stage 2 MF values
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, chosen_a2]
        q_stage2_mf[state_idx, chosen_a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Learner with OCI-Driven Response Stickiness
**Hypothesis**: Compulsive behavior often manifests as repetitive motor actions. This model posits that OCI specifically drives "Response Stickiness" at the second stage (choosing the alien). Participants with high OCI may perseverate on the same motor response (e.g., always choosing the left alien) regardless of which planet they are on.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Learner with OCI-modulated Stage 2 Response Stickiness.
    
    Hypothesis: High OCI increases motor perseveration (stickiness) specifically 
    at the second stage (Alien choice), regardless of the planet context.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - MB/MF weight.
    oci_resp_stick: [0, 5] - Strength of sticky bias added to previous Stage 2 choice.
    """
    learning_rate, beta, w, oci_resp_stick = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_2 = -1

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf

        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = int(state[trial])

        # policy for the second choice
        # Add stickiness bonus to the Q-values before softmax
        q_stage2_biased = q_stage2_mf[state_idx].copy()
        
        if last_action_2 != -1:
            stick_bonus = current_oci * oci_resp_stick
            q_stage2_biased[last_action_2] += stick_bonus
            
        exp_q2 = np.exp(beta * q_stage2_biased)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # Record action for next trial's stickiness
        last_action_2 = action_2[trial]
  
        chosen_a1 = action_1[trial]
        chosen_a2 = action_2[trial]

        delta_stage1 = q_stage2_mf[state_idx, chosen_a2] - q_stage1_mf[chosen_a1]
        q_stage1_mf[chosen_a1] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, chosen_a2]
        q_stage2_mf[state_idx, chosen_a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Learner with OCI-Driven Counterfactual Regret
**Hypothesis**: High OCI is associated with worry and rumination about potential mistakes. This model suggests that high OCI individuals engage in "fictitious play" or regret learning: they update the value of the *unchosen* alien as if they had chosen it and received the opposite outcome (assuming a zero-sum or anticorrelated structure, or simply regretting the miss).

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Learner with OCI-modulated Counterfactual Regret (Fictitious Play).
    
    Hypothesis: High OCI individuals update unchosen options based on a regret 
    assumption (if I got 0, the other must have been 1, and vice versa).
    
    Parameters:
    learning_rate: [0, 1] - Standard learning rate.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - MB/MF weight.
    oci_regret: [0, 1] - Weight of the counterfactual update for the unchosen option.
    """
    learning_rate, beta, w, oci_regret = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf

        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = int(state[trial])

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        chosen_a1 = action_1[trial]
        chosen_a2 = action_2[trial]
        unchosen_a2 = 1 - chosen_a2
        r = reward[trial]

        delta_stage1 = q_stage2_mf[state_idx, chosen_a2] - q_stage1_mf[chosen_a1]
        q_stage1_mf[chosen_a1] += learning_rate * delta_stage1
        
        # Standard update for chosen option
        delta_stage2 = r - q_stage2_mf[state_idx, chosen_a2]
        q_stage2_mf[state_idx, chosen_a2] += learning_rate * delta_stage2
        
        # Counterfactual update for UNCHOSEN option (Regret)
        # Assume the unchosen option would have yielded the opposite reward (1-r)
        # Scaled by OCI
        regret_learning_rate = learning_rate * current_oci * oci_regret
        assumed_reward = 1.0 - r
        delta_regret = assumed_reward - q_stage2_mf[state_idx, unchosen_a2]
        
        q_stage2_mf[state_idx, unchosen_a2] += regret_learning_rate * delta_regret
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```