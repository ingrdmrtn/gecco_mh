Here are three new cognitive models implemented as Python functions, incorporating the OCI score into the decision-making process.

### Model 1: Differential Stage 2 Learning
This model tests the hypothesis that OCI symptoms correlate with a hyper-focus on immediate outcomes (the alien's response) relative to distal choices (the spaceship). The OCI score modulates the learning rate specifically for the second stage ($\alpha_2$), creating a disconnect between how fast the participant learns about aliens versus spaceships.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: Differential Stage 2 Learning.
    
    Hypothesis: High OCI participants may be hyper-focused on the immediate 
    outcome of the task (the alien's coin), leading to a higher learning rate 
    for Stage 2 (Alien -> Reward) compared to Stage 1 (Spaceship -> State).
    
    The parameter `oci_alpha2_mult` allows the Stage 2 learning rate to scale 
    upwards based on the OCI score, while Stage 1 uses the base learning rate.

    Parameters:
    lr_base: [0, 1] Base learning rate (used for Stage 1).
    beta: [0, 10] Inverse temperature for softmax.
    w: [0, 1] Model-based / Model-free mixing weight.
    oci_alpha2_mult: [0, 5] Multiplier for OCI to boost Stage 2 learning rate.
                            alpha_2 = lr_base * (1 + oci_alpha2_mult * OCI)
    """
    lr_base, beta, w, oci_alpha2_mult = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
  
    # Calculate Stage 2 learning rate based on OCI
    # We clip it to ensure it stays valid [0, 1]
    lr_stage2 = lr_base * (1.0 + oci_alpha2_mult * current_oci)
    if lr_stage2 > 1.0:
        lr_stage2 = 1.0
    
    # Stage 1 uses the base rate
    lr_stage1 = lr_base

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Initialize Q-values
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Update Stage 1 using lr_stage1
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] = q_stage1_mf[a1] + lr_stage1 * delta_stage1
        
        # Update Stage 2 using OCI-modulated lr_stage2
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] = q_stage2_mf[s_idx, a2] + lr_stage2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Induced Post-Error Rigidity
This model posits that OCI relates to a specific reaction to failure. Instead of a constant change in randomness, high OCI participants become more rigid (higher inverse temperature $\beta$) specifically on trials immediately following a loss (0 coins). This reflects a "clamping down" or anxiety-driven attempt to control the outcome after a negative event.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Induced Post-Error Rigidity.
    
    Hypothesis: High OCI scores predict a behavioral "stiffening" or rigidity 
    specifically after a lack of reward (loss). While the participant operates 
    at a base temperature normally, a loss causes the inverse temperature (beta) 
    to increase for the next trial, making choices more deterministic.
    
    Parameters:
    learning_rate: [0, 1] Learning rate for value updates.
    beta_base: [0, 10] Baseline inverse temperature.
    w: [0, 1] Model-based weight.
    oci_loss_beta_boost: [0, 5] Additional beta added after a loss, scaled by OCI.
    """
    learning_rate, beta_base, w, oci_loss_beta_boost = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    prev_reward = 1.0 # Initialize assuming no previous loss to start

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Determine Dynamic Beta ---
        # If previous reward was 0 (loss), boost beta based on OCI
        if prev_reward == 0.0:
            current_beta = beta_base + (oci_loss_beta_boost * current_oci)
        else:
            current_beta = beta_base
            
        # Ensure beta doesn't explode, though higher beta just means more deterministic
        if current_beta > 20.0: current_beta = 20.0

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(current_beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        # We apply the same dynamic beta to stage 2
        exp_q2 = np.exp(current_beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] = q_stage1_mf[a1] + learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] = q_stage2_mf[s_idx, a2] + learning_rate * delta_stage2
        
        prev_reward = r

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Driven Win-Stay Bias
This model proposes that OCI drives a heuristic "Win-Stay" strategy that overrides value-based learning. Distinct from simple perseveration (stickiness), this bias only activates if the previous trial was rewarded. High OCI participants are assumed to be more prone to repeating what "just worked," regardless of the model-based probability of reaching that state again.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Driven Win-Stay Bias.
    
    Hypothesis: OCI is linked to a reliance on simple heuristic strategies over 
    complex probabilistic modeling. This model implements an OCI-modulated 
    "Win-Stay" bias. If the previous trial was rewarded, the value of the 
    previously chosen spaceship is artificially boosted in the decision process, 
    independent of the actual Q-values.
    
    Parameters:
    learning_rate: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Model-based weight.
    oci_wsls_amp: [0, 5] Amplitude of the Win-Stay bias scaled by OCI.
    """
    learning_rate, beta, w, oci_wsls_amp = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    last_reward = 0.0

    # Calculate the effective bias strength for this participant
    wsls_bias = oci_wsls_amp * current_oci

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Apply Win-Stay Bias
        # Create a copy to avoid modifying the actual Q-values for learning
        q_choice = q_net.copy()
        
        if last_action_1 != -1 and last_reward == 1.0:
            q_choice[last_action_1] += wsls_bias

        exp_q1 = np.exp(beta * q_choice)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] = q_stage1_mf[a1] + learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] = q_stage2_mf[s_idx, a2] + learning_rate * delta_stage2
        
        last_action_1 = a1
        last_reward = r

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```