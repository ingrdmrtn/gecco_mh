Here are three new cognitive models implemented as Python functions.

### Model 1: OCI-Modulated Rare Transition Learning
This model hypothesizes that OCI symptoms specifically alter how individuals learn from "surprising" or rare state transitions (e.g., choosing Spaceship A but ending up on Planet Y). High OCI might lead to over-interpreting these rare events (higher learning rate) or ignoring them as noise (lower learning rate), distinct from learning from common transitions.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Rare Transition Learning.
    
    Hypothesis: OCI scores modulate the learning rate specifically for trials 
    where a Rare transition occurred (Spaceship A -> Planet Y, or U -> X).
    Common transitions use a baseline learning rate.
    
    Parameters:
    beta: [0, 10] Inverse temperature (softmax randomness).
    w: [0, 1] Mixing weight (0=MF, 1=MB).
    lr_common: [0, 1] Learning rate for Common transitions.
    lr_rare_base: [0, 1] Baseline learning rate for Rare transitions.
    lr_rare_oci_slope: [-1, 1] Modulation of Rare LR by OCI score.
    """
    beta, w, lr_common, lr_rare_base, lr_rare_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate OCI-modulated learning rate for rare transitions
    # Sigmoid or clipping could be used, here we clip to [0,1]
    lr_rare = lr_rare_base + lr_rare_oci_slope * oci_score
    lr_rare = np.clip(lr_rare, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # state x action

    for trial in range(n_trials):
        # --- Stage 1 Selection ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        
        # Determine if transition was Common or Rare
        # Common: (A->X [0->0]) or (U->Y [1->1]). Rare: (A->Y [0->1]) or (U->X [1->0])
        is_rare = (a1 != s_idx)
        current_lr = lr_rare if is_rare else lr_common

        # --- Stage 2 Selection ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Updates ---
        # TD(0) updates using the specific learning rate for this transition type
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += current_lr * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += current_lr * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Win-Stay Lose-Shift (WSLS)
This model adds a heuristic "Win-Stay, Lose-Shift" bias to the Stage 1 choice, distinct from the reinforcement learning values. The strength of this heuristic is modulated by the OCI score, testing if higher OCI leads to more reactive, immediate-outcome-based switching behavior regardless of long-term value integration.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Modulated Win-Stay Lose-Shift (WSLS).
    
    Hypothesis: OCI affects the reliance on a reactive 'Win-Stay, Lose-Shift' heuristic.
    This adds a bias to the previous choice if rewarded, and subtracts if unrewarded,
    independent of the standard Q-learning value update.
    
    Parameters:
    lr: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Mixing weight (MB vs MF).
    wsls_base: [-2, 2] Baseline strength of the WSLS bias.
    wsls_oci_slope: [-2, 2] Modulation of WSLS strength by OCI score.
    """
    lr, beta, w, wsls_base, wsls_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    wsls_strength = wsls_base + wsls_oci_slope * oci_score

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_a1 = -1
    prev_reward = -1

    for trial in range(n_trials):
        # --- Stage 1 Selection ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Apply WSLS bias to the logits
        q_net_biased = q_net.copy()
        if prev_a1 != -1:
            if prev_reward == 1:
                q_net_biased[prev_a1] += wsls_strength # Win-Stay
            else:
                q_net_biased[prev_a1] -= wsls_strength # Lose-Shift (or Stay if param is negative)

        exp_q1 = np.exp(beta * q_net_biased)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        a1 = int(action_1[trial])
        prev_a1 = a1
        s_idx = int(state[trial])

        # --- Stage 2 Selection ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]
        prev_reward = r

        # --- Updates ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Stage 2 Exploration
This model proposes that OCI symptoms specifically impact decision noise (exploration) at the second stage (the "alien" choice), distinct from the first stage. High OCI might lead to higher uncertainty or erratic behavior specifically when facing the terminal reward-generating options, while Stage 1 planning remains stable.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Stage 2 Exploration.
    
    Hypothesis: OCI affects the inverse temperature (beta) specifically for the 
    Second Stage choice. Stage 1 uses a fixed beta, while Stage 2's beta varies 
    with OCI, reflecting specific uncertainty at the terminal step.
    
    Parameters:
    lr: [0, 1] Learning rate.
    w: [0, 1] Mixing weight.
    beta_stage1: [0, 10] Fixed inverse temperature for Stage 1.
    beta_stage2_base: [0, 10] Baseline inverse temperature for Stage 2.
    beta_stage2_oci_slope: [-5, 5] Modulation of Stage 2 Beta by OCI.
    """
    lr, w, beta_stage1, beta_stage2_base, beta_stage2_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate Stage 2 specific beta
    beta_stage2 = beta_stage2_base + beta_stage2_oci_slope * oci_score
    # Ensure beta stays non-negative
    beta_stage2 = np.maximum(beta_stage2, 0.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Selection (Uses beta_stage1) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_stage1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        a1 = int(action_1[trial])
        s_idx = int(state[trial])

        # --- Stage 2 Selection (Uses OCI-modulated beta_stage2) ---
        exp_q2 = np.exp(beta_stage2 * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```