Here are three new cognitive models for the two-step task, incorporating OCI scores into specific mechanisms of learning and decision-making.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model with OCI-Modulated Stage 1 Learning Rate.
    
    This model tests the hypothesis that OCI symptoms specifically affect the 
    plasticity of high-level choices (Stage 1) while leaving the immediate 
    reward association learning (Stage 2) intact.
    
    Parameters:
    lr_s1_base: [0, 1] Base learning rate for Stage 1 (Model-Free).
    lr_s1_oci_mod: [-1, 1] Modulation of Stage 1 learning rate by OCI score.
    lr_s2: [0, 1] Learning rate for Stage 2 (reward valuation).
    beta: [0, 10] Inverse temperature (softness of softmax).
    w: [0, 1] Weight for Model-Based control (0 = Pure MF, 1 = Pure MB).
    """
    lr_s1_base, lr_s1_oci_mod, lr_s2, beta, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate modulated Stage 1 learning rate
    lr_s1 = lr_s1_base + (lr_s1_oci_mod * oci_score)
    lr_s1 = np.clip(lr_s1, 0.0, 1.0)
    
    # Transition matrix (fixed for MB calculation)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # Model-Based Value: Expected value of best Stage 2 option
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Softmax Policy Stage 1
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        a1 = int(action_1[trial])
        s_idx = int(state[trial])

        # --- Stage 2 Decision ---
        # Softmax Policy Stage 2
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Learning ---
        # Stage 2 Update (Standard Q-learning)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr_s2 * delta_stage2
        
        # Stage 1 Update (Model-Free TD(1)-like update using Stage 2 PE)
        # Note: In standard hybrid models, MF Stage 1 is often updated by the Stage 2 PE directly
        q_stage1_mf[a1] += lr_s1 * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model with Asymmetric Learning Rates (OCI Modulates Negative LR).
    
    This model tests if OCI symptoms increase sensitivity to negative prediction errors 
    (punishment/omission), reflecting an avoidance or perfectionist tendency within 
    a goal-directed framework.
    
    Parameters:
    lr_pos: [0, 1] Learning rate for positive prediction errors.
    lr_neg_base: [0, 1] Base learning rate for negative prediction errors.
    lr_neg_oci_mod: [-1, 1] Modulation of negative learning rate by OCI.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Weight for Model-Based control.
    """
    lr_pos, lr_neg_base, lr_neg_oci_mod, beta, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate modulated Negative Learning Rate
    lr_neg = lr_neg_base + (lr_neg_oci_mod * oci_score)
    lr_neg = np.clip(lr_neg, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        a1 = int(action_1[trial])
        s_idx = int(state[trial])

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Learning with Asymmetry ---
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        # Select learning rate based on sign of PE
        if delta_stage2 >= 0:
            curr_lr = lr_pos
        else:
            curr_lr = lr_neg
            
        # Update Stage 2
        q_stage2_mf[s_idx, a2] += curr_lr * delta_stage2
        
        # Update Stage 1 (using the same asymmetric logic for the propagated error)
        q_stage1_mf[a1] += curr_lr * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model with Dynamic Transition Learning (OCI Modulates Transition LR).
    
    This model assumes the participant learns the transition matrix (T) over time 
    rather than keeping it fixed. OCI modulates how quickly this internal model 
    is updated, testing if high OCI leads to over-reaction to rare transitions.
    
    Parameters:
    lr_reward: [0, 1] Learning rate for reward values (Q-values).
    lr_trans_base: [0, 1] Base learning rate for the transition matrix.
    lr_trans_oci_mod: [-1, 1] Modulation of transition learning rate by OCI.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Weight for Model-Based control.
    """
    lr_reward, lr_trans_base, lr_trans_oci_mod, beta, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate modulated Transition Learning Rate
    lr_trans = lr_trans_base + (lr_trans_oci_mod * oci_score)
    lr_trans = np.clip(lr_trans, 0.0, 1.0)
    
    # Initialize Transition Matrix (Prior belief of 0.7 common)
    # Rows: Action 1 (0=A, 1=U), Cols: State (0=X, 1=Y)
    # Action A -> X (0.7), Y (0.3)
    # Action U -> X (0.3), Y (0.7)
    T = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        
        # Use dynamic T for Model-Based calculation
        q_stage1_mb = T @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        a1 = int(action_1[trial])
        s_idx = int(state[trial])

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Value Learning ---
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr_reward * delta_stage2
        q_stage1_mf[a1] += lr_reward * delta_stage2
        
        # --- Transition Learning ---
        # State outcome is s_idx (0 or 1). 
        # We update the row corresponding to action a1.
        # If s_idx is 0, target is [1, 0]. If s_idx is 1, target is [0, 1].
        
        # Current probability estimate for the state we arrived at
        p_observed = T[a1, s_idx]
        
        # Prediction Error for transition: (1 - P)
        trans_delta = 1.0 - p_observed
        
        # Update the observed transition probability
        T[a1, s_idx] += lr_trans * trans_delta
        
        # Ensure probabilities sum to 1 (update the other state)
        other_state = 1 - s_idx
        T[a1, other_state] = 1.0 - T[a1, s_idx]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```