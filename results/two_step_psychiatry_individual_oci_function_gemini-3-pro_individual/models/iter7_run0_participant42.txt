Here are three new cognitive models for the two-step task, incorporating OCI scores into specific mechanisms like transition probability distortion, outcome-dependent stickiness, and stage-specific learning rates.

### Model 1: Transition Probability Distortion
This model hypothesizes that OCI symptoms correlate with a distortion in the internal model of the environment's transition structure. Specifically, participants may subjectively over-weight or under-weight the probability of common transitions (0.7) versus rare transitions (0.3) when performing Model-Based planning.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid learner with Transition Probability Distortion modulated by OCI.
    
    Hypothesis: OCI scores correlate with a distortion of the learned transition matrix
    used in Model-Based planning. High OCI might lead to 'over-fitting' the common 
    transition (ignoring rare ones) or conversely, over-weighing rare events (anxiety/risk).
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for value updates.
    - beta: [0, 10] Inverse temperature (softmax sensitivity).
    - w: [0, 1] Mixing weight for Model-Based (1) vs Model-Free (0).
    - trans_dist_base: [-0.3, 0.3] Base distortion of the common transition probability.
    - trans_dist_oci: [-0.5, 0.5] Distortion slope per unit of OCI.
    """
    learning_rate, beta, w, trans_dist_base, trans_dist_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate subjective transition probability for the 'common' transition
    # Base is 0.7. Distortion is added to this.
    distortion = trans_dist_base + (trans_dist_oci * oci_score)
    
    # Clip distortion to ensure probabilities stay valid [0, 1]
    # If 0.7 + dist > 1.0, clip. If 0.7 + dist < 0.0, clip.
    # Max distortion possible is roughly +0.3 or -0.7.
    p_common = np.clip(0.7 + distortion, 0.01, 0.99)
    p_rare = 1.0 - p_common
    
    # Construct the subjective transition matrix
    # Row 0: Space A -> [Planet X (Common), Planet Y (Rare)]
    # Row 1: Space U -> [Planet X (Rare), Planet Y (Common)]
    subjective_transition_matrix = np.array([
        [p_common, p_rare], 
        [p_rare, p_common]
    ])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State (Planet), Action (Alien)

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        
        # Model-Based Value: V_MB = T_subjective * Max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = subjective_transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax Choice 1
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Decision ---
        state_idx = int(state[trial])
        
        # Softmax Choice 2
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning ---
        
        # Update Stage 2 (TD)
        # Q(s2, a2) += lr * (r - Q(s2, a2))
        rpe_2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * rpe_2
        
        # Update Stage 1 MF (TD(1) / Direct Reinforcement)
        # Q(s1, a1) += lr * (r - Q(s1, a1))
        # Note: In pure MF, this is often TD(1). Here we use the reward directly.
        rpe_1 = reward[trial] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * rpe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Win-Stay Lose-Shift (WSLS) Stickiness
This model investigates if OCI symptoms influence a heuristic "Win-Stay, Lose-Shift" strategy that operates alongside the standard reinforcement learning values. Unlike simple perseveration (always repeating the choice), this mechanism encourages repeating a choice only if it was rewarded, and switching if it was unrewarded, with the strength of this bias modulated by OCI.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid learner with Win-Stay Lose-Shift (WSLS) Stickiness modulated by OCI.
    
    Hypothesis: OCI scores correlate with the strength of a heuristic WSLS strategy.
    High OCI might lead to rigid adherence to this heuristic (e.g., strong switching 
    after failure) on top of value-based learning.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight for Model-Based vs Model-Free.
    - wsls_base: [0, 5] Base strength of the WSLS bias.
    - wsls_oci: [-5, 5] Modulation of WSLS strength by OCI.
    """
    learning_rate, beta, w, wsls_base, wsls_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Effective WSLS strength
    wsls_strength = wsls_base + (wsls_oci * oci_score)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    last_reward = -1

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Calculate Logits
        logits = beta * q_net
        
        # Apply WSLS Bias
        if last_action_1 != -1:
            if last_reward == 1:
                # Win-Stay: Add bonus to the previous action
                logits[last_action_1] += wsls_strength
            else: # last_reward == 0
                # Lose-Shift: Subtract bonus from the previous action (or add to other)
                logits[last_action_1] -= wsls_strength

        exp_q1 = np.exp(logits)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_action_1 = action_1[trial]

        # --- Stage 2 Decision ---
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning ---
        rpe_2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * rpe_2
        
        rpe_1 = reward[trial] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * rpe_1
        
        last_reward = reward[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Split Stage 1 (Habit) Learning Rate
This model proposes that OCI specifically affects the rate of habit formation (Stage 1 Model-Free learning) while leaving the learning of goal values (Stage 2) intact. This decouples the global learning rate into a fixed Stage 2 rate and an OCI-modulated Stage 1 rate.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid learner with Split Stage 1 (Habit) Learning Rate modulated by OCI.
    
    Hypothesis: OCI scores correlate with the speed of habit formation (Stage 1 Model-Free
    caching). High OCI might result in 'over-learning' or rapid caching of the first-step 
    action values, distinct from how they learn the specific alien rewards in Stage 2.
    
    Parameters:
    - lr_s2: [0, 1] Base learning rate for Stage 2 (Alien values).
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight for Model-Based vs Model-Free.
    - lr_s1_base: [0, 1] Base learning rate for Stage 1 (Spaceship values).
    - lr_s1_oci: [-1, 1] Modulation of Stage 1 learning rate by OCI.
    """
    lr_s2, beta, w, lr_s1_base, lr_s1_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate Stage 1 specific learning rate
    lr_s1 = lr_s1_base + (lr_s1_oci * oci_score)
    # Clip to valid range [0, 1]
    lr_s1 = np.clip(lr_s1, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Decision ---
        state_idx = int(state[trial])
        
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning ---
        
        # Update Stage 2 using the base LR
        rpe_2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr_s2 * rpe_2
        
        # Update Stage 1 MF using the OCI-modulated LR
        rpe_1 = reward[trial] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr_s1 * rpe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```