Here are 3 new cognitive models for the two-step task, incorporating OCI scores into different mechanisms of learning and decision-making.

### Model 1: Hybrid Model with OCI-Modulated Eligibility Traces
This model hypothesizes that OCI is related to the strength of the link between the second-stage outcome and the first-stage choice. In standard TD learning, an eligibility trace parameter ($\lambda$) controls how much credit the first stage gets for the second stage reward. Here, we test if OCI modulates this "credit assignment" efficiencyâ€”perhaps high OCI individuals are more rigid in credit assignment (higher $\lambda$, effectively TD(1) or Monte Carlo-like updates) or more segmented (lower $\lambda$, TD(0)).

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model with OCI-modulated eligibility trace (lambda).
    
    This model implements a hybrid reinforcement learning agent where the 
    Model-Free component uses an eligibility trace (lambda) to update stage 1 values
    based on the reward received at stage 2. The OCI score modulates this lambda parameter.
    
    Hypothesis: OCI affects how strongly the final outcome is linked back to the 
    initial choice (credit assignment).
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for value updates.
    - beta: [0, 10] Inverse temperature for softmax choice.
    - w: [0, 1] Weighting between Model-Based (1) and Model-Free (0) control.
    - lambda_base: [0, 1] Base eligibility trace parameter.
    - lambda_oci_slope: [-1, 1] Slope of OCI effect on lambda.
    """
    learning_rate, beta, w, lambda_base, lambda_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_val = oci[0]
    
    # Calculate effective lambda, bounded between 0 and 1
    eff_lambda = lambda_base + (lambda_oci_slope * oci_val)
    eff_lambda = np.clip(eff_lambda, 0.0, 1.0)

    # Initialize Q-values
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # 2 states (planets), 2 actions (aliens)
    
    # Fixed transition matrix for Model-Based calculation
    # Row 0: State 0 (Planet X), Row 1: State 1 (Planet Y)
    # Col 0: Action A, Col 1: Action B
    # Note: Task description says A -> X (0.7), U -> Y (0.7) usually.
    # We assume standard structure: Action 0 -> State 0 (0.7), State 1 (0.3)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based Value
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Value
        q_net_s1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax Policy Stage 1
        exp_q1 = np.exp(beta * q_net_s1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        s2 = state[trial] # The planet we arrived at

        # --- Stage 2 Choice ---
        # Standard Model-Free Q-learning at second stage
        exp_q2 = np.exp(beta * q_stage2_mf[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]
        
        # --- Learning ---
        
        # Prediction error at stage 2 (Reward - Expectation)
        delta_stage2 = r - q_stage2_mf[s2, a2]
        
        # Prediction error at stage 1 (Value of state 2 - Value of action 1)
        # Note: In standard TD(lambda), the stage 1 update includes the stage 2 PE scaled by lambda
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        
        # Update Stage 2 Q-values
        q_stage2_mf[s2, a2] += learning_rate * delta_stage2
        
        # Update Stage 1 Q-values
        # The update combines the immediate TD error (delta_stage1) and the eligibility trace 
        # of the second stage error (eff_lambda * delta_stage2)
        q_stage1_mf[a1] += learning_rate * (delta_stage1 + eff_lambda * delta_stage2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Loss Aversion (Asymmetric Learning Rate) Modulated by OCI
This model posits that individuals with high OCI might be hypersensitive to negative outcomes (missing rewards). Instead of a single learning rate, we use separate learning rates for positive prediction errors (better than expected) and negative prediction errors (worse than expected). The OCI score specifically modulates the learning rate for negative prediction errors, testing the hypothesis that compulsivity involves over-learning from failure or absence of reward.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free RL with OCI-modulated asymmetric learning rates (Loss Sensitivity).
    
    This model separates learning rates for positive and negative prediction errors.
    The learning rate for negative errors (alpha_neg) is modulated by the OCI score.
    High OCI might lead to stronger reactions to omitted rewards (0 coins).
    
    Parameters:
    - alpha_pos: [0, 1] Learning rate for positive prediction errors (RPE > 0).
    - alpha_neg_base: [0, 1] Base learning rate for negative prediction errors (RPE < 0).
    - alpha_neg_oci_mod: [-1, 1] How much OCI changes the negative learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Model-based weight.
    """
    alpha_pos, alpha_neg_base, alpha_neg_oci_mod, beta, w = model_parameters
    n_trials = len(action_1)
    oci_val = oci[0]

    # Calculate effective negative learning rate
    alpha_neg = alpha_neg_base + (alpha_neg_oci_mod * oci_val)
    alpha_neg = np.clip(alpha_neg, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_s1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_s1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        s2 = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]
        
        # --- Learning ---
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[s2, a2]
        lr_s2 = alpha_pos if delta_stage2 >= 0 else alpha_neg
        q_stage2_mf[s2, a2] += lr_s2 * delta_stage2
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        lr_s1 = alpha_pos if delta_stage1 >= 0 else alpha_neg
        q_stage1_mf[a1] += lr_s1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Transition Learning Rate (State Inference Rigidity)
Standard two-step models often assume the transition matrix (0.7/0.3) is fixed and known. However, participants might be learning these transition probabilities online. This model allows the agent to update its belief about the transition structure ($P(State|Action)$). The OCI score modulates the learning rate of this transition structure update (`lr_trans`). A high OCI might correlate with rigidity (low `lr_trans`), sticking to initial priors, or hyper-sensitivity to rare transitions (high `lr_trans`).

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid model where the transition matrix is learned, with OCI modulating
    the transition learning rate.
    
    Instead of a fixed [0.7, 0.3] matrix, the agent learns P(State|Action).
    The rate at which they update their belief about spaceship transitions 
    is determined by lr_trans, which is a function of OCI.
    
    Parameters:
    - learning_rate_rew: [0, 1] Learning rate for reward values (Q-values).
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Model-based weight.
    - lr_trans_base: [0, 1] Base learning rate for transition probabilities.
    - lr_trans_oci_slope: [-1, 1] OCI modulation of transition learning rate.
    """
    learning_rate_rew, beta, w, lr_trans_base, lr_trans_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_val = oci[0]
    
    # Effective transition learning rate
    lr_trans = lr_trans_base + (lr_trans_oci_slope * oci_val)
    lr_trans = np.clip(lr_trans, 0.0, 1.0)
    
    # Initialize transition beliefs (start with uniform or slightly biased prior?)
    # Let's assume they start with the true structure but can drift or tighten
    # Row 0: Action A -> [P(X), P(Y)]
    # Row 1: Action B -> [P(X), P(Y)]
    # Note: This is slightly different shape than standard to make updating easier
    # trans_probs[action, state]
    trans_probs = np.array([[0.7, 0.3], [0.3, 0.7]]) 
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based calculation using DYNAMIC transition matrix
        # We need to map trans_probs (2x2: Action x State) to the matrix format for multiplication
        # Standard matrix is State x Action usually, or we do manual expectation
        
        max_q_stage2 = np.max(q_stage2_mf, axis=1) # Value of best action in each state [V(X), V(Y)]
        
        # Q_MB(A) = P(X|A)*V(X) + P(Y|A)*V(Y)
        q_stage1_mb = np.zeros(2)
        q_stage1_mb[0] = trans_probs[0, 0] * max_q_stage2[0] + trans_probs[0, 1] * max_q_stage2[1]
        q_stage1_mb[1] = trans_probs[1, 0] * max_q_stage2[0] + trans_probs[1, 1] * max_q_stage2[1]
        
        q_net_s1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_s1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        s2 = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]
        
        # --- Value Learning ---
        delta_stage2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += learning_rate_rew * delta_stage2
        
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate_rew * delta_stage1
        
        # --- Transition Learning ---
        # Update belief about where action a1 leads
        # If we chose A (0) and went to X (0), we increase P(X|A)
        # 1-hot encoding of observed state
        observed_transition = np.zeros(2)
        observed_transition[s2] = 1.0 # [1, 0] if State 0, [0, 1] if State 1
        
        # Update the row corresponding to the chosen action
        trans_probs[a1] += lr_trans * (observed_transition - trans_probs[a1])
        
        # Ensure normalization (though simple delta rule usually preserves sum if initialized to 1)
        trans_probs[a1] /= np.sum(trans_probs[a1])

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```