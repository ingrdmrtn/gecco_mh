Here are three new cognitive models exploring different mechanisms for how OCI scores might influence decision-making in the two-step task, focusing on model-based/model-free weighting, learning rate asymmetry, and transition learning.

### Model 1: OCI-Modulated Model-Based Weighting (Hybrid Model)
This model hypothesizes that OCI symptoms affect the balance between Model-Based (planning) and Model-Free (habitual) control. High OCI might lead to either an over-reliance on habits (compulsivity) or an over-reliance on planning (perfectionism/checking). We model the weighting parameter `w` as a logistic function of OCI to keep it bounded between 0 and 1.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free with OCI-modulated weighting.
    
    Hypothesis: The balance between model-based (planning) and model-free (habit) systems
    is modulated by the participant's OCI score. A logistic function maps OCI to the weight w.
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    w_intercept: [-5, 5] (Intercept for the logistic function determining w)
    w_slope: [-5, 5] (Slope for how OCI affects the logit of w)
    """
    learning_rate, beta, w_intercept, w_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate w based on OCI using a logistic function to ensure 0 <= w <= 1
    w_logit = w_intercept + w_slope * oci_score
    w = 1 / (1 + np.exp(-w_logit))

    # Initialize values
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based Value: V_MB(s1) = T(s1, s2) * max(Q_MF(s2))
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value: Q_net = w * Q_MB + (1-w) * Q_MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax choice 1
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # --- Stage 2 Policy ---
        state_idx = int(state[trial])
        
        # Softmax choice 2 (purely model-free at this stage)
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Updating ---
        # Update Stage 2 Q-values (SARSA/Q-learning)
        # Prediction error at stage 2
        pe_2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * pe_2
        
        # Update Stage 1 Model-Free Q-values (TD(0))
        # Note: In standard hybrid models, stage 1 MF is updated by stage 2 value
        pe_1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += learning_rate * pe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Second-Stage Learning Rate Asymmetry
This model investigates if OCI scores relate to how participants learn from positive versus negative feedback (rewards vs. omissions) specifically at the second stage. High OCI might be associated with hypersensitivity to punishment (learning more from 0 coins) or reward deficiency.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model with Asymmetric Learning Rates modulated by OCI.
    
    Hypothesis: OCI affects the ratio of learning from positive outcomes (reward=1) 
    vs negative outcomes (reward=0). 
    
    Bounds:
    lr_pos: [0, 1] (Learning rate for positive rewards)
    lr_neg_base: [0, 1] (Base learning rate for zero rewards)
    lr_neg_oci_mod: [-1, 1] (Modulation of negative learning rate by OCI)
    beta: [0, 10]
    w: [0, 1]
    """
    lr_pos, lr_neg_base, lr_neg_oci_mod, beta, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate negative learning rate, clamped between 0 and 1
    lr_neg = lr_neg_base + (lr_neg_oci_mod * oci_score)
    lr_neg = np.clip(lr_neg, 0.0, 1.0)

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # --- Stage 2 Policy ---
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Updating ---
        # Determine effective learning rate based on reward outcome
        current_lr = lr_pos if reward[trial] == 1 else lr_neg
        
        # Update Stage 2
        pe_2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += current_lr * pe_2
        
        # Update Stage 1 MF
        pe_1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += current_lr * pe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Transition Learning (Dynamic Transition Model)
Standard models assume the transition matrix (0.7/0.3) is fixed and known. This model assumes the participant *learns* the transition probabilities, and OCI affects the learning rate of these transitions. High OCI might lead to "over-updating" beliefs about the world structure based on rare events (transition instability).

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model with Dynamic Transition Learning modulated by OCI.
    
    Hypothesis: Participants learn the transition matrix online rather than having it fixed.
    OCI modulates the learning rate for these state transitions (lr_trans).
    High OCI may lead to volatile beliefs about state transitions.
    
    Bounds:
    lr_reward: [0, 1] (Standard learning rate for value updates)
    beta: [0, 10]
    w: [0, 1]
    lr_trans_base: [0, 1] (Base learning rate for transition probabilities)
    lr_trans_oci_slope: [-1, 1] (How OCI affects transition learning rate)
    """
    lr_reward, beta, w, lr_trans_base, lr_trans_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate transition learning rate, clamped between 0 and 1
    lr_trans = lr_trans_base + (lr_trans_oci_slope * oci_score)
    lr_trans = np.clip(lr_trans, 0.0, 1.0)

    # Initialize estimated transition matrix (start with uniform 0.5 or slight bias)
    # Rows: Action 1 (0 or 1), Cols: State (0 or 1)
    # T_est[a, s] = P(state=s | action=a)
    transition_counts = np.array([[0.5, 0.5], [0.5, 0.5]]) 
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # Normalize transition counts to get probabilities
        row_sums = transition_counts.sum(axis=1, keepdims=True)
        T_est = transition_counts / row_sums
        
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        
        # MB value uses the *learned* transition matrix
        q_stage1_mb = T_est @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # --- Stage 2 Policy ---
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Updating ---
        # 1. Update Transition Beliefs
        # We use a delta rule approximation for updating probabilities towards the observed state
        # Target is 1 for the observed state, 0 for the unobserved
        act1 = int(action_1[trial])
        
        # Update toward observed state
        transition_counts[act1, state_idx] += lr_trans
        # Decay the unobserved state to keep sum roughly consistent (or just rely on normalization next step)
        # Here we just add to the count of the observed transition, effectively increasing its probability
        # relative to the other.
        
        # 2. Update Reward Values
        pe_2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += lr_reward * pe_2
        
        pe_1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += lr_reward * pe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```