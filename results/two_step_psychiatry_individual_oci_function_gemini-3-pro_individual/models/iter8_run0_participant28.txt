Here are 3 new cognitive models based on the hypothesis that OCI symptoms alter how internal conflicts are resolved, how "lucky" (rare) outcomes are processed, and how the stability of the environment is perceived.

### Model 1: Conflict-Induced Hesitation Model
This model posits that Obsessive-Compulsive symptoms increase sensitivity to internal conflict. When the Model-Based system (goal-directed) and the Model-Free system (habitual) disagree on the best spaceship, high-OCI participants experience "hesitation" or reduced confidence. This is modeled by suppressing the inverse temperature (`beta`) specifically during conflict trials, leading to more stochastic choices.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Conflict-Induced Hesitation Model.
    
    Hypothesis: High OCI participants experience reduced choice confidence (lower beta)
    when the Model-Based and Model-Free systems suggest different Stage 1 actions.
    
    Parameters:
    learning_rate: [0,1] - Value learning rate.
    beta_base: [0,10] - Baseline inverse temperature.
    w: [0,1] - Mixing weight (0=MF, 1=MB).
    oci_conflict_damp: [0,1] - How much OCI reduces beta during MB-MF conflict.
    """
    learning_rate, beta_base, w, oci_conflict_damp = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- STAGE 1 CHOICE ---
        
        # 1. Calculate Model-Based Values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Detect Conflict
        # Conflict exists if the preferred action differs between systems
        mb_choice = np.argmax(q_stage1_mb)
        mf_choice = np.argmax(q_stage1_mf)
        is_conflict = 1.0 if mb_choice != mf_choice else 0.0
        
        # 3. Modulate Beta based on Conflict and OCI
        # If conflict exists, reduce beta (increase randomness/hesitation)
        # damping factor is bounded [0, 1] effectively
        dampening = is_conflict * current_oci * oci_conflict_damp
        # Ensure beta doesn't go negative
        beta_effective = max(0.0, beta_base * (1.0 - dampening))
        
        # 4. Net Value and Choice Probability
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_effective * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- STAGE 2 CHOICE ---
        s_idx = int(state[trial])
        
        # Standard Stage 2 choice (using base beta for simplicity/parsimony)
        exp_q2 = np.exp(beta_base * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- UPDATES ---
        r = reward[trial]
        
        # Update Stage 2
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Update Stage 1 (TD-0 / SARSA style)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Rare-Outcome Skepticism Model
This model suggests that OCI relates to a distrust of "flukes." If a participant arrives at a planet via a Rare transition (e.g., choosing Spaceship A but ending up at Planet Y), high-OCI individuals discount the learning from that trial. They treat the outcome as unreliable or "lucky" rather than informative, suppressing the learning rate for that specific trial.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Rare-Outcome Skepticism Model.
    
    Hypothesis: OCI modulates the learning rate specifically after rare transitions.
    High OCI participants are skeptical of outcomes derived from low-probability 
    events and suppress learning (lower alpha) on those trials.
    
    Parameters:
    learning_rate_base: [0,1] - Base learning rate for common transitions.
    beta: [0,10] - Inverse temperature.
    w: [0,1] - Mixing weight.
    oci_rare_skepticism: [0,1] - Factor reducing learning rate on rare trials.
    """
    learning_rate_base, beta, w, oci_rare_skepticism = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Define transition structure to detect rare events
    # Spaceship 0 (A) -> Planet 0 (X) is Common (0.7)
    # Spaceship 1 (B) -> Planet 1 (Y) is Common (0.7)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        
        # --- STAGE 1 CHOICE ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- DETERMINE TRANSITION TYPE ---
        s_idx = int(state[trial])
        
        # Determine if the transition was common or rare
        # Common: (Action 0 -> State 0) or (Action 1 -> State 1)
        is_common = (a1 == s_idx) 
        
        # --- STAGE 2 CHOICE ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- UPDATES ---
        r = reward[trial]
        
        # Calculate Effective Learning Rate
        # If rare, reduce LR by OCI factor. If common, use base.
        if is_common:
            lr_eff = learning_rate_base
        else:
            # Apply skepticism: reduce LR based on OCI
            # Bound reduction to ensure LR >= 0
            penalty = current_oci * oci_rare_skepticism
            lr_eff = max(0.0, learning_rate_base * (1.0 - penalty))

        # Update Stage 2
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr_eff * delta_stage2
        
        # Update Stage 1
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr_eff * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Subjective Transition Distortion Model
This model proposes that OCI symptoms distort the *perception* of environmental probabilities used for Model-Based planning. While the true transition probability is 0.7, high-OCI participants may perceive the world as more deterministic (rigid, black-and-white thinking) or less deterministic (entropy). Here, OCI modulates the `p_common` value used specifically inside the Model-Based calculation, without affecting the actual environment or MF learning.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Subjective Transition Distortion Model.
    
    Hypothesis: OCI distorts the internal model of transition probabilities.
    High OCI participants perceive the 'common' transition as more certain (closer to 1.0)
    or less certain, deviating from the true 0.7 probability during MB planning.
    
    Parameters:
    learning_rate: [0,1] - Value learning rate.
    beta: [0,10] - Inverse temperature.
    w: [0,1] - Mixing weight.
    oci_trans_distortion: [0,1] - How much OCI increases the perceived common probability.
    """
    learning_rate, beta, w, oci_trans_distortion = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # True transition matrix (fixed in reality)
    # But the AGENT uses a subjective matrix
    
    # Calculate subjective p_common
    # Base is 0.7. OCI adds distortion.
    # We cap at 0.99 to prevent numerical instability or perfect certainty logic issues
    p_common_subjective = 0.7 + (current_oci * oci_trans_distortion * 0.29)
    if p_common_subjective > 0.99: p_common_subjective = 0.99
    
    subjective_matrix = np.array([
        [p_common_subjective, 1.0 - p_common_subjective], 
        [1.0 - p_common_subjective, p_common_subjective]
    ])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):

        # --- STAGE 1 CHOICE ---
        
        # 1. MB Calculation uses SUBJECTIVE matrix
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = subjective_matrix @ max_q_stage2
        
        # 2. Standard Integration
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- STAGE 2 CHOICE ---
        s_idx = int(state[trial])
        
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- UPDATES ---
        r = reward[trial]
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```