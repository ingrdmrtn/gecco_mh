Here are the 3 proposed cognitive models.

### Cognitive Model 1: The "Distorted Belief" Hybrid Model
This model hypothesizes that OCI symptoms distort the Model-Based system's internal representation of the environment's transition structure. Specifically, high OCI individuals may hold rigid beliefs that the transitions are more deterministic (closer to 1.0) or more chaotic (closer to 0.5) than reality, affecting their forward planning.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model where OCI distorts the Model-Based transition probability belief.
    Hypothesis: High OCI leads to beliefs that transitions are more deterministic (rigid) 
    or more chaotic, deviating from the true 0.7 probability.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate for MF Q-values.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - Mixing weight (0=MF, 1=MB).
    p_belief_base: [0.5, 1.0] - Base subjective belief about common transition probability.
    oci_p_shift: [-0.5, 0.5] - How OCI shifts this belief (positive = more rigid/deterministic).
    """
    learning_rate, beta, w, p_belief_base, oci_p_shift = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate subjective transition matrix based on OCI
    p_common = p_belief_base + (current_oci * oci_p_shift)
    p_common = np.clip(p_common, 0.0, 1.0) # Ensure valid probability
    
    # Matrix: Action 0 -> [State 0, State 1], Action 1 -> [State 0, State 1]
    # Assuming diagonal dominance (Action 0->State 0 is common)
    transition_matrix = np.array([[p_common, 1-p_common], 
                                  [1-p_common, p_common]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based Value: Expected value of next stage states
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax Choice 1
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        chosen_a1 = action_1[trial]
        state_idx = int(state[trial])
        
        # --- Stage 2 Policy ---
        # Standard MF choice based on Stage 2 Q-values
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        chosen_a2 = action_2[trial]
        r = reward[trial]
        
        # --- Updates ---
        # TD(0) update for Stage 1 MF (SARSA)
        # Note: Using Q-value of chosen stage 2 action, not max (SARSA)
        pe_1 = q_stage2_mf[state_idx, chosen_a2] - q_stage1_mf[chosen_a1]
        q_stage1_mf[chosen_a1] += learning_rate * pe_1
        
        # TD(0) update for Stage 2 MF
        pe_2 = r - q_stage2_mf[state_idx, chosen_a2]
        q_stage2_mf[state_idx, chosen_a2] += learning_rate * pe_2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2: The "Compulsive Lose-Stay" Model
This model hypothesizes that OCI is linked to a specific type of perseveration: the inability to switch away from an option after a loss (failure). This "Lose-Stay" bias overrides the standard value-based learning, representing a compulsive repetition of unrewarded actions.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid model with 'Lose-Stay' perseveration modulated by OCI.
    Hypothesis: High OCI participants compulsively repeat choices even after lack of reward,
    represented by a value bonus applied to the previous choice if the last trial was a loss.
    
    Parameters:
    learning_rate: [0, 1]
    beta: [0, 10]
    w: [0, 1]
    lose_stay_base: [0, 5] - Base bonus for repeating unrewarded actions.
    oci_ls_scale: [0, 5] - Additional lose-stay bonus per unit OCI.
    """
    learning_rate, beta, w, lose_stay_base, oci_ls_scale = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Fixed transition matrix for MB system
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    last_reward = 0
    
    # Calculate effective lose-stay bonus
    ls_bonus = lose_stay_base + (current_oci * oci_ls_scale)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Apply Lose-Stay Bonus
        # If previous trial existed and resulted in NO reward (loss), boost value of repeating
        if last_action_1 != -1 and last_reward == 0:
            q_net_1[last_action_1] += ls_bonus
            
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        chosen_a1 = action_1[trial]
        state_idx = int(state[trial])
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        chosen_a2 = action_2[trial]
        r = reward[trial]
        
        # --- Updates ---
        pe_1 = q_stage2_mf[state_idx, chosen_a2] - q_stage1_mf[chosen_a1]
        q_stage1_mf[chosen_a1] += learning_rate * pe_1
        
        pe_2 = r - q_stage2_mf[state_idx, chosen_a2]
        q_stage2_mf[state_idx, chosen_a2] += learning_rate * pe_2
        
        last_action_1 = chosen_a1
        last_reward = r

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3: The "Stage 2 Hyper-Learning" Model
This model hypothesizes that high OCI individuals allocate disproportionate attention to immediate outcomes (the alien's response), resulting in a boosted learning rate specifically for the second stage, while the first stage (spaceship choice) relies on a baseline learning rate and an eligibility trace.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF(lambda) model where OCI boosts the learning rate specifically for Stage 2.
    Hypothesis: High OCI leads to hyper-focus on immediate outcomes (Stage 2) over distal predictors,
    causing faster updating of Alien values compared to Spaceship values.
    
    Parameters:
    lr_base: [0, 1] - Base learning rate (used for Stage 1 MF).
    beta: [0, 10]
    w: [0, 1] - Mixing weight.
    lambda_elig: [0, 1] - Eligibility trace connecting Stage 2 reward to Stage 1 MF.
    oci_lr2_boost: [0, 2] - Boost to Stage 2 learning rate based on OCI.
    """
    lr_base, beta, w, lambda_elig, oci_lr2_boost = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Calculate Stage 2 specific learning rate
    lr_stage2 = lr_base + (current_oci * oci_lr2_boost)
    lr_stage2 = np.clip(lr_stage2, 0.0, 1.0)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        chosen_a1 = action_1[trial]
        state_idx = int(state[trial])
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        chosen_a2 = action_2[trial]
        r = reward[trial]
        
        # --- Updates ---
        # Stage 1 MF Update (TD(lambda))
        # PE1 is the error at stage 1 transition
        pe_1 = q_stage2_mf[state_idx, chosen_a2] - q_stage1_mf[chosen_a1]
        
        # PE2 is the error at stage 2 reward
        pe_2 = r - q_stage2_mf[state_idx, chosen_a2]
        
        # Update Stage 1: Uses lr_base and eligibility trace (lambda)
        # It learns from its own prediction error (pe_1) AND the subsequent error (pe_2) via trace
        q_stage1_mf[chosen_a1] += lr_base * pe_1 + lr_base * lambda_elig * pe_2
        
        # Update Stage 2: Uses boosted learning rate
        q_stage2_mf[state_idx, chosen_a2] += lr_stage2 * pe_2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```