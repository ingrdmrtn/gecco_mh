Here are three new cognitive models that incorporate the OCI score to explain individual variability in the two-step task.

### Model 1: OCI-Modulated Stage 2 Value Decay
This model hypothesizes that OCI symptoms relate to the persistence of memory for specific outcomes (the values of aliens at Stage 2). High OCI might be associated with "obsessive" retention (low decay) or "anxious" interference/forgetting (high decay) of the values of options that were *not* currently chosen.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Stage 2 Value Decay.
    
    Hypothesis: OCI modulates how quickly the values of unvisited states (aliens) 
    decay. This represents a memory persistence mechanism where OCI affects 
    the retention of specific outcome values over time.
    
    Parameters:
    lr: [0, 1] Learning rate for chosen options.
    beta: [0, 10] Inverse temperature (softmax sensitivity).
    w: [0, 1] Mixing weight (0=MF, 1=MB).
    decay_base: [0, 1] Baseline decay rate for unchosen Stage 2 values.
    decay_oci_slope: [-1, 1] Modulation of decay rate by OCI.
    """
    lr, beta, w, decay_base, decay_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate OCI-specific decay rate, clipped to [0, 1]
    decay_rate = decay_base + decay_oci_slope * oci_score
    decay_rate = np.clip(decay_rate, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Q-values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        
        # Action value updating
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += lr * delta_stage2
        
        # Decay unchosen option at Stage 2
        unchosen_a2 = 1 - a2
        q_stage2_mf[state_idx, unchosen_a2] *= (1.0 - decay_rate)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated "Disappointment" (Negative Reward Sensitivity)
This model hypothesizes that high OCI participants exhibit a "fear of failure" or heightened sensitivity to missing out. When they receive 0 coins, they do not treat it as a neutral zero, but as a negative outcome (a loss or punishment). The magnitude of this subjective penalty scales with their OCI score.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Modulated "Disappointment" (Negative Reward Sensitivity).
    
    Hypothesis: High OCI participants treat a reward of 0 not as neutral, 
    but as a negative outcome (loss/punishment). The 'penalty' parameter 
    defines how negative the effective reward becomes when 0 coins are received.
    
    Parameters:
    lr: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Mixing weight.
    penalty_base: [0, 2] Baseline penalty magnitude for 0 reward.
    penalty_oci_slope: [-2, 2] Modulation of penalty magnitude by OCI.
    """
    lr, beta, w, penalty_base, penalty_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate subjective penalty for 0 reward
    penalty_val = penalty_base + penalty_oci_slope * oci_score
    # Penalty should typically be positive (making the effective reward negative)
    # but we allow flexibility via bounds if the optimizer wants to minimize it.
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]

        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        
        # Apply OCI-modulated penalty to the update if reward is 0
        effective_delta = delta_stage2
        if r == 0:
            # If r=0, effective_reward = -penalty_val
            # delta = (-penalty_val) - Q
            # Since delta_stage2 is (0 - Q), we subtract penalty_val
            effective_delta -= penalty_val
            
        q_stage2_mf[state_idx, a2] += lr * effective_delta

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Counterfactual Updating
This model posits that OCI relates to "what if" thinking or magical thinking about causality. Participants update the value of the *unchosen* spaceship based on the outcome of the chosen one (fictitious play). For example, if they get no gold, they might assume the other spaceship would have yielded gold. OCI modulates the strength of this counterfactual learning.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Counterfactual Updating.
    
    Hypothesis: OCI modulates the tendency to engage in counterfactual ('fictitious') 
    updating of the unchosen Stage 1 option. Participants update the unchosen 
    spaceship as if it would have yielded the opposite outcome (1-reward).
    
    Parameters:
    lr: [0, 1] Learning rate for chosen options.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Mixing weight.
    cf_lr_base: [0, 1] Baseline learning rate for counterfactual update.
    cf_lr_oci_slope: [-1, 1] Modulation of counterfactual learning rate by OCI.
    """
    lr, beta, w, cf_lr_base, cf_lr_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate Counterfactual Learning Rate
    cf_lr = cf_lr_base + cf_lr_oci_slope * oci_score
    cf_lr = np.clip(cf_lr, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]

        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        # Counterfactual Update for Unchosen Stage 1 option
        # Assume unchosen option would have yielded (1 - r)
        unchosen_a1 = 1 - a1
        delta_cf = (1.0 - r) - q_stage1_mf[unchosen_a1]
        q_stage1_mf[unchosen_a1] += cf_lr * delta_cf

        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += lr * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```