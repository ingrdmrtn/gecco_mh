Here are three new cognitive models for the two-step task, incorporating OCI scores to explain individual variability in decision-making strategies.

### Cognitive Model 1: Perfectionist Punishment Model
This model hypothesizes that individuals with higher OCI scores exhibit a "fear of failure" or perfectionism. For them, the omission of reward (getting 0 coins) is not merely a neutral outcome but is perceived as a punishment (negative utility). This drives active avoidance of options that previously resulted in no reward, causing values to drop below the initialization point (0).

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Perfectionist Punishment Model.
    
    Hypothesis: High OCI participants perceive the absence of reward (0) as a 
    punishment (negative value), whereas low OCI participants perceive it as neutral.
    This 'Miss Penalty' drives stronger avoidance of unrewarded paths.

    Parameters:
    - learning_rate: [0, 1] Learning rate for value updates.
    - beta: [0, 10] Inverse temperature for softmax choice.
    - w: [0, 1] Mixing weight for Model-Based (1) vs Model-Free (0).
    - penalty_base: [0, 1] Baseline penalty magnitude for missing a reward.
    - penalty_oci: [0, 1] Additional penalty magnitude per unit of OCI.
    """
    learning_rate, beta, w, penalty_base, penalty_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate the effective penalty for a "miss" (0 reward)
    # The penalty increases with OCI score.
    miss_penalty = penalty_base + (penalty_oci * oci_score)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Value Updating ---
        # Determine effective reward
        raw_reward = reward[trial]
        if raw_reward == 0:
            effective_reward = -miss_penalty
        else:
            effective_reward = 1.0

        # Stage 2 update (Standard TD) with effective reward
        rpe_2 = effective_reward - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * rpe_2

        # Stage 1 update (TD(0))
        # Note: We use the stage 2 value of the chosen action as the target, 
        # consistent with standard MF implementation in this task.
        rpe_1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * rpe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2: Stress-Induced Habit Regression
This model hypothesizes that negative outcomes act as stressors that impair Model-Based control. High OCI participants, who may be more sensitive to stress or failure, are hypothesized to retreat to Model-Free (habitual) control strategies immediately following a loss (0 reward), effectively reducing their mixing weight `w` on the subsequent trial.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Stress-Induced Habit Regression Model.
    
    Hypothesis: High OCI participants reduce their reliance on Model-Based control (w)
    immediately after a loss (reward=0), reverting to Model-Free habits.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w_base: [0, 1] Baseline mixing weight (MB vs MF).
    - shift_base: [0, 1] Baseline reduction in w after a loss.
    - shift_oci: [0, 1] Additional reduction in w after a loss per unit of OCI.
    """
    learning_rate, beta, w_base, shift_base, shift_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    # Calculate the magnitude of the shift triggered by a loss
    w_loss_shift = shift_base + (shift_oci * oci_score)
    
    last_reward = 1 # Initialize as if previous trial was a win (no stress)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Determine current w based on previous outcome
        if last_reward == 0:
            w_current = w_base - w_loss_shift
        else:
            w_current = w_base
            
        # Clip w to valid range [0, 1]
        if w_current < 0: w_current = 0.0
        if w_current > 1: w_current = 1.0

        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w_current * q_stage1_mb + (1 - w_current) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Value Updating ---
        rpe_2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * rpe_2

        rpe_1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * rpe_1
        
        last_reward = reward[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3: Pessimistic Planning Model
This model modifies the Model-Based valuation step. Standard MB planning assumes the agent will choose the best option at the second stage (using `max(Q)`). This model hypothesizes that high OCI is associated with "pessimistic appraisal" or lack of confidence. Instead of assuming they will get the `max` value, these agents value a planet as a mix of its best (`max`) and worst (`min`) outcomes.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Pessimistic Planning Model.
    
    Hypothesis: In the Model-Based calculation, High OCI participants underestimate 
    the value of future states (Planets) by mixing the maximum potential value 
    with the minimum potential value (pessimism), rather than assuming optimal choice.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight (MB vs MF).
    - pessimism_base: [0, 1] Base weight given to the worst outcome (min Q).
    - pessimism_oci: [0, 1] Additional pessimism weight per unit of OCI.
    """
    learning_rate, beta, w, pessimism_base, pessimism_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    # Calculate pessimism factor
    pessimism = pessimism_base + (pessimism_oci * oci_score)
    # Clip to [0, 1] to ensure valid convex combination
    if pessimism > 1.0: pessimism = 1.0
    if pessimism < 0.0: pessimism = 0.0

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        
        # Standard MB: uses max(Q)
        # Pessimistic MB: uses (1-p)*max(Q) + p*min(Q)
        max_vals = np.max(q_stage2_mf, axis=1)
        min_vals = np.min(q_stage2_mf, axis=1)
        
        planet_values = (1 - pessimism) * max_vals + pessimism * min_vals
        
        q_stage1_mb = transition_matrix @ planet_values
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Value Updating ---
        rpe_2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * rpe_2

        rpe_1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * rpe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```