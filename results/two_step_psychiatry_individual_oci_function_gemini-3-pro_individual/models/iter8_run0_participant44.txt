Here are three new cognitive models exploring different mechanisms of how Obsessive-Compulsive symptoms (OCI) might influence decision-making in the two-step task, specifically focusing on mixing weights, inverse temperature modulation, and forgetting rates.

### Model 1: Hybrid Model with OCI-Modulated Model-Based Weighting (Mixing Weight)
This model hypothesizes that high OCI scores might correlate with a reliance on habit (model-free) over goal-directed (model-based) planning, or vice versa. It implements the classic "hybrid" reinforcement learning model where a mixing parameter $w$ controls the balance between model-based (MB) and model-free (MF) values. Here, $w$ is a linear function of the OCI score.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL model where the balance between Model-Based and Model-Free control (w)
    is modulated by the OCI score.

    Parameters:
    - learning_rate: [0, 1] Learning rate for value updates.
    - beta: [0, 10] Inverse temperature (softmax sensitivity).
    - w_intercept: [0, 1] Base mixing weight (0=Pure MF, 1=Pure MB) at OCI=0.
    - w_oci_slope: [-1, 1] How OCI changes the mixing weight. 
      (Note: w is clipped to [0,1] inside).

    Hypothesis: High OCI might lead to reduced Model-Based control (lower w).
    """
    learning_rate, beta, w_intercept, w_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_val = oci[0]

    # Calculate mixing weight w, constrained to [0, 1]
    w = w_intercept + (w_oci_slope * oci_val)
    w = np.clip(w, 0.0, 1.0)

    # Transition matrix (fixed for this task version typically, 70/30)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2 = np.zeros((2, 2)) # State (Planet), Action (Alien)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based Value Calculation: V_MB(s1) = T(s1, s2) * max(Q_s2)
        # Note: We use the max Q-value of stage 2 as the value of the state
        max_q_stage2 = np.max(q_stage2, axis=1) # Max value for each planet
        q_stage1_mb = transition_matrix @ max_q_stage2 # Expected value based on transitions

        # Hybrid Value: Q_net = w * Q_MB + (1-w) * Q_MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        s1_choice = action_1[trial]
        s2_state = state[trial] # Planet arrived at

        # --- Stage 2 Choice ---
        # Standard Model-Free choice at stage 2
        exp_q2 = np.exp(beta * q_stage2[s2_state])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        s2_choice = action_2[trial]
        r = reward[trial]

        # --- Updates ---
        # Stage 2 TD Error (Model-Free)
        delta_stage2 = r - q_stage2[s2_state, s2_choice]
        q_stage2[s2_state, s2_choice] += learning_rate * delta_stage2

        # Stage 1 TD Error (Model-Free)
        # Using the value of the state actually reached (SARSA-like logic often used here)
        # or the value of the best action at stage 2 (Q-learning). 
        # Standard Daw model uses Q(s2, a2) to update Q(s1, a1).
        delta_stage1 = q_stage2[s2_state, s2_choice] - q_stage1_mf[s1_choice]
        q_stage1_mf[s1_choice] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Exploratory Noise (Inverse Temperature)
This model posits that OCI symptoms relate to the consistency of choices (exploration/exploitation trade-off). High OCI might be associated with rigid, deterministic behavior (high beta) or anxious, erratic sampling (low beta). This model allows the inverse temperature parameter $\beta$ to vary as a function of the OCI score.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL model where the Inverse Temperature (Beta) is modulated by OCI.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - w: [0, 1] Mixing weight (0=MF, 1=MB).
    - beta_base: [0, 10] Base inverse temperature.
    - beta_oci_slope: [-5, 5] How OCI modifies beta. 
      (Resulting beta clipped to [0, 20]).

    Hypothesis: OCI affects the randomness of choice. High OCI might lead to 
    higher beta (rigidity/determinism).
    """
    learning_rate, w, beta_base, beta_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_val = oci[0]

    # Calculate effective beta
    beta = beta_base + (beta_oci_slope * oci_val)
    beta = np.clip(beta, 0.0, 20.0) # Ensure non-negative, allow high rigidity

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2 = np.zeros((2, 2)) 
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        s1_choice = action_1[trial]
        s2_state = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[s2_state])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        s2_choice = action_2[trial]
        r = reward[trial]

        # --- Updates ---
        delta_stage2 = r - q_stage2[s2_state, s2_choice]
        q_stage2[s2_state, s2_choice] += learning_rate * delta_stage2

        delta_stage1 = q_stage2[s2_state, s2_choice] - q_stage1_mf[s1_choice]
        q_stage1_mf[s1_choice] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Memory Decay (Forgetting)
This model investigates if OCI symptoms relate to how quickly value representations degrade over time. It introduces a value decay parameter $\phi$, where unchosen actions slowly revert to 0 (forgetting). The rate of this decay is modulated by OCI. This captures the idea that compulsive individuals might hold onto outdated value representations longer (low decay) or struggle to maintain them (high decay).

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL model with OCI-modulated Value Decay (Forgetting).
    Unchosen actions decay toward 0.5 (neutral) or 0.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight.
    - decay_base: [0, 1] Base decay rate (1 = no decay, 0 = instant forgetting).
    - decay_oci_mod: [-1, 1] How OCI modifies the decay rate.

    Hypothesis: OCI might relate to the persistence of value memory.
    """
    learning_rate, beta, w, decay_base, decay_oci_mod = model_parameters
    n_trials = len(action_1)
    oci_val = oci[0]

    # Calculate effective decay rate [0, 1]
    decay = decay_base + (decay_oci_mod * oci_val)
    decay = np.clip(decay, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        s1_choice = action_1[trial]
        s2_state = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[s2_state])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        s2_choice = action_2[trial]
        r = reward[trial]

        # --- Updates ---
        # 1. Decay ALL Q-values toward 0 before update (passive forgetting)
        q_stage1_mf *= decay
        q_stage2 *= decay

        # 2. Standard RL Update
        delta_stage2 = r - q_stage2[s2_state, s2_choice]
        q_stage2[s2_state, s2_choice] += learning_rate * delta_stage2

        delta_stage1 = q_stage2[s2_state, s2_choice] - q_stage1_mf[s1_choice]
        q_stage1_mf[s1_choice] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```