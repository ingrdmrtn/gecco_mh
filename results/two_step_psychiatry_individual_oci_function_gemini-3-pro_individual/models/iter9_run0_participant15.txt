Here are 3 new cognitive models expressed as Python functions.

### Model 1: OCI-Modulated Transition Belief
This model hypothesizes that OCI traits distort the participant's internal model of the environment's transition structure. While the true common transition probability is 0.7, high OCI might correlate with a belief that the world is more deterministic (rigid) or more chaotic, affecting the Model-Based value calculation.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model with OCI-Modulated Transition Belief.
    
    Hypothesis: OCI score modulates the 'perceived' transition probability used 
    in Model-Based planning. High OCI may lead to over-estimating determinism 
    (belief -> 1.0) or under-estimating it (belief -> 0.5), distinct from the 
    true probability (0.7).
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    w: [0, 1] (Weighting between MB and MF)
    belief_base: [-5, 5] (Base logit for transition belief)
    belief_oci_slope: [-5, 5] (Effect of OCI on transition belief)
    """
    learning_rate, beta, w, belief_base, belief_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate perceived transition probability using a sigmoid transform
    # Maps (-inf, inf) -> (0, 1)
    logit = belief_base + (belief_oci_slope * oci_score)
    p_perceived = 1.0 / (1.0 + np.exp(-logit))
    
    # Construct the internal model based on this perceived probability
    # Row 0: Action 0 -> [Prob State 0, Prob State 1]
    # Row 1: Action 1 -> [Prob State 0, Prob State 1]
    # Assuming symmetry in the belief structure (Action A->X is same strength as U->Y)
    transition_matrix = np.array([
        [p_perceived, 1.0 - p_perceived], 
        [1.0 - p_perceived, p_perceived]
    ])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based Value: Expected value of next stage using distorted transition matrix
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # --- Stage 2 Policy ---
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Updates ---
        # Stage 1 MF Update (TD(0))
        # Note: In pure 2-step, Stage 1 MF often uses TD(1) or eligibility traces.
        # Here we use simple TD(0) to keep parameter count low as we focus on MB distortion.
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1
        
        # Stage 2 MF Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Uncertainty Exploration
This model hypothesizes that OCI traits relate to an intolerance of uncertainty. This is modeled as an exploration bonus added to the value of actions that have not been chosen recently. The magnitude of this bonus is modulated by the OCI score.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model with OCI-Modulated Uncertainty Exploration.
    
    Hypothesis: OCI modulates an exploration bonus added to options based on 
    how long it has been since they were last chosen. High OCI might drive 
    uncertainty reduction (higher bonus) or avoidance (lower/negative bonus).
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    w: [0, 1]
    expl_base: [0, 5] (Base exploration weight)
    expl_oci_slope: [-5, 5] (Modulation by OCI)
    """
    learning_rate, beta, w, expl_base, expl_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate exploration weight
    expl_weight = expl_base + (expl_oci_slope * oci_score)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Transition matrix (Fixed/Standard for MB calculation here)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Track time since last choice for Stage 1 actions
    # Initialize with 0 (fresh) or 1
    trials_since_chosen = np.ones(2) 

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Calculate Uncertainty Bonus (simple linear growth with time)
        # Scaled by 0.1 to keep magnitude comparable to Q-values
        uncertainty_bonus = expl_weight * (trials_since_chosen * 0.1)
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf + uncertainty_bonus
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # Update counters
        trials_since_chosen += 1
        trials_since_chosen[int(action_1[trial])] = 0
        
        # --- Stage 2 Policy ---
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Post-Error Perseveration
This model differentiates between general stickiness (repeating a choice regardless of outcome) and "compulsive" stickiness (repeating a choice specifically after a loss/lack of reward). It hypothesizes that OCI specifically exacerbates the tendency to stick to a choice after negative feedback.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model with OCI-Modulated Post-Error Perseveration.
    
    Hypothesis: OCI modulates stickiness specifically after unrewarded trials 
    (Lose-Stay), representing compulsive repetition despite negative outcomes, 
    distinct from baseline stickiness.
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    w: [0, 1]
    stick_base: [0, 5] (General tendency to repeat choice)
    stick_loss_oci_slope: [-5, 5] (Additional stickiness after loss modulated by OCI)
    """
    learning_rate, beta, w, stick_base, stick_loss_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    last_action_1 = -1
    last_reward = 0

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Apply Stickiness
        if last_action_1 != -1:
            # Baseline stickiness
            stickiness_val = stick_base
            
            # Add OCI-modulated component ONLY if previous trial was a loss (reward == 0)
            if last_reward == 0:
                stickiness_val += (stick_loss_oci_slope * oci_score)
            
            q_net[int(last_action_1)] += stickiness_val

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # Store history
        last_action_1 = action_1[trial]
        last_reward = reward[trial]
        
        # --- Stage 2 Policy ---
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```