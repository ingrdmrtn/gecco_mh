Here are three new cognitive models that incorporate the OCI score to explain individual variability in the two-step task.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Habit Strength (Long-term Trace).
    
    This model incorporates a "Habit" trace that integrates choice history 
    over a longer timescale than simple 1-back stickiness. It posits that OCI
    symptoms relate to the strength of habitual control (action repetition)
    independent of reward value.
    
    The habit trace is an exponential moving average of past choices.
    
    Parameters:
    - learning_rate: [0, 1] Value learning rate (also used as decay for habit trace).
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weighting between MB (1) and MF (0).
    - habit_w_base: [0, 5] Baseline weight of the habit trace in decision making.
    - oci_habit_mod: [-5, 5] Modulation of habit weight by OCI score.
    """
    learning_rate, beta, w, habit_w_base, oci_habit_mod = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
  
    # Fixed transition matrix for the MB calculation
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Habit trace for stage 1 choices (initialized to 0)
    habit_trace = np.zeros(2)
    
    # Calculate effective habit weight based on OCI
    habit_weight = habit_w_base + (current_oci * oci_habit_mod)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based Value
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Value (Hybrid MB/MF)
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Policy mixes Value and Habit
        logits = beta * q_net + habit_weight * habit_trace
        
        exp_q1 = np.exp(logits)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # --- Stage 2 Policy ---
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Updates ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # 1. Update Habit Trace (Exponential Moving Average)
        # We reuse learning_rate for the trace decay to constrain parameters
        habit_trace *= (1 - learning_rate)
        habit_trace[a1] += learning_rate * 1.0 
        
        # 2. Update Stage 1 MF
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # 3. Update Stage 2 MF
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Modulated Uncertainty Avoidance (Risk Penalty).
    
    This model assumes participants track the variance (uncertainty) of rewards 
    in Stage 2. It hypothesizes that OCI correlates with "Intolerance of Uncertainty,"
    leading to a penalty for options with high reward variance.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for Q-values and Variance tracking.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weighting between MB (1) and MF (0).
    - risk_base: [0, 5] Baseline penalty for variance.
    - oci_risk_mod: [-5, 5] Modulation of risk penalty by OCI score.
    """
    learning_rate, beta, w, risk_base, oci_risk_mod = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Variance tracking for Stage 2 (State, Action)
    v_stage2 = np.zeros((2, 2)) 
    
    risk_penalty_weight = risk_base + (current_oci * oci_risk_mod)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Calculate risk-adjusted Stage 2 values for MB planning
        # We subtract penalty * sqrt(Variance)
        q_stage2_risk_adj = q_stage2_mf - risk_penalty_weight * np.sqrt(v_stage2)
        
        # MB uses risk-adjusted values to estimate state value
        max_q_stage2 = np.max(q_stage2_risk_adj, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # --- Stage 2 Policy ---
        state_idx = int(state[trial])
        
        # Apply risk penalty to Stage 2 choice locally
        q_s2_current = q_stage2_mf[state_idx, :] - risk_penalty_weight * np.sqrt(v_stage2[state_idx, :])
        
        exp_q2 = np.exp(beta * q_s2_current)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Updates ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Update Stage 1 MF
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Update Stage 2 MF and Variance
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        
        # Variance update: V <- V + alpha * (delta^2 - V)
        # This approximates the running variance of the reward prediction error
        v_stage2[state_idx, a2] += learning_rate * (delta_stage2**2 - v_stage2[state_idx, a2])
        
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Transition Structure Learning.
    
    This model allows the agent to learn the transition matrix (Spaceship -> Planet)
    dynamically rather than assuming it is fixed. It hypothesizes that OCI affects
    belief rigidity: high OCI may lead to very slow updating of the structural model 
    (rigidity) or very fast updating (chasing noise).
    
    Parameters:
    - learning_rate: [0, 1] Reward learning rate (Q-values).
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weighting between MB (1) and MF (0).
    - lr_trans_base: [0, 1] Baseline learning rate for transition structure.
    - oci_trans_mod: [-1, 1] Modulation of transition learning rate by OCI.
    """
    learning_rate, beta, w, lr_trans_base, oci_trans_mod = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
  
    # Initialize transition beliefs with the standard prior
    est_trans = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Calculate effective transition learning rate
    lr_trans = lr_trans_base + (current_oci * oci_trans_mod)
    lr_trans = np.clip(lr_trans, 0.0, 1.0)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        
        # Use estimated (learned) transition matrix for MB calculation
        q_stage1_mb = est_trans @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # --- Stage 2 Policy ---
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Updates ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # 1. Update Transition Beliefs
        # Target vector is one-hot for the state we actually arrived at
        target = np.zeros(2)
        target[state_idx] = 1.0
        
        # Update the row corresponding to the chosen spaceship
        est_trans[a1, :] += lr_trans * (target - est_trans[a1, :])
        
        # 2. Update Stage 1 MF
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # 3. Update Stage 2 MF
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```