Here are three new cognitive models that incorporate the OCI score into the decision-making process for the two-step task.

### Model 1: Hybrid Model with OCI-Modulated Stage 1 Perseveration
This model hypothesizes that OCI symptoms specifically drive "stickiness" or repetition of the first-stage choice (Spaceship), regardless of the reward outcome. This reflects the compulsive, habit-like nature of the disorder. It extends the standard hybrid (MB/MF) model by adding a perseveration bonus to the previously chosen spaceship, the magnitude of which is modulated by the OCI score.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model with OCI-Modulated Stage 1 Perseveration.
    
    Hypothesis: High OCI scores lead to increased 'stickiness' (perseveration) 
    at the first stage choice, independent of reward history or model-based planning.
    
    Parameters:
    learning_rate: [0,1] - Value learning rate for MF updates.
    beta: [0,10] - Inverse temperature (exploration/exploitation balance).
    w: [0,1] - Mixing weight (0=MF, 1=MB).
    pers_base: [0,5] - Baseline perseveration bonus.
    oci_pers_sens: [-5,5] - Sensitivity of perseveration to OCI.
    """
    learning_rate, beta, w, pers_base, oci_pers_sens = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate effective perseveration parameter
    perseveration = pers_base + (current_oci * oci_pers_sens)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based Value
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Value (Hybrid)
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add Perseveration Bonus to the logits of the previous choice
        logits_1 = beta * q_net_stage1
        if last_action_1 != -1:
            logits_1[int(last_action_1)] += perseveration
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1)) # Subtract max for numerical stability
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        last_action_1 = a1
        
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        r = reward[trial]

        # --- Updates ---
        # Update Stage 2 MF values (TD error at reward)
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

        # Update Stage 1 MF values (TD(1) style / Monte Carlo update from reward)
        # Note: Standard analysis often uses the reward directly to update stage 1 MF
        q_stage1_mf[a1] += learning_rate * (r - q_stage1_mf[a1])

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model with OCI-Modulated Directed Exploration
This model tests the hypothesis that OCI relates to altered "Directed Exploration" or uncertainty management. Instead of random noise (beta), this model adds a bonus to the Stage 1 options based on how long it has been since they were last chosen (uncertainty). High OCI might suppress this exploration (avoidance of the unknown) or enhance it (checking behavior).

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model with OCI-Modulated Directed Exploration (UCB-like).
    
    Hypothesis: OCI modulates the tendency to explore uncertain options.
    A bonus is added to the value of spaceships based on the time since they 
    were last visited.
    
    Parameters:
    learning_rate: [0,1] - Value learning rate.
    beta: [0,10] - Inverse temperature.
    w: [0,1] - Mixing weight (0=MF, 1=MB).
    expl_bonus_base: [0,5] - Baseline exploration bonus for unchosen options.
    oci_expl_sens: [-5,5] - Sensitivity of exploration bonus to OCI.
    """
    learning_rate, beta, w, expl_bonus_base, oci_expl_sens = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate exploration bonus weight
    expl_weight = expl_bonus_base + (current_oci * oci_expl_sens)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track time since last selection for Stage 1 options
    time_since_chosen = np.zeros(2) 
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add Exploration Bonus (UCB-style: proportional to sqrt of time since chosen)
        # We add 1 to time to avoid sqrt(0) issues if initialized at 0, though not strictly necessary
        exploration_bonus = expl_weight * np.sqrt(time_since_chosen + 1.0)
        
        logits_1 = beta * (q_net_stage1 + exploration_bonus)
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # Update time counters
        time_since_chosen += 1
        time_since_chosen[a1] = 0
        
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        r = reward[trial]

        # --- Updates ---
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

        q_stage1_mf[a1] += learning_rate * (r - q_stage1_mf[a1])

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model with OCI-Modulated Value Decay
This model hypothesizes that OCI relates to "obsessive doubt" or a lack of confidence in memory, implemented as the decay (forgetting) of learned values over time. High OCI participants may experience faster value decay, necessitating frequent "checking" or leading to behavior driven more by immediate rather than long-term history.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model with OCI-Modulated Value Decay (Forgetting).
    
    Hypothesis: OCI symptoms correlate with a 'leakage' or decay of learned 
    values (memory) on every trial. High OCI may imply faster forgetting, 
    representing obsessive doubt about the state of the world.
    
    Parameters:
    learning_rate: [0,1] - Value learning rate.
    beta: [0,10] - Inverse temperature.
    w: [0,1] - Mixing weight (0=MF, 1=MB).
    decay_base: [0,1] - Baseline decay rate (0 = no forgetting, 1 = instant forgetting).
    oci_decay_sens: [-1,1] - Sensitivity of decay rate to OCI.
    """
    learning_rate, beta, w, decay_base, oci_decay_sens = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate effective decay rate, clamped between 0 and 1
    decay_rate = decay_base + (current_oci * oci_decay_sens)
    if decay_rate < 0.0: decay_rate = 0.0
    if decay_rate > 1.0: decay_rate = 1.0
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net_stage1
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        logits_2 = beta * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        r = reward[trial]

        # --- Updates ---
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

        q_stage1_mf[a1] += learning_rate * (r - q_stage1_mf[a1])
        
        # --- Value Decay ---
        # All Q-values decay towards 0 (or neutral) at the end of the trial
        q_stage1_mf *= (1.0 - decay_rate)
        q_stage2_mf *= (1.0 - decay_rate)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```