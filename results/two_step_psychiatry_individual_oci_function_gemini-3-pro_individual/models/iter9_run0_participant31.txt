Here are 3 new cognitive models expressed as Python functions.

### Model 1: Outcome-Based Stickiness (State Perseveration)
This model hypothesizes that high OCI scores relate to a compulsion to return to a familiar *environment* (Planet), rather than just repeating a motor action (Spaceship). While standard "stickiness" repeats the Stage 1 choice, this model adds a "bonus" to the Stage 1 action that is most likely to lead to the *previous Planet visited*, regardless of whether that planet was rewarding.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: Outcome-Based Stickiness (State Perseveration).
    
    Standard "stickiness" models assume participants repeat the same motor choice (Spaceship).
    This model assumes high-OCI participants are "stuck" on the outcome State (Planet).
    They prefer the action that leads to the planet they just visited, driven by a 
    compulsion for environmental familiarity or checking.
    
    Parameters:
    - learning_rate: [0, 1] Standard Q-learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Model-based (1) vs Model-free (0) weight.
    - outcome_stick_oci: [0, 5] How much OCI scales the bonus for the action leading 
      to the previous state.
    """
    learning_rate, beta, w, outcome_stick_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Fixed transition probabilities for the MB system
    # Row 0: Space A -> [Planet X (0.7), Planet Y (0.3)]
    # Row 1: Space B -> [Planet X (0.3), Planet Y (0.7)]
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # [State, Action]
    
    prev_state = -1

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Outcome Stickiness Logic
        if prev_state != -1:
            # Which action commonly leads to the previous planet?
            # If prev_state was 0 (Planet X), Action 0 (Space A) is the best bet (0.7 prob).
            # We add a bonus proportional to the transition probability to the previous state.
            outcome_stickiness = outcome_stick_oci * oci_score
            
            # Bonus for Action 0 = stickiness * P(prev_state | Action 0)
            q_net[0] += outcome_stickiness * transition_matrix[0, prev_state]
            # Bonus for Action 1 = stickiness * P(prev_state | Action 1)
            q_net[1] += outcome_stickiness * transition_matrix[1, prev_state]

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # --- Stage 2 Policy ---
        state_idx = int(state[trial])
        q_s2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * q_s2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # --- Updates ---
        chosen_a1 = int(action_1[trial])
        chosen_a2 = int(action_2[trial])
        
        # MF Stage 1 Update (SARSA-like)
        delta_stage1 = q_stage2_mf[state_idx, chosen_a2] - q_stage1_mf[chosen_a1]
        q_stage1_mf[chosen_a1] += learning_rate * delta_stage1
        
        # MF Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, chosen_a2]
        q_stage2_mf[state_idx, chosen_a2] += learning_rate * delta_stage2
        
        prev_state = state_idx

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Stage 2 Specific Perseveration
Most models focus on Stage 1 decision-making. This model posits that OCI symptoms manifest as rigidity in the *immediate* action selection (asking the Alien), independent of the planning phase. High OCI participants may form a "habit" of asking the same Alien (left or right) regardless of the Planet they are on, creating a disconnect between the state and the optimal action.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: Stage 2 Specific Perseveration (Alien Stickiness).
    
    This model assumes that OCI-related rigidity manifests in the second stage.
    Participants with high OCI scores develop a 'motor habit' for the second stage 
    choice (e.g., always pressing 'left' for the alien), regardless of which planet 
    (context) they are on.
    
    Parameters:
    - learning_rate: [0, 1] Standard learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] MB/MF weight.
    - alien_stick_oci: [0, 5] Strength of stickiness to the previous Stage 2 action,
      scaled by OCI.
    """
    learning_rate, beta, w, alien_stick_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track the previous Stage 2 action (0 or 1)
    prev_a2 = -1

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        q_s2 = q_stage2_mf[state_idx].copy()
        
        # Apply Stage 2 Stickiness
        if prev_a2 != -1:
            stickiness_bonus = alien_stick_oci * oci_score
            q_s2[prev_a2] += stickiness_bonus
            
        exp_q2 = np.exp(beta * q_s2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        # --- Updates ---
        chosen_a1 = int(action_1[trial])
        chosen_a2 = int(action_2[trial])
        
        delta_stage1 = q_stage2_mf[state_idx, chosen_a2] - q_stage1_mf[chosen_a1]
        q_stage1_mf[chosen_a1] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, chosen_a2]
        q_stage2_mf[state_idx, chosen_a2] += learning_rate * delta_stage2
        
        prev_a2 = chosen_a2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Post-Loss Anxiety (Beta Relaxation)
This model proposes that negative outcomes (Reward = 0) induce a state of anxiety or "scrambling" in high-OCI participants. Rather than learning from the loss via prediction error (value update), the participant's choice consistency (`beta`) degrades on the subsequent trial. This reflects a "panic" response where the policy becomes more random after a failure.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: Post-Loss Anxiety (Beta Relaxation).
    
    Hypothesis: High OCI scores correlate with a stronger anxiety response to failure 
    (Reward = 0). This anxiety disrupts cognitive control on the *subsequent* trial, 
    making choices more random (lower beta). 
    
    If the previous trial was a loss, beta is reduced by a factor determined by OCI.
    
    Parameters:
    - learning_rate: [0, 1] Standard learning rate.
    - beta_base: [0, 10] Baseline inverse temperature (used after wins or at start).
    - w: [0, 1] MB/MF weight.
    - loss_beta_damp_oci: [0, 1] Proportion of beta reduction after a loss, scaled by OCI.
      (beta_effective = beta_base * (1 - damp * OCI))
    """
    learning_rate, beta_base, w, loss_beta_damp_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_reward = 1.0 # Assume start with "win" state (no anxiety)

    for trial in range(n_trials):
        
        # Calculate Current Beta
        # If previous reward was 0 (Loss), dampen beta based on OCI
        current_beta = beta_base
        if prev_reward == 0.0:
            # Ensure dampening doesn't make beta negative
            dampening_factor = np.clip(loss_beta_damp_oci * oci_score, 0, 0.99)
            current_beta = beta_base * (1.0 - dampening_factor)

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(current_beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        # Note: We apply the same anxious beta to stage 2
        q_s2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(current_beta * q_s2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        # --- Updates ---
        chosen_a1 = int(action_1[trial])
        chosen_a2 = int(action_2[trial])
        
        delta_stage1 = q_stage2_mf[state_idx, chosen_a2] - q_stage1_mf[chosen_a1]
        q_stage1_mf[chosen_a1] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, chosen_a2]
        q_stage2_mf[state_idx, chosen_a2] += learning_rate * delta_stage2
        
        prev_reward = reward[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```