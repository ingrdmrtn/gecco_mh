Here are three new cognitive models for the two-step decision task, incorporating the OCI score into the decision-making and learning mechanisms.

### Model 1: Hybrid Learner with OCI-Modulated Random Lapses
This model hypothesizes that high OCI scores correlate with "lapses" in attention or executive control, leading to random responding (noise) that overrides value-based decision-making. The model mixes the standard hybrid policy with a uniform random policy, where the mixing weight is determined by the OCI score.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Learner with OCI-Modulated Random Lapses.
    
    Hypothesis: High OCI increases the probability of 'lapses' (epsilon-greedy-like noise),
    where the participant chooses randomly instead of based on value.
    
    P(choice) = (1 - epsilon) * Softmax(Q) + epsilon * 0.5
    epsilon = oci * lapse_factor
    
    Parameters:
    learning_rate: [0, 1] - Rate of value updating.
    beta: [0, 10] - Inverse temperature for softmax.
    w: [0, 1] - Weighting between Model-Based (1) and Model-Free (0).
    lapse_factor: [0, 1] - Scales the impact of OCI on the random lapse rate.
    """
    learning_rate, beta, w, lapse_factor = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate epsilon (lapse rate) based on OCI, clipped to valid probability
    epsilon = np.clip(current_oci * lapse_factor, 0, 1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    
    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # Model-Based Value Calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax Probability
        exp_q1 = np.exp(beta * q_net)
        probs_1_softmax = exp_q1 / np.sum(exp_q1)
        
        # Mixture with Random Policy (Lapse)
        probs_1 = (1 - epsilon) * probs_1_softmax + epsilon * 0.5
        
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = int(state[trial])
        
        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2_softmax = exp_q2 / np.sum(exp_q2)
        
        # Mixture with Random Policy (Lapse)
        probs_2 = (1 - epsilon) * probs_2_softmax + epsilon * 0.5
        
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning ---
        chosen_a1 = action_1[trial]
        chosen_a2 = action_2[trial]
        r = reward[trial]
        
        # Stage 1 Update (TD-0)
        # Note: Standard hybrid models often use the stage 2 Q-value as the target
        # for the stage 1 MF update.
        delta_stage1 = q_stage2_mf[state_idx, chosen_a2] - q_stage1_mf[chosen_a1]
        q_stage1_mf[chosen_a1] += learning_rate * delta_stage1
        
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[state_idx, chosen_a2]
        q_stage2_mf[state_idx, chosen_a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Learner with OCI-Driven Spreading Activation
This model hypothesizes that high OCI scores lead to over-generalization or "contamination" of beliefs. When the participant learns about a specific action in one context (Planet X), they also update the value of that same action in the *other* context (Planet Y), driven by OCI.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Learner with OCI-Driven Spreading Activation.
    
    Hypothesis: High OCI causes 'spreading activation' or generalization errors.
    When updating the Q-value for (State S, Action A), the agent also updates
    (State Not-S, Action A) by a fraction determined by OCI.
    
    Parameters:
    learning_rate: [0, 1] - Base learning rate.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - MB/MF weight.
    spread_factor: [0, 1] - Fraction of the update applied to the unvisited state, scaled by OCI.
    """
    learning_rate, beta, w, spread_factor = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = int(state[trial])
        
        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning ---
        chosen_a1 = action_1[trial]
        chosen_a2 = action_2[trial]
        r = reward[trial]
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, chosen_a2] - q_stage1_mf[chosen_a1]
        q_stage1_mf[chosen_a1] += learning_rate * delta_stage1
        
        # Stage 2 Update with Spreading Activation
        delta_stage2 = r - q_stage2_mf[state_idx, chosen_a2]
        
        # Update visited state
        q_stage2_mf[state_idx, chosen_a2] += learning_rate * delta_stage2
        
        # Update unvisited state (Generalization/Contamination)
        # High OCI -> Higher spread
        other_state_idx = 1 - state_idx
        spread_lr = learning_rate * (current_oci * spread_factor)
        spread_lr = np.clip(spread_lr, 0, learning_rate) # Bound spread influence
        
        q_stage2_mf[other_state_idx, chosen_a2] += spread_lr * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Learner with OCI-Based Rare Transition Discounting
This model hypothesizes that high OCI individuals are rigid in their causal beliefs and tend to discount information obtained from "rare" or unexpected events. If a rare transition occurs (e.g., Spaceship A goes to Planet Y), the learning rate for that trial is reduced proportionally to the OCI score.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Learner with OCI-Based Rare Transition Discounting.
    
    Hypothesis: High OCI individuals trust 'rare' transitions less.
    If the transition was rare (Action != State), the learning rate for the 
    Stage 1 update is discounted by a factor of OCI.
    
    Parameters:
    learning_rate: [0, 1] - Base learning rate.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - MB/MF weight.
    rare_discount: [0, 1] - How much OCI dampens learning from rare transitions.
    """
    learning_rate, beta, w, rare_discount = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = int(state[trial])
        
        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning ---
        chosen_a1 = action_1[trial]
        chosen_a2 = action_2[trial]
        r = reward[trial]
        
        # Determine if transition was Common or Rare
        # Given matrix [[0.7, 0.3], [0.3, 0.7]], Action 0->State 0 is Common.
        # Action 1->State 1 is Common.
        # Thus, if action_1 == state, it is Common.
        is_common = (chosen_a1 == state_idx)
        
        # Calculate Effective Learning Rate
        alpha_eff = learning_rate
        if not is_common:
            # Discount learning if rare, modulated by OCI
            discount = np.clip(current_oci * rare_discount, 0, 1)
            alpha_eff = learning_rate * (1.0 - discount)
        
        # Stage 1 Update (MF) using effective learning rate
        delta_stage1 = q_stage2_mf[state_idx, chosen_a2] - q_stage1_mf[chosen_a1]
        q_stage1_mf[chosen_a1] += alpha_eff * delta_stage1
        
        # Stage 2 Update (standard, as the outcome is direct)
        delta_stage2 = r - q_stage2_mf[state_idx, chosen_a2]
        q_stage2_mf[state_idx, chosen_a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```