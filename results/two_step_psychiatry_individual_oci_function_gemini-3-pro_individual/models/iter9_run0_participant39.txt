Here are three new cognitive models for the two-step task, incorporating OCI scores to explain individual variability in decision-making.

### Model 1: OCI-Modulated Random Lapse (Noise Injection)
This model hypothesizes that high OCI scores correlate with "lapses" in optimal decision-making, potentially due to intrusive thoughts, anxiety, or disengagement. It models this as an "epsilon-greedy" mixture, where the participant follows their value-based policy with probability $(1-\epsilon)$ and chooses randomly with probability $\epsilon$. The rate of these lapses is modulated by their OCI score.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Random Lapse (Noise Injection).
    
    Hypothesis: OCI score predicts the rate of random 'lapses' (epsilon) in choice.
    High OCI may lead to higher noise/randomness due to anxiety or distraction, 
    mixing the softmax policy with a uniform random distribution.
    
    Parameters:
    lr: [0, 1] Learning rate for value updates.
    beta: [0, 10] Inverse temperature for softmax.
    w: [0, 1] Mixing weight (Model-Based vs Model-Free).
    lapse_base: [0, 1] Baseline lapse rate (epsilon).
    lapse_oci_slope: [-1, 1] Modulation of lapse rate by OCI score.
    """
    lr, beta, w, lapse_base, lapse_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate epsilon (lapse rate) and clip to valid probability range [0, 1]
    epsilon = lapse_base + lapse_oci_slope * oci_score
    epsilon = np.clip(epsilon, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax policy
        exp_q1 = np.exp(beta * q_net)
        probs_softmax = exp_q1 / np.sum(exp_q1)
        
        # Mixture with uniform random (lapse)
        probs_1 = (1 - epsilon) * probs_softmax + epsilon * 0.5
        
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        a1 = int(action_1[trial])
        s_idx = int(state[trial])

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Updates ---
        # SARSA-like update for Stage 1 MF
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        # Reward prediction error update for Stage 2
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Fixed Side Bias
This model proposes that OCI symptoms (compulsivity, ritualized behavior) manifest as a fixed preference (bias) for one spaceship over the other, independent of the rewards received. This is a static bias added to the value of Action 0 (Spaceship A), where the magnitude and direction of the bias depend on the OCI score.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Modulated Fixed Side Bias.
    
    Hypothesis: OCI relates to a fixed, compulsive preference for a specific 
    option (e.g., Spaceship A) regardless of reward history. This models a 
    static 'spatial' or 'option' bias in Stage 1.
    
    Parameters:
    lr: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Mixing weight (MB vs MF).
    bias_base: [-2, 2] Baseline bias towards Spaceship A (Action 0).
    bias_oci_slope: [-5, 5] Modulation of bias by OCI score.
    """
    lr, beta, w, bias_base, bias_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate static bias
    side_bias = bias_base + bias_oci_slope * oci_score

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Apply Fixed Bias to Action 0
        q_net[0] += side_bias
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        a1 = int(action_1[trial])
        s_idx = int(state[trial])

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Dynamic Transition Learning
This model suggests that OCI affects how participants learn the structure of the world (the transition matrix). Instead of assuming a fixed 70/30 structure, the agent updates its internal estimate of the transition probabilities based on observed transitions. The learning rate for this structural updating is modulated by OCI. High OCI might correspond to "over-updating" (chasing noise in transitions) or rigidity.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Dynamic Transition Learning.
    
    Hypothesis: Participants do not assume a fixed 70/30 transition matrix but 
    learn it dynamically. OCI modulates the 'Transition Learning Rate', 
    determining how quickly they update their belief about spaceship-planet 
    transitions based on observation.
    
    Parameters:
    lr_val: [0, 1] Learning rate for reward values (Q-values).
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Mixing weight (MB vs MF).
    lr_trans_base: [0, 1] Baseline learning rate for transition probabilities.
    lr_trans_oci_slope: [-1, 1] Modulation of transition learning rate by OCI.
    """
    lr_val, beta, w, lr_trans_base, lr_trans_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate transition learning rate
    lr_trans = lr_trans_base + lr_trans_oci_slope * oci_score
    lr_trans = np.clip(lr_trans, 0.0, 1.0)

    # Initialize estimated transition matrix (start at truth 0.7/0.3)
    # Row 0: P(X|A), P(Y|A). Row 1: P(X|U), P(Y|U).
    # Note: State 0 is Planet X, State 1 is Planet Y.
    # Action 0 is A, Action 1 is U.
    # A -> X (0.7), U -> Y (0.7 implies U->X is 0.3)
    trans_est = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        
        # Use Dynamic Estimated Transition Matrix for MB
        q_stage1_mb = trans_est @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        a1 = int(action_1[trial])
        s_idx = int(state[trial])

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Value Updates ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr_val * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr_val * delta_stage2
        
        # --- Structure Learning (Transition Update) ---
        # Update the row corresponding to the chosen spaceship (a1)
        # We observed state s_idx. Target vector is 1 at s_idx, 0 elsewhere.
        target = np.zeros(2)
        target[s_idx] = 1.0
        
        # Delta rule update for the probability row
        trans_est[a1] += lr_trans * (target - trans_est[a1])
        
        # Normalize to ensure probabilities sum to 1 (numerical stability)
        trans_est[a1] /= np.sum(trans_est[a1])

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```