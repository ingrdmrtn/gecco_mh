Here are three new cognitive models as Python functions, incorporating OCI scores into a hybrid Model-Based/Model-Free reinforcement learning framework.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model with OCI-modulated Reward Sensitivity.
    
    Hypothesis: OCI affects how strongly participants perceive the magnitude 
    of rewards/punishments (subjective valuation), scaling the effective reward signal
    before it generates prediction errors.
    
    Parameters:
    learning_rate: [0, 1] Standard learning rate.
    beta: [0, 10] Inverse temperature (softmax).
    w: [0, 1] Weight for Model-Based (1) vs Model-Free (0) control.
    rho_base: [0, 5] Base reward sensitivity scaler.
    rho_oci_mod: [-2, 2] Modulation of sensitivity by OCI score.
    """
    learning_rate, beta, w, rho_base, rho_oci_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective reward sensitivity
    rho = rho_base + (rho_oci_mod * oci_score)
    rho = np.clip(rho, 0.0, 10.0) # Sensitivity must be non-negative

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action

    for trial in range(n_trials):
        # --- Policy for the first choice ---
        # Model-Based Value Calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value mixing
        q_net_s1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_s1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        s_idx = int(state[trial])
        a1 = int(action_1[trial])
        
        # --- Policy for the second choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        
        # Apply modulated sensitivity to the reward
        r_eff = reward[trial] * rho 
        
        # --- Action Value Updating ---
        # Stage 1 MF update (TD logic)
        delta_s1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_s1
        
        # Stage 2 MF update
        delta_s2 = r_eff - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_s2
        
        # Eligibility trace: Propagate Stage 2 RPE to Stage 1
        q_stage1_mf[a1] += learning_rate * delta_s2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model with OCI-modulated Subjective Transition Probability.
    
    Hypothesis: OCI scores correlate with a distorted internal model of the 
    environment's structure. High OCI might lead to a belief that transitions 
    are more deterministic (rigid) or more random (uncertain) than reality.
    
    Parameters:
    learning_rate: [0, 1] Standard learning rate.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Weight for Model-Based (1) vs Model-Free (0).
    trans_prob_base: [0.5, 1.0] Base belief of the 'common' transition probability.
    trans_prob_oci_mod: [-0.5, 0.5] Modulation of this belief by OCI.
    """
    learning_rate, beta, w, trans_prob_base, trans_prob_oci_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate subjective transition probability
    p_common = trans_prob_base + (trans_prob_oci_mod * oci_score)
    p_common = np.clip(p_common, 0.0, 1.0)
    p_rare = 1.0 - p_common
    
    # Construct the subjective transition matrix used for MB valuation
    transition_matrix = np.array([[p_common, p_rare], [p_rare, p_common]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Policy for the first choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        
        # MB values derived from subjective matrix
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_s1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_s1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        s_idx = int(state[trial])
        a1 = int(action_1[trial])
        
        # --- Policy for the second choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Action Value Updating ---
        delta_s1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_s1
        
        delta_s2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_s2
        
        q_stage1_mf[a1] += learning_rate * delta_s2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model with OCI-modulated Stage 2 Learning Rate.
    
    Hypothesis: OCI differentially affects learning from immediate outcomes (Stage 2)
    versus the update of the starting state values (Stage 1). This tests if OCI
    relates to hyper-learning or rigidity specifically in response to direct feedback.
    
    Parameters:
    lr_s1: [0, 1] Fixed learning rate for Stage 1 updates.
    lr_s2_base: [0, 1] Base learning rate for Stage 2 updates.
    lr_s2_oci_mod: [-1, 1] Modulation of Stage 2 LR by OCI.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Weight for Model-Based (1) vs Model-Free (0).
    """
    lr_s1, lr_s2_base, lr_s2_oci_mod, beta, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate modulated Stage 2 learning rate
    lr_s2 = lr_s2_base + (lr_s2_oci_mod * oci_score)
    lr_s2 = np.clip(lr_s2, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Policy for the first choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_s1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_s1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        s_idx = int(state[trial])
        a1 = int(action_1[trial])
        
        # --- Policy for the second choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Action Value Updating ---
        # Stage 1 uses fixed lr_s1
        delta_s1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr_s1 * delta_s1
        
        # Stage 2 uses modulated lr_s2
        delta_s2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr_s2 * delta_s2
        
        # Eligibility trace for Stage 1 typically decays, but here we apply the full s2 error
        # using the Stage 1 learning rate to maintain consistency with S1 plasticity.
        q_stage1_mf[a1] += lr_s1 * delta_s2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```