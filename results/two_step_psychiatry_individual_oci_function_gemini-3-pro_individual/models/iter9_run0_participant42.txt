Here are three new cognitive models implemented as Python functions, incorporating OCI scores to explain individual variability in the two-step task.

### Model 1: Fictive (Counterfactual) Learning
This model hypothesizes that individuals with high OCI scores engage in more "fictive" or counterfactual learning. At the second stage (the aliens), they update the value of the *unchosen* alien as if they had chosen it and received the opposite outcome (e.g., if chosen won, unchosen lost). This reflects a compulsive evaluation of "what might have been."

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid learner with Fictive (Counterfactual) Learning at Stage 2 modulated by OCI.
    
    Hypothesis: High OCI participants update the value of the unchosen alien
    based on the counterfactual outcome (1 - observed_reward), reflecting
    compulsive 'what-if' processing.
    
    Parameters:
    - learning_rate: [0, 1] Standard learning rate for chosen options.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight for Model-Based (1) vs Model-Free (0).
    - fictive_base: [0, 1] Base rate of fictive updating.
    - fictive_oci: [-1, 1] Modulation of fictive updating by OCI.
    """
    learning_rate, beta, w, fictive_base, fictive_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective fictive learning rate, bounded between 0 and 1
    fictive_lr = fictive_base + (fictive_oci * oci_score)
    fictive_lr = np.clip(fictive_lr, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # 2 planets x 2 aliens

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = int(state[trial]) # Planet reached

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        chosen_a2 = action_2[trial]
        unchosen_a2 = 1 - chosen_a2
        r = reward[trial]

        # 1. Update Stage 2 Chosen (Standard)
        rpe_2 = r - q_stage2_mf[state_idx, chosen_a2]
        q_stage2_mf[state_idx, chosen_a2] += learning_rate * rpe_2

        # 2. Update Stage 2 Unchosen (Fictive)
        # Assume unchosen would have yielded opposite reward (1-r)
        fictive_r = 1.0 - r
        rpe_fictive = fictive_r - q_stage2_mf[state_idx, unchosen_a2]
        q_stage2_mf[state_idx, unchosen_a2] += fictive_lr * rpe_fictive

        # 3. Update Stage 1
        # TD(0) update using the Stage 2 value of the chosen action
        delta_stage1 = q_stage2_mf[state_idx, chosen_a2] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Side Bias (Spatial Preference)
This model hypothesizes that OCI scores correlate with a fixed bias towards one of the Stage 1 options (e.g., Spaceship A), independent of reward history. This represents a "default" or "safety" behavior where high-symptom individuals might habitually check the first option or have a spatial preference.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid learner with Stage 1 Side Bias modulated by OCI.
    
    Hypothesis: OCI scores correlate with an intrinsic bias towards Spaceship 0 (A),
    representing a spatial habit or default safety behavior independent of value.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight for Model-Based (1) vs Model-Free (0).
    - bias_base: [-5, 5] Base bias towards Spaceship 0.
    - bias_oci: [-5, 5] Modulation of bias by OCI.
    """
    learning_rate, beta, w, bias_base, bias_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective bias
    effective_bias = bias_base + (bias_oci * oci_score)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Apply bias to the logits (before softmax)
        logits = beta * q_net
        logits[0] += effective_bias # Add bias to Spaceship 0
        
        exp_q1 = np.exp(logits)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = int(state[trial])

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        chosen_a2 = action_2[trial]
        r = reward[trial]

        # Update Stage 2
        rpe_2 = r - q_stage2_mf[state_idx, chosen_a2]
        q_stage2_mf[state_idx, chosen_a2] += learning_rate * rpe_2

        # Update Stage 1
        delta_stage1 = q_stage2_mf[state_idx, chosen_a2] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Planet Stickiness (Transition Bias)
This model introduces a bias in the *Model-Based* system. Instead of using a static transition matrix, the agent's internal model of state transitions is biased towards the planet visited on the *previous* trial ("Planet Stickiness"). High OCI individuals might over-predict the recurrence of the recent environmental state (e.g., "I was just at Planet X, so I'll probably go there again").

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid learner with Planet Stickiness (Transition Bias) modulated by OCI.
    
    Hypothesis: High OCI participants bias their Model-Based planning towards 
    the planet visited on the previous trial, expecting state persistence.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight for Model-Based (1) vs Model-Free (0).
    - p_stick_base: [0, 1] Base tendency to predict the previous planet.
    - p_stick_oci: [-1, 1] Modulation of planet stickiness by OCI.
    """
    learning_rate, beta, w, p_stick_base, p_stick_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    stickiness_mag = p_stick_base + (p_stick_oci * oci_score)
    # Bound magnitude to ensure probabilities stay valid (loosely)
    stickiness_mag = np.clip(stickiness_mag, 0.0, 0.9)

    base_transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_planet = -1

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        
        # Modify transition matrix based on last planet visited
        current_transition = base_transition_matrix.copy()
        if last_planet != -1:
            # Add stickiness probability to the column corresponding to last_planet
            # and re-normalize rows
            current_transition[:, last_planet] += stickiness_mag
            row_sums = current_transition.sum(axis=1, keepdims=True)
            current_transition = current_transition / row_sums
        
        q_stage1_mb = current_transition @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = int(state[trial])
        last_planet = state_idx

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        chosen_a2 = action_2[trial]
        r = reward[trial]

        # Update Stage 2
        rpe_2 = r - q_stage2_mf[state_idx, chosen_a2]
        q_stage2_mf[state_idx, chosen_a2] += learning_rate * rpe_2

        # Update Stage 1
        delta_stage1 = q_stage2_mf[state_idx, chosen_a2] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```