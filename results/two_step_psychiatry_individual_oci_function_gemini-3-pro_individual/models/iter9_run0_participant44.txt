Here are three new cognitive models for the two-step decision task, incorporating the OCI score into different mechanisms:

### Model 1: Hybrid MB/MF with OCI-Modulated Stickiness
This model combines the standard Hybrid (Model-Based + Model-Free) architecture with a choice perseveration ("stickiness") bonus. The key hypothesis is that while the agent uses both goal-directed (MB) and habitual (MF) strategies, the tendency to compulsively repeat the previous choice (stickiness) is modulated by the OCI score.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model with OCI-modulated choice perseveration (stickiness).
    
    The agent computes a weighted average of Model-Based (MB) and Model-Free (MF) 
    values. A stickiness bonus is added to the net value of the previously 
    chosen action. The magnitude of this stickiness is determined by a base 
    level plus an OCI-dependent component.

    Parameters:
    - learning_rate: [0, 1] Update rate for MF Q-values.
    - beta: [0, 10] Inverse temperature (softmax randomness).
    - w: [0, 1] Weighting parameter (0=Pure MF, 1=Pure MB).
    - stickiness_base: [0, 5] Baseline tendency to repeat choice.
    - stickiness_oci_mod: [0, 5] How much OCI increases stickiness.
    """
    learning_rate, beta, w, stickiness_base, stickiness_oci_mod = model_parameters
    n_trials = len(action_1)
    oci_val = oci[0]
    
    # Calculate effective stickiness
    effective_stickiness = stickiness_base + (stickiness_oci_mod * oci_val)

    # Transition matrix (fixed for MB)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Initialize Q-values
    q_stage1_mf = np.zeros(2) # MF values for stage 1 (Spaceships)
    q_stage2_mf = np.zeros((2, 2)) # MF values for stage 2 (Aliens)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        # 1. Model-Based Value Calculation
        # Max value of each state in stage 2
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        # Expected value of stage 1 actions based on transitions
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Hybrid Value Calculation
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # 3. Add Stickiness
        if last_action_1 != -1:
            q_net[last_action_1] += effective_stickiness
            
        # Softmax choice 1
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Record choices and state
        a1 = action_1[trial]
        s2 = state[trial]
        last_action_1 = a1

        # --- Stage 2 Policy ---
        # Standard Softmax on Stage 2 MF values
        exp_q2 = np.exp(beta * q_stage2_mf[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]
        
        # --- Updates ---
        # Stage 2 Update (Direct Reinforcement)
        # delta_stage2 = r - Q(s2, a2)
        delta_stage2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += learning_rate * delta_stage2
        
        # Stage 1 Update (TD-Learning / SARSA-like)
        # Using the value of the state actually reached (or the Q of action taken)
        # Standard 2-step MF update often uses the Q-value of the chosen second stage action
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid MB/MF with OCI-Modulated Post-Loss Beta
This model hypothesizes that OCI affects how participants react to negative outcomes (receiving 0 coins). Specifically, it proposes that after a loss, the inverse temperature (`beta`) changes, representing a shift in exploration/exploitation strategy (e.g., becoming more erratic or more rigid). OCI modulates the magnitude of this shift.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model where OCI modulates the change in Beta (inverse temp)
    following a loss (0 reward).
    
    Hypothesis: High OCI scores might lead to 'frustrative' reactions, causing
    the agent to become more noisy (lower beta) or more rigid (higher beta) 
    immediately after failing to get a reward.

    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - w: [0, 1] MB/MF weighting.
    - beta_base: [0, 10] Baseline inverse temperature.
    - beta_loss_delta: [-5, 5] Change in beta after a loss (reward=0).
    - oci_loss_mod: [0, 5] How much OCI amplifies the beta change.
    """
    learning_rate, w, beta_base, beta_loss_delta, oci_loss_mod = model_parameters
    n_trials = len(action_1)
    oci_val = oci[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_reward = 1 # Start neutral/optimistic (assume previous win)

    for trial in range(n_trials):
        
        # Calculate current Beta based on previous outcome
        current_beta = beta_base
        if prev_reward == 0:
            # Apply modulation if previous trial was a loss
            mod = beta_loss_delta + (oci_val * oci_loss_mod)
            current_beta += mod
        
        # Ensure beta stays within reasonable positive bounds for stability
        current_beta = np.clip(current_beta, 0.0, 20.0)

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(current_beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        s2 = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(current_beta * q_stage2_mf[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]
        prev_reward = r # Store for next trial

        # --- Updates ---
        delta_stage2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid MB/MF with OCI-Modulated Learning Leak
This model introduces a "leak" or "spread" of learning in the Model-Free update. When the agent updates the value of the chosen spaceship, it also updates the unchosen spaceship by a smaller amount (in the same direction). OCI modulates the magnitude of this leak. This represents a failure to segregate options or a generalized "spread of effect" often seen in compulsive learning.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model with OCI-modulated 'Learning Leak'.
    
    When updating the Model-Free value of the chosen Stage 1 action, 
    the agent also updates the unchosen action by a fraction of the 
    prediction error. High OCI is hypothesized to increase this leak,
    representing a confusion or generalization between distinct options.

    Parameters:
    - learning_rate: [0, 1] Primary learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] MB/MF weighting.
    - leak_base: [0, 1] Baseline fraction of update applied to unchosen option.
    - leak_oci_mod: [0, 1] How much OCI increases the leak.
    """
    learning_rate, beta, w, leak_base, leak_oci_mod = model_parameters
    n_trials = len(action_1)
    oci_val = oci[0]
    
    # Calculate leak parameter
    leak_fraction = leak_base + (leak_oci_mod * oci_val)
    leak_fraction = np.clip(leak_fraction, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        s2 = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]

        # --- Updates ---
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += learning_rate * delta_stage2
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        
        # Update chosen action
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Leak Update to unchosen action
        unchosen_a1 = 1 - a1
        q_stage1_mf[unchosen_a1] += learning_rate * leak_fraction * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```