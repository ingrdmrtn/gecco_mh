def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Reinforcement Learning with OCI-modulated Choice Stickiness.
    
    This model adds a 'stickiness' parameter to the softmax decision rule.
    Stickiness captures the tendency to repeat the previous action regardless of
    value. Here, the magnitude of this stickiness is modulated by OCI, testing
    if obsessive-compulsive traits lead to repetitive, rigid behaviors.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - stick_base: [-5, 5] Baseline tendency to repeat choice (positive) or switch (negative).
    - stick_oci_mod: [-5, 5] How OCI modifies stickiness.
    """
    learning_rate, beta, stick_base, stick_oci_mod = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    stick_effective = stick_base + (stick_oci_mod * current_oci)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_a1 = -1 # No previous action for the first trial
    
    for trial in range(n_trials):

        qs = q_stage1_mf.copy()

        if last_a1 != -1:
            qs[last_a1] += stick_effective
            
        exp_q1 = np.exp(beta * qs)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        last_a1 = a1 # Update for next trial
        
        state_idx = int(state[trial])


        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        r = reward[trial]

        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss