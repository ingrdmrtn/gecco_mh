def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-modulated Learning Rate Asymmetry (Punishment Sensitivity).
    
    This model assumes that OCI scores affect how strongly the agent learns from 
    negative prediction errors (worse than expected outcomes) compared to positive ones.
    High OCI acts as a multiplier on the learning rate when the prediction error is negative.
    
    Parameters:
    - alpha_pos: [0, 1] Learning rate for positive prediction errors.
    - beta: [0, 10] Inverse temperature.
    - lambda_eligibility: [0, 1] Eligibility trace (how much Stage 2 reward updates Stage 1).
    - oci_neg_bias: [0, 5] Multiplier for learning rate when PE is negative, scaled by OCI.
    
    If delta < 0: effective_alpha = alpha_pos * (1 + oci_neg_bias * oci)
    If delta > 0: effective_alpha = alpha_pos
    """
    alpha_pos, beta, lambda_eligibility, oci_neg_bias = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):


        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        state_idx = int(state[trial])

        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        r = reward[trial]

        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]

        if delta_stage2 < 0:
            alpha_s2 = alpha_pos * (1.0 + (oci_neg_bias * oci_score))

            if alpha_s2 > 1.0: alpha_s2 = 1.0
        else:
            alpha_s2 = alpha_pos

        if delta_stage1 < 0:
            alpha_s1 = alpha_pos * (1.0 + (oci_neg_bias * oci_score))
            if alpha_s1 > 1.0: alpha_s1 = 1.0
        else:
            alpha_s1 = alpha_pos

        q_stage2_mf[state_idx, a2] += alpha_s2 * delta_stage2


        q_stage1_mf[a1] += alpha_s1 * delta_stage1 + (alpha_s1 * lambda_eligibility * delta_stage2)
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss