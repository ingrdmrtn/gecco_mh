def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-modulated Choice Perseveration (Stickiness).
    
    This model assumes that high OCI scores lead to 'stickiness' or behavioral rigidity.
    It implements a pure Model-Free learner but adds a perseveration bonus to the Q-values
    of the action taken on the previous trial. The magnitude of this bonus is determined
    by the OCI score.

    Bounds:
    learning_rate: [0,1]
    beta: [0,10]
    stickiness_base: [0,5] - Base tendency to repeat choices.
    oci_stickiness_mult: [0,5] - Additional stickiness multiplier based on OCI.
    """
    learning_rate, beta, stickiness_base, oci_stickiness_mult = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]


    stickiness = stickiness_base + (oci_stickiness_mult * oci_score)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    last_action_1 = -1 # Initialize with invalid action

    for trial in range(n_trials):


        q_eff_1 = q_stage1.copy()
        if last_action_1 != -1:
            q_eff_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_eff_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        curr_a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[curr_a1]
        
        curr_state = int(state[trial])


        exp_q2 = np.exp(beta * q_stage2[curr_state])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        curr_a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[curr_a2]

        r = reward[trial]


        pe_1 = q_stage2[curr_state, curr_a2] - q_stage1[curr_a1]
        q_stage1[curr_a1] += learning_rate * pe_1

        pe_2 = r - q_stage2[curr_state, curr_a2]
        q_stage2[curr_state, curr_a2] += learning_rate * pe_2

        last_action_1 = curr_a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss