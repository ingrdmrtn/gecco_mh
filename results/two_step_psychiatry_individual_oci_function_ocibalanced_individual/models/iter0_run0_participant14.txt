Here are three cognitive models designed to explain the participant's behavior, incorporating the OCI score to modulate learning and decision-making processes.

### Model 1: Hybrid Learner with OCI-modulated Model-Based Weight
This model assumes that individuals with lower OCI scores (like this participant) might rely more heavily on Model-Based (planning) strategies compared to Model-Free (habitual) strategies. The parameter `w` (mixing weight) determines the balance between these two systems, and this weight is dynamically adjusted by the participant's OCI score. A lower OCI increases the influence of the Model-Based system.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Reinforcement Learning Model with OCI-modulated Model-Based Weighting.
    
    This model combines Model-Free (MF) and Model-Based (MB) reinforcement learning.
    The balance between MB and MF control (w) is modulated by the OCI score.
    Lower OCI scores (indicating lower compulsivity) enhance the contribution of 
    the Model-Based system, reflecting flexible, goal-directed planning.
    
    Parameters:
    - learning_rate: Rate at which Q-values are updated [0, 1].
    - beta: Inverse temperature for softmax choice [0, 10].
    - w_base: Baseline mixing weight for Model-Based control [0, 1].
    """
    learning_rate, beta, w_base = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Transform OCI to modulate w. 
    # Lower OCI -> Higher w (more model-based). 
    # We constrain the effective w to be between 0 and 1.
    # If OCI is low (0.3), (1 - oci) is high (0.7), boosting the base weight.
    w = w_base * (1.0 - oci_score)
    w = np.clip(w, 0.0, 1.0)

    # Fixed transition matrix as described in task: A->X (0.7), U->Y (0.7)
    # Rows: Actions (A, U), Cols: States (X, Y)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)      # MF values for Stage 1 choices
    q_stage2_mf = np.zeros((2, 2)) # MF values for Stage 2 (State, Action)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- STAGE 1 CHOICE POLICY ---
        # Model-Based Value: T * max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value: w * MB + (1-w) * MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- STAGE 2 CHOICE POLICY ---
        # Standard Softmax on Stage 2 Q-values for the current state
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- UPDATING ---
        # Stage 2 RPE (Standard Q-learning)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 RPE (TD(0) update using Stage 2 value)
        # Note: In pure MF, we update based on the value of the state we landed in.
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-modulated Learning Rate Asymmetry
This model focuses on how compulsivity might affect the "stickiness" or rigidity of habits. It proposes that the learning rate is split into separate rates for positive and negative prediction errors. The OCI score modulates the learning rate for *negative* prediction errors. A low OCI score (low compulsivity) implies a higher sensitivity to negative feedback (unlearning bad habits faster), whereas high compulsivity might ignore negative feedback (perseveration).

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Reinforcement Learning with Asymmetric Learning Rates modulated by OCI.
    
    This model allows for different learning rates for positive (alpha_pos) 
    and negative (alpha_neg) prediction errors. 
    The OCI score modulates the negative learning rate. 
    Low OCI (healthy/low compulsivity) implies normal sensitivity to negative outcomes.
    
    Parameters:
    - alpha_pos: Learning rate for positive prediction errors [0, 1].
    - alpha_neg_base: Base learning rate for negative prediction errors [0, 1].
    - beta: Inverse temperature [0, 10].
    - lambd: Eligibility trace parameter connecting stage 2 to stage 1 [0, 1].
    """
    alpha_pos, alpha_neg_base, beta, lambd = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Modulate negative learning rate.
    # We model it such that higher OCI reduces alpha_neg (harder to unlearn).
    # Since this participant has low OCI (0.3), alpha_neg will be closer to base.
    # Formula: alpha_neg_effective = base / (1 + OCI)
    # Range check: if OCI is 0, alpha = base. If OCI is 1, alpha = base/2.
    alpha_neg = alpha_neg_base / (1.0 + oci_score)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- STAGE 1 CHOICE ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- STAGE 2 CHOICE ---
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- UPDATING ---
        # Stage 2 Update
        delta_2 = r - q_stage2[s_idx, a2]
        lr_2 = alpha_pos if delta_2 > 0 else alpha_neg
        q_stage2[s_idx, a2] += lr_2 * delta_2
        
        # Stage 1 Update (TD(1) / SARSA-like update with eligibility trace lambda)
        delta_1 = q_stage2[s_idx, a2] - q_stage1[a1]
        lr_1 = alpha_pos if delta_1 > 0 else alpha_neg
        
        # Update Stage 1 based on Stage 1 PE and Stage 2 PE passed back via lambda
        q_stage1[a1] += lr_1 * delta_1 + (lambd * lr_2 * delta_2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-modulated Exploration (Inverse Temperature)
This model posits that the OCI score relates to the exploration-exploitation trade-off. High compulsivity is often associated with rigid behavior (high beta/low randomness). Conversely, low OCI scores (like this participant) might indicate a more "relaxed" decision policy with higher exploration. Here, the `beta` parameter is directly scaled by the OCI score.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Based Learner with OCI-modulated Decision Noise (Beta).
    
    This model assumes a pure Model-Based planning strategy for the first stage.
    However, the decision noise (inverse temperature beta) is modulated by OCI.
    We hypothesize that lower OCI scores lead to lower beta values (more exploration/noise),
    while higher OCI leads to higher beta values (more deterministic/rigid choices).
    
    Parameters:
    - learning_rate: Update rate for values [0, 1].
    - beta_base: Baseline inverse temperature [0, 10].
    - oci_sensitivity: How strongly OCI affects beta [0, 5].
    """
    learning_rate, beta_base, oci_sensitivity = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Modulate Beta.
    # Effective Beta = beta_base * (1 + sensitivity * OCI)
    # If OCI is low (0.3), beta is closer to base.
    # If OCI is high, beta increases (becoming more rigid/greedy).
    beta = beta_base * (1.0 + (oci_sensitivity * oci_score))
    
    # Cap beta to prevent overflow issues, though bounds handle inputs
    beta = np.clip(beta, 0, 20) 

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Only tracking Stage 2 values, deriving Stage 1 via Model-Based calculation
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- STAGE 1 CHOICE (Pure Model-Based) ---
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- STAGE 2 CHOICE ---
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- UPDATING ---
        # Standard Q-learning for the second stage values
        delta = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += learning_rate * delta

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```