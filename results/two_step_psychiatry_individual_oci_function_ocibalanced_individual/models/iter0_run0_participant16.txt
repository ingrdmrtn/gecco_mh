Here are three cognitive models that incorporate the OCI (Obsessive-Compulsive Inventory) score to explain individual differences in decision-making within the two-step task.

### Model 1: OCI-Modulated Model-Based/Model-Free Balance
**Theory:** This model posits that higher OCI scores lead to a rigid reliance on Model-Based (habitual) control or, conversely, an over-reliance on explicit Model-Based planning due to anxiety about uncertainty. Here, we hypothesize that higher OCI scores shift the balance towards Model-Free (habitual) control, making behavior less sensitive to transition structures (the "w" parameter). The OCI score modulates the mixing weight `w` between Model-Based (MB) and Model-Free (MF) systems.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated MB/MF Balance.
    
    This model assumes that the balance between Model-Based (planning) and 
    Model-Free (habit) systems is influenced by the participant's OCI score.
    Higher OCI might lead to more habitual (MF) behavior (lower w).
    
    Parameters:
    lr: [0, 1] Learning rate for value updates.
    beta: [0, 10] Inverse temperature for softmax choice.
    w_base: [0, 1] Baseline mixing weight (0=Pure MF, 1=Pure MB).
    oci_sens: [0, 5] Sensitivity of the mixing weight to the OCI score.
    """
    lr, beta, w_base, oci_sens = model_parameters
    n_trials = len(action_1)
    oci = oci[0]
  
    # Fixed transition matrix as per task description
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2) # Model-free Q-values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Q-values for stage 2 (2 states, 2 actions)

    # Calculate effective w based on OCI
    # We model w such that higher OCI reduces model-based control (w gets smaller)
    # Using a sigmoid-like constraint or simple subtraction clipped at 0
    w_effective = w_base * (1.0 - (oci * oci_sens))
    # Clip w to be valid probability [0, 1]
    w_effective = np.clip(w_effective, 0.0, 1.0)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based Value Calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Q-value
        q_net = w_effective * q_stage1_mb + (1 - w_effective) * q_stage1_mf
        
        # Softmax Policy
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial]) # The planet we actually went to

        # --- Stage 2 Choice ---
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        # --- Learning ---
        # Update Stage 1 MF values (TD(1) logic or simple SARSA-like update)
        # Note: Standard two-step usually updates Stage 1 MF based on Stage 2 value
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += lr * delta_stage1
        
        # Update Stage 2 MF values
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += lr * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Driven Perseveration (Stickiness)
**Theory:** Obsessive-compulsive traits are often linked to repetitive behaviors or difficulty switching sets ("stuck in a set"). This model incorporates a "stickiness" parameter that biases the agent to repeat the previous action. The magnitude of this stickiness is directly scaled by the participant's OCI score.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Driven Perseveration.
    
    This model adds a choice autocorrelation (stickiness) bonus to the Q-values.
    The magnitude of this stickiness is modulated by the OCI score, testing the
    hypothesis that higher OCI leads to more repetitive choice patterns.
    
    Parameters:
    lr: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Mixing weight (MB vs MF).
    persev_base: [0, 5] Base perseveration bonus.
    oci_persev_mod: [0, 5] How strongly OCI amplifies perseveration.
    """
    lr, beta, w, persev_base, oci_persev_mod = model_parameters
    n_trials = len(action_1)
    oci = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Calculate total stickiness bonus
    # Higher OCI = Higher tendency to repeat previous choice
    stickiness = persev_base + (oci * oci_persev_mod)
    
    last_action_1 = -1 # Initialize as no previous action

    for trial in range(n_trials):

        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness bonus if we have a previous action
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])

        # --- Stage 2 Choice ---
        # Note: We are only applying stickiness to stage 1 for simplicity in this model,
        # as Stage 1 is the primary measure of planning vs habit.
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        # --- Learning ---
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += lr * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += lr * delta_stage2
        
        # Update history
        last_action_1 = int(action_1[trial])
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Learning Rate Asymmetry
**Theory:** Anxiety and compulsive symptoms are often associated with a negativity bias or altered sensitivity to punishment/lack of reward. This model allows for separate learning rates for positive and negative prediction errors (learning from success vs. failure). The OCI score modulates the learning rate specifically for negative outcomes (or lack of reward), hypothesizing that high OCI individuals might over-learn from failure (or lack of gold).

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Learning Rate Asymmetry.
    
    This model separates learning from positive outcomes (finding gold) and 
    neutral outcomes (no gold). It hypothesizes that OCI score modulates the 
    learning rate for unrewarded trials (lr_neg), reflecting altered sensitivity 
    to 'failure' or missing rewards.
    
    Parameters:
    lr_pos: [0, 1] Learning rate for rewarded trials (gold).
    lr_neg_base: [0, 1] Base learning rate for unrewarded trials (no gold).
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Mixing weight.
    oci_neg_mod: [0, 2] OCI multiplier for negative learning rate.
    """
    lr_pos, lr_neg_base, beta, w, oci_neg_mod = model_parameters
    n_trials = len(action_1)
    oci = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Calculate effective negative learning rate
    # Higher OCI increases learning from lack of reward (hypersensitivity to failure)
    lr_neg = lr_neg_base * (1.0 + oci * oci_neg_mod)
    lr_neg = np.clip(lr_neg, 0.0, 1.0)

    for trial in range(n_trials):

        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        state_idx = int(state[trial])

        # --- Stage 2 Choice ---
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        # --- Learning ---
        # Determine which learning rate to use based on reward
        current_lr = lr_pos if reward[trial] == 1 else lr_neg
        
        # Update Stage 1 MF
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += current_lr * delta_stage1
        
        # Update Stage 2 MF
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += current_lr * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```