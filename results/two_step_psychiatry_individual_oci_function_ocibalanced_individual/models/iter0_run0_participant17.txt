Here are three cognitive models that incorporate the participant's OCI score into the decision-making process in different theoretically motivated ways.

### Model 1: OCI-Modulated Model-Based/Model-Free Balance
**Theory:** This model hypothesizes that obsessive-compulsive traits influence the balance between goal-directed (Model-Based) and habitual (Model-Free) control. A higher OCI score might lead to a rigid over-reliance on habits (Model-Free) or, conversely, an anxious over-calculation of outcomes (Model-Based). Here, we model the mixing weight `w` as a logistic function of the OCI score, allowing the data to determine whether higher OCI pushes the participant towards MB or MF strategies.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: Hybrid Model-Based/Model-Free RL with OCI-modulated mixing weight.
    
    The balance between Model-Based (MB) and Model-Free (MF) control at the first stage
    is determined by a mixing weight `w`. This weight is derived from the OCI score,
    testing if symptom severity shifts the participant towards habits or goal-directed planning.
    
    Parameters:
    - learning_rate: [0, 1] Rate at which Q-values are updated.
    - beta: [0, 10] Inverse temperature for softmax choice.
    - w_slope: [0, 10] Sensitivity of the mixing weight to the OCI score.
    - w_intercept: [0, 1] Baseline bias for the mixing weight.
    """
    learning_rate, beta, w_slope, w_intercept = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate mixing weight w based on OCI (sigmoid transform to keep between 0 and 1)
    # If w is high -> More Model-Based. If w is low -> More Model-Free.
    w_logit = w_intercept + w_slope * (oci_score - 0.5) # Centering OCI roughly
    w = 1 / (1 + np.exp(-w_logit))

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2) # Model-free Q-values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Model-free Q-values for stage 2 (States X/Y, Actions L/R)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based value calculation: T * max(Q2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Q-value: Weighted sum of MB and MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # --- Stage 2 Policy ---
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Learning Updates ---
        # Prediction error for Stage 1 (SARSA-like logic for MF)
        chosen_a1 = int(action_1[trial])
        chosen_a2 = int(action_2[trial])
        r = reward[trial]
        
        # Update Stage 1 MF
        delta_stage1 = q_stage2_mf[state_idx, chosen_a2] - q_stage1_mf[chosen_a1]
        q_stage1_mf[chosen_a1] += learning_rate * delta_stage1
        
        # Update Stage 2 MF
        delta_stage2 = r - q_stage2_mf[state_idx, chosen_a2]
        q_stage2_mf[state_idx, chosen_a2] += learning_rate * delta_stage2
        
        # Note: Eligibility traces (lambda) are often used here to connect stage 2 reward 
        # back to stage 1, but for simplicity in this template constraint, 
        # we rely on the TD-chaining in the next trial via q_stage2_mf.

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Driven Stickiness (Perseveration)
**Theory:** Obsessive-compulsive symptoms are often characterized by repetition and difficulty switching sets. This model posits that the OCI score directly modulates a "stickiness" or perseveration parameter. A higher OCI score increases the probability of repeating the previous action, regardless of reward history, reflecting a compulsion to repeat.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: Model-Free RL with OCI-modulated choice stickiness (perseveration).
    
    The OCI score determines the magnitude of the 'stickiness' bonus. 
    Higher OCI scores lead to a higher tendency to repeat the previous choice 
    at Stage 1, independent of the reward received.
    
    Parameters:
    - learning_rate: [0, 1] Update rate.
    - beta: [0, 10] Inverse temperature.
    - stickiness_base: [0, 5] Baseline tendency to repeat choices.
    - oci_sens: [0, 5] How strongly OCI amplifies the stickiness.
    """
    learning_rate, beta, stickiness_base, oci_sens = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective stickiness based on OCI
    # Higher OCI = Higher tendency to repeat previous action
    effective_stickiness = stickiness_base + (oci_sens * oci_score)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1 # Initialize with no previous action

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        # Add stickiness bonus to the Q-value of the previously chosen action
        q_stage1_augmented = q_stage1_mf.copy()
        if last_action_1 != -1:
            q_stage1_augmented[last_action_1] += effective_stickiness
            
        exp_q1 = np.exp(beta * q_stage1_augmented)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        last_action_1 = int(action_1[trial])
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        # --- Learning Updates ---
        chosen_a1 = int(action_1[trial])
        chosen_a2 = int(action_2[trial])
        r = reward[trial]

        # TD Learning (MF)
        delta_stage1 = q_stage2_mf[state_idx, chosen_a2] - q_stage1_mf[chosen_a1]
        q_stage1_mf[chosen_a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[state_idx, chosen_a2]
        q_stage2_mf[state_idx, chosen_a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Learning Rate Asymmetry
**Theory:** Individuals with high OCI scores may exhibit altered sensitivity to positive versus negative feedback (perfectionism or fear of failure). This model implements dual learning rates (positive vs. negative prediction errors). The OCI score modulates the *ratio* or balance between these rates. Specifically, we model the hypothesis that higher OCI leads to "hyper-learning" from negative outcomes (punishment sensitivity) relative to positive ones.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: Asymmetric Learning Rates modulated by OCI.
    
    This model splits the learning rate into alpha_pos (for positive RPEs) 
    and alpha_neg (for negative RPEs). The OCI score scales the alpha_neg 
    parameter relative to a baseline, testing if OCI is associated with 
    higher sensitivity to worse-than-expected outcomes.
    
    Parameters:
    - alpha_base: [0, 1] Baseline learning rate.
    - beta: [0, 10] Inverse temperature.
    - oci_neg_bias: [0, 1] How much OCI boosts learning from negative errors.
    """
    alpha_base, beta, oci_neg_bias = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    # Define learning rates based on OCI
    # Positive learning rate is the baseline
    alpha_pos = alpha_base
    # Negative learning rate is boosted by OCI score (bounded at 1.0)
    alpha_neg = min(1.0, alpha_base + (oci_neg_bias * oci_score))

    for trial in range(n_trials):

        # --- Stage 1 Policy (Pure Model-Based for simplicity of this mechanism) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Learning Updates ---
        chosen_a1 = int(action_1[trial])
        chosen_a2 = int(action_2[trial])
        r = reward[trial]
        
        # We only update Stage 2 values here to isolate the specific effect of 
        # reward learning asymmetry (Stage 1 is computed MB from Stage 2).
        
        delta_stage2 = r - q_stage2_mf[state_idx, chosen_a2]
        
        # Apply asymmetric learning rates
        if delta_stage2 >= 0:
            eff_alpha = alpha_pos
        else:
            eff_alpha = alpha_neg
            
        q_stage2_mf[state_idx, chosen_a2] += eff_alpha * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```