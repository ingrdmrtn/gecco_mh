Here are three cognitive models designed to capture the behavior of a participant with medium OCI scores (0.6) in a two-step decision task. They vary in how the OCI score influences the balance between Model-Based (MB) and Model-Free (MF) learning, as well as perseveration (stickiness).

### Model 1: OCI-Modulated MB/MF Balance
This model hypothesizes that higher OCI scores lead to a shift in the balance between goal-directed (Model-Based) and habitual (Model-Free) control. Specifically, it posits that as OCI increases, the reliance on the Model-Based system decreases or becomes less flexible. The weighting parameter `w` is directly modulated by the OCI score.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free Reinforcement Learning with OCI-modulated Weighting.
    
    This model assumes that the balance between Model-Based (planning) and Model-Free (habitual)
    systems is influenced by the participant's OCI score. The mixing parameter 'w' is 
    dynamically adjusted based on OCI.
    
    Parameters:
    - learning_rate: Update rate for Q-values [0, 1].
    - beta: Inverse temperature for softmax choice [0, 10].
    - w_base: Base weighting for Model-Based control [0, 1].
    
    The effective weight (w_eff) is calculated as:
    w_eff = w_base * (1 - oci)
    
    A higher OCI score reduces the influence of the Model-Based system (w_eff decreases),
    reflecting a potential bias toward habitual responding in individuals with higher compulsivity.
    """
    learning_rate, beta, w_base = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Ensure inputs are integers for indexing
    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    
    # Initialize Q-values
    # Stage 1: 2 choices (Spaceships)
    q_stage1_mf = np.zeros(2)
    # Stage 2: 2 states (Planets) x 2 choices (Aliens)
    q_stage2_mf = np.zeros((2, 2))
    
    # Fixed transition matrix for Model-Based calculation
    # Row 0: Spaceship 0 -> [Planet 0, Planet 1]
    # Row 1: Spaceship 1 -> [Planet 0, Planet 1]
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Calculate effective weight based on OCI
    # If OCI is high, (1 - oci) is small, reducing the MB contribution.
    w_eff = w_base * (1.0 - current_oci)
    # Clip to ensure valid probability weighting
    w_eff = np.clip(w_eff, 0.0, 1.0)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based Value: Expected max value of next stage weighted by transition probs
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = w_eff * q_stage1_mb + (1 - w_eff) * q_stage1_mf
        
        # Softmax Policy
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Choice ---
        state_idx = state[trial]
        # Only MF values exist for Stage 2
        q_s2 = q_stage2_mf[state_idx]
        
        exp_q2 = np.exp(beta * q_s2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning / Updating ---
        # Stage 2 Update (TD Error)
        r = reward[trial]
        chosen_a2 = action_2[trial]
        delta_stage2 = r - q_stage2_mf[state_idx, chosen_a2]
        q_stage2_mf[state_idx, chosen_a2] += learning_rate * delta_stage2
        
        # Stage 1 Update (TD(0) - update towards value of state reached)
        # Note: In standard hybrid models, MF Q1 updates based on Q2 of the state actually reached
        chosen_a1 = action_1[trial]
        # Value of the state reached (Planet)
        v_state_reached = np.max(q_stage2_mf[state_idx]) 
        delta_stage1 = v_state_reached - q_stage1_mf[chosen_a1]
        q_stage1_mf[chosen_a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Driven Choice Stickiness (Perseveration)
This model posits that Obsessive-Compulsive traits manifest as "stickiness" or perseverationâ€”a tendency to repeat the previous choice regardless of reward history. Here, the OCI score scales a choice autocorrelation parameter (`stickiness`).

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Reinforcement Learning with OCI-modulated Choice Stickiness.
    
    This model focuses on the tendency to repeat previous choices (perseveration).
    It assumes that higher OCI scores lead to higher 'stickiness' in the first-stage choice,
    making the participant more resistant to switching strategies even after negative outcomes.
    
    Parameters:
    - learning_rate: Update rate for Q-values [0, 1].
    - beta: Inverse temperature for softmax choice [0, 10].
    - stick_param: Base stickiness magnitude [0, 5].
    
    The effective stickiness added to the Q-value of the previously chosen action is:
    stick_eff = stick_param * (1 + oci)
    
    This creates a bias towards repeating the last action taken at Stage 1.
    """
    learning_rate, beta, stick_param = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Variable to track the previous action for stickiness
    prev_action_1 = -1 
    
    # Calculate effective stickiness
    # OCI amplifies the base stickiness parameter
    stick_eff = stick_param * (1.0 + current_oci)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Add stickiness bonus to the Q-values temporarily for decision making
        q_net = q_stage1.copy()
        
        if prev_action_1 != -1:
            q_net[prev_action_1] += stick_eff
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Store current action as previous for next trial
        prev_action_1 = action_1[trial]
        
        # --- Stage 2 Choice ---
        state_idx = state[trial]
        q_s2 = q_stage2[state_idx]
        
        exp_q2 = np.exp(beta * q_s2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning ---
        r = reward[trial]
        chosen_a1 = action_1[trial]
        chosen_a2 = action_2[trial]
        
        # Stage 2 Update
        delta2 = r - q_stage2[state_idx, chosen_a2]
        q_stage2[state_idx, chosen_a2] += learning_rate * delta2
        
        # Stage 1 Update (SARSA-like logic or simply updating to state value)
        # Here we update Q1 based on the value of the state reached (common in these tasks)
        v_state_reached = np.max(q_stage2[state_idx])
        delta1 = v_state_reached - q_stage1[chosen_a1]
        q_stage1[chosen_a1] += learning_rate * delta1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Learning Rate Asymmetry
This model suggests that OCI scores affect how participants learn from positive versus negative prediction errors. Specifically, it tests the hypothesis that higher compulsivity might lead to hyper-learning from punishment (or lack of reward) or rigid learning from reward. Here, OCI splits the learning rate into separate components for positive and negative feedback.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Dual Learning Rate Model (Positive/Negative) Modulated by OCI.
    
    This model investigates if OCI affects sensitivity to reward vs. omission of reward.
    It uses two effective learning rates: one for positive prediction errors (better than expected)
    and one for negative prediction errors (worse than expected).
    
    Parameters:
    - alpha_base: Base learning rate [0, 1].
    - beta: Inverse temperature [0, 10].
    - asymmetry: Parameter controlling the bias towards negative learning [0, 1].
    
    Logic:
    alpha_pos = alpha_base
    alpha_neg = alpha_base * (1 + asymmetry * oci)
    
    If asymmetry > 0 and OCI is high, the participant learns more aggressively from 
    negative outcomes (disappointments) than positive ones.
    """
    alpha_base, beta, asymmetry = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    
    # Pure Model-Free setup for simplicity to isolate learning rate effects
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Calculate effective learning rates
    # alpha_pos is the standard base rate
    alpha_pos = alpha_base
    # alpha_neg is boosted by OCI and the asymmetry parameter
    alpha_neg = alpha_base * (1.0 + asymmetry * current_oci)
    
    # Clip to ensure stability
    alpha_pos = np.clip(alpha_pos, 0.0, 1.0)
    alpha_neg = np.clip(alpha_neg, 0.0, 1.0)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Choice ---
        state_idx = state[trial]
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning ---
        r = reward[trial]
        chosen_a1 = action_1[trial]
        chosen_a2 = action_2[trial]
        
        # Stage 2 Update
        delta2 = r - q_stage2[state_idx, chosen_a2]
        lr2 = alpha_pos if delta2 >= 0 else alpha_neg
        q_stage2[state_idx, chosen_a2] += lr2 * delta2
        
        # Stage 1 Update
        # Using the value of the state reached to update stage 1
        v_state_reached = np.max(q_stage2[state_idx])
        delta1 = v_state_reached - q_stage1[chosen_a1]
        lr1 = alpha_pos if delta1 >= 0 else alpha_neg
        q_stage1[chosen_a1] += lr1 * delta1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```