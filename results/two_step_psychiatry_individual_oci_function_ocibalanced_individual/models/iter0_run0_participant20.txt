Here are three cognitive models that incorporate the participant's OCI score into the decision-making process in different ways.

### Model 1: OCI-Modulated Model-Based/Model-Free Balance
This model hypothesizes that individuals with higher OCI scores rely more heavily on Model-Based (habitual/rigid) control rather than Model-Free (flexible) control. The mixing parameter `w` is directly modulated by the OCI score.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free RL with OCI-modulated weighting.
    
    Hypothesis: Higher OCI scores lead to a stronger reliance on Model-Based planning 
    (or conversely, stickiness to a model), shifting the balance 'w'.
    
    Parameters:
    learning_rate: [0, 1] - Rate of updating Q-values.
    beta: [0, 10] - Inverse temperature for softmax choice.
    w_base: [0, 1] - Baseline weighting for Model-Based control (0=MF, 1=MB).
    w_oci_slope: [0, 1] - How much OCI increases the Model-Based weight.
    """
    learning_rate, beta, w_base, w_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective mixing weight w, bounded between 0 and 1
    # People with higher OCI might over-rely on the transition structure (MB)
    w = w_base + (w_oci_slope * oci_score)
    w = np.clip(w, 0.0, 1.0)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)      # Model-free values for stage 1 (Actions 0, 1)
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for stage 2 (States 0, 1; Actions 0, 1)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based valuation: Q_MB(s1, a1) = Sum(P(s2|s1,a1) * max(Q(s2, a2)))
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated value: w * MB + (1-w) * MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax selection
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # Observe transition
        state_idx = int(state[trial]) # The planet we actually landed on

        # --- Stage 2 Policy ---
        # Standard Model-Free Q-learning at the second stage
        qs_stage2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_stage2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Learning / Updating ---
        # 1. Update Stage 2 values based on reward
        # Prediction error: R - Q(s2, a2)
        a2 = int(action_2[trial])
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # 2. Update Stage 1 MF values (TD(1) or direct update)
        # Prediction error: Q(s2, a2) - Q(s1, a1)
        a1 = int(action_1[trial])
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Driven Perseveration (Stickiness)
This model hypothesizes that OCI is related to behavioral rigidity or "stickiness." Instead of altering the learning strategy, OCI increases the tendency to repeat the previous action regardless of the reward outcome (perseveration).

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free RL with OCI-modulated choice perseverance (stickiness).
    
    Hypothesis: Higher OCI scores lead to higher 'stickiness' (repeating the previous choice),
    representing compulsive repetition.
    
    Parameters:
    learning_rate: [0, 1] - Rate of updating Q-values.
    beta: [0, 10] - Inverse temperature.
    perseveration_base: [0, 5] - Base tendency to repeat the last choice.
    oci_pers_scale: [0, 5] - Scaling factor for OCI's effect on perseveration.
    """
    learning_rate, beta, perseveration_base, oci_pers_scale = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective perseveration bonus
    pers_weight = perseveration_base + (oci_pers_scale * oci_score)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1 # No previous action for the first trial

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        q_net = q_stage1_mf.copy()
        
        # Add perseveration bonus if it's not the first trial
        if last_action_1 != -1:
            q_net[last_action_1] += pers_weight
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        # Standard softmax on stage 2 values
        qs_stage2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_stage2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        # --- Updating ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        
        # SARSA-style update for Stage 1 (TD(0))
        # Using the value of the state chosen in stage 2 to update stage 1
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Update Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Learning Rate Asymmetry
This model suggests that OCI affects how participants process positive versus negative feedback. Specifically, it tests if OCI modulates the learning rate for negative prediction errors (punishment/lack of reward), reflecting a potential hypersensitivity to failure or "checking" behavior driven by anxiety.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Dual Learning Rate Model (Positive/Negative) modulated by OCI.
    
    Hypothesis: OCI affects sensitivity to negative outcomes. The learning rate for 
    negative prediction errors (alpha_neg) is modulated by the OCI score.
    
    Parameters:
    alpha_pos: [0, 1] - Learning rate for positive prediction errors.
    alpha_neg_base: [0, 1] - Base learning rate for negative prediction errors.
    alpha_oci_mod: [0, 1] - How much OCI increases learning from negative errors.
    beta: [0, 10] - Inverse temperature.
    """
    alpha_pos, alpha_neg_base, alpha_oci_mod, beta = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Calculate effective negative learning rate
    alpha_neg = alpha_neg_base + (alpha_oci_mod * oci_score)
    alpha_neg = np.clip(alpha_neg, 0.0, 1.0)

    for trial in range(n_trials):
        # --- Stage 1 Policy (Pure Model-Based for simplicity in this variant) ---
        # Using a simplified MB integration to focus complexity on the learning rates
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        qs_stage2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_stage2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        # --- Updating ---
        a2 = int(action_2[trial])
        
        # Calculate Prediction Error
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        
        # Apply asymmetric learning rates
        if delta_stage2 >= 0:
            eff_lr = alpha_pos
        else:
            eff_lr = alpha_neg
            
        q_stage2_mf[state_idx, a2] += eff_lr * delta_stage2
        
        # Note: In this specific simplified MB model, we don't update Q_stage1_MF
        # because the agent is purely planning based on Stage 2 values.
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```