Here are three cognitive models designed to explain the participant's behavior, incorporating their Obsessive-Compulsive Inventory (OCI) score. The models hypothesize different ways high OCI scores might influence reinforcement learning, specifically focusing on habit formation, model-based vs. model-free control, and learning rates.

### Model 1: OCI-Modulated Model-Based/Model-Free Balance
This model hypothesizes that OCI scores influence the balance between goal-directed (Model-Based) and habitual (Model-Free) control. Specifically, higher OCI scores might lead to a stronger reliance on habitual, model-free learning (represented by the weighting parameter `w`), making behavior less sensitive to transition structures.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-modulated Model-Based/Model-Free trade-off.
    
    Hypothesis: Higher OCI scores bias the participant towards Model-Free (habitual) control 
    during the first stage choice. The parameter 'w_base' is the baseline weighting, 
    and 'w_oci_slope' determines how much the OCI score reduces Model-Based control.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for value updates.
    - beta: [0, 10] Inverse temperature for softmax choice.
    - w_base: [0, 1] Baseline weight for Model-Based control (1 = full MB, 0 = full MF).
    - w_oci_slope: [0, 1] Sensitivity of the MB/MF weight to the OCI score.
      (Effective w = w_base - w_oci_slope * OCI).
    """
    learning_rate, beta, w_base, w_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate the effective mixing weight w based on OCI
    # We clip to ensure w stays within [0, 1]
    # Hypothesis: Higher OCI reduces w (less Model-Based, more Model-Free)
    w = w_base - (w_oci_slope * oci_score)
    if w < 0: w = 0
    if w > 1: w = 1

    # Fixed transition matrix (Common: 0.7, Rare: 0.3)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # q_stage1_mf: Model-Free values for stage 1 (2 actions)
    q_stage1_mf = np.zeros(2) # Initialized at 0 (or 0.5)
    
    # q_stage2_mf: Model-Free values for stage 2 (2 states x 2 actions)
    q_stage2_mf = np.zeros((2, 2)) 

    for trial in range(n_trials):
        
        # --- Stage 1 Policy ---
        # Model-Based Value: Transition prob * Max Q of next stage
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Value: Weighted sum of MB and MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax for Stage 1
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Transition ---
        state_idx = int(state[trial]) # 0 or 1
        a1 = int(action_1[trial])
        
        # --- Stage 2 Policy ---
        # Standard Model-Free choice at Stage 2
        qs_stage2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_stage2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        r = reward[trial]
        
        # TD Error Stage 2
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # TD Error Stage 1 (SARSA-style update using Stage 2 value)
        # Note: In standard Daw 2011, Stage 1 MF is updated by Stage 2 Q-value
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Dependent Perseveration (Stickiness)
This model focuses on the tendency to repeat choices regardless of reward history. It hypothesizes that individuals with higher OCI scores exhibit higher "choice stickiness" or perseveration, reflecting the repetitive nature of compulsive symptoms.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-modulated Choice Stickiness (Perseveration).
    
    Hypothesis: Higher OCI scores lead to higher choice stickiness (perseveration)
    at the first stage. This captures the repetitive nature of compulsivity,
    making the participant more likely to repeat the previous action regardless of outcome.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight (MB vs MF).
    - stickiness_base: [0, 5] Baseline tendency to repeat the previous choice.
    - stickiness_oci: [0, 5] Additional stickiness scaling with OCI score.
    """
    learning_rate, beta, w, stickiness_base, stickiness_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective stickiness
    # Stickiness adds a bonus to the previously chosen action
    phi = stickiness_base + (stickiness_oci * oci_score)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1 # Indicator for previous action

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness bonus to q_net
        q_net_sticky = q_net.copy()
        if last_action_1 != -1:
            q_net_sticky[last_action_1] += phi
            
        exp_q1 = np.exp(beta * q_net_sticky)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        last_action_1 = a1

        # --- Stage 2 Policy ---
        qs_stage2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_stage2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        r = reward[trial]
        
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Dependent Learning Rate Asymmetry
This model suggests that OCI scores affect how participants learn from positive versus negative prediction errors. It hypothesizes that higher OCI is associated with an increased sensitivity to negative outcomes (or lack of reward), modeled as a separate learning rate for negative prediction errors that scales with the OCI score.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-modulated Learning Rate Asymmetry (Pos/Neg).
    
    Hypothesis: Higher OCI scores correlate with altered sensitivity to prediction errors.
    Here, we define a base learning rate for positive errors (alpha_pos) and allow
    the learning rate for negative errors (alpha_neg) to be modulated by OCI.
    This could reflect hypersensitivity to failure or "checking" behavior driven by punishment avoidance.
    
    Parameters:
    - alpha_pos: [0, 1] Learning rate for positive prediction errors (RPE > 0).
    - alpha_neg_base: [0, 1] Base learning rate for negative prediction errors (RPE < 0).
    - alpha_neg_oci: [0, 1] How much OCI increases the negative learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight.
    """
    alpha_pos, alpha_neg_base, alpha_neg_oci, beta, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective negative learning rate
    alpha_neg = alpha_neg_base + (alpha_neg_oci * oci_score)
    if alpha_neg > 1: alpha_neg = 1
    if alpha_neg < 0: alpha_neg = 0

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # --- Stage 2 Policy ---
        qs_stage2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_stage2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        r = reward[trial]
        
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        lr_2 = alpha_pos if delta_stage2 >= 0 else alpha_neg
        q_stage2_mf[state_idx, a2] += lr_2 * delta_stage2
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        lr_1 = alpha_pos if delta_stage1 >= 0 else alpha_neg
        q_stage1_mf[a1] += lr_1 * delta_stage1
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```