Here are three cognitive models designed to explain the participant's behavior in the two-step task, incorporating their OCI (Obsessive-Compulsive Inventory) score.

### Model 1: OCI-Modulated Model-Based/Model-Free Balance
This model hypothesizes that the balance between goal-directed (Model-Based) and habitual (Model-Free) control is influenced by obsessive-compulsive traits. Specifically, literature suggests that higher compulsivity is associated with a deficit in model-based control or an over-reliance on habitual control. Here, the `w` parameter (weighting between MB and MF) is modulated by the OCI score.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free Reinforcement Learning with OCI-modulated weighting.
    
    Hypothesis: The OCI score influences the balance (w) between Model-Based (planning)
    and Model-Free (habitual) systems. Higher OCI might reduce model-based control.
    
    Parameters:
    - learning_rate: [0, 1] Speed of updating Q-values.
    - beta: [0, 10] Inverse temperature for softmax choice.
    - w_base: [0, 1] Baseline weight for Model-Based control (0=Pure MF, 1=Pure MB).
    - oci_sens: [0, 1] Sensitivity of the weighting parameter to the OCI score.
    """
    learning_rate, beta, w_base, oci_sens = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate effective weight w based on OCI
    # We hypothesize higher OCI reduces MB control (reduces w).
    # We clip to ensure w stays in [0, 1]
    w = w_base * (1.0 - (current_oci * oci_sens))
    if w < 0: w = 0
    if w > 1: w = 1
    
    # Transition matrix: A->X (0->0) is 0.7, A->Y (0->1) is 0.3, etc.
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)       # Model-free values for stage 1 (Spaceships)
    q_stage2_mf = np.zeros((2, 2))  # Model-free values for stage 2 (Aliens per Planet)
    
    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based Value: Expected max value of next stage weighted by transition probs
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial]) # 0 for Planet X, 1 for Planet Y
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Learning ---
        # Prediction errors
        r = reward[trial]
        chosen_s1 = int(action_1[trial])
        chosen_s2 = int(action_2[trial])
        
        # Stage 2 Update (Standard Q-learning)
        delta_stage2 = r - q_stage2_mf[state_idx, chosen_s2]
        q_stage2_mf[state_idx, chosen_s2] += learning_rate * delta_stage2
        
        # Stage 1 Update (TD-learning using Stage 2 value as proxy for reward)
        # Note: In pure MF, we update s1 based on s2 value (SARSA-like or Q-learning)
        delta_stage1 = q_stage2_mf[state_idx, chosen_s2] - q_stage1_mf[chosen_s1]
        q_stage1_mf[chosen_s1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Driven Perseveration (Stickiness)
This model posits that obsessive-compulsive symptoms manifest as "stickiness" or perseverationâ€”a tendency to repeat the previous action regardless of reward history. Here, the OCI score directly scales the perseveration parameter, making high-OCI individuals more likely to repeat their Stage 1 choice.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free RL with OCI-modulated Choice Perseveration (Stickiness).
    
    Hypothesis: OCI is linked to repetitive behaviors. This model adds a 'stickiness'
    bonus to the previously chosen action in Stage 1, scaled by the OCI score.
    
    Parameters:
    - learning_rate: [0, 1] Speed of updating Q-values.
    - beta: [0, 10] Inverse temperature.
    - stickiness_base: [0, 5] Base tendency to repeat the last choice.
    - oci_amp: [0, 5] Amplification of stickiness by OCI score.
    """
    learning_rate, beta, stickiness_base, oci_amp = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate effective stickiness
    eff_stickiness = stickiness_base + (current_oci * oci_amp)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1 # No previous action for first trial
    
    for trial in range(n_trials):
        
        # --- Stage 1 Choice ---
        q_s1_modified = q_stage1_mf.copy()
        
        # Add stickiness bonus if not the first trial
        if last_action_1 != -1:
            q_s1_modified[last_action_1] += eff_stickiness
            
        exp_q1 = np.exp(beta * q_s1_modified)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Learning ---
        r = reward[trial]
        chosen_s1 = int(action_1[trial])
        chosen_s2 = int(action_2[trial])
        
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[state_idx, chosen_s2]
        q_stage2_mf[state_idx, chosen_s2] += learning_rate * delta_stage2
        
        # Stage 1 Update (TD(0))
        delta_stage1 = q_stage2_mf[state_idx, chosen_s2] - q_stage1_mf[chosen_s1]
        q_stage1_mf[chosen_s1] += learning_rate * delta_stage1
        
        # Store action for next trial's stickiness
        last_action_1 = chosen_s1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Enhanced Learning Rate Asymmetry
Some research suggests psychiatric traits can influence how individuals learn from positive versus negative prediction errors. This model suggests that OCI scores modulate the learning rate specifically for negative prediction errors (disappointments), reflecting a potential hypersensitivity to failure or "checking" behavior driven by negative outcomes.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Reinforcement Learning with Asymmetric Learning Rates modulated by OCI.
    
    Hypothesis: High OCI individuals may over-weight negative prediction errors
    (learning more from lack of reward than receipt of reward).
    
    Parameters:
    - alpha_pos: [0, 1] Learning rate for positive prediction errors (RPE > 0).
    - alpha_neg_base: [0, 1] Base learning rate for negative prediction errors.
    - beta: [0, 10] Inverse temperature.
    - oci_neg_mod: [0, 1] How much OCI increases learning from negative outcomes.
    """
    alpha_pos, alpha_neg_base, beta, oci_neg_mod = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate effective negative learning rate
    # Higher OCI increases alpha_neg (up to 1.0)
    alpha_neg = alpha_neg_base + (current_oci * oci_neg_mod)
    if alpha_neg > 1.0: alpha_neg = 1.0
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):

        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Learning ---
        r = reward[trial]
        chosen_s1 = int(action_1[trial])
        chosen_s2 = int(action_2[trial])
        
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[state_idx, chosen_s2]
        
        # Apply asymmetric learning rate based on sign of delta
        curr_alpha_2 = alpha_pos if delta_stage2 >= 0 else alpha_neg
        q_stage2_mf[state_idx, chosen_s2] += curr_alpha_2 * delta_stage2
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, chosen_s2] - q_stage1_mf[chosen_s1]
        
        # Apply asymmetric learning rate based on sign of delta
        curr_alpha_1 = alpha_pos if delta_stage1 >= 0 else alpha_neg
        q_stage1_mf[chosen_s1] += curr_alpha_1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```