Here are three cognitive models designed to explain the participant's behavior, incorporating their OCI score into the decision-making process.

### Model 1: OCI-Modulated Model-Based/Model-Free Balance
This model posits that the balance between goal-directed (Model-Based) and habitual (Model-Free) control is influenced by the OCI score. Specifically, higher OCI scores might push the participant towards more habitual (Model-Free) behavior, reducing the weight `w` placed on the Model-Based system. This reflects the theory that compulsivity is linked to over-reliance on habit systems.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free Reinforcement Learning model where the mixing weight 'w'
    is modulated by the OCI score.
    
    Hypothesis: Higher OCI scores lead to reduced Model-Based control (lower w).
    
    Bounds:
    learning_rate: [0,1] - Speed of value updating.
    beta: [0,10] - Inverse temperature (exploration vs exploitation).
    w_base: [0,1] - Baseline weight for Model-Based control.
    oci_sensitivity: [0, 5] - How strongly OCI reduces the model-based weight.
    """
    learning_rate, beta, w_base, oci_sensitivity = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0] # Extract scalar OCI score
  
    # Define fixed transition matrix: Row 0 -> [to Planet 0, to Planet 1]
    # Ideally, this should be learned, but usually fixed in simple hybrid models.
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)       # Model-Free values for stage 1 (Spaceships)
    q_stage2_mf = np.zeros((2, 2))  # Model-Free values for stage 2 (Aliens per Planet)

    # Calculate the effective mixing weight w based on OCI
    # We constrain w to be between 0 and 1.
    # Higher OCI -> subtracts more from w_base, pushing towards Model-Free (0).
    w = w_base - (oci_sensitivity * current_oci)
    if w < 0: w = 0
    if w > 1: w = 1

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        
        # 1. Model-Based Value Calculation (Bellman equation using transition matrix)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Hybrid Value Calculation
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # 3. Action Selection (Softmax)
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        # Store probability of the chosen action
        # Ensure indices are integers
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # Transition to state (planet)
        state_idx = int(state[trial])

        # --- Stage 2 Decision ---
        
        # 1. Action Selection (Softmax on Stage 2 MF values)
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning / Updating ---
        
        r = reward[trial]
        
        # 1. Stage 2 Update (TD Error)
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # 2. Stage 1 Update (TD Error using Stage 2 value as proxy for reward)
        # Note: In a full TD(lambda) model, eligibility traces are used. 
        # Here we use a simple SARSA-like update or Q-learning update for the MF component.
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Driven Perseveration (Stickiness)
This model assumes the core learning mechanism is a standard hybrid model, but the OCI score drives "choice stickiness" or perseveration. Individuals with higher OCI scores (compulsive tendencies) might be more likely to repeat their previous action regardless of the reward outcome, reflecting a "stuck" behavioral loop.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL model with Choice Stickiness (Perseveration) modulated by OCI.
    
    Hypothesis: Higher OCI scores increase the 'stickiness' parameter, causing 
    the participant to repeat the previous Stage 1 choice regardless of value.
    
    Bounds:
    learning_rate: [0,1]
    beta: [0,10]
    w: [0,1] - Fixed mixing weight.
    stickiness_factor: [0, 5] - Multiplier for OCI to determine total stickiness.
    """
    learning_rate, beta, w, stickiness_factor = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Calculate effective stickiness based on OCI
    phi = stickiness_factor * current_oci 

    last_action_1 = -1 # Initialize with no previous action

    for trial in range(n_trials):

        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Add stickiness bonus to the Q-value of the previously chosen action
        q_net_sticky = q_net.copy()
        if last_action_1 != -1:
            q_net_sticky[last_action_1] += phi

        exp_q1 = np.exp(beta * q_net_sticky)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # Update last action
        last_action_1 = a1
        
        state_idx = int(state[trial])

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
  
        # --- Updating ---
        r = reward[trial]
        
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Learning Rate Asymmetry
This model suggests that OCI affects how participants learn from positive versus negative prediction errors. It posits that higher OCI scores might lead to a heightened sensitivity to negative outcomes (punishment/loss of reward) or a rigidity where learning rates differ significantly for "good" vs "bad" surprises. Here, we model OCI as amplifying the learning rate specifically for negative prediction errors (disappointment).

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free RL model (w=0 implicit) where OCI modulates the learning rate 
    specifically for negative prediction errors.
    
    Hypothesis: Participants with higher OCI are more sensitive to negative outcomes,
    having a higher learning rate when the prediction error is negative.
    
    Bounds:
    alpha_pos: [0,1] - Learning rate for positive prediction errors.
    alpha_neg_base: [0,1] - Base learning rate for negative prediction errors.
    beta: [0,10]
    oci_boost: [0, 2] - How much OCI adds to the negative learning rate.
    """
    alpha_pos, alpha_neg_base, beta, oci_boost = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Calculate effective negative learning rate
    # We clamp it to max 1.0
    alpha_neg = alpha_neg_base + (oci_boost * current_oci)
    if alpha_neg > 1.0: alpha_neg = 1.0

    for trial in range(n_trials):

        # --- Stage 1 Decision (Pure Model-Free) ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        state_idx = int(state[trial])

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
  
        # --- Updating ---
        r = reward[trial]
        
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        
        if delta_stage2 >= 0:
            eff_alpha_2 = alpha_pos
        else:
            eff_alpha_2 = alpha_neg
            
        q_stage2_mf[state_idx, a2] += eff_alpha_2 * delta_stage2
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        
        if delta_stage1 >= 0:
            eff_alpha_1 = alpha_pos
        else:
            eff_alpha_1 = alpha_neg
            
        q_stage1_mf[a1] += eff_alpha_1 * delta_stage1
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```