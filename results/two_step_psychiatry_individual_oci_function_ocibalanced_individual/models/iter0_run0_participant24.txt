Here are three cognitive models designed to explain the participant's behavior, incorporating their Obsessive-Compulsive Inventory (OCI) score into the learning and decision-making processes.

### Model 1: OCI-Modulated Model-Based/Model-Free Balance
This model hypothesizes that the balance between goal-directed (Model-Based) and habitual (Model-Free) control is influenced by the participant's OCI score. Specifically, it tests the idea that higher OCI scores might lead to a reliance on habitual control (Model-Free) due to compulsive repetition, or conversely, rigid goal-directed control. Here, the `w` parameter (weighting of MB vs MF) is modulated by OCI.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: Hybrid Model-Based/Model-Free RL with OCI-modulated mixing weight.
    
    The balance between Model-Based (MB) and Model-Free (MF) control at Stage 1
    is determined by a mixing weight 'w'. This weight is linearly modulated by the 
    participant's OCI score, testing if symptom severity shifts the balance 
    between goal-directed planning and habitual responding.

    Parameters:
    - learning_rate: [0, 1] Rate at which Q-values are updated.
    - beta: [0, 10] Inverse temperature for softmax choice rule.
    - w_base: [0, 1] Base weight for Model-Based control (0=Pure MF, 1=Pure MB).
    - w_oci_slope: [-1, 1] How strongly OCI affects the MB/MF balance.
    """
    learning_rate, beta, w_base, w_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate the effective mixing weight w based on OCI
    # We clip it to ensure it stays between 0 and 1
    w = w_base + (w_oci_slope * oci_score)
    w = np.clip(w, 0.0, 1.0)
    
    # Transition matrix (fixed for this task version)
    # Row 0: Spaceship A -> [Planet X, Planet Y] (0.7, 0.3)
    # Row 1: Spaceship U -> [Planet X, Planet Y] (0.3, 0.7)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)      # Model-free values for Stage 1 (Spaceships)
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for Stage 2 (Aliens per Planet)
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # 1. Calculate Model-Based values: T * max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Integrate MB and MF values using w
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # 3. Softmax selection
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        # Record probability of the chosen action
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        state_idx = int(state[trial]) # The planet arrived at (0 or 1)
        
        # --- Stage 2 Policy ---
        # Standard softmax on Stage 2 Q-values
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning Updates ---
        r = reward[trial]
        
        # Update Stage 2 Q-values (Standard Q-learning)
        # PE = Reward - Prediction
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Update Stage 1 MF Q-values (TD(0) / SARSA-like update)
        # Using the value of the state actually reached (or the value of the action taken there)
        # Common implementation uses the max of the next stage or the value of the chosen option
        # Here we follow the template structure: Q(s1, a1) moves toward Q(s2, a2)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Driven Perseveration
This model posits that OCI symptoms manifest as "stickiness" or perseverationâ€”a tendency to repeat the previous choice regardless of reward history. This captures the compulsive aspect of OCD. The `perseveration` parameter is scaled directly by the OCI score.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: Model-Free RL with OCI-modulated Perseveration.
    
    This model assumes the participant relies on Model-Free learning but exhibits
    choice stickiness (perseveration). The strength of this stickiness is 
    influenced by the OCI score, hypothesizing that higher OCI leads to more 
    repetitive, compulsive behavior (repeating the previous choice).

    Parameters:
    - learning_rate: [0, 1] Rate at which Q-values are updated.
    - beta: [0, 10] Inverse temperature for softmax.
    - pers_weight: [0, 5] Base magnitude of the perseveration bonus.
    - lambda_eligibility: [0, 1] Eligibility trace decay (connects stage 2 outcome to stage 1).
    """
    learning_rate, beta, pers_weight, lambda_eligibility = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective perseveration bonus based on OCI
    # Higher OCI = Higher tendency to repeat last action
    effective_pers = pers_weight * (1.0 + oci_score) 
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    last_action_1 = -1 # Initialize as invalid index
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        qs_s1 = q_stage1.copy()
        
        # Add perseveration bonus to the previously chosen action
        if last_action_1 != -1:
            qs_s1[last_action_1] += effective_pers
            
        exp_q1 = np.exp(beta * qs_s1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        last_action_1 = a1
        
        state_idx = int(state[trial])
        
        # --- Stage 2 Policy ---
        qs_s2 = q_stage2[state_idx]
        exp_q2 = np.exp(beta * qs_s2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning Updates ---
        r = reward[trial]
        
        # Prediction error at Stage 2
        delta_stage2 = r - q_stage2[state_idx, a2]
        q_stage2[state_idx, a2] += learning_rate * delta_stage2
        
        # Prediction error at Stage 1
        # Using SARSA-style update: Q(s1,a1) moves toward Q(s2,a2)
        delta_stage1 = q_stage2[state_idx, a2] - q_stage1[a1]
        
        # Update Stage 1 with eligibility trace
        # lambda allows the stage 2 RPE (delta_stage2) to also affect stage 1 choice
        q_stage1[a1] += learning_rate * delta_stage1 + (learning_rate * lambda_eligibility * delta_stage2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Learning Rate Asymmetry
This model suggests that OCI scores affect how participants learn from positive versus negative outcomes. Specifically, it tests if higher OCI scores lead to hyper-sensitivity to punishment (or lack of reward) versus reward, creating separate learning rates for positive and negative prediction errors.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: Dual Learning Rate Model (Pos/Neg) modulated by OCI.
    
    This model splits the learning rate into alpha_pos (for positive RPEs) and
    alpha_neg (for negative RPEs). The OCI score modulates the bias between them.
    It hypothesizes that high OCI might be associated with an increased 
    sensitivity to 'failure' (negative RPEs) or 'success' (positive RPEs).

    Parameters:
    - alpha_base: [0, 1] The baseline learning rate.
    - beta: [0, 10] Inverse temperature.
    - bias_oci: [-1, 1] Modulates the ratio of pos/neg learning based on OCI.
      If bias_oci > 0, high OCI increases learning from positive outcomes.
      If bias_oci < 0, high OCI increases learning from negative outcomes.
    """
    alpha_base, beta, bias_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Define separate learning rates based on OCI
    # We create a split around the base alpha
    # OCI is [0,1]. We center the modulation.
    # If bias is negative, high OCI makes alpha_neg larger than alpha_pos.
    
    modulator = bias_oci * oci_score
    alpha_pos = np.clip(alpha_base * (1 + modulator), 0, 1)
    alpha_neg = np.clip(alpha_base * (1 - modulator), 0, 1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Pure Model-Based approach for Stage 1 evaluation to isolate learning rate effects
    # (Though we track MF values for the update mechanics)
    q_stage1_mf = np.zeros(2) 
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy (Hybrid MB/MF logic simplified to pure MB for structure) ---
        # Using Model-Based valuation driven by the learned Q-values of stage 2
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # We use purely MB values for choice here to focus the model on how 
        # the Stage 2 values are learned via the split alphas
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
  
        # --- Learning Updates ---
        r = reward[trial]
        
        # Calculate Prediction Error
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        
        # Apply split learning rates
        if delta_stage2 >= 0:
            eff_alpha = alpha_pos
        else:
            eff_alpha = alpha_neg
            
        q_stage2_mf[state_idx, a2] += eff_alpha * delta_stage2
        
        # Note: In this specific model configuration, we aren't updating a Stage 1 MF value
        # explicitly for choice, because we used the MB calculation above which relies 
        # on Q_stage2. This isolates the effect of OCI on learning stage 2 values.

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```