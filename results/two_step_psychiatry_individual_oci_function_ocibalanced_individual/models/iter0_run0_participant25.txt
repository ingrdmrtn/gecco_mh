Here are three cognitive models that incorporate the OCI-R score into the decision-making process for the two-step task.

### Model 1: OCI-Modulated Model-Based/Model-Free Balance
This model hypothesizes that higher obsessive-compulsive symptoms (OCI scores) influence the balance between goal-directed (Model-Based) and habitual (Model-Free) control. Specifically, it tests whether OCI modulates the mixing weight `w` directly, where higher OCI might lead to a stronger reliance on habitual (MF) or goal-directed (MB) systems.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free Reinforcement Learning with OCI-modulated mixing weight.
    
    This model assumes that the balance between Model-Based (MB) and Model-Free (MF) 
    systems is influenced by the participant's OCI score. The parameter 'w_base' 
    represents the baseline mixing weight, while 'w_oci' scales the effect of the 
    OCI score on this balance.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for value updates.
    - beta: [0, 10] Inverse temperature for softmax choice.
    - w_base: [0, 1] Baseline weight for Model-Based control (0 = pure MF, 1 = pure MB).
    - w_oci: [0, 1] Strength of OCI modulation on the mixing weight.
    """
    learning_rate, beta, w_base, w_oci = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0] # OCI is passed as an array, take the scalar value
  
    # Transition matrix: A->X (70%), U->Y (70%)
    # Rows: Space A, Space U. Cols: Planet X, Planet Y.
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)      # Model-free values for stage 1 (A, U)
    q_stage2_mf = np.zeros((2, 2)) # Values for stage 2 (Planet X: W/S, Planet Y: P/H)

    # Calculate effective mixing weight w based on OCI
    # We constrain w to be between 0 and 1 using a sigmoid-like or simple clipping approach
    # Here we use a linear modulation clipped to [0, 1]
    # Hypothesis: Does OCI shift reliance towards habits (MF) or planning (MB)?
    w_effective = w_base + (w_oci * (current_oci - 0.5)) 
    w_effective = np.clip(w_effective, 0.0, 1.0)

    for trial in range(n_trials):

        # --- Stage 1 Choice ---
        # Model-Based Value Calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Q-value (Hybrid)
        q_net = (w_effective * q_stage1_mb) + ((1 - w_effective) * q_stage1_mf)
        
        # Softmax Policy Stage 1
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        # Record probability of the chosen action
        # Ensure indices are integers
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        state_idx = int(state[trial]) # 0 for X, 1 for Y

        # --- Stage 2 Choice ---
        # Standard Model-Free Q-learning for the second stage
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
  
        # --- Learning ---
        r = reward[trial]
        
        # Prediction errors
        # Stage 2 PE: Difference between reward and Stage 2 Q-value
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        
        # Stage 1 PE: Difference between Stage 2 Q-value and Stage 1 Q-value
        # (Standard SARSA-like update for MF stage 1)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        
        # Update Q-values
        q_stage1_mf[a1] += learning_rate * delta_stage1
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Influenced Perseveration (Stickiness)
This model posits that OCI symptoms manifest as "stickiness" or perseverationâ€”a tendency to repeat the previous choice regardless of reward history. Here, the OCI score scales the `stickiness` parameter, testing if higher OCI leads to more repetitive behavior.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free Reinforcement Learning with OCI-modulated choice perseveration.
    
    This model focuses purely on model-free learning but adds a 'stickiness' bonus
    to the previously chosen action in Stage 1. The magnitude of this stickiness
    is modulated by the OCI score.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for value updates.
    - beta: [0, 10] Inverse temperature for softmax choice.
    - stick_base: [0, 5] Baseline tendency to repeat the previous Stage 1 choice.
    - stick_oci: [0, 5] Additional stickiness scaled by the OCI score.
    """
    learning_rate, beta, stick_base, stick_oci = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track previous choice for stickiness (initialize to -1 or handle first trial separately)
    prev_action_1 = -1

    # Calculate total stickiness bonus
    # Higher OCI might lead to higher compulsive repetition
    total_stickiness = stick_base + (stick_oci * current_oci)

    for trial in range(n_trials):

        # --- Stage 1 Choice ---
        # Calculate Q-values with stickiness bonus
        q_stage1_with_stick = q_stage1_mf.copy()
        
        if prev_action_1 != -1:
            q_stage1_with_stick[prev_action_1] += total_stickiness

        exp_q1 = np.exp(beta * q_stage1_with_stick)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        state_idx = int(state[trial])

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
  
        # --- Learning ---
        r = reward[trial]
        
        # Standard TD Updates
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Update previous action
        prev_action_1 = a1
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Learning Rate Asymmetry
This model suggests that OCI affects how participants learn from positive versus negative outcomes. Specifically, it introduces separate learning rates for positive prediction errors (better than expected) and negative prediction errors (worse than expected), with the balance or magnitude of this asymmetry influenced by OCI.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Reinforcement Learning with OCI-modulated asymmetric learning rates.
    
    This model allows for different learning rates for positive (alpha_pos) and 
    negative (alpha_neg) prediction errors. The OCI score modulates the 
    learning rate specifically for negative prediction errors, testing the 
    hypothesis that OCI relates to hypersensitivity to negative outcomes or 
    errors.
    
    Parameters:
    - alpha_pos: [0, 1] Learning rate for positive prediction errors.
    - alpha_neg_base: [0, 1] Baseline learning rate for negative prediction errors.
    - alpha_oci_mod: [0, 1] Modulation of negative learning rate by OCI.
    - beta: [0, 10] Inverse temperature.
    """
    alpha_pos, alpha_neg_base, alpha_oci_mod, beta = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    # Calculate effective negative learning rate
    # Hypothesis: Higher OCI increases sensitivity to negative outcomes (higher alpha_neg)
    alpha_neg_effective = alpha_neg_base + (alpha_oci_mod * current_oci)
    alpha_neg_effective = np.clip(alpha_neg_effective, 0.0, 1.0)

    for trial in range(n_trials):

        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        state_idx = int(state[trial])

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
  
        # --- Learning ---
        r = reward[trial]
        
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        if delta_stage2 >= 0:
            lr2 = alpha_pos
        else:
            lr2 = alpha_neg_effective
        q_stage2_mf[state_idx, a2] += lr2 * delta_stage2
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        if delta_stage1 >= 0:
            lr1 = alpha_pos
        else:
            lr1 = alpha_neg_effective
        q_stage1_mf[a1] += lr1 * delta_stage1
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```