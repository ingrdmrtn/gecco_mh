Here are three cognitive models that explain the participant's behavior in the two-step task, incorporating the OCI score to modulate learning and decision-making processes.

### Model 1: OCI-Modulated Model-Based/Model-Free Trade-off
This model hypothesizes that OCI symptoms influence the balance between Model-Based (planning) and Model-Free (habitual) control. Specifically, higher OCI scores might lead to a stronger reliance on habitual, model-free learning (represented by the mixing parameter `w`), potentially reflecting a compulsion to repeat reinforced actions regardless of the transition structure.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Model-Based/Model-Free Hybrid.
    
    This model assumes the balance between Model-Based (MB) and Model-Free (MF) 
    strategies is modulated by the OCI score. A base mixing weight (w_base) is 
    adjusted by the OCI score, testing if higher OCI shifts control towards 
    habitual (MF) or planning (MB) systems.

    Bounds:
    learning_rate: [0, 1] - Rate of updating Q-values.
    beta: [0, 10] - Inverse temperature for softmax choice.
    w_base: [0, 1] - Baseline weight for Model-Based control (0=Pure MF, 1=Pure MB).
    oci_sensitivity: [0, 5] - Strength of OCI impact on the mixing weight.
    """
    learning_rate, beta, w_base, oci_sensitivity = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate the effective mixing weight w based on OCI
    # We use a sigmoid-like transformation or simple clipping to keep w in [0,1].
    # Here: OCI increases (or decreases) the tendency for MB control relative to baseline.
    # If oci_sensitivity is high, the OCI score strongly pushes w away from w_base.
    # Hypothesis: Higher OCI might reduce MB control (w gets smaller).
    raw_w = w_base - (oci_sensitivity * (oci_score - 0.5)) 
    w = np.clip(raw_w, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)      # Model-free values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for stage 2 (2 states, 2 actions)

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # 1. Model-Based Value Calculation: V(s') = max_a Q(s', a)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Hybrid Value: Q_net = w * Q_MB + (1-w) * Q_MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax policy for Stage 1
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # --- Stage 2 Decision ---
        state_idx = int(state[trial])
        
        # Standard Model-Free Q-values for Stage 2
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # --- Learning / Updating ---
        
        # Prediction errors
        # Note: In standard TD(1) or hybrid models, Stage 1 MF is updated by the max of Stage 2
        # or the raw reward if using eligibility traces. Here we use a simple TD(0) structure for MF1.
        
        # Update Stage 1 MF: Q1(a1) += alpha * (Q2(s2, a2) - Q1(a1)) 
        # (SARSA-style update often used in these tasks)
        chosen_a1 = int(action_1[trial])
        chosen_a2 = int(action_2[trial])
        r = reward[trial]
        
        delta_stage1 = q_stage2_mf[state_idx, chosen_a2] - q_stage1_mf[chosen_a1]
        q_stage1_mf[chosen_a1] += learning_rate * delta_stage1
        
        # Update Stage 2 MF: Q2(s2, a2) += alpha * (Reward - Q2(s2, a2))
        delta_stage2 = r - q_stage2_mf[state_idx, chosen_a2]
        q_stage2_mf[state_idx, chosen_a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Driven Choice Stickiness
This model posits that OCI symptoms manifest as "stickiness" or perseverationâ€”a tendency to repeat the previous choice regardless of reward outcome. Here, the OCI score directly scales the magnitude of a stickiness parameter, adding a bonus to the previously chosen action.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Driven Choice Stickiness.
    
    This model assumes that individuals with higher OCI scores exhibit higher 
    behavioral rigidity or perseveration. The OCI score scales a 'stickiness' 
    parameter that biases the subject to repeat the action taken on the 
    previous trial, regardless of the outcome.

    Bounds:
    learning_rate: [0, 1] - Rate of updating Q-values.
    beta: [0, 10] - Inverse temperature for softmax choice.
    stickiness_base: [0, 5] - Baseline tendency to repeat choices.
    oci_mod: [0, 5] - How much OCI amplifies the stickiness.
    """
    learning_rate, beta, stickiness_base, oci_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective stickiness
    # Higher OCI leads to higher stickiness
    eff_stickiness = stickiness_base + (oci_mod * oci_score)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1 # Indicator for no previous action

    for trial in range(n_trials):

        # --- Stage 1 Decision ---
        # Pure Model-Free with Stickiness
        # We compute Q values and add stickiness bonus to the previously chosen index
        
        qs_stage1 = q_stage1_mf.copy()
        
        if prev_action_1 != -1:
            qs_stage1[prev_action_1] += eff_stickiness
            
        exp_q1 = np.exp(beta * qs_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # Record action for next trial's stickiness
        prev_action_1 = int(action_1[trial])
        state_idx = int(state[trial])

        # --- Stage 2 Decision ---
        # Standard Model-Free
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        # --- Learning ---
        chosen_a1 = int(action_1[trial])
        chosen_a2 = int(action_2[trial])
        r = reward[trial]

        # Update Stage 1
        delta_stage1 = q_stage2_mf[state_idx, chosen_a2] - q_stage1_mf[chosen_a1]
        q_stage1_mf[chosen_a1] += learning_rate * delta_stage1
        
        # Update Stage 2
        delta_stage2 = r - q_stage2_mf[state_idx, chosen_a2]
        q_stage2_mf[state_idx, chosen_a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Dependent Learning Rates (Positive vs Negative)
This model suggests that OCI relates to an asymmetry in how people learn from positive versus negative prediction errors. High OCI might be associated with hypersensitivity to errors (negative outcomes) or safety signals. The OCI score acts as a gate, shifting the learning rate for negative prediction errors relative to positive ones.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Dependent Asymmetric Learning Rates.
    
    This model investigates if OCI scores correlate with an imbalance in learning 
    from positive vs. negative prediction errors. It uses a base learning rate 
    and an OCI-dependent multiplier for negative prediction errors (punishment/lack of reward).
    
    Bounds:
    alpha_pos: [0, 1] - Learning rate for positive prediction errors.
    beta: [0, 10] - Inverse temperature.
    neg_alpha_mod: [0, 2] - Multiplier for negative PE learning rate based on OCI.
    """
    alpha_pos, beta, neg_alpha_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate negative learning rate
    # If neg_alpha_mod > 1 and OCI is high, they learn faster from disappointment (0 reward).
    # We clip alpha_neg to [0,1] to ensure stability.
    alpha_neg = np.clip(alpha_pos * (1 + (neg_alpha_mod - 1) * oci_score), 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Decision ---
        # Pure Model-Free formulation for simplicity to isolate learning rate effects
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])

        # --- Stage 2 Decision ---
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        # --- Learning ---
        chosen_a1 = int(action_1[trial])
        chosen_a2 = int(action_2[trial])
        r = reward[trial]
        
        # Determine Learning Rate for Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, chosen_a2] - q_stage1_mf[chosen_a1]
        lr_1 = alpha_pos if delta_stage1 >= 0 else alpha_neg
        q_stage1_mf[chosen_a1] += lr_1 * delta_stage1
        
        # Determine Learning Rate for Stage 2 Update
        delta_stage2 = r - q_stage2_mf[state_idx, chosen_a2]
        lr_2 = alpha_pos if delta_stage2 >= 0 else alpha_neg
        q_stage2_mf[state_idx, chosen_a2] += lr_2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```