Here are three cognitive models designed to explain the participant's behavior in the two-step task. These models incorporate the Obsessive-Compulsive Inventory (OCI) score to modulate specific cognitive mechanisms, such as the balance between model-based and model-free control, perseveration, or learning rates.

### Model 1: OCI-Modulated Model-Based/Model-Free Balance
This model posits that the OCI score influences the trade-off between Model-Based (planning-based) and Model-Free (habit-based) reinforcement learning. A core hypothesis in computational psychiatry is that compulsivity relates to an over-reliance on habit systems (Model-Free) at the expense of goal-directed planning (Model-Based). Here, the mixing weight `w` is a function of the OCI score.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free RL with OCI-modulated mixing weight.
    
    Hypothesis: Higher OCI scores (compulsivity) lead to a reduction in model-based
    planning (lower w), biasing the agent towards model-free habits.
    
    Parameters:
    learning_rate: [0, 1] Rate at which Q-values are updated.
    beta: [0, 10] Inverse temperature for softmax choice (exploration/exploitation).
    w_base: [0, 1] Baseline weight for model-based control.
    w_oci_slope: [-1, 1] How strongly OCI reduces (or increases) model-based control.
    """
    learning_rate, beta, w_base, w_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate mixing weight w based on OCI
    # We constrain w to be between 0 and 1 using a sigmoid-like or clipping approach
    # Here simple clipping for interpretability.
    # If slope is negative, higher OCI -> lower model-based control.
    w = w_base + (w_oci_slope * oci_score)
    w = np.clip(w, 0.0, 1.0)
    
    # Fixed transition matrix as described in task (70% common, 30% rare)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Initialization
    q_stage1_mf = np.zeros(2)       # Model-free values for stage 1 (Spaceships)
    q_stage2_mf = np.zeros((2, 2))  # Model-free values for stage 2 (Aliens)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based Value: V(state) = max(Q_stage2)
        # Q_MB(action1) = Transition_Matrix * V(state)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value: Q_net = w * Q_MB + (1-w) * Q_MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax selection for Stage 1
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # --- Stage 2 Policy ---
        state_idx = int(state[trial])
        
        # Standard Q-learning choice for Stage 2
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Learning / Updates ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Stage 2 RPE (Reward Prediction Error)
        # delta2 = r - Q_stage2(s, a2)
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 RPE (TD(0) update)
        # Note: In a full hybrid model, MF updates often use the value of the state chosen
        # or the max of the next state. Here we use SARSA-style or Q-learning style.
        # Using Q_stage2(s, a2) as the target for Stage 1 MF.
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Driven Choice Stickiness (Perseveration)
This model investigates whether obsessive-compulsive traits manifest as "stickiness" or perseverationâ€”a tendency to repeat the previous choice regardless of reward history. Here, the `stickiness` parameter is directly modulated by the OCI score.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Pure Model-Based Agent with OCI-modulated Stickiness.
    
    Hypothesis: OCI score drives choice perseveration (stickiness). 
    Higher OCI leads to a higher tendency to repeat the previous Stage 1 action,
    independent of the model-based value calculation.
    
    Parameters:
    learning_rate: [0, 1] Rate at which Q-values are updated.
    beta: [0, 10] Inverse temperature.
    stick_base: [0, 5] Baseline tendency to repeat choice.
    stick_oci_sens: [0, 5] Sensitivity of stickiness to OCI score.
    """
    learning_rate, beta, stick_base, stick_oci_sens = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective stickiness
    # Stickiness adds a bonus to the previously chosen action
    stickiness = stick_base + (stick_oci_sens * oci_score)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # We will use a pure Model-Based calculation for value, but add stickiness
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Track previous choice (initialize arbitrarily, e.g., -1 so it doesn't apply on trial 0)
    prev_action_1 = -1
    
    for trial in range(n_trials):
        
        # --- Stage 1 Policy ---
        # Calculate Model-Based values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Add Stickiness Bonus
        # Create a vector of bonuses [bonus_action_0, bonus_action_1]
        stick_vector = np.zeros(2)
        if prev_action_1 != -1:
            stick_vector[prev_action_1] = stickiness
            
        # Combine Value + Stickiness
        logits = beta * q_stage1_mb + stick_vector
        
        exp_q1 = np.exp(logits)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # Update previous action
        prev_action_1 = int(action_1[trial])
        
        # --- Stage 2 Policy ---
        state_idx = int(state[trial])
        
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Learning ---
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Only updating Stage 2 values is necessary for a Pure MB agent
        # The agent infers Stage 1 values from the transition matrix and Stage 2 values.
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Learning Rate Asymmetry
This model tests if OCI scores relate to how participants learn from positive versus negative outcomes. It implements separate learning rates for positive prediction errors (learning from reward) and negative prediction errors (learning from omission). The balance between these rates is shifted by the OCI score.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Reinforcement Learning with Asymmetric Learning Rates modulated by OCI.
    
    Hypothesis: OCI affects sensitivity to negative feedback. 
    We model a 'base' learning rate and an OCI-dependent modifier for negative prediction errors.
    This creates a distinction between alpha_pos (learning from gain) and alpha_neg (learning from loss).
    
    Parameters:
    alpha_pos: [0, 1] Learning rate for positive prediction errors.
    alpha_neg_base: [0, 1] Baseline learning rate for negative prediction errors.
    alpha_neg_oci_sens: [-1, 1] How OCI modifies the negative learning rate.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Fixed mixing weight for MB/MF (to keep parameter count low, we fix this structure but vary learning).
    """
    alpha_pos, alpha_neg_base, alpha_neg_oci_sens, beta, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate negative learning rate
    # alpha_neg = base + (sensitivity * OCI)
    alpha_neg = alpha_neg_base + (alpha_neg_oci_sens * oci_score)
    alpha_neg = np.clip(alpha_neg, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        
        # --- Stage 1 Policy (Hybrid MB/MF) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # --- Stage 2 Policy ---
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Learning with Asymmetry ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        eff_alpha2 = alpha_pos if delta_stage2 > 0 else alpha_neg
        q_stage2_mf[state_idx, a2] += eff_alpha2 * delta_stage2
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        eff_alpha1 = alpha_pos if delta_stage1 > 0 else alpha_neg
        q_stage1_mf[a1] += eff_alpha1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```