Here are three cognitive models designed to explain the participant's behavior in the two-step task, incorporating their OCI-R score (Obsessive-Compulsive Inventory) as a factor influencing decision-making parameters.

### Model 1: Hybrid Learner with OCI-Modulated Model-Based Weight (`cognitive_model1`)
This model assumes participants use a mixture of Model-Free (MF) and Model-Based (MB) reinforcement learning. The core hypothesis is that the degree of "model-basedness" (planning) is influenced by the OCI score. Specifically, individuals with higher OCI scores might exhibit more rigid, habit-like (MF) behavior or, conversely, over-deliberative (MB) behavior. Here, we model `w` (the mixing weight) as a function of OCI.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL model where the balance between Model-Based (MB) and Model-Free (MF) control
    is modulated by the OCI score.
    
    Parameters:
    lr: [0, 1] Learning rate for value updates.
    beta: [0, 10] Inverse temperature for softmax choice.
    w_base: [0, 1] Baseline weight for Model-Based control (0 = pure MF, 1 = pure MB).
    w_oci_slope: [-1, 1] How OCI affects the MB weight. Positive = higher OCI -> more MB.
    
    Hypothesis: OCI symptoms affect the trade-off between habit (MF) and planning (MB).
    """
    lr, beta, w_base, w_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective mixing weight w, bounded between 0 and 1
    # We center OCI around 0.5 for stability
    w = w_base + w_oci_slope * (oci_score - 0.5)
    w = np.clip(w, 0.0, 1.0)

    # Fixed transition matrix (A->X=0.7, U->Y=0.7)
    # actions: 0=A, 1=U; states: 0=X, 1=Y
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2) # MF values for stage 1 actions
    q_stage2_mf = np.zeros((2, 2)) # MF values for stage 2 (state, action)

    for trial in range(n_trials):
        s1_action = int(action_1[trial])
        s2_state = int(state[trial])
        s2_action = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        # Model-Based Value: T * max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value: w * MB + (1-w) * MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[s1_action]

        # --- Stage 2 Policy ---
        # Standard Softmax on Stage 2 Q-values
        exp_q2 = np.exp(beta * q_stage2_mf[s2_state, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[s2_action]

        # --- Learning ---
        # SARSA-style update for Stage 1 MF
        # Note: In pure Daw 2011, Stage 1 MF is often updated by TD(1) or TD(0).
        # Here we use a simple TD(1) approximation using the reward.
        
        # Update Stage 2 Q-values (Direct reinforcement)
        pe_2 = r - q_stage2_mf[s2_state, s2_action]
        q_stage2_mf[s2_state, s2_action] += lr * pe_2
        
        # Update Stage 1 MF Q-values (TD(1) logic: driven by final reward)
        pe_1 = r - q_stage1_mf[s1_action]
        q_stage1_mf[s1_action] += lr * pe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Driven Choice Stickiness (`cognitive_model2`)
This model focuses on "stickiness" or perseverationâ€”the tendency to repeat the previous action regardless of reward. High OCI scores are associated with compulsivity and repetition. This model posits that the OCI score directly scales a `stickiness` parameter, making high-OCI individuals more likely to repeat their Stage 1 choice.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner where OCI score modulates choice stickiness (perseveration).
    
    Parameters:
    lr: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    stick_base: [0, 5] Baseline tendency to repeat the previous Stage 1 choice.
    stick_oci_mod: [0, 5] Additional stickiness scaling with OCI score.
    
    Hypothesis: Higher OCI leads to higher compulsivity/repetition (stickiness).
    """
    lr, beta, stick_base, stick_oci_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective stickiness
    # Stickiness increases as OCI increases
    eff_stickiness = stick_base + (stick_oci_mod * oci_score)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    last_action_1 = -1 # No previous action for first trial

    for trial in range(n_trials):
        s1_action = int(action_1[trial])
        s2_state = int(state[trial])
        s2_action = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        # Q-values + Stickiness bonus
        q_net = q_stage1.copy()
        if last_action_1 != -1:
            q_net[last_action_1] += eff_stickiness
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[s1_action]
        
        # Save action for next trial's stickiness
        last_action_1 = s1_action

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2[s2_state, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[s2_action]

        # --- Learning ---
        # Standard TD learning
        
        # Stage 2 update
        pe_2 = r - q_stage2[s2_state, s2_action]
        q_stage2[s2_state, s2_action] += lr * pe_2
        
        # Stage 1 update (using Q-value of chosen stage 2 state as proxy for V(s'))
        # This is a TD(0) approach for Stage 1
        value_s2 = np.max(q_stage2[s2_state, :])
        pe_1 = value_s2 - q_stage1[s1_action]
        q_stage1[s1_action] += lr * pe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Dependent Learning Rates (`cognitive_model3`)
This model investigates learning flexibility. It hypothesizes that OCI symptoms might relate to how quickly or slowly individuals update their beliefs. Specifically, it tests if the OCI score modulates the learning rate (`alpha`). A high OCI might be associated with "over-learning" from negative feedback (high alpha) or rigidity (low alpha). Here we allow a single learning rate to be shifted by the OCI score.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Pure Model-Based learner where the Learning Rate is modulated by the OCI score.
    
    Parameters:
    lr_base: [0, 1] Baseline learning rate.
    lr_oci_mod: [-0.5, 0.5] Modification of learning rate by OCI.
    beta: [0, 10] Inverse temperature.
    
    Hypothesis: OCI affects the speed of value updating (plasticity).
    """
    lr_base, lr_oci_mod, beta = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective learning rate
    # Bounded between 0 and 1
    lr = lr_base + (lr_oci_mod * oci_score)
    lr = np.clip(lr, 0.01, 1.0) # Ensure it doesn't hit 0 exactly or exceed 1

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # We only track Stage 2 values because a pure MB learner calculates Stage 1 
    # values dynamically based on the transition matrix.
    q_stage2 = np.zeros((2, 2)) 

    for trial in range(n_trials):
        s1_action = int(action_1[trial])
        s2_state = int(state[trial])
        s2_action = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy (Pure Model-Based) ---
        # Calculate expected value of each spaceship based on current Stage 2 values
        max_q_stage2 = np.max(q_stage2, axis=1) # Value of Planet X and Planet Y
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[s1_action]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2[s2_state, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[s2_action]

        # --- Learning ---
        # Only Stage 2 values need updating in this simplified MB scheme
        pe_2 = r - q_stage2[s2_state, s2_action]
        q_stage2[s2_state, s2_action] += lr * pe_2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```