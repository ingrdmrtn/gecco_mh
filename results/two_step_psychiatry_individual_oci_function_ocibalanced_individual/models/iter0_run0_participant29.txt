Here are three cognitive models designed to explain the participant's behavior in the two-step decision task, incorporating their OCI-R score to modulate learning and decision-making parameters.

### Model 1: OCI-Modulated Model-Based/Model-Free Balance
This model hypothesizes that the balance between goal-directed (Model-Based) and habitual (Model-Free) control is influenced by the participant's obsessive-compulsive traits. Specifically, the mixing weight `w` (which determines the influence of the Model-Based system) is a linear function of the OCI score. This reflects literature suggesting that compulsivity may be linked to deficits in model-based control or an over-reliance on habits.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: Hybrid Model-Based/Model-Free RL with OCI-modulated mixing weight.
    
    The mixing parameter 'w' determines the balance between Model-Based (MB) and 
    Model-Free (MF) values at the first stage. Here, 'w' is dynamically adjusted 
    based on the participant's OCI score.
    
    w_base: Baseline mixing weight [0, 1]
    w_oci_slope: Sensitivity of mixing weight to OCI score [-1, 1] (constrained effectively in code)
    learning_rate: Learning rate for value updates [0, 1]
    beta: Inverse temperature for softmax [0, 10]
    """
    w_base, w_oci_slope, learning_rate, beta = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate effective mixing weight w, bounded between 0 and 1
    # We use a tanh-like or clipping approach to keep it valid, but simple clipping is safer for interpretation.
    w = w_base + (w_oci_slope * current_oci)
    if w < 0: w = 0
    if w > 1: w = 1

    # Transition matrix: A->X (0->0) is 0.7, U->Y (1->1) is 0.7
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)      # Model-free values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for stage 2 (State x Action)

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # Model-Based Calculation: V(S') = max_a Q(S', a)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value: Q_net = w * Q_MB + (1-w) * Q_MF
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Softmax Policy Stage 1
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # --- Stage 2 Decision ---
        state_idx = int(state[trial])
        
        # Softmax Policy Stage 2 (Pure Model-Free)
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Learning ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Prediction Errors
        # Stage 2 PE: R - Q2(S', a')
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        
        # Stage 1 PE: Q2(S', a') - Q1(S, a)  (SARSA-style update for stage 1 MF)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        
        # Update Values
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Driven Perseveration (Stickiness)
This model posits that higher OCI scores lead to increased choice perseveration ("stickiness"). It introduces a `stickiness` parameter that adds a bonus to the previously chosen action. The magnitude of this bonus is scaled by the participant's OCI score, testing the hypothesis that compulsive symptoms manifest as a repetitive adherence to previous choices regardless of reward outcomes.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: Pure Model-Free RL with OCI-modulated choice stickiness (perseveration).
    
    Stickiness is defined as a tendency to repeat the previous choice.
    The effective stickiness is: stickiness_base + (stickiness_oci_factor * oci).
    
    learning_rate: [0, 1]
    beta: [0, 10]
    stickiness_base: Base tendency to repeat choice [-5, 5] (usually >0 for stickiness)
    stickiness_oci_factor: How much OCI increases stickiness [-5, 5]
    """
    learning_rate, beta, stickiness_base, stickiness_oci_factor = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate effective stickiness
    eff_stickiness = stickiness_base + (stickiness_oci_factor * current_oci)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1 # Indicator for previous choice (none at start)

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # Add stickiness bonus to Q-values before softmax
        q_stage1_augmented = q_stage1_mf.copy()
        if last_action_1 != -1:
            q_stage1_augmented[last_action_1] += eff_stickiness

        exp_q1 = np.exp(beta * q_stage1_augmented)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # Record this action for the next trial's stickiness
        last_action_1 = int(action_1[trial])

        # --- Stage 2 Decision ---
        state_idx = int(state[trial])
        
        # Standard softmax for Stage 2
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Learning ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # TD(1) style update chain:
        # Update Stage 2
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Update Stage 1 based on the value of the state reached
        # (Note: Using max Q of next state is Q-learning, using actual Q is SARSA. 
        # Here we use Q-learning logic for the Stage 1 update target)
        delta_stage1 = np.max(q_stage2_mf[state_idx]) - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Dependent Learning Rates for Positive/Negative Outcomes
This model suggests that OCI scores affect how participants learn from positive versus negative prediction errors. It defines two learning rates: one for positive prediction errors (`alpha_pos`) and one for negative prediction errors (`alpha_neg`). The OCI score acts as a bias parameter, shifting the ratio between these two learning rates. This tests if higher OCI relates to hypersensitivity to negative outcomes (or failure) versus positive reinforcement.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: Dual Learning Rate Model (Pos/Neg) modulated by OCI.
    
    This model separates learning rates for positive and negative prediction errors.
    The OCI score scales the learning rate for negative errors specifically,
    testing the hypothesis of enhanced error monitoring/sensitivity in compulsivity.
    
    alpha_base: Base learning rate for positive errors [0, 1]
    alpha_neg_bias: Multiplier for negative learning rate relative to positive [0, 5]
    oci_sensitivity: Additional scaling of negative learning rate by OCI [0, 5]
    beta: Inverse temperature [0, 10]
    """
    alpha_base, alpha_neg_bias, oci_sensitivity, beta = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate specific learning rates
    # Alpha positive is the base rate
    lr_pos = alpha_base
    
    # Alpha negative is influenced by the bias parameter AND the OCI score
    # Higher OCI -> Higher learning rate from negative outcomes
    lr_neg = alpha_base * (alpha_neg_bias + (oci_sensitivity * current_oci))
    
    # Bounds check
    if lr_pos > 1: lr_pos = 1
    if lr_neg > 1: lr_neg = 1
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # --- Stage 2 Decision ---
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Learning ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        effective_lr2 = lr_pos if delta_stage2 >= 0 else lr_neg
        q_stage2_mf[state_idx, a2] += effective_lr2 * delta_stage2
        
        # Stage 1 Update
        # Using Q-learning update: Target is max(Q_stage2)
        delta_stage1 = np.max(q_stage2_mf[state_idx]) - q_stage1_mf[a1]
        effective_lr1 = lr_pos if delta_stage1 >= 0 else lr_neg
        q_stage1_mf[a1] += effective_lr1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```