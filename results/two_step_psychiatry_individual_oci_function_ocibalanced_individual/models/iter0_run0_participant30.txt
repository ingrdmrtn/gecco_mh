Here are three cognitive models designed to capture the behavior of a participant in a two-step decision-making task, incorporating their high OCI score (0.9333) into the learning and decision-making processes.

### Model 1: The "Habitual Reliance" Model
**Hypothesis:** High OCI scores are often associated with an over-reliance on habitual (Model-Free) control and a deficit in goal-directed (Model-Based) control. This model posits that the balance between these two systems (`w`) is directly modulated by the OCI score. Specifically, a higher OCI score reduces the weight of the model-based system, making the agent more habitual.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-modulated Model-Based vs. Model-Free weighting.
    
    Hypothesis: High OCI participants rely more on Model-Free (habitual) learning
    and less on Model-Based (goal-directed) planning. The parameter `w` (mixing weight)
    is scaled down by the OCI score.
    
    Parameters:
    learning_rate: [0,1] Learning rate for Q-value updates.
    beta: [0,10] Inverse temperature for softmax choice.
    w_base: [0,1] Base weight for Model-Based control (before OCI modulation).
    """
    learning_rate, beta, w_base = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0] # Scalar OCI score
  
    # Fixed transition matrix as described in task: A->X (common), U->Y (common)
    # 0->0 (0.7), 0->1 (0.3), 1->0 (0.3), 1->1 (0.7)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2) # Model-free values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for stage 2 (2 states, 2 actions)

    # OCI Modulation: Higher OCI reduces the effective weight of model-based control.
    # We constrain w to be between 0 and 1.
    # If OCI is high (near 1), w becomes smaller, pushing behavior towards MF.
    w_effective = w_base * (1.0 - current_oci)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based value calculation: T * max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Q-value: weighted sum of MB and MF
        q_net = w_effective * q_stage1_mb + (1 - w_effective) * q_stage1_mf
        
        # Softmax choice for Stage 1
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        # Store probability of the chosen action
        # action_1 is 0 or 1 (float in data, cast to int)
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Transition ---
        state_idx = int(state[trial]) # 0 or 1
        
        # --- Stage 2 Policy ---
        # Standard softmax on Stage 2 Q-values
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning / Updating ---
        r = reward[trial]
        
        # TD Error Stage 2 (Prediction Error)
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        
        # Update Stage 2 Q-values
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # TD Error Stage 1 (SARSA-style update using Stage 2 value)
        # We use the value of the state we actually arrived at
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        
        # Update Stage 1 MF Q-values
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: The "Stickiness" Model
**Hypothesis:** OCD is characterized by repetitive behaviors and difficulty shifting sets. This model hypothesizes that the OCI score predicts the degree of "choice stickiness" (perseveration). A high OCI score implies a higher tendency to simply repeat the previous Stage 1 action, regardless of the reward outcome or model-based inference.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-modulated Choice Stickiness (Perseveration).
    
    Hypothesis: High OCI participants exhibit higher choice stickiness (perseveration).
    The `stickiness` parameter is amplified by the OCI score, adding a bonus to the 
    Q-value of the previously chosen action.
    
    Parameters:
    learning_rate: [0,1] Learning rate.
    beta: [0,10] Inverse temperature.
    w: [0,1] Weight for Model-Based control.
    stickiness_base: [0,5] Base tendency to repeat the last choice.
    """
    learning_rate, beta, w, stickiness_base = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track previous action for stickiness
    prev_action_1 = -1 

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Q-value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add Stickiness Bonus
        # If OCI is high, the impact of the stickiness parameter is larger.
        effective_stickiness = stickiness_base * (1.0 + current_oci)
        
        q_net_sticky = q_net.copy()
        if prev_action_1 != -1:
            q_net_sticky[prev_action_1] += effective_stickiness

        exp_q1 = np.exp(beta * q_net_sticky)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updating ---
        r = reward[trial]
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Update history
        prev_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: The "Anxious Learning" Model (Enhanced Negative Learning Rate)
**Hypothesis:** Individuals with high OCI often exhibit heightened sensitivity to punishment or lack of reward (negative prediction errors) due to anxiety or fear of failure. This model separates learning rates for positive (`lr_pos`) and negative (`lr_neg`) prediction errors. The OCI score specifically amplifies the learning rate for negative outcomes (when reward is 0), causing the participant to flee states associated with failure more rapidly.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-modulated Asymmetric Learning Rates.
    
    Hypothesis: High OCI participants are hypersensitive to negative outcomes (0 reward).
    The learning rate for negative prediction errors is scaled up by the OCI score.
    
    Parameters:
    lr_base: [0,1] Base learning rate for positive outcomes.
    lr_neg_scale: [0,1] Scaling factor for negative learning rate relative to OCI.
    beta: [0,10] Inverse temperature.
    w: [0,1] Weight for Model-Based control.
    """
    lr_base, lr_neg_scale, beta, w = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updating with Asymmetric Learning Rates ---
        r = reward[trial]
        
        # Determine effective learning rate based on Prediction Error sign or Reward
        # Here we base it on Reward: 1 is positive, 0 is negative outcome.
        if r > 0:
            current_lr = lr_base
        else:
            # If outcome is negative (0 reward), learning is boosted by OCI
            # High OCI -> Higher learning rate from failure
            current_lr = lr_base + (lr_neg_scale * current_oci)
            # Clip to ensure it doesn't exceed 1
            if current_lr > 1.0:
                current_lr = 1.0
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += current_lr * delta_stage2
        
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += current_lr * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```