Here are three cognitive models designed to explain the participant's behavior, incorporating their high OCI-R score (high obsessive-compulsive symptoms) as a modulator of learning and decision-making parameters.

### Cognitive Model 1: OCI-modulated Model-Based/Model-Free Balance
This model hypothesizes that individuals with high OCI scores might rely more on habitual (model-free) control rather than goal-directed (model-based) planning, or vice-versa. Specifically, high compulsivity is often associated with rigid, habitual behaviors. Here, the mixing weight `w` (which balances Model-Based and Model-Free systems) is modulated by the OCI score.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-modulated Model-Based/Model-Free Balance.
    
    This model assumes that the balance between Model-Based (planning) and 
    Model-Free (habitual) control is influenced by the participant's OCI score.
    Higher OCI scores might bias the agent towards more habitual (Model-Free) 
    strategies, reducing the weight `w` given to the Model-Based system.
    
    Parameters:
    - learning_rate: [0, 1] Rate at which Q-values are updated.
    - beta: [0, 10] Inverse temperature for softmax choice.
    - w_base: [0, 1] Base mixing weight for Model-Based control (0 = pure MF, 1 = pure MB).
    - oci_sens: [0, 1] Sensitivity of the mixing weight to the OCI score.
    
    The effective mixing weight is: w_effective = w_base * (1 - oci_sens * oci)
    """
    learning_rate, beta, w_base, oci_sens = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0] # Extract scalar
  
    # Define transition matrix (A->X common, U->Y common)
    # A=0, U=1; X=0, Y=1
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)      # Model-free values for stage 1 (A, U)
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for stage 2 (State X, Y; Actions 0, 1)

    # Calculate effective mixing weight based on OCI
    # If oci_sens is high, high OCI reduces 'w', making the agent more Model-Free/Habitual.
    w_effective = w_base * (1.0 - (oci_sens * oci_score))
    # Clip to ensure bounds [0, 1]
    if w_effective < 0: w_effective = 0.0
    if w_effective > 1: w_effective = 1.0

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based value calculation: T * max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated value
        q_net = w_effective * q_stage1_mb + (1 - w_effective) * q_stage1_mf
        
        # Softmax policy
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        # Store probability of the chosen action
        # Inputs are floats, cast to int for indexing
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Choice ---
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        r = reward[trial]
        
        # Prediction errors
        # Stage 2 PE (Reward prediction error)
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        
        # Stage 1 PE (SARSA-style TD error for MF update)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        
        # Update Q-values
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        q_stage1_mf[a1] += learning_rate * delta_stage1 # Only update MF value here

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2: OCI-modulated Perseveration (Stickiness)
High OCI scores are associated with repetitive behaviors and difficulty shifting sets. This model introduces a "stickiness" parameter (perseveration) that biases the agent to repeat the previous Stage 1 choice. The magnitude of this stickiness is scaled by the OCI score.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-modulated Perseveration (Stickiness).
    
    This model posits that high OCI scores lead to higher choice perseveration 
    (stickiness). The agent receives an additional 'bonus' to the value of the 
    action chosen in the previous trial, making them more likely to repeat it 
    regardless of reward history.
    
    Parameters:
    - learning_rate: [0, 1] Rate at which Q-values are updated.
    - beta: [0, 10] Inverse temperature for softmax choice.
    - stickiness_base: [0, 5] Base tendency to repeat the previous choice.
    - oci_mult: [0, 5] Multiplier for OCI impact on stickiness.
    
    Effective stickiness = stickiness_base + (oci_mult * oci)
    """
    learning_rate, beta, stickiness_base, oci_mult = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track previous choice (initialize to None or -1)
    last_action_1 = -1

    # Calculate effective stickiness based on OCI
    stickiness_eff = stickiness_base + (oci_mult * oci_score)

    for trial in range(n_trials):

        # --- Stage 1 Choice ---
        # Pure Model-Free formulation for simplicity, augmented with stickiness
        q_net = q_stage1_mf.copy()
        
        # Add stickiness bonus if there was a previous trial
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness_eff

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # Update history
        last_action_1 = a1
        
        # --- Stage 2 Choice ---
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
  
        # --- Learning ---
        r = reward[trial]
        
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        
        q_stage1_mf[a1] += learning_rate * delta_stage1
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3: OCI-modulated Learning Rate Asymmetry
Obsessive-compulsive symptoms often involve an over-sensitivity to negative outcomes (fear of harm/error) or a rigidity in updating beliefs. This model suggests that the OCI score modulates the learning rate specifically for negative prediction errors (punishments or lack of reward) differently than positive ones. A high OCI might lead to "over-learning" from failure (high punishment sensitivity).

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-modulated Learning Rate Asymmetry (Punishment Sensitivity).
    
    This model assumes that OCI scores affect how strongly the agent learns from 
    negative prediction errors (worse than expected outcomes) compared to positive ones.
    High OCI acts as a multiplier on the learning rate when the prediction error is negative.
    
    Parameters:
    - alpha_pos: [0, 1] Learning rate for positive prediction errors.
    - beta: [0, 10] Inverse temperature.
    - lambda_eligibility: [0, 1] Eligibility trace (how much Stage 2 reward updates Stage 1).
    - oci_neg_bias: [0, 5] Multiplier for learning rate when PE is negative, scaled by OCI.
    
    If delta < 0: effective_alpha = alpha_pos * (1 + oci_neg_bias * oci)
    If delta > 0: effective_alpha = alpha_pos
    """
    alpha_pos, beta, lambda_eligibility, oci_neg_bias = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Choice ---
        # Standard Model-Free Q-learning choice
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        state_idx = int(state[trial])

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
  
        # --- Learning ---
        r = reward[trial]
        
        # Calculate Prediction Errors
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        
        # Determine Learning Rate for Stage 2 based on PE sign and OCI
        if delta_stage2 < 0:
            alpha_s2 = alpha_pos * (1.0 + (oci_neg_bias * oci_score))
            # Bound alpha to max 1.0
            if alpha_s2 > 1.0: alpha_s2 = 1.0
        else:
            alpha_s2 = alpha_pos
            
        # Determine Learning Rate for Stage 1 based on PE sign and OCI
        if delta_stage1 < 0:
            alpha_s1 = alpha_pos * (1.0 + (oci_neg_bias * oci_score))
            if alpha_s1 > 1.0: alpha_s1 = 1.0
        else:
            alpha_s1 = alpha_pos

        # Update Q-values using specific alphas
        q_stage2_mf[state_idx, a2] += alpha_s2 * delta_stage2
        
        # TD(1) style update: Stage 1 is updated by its own PE plus the Stage 2 PE (scaled by lambda)
        # This connects the outcome at the end to the first choice
        q_stage1_mf[a1] += alpha_s1 * delta_stage1 + (alpha_s1 * lambda_eligibility * delta_stage2)
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```