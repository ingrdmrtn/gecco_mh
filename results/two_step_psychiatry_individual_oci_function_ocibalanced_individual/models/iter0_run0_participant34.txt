Here are three cognitive models designed to capture the decision-making processes of a participant with a high OCI-R score (0.9) in a two-step task.

The high OCI score suggests potential deficits in goal-directed (model-based) control, over-reliance on habits (model-free), or specific alterations in learning rates (e.g., higher learning from punishment or perseveration).

### Model 1: OCI-Modulated Model-Based/Model-Free Balance
This model hypothesizes that the balance between Model-Based (planning) and Model-Free (habitual) control is directly influenced by the OCI score. Specifically, higher OCI scores might reduce the weight of model-based planning (`w`), pushing the participant towards habitual responding.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free Reinforcement Learning model.
    The mixing weight 'w' (balance between MB and MF) is modulated by the OCI score.
    High OCI is hypothesized to reduce model-based control (lower w).

    Parameters:
    learning_rate: [0, 1] - Rate of updating Q-values.
    beta: [0, 10] - Inverse temperature for softmax choice.
    w_base: [0, 1] - Baseline weight for Model-Based control (0=Pure MF, 1=Pure MB).
    oci_sensitivity: [0, 5] - How strongly OCI reduces the model-based weight.
    """
    learning_rate, beta, w_base, oci_sensitivity = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]  # OCI is a scalar for the participant

    # Calculate effective mixing weight w based on OCI
    # We constrain w to be between 0 and 1.
    # Hypothesis: Higher OCI reduces w (less model-based).
    w = w_base * (1.0 / (1.0 + oci_sensitivity * current_oci))
    
    # Fixed transition matrix (Spacecraft -> Planet)
    # A -> X (0.7), U -> Y (0.7) usually
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)       # Model-free values for stage 1 (Spacecraft A/U)
    q_stage2_mf = np.zeros((2, 2))  # Model-free values for stage 2 (Planet X/Y, Alien 0/1)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based Value Calculation: V(state) = max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value: Q_net = w * Q_MB + (1-w) * Q_MF
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax Policy Stage 1
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # --- Stage 2 Choice ---
        state_idx = int(state[trial]) # 0 for Planet X, 1 for Planet Y
        
        # Softmax Policy Stage 2 (Purely Model-Free based on Aliens)
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Updating ---
        # Prediction errors
        # Note: In standard Daw task, Stage 1 MF update uses Stage 2 state value (TD(0)) or reward (TD(1))
        # Here we use a simple TD(1) style update for Stage 1 MF based on final reward for simplicity in this template context
        
        # Update Stage 2 Q-values (Direct reinforcement)
        pe_2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * pe_2
        
        # Update Stage 1 MF Q-values (TD update using Stage 2 max value as proxy for immediate next state value)
        # Alternatively, we can update based on the final reward (TD(1))
        pe_1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += learning_rate * pe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Driven Perseveration (Stickiness)
High OCI is often associated with repetitive behaviors or "stickiness." This model assumes the participant is purely Model-Free (habitual) but has a "stickiness" parameter that biases them to repeat the previous choice. The magnitude of this stickiness is scaled by their OCI score.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Pure Model-Free Q-learning with Choice Perseveration (Stickiness).
    The degree of stickiness is modulated by the OCI score.
    High OCI leads to higher probability of repeating the previous Stage 1 choice.

    Parameters:
    learning_rate: [0, 1] - Rate of updating Q-values.
    beta: [0, 10] - Inverse temperature.
    stickiness_base: [0, 5] - Baseline tendency to repeat the last choice.
    oci_amp: [0, 5] - Amplification of stickiness by OCI score.
    """
    learning_rate, beta, stickiness_base, oci_amp = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    # Calculate effective stickiness
    # Stickiness adds a bonus to the Q-value of the previously chosen action.
    effective_stickiness = stickiness_base + (oci_amp * current_oci)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    last_action_1 = -1 # No previous action on trial 0

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        q_net_stage1 = q_stage1.copy()
        
        # Add stickiness bonus if not the first trial
        if last_action_1 != -1:
            q_net_stage1[last_action_1] += effective_stickiness

        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        last_action_1 = int(action_1[trial])
        state_idx = int(state[trial])

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Updating ---
        # Standard SARSA/Q-learning updates
        
        # Stage 2 update
        pe_2 = reward[trial] - q_stage2[state_idx, int(action_2[trial])]
        q_stage2[state_idx, int(action_2[trial])] += learning_rate * pe_2
        
        # Stage 1 update (using the value of the state actually reached)
        # This makes it a Model-Free learner sensitive to the transition outcome
        pe_1 = q_stage2[state_idx, int(action_2[trial])] - q_stage1[int(action_1[trial])]
        q_stage1[int(action_1[trial])] += learning_rate * pe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Dependent Asymmetric Learning Rates
This model posits that high OCI individuals might be hyper-sensitive to negative outcomes (or lack of reward) compared to positive ones. It implements separate learning rates for positive prediction errors (learning from reward) and negative prediction errors (learning from disappointment), where the ratio or magnitude is influenced by OCI.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner with Asymmetric Learning Rates (Dual Alpha).
    OCI modulates the learning rate for negative prediction errors (alpha_neg).
    Hypothesis: High OCI participants may over-adjust to lack of reward (anxiety/avoidance).

    Parameters:
    alpha_pos: [0, 1] - Learning rate for positive prediction errors (Reward > Expectation).
    alpha_neg_base: [0, 1] - Base learning rate for negative prediction errors.
    beta: [0, 10] - Inverse temperature.
    oci_neg_boost: [0, 2] - How much OCI increases sensitivity to negative outcomes.
    """
    alpha_pos, alpha_neg_base, beta, oci_neg_boost = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    # Calculate effective negative learning rate
    # Bounded at 1.0 to prevent instability
    alpha_neg = min(1.0, alpha_neg_base * (1.0 + oci_neg_boost * current_oci))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Updating ---
        # We apply the asymmetric learning rates to both stages
        
        # Stage 2 Update
        pe_2 = reward[trial] - q_stage2[state_idx, int(action_2[trial])]
        if pe_2 >= 0:
            q_stage2[state_idx, int(action_2[trial])] += alpha_pos * pe_2
        else:
            q_stage2[state_idx, int(action_2[trial])] += alpha_neg * pe_2
            
        # Stage 1 Update
        # Using the value of the second stage choice to update the first stage
        pe_1 = q_stage2[state_idx, int(action_2[trial])] - q_stage1[int(action_1[trial])]
        if pe_1 >= 0:
            q_stage1[int(action_1[trial])] += alpha_pos * pe_1
        else:
            q_stage1[int(action_1[trial])] += alpha_neg * pe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```