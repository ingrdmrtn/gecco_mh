Here are three cognitive models designed to explain the participant's behavior, specifically incorporating their high OCI-R score (0.75) which suggests high obsessive-compulsive symptoms.

In the context of reinforcement learning and the two-step task, high OCI/compulsivity is often hypothesized to relate to:
1.  **Habitual over-reliance:** A tendency to rely more on Model-Free (habitual) learning rather than Model-Based (goal-directed) planning.
2.  **Stickiness/Perseveration:** A tendency to repeat previous choices regardless of the outcome.
3.  ** altered learning rates:** Potentially learning faster from punishments or slower from rewards.

### Model 1: The "Compulsive Habit" Model (Model-Based/Model-Free Trade-off)
This model posits that the balance between Model-Based (planning) and Model-Free (habit) systems is modulated by the OCI score. A higher OCI score shifts control towards the Model-Free system (habitual responding), making the participant less sensitive to the transition structure of the task.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-modulated Model-Based/Model-Free Trade-off.
    
    This model assumes behavior is a mix of Model-Based (MB) and Model-Free (MF) strategies.
    The mixing weight (w) is determined by the OCI score. Higher OCI leads to lower 'w'
    (less Model-Based, more Model-Free), reflecting a reliance on habit over planning.
    
    Parameters:
    learning_rate: [0, 1] - Rate at which Q-values are updated.
    beta: [0, 10] - Inverse temperature for softmax choice stochasticity.
    w_base: [0, 1] - Baseline weight for Model-Based control (before OCI modulation).
    """
    learning_rate, beta, w_base = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0] # 0.75 for this participant
    
    # OCI modulation: High OCI reduces the model-based weight 'w'.
    # We clip to ensure w stays in [0, 1].
    # If OCI is high, (1 - oci_score) is small, reducing the effective w.
    w = w_base * (1.0 - oci_score) 
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2) # Model-free values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for stage 2 (State x Action)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based Value: V_MB = T * max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value: Q_net = w * Q_MB + (1-w) * Q_MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # --- Stage 2 Policy ---
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Learning ---
        # Update Stage 1 MF
        # We use a TD-0 like update using the value of the state reached
        # Note: Some models use SARSA, here we stick to simple TD for MF
        v_state_reached = q_stage2_mf[state_idx, int(action_2[trial])] # Simple approach
        delta_stage1 = v_state_reached - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1
        
        # Update Stage 2 MF
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: The "Compulsive Perseveration" Model
This model hypothesizes that high OCI scores relate to "stickiness" or choice perseverationâ€”a compulsion to repeat the last action regardless of whether it was rewarded or whether the transition was common/rare.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-modulated Choice Perseveration.
    
    This model is purely Model-Free but includes a 'stickiness' parameter.
    The OCI score scales the stickiness: higher OCI leads to a stronger urge 
    to repeat the previous Stage 1 choice, regardless of reward history.
    
    Parameters:
    learning_rate: [0, 1]
    beta: [0, 10]
    stickiness_base: [0, 5] - Base tendency to repeat choices.
    """
    learning_rate, beta, stickiness_base = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # OCI amplifies the stickiness parameter.
    # A high OCI participant will have a very high tendency to repeat actions.
    stickiness = stickiness_base * (1.0 + oci_score)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_choice_1 = -1 # Initialize with no previous choice

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Add stickiness bonus to the Q-value of the previously chosen action
        q_stage1_modified = q_stage1_mf.copy()
        if last_choice_1 != -1:
            q_stage1_modified[last_choice_1] += stickiness

        exp_q1 = np.exp(beta * q_stage1_modified)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # Track choice for next trial
        last_choice_1 = int(action_1[trial])
        
        # --- Stage 2 Policy ---
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Learning (Standard TD) ---
        # Update Stage 1
        # Using the value of the chosen second stage action as proxy for state value
        v_next = q_stage2_mf[state_idx, int(action_2[trial])]
        delta_stage1 = v_next - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1
        
        # Update Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: The "Anxious Learning" Model (Asymmetric Learning Rate)
This model suggests that OCI (often comorbid with anxiety) affects how *quickly* a participant learns from prediction errors. Specifically, high OCI might be associated with higher sensitivity to negative outcomes (or lack of reward) compared to positive ones, or simply a generally heightened learning rate driven by hyper-vigilance. Here, we model OCI as scaling the learning rate directly.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-modulated Learning Rate (Hyper-vigilance).
    
    This model assumes a Model-Based strategy where the learning rate is 
    influenced by the OCI score. A high OCI score increases the learning rate,
    representing hyper-vigilance or rapid updating of beliefs based on 
    immediate outcomes, potentially causing volatile behavior.
    
    Parameters:
    lr_base: [0, 1] - Base learning rate.
    beta: [0, 10] - Inverse temperature.
    mb_weight: [0, 1] - Fixed weight for model-based control (not OCI modulated here).
    """
    lr_base, beta, mb_weight = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # OCI modulation: High OCI increases the effective learning rate towards 1.0.
    # This reflects 'over-learning' from single events (volatile beliefs).
    # Formula: effective_lr = lr_base + (1 - lr_base) * oci * scaling_factor
    # Simplified: We just scale the base towards 1 based on OCI intensity.
    learning_rate = lr_base + (1.0 - lr_base) * (oci_score * 0.5)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy (Hybrid) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combine MB and MF
        q_net = mb_weight * q_stage1_mb + (1 - mb_weight) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # --- Stage 2 Policy ---
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Learning ---
        # The learning rate used here is the OCI-boosted one
        
        # Update Stage 1 MF
        v_next = q_stage2_mf[state_idx, int(action_2[trial])]
        delta_stage1 = v_next - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1
        
        # Update Stage 2 MF
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```