Here are three cognitive models designed to capture different hypotheses about how obsessive-compulsive traits (OCI scores) influence decision-making in the two-step task.

### Model 1: OCI-Modulated Model-Based/Model-Free Balance
**Hypothesis:** High OCI scores are associated with a deficit in goal-directed (model-based) control and a reliance on habitual (model-free) control. This model uses the OCI score to shift the mixing weight ($w$) between model-based and model-free values in the first stage. A higher OCI reduces the influence of the model-based system.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-modulated Model-Based/Model-Free Balance.
    
    This model assumes that individuals with higher OCI scores rely less on 
    model-based planning (goal-directed) and more on model-free learning (habitual).
    The mixing parameter 'w' is linearly modulated by the OCI score.
    
    Bounds:
    learning_rate: [0,1] - Rate of updating Q-values.
    beta: [0,10] - Inverse temperature for softmax choice.
    w_base: [0,1] - Baseline weight for model-based control.
    w_oci_slope: [0,1] - Strength of OCI reduction on model-based weight.
    """
    learning_rate, beta, w_base, w_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate mixing weight w based on OCI
    # High OCI reduces w (less model-based). We clip to ensure w stays in [0,1].
    w = w_base - (w_oci_slope * oci_score)
    if w < 0: w = 0
    if w > 1: w = 1
    
    # Transition matrix: A->X (0.7), U->Y (0.7)
    # Rows: Space A (0), Space U (1)
    # Cols: Planet X (0), Planet Y (1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2) # Model-free values for stage 1 (A, U)
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for stage 2 (Planet X: W, S; Planet Y: P, H)
    
    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # Model-based calculation: V(planet) = max(Q(aliens))
        max_q_stage2 = np.max(q_stage2_mf, axis=1) 
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Q-value: mix of MB and MF
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Softmax policy for stage 1
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        # Handle missing data (-1) or valid choices
        a1 = int(action_1[trial])
        if a1 != -1:
            p_choice_1[trial] = probs_1[a1]
        else:
            p_choice_1[trial] = 1.0 # Ignore missing trials in likelihood
            
        # If data is missing for the rest of the trial, skip updates
        if a1 == -1 or state[trial] == -1 or action_2[trial] == -1:
            continue
            
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 2 Decision ---
        # Standard model-free choice based on Q-values of aliens
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning Updates ---
        
        # Stage 2 Prediction Error (Reward - Expected)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        # Update Stage 2 Q-values
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 Prediction Error (Value of State 2 - Value of Action 1)
        # Using SARSA-style update (using Q of chosen stage 2 action)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        
        # Update Stage 1 MF Q-values
        # Note: In hybrid models, eligibility traces usually pass delta_stage2 back to stage 1.
        # Here we use a simplified TD(0) for clarity within the template constraints, 
        # plus the eligibility trace logic often found in Daw et al (2011).
        q_stage1_mf[a1] += learning_rate * delta_stage1
        q_stage1_mf[a1] += learning_rate * delta_stage2 # Eligibility trace effect

    eps = 1e-10
    # Filter out 0 probabilities from missing data steps to prevent log(0)
    p_choice_1 = np.maximum(p_choice_1, eps)
    p_choice_2 = np.maximum(p_choice_2, eps)
    
    log_loss = -(np.sum(np.log(p_choice_1)) + np.sum(np.log(p_choice_2)))
    return log_loss
```

### Model 2: OCI-Driven Perseveration (Stickiness)
**Hypothesis:** Obsessive-compulsive symptoms often manifest as repetitive behaviors or "stickiness." This model introduces a choice autocorrelation (perseveration) parameter. The OCI score scales this parameter, hypothesizing that higher OCI leads to a higher tendency to repeat the previous Stage 1 choice, regardless of reward history.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Driven Perseveration.
    
    This model posits that high OCI scores increase choice 'stickiness' (perseveration).
    The model is primarily model-free, but includes a repetition bonus for the 
    previously chosen stage 1 action. The magnitude of this bonus is determined 
    by the OCI score.
    
    Bounds:
    learning_rate: [0,1]
    beta: [0,10]
    stickiness_base: [0,5] - Baseline tendency to repeat choices.
    stickiness_oci: [0,5] - Additional stickiness added per unit of OCI.
    """
    learning_rate, beta, stickiness_base, stickiness_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate total stickiness parameter
    # Higher OCI = Higher tendency to repeat previous action
    rho = stickiness_base + (stickiness_oci * oci_score)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        
        # --- Stage 1 Decision ---
        # Calculate logits (Q-values + stickiness bonus)
        logits_1 = beta * q_stage1_mf.copy()
        
        # Add stickiness bonus if there was a previous valid action
        if last_action_1 != -1:
            logits_1[last_action_1] += rho
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1)) # Subtract max for numerical stability
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        if a1 != -1:
            p_choice_1[trial] = probs_1[a1]
            last_action_1 = a1 # Update history
        else:
            p_choice_1[trial] = 1.0
            # If current action is missing, we don't update last_action_1 
            # (or we treat it as breaking the chain, here we keep the memory of the last valid one)
            
        if a1 == -1 or state[trial] == -1 or action_2[trial] == -1:
            continue

        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning Updates (Pure Model-Free TD) ---
        
        # Stage 2 RPE
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 RPE
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Note: No eligibility trace back to stage 1 from stage 2 reward in this specific
        # variant to isolate the effect of stickiness vs reinforcement, 
        # though standard TD(lambda) often includes it.

    eps = 1e-10
    p_choice_1 = np.maximum(p_choice_1, eps)
    p_choice_2 = np.maximum(p_choice_2, eps)
    log_loss = -(np.sum(np.log(p_choice_1)) + np.sum(np.log(p_choice_2)))
    return log_loss
```

### Model 3: OCI-Modulated Learning Rate Asymmetry
**Hypothesis:** Individuals with high compulsivity might over-learn from punishments (zero rewards) or under-learn from rewards, reflecting a fear of failure or perfectionism. This model splits the learning rate into positive ($\alpha_{pos}$) and negative ($\alpha_{neg}$) updates. The OCI score modulates the ratio, effectively changing how much weight is given to negative outcomes (0 coins) versus positive outcomes (1 coin).

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Learning Rate Asymmetry.
    
    This model investigates if OCI scores correlate with a bias in learning from 
    positive vs. negative prediction errors.
    The base learning rate is modified by OCI specifically for negative prediction errors 
    (when outcome is worse than expected), reflecting potential hypersensitivity to failure.
    
    Bounds:
    lr_base: [0,1] - Base learning rate.
    beta: [0,10]
    w_mb: [0,1] - Fixed model-based weight (constant across OCI).
    oci_neg_bias: [0,1] - How much OCI increases learning rate for negative PEs.
    """
    lr_base, beta, w_mb, oci_neg_bias = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        
        # --- Stage 1 Decision (Hybrid MB/MF) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w_mb * q_stage1_mb) + ((1 - w_mb) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        if a1 != -1:
            p_choice_1[trial] = probs_1[a1]
        else:
            p_choice_1[trial] = 1.0
            
        if a1 == -1 or state[trial] == -1 or action_2[trial] == -1:
            continue
            
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning Updates with Asymmetry ---
        
        # Calculate Stage 2 Prediction Error
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        # Determine effective learning rate based on sign of delta
        # If delta is negative (outcome worse than expected), OCI increases learning rate
        if delta_stage2 < 0:
            effective_lr = lr_base + (oci_neg_bias * oci_score)
            # Clip to [0, 1]
            if effective_lr > 1: effective_lr = 1
        else:
            effective_lr = lr_base
            
        q_stage2_mf[s_idx, a2] += effective_lr * delta_stage2
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        
        # We apply the same asymmetry logic to Stage 1 updates
        if delta_stage1 < 0:
            effective_lr_s1 = lr_base + (oci_neg_bias * oci_score)
            if effective_lr_s1 > 1: effective_lr_s1 = 1
        else:
            effective_lr_s1 = lr_base

        q_stage1_mf[a1] += effective_lr_s1 * delta_stage1
        
        # Eligibility trace for Stage 2 outcome impacting Stage 1
        # Using the Stage 2 effective LR for consistency with the outcome event
        q_stage1_mf[a1] += effective_lr * delta_stage2

    eps = 1e-10
    p_choice_1 = np.maximum(p_choice_1, eps)
    p_choice_2 = np.maximum(p_choice_2, eps)
    log_loss = -(np.sum(np.log(p_choice_1)) + np.sum(np.log(p_choice_2)))
    return log_loss
```