Here are three cognitive models designed to capture the behavior of a participant with high obsessive-compulsive symptoms (OCI-R score > 0.66) in a two-step decision-making task.

### Model 1: Habit-Heavy Hybrid with OCI-modulated Model-Based Control
This model hypothesizes that high OCI scores are associated with a deficit in goal-directed (model-based) control and a reliance on habitual (model-free) systems. The `w` parameter (mixing weight) is directly modulated by the OCI score, such that higher OCI leads to a lower weight on the model-based system.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free Reinforcement Learning.
    
    Hypothesis: High OCI scores correlate with reduced goal-directed (model-based) control.
    The mixing weight 'w' is modulated by OCI: w_effective = w_base * (1 - oci).
    
    Parameters:
    learning_rate: [0, 1] - Rate of updating Q-values.
    beta: [0, 10] - Inverse temperature for softmax choice.
    w_base: [0, 1] - Baseline weight for Model-Based system (0=pure MF, 1=pure MB).
    """
    learning_rate, beta, w_base = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]
    
    # Transform actions/states to 0-indexed integers for array access
    # Given data uses -1.0/1.0 or 0.0/1.0, we normalize to 0 and 1.
    # Assuming standard encoding: -1->0, 1->1 or 0->0, 1->1.
    # Based on data snippet: spaceship -1.0/1.0 -> 0/1. planet -1.0/1.0 -> 0/1 (or 0/1).
    # We will use simple mapping logic inside the loop.

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)       # Model-free values for stage 1
    q_stage2_mf = np.zeros((2, 2))  # Model-free values for stage 2 (2 states, 2 actions)
    
    # OCI Modulation: Higher OCI reduces the effective weight of the model-based system
    # If OCI is high (~0.78), w_effective becomes small, favoring MF.
    w_effective = w_base * (1.0 - participant_oci)

    for trial in range(n_trials):
        # Normalize indices to integers 0 or 1
        a1 = 0 if action_1[trial] == -1.0 else 1
        s_idx = 0 if state[trial] == -1.0 or state[trial] == 0.0 else 1 # Handling 0.0/1.0 or -1.0/1.0 variability
        a2 = 0 if action_2[trial] == -1.0 or action_2[trial] == 0.0 else 1 # Assuming alien 0/1 or -1/1

        # --- Stage 1 Policy ---
        # Model-Based Calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = w_effective * q_stage1_mb + (1 - w_effective) * q_stage1_mf
        
        # Softmax
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        # Standard Model-Free Softmax
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        # Stage 2 Update (TD error)
        r = reward[trial]
        # Handling reward encoding: if -1.0 is loss, map to 0? Or keep as scalar?
        # Usually standard RL assumes scalar rewards. The data shows -1.0, 0.0, 1.0. 
        # We use the raw value provided.
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 Update (TD-1 / SARSA-like update using Stage 2 value)
        # Note: In standard 2-step, Stage 1 MF is updated by the value of the state reached (s_idx)
        # or the chosen action at stage 2. A common implementation uses q_stage2_mf[s_idx, a2].
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Perseveration Bias Model
This model hypothesizes that high OCI leads to "stickiness" or perseverationâ€”a tendency to repeat the previous action regardless of the outcome or model-based inference. The OCI score scales a perseveration parameter `pers`, making high-OCI individuals more likely to repeat their Stage 1 choice.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Based/Model-Free Hybrid with OCI-driven Perseveration.
    
    Hypothesis: High OCI scores increase choice stickiness (perseveration).
    An extra 'bonus' is added to the Q-value of the previously chosen action,
    scaled by OCI.
    
    Parameters:
    learning_rate: [0, 1]
    beta: [0, 10]
    w: [0, 1] - Mixing weight (fixed, not modulated here).
    pers_base: [0, 5] - Baseline perseveration bonus.
    """
    learning_rate, beta, w, pers_base = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Calculate effective perseveration: Base parameter * OCI intensity
    pers_effective = pers_base * participant_oci
    
    # Track previous choice (initialize to None or -1)
    last_action_1 = -1

    for trial in range(n_trials):
        a1 = 0 if action_1[trial] == -1.0 else 1
        s_idx = 0 if state[trial] == -1.0 or state[trial] == 0.0 else 1
        a2 = 0 if action_2[trial] == -1.0 or action_2[trial] == 0.0 else 1
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add perseveration bonus
        if last_action_1 != -1:
            q_net[last_action_1] += pers_effective
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Update last action
        last_action_1 = a1

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Learning Rate Asymmetry (Anxiety Bias)
This model posits that high OCI (often comorbid with anxiety) leads to an asymmetry in how positive vs. negative prediction errors are processed. High OCI participants might be more sensitive to punishments (negative RPEs) or less sensitive to rewards. Here, OCI increases the learning rate for negative outcomes (losses), reflecting a hyper-responsiveness to failure.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Pure Model-Free with Asymmetric Learning Rates modulated by OCI.
    
    Hypothesis: High OCI individuals are hyper-sensitive to negative outcomes.
    Learning rate for negative prediction errors (alpha_neg) is boosted by OCI.
    
    Parameters:
    alpha_pos: [0, 1] - Learning rate for positive prediction errors.
    alpha_neg_base: [0, 1] - Base learning rate for negative prediction errors.
    beta: [0, 10]
    """
    alpha_pos, alpha_neg_base, beta = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Pure Model-Free setup (no transition matrix used for decision)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Effective negative learning rate scales with OCI
    # We clip to max 1.0 to ensure stability
    alpha_neg_effective = min(1.0, alpha_neg_base * (1.0 + participant_oci))

    for trial in range(n_trials):
        a1 = 0 if action_1[trial] == -1.0 else 1
        s_idx = 0 if state[trial] == -1.0 or state[trial] == 0.0 else 1
        a2 = 0 if action_2[trial] == -1.0 or action_2[trial] == 0.0 else 1
        r = reward[trial]

        # --- Stage 1 Policy (Pure MF) ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        lr2 = alpha_pos if delta_stage2 >= 0 else alpha_neg_effective
        q_stage2_mf[s_idx, a2] += lr2 * delta_stage2
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        lr1 = alpha_pos if delta_stage1 >= 0 else alpha_neg_effective
        q_stage1_mf[a1] += lr1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```