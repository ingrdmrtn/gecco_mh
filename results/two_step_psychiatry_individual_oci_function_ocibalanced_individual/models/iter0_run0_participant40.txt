Here are three cognitive models incorporating the Obsessive-Compulsive Inventory (OCI) score into the reinforcement learning framework for the two-step task.

In this context, high OCI scores (specifically > 0.66, as seen in this participant) are often hypothesized to relate to:
1.  **Habitual control:** A stronger reliance on model-free (habitual) learning over model-based (goal-directed) planning.
2.  **Stickiness/Perseveration:** A tendency to repeat previous actions regardless of outcomes.
3.  **Learning Rate Rigidity:** Differences in how quickly positive vs. negative feedback is integrated.

### Model 1: OCI-Modulated Model-Based/Model-Free Balance
This model hypothesizes that the balance between Model-Based (planning) and Model-Free (habit) systems is modulated by the OCI score. Specifically, higher OCI scores might push the agent towards more Model-Free behavior (less reliance on the transition matrix).

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-modulated Model-Based vs. Model-Free weighting.
    
    This model assumes behavior is a mix of Model-Based (MB) and Model-Free (MF) control.
    The mixing parameter 'w' determines the relative contribution.
    We hypothesize that 'w' is a linear function of the OCI score:
    w_effective = w_base + w_oci * oci
    (clipped between 0 and 1).
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for value updates.
    - beta: [0, 10] Inverse temperature for softmax.
    - w_base: [0, 1] Baseline weighting for Model-Based control (0=Pure MF, 1=Pure MB).
    - w_oci: [-1, 1] How OCI score shifts the balance. Negative implies OCI reduces MB control.
    """
    learning_rate, beta, w_base, w_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective weight w based on OCI
    # If w_oci is negative, high OCI reduces model-based control (more habitual)
    w = w_base + (w_oci * oci_score)
    w = np.clip(w, 0.0, 1.0) 

    # Fixed transition matrix (A->X=0.7, U->Y=0.7)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)      # Model-free values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for stage 2 (2 states, 2 actions)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based Value: Transition * Max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Value: w * MB + (1-w) * MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax choice 1
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        current_state = int(state[trial])
        current_a1 = int(action_1[trial])
        
        # --- Stage 2 Policy ---
        # Standard softmax on stage 2 Q-values
        exp_q2 = np.exp(beta * q_stage2_mf[current_state])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        current_a2 = int(action_2[trial])
        r = reward[trial]

        # --- Learning ---
        # 1. Update Stage 2 Q-values (TD error)
        # Q2(s, a2) += alpha * (r - Q2(s, a2))
        delta_stage2 = r - q_stage2_mf[current_state, current_a2]
        q_stage2_mf[current_state, current_a2] += learning_rate * delta_stage2
        
        # 2. Update Stage 1 MF Q-values (TD error using Q2)
        # Q1(a1) += alpha * (Q2(s, a2) - Q1(a1)) + lambda * delta_stage2 (simplified here to standard TD)
        # We use the updated Q2 value for the TD target of stage 1
        delta_stage1 = q_stage2_mf[current_state, current_a2] - q_stage1_mf[current_a1]
        q_stage1_mf[current_a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Driven Choice Perseveration (Stickiness)
High OCI scores are associated with repetitive behaviors. This model tests the hypothesis that OCI modulates "choice stickiness"â€”the tendency to repeat the previous first-stage action regardless of reward history.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-modulated Choice Stickiness.
    
    This model is a pure Model-Free learner but includes a 'stickiness' parameter 
    that biases the agent to repeat the last chosen action at Stage 1.
    The magnitude of this stickiness is scaled by the OCI score.
    
    Parameters:
    - learning_rate: [0, 1]
    - beta: [0, 10]
    - stickiness_base: [-5, 5] Base tendency to repeat (positive) or switch (negative).
    - stickiness_oci: [0, 5] Additional stickiness contributed by OCI score.
    """
    learning_rate, beta, stickiness_base, stickiness_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate total stickiness
    # High OCI -> Higher tendency to repeat action_1
    phi = stickiness_base + (stickiness_oci * oci_score)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    last_action_1 = -1 # No previous action for the first trial

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Add stickiness bonus to the Q-value of the previously chosen action
        q_net = q_stage1.copy()
        if last_action_1 != -1:
            q_net[last_action_1] += phi
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        current_state = int(state[trial])
        current_a1 = int(action_1[trial])
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2[current_state])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        current_a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Learning ---
        # Update Stage 2
        delta_stage2 = r - q_stage2[current_state, current_a2]
        q_stage2[current_state, current_a2] += learning_rate * delta_stage2
        
        # Update Stage 1 (SARSA-style TD)
        delta_stage1 = q_stage2[current_state, current_a2] - q_stage1[current_a1]
        q_stage1[current_a1] += learning_rate * delta_stage1
        
        # Store action for next trial's stickiness
        last_action_1 = current_a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Dependent Learning Rate Asymmetry
This model investigates if OCI affects how individuals learn from positive versus negative outcomes. It posits that high OCI might be associated with hypersensitivity to negative outcomes (or lack of reward), leading to different learning rates for rewarded vs. unrewarded trials, modulated by the OCI score.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Dependent Learning Rate Asymmetry.
    
    This model splits the learning rate into alpha_pos (for rewards) and alpha_neg (for non-rewards).
    The OCI score modulates the alpha_neg specifically, testing if high OCI leads to 
    over-correction or rigidity after failure (0 coins).
    
    Parameters:
    - alpha_pos: [0, 1] Learning rate for rewarded trials.
    - alpha_neg_base: [0, 1] Base learning rate for unrewarded trials.
    - alpha_neg_oci: [-1, 1] Modulation of negative learning rate by OCI.
    - beta: [0, 10]
    """
    alpha_pos, alpha_neg_base, alpha_neg_oci, beta = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective negative learning rate
    # e.g., if alpha_neg_oci is positive, high OCI learns faster from failures
    alpha_neg = alpha_neg_base + (alpha_neg_oci * oci_score)
    alpha_neg = np.clip(alpha_neg, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Using a simple hybrid MB/MF structure where MB is fixed (pure MB planning on stage 1)
    # but the Stage 2 values are learned via the asymmetric rates.
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy (Pure Model-Based for simplicity of this specific mechanism) ---
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        current_state = int(state[trial])
        current_a2 = int(action_2[trial])
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2[current_state])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[current_a2]
        
        r = reward[trial]
        
        # --- Learning ---
        # Select learning rate based on outcome
        if r > 0:
            eff_alpha = alpha_pos
        else:
            eff_alpha = alpha_neg
            
        # Update Stage 2 values
        delta = r - q_stage2[current_state, current_a2]
        q_stage2[current_state, current_a2] += eff_alpha * delta
        
        # Note: In this simplified MB model, we don't update a separate Q_stage1_MF
        # because the agent plans using the updated Q_stage2.

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```