Here are three cognitive models designed to capture the behavior of a participant with a high OCI score (1.0), focusing on how obsessive-compulsive traits might influence reinforcement learning in a two-step task.

### Model 1: Habitual Over-Reliance with OCI Modulation
**Hypothesis:** High OCI scores are often associated with excessive habit formation (Model-Free control) and a deficit in goal-directed (Model-Based) planning. This model posits that the mixing weight `w` (balancing Model-Based and Model-Free values) is directly modulated by the OCI score. A higher OCI score pushes the agent towards pure Model-Free learning, ignoring the transition structure of the task.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: Habitual Over-Reliance.
    
    This model assumes that the balance between Model-Based (MB) and Model-Free (MF) 
    control is determined by the OCI score. Higher OCI scores lead to a lower mixing 
    weight 'w', resulting in behavior dominated by habitual (MF) values rather than 
    goal-directed (MB) planning.
    
    Parameters:
    learning_rate: [0, 1] Rate at which Q-values are updated.
    beta: [0, 10] Inverse temperature for softmax choice (exploration/exploitation).
    w_baseline: [0, 1] The baseline mixing weight for an individual with OCI=0.
    """
    learning_rate, beta, w_baseline = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective mixing weight w based on OCI.
    # High OCI reduces w, making the agent more Model-Free (habitual).
    # We clip to ensure it stays in [0, 1].
    w = w_baseline * (1.0 - oci_score)
    w = np.clip(w, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2) # Model-free values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for stage 2 (2 states, 2 actions)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based Value Calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Q-value (Hybrid)
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax choice 1
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial]) # The planet arrived at (0 or 1)
        
        # --- Stage 2 Policy ---
        # Standard model-free choice at the second stage
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # --- Learning / Updates ---
        # Prediction errors
        # 1. Stage 1 MF update (SARSA-like logic for stage 1 MF)
        # Note: In standard 2-step models, stage 1 MF is often updated by the stage 2 Q-value 
        # of the chosen action (TD(0)) or the reward (TD(1)). Here we use a simple TD(1) style 
        # direct reinforcement from the final reward for simplicity in this template structure, 
        # or the value of the next state. Let's use the standard TD-error from the second stage value.
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1
        
        # 2. Stage 2 MF update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: "Stickiness" and Perseveration
**Hypothesis:** Individuals with high OCI often exhibit repetitive behaviors or "stickiness" to previous choices, regardless of the reward outcome. This model introduces a choice autocorrelation (stickiness) parameter that is amplified by the OCI score.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Amplified Choice Stickiness.
    
    This model assumes the participant is primarily Model-Free but exhibits 
    strong choice perseveration (stickiness). The degree of stickiness is 
    scaled by the OCI score: higher OCI = higher tendency to repeat the 
    previous Stage 1 action, regardless of reward.
    
    Parameters:
    learning_rate: [0, 1] Learning rate for Q-values.
    beta: [0, 10] Inverse temperature.
    stickiness_base: [0, 5] Base bonus added to the Q-value of the previously chosen action.
    """
    learning_rate, beta, stickiness_base = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Stickiness is amplified by OCI. 
    # If OCI is 1.0, stickiness is doubled relative to base (or just scaled up).
    # Here we define effective stickiness as base * (1 + OCI).
    eff_stickiness = stickiness_base * (1.0 + oci_score)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1 # Initialize with no previous action

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        # Calculate values
        current_values = q_stage1_mf.copy()
        
        # Add stickiness bonus if a previous action exists
        if last_action_1 != -1:
            current_values[last_action_1] += eff_stickiness
        
        exp_q1 = np.exp(beta * current_values)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        # --- Updates ---
        # Standard TD learning
        # Update Stage 1 based on Stage 2 value (TD(0))
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1
        
        # Update Stage 2 based on reward
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2
        
        # Store action for next trial's stickiness
        last_action_1 = int(action_1[trial])

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hyper-Learning / Over-Reaction to Punishment
**Hypothesis:** High OCI is associated with anxiety and an intolerance of uncertainty or failure. This model proposes that the participant has separate learning rates for positive (reward) and negative (no reward) outcomes. Specifically, the OCI score amplifies the learning rate for negative outcomes (`alpha_neg`), causing the agent to over-adjust behavior after a failure (0 coins), reflecting a "fear of failure" mechanism.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: Asymmetric Learning Rates (Anxiety/Failure Focus).
    
    This model posits that OCI modulates the learning rate specifically for 
    negative outcomes (0 reward). High OCI leads to a higher `alpha_neg`, 
    meaning the participant abandons non-rewarded options more aggressively 
    than they reinforce rewarded ones.
    
    Parameters:
    alpha_pos: [0, 1] Learning rate for positive rewards (coins = 1).
    alpha_neg_base: [0, 1] Base learning rate for negative rewards (coins = 0).
    beta: [0, 10] Inverse temperature.
    """
    alpha_pos, alpha_neg_base, beta = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective negative learning rate.
    # High OCI increases sensitivity to negative feedback.
    # We blend the base rate towards 1.0 based on OCI.
    alpha_neg = alpha_neg_base + (1.0 - alpha_neg_base) * oci_score * 0.8 
    # The 0.8 is a scaling factor to prevent it from always hitting 1.0 immediately.
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # We use a hybrid structure here to allow the learning rate effect to propagate
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy (Model-Based/Free Hybrid fixed at 0.5 for simplicity to isolate LR effect) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Fixed 50/50 mix for this model to focus on the learning rate mechanism
        q_net = 0.5 * q_stage1_mb + 0.5 * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        # --- Updates ---
        # Determine which learning rate to use based on the outcome
        if reward[trial] > 0:
            lr = alpha_pos
        else:
            lr = alpha_neg
            
        # Update Stage 1 (using Stage 2 value as proxy for outcome)
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += lr * delta_stage1
        
        # Update Stage 2 (using actual reward)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += lr * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```