Here are three cognitive models designed to capture the behavior of a participant in the two-step task, specifically incorporating their high OCI-R score (indicating high obsessive-compulsive symptoms).

### Model 1: The "Compulsive Habit" Hybrid Model
**Hypothesis:** High OCI scores are associated with an over-reliance on Model-Free (habitual) learning and a deficit in Model-Based (goal-directed) planning. This model uses the OCI score to weigh the balance between these two systems. A high OCI score shifts the weight `w` towards 0 (pure model-free), while a low score shifts it towards 1.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free Reinforcement Learning.
    
    The balance between Model-Based (MB) and Model-Free (MF) control is modulated 
    by the OCI score. High OCI implies stronger habit formation (MF dominance).
    
    w_base: Baseline mixing parameter [0, 1]. 
            w = w_base * (1 - oci) implies high OCI reduces MB influence.
    learning_rate: Alpha for TD updates [0, 1].
    beta: Inverse temperature for softmax [0, 10].
    """
    w_base, learning_rate, beta = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate the effective mixing weight w.
    # If OCI is high (near 1), w becomes small (dominance of MF).
    # If OCI is low, w is closer to w_base.
    w = w_base * (1.0 - oci_score)

    # Fixed transition matrix: A->X (70%), U->Y (70%)
    # Row 0: A (to X, Y), Row 1: U (to X, Y)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)      # Model-free values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Values for stage 2 (State X/Y, Alien 1/2)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based Calculation: V_MB(s1) = T * max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value: Q_net = w * Q_MB + (1-w) * Q_MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax choice 1
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # --- Stage 2 Policy ---
        state_idx = int(state[trial])
        
        # Standard softmax on stage 2 values
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Learning / Updates ---
        # Prediction errors
        # Stage 1 TD error (using the value of the state actually reached)
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1
        
        # Stage 2 TD error
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: The "Stickiness" Perseveration Model
**Hypothesis:** Obsessive-compulsive traits are often linked to behavioral rigidity or "stickiness"â€”a tendency to repeat previous actions regardless of outcome. This model introduces a choice autocorrelation parameter (`stickiness`) that is amplified by the OCI score.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free RL with OCI-modulated Perseveration (Stickiness).
    
    High OCI scores increase the tendency to repeat the previous Stage 1 choice,
    regardless of reward history.
    
    learning_rate: [0, 1]
    beta: [0, 10]
    stickiness_base: Base tendency to repeat choices [0, 5].
                     Effective stickiness = stickiness_base * (1 + oci).
    """
    learning_rate, beta, stickiness_base = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Amplify stickiness based on OCI.
    # A high OCI (approx 1.0) doubles the base stickiness compared to OCI=0.
    stickiness_param = stickiness_base * (1.0 + oci_score)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    # Track previous choice for stickiness (initialize to -1 or handle first trial)
    last_choice_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Add stickiness bonus to the Q-values
        q_stage1_modified = q_stage1.copy()
        if last_choice_1 != -1:
            q_stage1_modified[last_choice_1] += stickiness_param

        exp_q1 = np.exp(beta * q_stage1_modified)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # Update last choice
        last_choice_1 = int(action_1[trial])
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        # --- Learning ---
        # SARSA / TD(0) style updates
        # Update Stage 1 Q-value based on the value of the state reached (Stage 2 Q-value)
        delta_stage1 = q_stage2[state_idx, int(action_2[trial])] - q_stage1[int(action_1[trial])]
        q_stage1[int(action_1[trial])] += learning_rate * delta_stage1
        
        # Update Stage 2 Q-value based on reward
        delta_stage2 = reward[trial] - q_stage2[state_idx, int(action_2[trial])]
        q_stage2[state_idx, int(action_2[trial])] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: The "Anxious Learning" Asymmetric Update Model
**Hypothesis:** High OCI is often comorbid with anxiety and negative affect. This model posits that high OCI individuals learn differently from positive versus negative prediction errors. Specifically, high OCI might lead to "punishment sensitivity" (or lack of reward sensitivity), modeled here as a learning rate that scales down for positive outcomes based on the OCI score.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Asymmetric Learning Rate Model.
    
    This model assumes high OCI participants have dampened learning from 
    positive prediction errors (anhedonia/pessimism) or heightened learning
    from negative ones.
    
    lr_base: Base learning rate [0, 1].
    beta: Inverse temperature [0, 10].
    lambda_param: Asymmetry factor [0, 1]. 
                  If delta > 0, effective_lr = lr_base * (1 - lambda * oci).
                  High OCI + High Lambda = Reduced learning from "good" surprises.
    """
    lr_base, beta, lambda_param = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Pure Model-Based setup for Stage 1 evaluation, but with asymmetric Stage 2 updates
    # We track values, but the learning rate changes dynamically.
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy (Pure Model-Free for simplicity in this variant) ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        # --- Learning ---
        
        # Calculate Prediction Errors
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        
        # Determine Effective Learning Rate for Stage 2 (Reward learning)
        # If the outcome was better than expected (delta > 0), high OCI might dampen the update.
        if delta_stage2 > 0:
            eff_lr_2 = lr_base * (1.0 - (lambda_param * oci_score))
        else:
            # Normal learning from negative/neutral outcomes
            eff_lr_2 = lr_base
            
        # Apply updates
        # Stage 1 update (using base LR for transition learning)
        q_stage1_mf[int(action_1[trial])] += lr_base * delta_stage1
        
        # Stage 2 update (using asymmetric LR)
        q_stage2_mf[state_idx, int(action_2[trial])] += eff_lr_2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```