Here are three cognitive models designed to explain the participant's behavior, incorporating their high OCI-R score (indicating high obsessive-compulsive symptoms).

### Model 1: The "Habit-Prone" Hybrid Model
**Hypothesis:** Participants with high OCI scores rely more heavily on Model-Free (habitual) learning rather than Model-Based (planning) learning. This model modulates the mixing weight (`w`) between the two systems based on the OCI score. A higher OCI score pushes `w` towards 0 (pure model-free), reflecting a tendency towards repetitive, habit-driven actions despite changes in the environment structure.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-modulated Model-Based vs. Model-Free trade-off.
    
    This model assumes that high OCI scores are associated with a deficit in model-based control
    and a reliance on model-free (habitual) reinforcement learning.
    
    The mixing parameter 'w' determines the balance between Model-Based (MB) and Model-Free (MF) values.
    We model 'w' as a logistic function of the raw mixing parameter and the OCI score.
    High OCI reduces the effective 'w', leading to more MF influence.

    Bounds:
    learning_rate: [0,1] - Rate of updating Q-values.
    beta: [0,10] - Inverse temperature for softmax choice.
    w_base: [0,1] - Baseline mixing weight (1=Full MB, 0=Full MF).
    oci_penalty: [0,5] - Strength of OCI reduction on model-based control.
    """
    learning_rate, beta, w_base, oci_penalty = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective w: w_base adjusted by OCI. 
    # High OCI reduces w, pushing behavior toward Model-Free.
    # We clip to ensure it stays in [0, 1].
    w = w_base - (oci_penalty * oci_score)
    w = np.clip(w, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)      # Model-free values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for stage 2 (State x Action)

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # Model-Based Value Calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value: Weighted sum of MB and MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax Policy Stage 1
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        curr_state = int(state[trial])
        curr_a1 = int(action_1[trial])

        # --- Stage 2 Decision ---
        # Standard Q-learning at second stage
        exp_q2 = np.exp(beta * q_stage2_mf[curr_state])
        probs_2 = exp_q2 / np.sum(exp_q2)
        curr_a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[curr_a2]

        # --- Learning / Updates ---
        r = reward[trial]
        
        # Prediction errors
        pe_1 = q_stage2_mf[curr_state, curr_a2] - q_stage1_mf[curr_a1]
        pe_2 = r - q_stage2_mf[curr_state, curr_a2]
        
        # Update Stage 1 MF value (TD(0))
        q_stage1_mf[curr_a1] += learning_rate * pe_1
        
        # Update Stage 2 MF value
        q_stage2_mf[curr_state, curr_a2] += learning_rate * pe_2
        
        # Note: Eligibility traces (TD(lambda)) are often used to update Stage 1 
        # based on Stage 2 outcome, but omitted here to stay within parameter limits 
        # and focus on the 'w' mechanism.

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: The "Sticky" Perseveration Model
**Hypothesis:** High OCI scores are linked to behavioral rigidity or "stickiness"â€”a tendency to repeat the previous choice regardless of reward outcomes. This model adds a choice autocorrelation (perseveration) parameter that scales with the OCI score. A high OCI score increases the "stickiness" bonus added to the previously chosen option.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-modulated Choice Perseveration (Stickiness).
    
    This model assumes that high OCI scores lead to 'stickiness' or behavioral rigidity.
    It implements a pure Model-Free learner but adds a perseveration bonus to the Q-values
    of the action taken on the previous trial. The magnitude of this bonus is determined
    by the OCI score.

    Bounds:
    learning_rate: [0,1]
    beta: [0,10]
    stickiness_base: [0,5] - Base tendency to repeat choices.
    oci_stickiness_mult: [0,5] - Additional stickiness multiplier based on OCI.
    """
    learning_rate, beta, stickiness_base, oci_stickiness_mult = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate total stickiness parameter
    # High OCI increases the tendency to repeat the previous action
    stickiness = stickiness_base + (oci_stickiness_mult * oci_score)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    last_action_1 = -1 # Initialize with invalid action

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # Calculate effective values (Q + stickiness bonus)
        q_eff_1 = q_stage1.copy()
        if last_action_1 != -1:
            q_eff_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_eff_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        curr_a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[curr_a1]
        
        curr_state = int(state[trial])
        
        # --- Stage 2 Decision ---
        # No stickiness modeled for stage 2 in this simple version
        exp_q2 = np.exp(beta * q_stage2[curr_state])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        curr_a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[curr_a2]
        
        # --- Learning ---
        r = reward[trial]
        
        # SARSA / TD updates
        # Update Stage 1 based on Stage 2 value (TD(0))
        pe_1 = q_stage2[curr_state, curr_a2] - q_stage1[curr_a1]
        q_stage1[curr_a1] += learning_rate * pe_1
        
        # Update Stage 2 based on reward
        pe_2 = r - q_stage2[curr_state, curr_a2]
        q_stage2[curr_state, curr_a2] += learning_rate * pe_2
        
        # Update history
        last_action_1 = curr_a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: The "Anxious Learning" Model (High Learning Rate)
**Hypothesis:** High OCI scores (often comorbid with anxiety) might lead to hyper-reactivity to prediction errors. Instead of integrating rewards slowly over time, individuals with high OCI might update their beliefs too rapidly (high learning rate), making their behavior unstable or "jittery" in response to noise. This model scales the learning rate `alpha` by the OCI score.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-modulated Learning Rate (Hyper-reactivity).
    
    This model assumes high OCI is linked to over-sensitivity to immediate outcomes.
    The learning rate (alpha) is modeled as a base rate plus an OCI-dependent component.
    A high OCI score results in a higher learning rate, causing the agent to 'chase'
    recent rewards or punishments too aggressively (volatility).
    
    It uses a simplified hybrid architecture (fixed weighting) but focuses the OCI effect
    on the learning rate itself.

    Bounds:
    alpha_base: [0,1] - Base learning rate.
    alpha_oci_sens: [0,1] - Sensitivity of learning rate to OCI score.
    beta: [0,10] - Inverse temperature.
    mb_weight: [0,1] - Fixed weight for Model-Based control.
    """
    alpha_base, alpha_oci_sens, beta, mb_weight = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective learning rate
    # We constrain the sum to be <= 1.0 to maintain validity
    learning_rate = alpha_base + (alpha_oci_sens * oci_score)
    learning_rate = np.clip(learning_rate, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # MB Component
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = mb_weight * q_stage1_mb + (1 - mb_weight) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        curr_a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[curr_a1]
        
        curr_state = int(state[trial])
        
        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[curr_state])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        curr_a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[curr_a2]
        
        # --- Learning ---
        r = reward[trial]
        
        # Update Stage 1 MF
        # Note: Using simple Q-learning update for Stage 1 MF based on Stage 2 Q-value
        # This represents the cached value of the chosen action
        pe_1 = q_stage2_mf[curr_state, curr_a2] - q_stage1_mf[curr_a1]
        q_stage1_mf[curr_a1] += learning_rate * pe_1
        
        # Update Stage 2 MF
        pe_2 = r - q_stage2_mf[curr_state, curr_a2]
        q_stage2_mf[curr_state, curr_a2] += learning_rate * pe_2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```