Here are three cognitive models designed to explain the behavior of a participant with high obsessive-compulsive symptoms (OCI score = 0.7) in a two-step decision task.

These models hypothesize different ways high OCI might influence reinforcement learning:
1.  **Model 1 (Habitual Reliance):** High OCI leads to a stronger reliance on "model-free" (habitual) learning over "model-based" (goal-directed) planning.
2.  **Model 2 (Perseveration/Stickiness):** High OCI manifests as a tendency to repeat previous choices regardless of reward (behavioral stickiness).
3.  **Model 3 (Rigid Learning):** High OCI results in inflexible learning rates, specifically a reduced ability to update values after negative outcomes (loss aversion or rigidity).

### Model 1: The "Compulsive Habit" Model
**Hypothesis:** Participants with high OCI symptoms rely more heavily on habit-based (model-free) learning strategies and less on goal-directed (model-based) planning. The parameter `w` (mixing weight) is modulated by the OCI score, pushing the agent towards pure model-free behavior as OCI increases.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Model-Based/Model-Free Trade-off.
    
    This model assumes that high OCI scores are associated with a shift away from 
    goal-directed (model-based) control towards habitual (model-free) control.
    The mixing parameter 'w' is dynamically adjusted by the OCI score.
    
    Parameters:
    learning_rate: [0,1] - Rate at which Q-values are updated.
    beta: [0,10] - Inverse temperature for softmax choice rule.
    w_base: [0,1] - Baseline mixing weight (1=Model-Based, 0=Model-Free).
    """
    learning_rate, beta, w_base = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0] # 0.7 for this participant

    # Modulate mixing weight w by OCI. 
    # High OCI reduces w, pushing behavior towards Model-Free (0).
    # We constrain the effective w to be between 0 and 1.
    w = w_base * (1.0 - oci_score) 
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2) # Model-free values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for stage 2 (2 states, 2 actions)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based value calculation: Transition * Max(Stage 2 Values)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Q-value: weighted sum of MB and MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial]) # The planet landed on (0 or 1)

        # --- Stage 2 Choice ---
        # Standard model-free choice at the second stage
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Learning ---
        # Stage 2 Prediction Error (Reward - Expected)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2
        
        # Stage 1 Prediction Error (TD(1) approach: uses Stage 2 prediction error to update Stage 1)
        # Note: In standard MB/MF models, MF Stage 1 is updated by the value of the state reached.
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1 
        # Additionally, eligibility trace often passes the stage 2 RPE back to stage 1
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: The "Sticky Compulsion" Model
**Hypothesis:** High OCI is linked to behavioral rigidity or "stickiness"â€”a tendency to repeat the previous action regardless of the outcome. This model adds a choice autocorrelation parameter (`stickiness`) that is amplified by the OCI score.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Amplified Choice Stickiness (Perseveration).
    
    This model posits that high OCI leads to repetitive behavior. The 'stickiness'
    parameter adds a bonus to the previously chosen action. The magnitude of this
    bonus is scaled by the OCI score.
    
    Parameters:
    learning_rate: [0,1]
    beta: [0,10]
    stickiness_base: [0,5] - Base tendency to repeat choices.
    """
    learning_rate, beta, stickiness_base = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Effective stickiness is the base parameter scaled up by OCI severity.
    # Higher OCI = Higher tendency to repeat Action 1.
    eff_stickiness = stickiness_base * (1.0 + oci_score)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1 # Initialize as no previous action

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Pure Model-Free logic for simplicity, but with added Stickiness
        q_values_s1 = q_stage1_mf.copy()
        
        # Add stickiness bonus if not the first trial
        if last_action_1 != -1:
            q_values_s1[last_action_1] += eff_stickiness

        exp_q1 = np.exp(beta * q_values_s1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Learning ---
        # Update Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2
        
        # Update Stage 1 (SARSA-like update using value of next state)
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1
        
        # Store action for next trial's stickiness
        last_action_1 = int(action_1[trial])

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: The "Rigid Updating" Model
**Hypothesis:** High OCI creates an asymmetry in learning from positive versus negative outcomes. Specifically, high OCI might be associated with an inability to "unlearn" or reduce value estimates after failure (a form of rigidity). This model splits the learning rate into positive (`alpha_pos`) and negative (`alpha_neg`), where `alpha_neg` is dampened by the OCI score.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Dampened Negative Learning Rate.
    
    This model suggests that high OCI interferes with the ability to update 
    value expectations downwards after a lack of reward (rigidity).
    The learning rate for negative prediction errors is reduced by the OCI score.
    
    Parameters:
    alpha_pos: [0,1] - Learning rate for positive prediction errors (Reward > Expected).
    alpha_neg_base: [0,1] - Base learning rate for negative prediction errors.
    beta: [0,10]
    """
    alpha_pos, alpha_neg_base, beta = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Dampen the negative learning rate by OCI.
    # Higher OCI -> Lower alpha_neg -> Slower unlearning of bad options.
    alpha_neg = alpha_neg_base * (1.0 - (0.5 * oci_score)) 

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Simple Model-Free Q-learning throughout
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Learning ---
        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2[state_idx, int(action_2[trial])]
        
        # Select learning rate based on sign of prediction error
        lr_s2 = alpha_pos if delta_stage2 >= 0 else alpha_neg
        q_stage2[state_idx, int(action_2[trial])] += lr_s2 * delta_stage2
        
        # Stage 1 Update
        # Using the value of the chosen stage 2 state as the target
        target_val = q_stage2[state_idx, int(action_2[trial])]
        delta_stage1 = target_val - q_stage1[int(action_1[trial])]
        
        lr_s1 = alpha_pos if delta_stage1 >= 0 else alpha_neg
        q_stage1[int(action_1[trial])] += lr_s1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```