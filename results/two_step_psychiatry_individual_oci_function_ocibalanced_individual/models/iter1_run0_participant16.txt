Here are three new cognitive models that explore different mechanisms by which Obsessive-Compulsive Inventory (OCI) scores might influence decision-making in the two-step task.

### Model 1: OCI-Modulated Model-Based Weighting
This model tests the hypothesis that OCI symptoms interfere with goal-directed (Model-Based) planning. High OCI scores might reflect a reliance on habitual (Model-Free) control or a deficit in using the transition structure of the task. Here, the mixing weight `w` (which balances Model-Based and Model-Free control) is a function of the OCI score.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Model-Based Weighting.
    
    This model assumes that the balance between Model-Based (MB) and Model-Free (MF)
    control is influenced by the participant's OCI score. Specifically, the weight `w`
    is derived from a base parameter plus an OCI-dependent shift.
    
    Parameters:
    lr: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    w_base: [0, 1] Base mixing weight (0=Pure MF, 1=Pure MB) for low OCI.
    oci_w_mod: [-1, 1] Modulation of w by OCI.
    """
    lr, beta, w_base, oci_w_mod = model_parameters
    n_trials = len(action_1)
    oci = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    # Calculate the effective mixing weight w, constrained to [0, 1]
    # If oci_w_mod is negative, higher OCI reduces MB control.
    w = w_base + (oci * oci_w_mod)
    w = np.clip(w, 0.0, 1.0)

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combined Q-value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])

        # policy for the second choice
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        # Update Stage 1 MF
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += lr * delta_stage1
        
        # Update Stage 2 MF
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += lr * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Driven Punishment Sensitivity
This model investigates whether OCI scores relate to an increased sensitivity to "punishment" (lack of reward). It uses separate learning rates for positive outcomes (rewards) and negative outcomes (no reward). The learning rate for negative outcomes is modulated by the OCI score, hypothesizing that individuals with higher OCI might over-learn from failures (or under-learn, depending on the fit).

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Driven Punishment Sensitivity.
    
    This model separates learning from positive rewards (lr_pos) and 
    negative outcomes/omissions (lr_neg). The negative learning rate is 
    modulated by OCI, testing if symptoms correlate with over-sensitivity 
    to failure.
    
    Parameters:
    lr_pos: [0, 1] Learning rate for rewarded trials.
    lr_neg_base: [0, 1] Base learning rate for unrewarded trials.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Mixing weight (MB vs MF).
    oci_neg_mod: [-1, 1] How OCI modifies the negative learning rate.
    """
    lr_pos, lr_neg_base, beta, w, oci_neg_mod = model_parameters
    n_trials = len(action_1)
    oci = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Calculate effective negative learning rate
    lr_neg = lr_neg_base + (oci * oci_neg_mod)
    lr_neg = np.clip(lr_neg, 0.0, 1.0)

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])

        # policy for the second choice
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        # Determine current trial learning rate based on REWARD
        current_lr = lr_pos if reward[trial] == 1 else lr_neg

        # Update Stage 1 MF
        # Note: Standard TD uses the same LR for both stages usually, or splits them. 
        # Here we apply the punishment sensitivity logic to the final outcome update.
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += current_lr * delta_stage1
        
        # Update Stage 2 MF
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += current_lr * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Choice Randomness (Temperature)
This model tests if OCI scores influence the exploration/exploitation trade-off. Specifically, it posits that the inverse temperature parameter `beta` (which controls choice consistency) is a function of OCI. Higher OCI could lead to more rigid, deterministic choices (higher beta) or, conversely, more erratic behavior (lower beta) due to anxiety or doubt.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Choice Randomness (Temperature).
    
    This model proposes that the consistency of choices (inverse temperature beta) 
    varies as a function of OCI. High OCI might lead to "hyper-exploitation" 
    (very high beta) or erratic "doubt" (low beta).
    
    Parameters:
    lr: [0, 1] Learning rate.
    beta_base: [0, 10] Base inverse temperature.
    w: [0, 1] Mixing weight (MB vs MF).
    oci_beta_mod: [-5, 5] Modulation of beta by OCI.
    """
    lr, beta_base, w, oci_beta_mod = model_parameters
    n_trials = len(action_1)
    oci = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    # Calculate effective beta
    beta = beta_base + (oci * oci_beta_mod)
    # Ensure beta stays within reasonable positive bounds [0, 20] 
    # (upper bound slightly relaxed to allow for high rigidity)
    beta = np.clip(beta, 0.0, 20.0) 

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])

        # policy for the second choice
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        # Update Stage 1 MF
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += lr * delta_stage1
        
        # Update Stage 2 MF
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += lr * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```