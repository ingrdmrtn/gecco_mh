Here are three new cognitive models that incorporate the OCI score into the decision-making process in the two-step task. These models explore different mechanisms by which obsessive-compulsive traits might influence learning and choice, specifically focusing on the balance between model-based/model-free control and learning rates.

### Model 1: OCI-Modulated Model-Based vs. Model-Free Weighting
This model tests the hypothesis that higher OCI scores relate to a deficit in goal-directed (model-based) control, leading to a heavier reliance on habit (model-free) systems. The weighting parameter `w` determines the mix between these two systems, and this weight is linearly modulated by the OCI score.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based / Model-Free RL with OCI-modulated Weighting.
    
    This model assumes behavior is a mix of Model-Based (MB) and Model-Free (MF) systems.
    The mixing weight 'w' is modulated by the OCI score.
    Higher OCI might shift the balance towards MF (habitual) or MB (goal-directed) control.
    
    Parameters:
    - learning_rate: Rate at which Q-values are updated.
    - beta: Inverse temperature for softmax choice.
    - w_intercept: Base weighting for Model-Based control (when OCI is 0).
    - w_slope: How the weighting changes with OCI score.
    
    Bounds:
    learning_rate: [0,1]
    beta: [0,10]
    w_intercept: [0,1]
    w_slope: [-1,1]
    """
    learning_rate, beta, w_intercept, w_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective weight w, clamped between 0 (pure MF) and 1 (pure MB)
    w = w_intercept + (w_slope * oci_score)
    w = np.clip(w, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based Value: V(s') = max(Q_MF(s', a'))
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Choice ---
        curr_state = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[curr_state, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        r = reward[trial]
        
        # MF Update Stage 1 (TD(1) logic usually, but here simple TD(0) to MF value)
        # Note: Standard Daw et al. uses TD(1) eligibility traces, but simple TD(0) is often used in simplified variants.
        # We update MF Q1 based on the value of the state reached.
        delta_stage1 = q_stage2_mf[curr_state, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # MF Update Stage 2
        delta_stage2 = r - q_stage2_mf[curr_state, a2]
        q_stage2_mf[curr_state, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Punishment Sensitivity
This model hypothesizes that OCI is related to altered sensitivity to negative outcomes (punishment). Instead of a single learning rate, this model uses separate learning rates for positive prediction errors (better than expected) and negative prediction errors (worse than expected). The learning rate for negative errors is modulated by the OCI score.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free RL with OCI-modulated Punishment Learning Rate.
    
    This model separates learning from positive vs negative prediction errors (Dual Alpha).
    The learning rate for negative prediction errors (alpha_neg) is modulated by OCI.
    
    Parameters:
    - alpha_pos: Learning rate for positive prediction errors.
    - alpha_neg_base: Base learning rate for negative prediction errors.
    - alpha_neg_oci: Effect of OCI on negative learning rate.
    - beta: Inverse temperature.
    
    Bounds:
    alpha_pos: [0,1]
    alpha_neg_base: [0,1]
    alpha_neg_oci: [-1,1]
    beta: [0,10]
    """
    alpha_pos, alpha_neg_base, alpha_neg_oci, beta = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective negative learning rate
    alpha_neg = alpha_neg_base + (alpha_neg_oci * oci_score)
    alpha_neg = np.clip(alpha_neg, 0.0, 1.0)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice (Pure Model-Free for simplicity to isolate learning rate effect) ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Choice ---
        curr_state = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[curr_state, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        r = reward[trial]
        
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[curr_state, a2]
        lr_2 = alpha_pos if delta_stage2 > 0 else alpha_neg
        q_stage2_mf[curr_state, a2] += lr_2 * delta_stage2
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[curr_state, a2] - q_stage1_mf[a1]
        lr_1 = alpha_pos if delta_stage1 > 0 else alpha_neg
        q_stage1_mf[a1] += lr_1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Randomness (Temperature)
This model posits that OCI affects the exploration-exploitation trade-off. Specifically, it tests if higher OCI scores lead to more deterministic (lower temperature, higher beta) or more chaotic (higher temperature, lower beta) responding. The inverse temperature parameter `beta` is modulated by the OCI score.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Based RL with OCI-modulated Inverse Temperature (Beta).
    
    This model assumes a pure Model-Based strategy where the randomness of choice (beta)
    depends on the OCI score. High OCI might lead to more rigid (high beta) or 
    more uncertain (low beta) choices.
    
    Parameters:
    - learning_rate: Rate at which Stage 2 values are updated.
    - beta_base: Base inverse temperature.
    - beta_oci_slope: How beta changes with OCI score.
    - w_mb: Weighting for Model-Based control (fixed parameter to allow focus on beta).
    
    Bounds:
    learning_rate: [0,1]
    beta_base: [0,10]
    beta_oci_slope: [-5,5]
    w_mb: [0,1]
    """
    learning_rate, beta_base, beta_oci_slope, w_mb = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective beta
    beta_eff = beta_base + (beta_oci_slope * oci_score)
    # Ensure beta stays positive and within reasonable bounds
    beta_eff = np.clip(beta_eff, 0.0, 20.0) 

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # We track MF values for stage 1 just to have a baseline, 
    # but the primary driver will be the MB calculation mixed by w_mb
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Mixing MB and MF (fixed weight)
        q_net = (w_mb * q_stage1_mb) + ((1 - w_mb) * q_stage1_mf)
        
        exp_q1 = np.exp(beta_eff * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Choice ---
        curr_state = int(state[trial])
        exp_q2 = np.exp(beta_eff * q_stage2_mf[curr_state, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        r = reward[trial]
        
        # Update Stage 2
        delta_stage2 = r - q_stage2_mf[curr_state, a2]
        q_stage2_mf[curr_state, a2] += learning_rate * delta_stage2
        
        # Update Stage 1 MF (standard TD)
        delta_stage1 = q_stage2_mf[curr_state, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```