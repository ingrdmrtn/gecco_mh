Here are 3 new cognitive models that explore different mechanisms by which OCI scores might influence learning and decision-making in the two-step task.

### Model 1: OCI-Modulated Inverse Temperature (Exploration/Exploitation Balance)
This model hypothesizes that OCI symptoms relate to the rigidity of choice behavior. Specifically, it tests whether higher OCI scores lead to higher `beta` (inverse temperature), resulting in more deterministic (exploitative) choices and less exploration, regardless of the learning strategy. It assumes a standard hybrid reinforcement learning structure but makes the softmax temperature a function of OCI.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL with OCI-modulated choice consistency (Inverse Temperature).
    
    Hypothesis: Higher OCI scores lead to more rigid, deterministic choice behavior 
    (higher beta), reducing exploration.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate for Q-value updates.
    w: [0, 1] - Weighting parameter between Model-Based (1) and Model-Free (0).
    beta_base: [0, 10] - Baseline inverse temperature.
    beta_oci_slope: [0, 5] - How much OCI increases the inverse temperature.
    """
    learning_rate, w, beta_base, beta_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    # Calculate effective beta based on OCI
    # Higher OCI -> Higher beta -> More deterministic/rigid choices
    beta = beta_base + (beta_oci_slope * oci_score)
    # Ensure beta stays within reasonable bounds for numerical stability
    beta = np.clip(beta, 0.0, 20.0) 

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        qs_stage2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_stage2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        # --- Updates ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        
        # TD Error Stage 1 (SARSA-style update using Stage 2 Q-value)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # TD Error Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Eligibility Trace (Lambda)
This model investigates if OCI scores affect how credit is assigned to past actions. It implements a temporal difference learning algorithm with an eligibility trace parameter `lambda`. The hypothesis is that OCI might correlate with a stronger or weaker link between the second-stage outcome and the first-stage choice (Model-Free eligibility), essentially modulating how much the Stage 2 reward "drifts back" to update Stage 1 values directly, bypassing the model-based structure.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free RL with OCI-modulated Eligibility Trace (TD-Lambda).
    
    Hypothesis: OCI modulates the eligibility trace (lambda), affecting how strongly 
    outcomes in the second stage directly reinforce the first-stage choice. 
    (Note: This model is purely Model-Free to isolate the lambda effect).
    
    Parameters:
    learning_rate: [0, 1] - Learning rate.
    beta: [0, 10] - Inverse temperature.
    lambda_base: [0, 1] - Baseline eligibility trace decay.
    lambda_oci_mod: [-1, 1] - Modulation of lambda by OCI.
    """
    learning_rate, beta, lambda_base, lambda_oci_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    # Calculate effective lambda
    # We use a sigmoid-like transform or simple clipping to keep it in [0,1]
    # Here we use simple linear + clip
    lambda_param = lambda_base + (lambda_oci_mod * oci_score)
    lambda_param = np.clip(lambda_param, 0.0, 1.0)
    
    # No transition matrix needed for pure MF, but keeping structure similar
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        qs_stage2 = q_stage2[state_idx]
        exp_q2 = np.exp(beta * qs_stage2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        # --- Updates ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        
        # Stage 1 prediction error (using Stage 2 Q-value as proxy for value of state)
        delta_1 = q_stage2[state_idx, a2] - q_stage1[a1]
        
        # Stage 2 prediction error (reward outcome)
        delta_2 = reward[trial] - q_stage2[state_idx, a2]
        
        # Update Stage 2
        q_stage2[state_idx, a2] += learning_rate * delta_2
        
        # Update Stage 1:
        # 1. Direct update from transition (TD(0) part)
        # 2. Eligibility trace update from Stage 2 reward (TD(1) part scaled by lambda)
        q_stage1[a1] += learning_rate * (delta_1 + lambda_param * delta_2)
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Learning Rate Asymmetry (Positive vs Negative)
This model posits that OCI affects sensitivity to positive versus negative prediction errors differently. It introduces separate learning rates for positive (`alpha_pos`) and negative (`alpha_neg`) prediction errors. The `oci` score is used to bias the ratio between them. For example, high OCI might be associated with "hyper-learning" from negative outcomes (avoidance) or failures, represented by a higher learning rate for negative prediction errors.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL with OCI-modulated Asymmetric Learning Rates.
    
    Hypothesis: OCI scores alter the balance between learning from positive prediction errors 
    (better than expected) vs negative prediction errors (worse than expected).
    
    Parameters:
    alpha_base: [0, 1] - Base learning rate.
    alpha_asym_oci: [-1, 1] - OCI modulation of asymmetry. 
                              Positive -> OCI increases alpha_neg relative to alpha_pos.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - Model-based weight.
    """
    alpha_base, alpha_asym_oci, beta, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    # Define asymmetric learning rates based on OCI
    # If alpha_asym_oci is positive, high OCI boosts learning from negative errors (anxiety/avoidance)
    # If alpha_asym_oci is negative, high OCI boosts learning from positive errors
    
    # We define a 'bias' factor centered around 0.5
    bias = 0.5 + (alpha_asym_oci * oci_score * 0.5) 
    bias = np.clip(bias, 0.01, 0.99)
    
    # Split base learning rate:
    # We want the average to be roughly alpha_base, but split by bias
    # This is a heuristic to keep parameters identifiable
    alpha_pos = np.clip(alpha_base * (1 - bias) * 2, 0, 1)
    alpha_neg = np.clip(alpha_base * (bias) * 2, 0, 1)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        qs_stage2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_stage2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        # --- Updates ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        
        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        eff_alpha2 = alpha_pos if delta_stage2 > 0 else alpha_neg
        q_stage2_mf[state_idx, a2] += eff_alpha2 * delta_stage2
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        eff_alpha1 = alpha_pos if delta_stage1 > 0 else alpha_neg
        q_stage1_mf[a1] += eff_alpha1 * delta_stage1
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```