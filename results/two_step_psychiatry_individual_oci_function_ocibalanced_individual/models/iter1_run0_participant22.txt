Here are three new cognitive models. These models focus on different hypothesized mechanisms for how obsessive-compulsive symptoms (OCI) might influence decision-making, specifically targeting the balance between model-based and model-free control, learning rates, and uncertainty.

### Model 1: OCI-Modulated Model-Based/Model-Free Balance (Hybrid)
This model hypothesizes that OCI scores influence the balance between goal-directed (model-based) and habitual (model-free) control. High OCI is often associated with rigid, habitual behaviors. Therefore, this model proposes that the weighting parameter `w` (which governs the trade-off between MB and MF values) is a function of the OCI score. Specifically, higher OCI leads to a lower `w`, favoring Model-Free control.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based / Model-Free RL with OCI-modulated Weighting.

    Hypothesis: OCI score modulates the balance between Model-Based (planning) and 
    Model-Free (habitual) systems. Higher OCI reduces the model-based weight (w), 
    leading to more habitual behavior.

    Parameters:
    - learning_rate: [0, 1] Learning rate for Q-value updates.
    - beta: [0, 10] Inverse temperature for softmax.
    - w_max: [0, 1] Maximum possible weight for Model-Based control (at OCI=0).
    - oci_decay: [0, 5] Rate at which MB weight decays as OCI increases.
    """
    learning_rate, beta, w_max, oci_decay = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    # Calculate the effective weight 'w' based on OCI.
    # Higher OCI -> smaller w -> more Model-Free.
    # Using an exponential decay function to keep w bounded [0, w_max].
    w = w_max * np.exp(-current_oci * oci_decay)

    # Transition matrix (fixed for this task structure)
    # 0.7 probability of common transition, 0.3 rare
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)      # Model-free values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for stage 2 (states x actions)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based value calculation: V(s') * Transition_Prob
        # We use max(Q_stage2) as a proxy for V(s')
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Q-value: Weighted sum of MB and MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # --- Stage 2 Policy ---
        # Standard softmax on Stage 2 Q-values
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # --- Learning / Updates ---
        r = reward[trial]
        a2 = int(action_2[trial])
        
        # Update Stage 2 MF values (TD(0))
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Update Stage 1 MF values (TD(1) / SARSA-like backup)
        # Using the value of the state actually reached
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Punishment Sensitivity
This model explores the idea that OCI is related to enhanced sensitivity to negative outcomes (or lack of reward). Instead of a single learning rate, this model splits learning into positive (`alpha_pos`) and negative (`alpha_neg`) components. The negative learning rate is modulated by the OCI score, suggesting that individuals with higher OCI symptoms might over-update their value estimates following a loss (0 coins), reflecting "hyper-learning" from failure.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free RL with OCI-modulated Punishment Sensitivity.

    Hypothesis: High OCI individuals are hypersensitive to negative feedback (0 reward).
    This model scales the learning rate for negative prediction errors (alpha_neg)
    based on the OCI score, while alpha_pos remains baseline.

    Parameters:
    - alpha_pos: [0, 1] Learning rate for positive prediction errors.
    - alpha_neg_base: [0, 1] Base learning rate for negative prediction errors.
    - beta: [0, 10] Inverse temperature.
    - oci_sens: [0, 5] Multiplier for OCI impact on negative learning rate.
    """
    alpha_pos, alpha_neg_base, beta, oci_sens = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    # Calculate effective negative learning rate
    # Bounded at 1.0 to prevent instability
    eff_alpha_neg = min(1.0, alpha_neg_base + (current_oci * oci_sens))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Pure Model-Free approach as per previous "best model" hint structure
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        # Stage 1 Choice
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # Stage 2 Choice
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        r = reward[trial]
        a2 = int(action_2[trial])

        # Prediction Error Stage 2
        delta_stage2 = r - q_stage2[state_idx, a2]
        
        # Select learning rate based on sign of PE
        lr_s2 = alpha_pos if delta_stage2 >= 0 else eff_alpha_neg
        q_stage2[state_idx, a2] += lr_s2 * delta_stage2

        # Prediction Error Stage 1
        # Update based on the value of the state reached
        delta_stage1 = q_stage2[state_idx, a2] - q_stage1[a1]
        
        # Select learning rate based on sign of PE
        lr_s1 = alpha_pos if delta_stage1 >= 0 else eff_alpha_neg
        q_stage1[a1] += lr_s1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Eligibility Traces (Lambda)
This model investigates how OCI affects the credit assignment process between the two stages. The parameter lambda ($\lambda$) in reinforcement learning controls how much the Stage 1 choice is updated based on the Stage 2 outcome (eligibility trace). A higher lambda implies that the second-stage outcome strongly reinforces the first-stage choice. This model hypothesizes that OCI might correlate with "over-attribution" of outcomes to distal causes (high lambda) or a failure to link them (low lambda). We model lambda as a function of OCI.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    TD(lambda) RL with OCI-modulated Eligibility Trace.

    Hypothesis: OCI affects the credit assignment between stages. 
    This model modulates the eligibility trace parameter (lambda) by the OCI score.
    This controls how strongly the reward received at the end of the trial updates 
    the value of the first-stage choice.

    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - lambda_base: [0, 1] Base eligibility trace parameter.
    - oci_mod: [-1, 1] Modulation of lambda by OCI. 
                 (Can increase or decrease lambda depending on sign).
    """
    learning_rate, beta, lambda_base, oci_mod = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    # Calculate effective lambda
    # We use a sigmoid-like transformation or clipping to keep lambda in [0, 1]
    # Here we use simple clipping for interpretability of oci_mod
    raw_lambda = lambda_base + (current_oci * oci_mod)
    eff_lambda = max(0.0, min(1.0, raw_lambda))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        # Stage 1 Choice
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # Stage 2 Choice
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        r = reward[trial]
        a2 = int(action_2[trial])

        # Stage 2 Update (TD error)
        delta_stage2 = r - q_stage2[state_idx, a2]
        q_stage2[state_idx, a2] += learning_rate * delta_stage2

        # Stage 1 Update
        # In TD(lambda) for this simple chain, the update for Stage 1 is:
        # alpha * ( (Q_s2 - Q_s1) + lambda * (r - Q_s2) )
        # This simplifies to combining the immediate PE at stage 1 transition
        # and the PE at stage 2 outcome weighted by lambda.
        
        # PE from transition (Stage 1 -> Stage 2)
        # Note: In this task, reward at transition is 0.
        delta_stage1_immediate = q_stage2[state_idx, a2] - q_stage1[a1]
        
        # Total update combines immediate transition error and weighted stage 2 error
        update_s1 = delta_stage1_immediate + (eff_lambda * delta_stage2)
        
        q_stage1[a1] += learning_rate * update_s1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```