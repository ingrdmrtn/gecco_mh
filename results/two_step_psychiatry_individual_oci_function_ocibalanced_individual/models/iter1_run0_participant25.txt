Here are three new cognitive models that explore different mechanisms by which Obsessive-Compulsive symptoms (OCI) might influence decision-making in this task. They move beyond simple asymmetric learning rates and explore mixing model-based/model-free control, perseveration (stickiness), and reward sensitivity.

### Model 1: OCI-Modulated Model-Based vs. Model-Free Weighting
This model tests the hypothesis that higher OCI scores relate to a reliance on habitual (model-free) control over goal-directed (model-based) planning. It uses a hybrid reinforcement learning framework where the mixing weight `w` is modulated by the participant's OCI score.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free RL with OCI-modulated mixing weight.
    
    This model assumes behavior is a mix of model-based (planning using transition structure)
    and model-free (habitual caching of values) strategies. The mixing parameter 'w'
    determines the balance. Here, 'w' is a function of the OCI score, testing if 
    compulsivity shifts the balance towards model-free habits (or model-based planning).
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for value updates.
    - beta: [0, 10] Inverse temperature (softmax randomness).
    - w_intercept: [0, 1] Baseline mixing weight (0=MF, 1=MB).
    - w_slope: [-1, 1] How OCI changes the mixing weight.
    """
    learning_rate, beta, w_intercept, w_slope = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate effective w based on OCI (logistic function to keep it in [0,1])
    # We use a simple linear clamp here for interpretability within bounds
    w_effective = w_intercept + (w_slope * current_oci)
    w_effective = np.clip(w_effective, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based Value: V_MB(s1) = T * max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value: Q_net = w * Q_MB + (1-w) * Q_MF
        q_stage1_net = w_effective * q_stage1_mb + (1 - w_effective) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_stage1_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        state_idx = int(state[trial])
        
        # --- Stage 2 Choice ---
        # Standard Model-Free choice at stage 2
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        r = reward[trial]
        
        # --- Updates ---
        # Stage 2 Update (TD)
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 Update (TD-0 / SARSA-0 logic for the MF component)
        # Note: In hybrid models, the MF component often updates based on the stage 2 value
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Choice Stickiness (Perseveration)
This model posits that OCI relates to behavioral rigidity or "stickiness"â€”a tendency to repeat the previous choice regardless of reward history. This is distinct from learning; it is a direct autocorrelation in action selection.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Reinforcement Learning with OCI-modulated Choice Stickiness.
    
    This model adds a 'stickiness' parameter to the softmax decision rule.
    Stickiness captures the tendency to repeat the previous action regardless of
    value. Here, the magnitude of this stickiness is modulated by OCI, testing
    if obsessive-compulsive traits lead to repetitive, rigid behaviors.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - stick_base: [-5, 5] Baseline tendency to repeat choice (positive) or switch (negative).
    - stick_oci_mod: [-5, 5] How OCI modifies stickiness.
    """
    learning_rate, beta, stick_base, stick_oci_mod = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate effective stickiness
    stick_effective = stick_base + (stick_oci_mod * current_oci)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_a1 = -1 # No previous action for the first trial
    
    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        qs = q_stage1_mf.copy()
        
        # Add stickiness bonus to the Q-value of the previously chosen action
        if last_a1 != -1:
            qs[last_a1] += stick_effective
            
        exp_q1 = np.exp(beta * qs)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        last_a1 = a1 # Update for next trial
        
        state_idx = int(state[trial])
        
        # --- Stage 2 Choice ---
        # No stickiness modeled for stage 2 (aliens) to keep parameters low
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        r = reward[trial]
        
        # --- Updates ---
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Reward Sensitivity
This model investigates if OCI affects the subjective valuation of rewards. Instead of changing how fast one learns (learning rate) or how they plan (MB vs MF), this model scales the reward signal itself based on OCI. This tests if high OCI is associated with blunted or heightened sensitivity to positive outcomes.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Reinforcement Learning with OCI-modulated Reward Sensitivity.
    
    This model assumes that the effective reward signal received by the agent
    is scaled by their OCI score. R_effective = R_observed * (rho_base + rho_oci * OCI).
    This tests hypotheses regarding anhedonia or heightened reward salience 
    associated with compulsivity.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - rho_base: [0, 5] Baseline reward sensitivity scalar.
    - rho_oci_mod: [-2, 2] Modulation of reward sensitivity by OCI.
    """
    learning_rate, beta, rho_base, rho_oci_mod = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate effective reward sensitivity
    rho_effective = rho_base + (rho_oci_mod * current_oci)
    # Ensure reward sensitivity doesn't flip sign (unless we want to model reward reversal, which is unlikely here)
    rho_effective = np.maximum(rho_effective, 0.0) 
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        state_idx = int(state[trial])
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        r_observed = reward[trial]
        # Modulate the reward signal
        r_effective = r_observed * rho_effective
        
        # --- Updates ---
        delta_stage2 = r_effective - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```