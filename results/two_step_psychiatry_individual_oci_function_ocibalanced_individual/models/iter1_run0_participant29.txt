Here are three new cognitive models that explore different mechanisms by which obsessive-compulsive traits (OCI) might influence reinforcement learning in a two-step task.

### Model 1: Hybrid Model-Based/Model-Free with OCI-modulated Mixing Weight
This model tests the hypothesis that higher OCI scores lead to a shift in the balance between goal-directed (Model-Based) and habitual (Model-Free) control. Specifically, it posits that OCI modulates the mixing weight `w`, determining the relative contribution of the Model-Based system.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: Hybrid Model-Based/Model-Free RL with OCI-modulated mixing weight.
    
    Hypothesis: OCI score influences the trade-off between Model-Based (planning) and 
    Model-Free (habitual) systems. Higher OCI might reflect reliance on habits (lower w) 
    or rigid rules (higher w), modeled here linearly.
    
    Parameters:
    learning_rate: [0, 1] - Update rate for Q-values.
    beta: [0, 10] - Inverse temperature for softmax.
    w_base: [0, 1] - Baseline mixing weight (0=Pure MF, 1=Pure MB).
    w_oci_factor: [-1, 1] - How OCI shifts the mixing weight.
    """
    learning_rate, beta, w_base, w_oci_factor = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate effective mixing weight w, clamped between 0 and 1
    w = w_base + (w_oci_factor * current_oci)
    w = np.clip(w, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based Value: V_MB(s1) = T * max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value: Q_net = w * Q_MB + (1-w) * Q_MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # --- Stage 2 Choice ---
        state_idx = int(state[trial])
        
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Learning ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Stage 2 update (TD)
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 update (TD using stage 2 value)
        # Note: Standard Daw et al. 2011 uses Q(s2, a2) for the update target
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Asymmetric Learning Rates Modulated by OCI
This model investigates if OCI affects how participants learn from positive versus negative prediction errors. It proposes that individuals with different OCI levels might be differentially sensitive to "good news" (positive RPE) versus "bad news" (negative RPE), potentially reflecting anxiety-driven avoidance or perfectionism.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: Model-Free RL with OCI-modulated asymmetric learning rates.
    
    Hypothesis: OCI affects the balance between learning from positive and negative outcomes.
    Instead of a single alpha, we have alpha_pos and alpha_neg.
    OCI modulates the ratio or bias between them.
    
    Parameters:
    alpha_base: [0, 1] - Base learning rate.
    beta: [0, 10] - Inverse temperature.
    oci_pos_bias: [-1, 1] - Effect of OCI on learning from positive RPEs.
    oci_neg_bias: [-1, 1] - Effect of OCI on learning from negative RPEs.
    """
    alpha_base, beta, oci_pos_bias, oci_neg_bias = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate effective learning rates
    # We clip them to be valid probabilities [0, 1]
    alpha_pos = np.clip(alpha_base + (oci_pos_bias * current_oci), 0.0, 1.0)
    alpha_neg = np.clip(alpha_base + (oci_neg_bias * current_oci), 0.0, 1.0)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        
        # --- Stage 1 Choice (Pure Model-Free) ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Learning ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Stage 2 Prediction Error
        delta2 = r - q_stage2[state_idx, a2]
        
        # Apply asymmetric learning rate
        if delta2 >= 0:
            q_stage2[state_idx, a2] += alpha_pos * delta2
        else:
            q_stage2[state_idx, a2] += alpha_neg * delta2
            
        # Stage 1 Prediction Error (SARSA-style update from stage 2 Q-value)
        delta1 = q_stage2[state_idx, a2] - q_stage1[a1]
        
        if delta1 >= 0:
            q_stage1[a1] += alpha_pos * delta1
        else:
            q_stage1[a1] += alpha_neg * delta1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Inverse Temperature (Exploration/Exploitation)
This model tests whether OCI relates to the randomness of choice (exploration vs. exploitation). High OCI could be associated with more rigid, deterministic responding (high beta) due to compulsivity, or alternatively, more noise/uncertainty (low beta).

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: Standard Hybrid RL where OCI modulates the Inverse Temperature (Beta).
    
    Hypothesis: OCI traits influence the exploration-exploitation trade-off.
    High OCI might lead to more deterministic choices (higher beta/rigidity) 
    or more anxious exploration (lower beta).
    
    Parameters:
    learning_rate: [0, 1]
    w: [0, 1] - Mixing weight (fixed for this model to isolate beta effects).
    beta_base: [0, 10] - Baseline inverse temperature.
    beta_oci_slope: [-5, 5] - How OCI changes beta.
    """
    learning_rate, w, beta_base, beta_oci_slope = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate effective beta, ensuring it stays non-negative
    # We use a softplus-like or simple max to keep beta > 0
    beta_eff = beta_base + (beta_oci_slope * current_oci)
    beta_eff = np.maximum(beta_eff, 0.0) 

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Choice ---
        # Model-Based Calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_eff * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta_eff * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Learning ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```