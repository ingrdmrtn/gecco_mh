Here are three new cognitive models that explore different mechanisms by which high OCI scores might influence decision-making in the two-step task. These models focus on mixing model-based/model-free strategies, modulating learning rates based on valence (wins vs. losses), and altering the balance between exploitation and exploration.

### Model 1: Hybrid Model-Based/Model-Free with OCI-modulated Mixing
This model hypothesizes that OCI levels influence the balance between goal-directed (Model-Based) and habitual (Model-Free) control. High OCI might be associated with a stronger reliance on habitual (Model-Free) responding, potentially due to over-training or rigidity.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: Hybrid MB/MF with OCI-modulated Mixing Weight.

    Hypothesis: The balance between Model-Based (planning) and Model-Free (habit) systems
    is modulated by OCI. High OCI might lead to increased reliance on Model-Free habits 
    (lower w) or Model-Based planning (higher w), depending on the fit.
    
    Parameters:
    learning_rate: [0, 1] Learning rate for value updates.
    beta: [0, 10] Inverse temperature for softmax.
    w_base: [0, 1] Baseline mixing weight (0=Pure MF, 1=Pure MB).
    w_oci_factor: [-1, 1] How much OCI shifts the mixing weight.
    """
    learning_rate, beta, w_base, w_oci_factor = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate mixing weight w based on OCI
    # We clamp w between 0 and 1 to ensure it remains a valid probability weight
    w_raw = w_base + (w_oci_factor * oci_score)
    w = np.clip(w_raw, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Q-values
    q_stage1_mf = np.zeros(2)      # MF values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # MF values for stage 2 (aliens)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        # Model-Based Value: V(s') = max(Q_stage2(s', a'))
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value: w * MB + (1-w) * MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        # Update Stage 2 MF values (standard TD)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Update Stage 1 MF values (TD(1) logic - direct reinforcement from outcome)
        # Note: In a full TD(lambda) model, this would use eligibility traces,
        # but here we use a simplified direct update common in these tasks.
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Asymmetric Learning Rates Modulated by OCI
This model investigates if OCI affects how participants learn from positive versus negative feedback. High OCI is often linked to anxiety and harm avoidance. This model tests if high OCI participants have a heightened learning rate for losses (or lack of reward) compared to wins.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: Asymmetric Learning Rates (Win/Loss) Modulated by OCI.
    
    Hypothesis: High OCI participants might be hyper-sensitive to negative outcomes (losses)
    or failure to obtain reward. This model splits learning rates into alpha_pos and alpha_neg,
    where alpha_neg is modulated by OCI.
    
    Parameters:
    alpha_pos: [0, 1] Learning rate for positive prediction errors (RPE > 0).
    alpha_neg_base: [0, 1] Baseline learning rate for negative prediction errors (RPE < 0).
    alpha_neg_oci: [-1, 1] Modulation of negative learning rate by OCI.
    beta: [0, 10] Inverse temperature.
    """
    alpha_pos, alpha_neg_base, alpha_neg_oci, beta = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate OCI-specific negative learning rate
    alpha_neg = np.clip(alpha_neg_base + (alpha_neg_oci * oci_score), 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy (Pure Model-Based for simplicity of interaction) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        # Stage 2 Update with asymmetric learning rates
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        if delta_stage2 >= 0:
            q_stage2_mf[s_idx, a2] += alpha_pos * delta_stage2
        else:
            q_stage2_mf[s_idx, a2] += alpha_neg * delta_stage2
            
        # No Stage 1 MF update in this specific model variant to isolate the MB learning effect
        # The Q_stage1_mb updates implicitly because q_stage2_mf changes.

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Inverse Temperature (Exploration/Exploitation)
This model posits that the core difference lies in the stochasticity of choice. High OCI might lead to more deterministic (exploitative) behavior, reducing exploration (randomness). This is modeled by scaling the inverse temperature parameter `beta` by the OCI score.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Inverse Temperature (Exploration/Exploitation).
    
    Hypothesis: High OCI scores relate to reduced exploration and higher determinism 
    in choice (higher beta). This model scales the inverse temperature parameter 
    based on the OCI score.
    
    Parameters:
    learning_rate: [0, 1] Learning rate.
    beta_base: [0, 10] Baseline inverse temperature.
    beta_oci_slope: [-5, 5] Effect of OCI on beta (positive = more rigid/exploitative).
    """
    learning_rate, beta_base, beta_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective beta
    # We ensure beta stays non-negative.
    beta_eff = max(0.0, beta_base + (beta_oci_slope * oci_score))

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage2_mf = np.zeros((2, 2))
    
    # Simple MF Q-values for stage 1 to allow for general learning
    q_stage1_mf = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy (Model-Based) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Using effective beta
        exp_q1 = np.exp(beta_eff * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta_eff * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Implicit MB update happens via q_stage2_mf changes

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```