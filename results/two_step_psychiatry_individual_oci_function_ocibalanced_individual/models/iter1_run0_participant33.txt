Here are three new cognitive models for the two-step task, incorporating the OCI score into different mechanisms of the reinforcement learning process.

### Model 1: OCI-Modulated Decision Rigidity (Beta Modulation)
This model hypothesizes that obsessive-compulsive symptoms relate to "decision rigidity" or a reduced tolerance for exploration. Here, the OCI score modulates the inverse temperature (`beta`) parameter. Higher OCI scores lead to a higher `beta`, resulting in more deterministic (greedy) choices and less stochastic exploration.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Decision Rigidity (Beta Modulation).
    
    This model assumes that higher OCI scores lead to more rigid, deterministic behavior
    (reduced exploration). The OCI score scales the inverse temperature (beta) upwards.
    
    Parameters:
    - learning_rate: [0, 1] Update rate for Q-values.
    - w: [0, 1] Mixing weight (0=Pure MF, 1=Pure MB).
    - lambda_eligibility: [0, 1] Eligibility trace for Stage 1 updates.
    - beta_base: [0, 10] Baseline inverse temperature.
    - oci_beta_slope: [0, 5] Sensitivity of beta to OCI score.
    
    Mechanism:
    effective_beta = beta_base * (1 + oci_beta_slope * oci)
    (Capped at 20 to prevent overflow, though bounds keep base low).
    """
    learning_rate, w, lambda_eligibility, beta_base, oci_beta_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    # Modulate Beta based on OCI
    beta = beta_base * (1.0 + (oci_beta_slope * oci_score))
    if beta > 20.0: beta = 20.0 # Safety cap

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based Value Calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net_stage1 = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Softmax Choice 1
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        # Softmax Choice 2 (Pure Model-Free)
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        r = reward[trial]
        
        # Prediction Errors
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        
        # Update Stage 2
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Update Stage 1 (with eligibility trace)
        q_stage1_mf[a1] += learning_rate * delta_stage1 + (learning_rate * lambda_eligibility * delta_stage2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Habitual Chaining (Lambda Modulation)
This model tests the hypothesis that OCI affects the "stickiness" or strength of the causal chain between the first choice and the final reward. A higher eligibility trace (`lambda`) implies that the outcome at the second stage strongly reinforces the first stage choice, treating the two steps as a single habitual "chunk." OCI increases this parameter.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Modulated Habitual Chaining (Lambda Modulation).
    
    This model assumes that OCI affects the eligibility trace (lambda), which determines
    how much the Stage 2 reward outcome updates the Stage 1 choice value.
    High OCI is hypothesized to increase lambda, strengthening the habitual link 
    between the first action and the final outcome (chunking).
    
    Parameters:
    - learning_rate: [0, 1] Update rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] MB/MF mixing weight.
    - lambda_base: [0, 1] Base eligibility trace.
    - oci_lambda_slope: [0, 1] Sensitivity of lambda to OCI.
    
    Mechanism:
    effective_lambda = lambda_base + (oci_lambda_slope * oci)
    (Capped at 1.0).
    """
    learning_rate, beta, w, lambda_base, oci_lambda_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    # Modulate Lambda based on OCI
    lambda_eff = lambda_base + (oci_lambda_slope * oci_score)
    if lambda_eff > 1.0: lambda_eff = 1.0
    if lambda_eff < 0.0: lambda_eff = 0.0

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        r = reward[trial]
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Use effective lambda here
        q_stage1_mf[a1] += learning_rate * delta_stage1 + (learning_rate * lambda_eff * delta_stage2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Global Plasticity (Learning Rate Modulation)
This model tests if OCI is associated with a generalized alteration in learning speed (plasticity). Unlike the previous "best model" which applied asymmetry to negative outcomes only, this model applies a global modulation to the base learning rate. It tests if high OCI participants are generally hyper-reactive to prediction errors (higher alpha) or more sluggish/stable (lower alpha), parameterized by a slope.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Global Plasticity (Learning Rate Modulation).
    
    This model assumes that OCI affects the global learning rate (plasticity).
    It allows the model to capture if high-OCI individuals update their value 
    estimates more aggressively (hyper-learning) or more slowly (rigidity) 
    in response to all prediction errors.
    
    Parameters:
    - alpha_base: [0, 1] Base learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] MB/MF mixing weight.
    - lambda_eligibility: [0, 1] Eligibility trace.
    - oci_alpha_slope: [0, 1] Sensitivity of alpha to OCI.
    
    Mechanism:
    effective_alpha = alpha_base + (oci_alpha_slope * oci)
    (Capped at 1.0).
    """
    alpha_base, beta, w, lambda_eligibility, oci_alpha_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    # Modulate Learning Rate based on OCI (Additive modulation)
    # Note: oci_alpha_slope can be interpreted as increasing plasticity
    alpha_eff = alpha_base + (oci_alpha_slope * oci_score)
    if alpha_eff > 1.0: alpha_eff = 1.0
    if alpha_eff < 0.0: alpha_eff = 0.0

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        r = reward[trial]
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        
        # Use effective alpha here
        q_stage2_mf[state_idx, a2] += alpha_eff * delta_stage2
        q_stage1_mf[a1] += alpha_eff * delta_stage1 + (alpha_eff * lambda_eligibility * delta_stage2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```