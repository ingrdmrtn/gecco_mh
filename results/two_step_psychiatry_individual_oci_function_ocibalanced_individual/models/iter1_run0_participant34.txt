Here are three new cognitive models exploring different mechanisms for how high OCI scores might influence decision-making in this two-step task.

### Model 1: Hybrid Model-Based/Model-Free with OCI-modulated Mixing Weight
This model hypothesizes that individuals with high OCI symptoms may rely more heavily on habit-based (Model-Free) responding rather than goal-directed (Model-Based) planning. High compulsivity is often associated with rigid, habitual behaviors. Here, the mixing weight `w` (which balances MB and MF control) is dampened by the OCI score.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based / Model-Free Reinforcement Learning.
    The balance between MB and MF systems (w) is modulated by OCI.
    High OCI reduces the Model-Based weight (w), leading to more Model-Free behavior.

    Parameters:
    learning_rate: [0, 1] - Update rate for Q-values.
    beta: [0, 10] - Inverse temperature for softmax.
    w_base: [0, 1] - Baseline weight for Model-Based control (0=Pure MF, 1=Pure MB).
    oci_damp: [0, 1] - Factor by which OCI reduces the MB weight.
    """
    learning_rate, beta, w_base, oci_damp = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    # Calculate effective w: higher OCI reduces w, pushing towards MF (habit)
    # Ensure w stays between 0 and 1
    w = w_base * (1.0 - (oci_damp * current_oci))
    w = np.clip(w, 0.0, 1.0)

    # Fixed transition matrix as per task description
    # A(0) -> X(0) (0.7), Y(1) (0.3)
    # U(1) -> X(0) (0.3), Y(1) (0.7)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Q-values
    q_mf_stage1 = np.zeros(2)      # Model-free values for stage 1
    q_mf_stage2 = np.zeros((2, 2)) # Model-free values for stage 2 (State x Action)

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        
        # 1. Model-Based Value Calculation (Bellman equation using transition matrix)
        # Max value of next stage states
        v_stage2 = np.max(q_mf_stage2, axis=1) 
        q_mb_stage1 = transition_matrix @ v_stage2

        # 2. Hybrid Value
        q_net_stage1 = (w * q_mb_stage1) + ((1 - w) * q_mf_stage1)

        # 3. Action Selection Stage 1
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]

        a1 = int(action_1[trial])
        s_idx = int(state[trial])

        # --- Stage 2 Decision ---
        
        # Action Selection Stage 2 (Purely Model-Free at this stage)
        exp_q2 = np.exp(beta * q_mf_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Learning ---

        # 1. Stage 2 Update (TD)
        # Q_MF(s, a2) = Q_MF(s, a2) + alpha * (r - Q_MF(s, a2))
        pe_2 = r - q_mf_stage2[s_idx, a2]
        q_mf_stage2[s_idx, a2] += learning_rate * pe_2

        # 2. Stage 1 Update (TD(1) / Sarsa-style for MF)
        # Q_MF(a1) = Q_MF(a1) + alpha * (Q_MF(s, a2) - Q_MF(a1)) + lambda*pe_2 (simplified to TD(0) here)
        # Standard TD(0) update using the value of the state actually reached
        pe_1 = q_mf_stage2[s_idx, a2] - q_mf_stage1[a1]
        q_mf_stage1[a1] += learning_rate * pe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Uncertainty-Based Exploration Penalty
This model posits that high OCI is associated with intolerance of uncertainty. Instead of a standard softmax exploration, this model includes an "uncertainty penalty" in the value function. If the variance/uncertainty of an action's outcome is high, a high-OCI participant will devalue it. We approximate uncertainty tracking by counting visits (inverse of visit count proxy for uncertainty).

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free Q-learning with Uncertainty Avoidance.
    OCI score modulates a penalty for actions that have been visited less frequently.
    High OCI leads to avoiding uncertain (less explored) options.

    Parameters:
    learning_rate: [0, 1] - Learning rate.
    beta: [0, 10] - Inverse temperature.
    uncertainty_aversion: [0, 5] - Base penalty for uncertainty.
    oci_mod: [0, 5] - How much OCI amplifies the aversion to uncertainty.
    decay: [0, 1] - Decay rate for uncertainty counts (simulating forgetting/dynamic uncertainty).
    """
    learning_rate, beta, uncertainty_aversion, oci_mod, decay = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    total_aversion = uncertainty_aversion + (oci_mod * current_oci)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    # Counts to track uncertainty (higher count = lower uncertainty)
    # We initialize with 1 to avoid division by zero
    counts_stage1 = np.ones(2) 
    
    # We don't apply uncertainty penalty to stage 2 for simplicity in this variant, 
    # focusing on the primary choice.

    for trial in range(n_trials):
        
        # --- Stage 1 Decision ---
        # Calculate uncertainty metric: Inverse of visitation counts
        uncertainty_s1 = 1.0 / np.sqrt(counts_stage1)
        
        # Subtract penalty from Q-values: Q_net = Q - (penalty * uncertainty)
        q_net_stage1 = q_stage1 - (total_aversion * uncertainty_s1)

        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]

        a1 = int(action_1[trial])
        s_idx = int(state[trial])

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Updates ---
        
        # Update counts with decay (uncertainty grows if not visited)
        counts_stage1 *= decay # Forgetting
        counts_stage1[a1] += 1 # Visit
        
        # TD Update Stage 2
        pe_2 = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += learning_rate * pe_2

        # TD Update Stage 1
        pe_1 = q_stage2[s_idx, a2] - q_stage1[a1]
        q_stage1[a1] += learning_rate * pe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Asymmetric Learning Rates (Perfectionism)
This model suggests that OCI relates to perfectionism or negative reinforcement sensitivity. High OCI participants might learn differently from failures (0 coins) than successes (1 coin). Specifically, they might over-update on negative outcomes (fear of failure) or positive outcomes (perfectionism). We allow OCI to shift the balance between `alpha_pos` and `alpha_neg`.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free Q-learning with Asymmetric Learning Rates modulated by OCI.
    OCI score influences the ratio between learning from positive rewards vs. lack of rewards (0).
    A 'bias' parameter determines if OCI amplifies positive or negative learning.

    Parameters:
    alpha_base: [0, 1] - Base learning rate.
    beta: [0, 10] - Inverse temperature.
    oci_bias: [0, 1] - Magnitude of OCI impact on asymmetry.
    direction: [0, 1] - Binary-ish switch (treated continuous). 
                        < 0.5: OCI increases alpha_neg (fear of failure). 
                        > 0.5: OCI increases alpha_pos (reward sensitivity).
    """
    alpha_base, beta, oci_bias, direction = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    # Calculate asymmetric learning rates
    # If direction < 0.5, OCI boosts negative learning (sensitivity to 0 reward)
    # If direction > 0.5, OCI boosts positive learning (sensitivity to 1 reward)
    
    if direction < 0.5:
        # Fear of failure mode
        alpha_pos = alpha_base
        alpha_neg = alpha_base + (oci_bias * current_oci * (1.0 - alpha_base))
    else:
        # Reward focus mode
        alpha_pos = alpha_base + (oci_bias * current_oci * (1.0 - alpha_base))
        alpha_neg = alpha_base

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]

        a1 = int(action_1[trial])
        s_idx = int(state[trial])

        # --- Stage 2 ---
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Learning Stage 2 ---
        pe_2 = r - q_stage2[s_idx, a2]
        
        current_alpha_2 = alpha_pos if pe_2 > 0 else alpha_neg
        q_stage2[s_idx, a2] += current_alpha_2 * pe_2

        # --- Learning Stage 1 ---
        # Note: Stage 1 update uses the value of the state reached
        target_val = q_stage2[s_idx, a2]
        pe_1 = target_val - q_stage1[a1]
        
        # We determine alpha based on the sign of the prediction error at stage 1
        current_alpha_1 = alpha_pos if pe_1 > 0 else alpha_neg
        q_stage1[a1] += current_alpha_1 * pe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```