Here are three new cognitive models that explore different mechanisms for how Obsessive-Compulsive (OCI) symptoms might influence decision-making in the two-step task. These models focus on habit formation, mixing model-based/model-free strategies, and learning rate modulation.

### Model 1: OCI-modulated Model-Based vs. Model-Free Trade-off
This model hypothesizes that high OCI scores are associated with a deficit in goal-directed (Model-Based) control, leading to a greater reliance on habitual (Model-Free) systems. The `w` parameter (weighting between systems) is modulated by the OCI score.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-modulated Model-Based/Model-Free Hybrid.
    
    This model posits that the balance between Model-Based (MB) and Model-Free (MF) 
    systems is influenced by OCI symptoms. Specifically, higher OCI scores reduce 
    the weight (w) of the Model-Based system, reflecting a shift towards habitual control.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate for value updates.
    beta: [0, 10] - Inverse temperature for softmax choice.
    w_max: [0, 1] - The maximum possible weight for the Model-Based system.
    """
    learning_rate, beta, w_max = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Hypothesis: Higher OCI reduces Model-Based control.
    # If OCI is 0, w = w_max. If OCI is 1, w approaches 0.
    w = w_max * (1.0 - oci_score)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based Value Calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value Calculation
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Learning ---
        # Stage 1 MF update (TD(0))
        # Note: In hybrid models, MF is usually updated by the best Stage 2 value or the state value
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1
        
        # Stage 2 MF update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-modulated Punishment Sensitivity
This model suggests that OCI is related to an asymmetry in learning from positive vs. negative outcomes. Specifically, individuals with high OCI might be hyper-sensitive to "punishment" (lack of reward), leading to different learning rates for rewarded vs. unrewarded trials.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-modulated Punishment Sensitivity (Asymmetric Learning Rates).
    
    This model is purely Model-Free but splits the learning rate into 
    positive (alpha_pos) and negative (alpha_neg) components.
    The OCI score amplifies the learning rate for unrewarded trials (punishment),
    reflecting a hyper-sensitivity to failure or error often seen in compulsion.
    
    Parameters:
    lr_base: [0, 1] - Base learning rate for rewarded trials.
    beta: [0, 10] - Inverse temperature.
    punishment_mult: [0, 5] - Multiplier for unrewarded learning rate based on OCI.
    """
    lr_base, beta, punishment_mult = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective learning rates
    alpha_pos = lr_base
    # Higher OCI increases sensitivity to lack of reward (0 reward)
    alpha_neg = lr_base * (1.0 + punishment_mult * oci_score)
    # Clip alpha_neg to ensure stability [0, 1]
    alpha_neg = min(alpha_neg, 1.0) 

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # Determine current learning rate based on outcome
        current_lr = alpha_pos if reward[trial] == 1 else alpha_neg
        
        # --- Learning ---
        # Update Stage 1
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += current_lr * delta_stage1
        
        # Update Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += current_lr * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-modulated Eligibility Trace (Lambda)
This model introduces an eligibility trace parameter ($\lambda$) to a Model-Free Reinforcement Learning agent. The hypothesis is that OCI symptoms affect how credit is assigned to previous actions. High OCI might lead to "over-attribution" or a longer memory trace for the first-stage choice when the second-stage outcome occurs.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-modulated Eligibility Trace (TD-Lambda).
    
    This model uses Temporal Difference learning with eligibility traces (TD(lambda)).
    The lambda parameter controls how much the Stage 2 reward updates the Stage 1 value directly.
    Here, lambda is modulated by OCI, hypothesizing that OCI affects the credit assignment 
    between distal outcomes and proximal choices.
    
    Parameters:
    learning_rate: [0, 1]
    beta: [0, 10]
    lambda_base: [0, 1] - Base eligibility trace decay parameter.
    """
    learning_rate, beta, lambda_base = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # OCI modulates the eligibility trace. 
    # Hypothesis: Higher OCI leads to stronger connection between Stage 2 reward and Stage 1 choice.
    # We constrain lambda_eff to be between 0 and 1.
    lambda_eff = lambda_base + (1.0 - lambda_base) * oci_score
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Learning with Eligibility Trace ---
        
        # 1. Prediction error at Stage 1 (TD error based on transition to Stage 2)
        # Value of state 2 is q_stage2_mf[state, action2]
        delta_1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        
        # Update Stage 1 based on transition
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_1
        
        # 2. Prediction error at Stage 2 (TD error based on reward)
        delta_2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        
        # Update Stage 2
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_2
        
        # Update Stage 1 again based on Stage 2 error (Eligibility trace)
        # The trace is 1 for the action just taken, decayed by lambda
        q_stage1_mf[int(action_1[trial])] += learning_rate * lambda_eff * delta_2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```