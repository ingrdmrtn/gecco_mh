Here are three new cognitive models that explore different mechanistic roles for OCI scores in reinforcement learning, specifically focusing on how obsessive-compulsive traits might influence learning rates, model-based vs. model-free control, and uncertainty-driven exploration.

### Model 1: OCI-Modulated Learning Rate Asymmetry
This model hypothesizes that individuals with high OCI scores might learn differently from positive versus negative prediction errors. Specifically, high compulsivity might be associated with a "better safe than sorry" approach, leading to higher learning rates for negative outcomes (avoidance learning) compared to positive outcomes.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Learning Rate Asymmetry.
    
    This model posits that OCI scores modulate the asymmetry between learning from
    positive prediction errors (learning_rate_pos) and negative prediction errors 
    (learning_rate_neg). High OCI increases the weight on negative errors relative 
    to positive ones, reflecting a potential bias towards avoidance or safety behaviors.
    
    Bounds:
    lr_base: [0, 1] - Baseline learning rate.
    lr_asymmetry: [0, 1] - Degree to which OCI skews learning towards negative errors.
    beta: [0, 10] - Inverse temperature.
    w_mb: [0, 1] - Weight of model-based control (fixed mixing).
    """
    lr_base, lr_asymmetry, beta, w_mb = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate specific learning rates based on OCI
    # If OCI is high, lr_neg becomes larger relative to lr_pos
    # We maintain lr_base as the center.
    # lr_pos decreases with OCI * asymmetry, lr_neg increases.
    
    # Clamp factor to ensure rates stay in [0,1]
    modulation = lr_asymmetry * oci_score
    lr_pos = lr_base * (1.0 - modulation)
    lr_neg = lr_base * (1.0 + modulation)
    
    # Safety clamping
    lr_pos = np.clip(lr_pos, 0.0, 1.0)
    lr_neg = np.clip(lr_neg, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2) + 0.5
    q_stage2_mf = np.zeros((2, 2)) + 0.5

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based Value Calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net_s1 = (1 - w_mb) * q_stage1_mf + w_mb * q_stage1_mb
        
        logits_1 = beta * q_net_s1
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        if a1 != -1:
            p_choice_1[trial] = probs_1[a1]
        else:
            p_choice_1[trial] = 1.0

        # Check for missing data
        if a1 == -1 or state[trial] == -1 or action_2[trial] == -1:
            continue

        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 2 Policy ---
        logits_2 = beta * q_stage2_mf[s_idx, :]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updating ---
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        eff_lr2 = lr_pos if delta_stage2 > 0 else lr_neg
        q_stage2_mf[s_idx, a2] += eff_lr2 * delta_stage2

        # Stage 1 Update (TD-0)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        eff_lr1 = lr_pos if delta_stage1 > 0 else lr_neg
        q_stage1_mf[a1] += eff_lr1 * delta_stage1

    eps = 1e-10
    p_choice_1 = np.maximum(p_choice_1, eps)
    p_choice_2 = np.maximum(p_choice_2, eps)
    log_loss = -(np.sum(np.log(p_choice_1)) + np.sum(np.log(p_choice_2)))
    return log_loss
```

### Model 2: OCI-Driven Habitual Control (MB/MF Trade-off)
This model tests the hypothesis that high OCI scores are associated with a deficit in goal-directed (model-based) control, leading to a reliance on habitual (model-free) systems. Instead of a fixed mixing weight `w`, the weight is a function of the OCI score.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Driven Habitual Control.
    
    This model posits that the balance between Model-Based (MB) and Model-Free (MF)
    control is determined by the OCI score. Specifically, higher OCI scores 
    reduce the weight of the model-based system (w_mb), leading to more habitual behavior.
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    w_mb_max: [0, 1] - The maximum model-based weight (for OCI=0).
    w_mb_decay: [0, 1] - How strongly OCI reduces the MB weight.
    """
    learning_rate, beta, w_mb_max, w_mb_decay = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate mixing weight based on OCI
    # Higher OCI -> Lower w_mb (more model-free)
    w_mb = w_mb_max * np.exp(-w_mb_decay * oci_score * 5.0) 
    # Multiplied OCI by 5 to make the decay parameter more sensitive in the 0-1 range
    
    w_mb = np.clip(w_mb, 0.0, 1.0)
    w_mf = 1.0 - w_mb

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2) + 0.5
    q_stage2_mf = np.zeros((2, 2)) + 0.5

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Weighted sum of MB and MF values
        q_net_s1 = w_mf * q_stage1_mf + w_mb * q_stage1_mb
        
        logits_1 = beta * q_net_s1
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        if a1 != -1:
            p_choice_1[trial] = probs_1[a1]
        else:
            p_choice_1[trial] = 1.0

        if a1 == -1 or state[trial] == -1 or action_2[trial] == -1:
            continue

        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 2 Policy ---
        logits_2 = beta * q_stage2_mf[s_idx, :]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updating ---
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

        # Stage 1 Update
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    p_choice_1 = np.maximum(p_choice_1, eps)
    p_choice_2 = np.maximum(p_choice_2, eps)
    log_loss = -(np.sum(np.log(p_choice_1)) + np.sum(np.log(p_choice_2)))
    return log_loss
```

### Model 3: OCI-Linked Inverse Temperature (Anxiety/Noise)
This model investigates if OCI scores relate to the stochasticity of choice. High OCI is often correlated with anxiety, which might manifest as "neural noise" or inconsistent decision-making despite learned values. Alternatively, it could manifest as rigidity (high beta). This model allows the data to determine if OCI increases or decreases the inverse temperature `beta`.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Linked Inverse Temperature.
    
    This model posits that OCI affects the exploration-exploitation trade-off parameter (beta).
    It models beta as a linear function of OCI. This allows the model to capture if 
    high OCI participants are more random (lower beta) or more deterministic/rigid 
    (higher beta) in their choices.
    
    Bounds:
    learning_rate: [0, 1]
    beta_intercept: [0, 10] - Beta value when OCI is 0.
    beta_slope: [-5, 5] - Change in beta per unit of OCI.
    w_mb: [0, 1] - Fixed model-based weight.
    """
    learning_rate, beta_intercept, beta_slope, w_mb = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective beta
    # We clip beta to be non-negative to prevent inverse preference
    beta_eff = beta_intercept + (beta_slope * oci_score)
    beta_eff = np.clip(beta_eff, 0.0, 20.0) 

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2) + 0.5
    q_stage2_mf = np.zeros((2, 2)) + 0.5

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_s1 = (1 - w_mb) * q_stage1_mf + w_mb * q_stage1_mb
        
        logits_1 = beta_eff * q_net_s1
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        if a1 != -1:
            p_choice_1[trial] = probs_1[a1]
        else:
            p_choice_1[trial] = 1.0

        if a1 == -1 or state[trial] == -1 or action_2[trial] == -1:
            continue

        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 2 Policy ---
        logits_2 = beta_eff * q_stage2_mf[s_idx, :]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updating ---
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    p_choice_1 = np.maximum(p_choice_1, eps)
    p_choice_2 = np.maximum(p_choice_2, eps)
    log_loss = -(np.sum(np.log(p_choice_1)) + np.sum(np.log(p_choice_2)))
    return log_loss
```