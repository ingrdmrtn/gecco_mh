Here are three new cognitive models that explore different mechanistic roles for OCI scores in reinforcement learning, focusing on model-based/model-free arbitration, reward sensitivity, and learning rates.

### Model 1: Hybrid Learner with OCI-modulated Model-Based Weight
This model tests the hypothesis that high OCI scores (compulsivity) might be associated with a rigid reliance on habit (Model-Free) over goal-directed planning (Model-Based), or vice versa. Here, the mixing weight `w` between MB and MF systems is a function of the OCI score.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free learner where OCI modulates the balance (w).
    High OCI might shift the balance towards Model-Free (habitual) or Model-Based control.
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    w_intercept: [0, 1] - Baseline weighting for Model-Based system
    w_slope: [-1, 1] - How OCI changes the weighting (positive = more MB, negative = more MF)
    """
    learning_rate, beta, w_intercept, w_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate mixing weight w, constrained to [0, 1]
    # w = 1 implies full Model-Based, w = 0 implies full Model-Free
    w = w_intercept + (w_slope * oci_score)
    w = np.clip(w, 0.0, 1.0)
    
    # Fixed transition matrix for the task
    # A -> X (0.7), A -> Y (0.3)
    # U -> Y (0.7), U -> X (0.3)
    # Mapping actions 0/1 to planets 0/1 requires knowing the specific setup, 
    # but the standard template assumes a fixed matrix structure.
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)     # MF values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # MF values for stage 2 (also used for MB planning)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based value calculation: V_MB(s1) = T * max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid value: Q_net = w * Q_MB + (1-w) * Q_MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Choice ---
        s_idx = int(state[trial])
        
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        r = reward[trial]
        
        # Update Stage 2 Q-values (TD(0))
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Update Stage 1 MF Q-values (TD(1) / SARSA-like update using stage 2 value)
        # Note: In standard two-step, MF update often uses the Q-value of the chosen 2nd stage option
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Loss Aversion Learner with OCI Modulation
This model hypothesizes that individuals with high OCI symptoms may be more sensitive to the absence of rewards (losses) or perceive outcomes differently. It implements separate learning rates for positive and negative prediction errors, where the learning rate for negative outcomes is modulated by the OCI score.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner with asymmetric learning rates modulated by OCI.
    OCI specifically affects learning from negative prediction errors (alpha_neg).
    
    Bounds:
    alpha_pos: [0, 1] - Learning rate for positive prediction errors
    alpha_neg_base: [0, 1] - Base learning rate for negative prediction errors
    beta: [0, 10]
    oci_sens: [0, 5] - Multiplier for OCI impact on negative learning rate
    """
    alpha_pos, alpha_neg_base, beta, oci_sens = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective negative learning rate
    # High OCI might make people 'overlearn' from failures/zeros
    alpha_neg = alpha_neg_base + (oci_sens * oci_score)
    alpha_neg = np.clip(alpha_neg, 0.0, 1.0)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    for trial in range(n_trials):
        # Stage 1 Policy
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        s_idx = int(state[trial])
        
        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        r = reward[trial]
        
        # Update Stage 2
        delta2 = r - q_stage2[s_idx, a2]
        lr2 = alpha_pos if delta2 > 0 else alpha_neg
        q_stage2[s_idx, a2] += lr2 * delta2
        
        # Update Stage 1
        delta1 = q_stage2[s_idx, a2] - q_stage1[a1]
        lr1 = alpha_pos if delta1 > 0 else alpha_neg
        q_stage1[a1] += lr1 * delta1
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Inverse Temperature (Noise) Modulation by OCI
This model proposes that OCI symptoms relate to the determinism of choice (exploration/exploitation balance). High OCI might lead to more rigid, deterministic behavior (higher beta) or, conversely, more erratic behavior due to anxiety (lower beta). This model allows the `beta` parameter to scale linearly with the OCI score.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner where OCI modulates the inverse temperature (beta).
    This tests if OCI is linked to decision noise/determinism.
    
    Bounds:
    learning_rate: [0, 1]
    beta_base: [0, 10] - Baseline inverse temperature
    beta_oci_slope: [-5, 5] - How OCI affects beta (can increase or decrease rigidity)
    eligibility_trace: [0, 1] - Parameter connecting stage 1 and stage 2 updates
    """
    learning_rate, beta_base, beta_oci_slope, eligibility_trace = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective beta
    beta_eff = beta_base + (beta_oci_slope * oci_score)
    # Ensure beta stays within reasonable positive bounds (or 0)
    beta_eff = np.maximum(beta_eff, 0.0)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    for trial in range(n_trials):
        # Stage 1 Choice
        exp_q1 = np.exp(beta_eff * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        s_idx = int(state[trial])
        
        # Stage 2 Choice
        exp_q2 = np.exp(beta_eff * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        r = reward[trial]
        
        # Update Stage 2
        delta2 = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += learning_rate * delta2
        
        # Update Stage 1 using eligibility trace logic (TD(lambda) simplified)
        # Often in 2-step: Q1(a1) += alpha * (Q2(s,a2) - Q1(a1)) + lambda * delta2
        # Here we use a simplified form where eligibility_trace scales the impact of stage 2 prediction error on stage 1
        delta1 = q_stage2[s_idx, a2] - q_stage1[a1]
        q_stage1[a1] += learning_rate * delta1 + (eligibility_trace * delta2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```