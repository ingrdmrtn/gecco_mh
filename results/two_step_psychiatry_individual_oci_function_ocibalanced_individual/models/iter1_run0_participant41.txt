Here are three new cognitive models that incorporate the OCI score to explain decision-making in the two-step task.

### Model 1: OCI-Modulated Model-Based vs. Model-Free Weighting
This model tests the hypothesis that high OCI scores relate to a rigid reliance on habit (model-free) over goal-directed planning (model-based). The mixing parameter `w` determines the balance between these two systems. Here, `w` is not a static parameter but is directly modulated by the OCI score, such that higher OCI forces a lower weight on the model-based system (pushing behavior towards model-free).

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Model-Based/Model-Free Hybrid.
    
    This model assumes a hybrid controller where the balance (w) between Model-Based (MB)
    and Model-Free (MF) control is determined by the OCI score. 
    High OCI is hypothesized to reduce Model-Based control (goal-directedness) 
    and increase reliance on Model-Free (habitual) control.
    
    w_raw: [0, 1] Baseline weighting for Model-Based control.
    oci_penalty: [0, 1] How much the OCI score reduces the MB weight.
    learning_rate: [0, 1] Learning rate for value updates.
    beta: [0, 10] Inverse temperature for softmax.
    """
    w_raw, oci_penalty, learning_rate, beta = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective weight w. 
    # High OCI reduces w, making the agent more Model-Free.
    # We clip to ensure w stays in [0, 1].
    w = np.clip(w_raw - (oci_penalty * oci_score), 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based Value: V(S2) weighted by transition probabilities
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])

        # --- Stage 2 Choice ---
        # Standard Model-Free choice at stage 2
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # --- Updates ---
        # TD(1) / SARSA-like update for Stage 1 MF
        # Note: In standard hybrid models, MB is fixed (transition matrix known), 
        # MF values update via TD.
        
        # Prediction error at stage 2 (commonly used to update stage 1 MF)
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1
        
        # Prediction error at outcome
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2
        
        # Eligibility trace for stage 1 (often simplified in these tasks as direct update from reward)
        # Here we use the standard TD chain. To make it strictly TD(1) for stage 1, 
        # one might add learning_rate * lambda * delta_stage2 to stage 1. 
        # We stick to the simple TD(0) chain often used in basic hybrid implementations.
        q_stage1_mf[int(action_1[trial])] += learning_rate * 0.0 # Placeholder if we wanted lambda

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Driven Asymmetric Learning Rates
This model suggests that high OCI scores might correspond to a hypersensitivity to negative outcomes (or lack of reward) versus positive outcomes. Instead of a single learning rate, the model uses separate rates for positive prediction errors (receiving gold) and negative prediction errors (receiving nothing), where the balance is shifted by the OCI score.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Driven Asymmetric Learning (Pos/Neg).
    
    This model posits that OCI affects how the agent learns from positive vs negative errors.
    Specifically, we define a base learning rate, and an 'asymmetry' parameter.
    The OCI score amplifies the learning rate for negative prediction errors (losses/omissions),
    reflecting a potential 'fear of failure' or hyper-correction typical in OC traits.
    
    lr_base: [0, 1] Base learning rate for positive outcomes.
    neg_bias: [0, 5] Multiplier for negative learning rate, scaled by OCI.
    beta: [0, 10] Inverse temperature.
    """
    lr_base, neg_bias, beta = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate learning rates
    # Alpha positive is the base rate.
    # Alpha negative is boosted by OCI.
    alpha_pos = lr_base
    alpha_neg = np.clip(lr_base * (1.0 + neg_bias * oci_score), 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Choice (Pure Model-Free for simplicity to isolate learning rate effect) ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        # --- Updates ---
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        # Use alpha_pos if delta > 0, else alpha_neg
        lr_1 = alpha_pos if delta_stage1 > 0 else alpha_neg
        q_stage1_mf[int(action_1[trial])] += lr_1 * delta_stage1
        
        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        lr_2 = alpha_pos if delta_stage2 > 0 else alpha_neg
        q_stage2_mf[state_idx, int(action_2[trial])] += lr_2 * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Based Uncertainty Aversion (Exploration Penalty)
This model hypothesizes that high OCI leads to an aversion to uncertainty. The model tracks how often states/actions are visited. Actions that are less explored have higher uncertainty. While a standard agent might explore (uncertainty bonus), a high OCI agent might avoid the unknown (uncertainty penalty).

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Based Uncertainty Aversion.
    
    This model assumes the participant tracks the familiarity (visit counts) of actions.
    Usually, low counts might encourage exploration (bonus).
    Here, we hypothesize that high OCI leads to 'uncertainty aversion', applying a 
    penalty to less-visited actions. The magnitude of this penalty is scaled by OCI.
    
    learning_rate: [0, 1] Standard learning rate.
    beta: [0, 10] Inverse temperature.
    aversion_strength: [0, 5] Scaling factor for the uncertainty penalty.
    """
    learning_rate, beta, aversion_strength = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Counts for Stage 1 actions to track familiarity
    counts_stage1 = np.ones(2) # Start with 1 to avoid division by zero
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Choice ---
        # Calculate uncertainty: Inverse of visit counts (1/N)
        # High counts -> Low uncertainty (small number)
        # Low counts -> High uncertainty (large number)
        uncertainty = 1.0 / counts_stage1
        
        # Penalty: OCI * aversion_strength * uncertainty
        # If OCI is high, the penalty for being uncertain is large.
        penalty = oci_score * aversion_strength * uncertainty
        
        # Net value = Q-value - Penalty
        net_value = q_stage1_mf - penalty
        
        exp_q1 = np.exp(beta * net_value)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # Update counts
        counts_stage1[int(action_1[trial])] += 1
        
        state_idx = int(state[trial])

        # --- Stage 2 Choice ---
        # Standard choice mechanism for stage 2
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1

        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```