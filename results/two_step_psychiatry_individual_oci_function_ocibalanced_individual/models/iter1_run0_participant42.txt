Here are 3 new cognitive models that incorporate the high OCI score into the decision-making process in different theoretical ways.

### Model 1: Hybrid RL with OCI-Driven Model-Based Suppression
This model hypothesizes that high OCI scores (compulsivity) are associated with a deficit in Model-Based control. Instead of a fixed mixing weight `w`, the balance between Model-Based (planning) and Model-Free (habit) systems is modulated by the OCI score. A high OCI score reduces the influence of the Model-Based system, leading to more habitual behavior.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL where OCI score suppresses Model-Based control.
    
    The mixing weight 'w' (0=MF, 1=MB) is dynamically calculated:
    w = w_base * (1 - oci_sensitivity * oci)
    High OCI leads to lower 'w', meaning more reliance on Model-Free habits.

    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    w_base: [0, 1] (Baseline model-based weight)
    oci_sensitivity: [0, 1] (How strongly OCI suppresses MB control)
    """
    learning_rate, beta, w_base, oci_sensitivity = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective w (mixing weight). 
    # If oci is high and sensitivity is high, w approaches 0 (Pure MF).
    w = w_base * (1.0 - oci_sensitivity * oci_score)
    w = np.clip(w, 0.0, 1.0) # Ensure w stays valid

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2) # Model-free Q-values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Model-free Q-values for stage 2

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based Calculation: V(state) = max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value: Mix MF and MB
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # --- Learning ---
        # Update Stage 1 MF (TD(1) or SARSA-like update)
        # Note: Standard Daw task usually updates Q_MF1 based on Q_MF2
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1
        
        # Update Stage 2 MF
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2
        
        # Also update Stage 1 MF based on reward (eligibility trace logic often simplified here)
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Asymmetric Learning Rate Modulated by OCI
This model proposes that high OCI scores lead to an asymmetry in how positive and negative prediction errors are processed. Specifically, individuals with high compulsivity might be hyper-sensitive to negative outcomes (punishment/lack of reward) or "misses," leading to different learning rates for positive (`alpha_pos`) vs negative (`alpha_neg`) prediction errors. The OCI score amplifies the learning rate for negative outcomes.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free RL with OCI-modulated Asymmetric Learning Rates.
    
    The learning rate for negative prediction errors (alpha_neg) is boosted by OCI.
    alpha_neg_effective = alpha_base * (1 + oci_boost * oci)
    
    This reflects a 'fear of failure' or hyper-sensitivity to errors common in OCD.

    Bounds:
    alpha_base: [0, 1] (Base learning rate)
    beta: [0, 10]
    oci_boost: [0, 5] (Multiplier for negative learning rate based on OCI)
    """
    alpha_base, beta, oci_boost = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective learning rates
    alpha_pos = alpha_base
    alpha_neg_effective = alpha_base * (1.0 + oci_boost * oci_score)
    alpha_neg_effective = np.clip(alpha_neg_effective, 0.0, 1.0)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # --- Learning ---
        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2[state_idx, int(action_2[trial])]
        if delta_stage2 >= 0:
            lr = alpha_pos
        else:
            lr = alpha_neg_effective
        q_stage2[state_idx, int(action_2[trial])] += lr * delta_stage2

        # Stage 1 Update (TD)
        delta_stage1 = q_stage2[state_idx, int(action_2[trial])] - q_stage1[int(action_1[trial])]
        # We apply the same asymmetry logic to the TD error at stage 1
        if delta_stage1 >= 0:
            lr_s1 = alpha_pos
        else:
            lr_s1 = alpha_neg_effective
            
        q_stage1[int(action_1[trial])] += lr_s1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Inverse Temperature (Beta) Modulation by OCI
This model posits that high OCI affects the exploration-exploitation trade-off. Specifically, high compulsivity might lead to more deterministic (rigid) behavior, reducing noise in decision-making. Here, the OCI score scales the inverse temperature parameter `beta`. A higher OCI leads to a higher `beta`, making choices more "greedy" and less exploratory.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free RL with OCI-modulated Inverse Temperature (Beta).
    
    High OCI scores increase 'beta', making the softmax function steeper.
    This leads to more deterministic/rigid choices (less exploration).
    
    effective_beta = beta_base * (1 + rigidity_factor * oci)

    Bounds:
    learning_rate: [0, 1]
    beta_base: [0, 10]
    rigidity_factor: [0, 5] (How much OCI increases beta)
    """
    learning_rate, beta_base, rigidity_factor = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective beta
    # Higher OCI -> Higher Beta -> More rigid/deterministic choices
    effective_beta = beta_base * (1.0 + rigidity_factor * oci_score)
    # Cap beta to prevent overflow in exp()
    effective_beta = np.clip(effective_beta, 0.0, 20.0) 

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        exp_q1 = np.exp(effective_beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(effective_beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # --- Learning ---
        # Standard Model-Free Q-learning updates
        delta_stage2 = reward[trial] - q_stage2[state_idx, int(action_2[trial])]
        q_stage2[state_idx, int(action_2[trial])] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2[state_idx, int(action_2[trial])] - q_stage1[int(action_1[trial])]
        q_stage1[int(action_1[trial])] += learning_rate * delta_stage1
        
        # Second stage reward propagating to first stage
        q_stage1[int(action_1[trial])] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```