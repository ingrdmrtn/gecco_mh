Here are 3 new cognitive models that incorporate the OCI-R score into the decision-making process, exploring different mechanisms than those previously tested.

### Model 1: OCI-Modulated Eligibility Traces
This model hypothesizes that high OCI leads to an over-attribution of outcomes to the initial choice, regardless of the intermediate state. In reinforcement learning terms, this is modeled as an enhanced "eligibility trace" ($\lambda$) where the reward at the second stage updates the first-stage value more strongly than usual, blurring the distinction between the two stages. High OCI increases this direct update, reflecting a compulsive linkage between action and distal outcome.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Eligibility Traces.
    
    This model posits that high OCI scores increase the strength of the 'eligibility trace' (lambda),
    causing the first-stage action to be updated more strongly by the second-stage reward directly.
    This reflects a compulsive tendency to link initial actions directly to outcomes, bypassing
    the causal structure of the transition.
    
    Parameters:
    learning_rate: [0,1] - Standard learning rate for value updates.
    beta: [0,10] - Inverse temperature for softmax choice.
    lambda_base: [0,1] - Base eligibility trace parameter.
    """
    learning_rate, beta, lambda_base = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # OCI modulates lambda: higher OCI -> higher lambda (stronger direct link)
    # We constrain lambda to be within [0, 1]
    lambda_param = lambda_base + (oci_score * (1.0 - lambda_base))
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        # Simple Model-Free policy for Stage 1 in this variant to isolate the trace effect
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updating ---
        
        # Stage 2 Prediction Error
        delta_stage2 = r - q_stage2[s_idx, a2]
        
        # Stage 1 Prediction Error (TD(0) part)
        delta_stage1 = q_stage2[s_idx, a2] - q_stage1[a1]
        
        # Update Stage 2
        q_stage2[s_idx, a2] += learning_rate * delta_stage2
        
        # Update Stage 1
        # Standard TD update + Eligibility Trace update from Stage 2 error
        # The trace allows the stage 2 reward error to propagate back to stage 1 immediately
        q_stage1[a1] += learning_rate * delta_stage1 + (learning_rate * lambda_param * delta_stage2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Driven Uncertainty Intolerance (Inverse Temperature Modulation)
This model suggests that OCI is related to "intolerance of uncertainty." In this computational framework, high uncertainty (when Q-values are close to 0.5 or unlearned) makes high-OCI individuals more erratic (lower beta), whereas when values are distinct, they become more rigid (higher beta). Here, we model a simpler version: OCI directly modulates the `beta` (inverse temperature) parameter specifically for the *second* stage choice, where the immediate reward is at stake. High OCI leads to "hyper-focused" or rigid selection (higher beta) at the proximal reward stage.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Driven Stage 2 Rigidity.
    
    This model proposes that OCI affects the exploration-exploitation balance (beta) 
    differently at the second stage. High OCI scores lead to a higher beta (more deterministic/rigid) 
    at the second stage, reflecting a compulsive need to exploit known rewards immediately 
    once the state is revealed.
    
    Parameters:
    learning_rate: [0,1]
    beta_stage1: [0,10] - Inverse temperature for the first stage choice.
    beta_stage2_base: [0,10] - Base inverse temperature for the second stage choice.
    """
    learning_rate, beta_stage1, beta_stage2_base = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # OCI increases rigidity (beta) at stage 2
    # We scale it such that high OCI adds to the base beta
    beta_stage2 = beta_stage2_base * (1.0 + oci_score)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy (Model-Based formulation from template) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # We use a pure Model-Based evaluation for Stage 1 here to simplify parameters
        # and focus the OCI effect on Stage 2 beta.
        exp_q1 = np.exp(beta_stage1 * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy (OCI Modulated Beta) ---
        exp_q2 = np.exp(beta_stage2 * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updating ---
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Note: We don't update a Stage 1 MF value here because the policy is purely MB 
        # (driven by max_q_stage2). This is a valid simplification for this specific hypothesis.
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Biased Initial Beliefs (Priors)
This model tests the hypothesis that high OCI is associated with strong, resistant prior beliefs. Instead of starting Q-values at 0.0 or 0.5 (neutral), high OCI participants might start with biased expectations (e.g., pessimism or optimism) that take time to unlearn. In this model, the initial Q-values are set based on the OCI score, and `learning_rate` determines how fast they move away from this prior.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Biased Initial Beliefs.
    
    This model hypothesizes that high OCI scores correlate with biased initial expectations (priors).
    Rather than starting with neutral values (0), the Q-values are initialized to a value 
    determined by a 'prior_bias' parameter scaled by OCI. This reflects the clinical observation 
    of 'fixed beliefs' or strong priors in OCD.
    
    Parameters:
    learning_rate: [0,1]
    beta: [0,10]
    prior_bias: [0,1] - The magnitude of the initial bias. 
                        If > 0.5, optimistic start; < 0.5, pessimistic start.
    """
    learning_rate, beta, prior_bias = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Initialize Q-values based on OCI and prior_bias parameter.
    # If OCI is 0, start at 0.5 (neutral).
    # If OCI is high, start closer to prior_bias.
    # We map prior_bias to a range [0, 1] for initialization.
    # A simple formulation: start_val = 0.5 + (prior_bias - 0.5) * oci_score
    # This means high OCI pushes the start value away from 0.5 towards the bias direction.
    
    start_val = 0.5 + (prior_bias - 0.5) * oci_score
    
    # Clip to ensure valid range [0,1]
    start_val = np.clip(start_val, 0.0, 1.0)

    q_stage1 = np.full(2, start_val)
    q_stage2 = np.full((2, 2), start_val)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updating ---
        # Standard SARSA/Q-learning updates
        
        # Stage 2 update
        delta_stage2 = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 update (TD(0))
        # Using the value of the state actually reached
        delta_stage1 = q_stage2[s_idx, a2] - q_stage1[a1]
        q_stage1[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```