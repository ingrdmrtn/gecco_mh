Here are three new cognitive models exploring different mechanisms for how OCI scores might influence decision-making in the two-step task, specifically focusing on learning rates and model-based control.

### Model 1: OCI-Modulated Model-Based Weighting
This model tests the hypothesis that higher OCI symptoms are associated with a rigid reliance on habitual (model-free) control, leading to a reduction in goal-directed (model-based) planning. The mixing weight `w` is derived from a base parameter and modulated by the OCI score.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Model-Based Weighting.
    
    This model hypothesizes that OCI symptoms interfere with model-based control.
    The mixing weight 'w' (which balances model-based and model-free values)
    is dynamically adjusted based on the participant's OCI score.
    Higher OCI reduces 'w', pushing the agent towards model-free habits.
    
    Parameters:
    lr: [0, 1] Learning rate for value updates.
    beta: [0, 10] Inverse temperature for softmax choice.
    w_base: [0, 1] Baseline mixing weight (model-basedness) for an average person.
    oci_w_decay: [0, 1] Strength of reduction in 'w' due to OCI.
    """
    lr, beta, w_base, oci_w_decay = model_parameters
    n_trials = len(action_1)
    oci = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    # Calculate effective w, ensuring it stays within [0, 1]
    # We hypothesize OCI reduces model-based control
    w = w_base * (1.0 - (oci * oci_w_decay))
    w = np.clip(w, 0.0, 1.0)

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combined value: w * MB + (1-w) * MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        state_idx = int(state[trial])

        # policy for the second choice
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        # Update Q-values
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += lr * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += lr * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Specific Learning Rate Asymmetry (Punishment Sensitivity)
This model investigates if OCI scores correlate with an increased sensitivity to "punishment" (lack of reward). It uses separate learning rates for positive and negative prediction errors, where the learning rate for negative errors is amplified by the OCI score. This reflects a "fear of failure" or hyper-correction mechanism often seen in compulsive behaviors.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Specific Punishment Sensitivity.
    
    This model separates learning from positive prediction errors (reward)
    and negative prediction errors (omission of reward). It hypothesizes that
    higher OCI leads to faster learning from negative outcomes (punishment).
    
    Parameters:
    lr_pos: [0, 1] Learning rate for positive prediction errors.
    lr_neg_base: [0, 1] Base learning rate for negative prediction errors.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Mixing weight (MB vs MF).
    oci_neg_amp: [0, 5] Amplification of negative learning rate by OCI.
    """
    lr_pos, lr_neg_base, beta, w, oci_neg_amp = model_parameters
    n_trials = len(action_1)
    oci = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    # Calculate effective negative learning rate
    lr_neg = lr_neg_base + (oci * oci_neg_amp)
    lr_neg = np.clip(lr_neg, 0.0, 1.0)

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        state_idx = int(state[trial])

        # policy for the second choice
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        # Update Stage 1
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        if delta_stage1 >= 0:
            q_stage1_mf[int(action_1[trial])] += lr_pos * delta_stage1
        else:
            q_stage1_mf[int(action_1[trial])] += lr_neg * delta_stage1
        
        # Update Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        if delta_stage2 >= 0:
            q_stage2_mf[state_idx, int(action_2[trial])] += lr_pos * delta_stage2
        else:
            q_stage2_mf[state_idx, int(action_2[trial])] += lr_neg * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Eligibility Trace (Lambda)
This model introduces an eligibility trace parameter ($\lambda$) to the standard hybrid model. The eligibility trace controls how much the second-stage reward directly updates the first-stage choice (bypassing the second-stage value estimate). Here, OCI modulates $\lambda$. A high $\lambda$ driven by OCI might represent a compulsion where the final outcome is rigidly associated with the very first action taken, ignoring the intermediate state structure.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Eligibility Trace.
    
    This model implements a TD(lambda) style update for the first stage.
    Instead of just using the second-stage value to update the first stage,
    the reward itself propagates back with decay lambda.
    OCI modulates this lambda parameter, testing if OCI is associated with 
    stronger direct association between start-action and end-reward.
    
    Parameters:
    lr: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Mixing weight (MB vs MF).
    lambda_base: [0, 1] Base eligibility trace decay.
    oci_lambda_mod: [0, 1] Modulation of lambda by OCI.
    """
    lr, beta, w, lambda_base, oci_lambda_mod = model_parameters
    n_trials = len(action_1)
    oci = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Calculate effective lambda
    # We hypothesize OCI might increase the direct credit assignment (higher lambda)
    # or decrease it. We formulate it additively.
    lam = lambda_base + (oci * oci_lambda_mod)
    lam = np.clip(lam, 0.0, 1.0)

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        state_idx = int(state[trial])

        # policy for the second choice
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        # Update Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += lr * delta_stage2
        
        # Update Stage 1 using Eligibility Trace logic
        # Standard TD(0) update part:
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        
        # The update to Stage 1 includes a proportion of the Stage 2 prediction error
        # driven by lambda. If lambda=0, it's standard TD. If lambda=1, it's Monte Carlo-like.
        combined_delta = delta_stage1 + (lam * delta_stage2)
        
        q_stage1_mf[int(action_1[trial])] += lr * combined_delta
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```