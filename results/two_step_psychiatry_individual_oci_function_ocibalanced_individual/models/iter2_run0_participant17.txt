Here are three new cognitive models that explore different mechanisms by which Obsessive-Compulsive Inventory (OCI) scores might influence learning and decision-making in the two-step task.

### Model 1: Hybrid Learner with OCI-modulated Model-Based Weight (`w`)
This model hypothesizes that individuals with higher OCI scores rely more on habitual (model-free) control and less on goal-directed (model-based) planning. This is often discussed in psychiatry as a shift towards compulsivity (habit). The parameter `w` controls the balance between model-based and model-free values, and here `w` is inversely modulated by OCI.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: Hybrid MB/MF learner where OCI reduces Model-Based control (w).
    
    Hypothesis: Higher OCI scores are associated with reduced goal-directed (Model-Based) 
    planning and increased reliance on habitual (Model-Free) control.
    
    Parameters:
    - learning_rate: [0, 1] Value update rate.
    - beta: [0, 10] Inverse temperature (softmax randomness).
    - w_base: [0, 1] Baseline weight for model-based values (0=pure MF, 1=pure MB).
    - oci_w_reduction: [0, 1] Strength of OCI in reducing 'w'.
    """
    learning_rate, beta, w_base, oci_w_reduction = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective mixing weight w. 
    # Higher OCI subtracts from w_base, pushing the agent towards Model-Free (0).
    w_eff = w_base - (oci_w_reduction * oci_score)
    w_eff = np.clip(w_eff, 0.0, 1.0)

    # Fixed transition matrix for the task (70% common, 30% rare)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)      # Model-free values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for stage 2 (also used for MB planning)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based Value Calculation
        # Max value of each planet (state)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        # Expected value based on transition probabilities
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combined Value
        q_net = w_eff * q_stage1_mb + (1 - w_eff) * q_stage1_mf
        
        # Softmax Policy
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Choice ---
        s_idx = int(state[trial])
        
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning / Updating ---
        r = reward[trial]
        
        # Stage 2 Update (Standard Q-learning)
        # Prediction error at second stage
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 Update (TD(1) / SARSA-like)
        # Prediction error at first stage (using the value of the state actually reached)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        
        # In this simplified hybrid model, we often update MF values using the full chain
        # Note: Some versions use lambda (eligibility). Here we assume lambda=1 for simplicity 
        # to focus on the 'w' parameter.
        q_stage1_mf[a1] += learning_rate * (delta_stage1 + delta_stage2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Perseveration (Stickiness) Modulated by OCI
This model focuses on "stickiness" or choice perseverationâ€”the tendency to repeat the previous choice regardless of reward. It hypothesizes that higher OCI scores are linked to higher perseveration (a form of behavioral rigidity or compulsion to repeat), independent of the learning rate.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: Model-Free Learner with OCI-modulated Choice Stickiness.
    
    Hypothesis: Higher OCI leads to increased choice perseveration (stickiness) 
    at the first stage, representing a compulsion to repeat actions regardless of outcome.
    
    Parameters:
    - learning_rate: [0, 1] Update rate.
    - beta: [0, 10] Inverse temperature.
    - stickiness_base: [0, 5] Baseline tendency to repeat the previous choice.
    - oci_stickiness_boost: [0, 5] Additional stickiness added per unit of OCI.
    """
    learning_rate, beta, stickiness_base, oci_stickiness_boost = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective stickiness
    stickiness_eff = stickiness_base + (oci_stickiness_boost * oci_score)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track previous choice for stickiness (initialize with -1 or handle first trial separately)
    last_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Add stickiness bonus to the Q-values before softmax
        q_stage1_with_stick = q_stage1_mf.copy()
        
        if last_action_1 != -1:
            q_stage1_with_stick[last_action_1] += stickiness_eff
        
        exp_q1 = np.exp(beta * q_stage1_with_stick)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # Update last action
        last_action_1 = a1
        
        # --- Stage 2 Choice ---
        s_idx = int(state[trial])
        
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        r = reward[trial]
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Simple TD(0) update for Stage 1 for this specific model to isolate stickiness effects
        # (Using state value of the arrived state)
        v_stage2_arrival = q_stage2_mf[s_idx, a2] # SARSA style
        delta_stage1 = v_stage2_arrival - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Asymmetric Learning Rates Modulated by OCI
This model investigates if OCI affects how participants process positive versus negative prediction errors. It hypothesizes that higher OCI might lead to "hyper-learning" from negative outcomes (punishment sensitivity) or a failure to unlearn bad habits, modeled as a specific boost to the learning rate for negative prediction errors (`alpha_neg`).

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: Dual Learning Rate Model (Pos/Neg) with OCI modulating Negative Learning.
    
    Hypothesis: High OCI individuals may over-weight negative prediction errors 
    (disappointments), leading to faster avoidance learning or anxiety-driven shifting.
    
    Parameters:
    - alpha_pos: [0, 1] Learning rate for positive prediction errors (reward > expectation).
    - alpha_neg_base: [0, 1] Baseline learning rate for negative prediction errors.
    - beta: [0, 10] Inverse temperature.
    - oci_neg_lr_scale: [0, 1] Scaling factor: how much OCI increases alpha_neg.
    """
    alpha_pos, alpha_neg_base, beta, oci_neg_lr_scale = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective negative learning rate
    # We constrain it to [0, 1]
    alpha_neg_eff = alpha_neg_base + (oci_neg_lr_scale * oci_score)
    alpha_neg_eff = np.clip(alpha_neg_eff, 0.0, 1.0)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Choice ---
        s_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        r = reward[trial]
        
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        # Select learning rate based on sign of PE
        lr_2 = alpha_pos if delta_stage2 >= 0 else alpha_neg_eff
        q_stage2_mf[s_idx, a2] += lr_2 * delta_stage2
        
        # Stage 1 Update
        # Using SARSA-style update (bootstrapping from Q-value of chosen 2nd stage action)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        
        lr_1 = alpha_pos if delta_stage1 >= 0 else alpha_neg_eff
        q_stage1_mf[a1] += lr_1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```