Here are three new cognitive models that incorporate the OCI score into the decision-making process in the two-step task. These models introduce different mechanisms for how obsessive-compulsive traits might influence learning and choice, specifically focusing on mixing model-based/model-free strategies, learning rate asymmetries, and inverse temperature modulation.

### Model 1: Hybrid Model-Based/Model-Free with OCI-Modulated Mixing Weight
This model tests the hypothesis that higher OCI scores relate to a deficit in model-based control (goal-directed planning) in favor of habitual (model-free) control. The mixing weight `w` determines the balance between these two systems, and this weight is linearly modulated by the OCI score.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free RL with OCI-modulated Mixing Weight.
    
    The contribution of the Model-Based system (w) is modulated by OCI.
    w = w_base + w_oci * oci_score.
    If w is closer to 1, behavior is more model-based. If closer to 0, more model-free.
    
    Parameters:
    - learning_rate: Rate at which Q-values are updated.
    - beta: Inverse temperature for softmax.
    - w_base: Baseline weighting for model-based values.
    - w_oci: Modulation of the weighting parameter by OCI score.
    
    Bounds:
    learning_rate: [0,1]
    beta: [0,10]
    w_base: [0,1]
    w_oci: [-1,1] (allows OCI to either increase or decrease MB reliance)
    """
    learning_rate, beta, w_base, w_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective w and clamp it between 0 and 1
    w = w_base + (w_oci * oci_score)
    w = np.clip(w, 0.0, 1.0)

    # Fixed transition matrix for the task (A->X 70%, U->Y 70%)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2) # Model-free Q-values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Q-values for stage 2 (2 states, 2 actions)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based Value Calculation: V_MB = Transition * max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value: Q_net = w * Q_MB + (1-w) * Q_MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        if a1 != -1: # Handle potential missing data if represented as -1
            p_choice_1[trial] = probs_1[a1]
        else:
            p_choice_1[trial] = 1.0 # Ignore missing trials in likelihood
            
        # --- Stage 2 Policy ---
        curr_state = int(state[trial])
        if curr_state != -1:
            exp_q2 = np.exp(beta * q_stage2_mf[curr_state, :])
            probs_2 = exp_q2 / np.sum(exp_q2)
            
            a2 = int(action_2[trial])
            if a2 != -1:
                p_choice_2[trial] = probs_2[a2]
                
                # --- Learning ---
                r = reward[trial]
                
                # TD Error Stage 2
                delta_stage2 = r - q_stage2_mf[curr_state, a2]
                q_stage2_mf[curr_state, a2] += learning_rate * delta_stage2
                
                # TD Error Stage 1 (SARSA-style update using Stage 2 value)
                # Note: In standard hybrid models, MF Q1 updates towards Q2 of chosen state/action
                delta_stage1 = q_stage2_mf[curr_state, a2] - q_stage1_mf[a1]
                q_stage1_mf[a1] += learning_rate * delta_stage1
            else:
                p_choice_2[trial] = 1.0
        else:
             p_choice_2[trial] = 1.0

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Asymmetric Learning Rates Modulated by OCI
This model hypothesizes that OCI is related to how individuals process positive versus negative prediction errors. High OCI might be associated with over-learning from punishment (or lack of reward) or under-learning from reward. Here, the learning rate for negative prediction errors (`alpha_neg`) is specifically modulated by the OCI score, while `alpha_pos` is constant.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free RL with Asymmetric Learning Rates, where Alpha_Neg is OCI-modulated.
    
    Parameters:
    - alpha_pos: Learning rate for positive prediction errors (RPE > 0).
    - alpha_neg_base: Base learning rate for negative prediction errors (RPE < 0).
    - alpha_neg_oci: Modulation of negative learning rate by OCI.
    - beta: Inverse temperature.
    
    Bounds:
    alpha_pos: [0,1]
    alpha_neg_base: [0,1]
    alpha_neg_oci: [-1,1]
    beta: [0,10]
    """
    alpha_pos, alpha_neg_base, alpha_neg_oci, beta = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective negative learning rate
    alpha_neg = alpha_neg_base + (alpha_neg_oci * oci_score)
    alpha_neg = np.clip(alpha_neg, 0.0, 1.0)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        if a1 != -1:
            p_choice_1[trial] = probs_1[a1]
        else:
            p_choice_1[trial] = 1.0

        # --- Stage 2 Choice ---
        curr_state = int(state[trial])
        if curr_state != -1:
            exp_q2 = np.exp(beta * q_stage2[curr_state, :])
            probs_2 = exp_q2 / np.sum(exp_q2)
            
            a2 = int(action_2[trial])
            if a2 != -1:
                p_choice_2[trial] = probs_2[a2]
                
                # --- Learning ---
                r = reward[trial]
                
                # Stage 2 Update
                delta2 = r - q_stage2[curr_state, a2]
                eff_alpha2 = alpha_pos if delta2 > 0 else alpha_neg
                q_stage2[curr_state, a2] += eff_alpha2 * delta2
                
                # Stage 1 Update
                # Using the value of the state chosen in stage 2 as the target
                delta1 = q_stage2[curr_state, a2] - q_stage1[a1]
                eff_alpha1 = alpha_pos if delta1 > 0 else alpha_neg
                q_stage1[a1] += eff_alpha1 * delta1
            else:
                p_choice_2[trial] = 1.0
        else:
            p_choice_2[trial] = 1.0

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Inverse Temperature (Exploration/Exploitation)
This model investigates if OCI scores correlate with the randomness of choice behavior (exploration vs. exploitation). A higher OCI might lead to more rigid, deterministic behavior (higher beta) or more erratic behavior (lower beta). The inverse temperature `beta` is defined as a linear function of the OCI score.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Based RL with OCI-modulated Inverse Temperature (Beta).
    
    This model assumes OCI affects the precision of choices (exploration/exploitation balance).
    Beta = beta_base + beta_oci * oci_score.
    
    Parameters:
    - learning_rate: Rate at which Q-values are updated.
    - beta_base: Baseline inverse temperature.
    - beta_oci: Slope of beta modulation by OCI.
    - w: Fixed weighting parameter for model-based control (0=MF, 1=MB).
    
    Bounds:
    learning_rate: [0,1]
    beta_base: [0,10]
    beta_oci: [-5,5] (Allows beta to increase or decrease with OCI)
    w: [0,1]
    """
    learning_rate, beta_base, beta_oci, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective beta
    beta = beta_base + (beta_oci * oci_score)
    # Ensure beta stays non-negative and within reasonable bounds
    beta = np.clip(beta, 0.0, 20.0) 
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Calculate Model-Based values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Weighted sum of MB and MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        if a1 != -1:
            p_choice_1[trial] = probs_1[a1]
        else:
            p_choice_1[trial] = 1.0

        # --- Stage 2 Choice ---
        curr_state = int(state[trial])
        if curr_state != -1:
            exp_q2 = np.exp(beta * q_stage2_mf[curr_state, :])
            probs_2 = exp_q2 / np.sum(exp_q2)
            
            a2 = int(action_2[trial])
            if a2 != -1:
                p_choice_2[trial] = probs_2[a2]
                
                # --- Learning ---
                r = reward[trial]
                
                # Update Stage 2 MF values
                delta2 = r - q_stage2_mf[curr_state, a2]
                q_stage2_mf[curr_state, a2] += learning_rate * delta2
                
                # Update Stage 1 MF values
                delta1 = q_stage2_mf[curr_state, a2] - q_stage1_mf[a1]
                q_stage1_mf[a1] += learning_rate * delta1
            else:
                p_choice_2[trial] = 1.0
        else:
            p_choice_2[trial] = 1.0

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```