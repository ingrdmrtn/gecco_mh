Here are three new cognitive models that incorporate the OCI score into the decision-making process in different ways.

### Model 1: OCI-Modulated Model-Based Weighting
This model hypothesizes that individuals with higher OCI scores rely more heavily on habit-based (Model-Free) learning rather than goal-directed (Model-Based) planning. This aligns with psychiatric theories suggesting compulsivity is linked to an over-reliance on habits. Here, the weighting parameter `w` is directly modulated by the OCI score.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL where OCI score modulates the balance between Model-Based and Model-Free control.
    
    Hypothesis: Higher OCI scores are associated with reduced model-based control (lower w),
    reflecting a shift towards habitual (model-free) decision making.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate for Q-value updates.
    beta: [0, 10] - Inverse temperature for softmax choice.
    w_intercept: [0, 1] - Baseline weighting for Model-Based control (at OCI=0).
    w_slope: [-1, 1] - The rate at which OCI reduces (or increases) Model-Based weighting.
    """
    learning_rate, beta, w_intercept, w_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate w based on OCI. We clip it to ensure it stays between 0 and 1.
    # If w_slope is negative, higher OCI reduces model-basedness.
    w = w_intercept + (w_slope * oci_score)
    w = np.clip(w, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combine MB and MF values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        qs_stage2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_stage2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # --- Updates ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])

        # TD(1) update for stage 1 MF
        # Note: In standard hybrid models, stage 1 MF is often updated via SARSA(lambda) or similar.
        # Here we use the immediate TD error from stage 2 value.
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

        # TD(0) update for stage 2 MF
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Choice Stickiness (Perseveration)
This model introduces a "stickiness" parameter that biases the agent to repeat the previous action. It hypothesizes that obsessive-compulsive traits lead to repetitive behaviors regardless of reward (perseveration). The strength of this stickiness is a function of the OCI score.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL with OCI-modulated choice perseveration (stickiness).
    
    Hypothesis: Higher OCI scores lead to increased 'stickiness' or perseveration 
    in the first-stage choice, meaning the participant is more likely to repeat 
    their previous spaceship choice regardless of the outcome.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - MB/MF weighting.
    stickiness_oci: [0, 5] - How much the OCI score amplifies the tendency to repeat the last choice.
    """
    learning_rate, beta, w, stickiness_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate stickiness bonus based on OCI
    stickiness_bonus = stickiness_oci * oci_score

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1 # Indicator for the previous action

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness to the Q-value of the previously chosen action
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness_bonus

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        qs_stage2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_stage2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # --- Updates ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])

        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        last_action_1 = a1 # Store action for next trial

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Learning Rate Asymmetry
This model suggests that OCI affects how individuals learn from positive versus negative prediction errors. Specifically, it tests if higher OCI scores make individuals more sensitive to punishments (or lack of reward) compared to rewards, potentially driving anxiety-based avoidance or checking behaviors.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL with OCI-modulated learning rate asymmetry.
    
    Hypothesis: OCI modulates the learning rate specifically for negative prediction errors 
    (disappointment/punishment sensitivity). High OCI might lead to faster updating 
    when outcomes are worse than expected.
    
    Parameters:
    alpha_base: [0, 1] - Base learning rate for positive prediction errors.
    alpha_neg_oci: [0, 1] - Additional learning rate magnitude for negative PEs, scaled by OCI.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - MB/MF weighting.
    """
    alpha_base, alpha_neg_oci, beta, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        qs_stage2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_stage2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # --- Updates ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])

        # Define effective learning rate based on sign of PE
        # We calculate PE for stage 2 first to determine if it was a 'disappointment'
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        
        current_alpha = alpha_base
        if delta_stage2 < 0:
             # If PE is negative, modify alpha based on OCI
             # We ensure alpha stays within [0, 1]
             current_alpha = alpha_base + (alpha_neg_oci * oci_score)
             current_alpha = np.clip(current_alpha, 0.0, 1.0)

        # Update Stage 1 (MF)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += current_alpha * delta_stage1

        # Update Stage 2
        q_stage2_mf[state_idx, a2] += current_alpha * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```