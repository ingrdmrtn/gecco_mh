Here are three new cognitive models exploring different mechanisms for how OCI might influence reinforcement learning processes in this two-step task.

### Model 1: Hybrid MB/MF with OCI-modulated Mixing Weight
This model hypothesizes that OCI symptoms correlate with a rigidity that favors habit (Model-Free) over goal-directed planning (Model-Based). It uses a mixing weight parameter $w$ that determines the balance between MB and MF values in the first stage. The OCI score modulates this weight, pushing the participant towards more Model-Free behavior as OCI increases.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based / Model-Free RL with OCI-modulated Mixing Weight.
    
    Hypothesis: Higher OCI scores are associated with a reliance on habitual (Model-Free) 
    control over goal-directed (Model-Based) planning. The mixing weight 'w' represents 
    the proportion of Model-Based control. OCI reduces 'w'.

    Parameters:
    - learning_rate: [0, 1] Learning rate for Q-value updates.
    - beta: [0, 10] Inverse temperature (softmax sensitivity).
    - w_base: [0, 1] Baseline weight for Model-Based control (at OCI=0).
    - oci_mod: [0, 1] Strength of OCI reduction on Model-Based weight.
      Effective w = w_base * (1 - oci * oci_mod).
    """
    learning_rate, beta, w_base, oci_mod = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    # Calculate effective mixing weight w. 
    # If oci_mod is high, higher OCI significantly reduces MB control.
    # We clip to ensure it stays in [0, 1].
    w = w_base * (1.0 - (current_oci * oci_mod))
    w = np.clip(w, 0.0, 1.0)

    # Fixed transition matrix for MB calculation (70% common, 30% rare)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)      # MF values for Stage 1
    q_stage2_mf = np.zeros((2, 2)) # MF values for Stage 2 (Aliens)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based Value: Q_MB(s1, a1) = Sum(P(s2|s1,a1) * max_a2 Q_MF(s2, a2))
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value: Q_net = w * Q_MB + (1-w) * Q_MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # --- Stage 2 Policy ---
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # --- Learning ---
        r = reward[trial]
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        
        # Update Stage 2 Q-values (TD(0))
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Update Stage 1 MF Q-values (TD(1) / Sarsa-like using Stage 2 value)
        # Note: In standard 2-step, MF updates often use the Q-value of the state reached
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Asymmetric Learning Rates Modulated by OCI
This model explores the idea that OCI is related to enhanced sensitivity to negative prediction errors (avoidance of failure) or positive prediction errors (perfectionism). Here, we model it as OCI modulating the learning rate specifically for *negative* prediction errors, creating an asymmetry in how "good" vs "bad" outcomes update beliefs.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free RL with OCI-modulated Asymmetric Learning Rates.
    
    Hypothesis: OCI is associated with altered sensitivity to errors. This model 
    splits the learning rate into alpha_pos (for positive RPEs) and alpha_neg 
    (for negative RPEs). The OCI score specifically scales alpha_neg, reflecting 
    potential hypersensitivity to worse-than-expected outcomes.

    Parameters:
    - alpha_base: [0, 1] Base learning rate.
    - beta: [0, 10] Inverse temperature.
    - pos_bias: [0, 2] Multiplier for positive RPEs (alpha_pos = alpha_base * pos_bias).
    - oci_neg_scale: [0, 5] Scaling factor for OCI effect on negative RPEs.
      alpha_neg = alpha_base * (1 + oci * oci_neg_scale).
    """
    alpha_base, beta, pos_bias, oci_neg_scale = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    # Define asymmetric learning rates
    alpha_pos = np.clip(alpha_base * pos_bias, 0, 1)
    # OCI increases sensitivity to negative errors
    alpha_neg = np.clip(alpha_base * (1.0 + current_oci * oci_neg_scale), 0, 1)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # Stage 1 Choice
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])

        # Stage 2 Choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # Learning
        r = reward[trial]
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])

        # Stage 2 Update
        delta2 = r - q_stage2_mf[state_idx, a2]
        eff_alpha2 = alpha_pos if delta2 > 0 else alpha_neg
        q_stage2_mf[state_idx, a2] += eff_alpha2 * delta2

        # Stage 1 Update
        delta1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        eff_alpha1 = alpha_pos if delta1 > 0 else alpha_neg
        q_stage1_mf[a1] += eff_alpha1 * delta1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Eligibility Traces with OCI-modulated Decay
This model introduces an eligibility trace parameter $\lambda$, which controls how much credit the first-stage action gets for the second-stage outcome. The hypothesis here is that OCI might affect the temporal window of credit assignmentâ€”perhaps high OCI leads to "over-thinking" or attributing outcomes to distal causes more strongly (higher $\lambda$), or conversely, focusing narrowly on immediate states.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    TD(lambda) RL with OCI-modulated Eligibility Trace Decay.
    
    Hypothesis: OCI affects the credit assignment process between stages.
    Lambda controls how strongly the Stage 1 choice is updated by the Stage 2 reward directly.
    We model lambda as a function of OCI. Higher OCI increases lambda, implying 
    stronger direct reinforcement of the first stage action by the final reward.

    Parameters:
    - learning_rate: [0, 1] Step size.
    - beta: [0, 10] Inverse temperature.
    - lambda_base: [0, 1] Baseline eligibility trace decay parameter.
    - oci_lambda_boost: [0, 1] Factor by which OCI increases lambda.
      lambda = lambda_base + (oci * oci_lambda_boost).
    """
    learning_rate, beta, lambda_base, oci_lambda_boost = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    # Calculate lambda
    lam = lambda_base + (current_oci * oci_lambda_boost)
    lam = np.clip(lam, 0.0, 1.0)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        # Stage 1 Choice
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # Stage 2 Choice
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]

        # TD Learning with Eligibility Traces
        # 1. Prediction error at stage 1 (transition to stage 2)
        # Note: Reward at stage 1 is 0.
        delta1 = q_stage2[state_idx, a2] - q_stage1[a1]
        
        # 2. Prediction error at stage 2 (reward receipt)
        delta2 = r - q_stage2[state_idx, a2]

        # Update Stage 1:
        # Standard TD(0) part: alpha * delta1
        # Eligibility trace part from stage 2: alpha * lambda * delta2
        q_stage1[a1] += learning_rate * (delta1 + lam * delta2)
        
        # Update Stage 2:
        q_stage2[state_idx, a2] += learning_rate * delta2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```