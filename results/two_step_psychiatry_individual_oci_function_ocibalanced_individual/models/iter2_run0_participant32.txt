def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: Hybrid Learner with OCI-modulated Eligibility Trace.
    
    Hypothesis: OCI scores relate to the ability to assign credit to past actions (Stage 1) 
    based on current outcomes (Stage 2). High OCI might disrupt or enhance this 'eligibility trace' (lambda),
    affecting how effectively the agent bridges the two stages in a Model-Free manner.
    
    Parameters:
    learning_rate: [0, 1] Learning rate for value updates.
    beta: [0, 10] Inverse temperature for softmax choices (both stages).
    w: [0, 1] Mixing weight for Stage 1 (0=Pure MF, 1=Pure MB).
    lam_base: [0, 1] Baseline eligibility trace decay (lambda).
    lam_oci_slope: [-1, 1] Modulation of lambda by OCI score.
    """
    learning_rate, beta, w, lam_base, lam_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate OCI-specific lambda, bounded between 0 and 1
    lam = lam_base + (lam_oci_slope * oci_score)
    lam = np.clip(lam, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based Value
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Updates ---
        # Prediction errors
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        
        # Update Stage 1 MF using eligibility trace (lambda)
        # This allows the Stage 2 RPE (delta_stage2) to update Stage 1 values
        q_stage1_mf[a1] += learning_rate * (delta_stage1 + lam * delta_stage2)
        
        # Update Stage 2 MF
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: Hybrid Learner with OCI-modulated Stage 2 Temperature.
    
    Hypothesis: OCI symptoms (e.g., intolerance of uncertainty) specifically impact decision-making 
    under immediate reward ambiguity (Stage 2), leading to different exploration/exploitation 
    balances compared to the planning stage (Stage 1).
    
    Parameters:
    learning_rate: [0, 1] Learning rate.
    beta_stage1: [0, 10] Inverse temperature for Stage 1.
    beta_stage2_base: [0, 10] Baseline inverse temperature for Stage 2.
    beta_stage2_oci_slope: [-5, 5] Effect of OCI on Stage 2 temperature.
    w: [0, 1] Mixing weight for Stage 1 (0=Pure MF, 1=Pure MB).
    """
    learning_rate, beta_stage1, beta_stage2_base, beta_stage2_oci_slope, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate Stage 2 Beta
    beta_stage2 = beta_stage2_base + (beta_stage2_oci_slope * oci_score)
    beta_stage2 = np.clip(beta_stage2, 0.0, 10.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_stage1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # --- Stage 2 Choice (Using OCI-modulated Beta) ---
        exp_q2 = np.exp(beta_stage2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: Hybrid Learner with OCI-modulated Value Decay (Forgetting).
    
    Hypothesis: High OCI scores correlate with 'doubt' or 'checking' behaviors, 
    modeled here as a faster decay (forgetting) of values for unchosen options. 
    The agent becomes less confident in the values of aliens they haven't visited recently.
    
    Parameters:
    learning_rate: [0, 1] Learning rate for chosen options.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Mixing weight.
    decay_base: [0, 1] Baseline decay rate for unchosen options (0=no decay, 1=reset to 0).
    decay_oci_slope: [-1, 1] Effect of OCI on decay rate.
    """
    learning_rate, beta, w, decay_base, decay_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate decay rate
    decay = decay_base + (decay_oci_slope * oci_score)
    decay = np.clip(decay, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        
        # Update chosen option
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Decay unchosen option in the current state
        unchosen_a2 = 1 - a2
        q_stage2_mf[state_idx, unchosen_a2] *= (1.0 - decay)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss