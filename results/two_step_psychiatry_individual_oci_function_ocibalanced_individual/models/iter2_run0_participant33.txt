def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Unchosen Value Decay.
    
    This is a hybrid Model-Based / Model-Free reinforcement learning model.
    It hypothesizes that OCI scores modulate the 'forgetting' or decay rate of 
    unchosen options at the first stage. High OCI might lead to obsessive retention 
    (low decay) or anxious forgetting (high decay) of the path not taken.
    
    Parameters:
    - alpha: [0, 1] Learning rate for chosen actions.
    - beta: [0, 10] Inverse temperature (softmax randomness).
    - w: [0, 1] Mixing weight (0 = pure Model-Free, 1 = pure Model-Based).
    - decay_base: [0, 1] Baseline decay rate for unchosen Stage 1 Q-values.
    - oci_decay_mod: [0, 1] Strength of OCI modulation.
      Effective decay = decay_base * (1.0 - oci_decay_mod * oci).
      (Higher OCI leads to lower decay, i.e., more obsessive holding onto values).
    """
    alpha, beta, w, decay_base, oci_decay_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    # Transition matrix (fixed structure of the task: A->X(70%), U->Y(70%))
    # Row 0: Space A, Row 1: Space U. Col 0: Planet X, Col 1: Planet Y.
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2) # Model-free values for spaceships
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for aliens (State x Action)

    # Calculate effective decay rate based on OCI
    # We clip to ensure it stays in valid [0, 1] range
    decay_effective = decay_base * (1.0 - (oci_decay_mod * oci_score))
    if decay_effective < 0: decay_effective = 0.0
    if decay_effective > 1: decay_effective = 1.0

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # 1. Model-Based Value: Expected max value of next stage weighted by transition probs
        max_q_stage2 = np.max(q_stage2_mf, axis=1) # Max value available at each planet
        q_stage1_mb = transition_matrix @ max_q_stage2 # Expected value for each spaceship
        
        # 2. Net Value: Weighted sum of MB and MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # 3. Softmax Choice
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        s_idx = int(state[trial]) # Planet reached
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        r = reward[trial]
        
        # Stage 2 Update (Standard Q-learning)
        # PE = Reward - Expectation
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += alpha * delta_stage2
        
        # Stage 1 Update (TD(1) / Eligibility Trace)
        # We update the chosen spaceship using the Stage 2 prediction error (direct reinforcement)
        # This assumes lambda=1 for simplicity to fit within parameter count constraints
        q_stage1_mf[a1] += alpha * delta_stage2
        
        # Decay Unchosen Stage 1 Option
        unchosen_a1 = 1 - a1
        q_stage1_mf[unchosen_a1] *= (1.0 - decay_effective)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Modulated Transition Belief (Distorted World Model).
    
    This hybrid model hypothesizes that OCI affects the Model-Based component 
    by distorting the agent's belief about the transition probabilities.
    High OCI might lead to 'fatalism' (believing transitions are more random/50-50)
    or 'rigidity' (believing transitions are deterministic/100-0).
    
    Parameters:
    - alpha: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight.
    - oci_trans_shift: [0, 1] OCI-based shift of transition belief.
      0.5 is neutral. >0.5 increases belief in common transition (determinism).
      <0.5 decreases belief (randomness).
    """
    alpha, beta, w, oci_trans_shift = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate Subjective Transition Probability
    # Base is 0.7. Modifier shifts this based on OCI.
    # Shift range: (oci_trans_shift - 0.5) gives range [-0.5, 0.5]
    # We scale this effect so OCI can push prob towards 0.5 or 1.0
    shift = (oci_trans_shift - 0.5) * 0.6 * oci_score # Scale factor 0.6 allows reaching ~1.0 or ~0.4
    
    p_common = 0.7 + shift
    # Clamp to valid probability range [0, 1]
    if p_common > 1.0: p_common = 1.0
    if p_common < 0.0: p_common = 0.0
    p_rare = 1.0 - p_common
    
    # Subjective Transition Matrix
    transition_matrix = np.array([[p_common, p_rare], [p_rare, p_common]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy (Hybrid) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2 # Uses subjective matrix
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        s_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        r = reward[trial]
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        q_stage2_mf[s_idx, a2] += alpha * delta_stage2
        q_stage1_mf[a1] += alpha * delta_stage2 # Direct reinforcement of stage 1 (Lambda=1)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Loss Aversion (Subjective Utility).
    
    This hybrid model hypothesizes that OCI correlates with Harm Avoidance.
    Negative rewards are subjectively amplified by OCI scores, making the agent
    perceive losses as worse than they objectively are. This affects the Q-value
    updates.
    
    Parameters:
    - alpha: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight.
    - oci_loss_amp: [0, 1] Amplification factor for negative rewards.
      If Reward < 0, Effective_Reward = Reward * (1 + oci_loss_amp * oci * 5).
    """
    alpha, beta, w, oci_loss_amp = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        s_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        r = reward[trial]
        
        # Apply Loss Aversion Logic
        # If reward is negative, amplify it based on OCI
        effective_r = r
        if r < 0:
            # Multiply by a factor > 1. E.g., -1 becomes -1.5
            # Factor 5.0 is a scaling constant to make the parameter range [0,1] meaningful
            effective_r = r * (1.0 + (oci_loss_amp * oci_score * 5.0))
        
        delta_stage2 = effective_r - q_stage2_mf[s_idx, a2]
        
        q_stage2_mf[s_idx, a2] += alpha * delta_stage2
        q_stage1_mf[a1] += alpha * delta_stage2 # Lambda=1 update

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss