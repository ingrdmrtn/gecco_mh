Here are three new cognitive models that incorporate the OCI score into the decision-making process in the two-step task. These models explore different mechanisms: modulation of the trade-off between model-based and model-free control, modulation of reward sensitivity, and modulation of learning rates based on reward valence (positive vs negative).

### Model 1: OCI-Modulated Model-Based/Model-Free Trade-off
This model hypothesizes that high OCI scores (associated with compulsivity) might bias an individual towards habitual (Model-Free) control over goal-directed (Model-Based) control. The mixing parameter `w` determines the balance, where `w=1` is pure Model-Based and `w=0` is pure Model-Free. Here, the baseline `w` is reduced by the OCI score.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based / Model-Free Reinforcement Learning.
    The mixing weight 'w' (Model-Based weight) is modulated by OCI.
    Hypothesis: High OCI reduces Model-Based control (more habitual/compulsive).
    
    w_effective = w_base * (1 - oci_impact * oci)
    If oci_impact is high, high OCI participants become more Model-Free.

    Parameters:
    learning_rate: [0, 1]
    beta: [0, 10]
    w_base: [0, 1] - Baseline weight for Model-Based values.
    oci_impact: [0, 1] - How strongly OCI reduces the Model-Based weight.
    """
    learning_rate, beta, w_base, oci_impact = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate effective w, ensuring it stays in [0, 1]
    # Higher OCI -> Lower w -> More Model-Free
    w_effective = w_base * (1.0 - (oci_impact * current_oci))
    w_effective = np.clip(w_effective, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based Value: Transition * Max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_hybrid = (w_effective * q_stage1_mb) + ((1 - w_effective) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_hybrid)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # --- Learning ---
        # Stage 2 Update (TD)
        # Prediction Error: Reward - Expected
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2
        
        # Stage 1 Update (TD)
        # Using the value of the state actually reached
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Reward Sensitivity (Lapse Rate)
This model assumes that OCI affects how "noisy" or random decisions are, specifically by modulating a lapse rate (epsilon). High OCI might be associated with more rigid adherence to values (low noise) or conversely, high anxiety leading to inconsistent choices (high noise). This model tests if OCI scales the random exploration component `epsilon`.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Pure Model-Free Q-learning with Epsilon-Greedy-like noise modulated by OCI.
    Instead of just Softmax temperature, we introduce a mixture of random choice.
    
    The probability of choosing randomly (lapse rate) is:
    epsilon = base_epsilon + (oci_noise_factor * oci)
    
    This tests if high OCI participants are more 'noisy' or inconsistent.

    Parameters:
    learning_rate: [0, 1]
    beta: [0, 10]
    base_epsilon: [0, 0.5] - Baseline probability of random choice.
    oci_noise_factor: [0, 0.5] - How much OCI increases the noise/lapse rate.
    """
    learning_rate, beta, base_epsilon, oci_noise_factor = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate effective epsilon
    epsilon = base_epsilon + (oci_noise_factor * current_oci)
    epsilon = np.clip(epsilon, 0.0, 0.5) # Cap at 0.5 (random guessing)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        softmax_probs_1 = exp_q1 / np.sum(exp_q1)
        
        # Mixture: (1-eps)*Softmax + eps*Random(0.5)
        final_probs_1 = (1 - epsilon) * softmax_probs_1 + (epsilon * 0.5)
        p_choice_1[trial] = final_probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        softmax_probs_2 = exp_q2 / np.sum(exp_q2)
        
        final_probs_2 = (1 - epsilon) * softmax_probs_2 + (epsilon * 0.5)
        p_choice_2[trial] = final_probs_2[int(action_2[trial])]

        # --- Learning ---
        # Standard TD updates
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Punishment Sensitivity
This model investigates if OCI is linked to an asymmetry in learning from positive vs. negative outcomes. Specifically, it tests if higher OCI scores lead to higher learning rates for negative prediction errors (disappointments), reflecting a "fear of failure" or perfectionism often seen in OCI profiles.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Based / Model-Free hybrid with asymmetric learning rates.
    The learning rate for NEGATIVE prediction errors is modulated by OCI.
    
    alpha_neg = alpha_base + (oci_punishment_sensitivity * oci)
    alpha_pos = alpha_base
    
    Hypothesis: High OCI individuals over-update on negative outcomes.

    Parameters:
    alpha_base: [0, 1] - Base learning rate (used for positive PE).
    beta: [0, 10]
    w: [0, 1] - Fixed Model-Based weight.
    oci_punish_sens: [0, 1] - Additional learning rate magnitude for negative PEs based on OCI.
    """
    alpha_base, beta, w, oci_punish_sens = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Define learning rates
    alpha_pos = alpha_base
    alpha_neg = alpha_base + (oci_punish_sens * current_oci)
    alpha_neg = np.clip(alpha_neg, 0.0, 1.0) # Ensure bounds

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_hybrid = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_hybrid)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # --- Learning ---
        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        
        if delta_stage2 >= 0:
            lr_s2 = alpha_pos
        else:
            lr_s2 = alpha_neg
            
        q_stage2_mf[state_idx, int(action_2[trial])] += lr_s2 * delta_stage2
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        
        if delta_stage1 >= 0:
            lr_s1 = alpha_pos
        else:
            lr_s1 = alpha_neg
            
        q_stage1_mf[int(action_1[trial])] += lr_s1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```