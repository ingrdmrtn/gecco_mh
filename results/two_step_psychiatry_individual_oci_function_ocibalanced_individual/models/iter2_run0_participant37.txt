Here are three new cognitive models that explore different mechanisms by which Obsessive-Compulsive Inventory (OCI) scores might influence decision-making in the two-step task, specifically focusing on how high OCI scores (like the participant's 0.733) might relate to habitual behavior, learning rates, or uncertainty.

### Model 1: OCI-Modulated Model-Based/Model-Free Balance
This model hypothesizes that individuals with higher OCI scores rely less on model-based planning (goal-directed) and more on model-free (habitual) reinforcement learning. The weighting parameter `w` (which controls the mix between MB and MF values) is dynamically adjusted based on the OCI score.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Model-Based/Model-Free Balance.

    This model assumes that the balance between Model-Based (MB) and Model-Free (MF) 
    control is influenced by the OCI score. Higher OCI scores are hypothesized to 
    reduce the contribution of the MB system (w), pushing the agent towards more 
    habitual MF control.

    Bounds:
    learning_rate: [0,1]
    beta: [0,10]
    w_intercept: [0,1] - Baseline weight for MB system (at OCI=0).
    w_slope: [-1,1] - Change in MB weight per unit of OCI.
    """
    learning_rate, beta, w_intercept, w_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate the mixing weight w based on OCI
    # w represents the strength of Model-Based control [0 = Pure MF, 1 = Pure MB]
    # We clip it to ensure it stays valid.
    w = w_intercept + (w_slope * oci_score)
    w = np.clip(w, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based Value Calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Q-value (Weighted mix of MB and MF)
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        if a1 != -1:
            p_choice_1[trial] = probs_1[a1]
        else:
            p_choice_1[trial] = 1.0

        # Skip update if data is missing
        if a1 == -1 or state[trial] == -1 or action_2[trial] == -1:
            continue

        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning Updates ---
        # Stage 2 Update (TD)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 Update (TD - Model Free)
        # Note: We update the MF value regardless of the w used for choice
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    # Handle missing data or numerical issues
    p_choice_1 = np.maximum(p_choice_1, eps)
    p_choice_2 = np.maximum(p_choice_2, eps)
    
    log_loss = -(np.sum(np.log(p_choice_1)) + np.sum(np.log(p_choice_2)))
    return log_loss
```

### Model 2: OCI-Dependent Punishment Sensitivity
This model investigates if high OCI scores lead to different learning from positive versus negative outcomes (or lack of reward). Specifically, it splits the learning rate into a base rate and a specific "punishment" (zero reward) multiplier that scales with OCI. This reflects the hypothesis that OCD-like traits might involve hyper-sensitivity to errors or missed rewards.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Dependent Punishment Sensitivity.

    This model assumes that OCI scores modulate how strongly the agent learns from 
    non-rewarded trials (punishments/omissions).
    
    Bounds:
    lr_reward: [0,1] - Learning rate for rewarded trials (r=1).
    beta: [0,10]
    punish_mult_base: [0,5] - Base multiplier for learning from non-reward (r=0).
    punish_mult_oci: [-2,2] - How OCI changes the punishment multiplier.
    w_mb: [0,1] - Fixed weight for Model-Based control.
    """
    lr_reward, beta, punish_mult_base, punish_mult_oci, w_mb = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective learning rate for punishment (r=0)
    # If punish_mult > 1, they learn faster from failure than success.
    # If punish_mult < 1, they learn slower from failure.
    punish_mult = punish_mult_base + (punish_mult_oci * oci_score)
    punish_mult = np.maximum(punish_mult, 0.0) # Ensure non-negative

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w_mb * q_stage1_mb) + ((1 - w_mb) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        if a1 != -1:
            p_choice_1[trial] = probs_1[a1]
        else:
            p_choice_1[trial] = 1.0

        if a1 == -1 or state[trial] == -1 or action_2[trial] == -1:
            continue

        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning Updates ---
        # Determine learning rate for this trial based on outcome
        if r > 0:
            current_alpha = lr_reward
        else:
            current_alpha = lr_reward * punish_mult
            # Clip to ensure stability [0, 1]
            current_alpha = np.clip(current_alpha, 0.0, 1.0)

        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += current_alpha * delta_stage2
        
        # Stage 1 Update (TD)
        # Using the same alpha for consistency in this hypothesis
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += current_alpha * delta_stage1

    eps = 1e-10
    p_choice_1 = np.maximum(p_choice_1, eps)
    p_choice_2 = np.maximum(p_choice_2, eps)
    log_loss = -(np.sum(np.log(p_choice_1)) + np.sum(np.log(p_choice_2)))
    return log_loss
```

### Model 3: OCI-Modulated Eligibility Trace (Lambda)
This model introduces an eligibility trace parameter ($\lambda$) that connects the Stage 2 outcome directly to the Stage 1 choice. It tests the hypothesis that high OCI scores affect the "credit assignment" processâ€”specifically, how much the outcome at the end of the chain reinforces the very first choice made.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Eligibility Trace.

    This model uses a TD(lambda) inspired update for the Stage 1 values.
    Instead of just updating Stage 1 based on the transition to Stage 2, 
    the reward from Stage 2 is allowed to 'leak' back to Stage 1 directly via 
    an eligibility trace parameter (lambda). The strength of this trace is 
    modulated by the OCI score.
    
    Bounds:
    learning_rate: [0,1]
    beta: [0,10]
    lambda_base: [0,1] - Base eligibility trace.
    lambda_oci: [-1,1] - Modulation of trace by OCI.
    w_mb: [0,1]
    """
    learning_rate, beta, lambda_base, lambda_oci, w_mb = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate lambda based on OCI
    # Lambda controls direct reinforcement of Stage 1 by the final Reward.
    # High lambda -> Stage 1 acts more like it's directly rewarded.
    lam = lambda_base + (lambda_oci * oci_score)
    lam = np.clip(lam, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w_mb * q_stage1_mb) + ((1 - w_mb) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        if a1 != -1:
            p_choice_1[trial] = probs_1[a1]
        else:
            p_choice_1[trial] = 1.0

        if a1 == -1 or state[trial] == -1 or action_2[trial] == -1:
            continue

        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning Updates ---
        
        # Standard TD Error for Stage 2
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 Update with Eligibility Trace
        # The update has two components:
        # 1. The standard TD component: (Q_stage2 - Q_stage1)
        # 2. The direct reward component via lambda: lambda * delta_stage2
        
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        
        # If lambda is 0, this is standard TD(0): update based on next state value.
        # If lambda is 1, this approaches Monte Carlo: update based on final reward.
        combined_error = delta_stage1 + (lam * delta_stage2)
        
        q_stage1_mf[a1] += learning_rate * combined_error

    eps = 1e-10
    p_choice_1 = np.maximum(p_choice_1, eps)
    p_choice_2 = np.maximum(p_choice_2, eps)
    log_loss = -(np.sum(np.log(p_choice_1)) + np.sum(np.log(p_choice_2)))
    return log_loss
```