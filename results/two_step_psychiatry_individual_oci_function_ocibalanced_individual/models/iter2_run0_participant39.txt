Here are three new cognitive models that incorporate the OCI score to modulate learning and decision-making in the two-step task.

### Model 1: Hybrid Learner with OCI-modulated Model-Based Weight
This model posits that individuals with high OCI scores might rely more on habitual (Model-Free) control rather than goal-directed (Model-Based) planning. It introduces a mixing parameter `w` that determines the balance between MF and MB values, where `w` is a function of the OCI score. Specifically, higher OCI scores reduce the weight of the Model-Based system.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid learner where the balance between Model-Based (MB) and Model-Free (MF) 
    control is modulated by OCI.
    
    Higher OCI scores reduce the weight (w) of the Model-Based system, 
    biasing the agent towards Model-Free habits.
    
    w_effective = w_max * (1 - oci_impact * oci_score)
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    w_max: [0, 1] - Maximum possible MB weight (for OCI=0)
    oci_impact: [0, 1] - How strongly OCI reduces MB weight
    """
    learning_rate, beta, w_max, oci_impact = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective weight w. 
    # If oci_impact is high and oci_score is high, w becomes small (more MF).
    w_effective = w_max * (1.0 - (oci_impact * oci_score))
    # Ensure w stays in [0, 1]
    w_effective = np.clip(w_effective, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # Used for both MF stage 2 and MB calculation

    for trial in range(n_trials):
        
        # --- Stage 1 Decision ---
        # Model-Based Value: Transition * Max(Q2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value: w * MB + (1-w) * MF
        q_net = (w_effective * q_stage1_mb) + ((1 - w_effective) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Decision ---
        s_idx = int(state[trial])
        
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        r = reward[trial]
        
        # Update Stage 2 Q-values (Standard Q-learning)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Update Stage 1 MF Q-values (TD(0))
        # Note: In standard hybrid models, MF updates based on Q2
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Model-Free Learner with OCI-modulated Punishment Sensitivity
This model tests the hypothesis that high OCI (associated with anxiety and harm avoidance) leads to an increased sensitivity to negative outcomes (lack of reward). The learning rate is split into a base rate for rewards and a separate rate for non-rewards (punishments), where the punishment learning rate is amplified by the OCI score.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner with separate learning rates for reward (alpha_pos) 
    and no-reward (alpha_neg). The learning rate for no-reward is modulated by OCI.
    
    alpha_neg_effective = alpha_neg_base + (oci_sens * oci_score)
    
    Bounds:
    alpha_pos: [0, 1] - Learning rate for positive rewards
    alpha_neg_base: [0, 1] - Base learning rate for zero rewards
    beta: [0, 10]
    oci_sens: [0, 1] - Sensitivity of negative learning rate to OCI
    """
    alpha_pos, alpha_neg_base, beta, oci_sens = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective negative learning rate
    alpha_neg_effective = alpha_neg_base + (oci_sens * oci_score)
    alpha_neg_effective = np.clip(alpha_neg_effective, 0.0, 1.0)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Decision ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Decision ---
        s_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        r = reward[trial]
        
        # Determine which learning rate to use
        current_alpha = alpha_pos if r > 0 else alpha_neg_effective
        
        # Update Stage 2
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += current_alpha * delta_stage2
        
        # Update Stage 1
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += current_alpha * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Model-Based Learner with OCI-modulated Forgetting
This model suggests that OCI relates to memory or uncertainty management. Specifically, it implements a decay (forgetting) parameter for unchosen options. High OCI scores might lead to "hyper-active" updating or doubt, modeled here as an increased decay rate for the Q-values of options that were *not* chosen, pulling them back towards a neutral value (0.5).

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Based learner (pure MB for stage 1) with passive decay (forgetting) 
    for unchosen options. The rate of decay is modulated by OCI.
    
    decay_rate = decay_base + (oci_factor * oci_score)
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    decay_base: [0, 0.5] - Base decay toward 0.5 for unchosen options
    oci_factor: [0, 0.5] - How much OCI increases the decay rate
    """
    learning_rate, beta, decay_base, oci_factor = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    decay_rate = decay_base + (oci_factor * oci_score)
    decay_rate = np.clip(decay_rate, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Initialize Q-values at 0.5 (neutral expectation)
    q_stage2_mf = np.full((2, 2), 0.5) 

    for trial in range(n_trials):

        # --- Stage 1 Decision (Pure Model-Based) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Decision ---
        s_idx = int(state[trial])
        
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        r = reward[trial]
        
        # Update CHOSEN option in Stage 2
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Decay UNCHOSEN options in Stage 2 towards 0.5
        # This reflects uncertainty/forgetting about unvisited states
        unchosen_a2 = 1 - a2
        q_stage2_mf[s_idx, unchosen_a2] += decay_rate * (0.5 - q_stage2_mf[s_idx, unchosen_a2])
        
        # Also decay options in the unvisited state
        unvisited_state = 1 - s_idx
        q_stage2_mf[unvisited_state, 0] += decay_rate * (0.5 - q_stage2_mf[unvisited_state, 0])
        q_stage2_mf[unvisited_state, 1] += decay_rate * (0.5 - q_stage2_mf[unvisited_state, 1])

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```