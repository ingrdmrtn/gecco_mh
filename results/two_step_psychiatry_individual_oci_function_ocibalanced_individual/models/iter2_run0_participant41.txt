Here are three new cognitive models that incorporate the OCI score into the decision-making process using mechanisms distinct from those previously tried.

### Model 1: OCI-Stiffened Inverse Temperature (Rigidity)
This model hypothesizes that high OCI scores are associated with more rigid, deterministic choice behaviors (less exploration). The OCI score scales the inverse temperature parameter ($\beta$), making the softmax function "stiffer" (higher $\beta$) for participants with higher OCI.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Stiffened Inverse Temperature.
    
    This model assumes a Model-Free Q-learning process where the randomness 
    of choices (exploration vs. exploitation) is modulated by OCI.
    High OCI leads to a higher effective beta (inverse temperature), resulting 
    in more deterministic (rigid) behavior.
    
    Parameters:
    learning_rate: [0, 1] Learning rate for Q-value updates.
    beta_base: [0, 10] Baseline inverse temperature for low OCI.
    oci_stiffness: [0, 5] Scaling factor; determines how much OCI increases beta.
    """
    learning_rate, beta_base, oci_stiffness = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Effective Beta increases with OCI
    beta_eff = beta_base * (1.0 + oci_stiffness * oci_score)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values for Stage 1 (2 options) and Stage 2 (2 states x 2 options)
    q_stage1 = np.full(2, 0.5) 
    q_stage2 = np.full((2, 2), 0.5) 

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta_eff * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # --- Stage 2 Choice ---
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta_eff * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Learning ---
        # Prediction Error Stage 1: Driven by value of state reached
        # Using Max Q of stage 2 as the value of the state (Q-learning)
        v_stage2 = np.max(q_stage2[state_idx])
        delta_stage1 = v_stage2 - q_stage1[int(action_1[trial])]
        q_stage1[int(action_1[trial])] += learning_rate * delta_stage1
        
        # Prediction Error Stage 2: Driven by reward
        delta_stage2 = reward[trial] - q_stage2[state_idx, int(action_2[trial])]
        q_stage2[state_idx, int(action_2[trial])] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Mediated Eligibility Trace (Credit Assignment)
This model hypothesizes that OCI affects how participants assign credit to past actions. Specifically, it implements a TD($\lambda$) mechanism where the eligibility trace parameter $\lambda$ is determined by the OCI score. High OCI participants might overly connect distal outcomes (rewards) to the initial choice (Stage 1), effectively "blaming" or "crediting" the first step more strongly for the final result.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Mediated Eligibility Trace.
    
    This model uses temporal difference learning where the eligibility trace (lambda)
    depends on OCI. A higher OCI score increases lambda, meaning the Stage 1 Q-value 
    is more strongly updated by the Stage 2 Reward Prediction Error. This represents 
    a stronger (potentially superstitious) link between distal choices and outcomes.
    
    Parameters:
    learning_rate: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    lambda_scale: [0, 1] Scaling factor for eligibility trace. Lambda = lambda_scale * OCI.
    """
    learning_rate, beta, lambda_scale = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Lambda is bounded between 0 and 1
    eligibility_lambda = np.clip(lambda_scale * oci_score, 0.0, 1.0)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.full(2, 0.5)
    q_stage2 = np.full((2, 2), 0.5)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # --- Stage 2 Policy ---
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Learning ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        
        # Stage 1 RPE: Difference between Stage 2 Value and Stage 1 Value
        # Here we use the value of the chosen option in Stage 2 (SARSA-like connection)
        delta_stage1 = q_stage2[state_idx, a2] - q_stage1[a1]
        
        # Stage 2 RPE: Difference between Reward and Stage 2 Value
        delta_stage2 = reward[trial] - q_stage2[state_idx, a2]
        
        # Update Stage 2
        q_stage2[state_idx, a2] += learning_rate * delta_stage2
        
        # Update Stage 1
        # Direct update from transition (TD(0) part)
        q_stage1[a1] += learning_rate * delta_stage1
        # Eligibility trace update: Propagate Stage 2 error back to Stage 1 (TD(lambda) part)
        q_stage1[a1] += learning_rate * eligibility_lambda * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Induced Memory Decay (Interference)
This model hypothesizes that obsessive-compulsive symptoms involve intrusive thoughts that interfere with working memory. As a result, stored values for actions that were *not* chosen decay (forgetting) faster for individuals with high OCI scores.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Induced Memory Decay.
    
    This model assumes a Model-Free learner where unchosen Q-values decay over time.
    The rate of decay is proportional to the OCI score, simulating memory interference
    or the need to 'refresh' certainty.
    
    Parameters:
    learning_rate: [0, 1] Learning rate for chosen options.
    beta: [0, 10] Inverse temperature.
    forget_scale: [0, 1] Scaling factor for decay. Decay rate = forget_scale * OCI.
    """
    learning_rate, beta, forget_scale = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    decay_rate = np.clip(forget_scale * oci_score, 0.0, 1.0)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.full(2, 0.5)
    q_stage2 = np.full((2, 2), 0.5)
    
    initial_value = 0.5

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # --- Stage 2 Policy ---
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Learning ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        
        # Standard Q-Learning Updates
        v_stage2 = np.max(q_stage2[state_idx])
        delta_stage1 = v_stage2 - q_stage1[a1]
        q_stage1[a1] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2[state_idx, a2]
        q_stage2[state_idx, a2] += learning_rate * delta_stage2
        
        # --- Forgetting (Decay) for Unchosen Options ---
        # Decay Stage 1 unchosen option
        unchosen_a1 = 1 - a1
        q_stage1[unchosen_a1] = (1 - decay_rate) * q_stage1[unchosen_a1] + decay_rate * initial_value
        
        # Decay Stage 2 unchosen option (in the current state)
        unchosen_a2 = 1 - a2
        q_stage2[state_idx, unchosen_a2] = (1 - decay_rate) * q_stage2[state_idx, unchosen_a2] + decay_rate * initial_value
        
        # Note: We do not decay the unvisited state's values in this simple version, 
        # though one could argue interference affects all memory.

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```