Here are three new cognitive models exploring different mechanisms for how high OCI scores (indicating high obsessive-compulsive symptoms) might influence decision-making in the two-step task. These models focus on habit formation, uncertainty-driven exploration, and alterations in model-based planning.

### Model 1: OCI-Modulated Habit Strength (Perseveration)
This model hypothesizes that high OCI leads to stronger habit formation or "stickiness." Individuals with high compulsivity may be more likely to repeat previous actions regardless of the outcome. Here, the OCI score scales a "stickiness" parameter that adds a bonus to the previously chosen action in the first stage, making behavior more repetitive and resistant to change.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Habit Strength (Perseveration).
    
    This model assumes that higher OCI scores lead to increased choice stickiness (perseveration).
    A 'stickiness' bonus is added to the Q-value of the action chosen in the previous trial.
    The magnitude of this stickiness is scaled by the participant's OCI score.
    
    Parameters:
    learning_rate: [0,1] - Standard learning rate for Q-value updates.
    beta: [0,10] - Inverse temperature for softmax.
    stickiness_weight: [0,5] - Base weight for repeating the previous action.
    """
    learning_rate, beta, stickiness_weight = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Scale stickiness by OCI score. Higher OCI -> more sticky.
    # We add a baseline + scaling to ensure effect is prominent for high OCI.
    effective_stickiness = stickiness_weight * (1.0 + oci_score)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values for Model-Free (MF) system
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        
        # --- Stage 1 Choice ---
        # Calculate action values with stickiness bonus
        q_stage1_modified = q_stage1.copy()
        if last_action_1 != -1:
            q_stage1_modified[int(last_action_1)] += effective_stickiness
            
        exp_q1 = np.exp(beta * q_stage1_modified)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        # Store probability of chosen action
        current_action_1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[current_action_1]
        
        state_idx = int(state[trial])

        # --- Stage 2 Choice ---
        # Standard Softmax on Stage 2 Q-values
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        current_action_2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[current_action_2]

        # --- Learning ---
        # Update Stage 2 Q-values (Standard Q-learning)
        # Prediction Error: Reward - Current Expectation
        pe_2 = reward[trial] - q_stage2[state_idx, current_action_2]
        q_stage2[state_idx, current_action_2] += learning_rate * pe_2
        
        # Update Stage 1 Q-values (TD(0))
        # We use the value of the state we arrived at (max Q or actual Q)
        # Here we use the value of the chosen action in stage 2 as the target
        value_state_2 = q_stage2[state_idx, current_action_2]
        pe_1 = value_state_2 - q_stage1[current_action_1]
        q_stage1[current_action_1] += learning_rate * pe_1

        last_action_1 = current_action_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Driven Mixing Weight (Model-Based vs Model-Free)
This model proposes that high OCI affects the balance between Goal-Directed (Model-Based) and Habitual (Model-Free) control. Specifically, it tests the hypothesis that high compulsivity correlates with a deficit in Model-Based control, leading to a reliance on Model-Free (habitual) systems. The mixing parameter `w` (where 1 is pure MB and 0 is pure MF) is constrained to be lower for higher OCI scores.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Driven Mixing Weight (MB vs MF Balance).
    
    This model integrates Model-Based (MB) and Model-Free (MF) reinforcement learning.
    The weight 'w' determines the contribution of the MB system to the final choice.
    We model 'w' as decreasing with OCI score, reflecting the hypothesis that 
    compulsivity is associated with reduced goal-directed control (lower w).
    
    Parameters:
    learning_rate: [0,1] - Learning rate for MF updates.
    beta: [0,10] - Inverse temperature.
    w_max: [0,1] - The maximum model-based weight (for OCI=0).
    """
    learning_rate, beta, w_max = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate mixing weight w based on OCI.
    # Higher OCI reduces w, pushing behavior towards Model-Free.
    # We use a linear scaling: w = w_max * (1 - OCI)
    w = w_max * (1.0 - oci_score)

    # Fixed transition matrix for the task (MB system)
    # A -> X (0.7), A -> Y (0.3) | U -> X (0.3), U -> Y (0.7)
    # Rows: Choice (A, U), Cols: State (X, Y)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)
    q_stage2 = np.zeros((2, 2)) # Used by both MF and MB for terminal values

    for trial in range(n_trials):
        
        # --- Stage 1 Choice ---
        # 1. Model-Free Value
        q_net_mf = q_stage1_mf
        
        # 2. Model-Based Value (Bellman Equation using known transitions)
        # V(state) = max_action Q(state, action)
        max_q_stage2 = np.max(q_stage2, axis=1) # Max value of each planet
        q_net_mb = transition_matrix @ max_q_stage2
        
        # 3. Integrated Value
        q_net = (w * q_net_mb) + ((1 - w) * q_net_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        current_action_1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[current_action_1]
        
        state_idx = int(state[trial])

        # --- Stage 2 Choice ---
        # Purely model-free at the second stage (no further transitions)
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        current_action_2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[current_action_2]

        # --- Learning ---
        # Update Stage 2 (Terminal) Values
        pe_2 = reward[trial] - q_stage2[state_idx, current_action_2]
        q_stage2[state_idx, current_action_2] += learning_rate * pe_2
        
        # Update Stage 1 MF Values (TD(1) / SARSA-like update)
        # Note: In standard 2-step models, Stage 1 MF is often updated via eligibility traces 
        # or direct TD from stage 2 value. Here we use TD(0) from stage 2 value.
        value_state_2 = q_stage2[state_idx, current_action_2]
        pe_1 = value_state_2 - q_stage1_mf[current_action_1]
        q_stage1_mf[current_action_1] += learning_rate * pe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Enhanced Loss Sensitivity
This model posits that high OCI creates a hypersensitivity to negative outcomes (or lack of reward). Instead of a single learning rate, the model splits learning into positive (`alpha_pos`) and negative (`alpha_neg`) components. The OCI score specifically amplifies the `alpha_neg` parameter, meaning high-OCI participants react more strongly to omission of rewards, potentially leading to faster switching or "unlearning" of behaviors.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Enhanced Loss Sensitivity (Asymmetric Learning Rates).
    
    This model hypothesizes that high OCI makes individuals hypersensitive to 
    negative outcomes (reward omission). The learning rate for negative prediction 
    errors (alpha_neg) is boosted by the OCI score.
    
    Parameters:
    alpha_pos: [0,1] - Learning rate for positive prediction errors (Reward=1).
    alpha_neg_base: [0,1] - Base learning rate for negative prediction errors (Reward=0).
    beta: [0,10] - Inverse temperature.
    loss_sensitivity_boost: [0,2] - How much OCI increases alpha_neg.
    """
    alpha_pos, alpha_neg_base, beta, loss_sensitivity_boost = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective negative learning rate
    # High OCI increases the rate at which value is decreased after no reward.
    # We clip to 1.0 to maintain valid bounds.
    alpha_neg = min(1.0, alpha_neg_base + (loss_sensitivity_boost * oci_score))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        current_action_1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[current_action_1]
        
        state_idx = int(state[trial])

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        current_action_2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[current_action_2]

        # --- Learning ---
        
        # Stage 2 Update
        # Calculate Prediction Error
        pe_2 = reward[trial] - q_stage2[state_idx, current_action_2]
        
        # Select Learning Rate based on sign of PE
        lr_2 = alpha_pos if pe_2 >= 0 else alpha_neg
        
        q_stage2[state_idx, current_action_2] += lr_2 * pe_2

        # Stage 1 Update
        # We use the updated value of stage 2 as the target
        target_val = q_stage2[state_idx, current_action_2]
        pe_1 = target_val - q_stage1[current_action_1]
        
        # Select Learning Rate based on sign of PE
        lr_1 = alpha_pos if pe_1 >= 0 else alpha_neg
        
        q_stage1[current_action_1] += lr_1 * pe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```