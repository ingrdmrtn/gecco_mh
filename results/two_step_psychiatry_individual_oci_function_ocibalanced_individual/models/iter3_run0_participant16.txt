Here are three new cognitive models exploring different mechanisms of how Obsessive-Compulsive Inventory (OCI) scores might influence decision-making in a two-step task.

### Model 1: OCI-Modulated Second-Stage Learning Rate Asymmetry
This model hypothesizes that individuals with higher OCI scores might learn differently from positive versus negative outcomes at the second stage (the direct interaction with aliens). High OCI is often associated with perfectionism or fear of negative evaluation. This model tests if OCI amplifies learning from negative prediction errors (punishment sensitivity) relative to positive ones specifically in the model-free system.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Asymmetric Learning Rates.
    
    This model posits that OCI specifically modulates the learning rate for 
    negative prediction errors (disappointments) in the second stage.
    High OCI might lead to over-correction after receiving 0 coins.
    
    Parameters:
    lr_pos: [0, 1] Learning rate for positive prediction errors (reward).
    lr_neg_base: [0, 1] Base learning rate for negative prediction errors (omission).
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Mixing weight (MB vs MF).
    oci_neg_mod: [0, 1] How much OCI increases the negative learning rate.
    """
    lr_pos, lr_neg_base, beta, w, oci_neg_mod = model_parameters
    n_trials = len(action_1)
    oci = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    # Calculate individual negative learning rate based on OCI
    # We clip to ensure it stays within [0, 1]
    lr_neg = np.clip(lr_neg_base + (oci * oci_neg_mod), 0.0, 1.0)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # --- Updates ---
        # Stage 2 RPE
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        
        # Apply asymmetric learning rates
        current_lr = lr_pos if delta_stage2 >= 0 else lr_neg
        q_stage2_mf[state_idx, int(action_2[trial])] += current_lr * delta_stage2

        # Stage 1 Update (SARSA-style TD(0) for MF)
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += current_lr * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Driven Choice Stickiness (Perseveration)
This model investigates whether OCI is related to behavioral rigidity or "stickiness." Instead of modifying how values are learned, this model modifies how choices are made. It adds a perseveration bonus to the action chosen on the previous trial. The magnitude of this bonus is scaled by the participant's OCI score, testing the hypothesis that higher compulsivity leads to repetitive behavior regardless of reward outcomes.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Driven Choice Stickiness.
    
    This model adds a perseveration parameter (stickiness) to the choice softmax.
    The strength of this stickiness is modulated by OCI.
    High OCI subjects may be more likely to repeat the previous Stage 1 choice
    regardless of the model-based or model-free value.
    
    Parameters:
    lr: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Mixing weight.
    stick_base: [0, 5] Base level of choice stickiness (perseveration).
    oci_stick_mod: [0, 5] Modulation of stickiness by OCI.
    """
    lr, beta, w, stick_base, oci_stick_mod = model_parameters
    n_trials = len(action_1)
    oci = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    # Calculate stickiness parameter
    stickiness = stick_base + (oci * oci_stick_mod)
    
    # Track previous choice (initialized to -1 so it doesn't affect trial 0)
    prev_choice = -1

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness bonus to the Q-value of the previous action
        logits = beta * q_net
        if prev_choice != -1:
            logits[prev_choice] += stickiness
            
        exp_q1 = np.exp(logits)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # --- Updates ---
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += lr * delta_stage2

        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += lr * delta_stage1
        
        # Update previous choice for next trial
        prev_choice = int(action_1[trial])

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Transition Learning Rate (State Inference)
Standard models assume the transition matrix (0.7/0.3) is fixed and known. However, anxious or obsessive individuals might constantly monitor for changes in the environment (uncertainty intolerance). This model assumes the participant is learning the transition probabilities (Model-Based learning) rather than treating them as static, and that OCI modulates how quickly they update their belief about these transitions. High OCI might lead to "over-updating" the internal model of the world based on recent rare transitions.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Transition Learning (State Inference).
    
    This model assumes the agent learns the transition matrix dynamically.
    OCI modulates the learning rate of the transition matrix (lr_trans).
    High OCI might imply hyper-sensitivity to rare transitions, causing the 
    agent to rapidly change their model of the spaceship-planet structure.
    
    Parameters:
    lr_reward: [0, 1] Learning rate for reward values (Q-values).
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Mixing weight.
    lr_trans_base: [0, 1] Base learning rate for transition probabilities.
    oci_trans_mod: [0, 1] Modulation of transition learning rate by OCI.
    """
    lr_reward, beta, w, lr_trans_base, oci_trans_mod = model_parameters
    n_trials = len(action_1)
    oci = oci[0]
  
    # Initialize transition beliefs (start with uniform or slight prior)
    # Rows: Action 1 (Spaceship), Cols: State (Planet)
    # We track raw counts/strengths to normalize later
    trans_counts = np.array([[0.5, 0.5], [0.5, 0.5]]) 
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    # Calculate transition learning rate
    lr_trans = np.clip(lr_trans_base + (oci * oci_trans_mod), 0.0, 1.0)

    for trial in range(n_trials):
        # Normalize counts to get probabilities
        row_sums = trans_counts.sum(axis=1, keepdims=True)
        estimated_transitions = trans_counts / row_sums

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        # Use Dynamic estimated transitions instead of fixed 0.7/0.3
        q_stage1_mb = estimated_transitions @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # --- Observe State Transition ---
        chosen_ship = int(action_1[trial])
        arrived_planet = int(state[trial])
        
        # Update Transition Beliefs
        # We increase the weight of the observed transition and decay the others
        # This is equivalent to a delta rule on the probabilities
        # 1. Create a target vector (one-hot for the arrived planet)
        target = np.zeros(2)
        target[arrived_planet] = 1.0
        
        # 2. Update the row corresponding to the chosen spaceship
        # We update the counts effectively by moving the probability distribution
        # Note: To keep it simple and within the function constraints, we just update the belief matrix directly
        # using a simple delta rule approximation on the normalized matrix logic
        curr_probs = estimated_transitions[chosen_ship]
        new_probs = curr_probs + lr_trans * (target - curr_probs)
        
        # Update the counts to reflect these new probs (maintaining row sum of 1 for simplicity in logic)
        trans_counts[chosen_ship] = new_probs

        # --- Stage 2 Policy ---
        qs_current_state = q_stage2_mf[arrived_planet]
        exp_q2 = np.exp(beta * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # --- Value Updates ---
        delta_stage2 = reward[trial] - q_stage2_mf[arrived_planet, int(action_2[trial])]
        q_stage2_mf[arrived_planet, int(action_2[trial])] += lr_reward * delta_stage2

        delta_stage1 = q_stage2_mf[arrived_planet, int(action_2[trial])] - q_stage1_mf[chosen_ship]
        q_stage1_mf[chosen_ship] += lr_reward * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```