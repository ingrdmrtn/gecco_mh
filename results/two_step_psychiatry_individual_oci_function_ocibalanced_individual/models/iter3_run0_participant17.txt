Here are three new cognitive models that explore different mechanisms by which OCI scores might influence decision-making in the two-step task, distinct from the feedback provided.

### Model 1: Hybrid Learner with OCI-modulated Model-Based Weighting
This model tests the hypothesis that higher OCI symptoms are associated with a deficit in goal-directed (model-based) control, leading to a stronger reliance on habitual (model-free) systems. It modulates the mixing parameter `w` based on the OCI score.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: Hybrid Learner with OCI-modulated Model-Based Weighting.
    
    Hypothesis: Higher OCI scores correlate with reduced model-based control (goal-directedness).
    The weighting parameter 'w' (0 = pure model-free, 1 = pure model-based) is 
    calculated as a logistic function of OCI, allowing the balance to shift.
    
    Parameters:
    - learning_rate: [0, 1] Value update rate.
    - beta: [0, 10] Inverse temperature (softness of choice).
    - w_intercept: [-5, 5] Base log-odds of using model-based planning.
    - w_oci_slope: [-5, 5] Effect of OCI on the probability of using model-based planning.
    """
    learning_rate, beta, w_intercept, w_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate mixing weight w using a sigmoid function to keep it in [0, 1]
    # If slope is negative, higher OCI reduces w (more Model-Free)
    w_logit = w_intercept + (w_oci_slope * oci_score)
    w = 1.0 / (1.0 + np.exp(-w_logit))
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based Value: V_MB(s1) = T * max(Q(s2))
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value: Q_net = w * Q_MB + (1-w) * Q_MF
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Choice ---
        s_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        r = reward[trial]
        
        # Stage 2 Update (Model-Free)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 Update (Model-Free TD(0))
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Asymmetric Learning Rates with OCI-driven Punishment Sensitivity
This model hypothesizes that OCI is related to an increased sensitivity to negative outcomes (or lack of reward) specifically during the update process. It splits the learning rate into positive and negative components, where the negative learning rate is boosted by the OCI score.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: Model-Free Learner with OCI-driven Punishment Sensitivity.
    
    Hypothesis: High OCI individuals over-learn from "punishments" (zero reward).
    The learning rate for negative prediction errors (alpha_neg) is scaled 
    relative to the base positive learning rate by the OCI score.
    
    Parameters:
    - alpha_pos: [0, 1] Learning rate for positive prediction errors (reward > expectation).
    - alpha_neg_base: [0, 1] Baseline learning rate for negative prediction errors.
    - beta: [0, 10] Inverse temperature.
    - oci_neg_scaling: [0, 5] Multiplier for how much OCI increases alpha_neg.
    """
    alpha_pos, alpha_neg_base, beta, oci_neg_scaling = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective negative learning rate, capped at 1.0
    alpha_neg_eff = min(1.0, alpha_neg_base + (oci_neg_scaling * oci_score))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Choice ---
        s_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        r = reward[trial]
        
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        lr2 = alpha_pos if delta_stage2 > 0 else alpha_neg_eff
        q_stage2_mf[s_idx, a2] += lr2 * delta_stage2
        
        # Stage 1 Update
        # Note: Using Q-value of chosen stage 2 state as target (SARSA-like connection)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        lr1 = alpha_pos if delta_stage1 > 0 else alpha_neg_eff
        q_stage1_mf[a1] += lr1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Exploration (Inverse Temperature)
This model posits that OCI affects the exploration-exploitation trade-off. Specifically, it tests if higher OCI leads to more deterministic (rigid) behavior by increasing the inverse temperature `beta`, thereby reducing random exploration.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: Model-Based Learner with OCI-Modulated Inverse Temperature.
    
    Hypothesis: Higher OCI scores lead to more rigid, deterministic choices 
    (reduced exploration). The inverse temperature (beta) increases linearly 
    with OCI score.
    
    Parameters:
    - learning_rate: [0, 1] Update rate.
    - beta_base: [0, 10] Baseline inverse temperature.
    - beta_oci_slope: [-5, 5] Effect of OCI on beta (positive = more rigid).
    - w: [0, 1] Fixed weighting parameter for model-based vs model-free.
    """
    learning_rate, beta_base, beta_oci_slope, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective beta, ensuring it stays non-negative
    beta_eff = max(0.0, beta_base + (beta_oci_slope * oci_score))
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta_eff * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Choice ---
        s_idx = int(state[trial])
        exp_q2 = np.exp(beta_eff * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        r = reward[trial]
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```