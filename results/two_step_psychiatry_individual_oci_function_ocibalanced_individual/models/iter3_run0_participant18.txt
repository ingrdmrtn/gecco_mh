Here are three new cognitive models that explore different mechanisms for how OCI scores might influence decision-making in the two-step task, specifically focusing on the balance between model-based and model-free control, learning rates, and exploration.

### Model 1: Hybrid Model with OCI-Modulated Model-Based Weighting
This model tests the hypothesis that higher OCI scores relate to a deficit in goal-directed (model-based) control, leading to a reliance on habitual (model-free) systems. The weighting parameter `w` (mixing fraction) is dynamically adjusted by the OCI score.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free RL with OCI-modulated Model-Based Weighting.
    
    This model assumes behavior is a mix of Model-Based (MB) and Model-Free (MF) strategies.
    The mixing weight 'w' is linear function of OCI.
    Hypothesis: Higher OCI might lead to reduced MB control (lower w).
    
    Parameters:
    - learning_rate: Rate at which Q-values are updated.
    - beta: Inverse temperature for softmax.
    - w_intercept: Baseline weighting for Model-Based control (at OCI=0).
    - w_slope: How OCI score changes the MB weighting.
    
    Bounds:
    learning_rate: [0,1]
    beta: [0,10]
    w_intercept: [0,1]
    w_slope: [-1,1] (Can be negative if OCI reduces MB control)
    """
    learning_rate, beta, w_intercept, w_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective w, bounded between 0 and 1
    w_eff = w_intercept + (w_slope * oci_score)
    w_eff = np.clip(w_eff, 0.0, 1.0)

    # Fixed transition matrix as per task description
    # A -> X (0.7), U -> Y (0.7) usually implies:
    # Action 0 (A) -> State 0 (X) with 0.7? Or Action 0 -> State 1?
    # Usually: Action 0 -> State 1 (70%), Action 1 -> State 2 (70%) in standard tasks.
    # Here, let's assume indices: Action 0 -> State 0 (0.7), Action 1 -> State 1 (0.7).
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        curr_state = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        # Model-Based Value: Q_MB(s1, a1) = Sum(P(s2|s1,a1) * max_a2 Q_MF(s2, a2))
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value: Q_net = w * Q_MB + (1-w) * Q_MF
        q_net = w_eff * q_stage1_mb + (1 - w_eff) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        # Standard Model-Free choice at second stage
        exp_q2 = np.exp(beta * q_stage2_mf[curr_state, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Update Stage 1 MF Q-value (TD(0))
        # Note: In full hybrid models, sometimes eligibility traces (lambda) are used. 
        # Here we use simple TD(0) for MF component as per template structure.
        delta_stage1 = q_stage2_mf[curr_state, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Update Stage 2 MF Q-value
        delta_stage2 = r - q_stage2_mf[curr_state, a2]
        q_stage2_mf[curr_state, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Dual Learning Rate Model (OCI-Modulated Negative Learning)
This model investigates if OCI is associated with an asymmetry in how positive and negative prediction errors are processed. Specifically, it tests if higher OCI leads to hyper-sensitivity to negative outcomes (punishment/lack of reward), effectively creating a different learning rate for negative prediction errors.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free RL with OCI-modulated Asymmetric Learning Rates.
    
    This model posits that OCI affects sensitivity to negative prediction errors.
    Positive errors use a base learning rate.
    Negative errors use a rate modified by OCI.
    
    Parameters:
    - alpha_pos: Learning rate for positive prediction errors (RPE > 0).
    - alpha_neg_base: Base learning rate for negative prediction errors (RPE < 0).
    - alpha_neg_oci_scale: Scaling factor for OCI's effect on negative learning rate.
    - beta: Inverse temperature.
    
    Bounds:
    alpha_pos: [0,1]
    alpha_neg_base: [0,1]
    alpha_neg_oci_scale: [0,1]
    beta: [0,10]
    """
    alpha_pos, alpha_neg_base, alpha_neg_oci_scale, beta = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate OCI-dependent negative learning rate
    # We clip to ensure it stays valid [0, 1]
    alpha_neg_eff = alpha_neg_base + (alpha_neg_oci_scale * oci_score)
    alpha_neg_eff = np.clip(alpha_neg_eff, 0.0, 1.0)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        curr_state = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[curr_state, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[curr_state, a2]
        if delta_stage2 >= 0:
            eff_alpha = alpha_pos
        else:
            eff_alpha = alpha_neg_eff
        q_stage2_mf[curr_state, a2] += eff_alpha * delta_stage2
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[curr_state, a2] - q_stage1_mf[a1]
        if delta_stage1 >= 0:
            eff_alpha = alpha_pos
        else:
            eff_alpha = alpha_neg_eff
        q_stage1_mf[a1] += eff_alpha * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Randomness (Inverse Temperature)
This model explores the idea that OCI might relate to the "determinism" of choices. It modifies the softmax inverse temperature ($\beta$) based on the OCI score. A high OCI might lead to more rigid/deterministic exploitation (higher $\beta$) or more anxious/uncertain exploration (lower $\beta$).

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free RL with OCI-modulated Inverse Temperature (Beta).
    
    This model assumes the randomness of choice (exploration vs exploitation) 
    is a function of the OCI score.
    
    Parameters:
    - learning_rate: Rate at which Q-values are updated.
    - beta_base: Baseline inverse temperature.
    - beta_oci_slope: How OCI score scales the beta parameter.
    - stickiness: Choice perseveration bonus (constant).
    
    Bounds:
    learning_rate: [0,1]
    beta_base: [0,10]
    beta_oci_slope: [-5,5] (Allows OCI to increase or decrease determinism)
    stickiness: [0,5]
    """
    learning_rate, beta_base, beta_oci_slope, stickiness = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective beta
    # Ensure beta stays non-negative.
    beta_eff = beta_base + (beta_oci_slope * oci_score)
    beta_eff = np.maximum(beta_eff, 0.0)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        curr_state = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        q_stage1_total = q_stage1_mf.copy()
        if last_action_1 != -1:
            q_stage1_total[last_action_1] += stickiness

        exp_q1 = np.exp(beta_eff * q_stage1_total)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        # We apply the same beta modulation to stage 2
        exp_q2 = np.exp(beta_eff * q_stage2_mf[curr_state, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[curr_state, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[curr_state, a2]
        q_stage2_mf[curr_state, a2] += learning_rate * delta_stage2
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```