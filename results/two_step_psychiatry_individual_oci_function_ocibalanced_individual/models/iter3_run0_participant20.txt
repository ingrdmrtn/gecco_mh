Here are three new cognitive models for the two-step task, incorporating OCI scores into the decision-making process.

### Model 1: OCI-Modulated Transition Uncertainty
This model hypothesizes that individuals with higher OCI scores have reduced confidence in the "common" transition structure of the task. While the true common transition probability is 0.7, high OCI scores flatten this belief towards 0.5 (randomness), reflecting an intolerance of uncertainty or a distrust of the stable structure. This affects the Model-Based value calculation.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL with OCI-modulated transition belief (Uncertainty).
    
    Hypothesis: Higher OCI scores lead to a 'flattening' of the internal transition 
    matrix. High OCI participants may distrust the stability of the 70/30 structure, 
    behaving as if the transitions are more random (closer to 50/50).
    
    Parameters:
    learning_rate: [0, 1] - Learning rate for value updates.
    beta: [0, 10] - Inverse temperature (exploration/exploitation).
    w: [0, 1] - Weighting between Model-Based and Model-Free values.
    uncertainty_oci: [0, 0.5] - How much OCI reduces the perceived probability of common transitions.
                               Max reduction of 0.4 would turn 0.7 into 0.3 (flipping), 
                               so we bound the effect to flatten towards 0.5.
    """
    learning_rate, beta, w, uncertainty_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Modulate transition matrix based on OCI
    # Standard common prob is 0.7. OCI reduces this belief.
    # We clip to ensure it doesn't go below 0.5 (pure uncertainty) or flip structure excessively.
    perceived_common_prob = 0.7 - (uncertainty_oci * oci_score)
    perceived_common_prob = np.maximum(0.5, perceived_common_prob) 
    
    # Construct the subjective transition matrix
    # T[0,0] is A->X, T[0,1] is A->Y, etc.
    transition_matrix = np.array([
        [perceived_common_prob, 1 - perceived_common_prob], 
        [1 - perceived_common_prob, perceived_common_prob]
    ])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based Value
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])

        # --- Stage 2 Policy ---
        qs_stage2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_stage2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Stage 1 MF Update (SARSA-style using Stage 2 Q-value)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

        # Stage 2 MF Update (Reward Prediction Error)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Loss Aversion
This model hypothesizes that high OCI is associated with perfectionism or fear of failure. Consequently, receiving 0 coins (a "loss" or omission of reward) is not treated as neutral (value 0) but as a punishment (negative value). The magnitude of this negative valuation scales with the OCI score.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL with OCI-modulated Loss Aversion.
    
    Hypothesis: High OCI participants perceive the lack of reward (0 coins) 
    as an active punishment rather than a neutral outcome. This 'loss aversion' 
    or 'fear of failure' drives learning more strongly than in low OCI participants.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - MB/MF weighting.
    loss_sensitivity_oci: [0, 5] - Scales the negative utility of getting 0 coins based on OCI.
    """
    learning_rate, beta, w, loss_sensitivity_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate the subjective penalty for a "miss" based on OCI
    loss_penalty = loss_sensitivity_oci * oci_score

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])

        # --- Stage 2 Policy ---
        qs_stage2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_stage2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Determine Effective Reward ---
        # If reward is 0, treat it as negative based on OCI
        r_actual = reward[trial]
        if r_actual == 0:
            r_effective = -1.0 * loss_penalty
        else:
            r_effective = r_actual

        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

        # Update Stage 2 using the OCI-modified effective reward
        delta_stage2 = r_effective - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Stage 1 Precision
This model separates the exploration/exploitation parameter (`beta`) for the two stages. It hypothesizes that OCI specifically impacts decision noise at the high-level planning stage (Stage 1), making it distinct from the base decision noise at the outcome stage (Stage 2).

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL with OCI-modulated Stage 1 Inverse Temperature.
    
    Hypothesis: OCI affects the consistency of choices differently at the planning stage 
    (Spaceship choice) versus the terminal stage (Alien choice). High OCI might lead to 
    more rigid (higher beta) or more erratic (lower beta) planning choices compared to baseline.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate.
    w: [0, 1] - MB/MF weighting.
    beta_base: [0, 10] - Baseline inverse temperature (used for Stage 2).
    beta_1_oci_mod: [-5, 5] - Modifier for Stage 1 beta based on OCI. 
                              Stage 1 Beta = beta_base + (mod * oci).
    """
    learning_rate, w, beta_base, beta_1_oci_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate stage-specific betas
    # We ensure beta doesn't drop below 0
    beta_stage2 = beta_base
    beta_stage1 = max(0.0, beta_base + (beta_1_oci_mod * oci_score))

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy (Uses OCI-modulated Beta) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_stage1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])

        # --- Stage 2 Policy (Uses Base Beta) ---
        qs_stage2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta_stage2 * qs_stage2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```