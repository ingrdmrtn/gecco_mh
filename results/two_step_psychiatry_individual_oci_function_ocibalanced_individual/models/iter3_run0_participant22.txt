Here are 3 new cognitive models expressed as Python functions. These models explore different mechanistic hypotheses about how OCI symptoms might alter decision-making in a two-step task, specifically focusing on mixing model-based/model-free strategies, learning rates for rare transitions, and forgetting rates.

### Model 1: Hybrid Model with OCI-Modulated Model-Based Weighting
This model hypothesizes that high OCI scores might correlate with a rigid adherence to a "habitual" (model-free) system or, conversely, an over-reliance on a "planning" (model-based) system. Here, we test the hypothesis that OCI modulates the balance parameter $w$ between Model-Based (MB) and Model-Free (MF) control. A higher $w$ indicates more model-based planning.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF Model where OCI modulates the weighting parameter 'w'.
    
    Hypothesis: OCI scores influence the trade-off between goal-directed (Model-Based)
    and habitual (Model-Free) control.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for Q-value updates.
    - beta: [0, 10] Inverse temperature for softmax.
    - w_intercept: [0, 1] Base weighting for Model-Based control (0=Pure MF, 1=Pure MB).
    - w_oci_slope: [-1, 1] How OCI score shifts the weighting. 
      (Positive = OCI increases MB planning; Negative = OCI increases MF habits).
      
    Bounds:
    learning_rate: [0,1]
    beta: [0,10]
    w_intercept: [0,1]
    w_oci_slope: [-1,1]
    """
    learning_rate, beta, w_intercept, w_oci_slope = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate effective w, clamped between 0 and 1
    w = w_intercept + (current_oci * w_oci_slope)
    w = np.clip(w, 0.0, 1.0)

    # Fixed transition matrix as per task description
    # A(0) -> X(0) is 0.7, A(0) -> Y(1) is 0.3
    # U(1) -> X(0) is 0.3, U(1) -> Y(1) is 0.7
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2) # Model-free Q-values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Q-values for stage 2 (2 planets, 2 aliens)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based value calculation: T * max(Q2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid value: combination of MB and MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Updates ---
        # TD(0) update for Stage 2
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # TD(1) / SARSA-like update for Stage 1 Model-Free value
        # Note: In standard 2-step models, stage 1 MF is often updated via 
        # the stage 2 value (TD error) or the reward directly (eligibility traces).
        # Here we use the standard TD update using the value of the state reached.
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: "Surprise Sensitivity" Model
This model posits that OCI symptoms relate to how individuals handle "rare" or surprising transition events. Individuals with high OCI might over-react (or under-react) to prediction errors specifically when the transition structure is violated (rare transitions), potentially driving anxiety-driven updates.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free RL with OCI-modulated Surprise Sensitivity.
    
    Hypothesis: OCI affects learning rates specifically after rare transitions.
    High OCI might lead to higher learning rates when the world model is violated
    (rare transition), representing hyper-vigilance to error.
    
    Parameters:
    - lr_common: [0, 1] Learning rate after common transitions.
    - beta: [0, 10] Inverse temperature.
    - surprise_boost_base: [0, 1] Base boost to LR for rare transitions.
    - oci_surprise_mod: [0, 2] How much OCI scales the surprise boost.
    
    Bounds:
    lr_common: [0,1]
    beta: [0,10]
    surprise_boost_base: [0,1]
    oci_surprise_mod: [0,2]
    """
    lr_common, beta, surprise_boost_base, oci_surprise_mod = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate learning rate for rare transitions based on OCI
    # We ensure it doesn't exceed 1.0
    boost = surprise_boost_base + (current_oci * oci_surprise_mod)
    lr_rare = np.clip(lr_common + boost, 0.0, 1.0)
    
    # Transition definitions to detect surprise
    # A(0) -> X(0) common, U(1) -> Y(1) common. Everything else rare.
    # Common pairs: (0,0) and (1,1). Rare pairs: (0,1) and (1,0).
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        
        # Determine if transition was common or rare
        is_common = (a1 == s_idx)
        current_lr = lr_common if is_common else lr_rare

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Updates ---
        # Update Stage 2
        delta2 = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += current_lr * delta2
        
        # Update Stage 1
        # Using the value of the chosen stage 2 state to update stage 1
        delta1 = q_stage2[s_idx, a2] - q_stage1[a1]
        q_stage1[a1] += current_lr * delta1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Decay/Forgetting Model
This model investigates if OCI is associated with memory persistence. It introduces a decay parameter that pulls unchosen Q-values back to zero (forgetting). The hypothesis is that high OCI might be associated with lower forgetting (rumination/persistence) or higher forgetting (cognitive interference).

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Q-Learning with OCI-modulated Memory Decay.
    
    Hypothesis: OCI relates to the persistence of value representations. 
    Unchosen actions decay toward 0. OCI modulates the rate of this decay.
    
    Parameters:
    - learning_rate: [0, 1] Standard learning rate.
    - beta: [0, 10] Inverse temperature.
    - decay_base: [0, 1] Base decay rate for unchosen options.
    - oci_decay_slope: [-0.5, 0.5] OCI modulation of decay.
    
    Bounds:
    learning_rate: [0,1]
    beta: [0,10]
    decay_base: [0,1]
    oci_decay_slope: [-0.5,0.5]
    """
    learning_rate, beta, decay_base, oci_decay_slope = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate effective decay rate
    eff_decay = decay_base + (current_oci * oci_decay_slope)
    eff_decay = np.clip(eff_decay, 0.0, 1.0)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Updates ---
        # Update chosen Stage 2
        delta2 = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += learning_rate * delta2
        
        # Decay unchosen Stage 2 in the current state
        unchosen_a2 = 1 - a2
        q_stage2[s_idx, unchosen_a2] *= (1 - eff_decay)
        
        # Update chosen Stage 1
        delta1 = q_stage2[s_idx, a2] - q_stage1[a1]
        q_stage1[a1] += learning_rate * delta1
        
        # Decay unchosen Stage 1
        unchosen_a1 = 1 - a1
        q_stage1[unchosen_a1] *= (1 - eff_decay)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```