Here are three new cognitive models that explore different mechanisms by which Obsessive-Compulsive Inventory (OCI) scores might influence decision-making in the two-step task, specifically focusing on how high OCI (like this participant's 0.83) might relate to distinct learning or choice biases.

### Model 1: Hybrid Learner with OCI-modulated Model-Based Weight (`w`)
This model hypothesizes that OCI scores directly influence the balance between Model-Based (planning) and Model-Free (habitual) control. High OCI is often associated with reliance on habits (Model-Free) over flexible goal-directed planning (Model-Based). Here, the mixing parameter `w` is a function of OCI.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: Hybrid Learner with OCI-modulated Model-Based Weight.
    
    Hypothesis: High OCI scores are associated with a shift away from goal-directed 
    (Model-Based) control towards habitual (Model-Free) control.
    The mixing weight 'w' is determined by a baseline and an OCI-dependent slope.
    
    Parameters:
    learning_rate: [0, 1] Update rate for Q-values.
    beta: [0, 10] Inverse temperature for softmax choice.
    w_base: [0, 1] Baseline weight for Model-Based control (at OCI=0).
    w_oci_mod: [-1, 1] Modulation of 'w' by OCI score. 
                 (w = w_base + w_oci_mod * oci, clipped to [0,1]).
    """
    learning_rate, beta, w_base, w_oci_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate mixing weight w based on OCI
    w = w_base + (w_oci_mod * oci_score)
    # Ensure w stays within valid [0, 1] bounds
    if w > 1.0: w = 1.0
    if w < 0.0: w = 0.0

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Q-values initialization
    q_stage1_mf = np.zeros(2)      # Model-free values for first stage
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for second stage (aliens)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based value calculation: Transition * Max(Stage 2 Values)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid value: weighted sum of MB and MF
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial]) # The planet reached

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # --- Learning ---
        # Update Stage 1 MF value (TD(0) - simple update based on Stage 2 value)
        # Note: In full models, eligibility traces are often used, but here we use a simple TD structure
        # Using the value of the chosen stage 2 state as the target for stage 1
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1
        
        # Update Stage 2 MF value based on reward
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Loss Aversion Learner with OCI Modulation
This model posits that OCI relates to anxiety and sensitivity to negative outcomes (or lack of reward). Instead of a single learning rate, the model uses separate learning rates for positive prediction errors (better than expected) and negative prediction errors (worse than expected). The "negative" learning rate is modulated by OCI, testing if high OCI leads to over-learning from failures.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: Dual Learning Rate (Loss Aversion) with OCI Modulation.
    
    Hypothesis: OCI scores relate to sensitivity to negative outcomes.
    This model separates learning rates for positive (alpha_pos) and negative (alpha_neg)
    prediction errors. The negative learning rate is modulated by OCI.
    
    Parameters:
    alpha_pos: [0, 1] Learning rate for positive prediction errors.
    alpha_neg_base: [0, 1] Baseline learning rate for negative prediction errors.
    alpha_neg_oci_sens: [-1, 1] How much OCI affects negative learning rate.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Fixed Model-Based weight.
    """
    alpha_pos, alpha_neg_base, alpha_neg_oci_sens, beta, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate OCI-modulated negative learning rate
    alpha_neg = alpha_neg_base + (alpha_neg_oci_sens * oci_score)
    if alpha_neg > 1.0: alpha_neg = 1.0
    if alpha_neg < 0.0: alpha_neg = 0.0
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # --- Learning ---
        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        if delta_stage2 >= 0:
            eff_alpha = alpha_pos
        else:
            eff_alpha = alpha_neg
            
        q_stage2_mf[state_idx, int(action_2[trial])] += eff_alpha * delta_stage2
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        if delta_stage1 >= 0:
            eff_alpha_s1 = alpha_pos
        else:
            eff_alpha_s1 = alpha_neg
            
        q_stage1_mf[int(action_1[trial])] += eff_alpha_s1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Uncertainty-Driven Exploration with OCI
This model investigates if OCI affects how uncertainty guides exploration. High OCI might lead to intolerance of uncertainty (avoiding unknown options) or checking behavior (exploring them). An "uncertainty bonus" is added to the Q-values, calculated as `1 - (number of times chosen / total trials)`, scaled by an exploration parameter that depends on OCI.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: Uncertainty Bonus with OCI Modulation.
    
    Hypothesis: OCI affects tolerance of uncertainty. This model adds an exploration 
    bonus to actions that have been chosen less frequently. The magnitude of this 
    bonus is modulated by OCI.
    
    Parameters:
    learning_rate: [0, 1] Update rate.
    beta: [0, 10] Inverse temperature.
    phi_base: [-1, 1] Baseline exploration bonus weight (positive = seek novelty, negative = avoid).
    phi_oci_sens: [-1, 1] OCI sensitivity of exploration bonus.
    w: [0, 1] Model-Based weight.
    """
    learning_rate, beta, phi_base, phi_oci_sens, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate exploration parameter phi
    phi = phi_base + (phi_oci_sens * oci_score)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Counts for calculating uncertainty (inverse frequency)
    counts_stage1 = np.zeros(2) 
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Calculate uncertainty bonus for Stage 1
        # Simple heuristic: 1 / (sqrt(count) + 1)
        # This decays as options are sampled more
        uncertainty_bonus = 1.0 / (np.sqrt(counts_stage1) + 1.0)
        
        # Add bonus to Q-values
        q_net_augmented = q_net + (phi * uncertainty_bonus)
        
        exp_q1 = np.exp(beta * q_net_augmented)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # Update counts
        counts_stage1[int(action_1[trial])] += 1
        state_idx = int(state[trial])

        # --- Stage 2 Choice ---
        # (Standard softmax for stage 2 to keep model complexity manageable)
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # --- Learning ---
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```