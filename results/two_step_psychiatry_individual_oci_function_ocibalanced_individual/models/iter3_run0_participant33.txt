Here are three new cognitive models for the two-step task, incorporating OCI scores into different mechanisms of learning and valuation.

### Model 1: OCI-modulated Memory Decay
This model hypothesizes that OCI symptoms interfere with the maintenance of value representations for unchosen options. It introduces a decay parameter $\gamma$ for unchosen actions, where the rate of decay is modulated by the participant's OCI score. This tests if OCI is associated with "obsessive" retention of old values (low decay) or rapid forgetting/instability (high decay).

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-modulated Memory Decay (Unchosen Value Forgetting).
    
    This model posits that OCI symptoms affect the ability to maintain value representations 
    for unchosen options. On each trial, the Q-values of actions NOT chosen decay towards 0.
    The rate of this decay is modulated by the OCI score.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for chosen options.
    - beta: [0, 10] Inverse temperature (softmax).
    - w: [0, 1] Mixing weight (0=MF, 1=MB).
    - decay_base: [0, 1] Base decay rate (1.0 = no decay, 0.0 = instant forgetting).
    - oci_decay_slope: [-1, 1] How OCI affects decay. 
      gamma = decay_base + (slope * oci). Clipped to [0,1].
    """
    learning_rate, beta, w, decay_base, oci_decay_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    # Calculate effective decay rate
    decay_rate = decay_base + (oci_decay_slope * oci_score)
    decay_rate = np.clip(decay_rate, 0.0, 1.0)

    # Fixed transition matrix for MB calculation
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2) 
    q_stage2_mf = np.zeros((2, 2)) # State x Action

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # Model-Based Value: T * max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax Choice 1
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Decision ---
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning & Decay ---
        r = reward[trial]
        
        # Update Stage 2 Chosen
        q_stage2_mf[state_idx, a2] += learning_rate * (r - q_stage2_mf[state_idx, a2])
        # Decay Stage 2 Unchosen
        q_stage2_mf[state_idx, 1-a2] *= decay_rate
        # Decay Stage 2 Unvisited State (both actions)
        q_stage2_mf[1-state_idx, :] *= decay_rate

        # Update Stage 1 Chosen (TD-0 for simplicity in this hybrid setup)
        # Using the value of the state actually reached
        v_state_reached = np.max(q_stage2_mf[state_idx])
        q_stage1_mf[a1] += learning_rate * (v_state_reached - q_stage1_mf[a1])
        
        # Decay Stage 1 Unchosen
        q_stage1_mf[1-a1] *= decay_rate

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-modulated Transition Learning
Standard models assume participants know the 70/30 transition probabilities. This model assumes participants *learn* the transition matrix dynamically. It hypothesizes that OCI scores relate to the rigidity or flexibility of this structural learning. High OCI might correlate with a lower learning rate for structural changes (rigidity) or a higher one (hyper-sensitivity to rare transitions).

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-modulated Transition Learning Rate.
    
    This model assumes the agent learns the transition matrix (Action->Planet) over time, 
    rather than having it fixed. OCI scores modulate how quickly the participant updates 
    their internal model of the spaceship transitions.
    
    Parameters:
    - learning_rate: [0, 1] Reward value learning rate (for Q-values).
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight (0=MF, 1=MB).
    - lr_trans_base: [0, 1] Base learning rate for transition matrix updates.
    - oci_lr_trans_slope: [-1, 1] Modulation of transition learning rate by OCI.
      alpha_trans = base + (slope * oci).
    """
    learning_rate, beta, w, lr_trans_base, oci_lr_trans_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate transition learning rate
    alpha_trans = lr_trans_base + (oci_lr_trans_slope * oci_score)
    alpha_trans = np.clip(alpha_trans, 0.0, 1.0)
    
    # Initialize dynamic transition matrix (Action x State)
    # Starts uniform (uncertainty)
    trans_probs = np.array([[0.5, 0.5], [0.5, 0.5]]) 
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # MB Value uses current learned transition matrix
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = trans_probs @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Decision ---
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        r = reward[trial]
        
        # 1. Update Transition Matrix (State Prediction Error)
        # Increase prob of reached state for chosen action
        trans_probs[a1, state_idx] += alpha_trans * (1 - trans_probs[a1, state_idx])
        # Decrease prob of unreached state
        trans_probs[a1, 1-state_idx] += alpha_trans * (0 - trans_probs[a1, 1-state_idx])
        
        # 2. Update Q-values (MF)
        # Stage 2
        q_stage2_mf[state_idx, a2] += learning_rate * (r - q_stage2_mf[state_idx, a2])
        # Stage 1 (using simple TD)
        v_state_reached = np.max(q_stage2_mf[state_idx])
        q_stage1_mf[a1] += learning_rate * (v_state_reached - q_stage1_mf[a1])

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-modulated Subjective "Relief" Value
This model proposes that high OCI scores alter the perception of the "0 coin" outcome. Instead of being a neutral event, the absence of reward is experienced as a "punishment" or negative utility. This drives active avoidance of non-rewarded states more strongly than standard extinction.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-modulated Subjective "Relief" Value (Zero as Punishment).
    
    This model proposes that for high OCI participants, the absence of a reward (0 coins)
    is perceived not as neutral, but as a negative outcome (punishment). 
    This modifies the reward signal fed into the RL update.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight.
    - lambda_eligibility: [0, 1] Eligibility trace for Stage 1 updates.
    - oci_zero_punish: [0, 5] Scaling factor for negative subjective value of 0 outcome.
    
    If reward == 0: effective_reward = -1.0 * oci_zero_punish * oci_score
    If reward == 1: effective_reward = 1.0
    """
    learning_rate, beta, w, lambda_eligibility, oci_zero_punish = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 ---
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Reward Processing ---
        r_obs = reward[trial]
        
        # Calculate subjective reward
        if r_obs == 0:
            r_eff = -1.0 * oci_zero_punish * oci_score
        else:
            r_eff = 1.0
            
        # --- Updates (with Eligibility Trace) ---
        # Stage 2 PE
        delta_stage2 = r_eff - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 PE
        # Using the value of the state reached for the TD error
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1 + (learning_rate * lambda_eligibility * delta_stage2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```