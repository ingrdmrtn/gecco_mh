Here are 3 new cognitive models that incorporate the OCI score into the decision-making process in the two-step task.

### Model 1: Hybrid Model with OCI-Modulated Model-Based Weight
This model assumes that individuals use a mixture of Model-Free (MF) and Model-Based (MB) strategies. The core hypothesis here is that high OCI (compulsivity) is associated with a deficit in goal-directed (MB) control, leading to a reliance on habitual (MF) systems. Therefore, the weighting parameter `w` (which balances MB and MF values) is negatively modulated by the OCI score.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free RL with OCI-modulated Weighting.
    
    Hypothesis: High OCI scores reduce the reliance on model-based planning (goal-directed),
    shifting the agent towards model-free (habitual) control.
    
    Parameters:
    learning_rate: [0, 1] - Update rate for Q-values.
    beta: [0, 10] - Inverse temperature for softmax.
    w_base: [0, 1] - Baseline weight for Model-Based control (when OCI is 0).
    oci_w_decay: [0, 1] - How much OCI reduces the Model-Based weight.
    """
    learning_rate, beta, w_base, oci_w_decay = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    # Calculate effective weight w. 
    # High OCI reduces w, making behavior more Model-Free (habitual).
    # We clip to ensure w stays in [0, 1].
    w = np.clip(w_base - (oci_w_decay * current_oci), 0.0, 1.0)

    # Transition matrix (fixed for this task structure)
    # A -> X (0.7), A -> Y (0.3)
    # U -> X (0.3), U -> Y (0.7)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Q-values
    q_stage1_mf = np.zeros(2) # Model-free stage 1
    q_stage2_mf = np.zeros((2, 2)) # Model-free stage 2 (terminal states)

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        
        # 1. Model-Based Value Calculation
        # Max value of next stage states
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        # Expected value based on transition probabilities
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Hybrid Value Calculation
        q_net_stage1 = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Softmax choice 1
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # --- Stage 2 Decision ---
        state_idx = int(state[trial])
        
        # Softmax choice 2 (purely model-free based on terminal values)
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # --- Learning ---
        
        # Prediction errors
        # Stage 2 PE
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        
        # Stage 1 PE (SARSA-style for MF)
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        
        # Update Values
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2
        
        # TD(1) update: Stage 1 MF value also learns from Stage 2 outcome
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage2 # Eligibility trace eligibility=1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Loss Aversion Model with OCI-Dependent Sensitivity
This model is a pure Model-Free learner, but it differentiates between positive and negative prediction errors. The hypothesis is that high OCI individuals are hypersensitive to "failure" or lack of reward (loss aversion/perfectionism). In this task, receiving a 0 reward is treated as a loss. The learning rate for negative prediction errors is amplified by the OCI score.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Asymmetric Learning Rate Model (Risk/Loss Sensitivity).
    
    Hypothesis: High OCI relates to perfectionism or fear of error.
    This is modeled as an increased learning rate specifically for 
    negative prediction errors (disappointments), leading to faster 
    abandonment of actions that fail to yield gold.

    Parameters:
    alpha_pos: [0, 1] - Learning rate for positive prediction errors (reward).
    alpha_neg_base: [0, 1] - Baseline learning rate for negative PEs.
    beta: [0, 10] - Inverse temperature.
    oci_neg_scale: [0, 5] - Scaling factor for OCI on negative learning rate.
    """
    alpha_pos, alpha_neg_base, beta, oci_neg_scale = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate effective negative learning rate
    # Higher OCI -> Higher sensitivity to negative outcomes
    alpha_neg = np.clip(alpha_neg_base + (oci_neg_scale * current_oci), 0.0, 1.0)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    for trial in range(n_trials):
        # Stage 1 Choice
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        
        # Stage 2 Choice
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # Learning
        
        # Stage 2 Update
        pe_2 = reward[trial] - q_stage2[state_idx, int(action_2[trial])]
        if pe_2 >= 0:
            q_stage2[state_idx, int(action_2[trial])] += alpha_pos * pe_2
        else:
            q_stage2[state_idx, int(action_2[trial])] += alpha_neg * pe_2
            
        # Stage 1 Update
        # Note: Using the updated Stage 2 value as the target (common in these tasks)
        pe_1 = q_stage2[state_idx, int(action_2[trial])] - q_stage1[int(action_1[trial])]
        
        if pe_1 >= 0:
            q_stage1[int(action_1[trial])] += alpha_pos * pe_1
        else:
            q_stage1[int(action_1[trial])] += alpha_neg * pe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Uncertainty-Driven Exploration with OCI Modulation
This model posits that OCI affects how individuals handle uncertainty. Specifically, it adds an "uncertainty bonus" (or penalty) to the Q-values based on how long it has been since an option was chosen. The `oci_uncert_param` determines if the participant is uncertainty-averse (avoids unchosen options, typical of rigid/compulsive behavior) or uncertainty-seeking.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free Q-learning with Uncertainty/Novelty Bonus.
    
    Hypothesis: OCI modulates the reaction to uncertainty (time since last chosen).
    A negative modulation implies uncertainty aversion (avoiding the unknown/rigid behavior),
    while positive implies novelty seeking.
    
    Parameters:
    learning_rate: [0, 1] - Standard learning rate.
    beta: [0, 10] - Inverse temperature.
    decay_rate: [0, 1] - Rate at which unchosen options accumulate uncertainty.
    oci_uncert_param: [0, 1] - How strongly OCI affects the valuation of uncertainty.
                              (Here modeled as a penalty for uncertainty).
    """
    learning_rate, beta, decay_rate, oci_uncert_param = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate uncertainty penalty weight based on OCI
    # Higher OCI -> Stronger aversion to uncertainty (higher penalty)
    uncertainty_weight = -1.0 * (oci_uncert_param * current_oci)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    # Track how many trials since an option was last chosen
    # Initialized at 0, increments every trial not chosen
    uncertainty_s1 = np.zeros(2) 
    
    for trial in range(n_trials):
        # Stage 1 Choice with Uncertainty Term
        # Q_net = Q_value + (Weight * Uncertainty)
        # If Weight is negative, this penalizes options not chosen recently (habit/rigidity)
        q_net_stage1 = q_stage1 + (uncertainty_weight * uncertainty_s1)
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # Update uncertainty counters
        uncertainty_s1 += 1.0 # Increment all
        uncertainty_s1[int(action_1[trial])] = 0.0 # Reset chosen
        
        # Decay uncertainty (prevents explosion)
        uncertainty_s1 = uncertainty_s1 * decay_rate
        
        state_idx = int(state[trial])
        
        # Stage 2 Choice (Standard Softmax)
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # Standard Q-Learning Updates
        pe_2 = reward[trial] - q_stage2[state_idx, int(action_2[trial])]
        q_stage2[state_idx, int(action_2[trial])] += learning_rate * pe_2
        
        pe_1 = q_stage2[state_idx, int(action_2[trial])] - q_stage1[int(action_1[trial])]
        q_stage1[int(action_1[trial])] += learning_rate * pe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```