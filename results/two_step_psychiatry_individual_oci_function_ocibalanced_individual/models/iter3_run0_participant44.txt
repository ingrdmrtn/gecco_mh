Here are three new cognitive models that incorporate the OCI score to explain individual variability in the two-step task. These models focus on how obsessive-compulsive traits might influence the balance between model-based and model-free control, or how rewards are processed.

### Model 1: OCI-Modulated Model-Based Weight
This model tests the hypothesis that high OCI scores are associated with a deficit in goal-directed (model-based) control, leading to a stronger reliance on habitual (model-free) strategies. The parameter `w` (mixing weight) determines the balance between these systems. Here, the effective `w` is modulated by the OCI score, such that higher OCI reduces the contribution of the model-based system.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Model-Based Weight.
    
    Hypothesis: High OCI participants have reduced goal-directed (model-based) control.
    The mixing weight 'w' (0=Pure MF, 1=Pure MB) is dampened by the OCI score.
    
    Parameters:
    learning_rate: [0,1] - Update rate for Q-values.
    beta: [0,10] - Inverse temperature for softmax.
    w_raw: [0,1] - Baseline weighting for model-based control before OCI modulation.
    """
    learning_rate, beta, w_raw = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Effective w is reduced by high OCI scores. 
    # If OCI is high (near 1), w becomes smaller (more Model-Free).
    # We define a scaling factor such that high OCI pulls w towards 0.
    w = w_raw * (1.0 - (0.8 * oci_score))

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based Value: Transition Matrix * Max(Stage 2 Q-values)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Value
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # --- Updates ---
        # Stage 2 RPE
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2
        
        # Stage 1 RPE (TD(0))
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Enhanced Choice Stickiness (Perseveration)
This model posits that obsessive-compulsive traits manifest as behavioral rigidity or "stickiness"â€”a tendency to repeat the previous choice regardless of reward outcomes. Here, the OCI score directly scales a stickiness parameter that biases the first-stage choice towards the action taken on the immediately preceding trial.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Enhanced Choice Stickiness.
    
    Hypothesis: High OCI leads to "stickiness" or perseveration (repeating choices),
    independent of reward history.
    
    Parameters:
    learning_rate: [0,1] - Update rate.
    beta: [0,10] - Inverse temperature.
    perseveration_factor: [0,5] - Base magnitude of the stickiness bonus.
    """
    learning_rate, beta, perseveration_factor = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective stickiness based on OCI
    # Higher OCI -> Higher tendency to repeat previous choice
    stickiness_weight = perseveration_factor * (1.0 + oci_score)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1 # Indicator for previous choice

    for trial in range(n_trials):

        # --- Stage 1 Choice ---
        # Pure Model-Free with Stickiness
        q_net = q_stage1_mf.copy()
        
        # Add stickiness bonus to the previously chosen action
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness_weight
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # Store action for next trial's stickiness
        last_action_1 = int(action_1[trial])
        state_idx = int(state[trial])

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # --- Updates ---
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2
        
        # Standard TD update for stage 1
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Dependent Eligibility Trace (Lambda)
This model explores the idea that OCI affects how credit is assigned to past actions. The eligibility trace parameter `lambda` controls how much the second-stage reward updates the first-stage value directly (TD(lambda)). A higher `lambda` connects outcomes more strongly to the initial choice. This model tests if high OCI is associated with a more rigid "chaining" of events (higher lambda), making the first-stage value update more dependent on the immediate second-stage outcome rather than the estimated value of the second-stage state.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Dependent Eligibility Trace (Lambda).
    
    Hypothesis: OCI affects the efficiency of credit assignment between stages.
    High OCI might lead to stronger 'chaining' (higher lambda), making Stage 1 
    values highly sensitive to immediate Stage 2 rewards (ignoring transition structure).
    
    Parameters:
    learning_rate: [0,1]
    beta: [0,10]
    lambda_base: [0,1] - Base eligibility trace parameter.
    """
    learning_rate, beta, lambda_base = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Effective lambda is modulated by OCI.
    # We hypothesize OCI increases the 'direct' connection (habitual chaining).
    # We clip to ensure it stays within [0, 1].
    eligibility_lambda = np.clip(lambda_base + (0.5 * oci_score), 0.0, 1.0)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # --- Updates ---
        # 1. Update Stage 2 Value based on Reward
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2
        
        # 2. Update Stage 1 Value
        # In TD(lambda) for this task, the Stage 1 update is a mix of:
        # - The Stage 2 State Value (Prediction Error 1)
        # - The Stage 2 Reward (Prediction Error 2 passed back)
        
        # PE at stage 1 (driven by Stage 2 Q-value)
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        
        # The update combines the direct transition error and the subsequent reward error scaled by lambda
        combined_update = delta_stage1 + (eligibility_lambda * delta_stage2)
        
        q_stage1_mf[int(action_1[trial])] += learning_rate * combined_update

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```