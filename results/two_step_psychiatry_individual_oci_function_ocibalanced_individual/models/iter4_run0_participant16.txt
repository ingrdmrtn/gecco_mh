Here are three new cognitive models that introduce different mechanisms for how OCI might influence reinforcement learning in this two-step task. These models explore ideas of habit formation, uncertainty-based learning rates, and separate sensitivities to rewards versus punishments.

### Model 1: OCI-Modulated Habit Strength (Model-Free Dominance)
This model hypothesizes that individuals with higher OCI scores rely more heavily on habit-based (model-free) learning rather than goal-directed (model-based) planning. Instead of a fixed mixing weight `w`, the weight is dynamically determined by the OCI score, where higher OCI pulls the weight towards 0 (pure model-free).

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Habit Strength (Model-Free Dominance).
    
    This model posits that OCI symptoms correlate with a shift towards habitual (model-free) control.
    The mixing weight 'w' (0=MF, 1=MB) is calculated as a base weight minus an OCI-dependent factor.
    Higher OCI reduces 'w', making the agent more model-free.
    
    Parameters:
    lr: [0, 1] Learning rate for value updates.
    beta: [0, 10] Inverse temperature for softmax choice.
    w_start: [0, 1] The starting mixing weight for a person with 0 OCI.
    oci_habit_strength: [0, 1] How strongly OCI reduces the model-based weight (pushes towards habit).
    """
    lr, beta, w_start, oci_habit_strength = model_parameters
    n_trials = len(action_1)
    oci = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    # Calculate the effective mixing weight based on OCI
    # We clip to ensure it stays within [0, 1]. 
    # If oci_habit_strength is high, w becomes small (MF dominance).
    w = w_start - (oci * oci_habit_strength)
    if w < 0: w = 0.0
    if w > 1: w = 1.0

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combined value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        state_idx = int(state[trial])

        # policy for the second choice
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        # Update stage 2 values
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += lr * delta_stage2
        
        # Update stage 1 values (TD(0) update using stage 2 value)
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += lr * delta_stage1
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Driven Punishment Sensitivity
This model investigates if OCI is associated with an increased sensitivity to negative outcomes (punishment) specifically in the learning rate. It separates the learning rate into `lr_reward` (fixed) and `lr_punishment`, where `lr_punishment` is amplified by the OCI score. This reflects the hypothesis that compulsive behavior is driven by excessive avoidance learning or fear of negative outcomes (zero coins).

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Driven Punishment Sensitivity.
    
    This model separates learning from positive outcomes (reward=1) and negative outcomes (reward=0).
    It hypothesizes that OCI specifically amplifies the learning rate for negative outcomes (punishment/omission),
    making the agent update values more drastically after a loss.
    
    Parameters:
    lr_rew: [0, 1] Learning rate for rewarded trials (reward=1).
    lr_pun_base: [0, 1] Base learning rate for unrewarded trials (reward=0).
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Mixing weight (model-based vs model-free).
    oci_pun_mod: [0, 1] How much OCI increases the punishment learning rate.
    """
    lr_rew, lr_pun_base, beta, w, oci_pun_mod = model_parameters
    n_trials = len(action_1)
    oci = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Calculate punishment learning rate modulated by OCI
    lr_pun = lr_pun_base + (oci * oci_pun_mod)
    if lr_pun > 1.0: lr_pun = 1.0

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        state_idx = int(state[trial])

        # policy for the second choice
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        # Determine effective learning rate for this trial
        if reward[trial] == 1:
            current_lr = lr_rew
        else:
            current_lr = lr_pun

        # Update stage 2 values
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += current_lr * delta_stage2
        
        # Update stage 1 values
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += current_lr * delta_stage1
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Dependent Exploration (Temperature Modulation)
This model suggests that OCI relates to decision noise or exploration/exploitation balance. It posits that high OCI might lead to "rigid" behavior (low temperature, high beta) or conversely "uncertain" behavior (high temperature, low beta). Here, we model `beta` as a function of OCI, allowing the data to determine if OCI increases or decreases choice stochasticity.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Dependent Exploration (Temperature Modulation).
    
    This model tests if OCI affects the inverse temperature (beta) of the softmax function.
    Instead of a fixed beta, the effective beta is determined by a base level plus an OCI-dependent shift.
    This allows the model to capture if high OCI subjects are more deterministic (higher beta) 
    or more random/exploratory (lower beta) in their choices.
    
    Parameters:
    lr: [0, 1] Learning rate.
    beta_base: [0, 10] Base inverse temperature.
    w: [0, 1] Mixing weight.
    lambda_eligibility: [0, 1] Eligibility trace decay (allowing Stage 2 to update Stage 1 directly).
    oci_beta_shift: [0, 5] Magnitude of OCI effect on beta (can be interpreted directionally by bounds).
    """
    lr, beta_base, w, lambda_eligibility, oci_beta_shift = model_parameters
    n_trials = len(action_1)
    oci = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    # Modulate beta based on OCI. 
    # Note: Since we only have positive bounds, we model this as an INCREASE in beta (more rigid/exploitative).
    # To model a decrease, one would need a different parameterization or negative bounds (not allowed here).
    # We assume OCI leads to higher rigidity.
    beta = beta_base + (oci * oci_beta_shift)
    
    # Cap beta to prevent overflow issues, though bounds usually handle this.
    if beta > 20: beta = 20

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        state_idx = int(state[trial])

        # policy for the second choice
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        # Update stage 2 values
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += lr * delta_stage2
        
        # Update stage 1 values using eligibility trace (lambda)
        # This updates Q1 based on the reward received at stage 2 directly
        delta_stage1_direct = reward[trial] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += lr * lambda_eligibility * delta_stage1_direct
        
        # Also standard TD update from stage 1 to stage 2 transition
        delta_stage1_td = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += lr * (1 - lambda_eligibility) * delta_stage1_td
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```