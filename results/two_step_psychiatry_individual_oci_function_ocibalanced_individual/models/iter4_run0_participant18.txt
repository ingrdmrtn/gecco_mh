Here are three new cognitive models exploring different mechanisms for how OCI symptoms might influence decision-making in the two-step task, specifically focusing on the balance between model-based (planning) and model-free (habit) systems, as well as sensitivity to rewards versus punishments.

### Model 1: Hybrid Learner with OCI-modulated Model-Based Weighting
This model tests the hypothesis that higher OCI scores relate to a deficit in goal-directed (model-based) control, leading to a greater reliance on habitual (model-free) strategies. It implements a hybrid reinforcement learning agent where the weighting parameter $w$ (mixing factor between MB and MF values) is a function of the OCI score.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free learner where the weighting parameter 'w'
    is modulated by the OCI score.
    
    Hypothesis: Higher OCI scores might correlate with reduced model-based control
    (lower w) or increased rigidity.
    
    Parameters:
    - learning_rate: Rate at which Q-values are updated.
    - beta: Inverse temperature for softmax.
    - w_max: The maximum possible model-based weight.
    - w_oci_penalty: How much OCI reduces the model-based weight.
    
    Bounds:
    learning_rate: [0,1]
    beta: [0,10]
    w_max: [0,1]
    w_oci_penalty: [0,1]
    """
    learning_rate, beta, w_max, w_oci_penalty = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective w (weight of model-based system)
    # Ensure w stays within [0, 1]
    w = w_max - (w_oci_penalty * oci_score)
    w = np.clip(w, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        # Model-Based Value calculation
        # Max Q-value at stage 2 for each state
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        # Bellman equation using known transition matrix
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        # Update Stage 1 MF (TD(1) logic roughly, or simple SARSA-like update)
        # Here using the standard hybrid update structure often used in Daw 2011
        # Update Stage 1 using the value of the state reached
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Update Stage 2 MF
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Asymmetric Learning Rates Modulated by OCI
This model investigates whether OCI symptoms are associated with an asymmetry in how positive versus negative prediction errors are processed. It posits that OCI might influence learning from negative outcomes (punishments/lack of reward) differently than positive ones, potentially driving avoidance behaviors or hyper-responsiveness to failure.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner with asymmetric learning rates for positive and negative
    prediction errors, where the negative learning rate is modulated by OCI.
    
    Hypothesis: High OCI individuals may over-learn from negative outcomes 
    (or lack of reward).
    
    Parameters:
    - alpha_pos: Learning rate for positive prediction errors (R > Q).
    - alpha_neg_base: Base learning rate for negative prediction errors (R < Q).
    - alpha_neg_oci: Additional sensitivity to negative errors due to OCI.
    - beta: Inverse temperature.
    - w: Weighting between model-based and model-free (fixed).
    
    Bounds:
    alpha_pos: [0,1]
    alpha_neg_base: [0,1]
    alpha_neg_oci: [0,1]
    beta: [0,10]
    w: [0,1]
    """
    alpha_pos, alpha_neg_base, alpha_neg_oci, beta, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective negative learning rate
    alpha_neg = alpha_neg_base + (alpha_neg_oci * oci_score)
    alpha_neg = np.clip(alpha_neg, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        if delta_stage2 >= 0:
            q_stage2_mf[s_idx, a2] += alpha_pos * delta_stage2
        else:
            q_stage2_mf[s_idx, a2] += alpha_neg * delta_stage2
            
        # Stage 1 Update (using TD-error from stage 2 transition)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        if delta_stage1 >= 0:
             q_stage1_mf[a1] += alpha_pos * delta_stage1
        else:
             q_stage1_mf[a1] += alpha_neg * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Inverse Temperature (Exploration/Exploitation)
This model posits that OCI scores affect the stochasticity of choice behavior (exploration vs. exploitation). Specifically, it tests if higher OCI leads to more deterministic (lower entropy) choices, reflecting a potential intolerance of uncertainty or rigid behavioral patterns.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid learner where the inverse temperature (beta) is modulated by OCI.
    
    Hypothesis: OCI scores influence the exploration-exploitation trade-off.
    Higher beta means more exploitation (rigidity), lower beta means more randomness.
    
    Parameters:
    - learning_rate: Learning rate.
    - beta_base: Baseline inverse temperature.
    - beta_oci_scale: How much OCI scales the beta parameter.
    - w: Weighting between model-based and model-free.
    - stickiness: Choice stickiness (perseveration).
    
    Bounds:
    learning_rate: [0,1]
    beta_base: [0,10]
    beta_oci_scale: [0,10] 
    w: [0,1]
    stickiness: [0,5]
    """
    learning_rate, beta_base, beta_oci_scale, w, stickiness = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective beta
    # If beta_oci_scale is positive, higher OCI -> higher beta -> more rigid/exploitative
    eff_beta = beta_base + (beta_oci_scale * oci_score)
    # Ensure beta doesn't explode or go negative (though bounds usually handle pos)
    eff_beta = np.maximum(eff_beta, 0.0) 

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
        
        exp_q1 = np.exp(eff_beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(eff_beta * q_stage2_mf[s_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```