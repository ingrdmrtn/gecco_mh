Here are three new cognitive models exploring different mechanisms of how OCI scores might influence reinforcement learning processes in a two-step task.

### Model 1: OCI-Modulated Model-Based Weighting (Parametric $w$)
This model tests the hypothesis that higher OCI symptoms correlate with a reliance on Model-Based (habit-free, goal-directed) control, but specifically affects the *balance* between Model-Based (MB) and Model-Free (MF) systems. Instead of treating $w$ as a static parameter or a simple linear slope, this model treats $w$ as a logistic function of OCI, allowing the mixing weight to saturate between 0 and 1 naturally.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL where the Model-Based weighting (w) is a logistic function of OCI.
    
    Hypothesis: OCI scores drive the balance between goal-directed (MB) and 
    habitual (MF) control. A logistic transform ensures w stays within [0,1].
    
    Parameters:
    learning_rate: [0, 1] - Learning rate for value updates.
    beta: [0, 10] - Inverse temperature for softmax choice.
    w_bias: [0, 10] - Baseline shift for the weighting function.
    w_slope: [0, 10] - Sensitivity of MB-weighting to OCI score.
    """
    learning_rate, beta, w_bias, w_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Logistic function to determine w based on OCI
    # If w_slope is positive, higher OCI -> higher w (more Model-Based)
    # The input to exp is centered/scaled. 
    # We use a standard logistic form: 1 / (1 + exp(-(slope * oci + bias)))
    # Note: bias can be negative in optimization, but bounds are [0,10].
    # To allow negative bias relative to 0, we can interpret w_bias as an offset from a center.
    # Let's use: w = 1 / (1 + exp(-(w_slope * (oci_score - 0.5) + (w_bias - 5))))
    # This centers the bias search space around 0.
    
    logit = w_slope * (oci_score - 0.5) + (w_bias - 5.0)
    w = 1.0 / (1.0 + np.exp(-logit))

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        qs_stage2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_stage2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # --- Learning ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])

        # MF Update Stage 1 (SARSA-like)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

        # MF Update Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Learning Rate Asymmetry (Pos/Neg)
This model investigates if OCI symptoms relate to an asymmetry in how participants learn from positive versus negative prediction errors. High OCI is often associated with perfectionism or fear of failure (negative outcomes). This model posits that the learning rate for negative prediction errors ($\alpha_{neg}$) is modulated by OCI, while the positive learning rate ($\alpha_{pos}$) is fixed.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Dual Learning Rate model where OCI modulates sensitivity to negative errors.
    
    Hypothesis: High OCI participants may over-weight or under-weight negative 
    outcomes (disappointment) compared to positive ones.
    
    Parameters:
    alpha_pos: [0, 1] - Learning rate for positive prediction errors (RPE > 0).
    alpha_neg_base: [0, 1] - Base learning rate for negative prediction errors.
    alpha_neg_oci: [0, 1] - How much OCI scales the negative learning rate.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - MB/MF weighting.
    """
    alpha_pos, alpha_neg_base, alpha_neg_oci, beta, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective negative learning rate, bounded at 1
    # If alpha_neg_oci is high, OCI increases learning from failure.
    alpha_neg = min(1.0, alpha_neg_base + alpha_neg_oci * oci_score)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        qs_stage2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_stage2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # --- Learning ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])

        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        lr_1 = alpha_pos if delta_stage1 > 0 else alpha_neg
        q_stage1_mf[a1] += lr_1 * delta_stage1

        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        lr_2 = alpha_pos if delta_stage2 > 0 else alpha_neg
        q_stage2_mf[state_idx, a2] += lr_2 * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Eligibility Trace (Lambda)
This model tests if OCI affects how credit is assigned to past actions. In standard TD($\lambda$), the eligibility trace parameter $\lambda$ controls how much the Stage 1 choice is updated by the Stage 2 reward directly. A high $\lambda$ connects the reward at the end of the trial strongly back to the very first choice (Model-Free efficient credit assignment). This model proposes that OCI modulates this "memory trace" or credit assignment efficiency.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    TD(lambda) model where OCI modulates the eligibility trace decay (lambda).
    
    Hypothesis: OCI affects the efficiency of credit assignment. High OCI might
    imply rigid chains of cause-and-effect (high lambda) or fragmented
    associations (low lambda).
    
    Parameters:
    learning_rate: [0, 1] - Learning rate.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - MB/MF weighting.
    lambda_oci_scale: [0, 1] - Scaling factor for lambda based on OCI.
    """
    learning_rate, beta, w, lambda_oci_scale = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Lambda is determined by OCI. 
    # We assume base lambda is implicitly 0 or handled by the scale.
    # Here we define lambda purely as a function of OCI for simplicity of parameter count.
    # lambda = oci * scale. Bounded at 1.
    eligibility_lambda = min(1.0, oci_score * lambda_oci_scale)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        qs_stage2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_stage2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # --- Learning with Eligibility Trace ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])

        # 1. Update Stage 1 based on Stage 2 value (Standard TD)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # 2. Update Stage 2 based on Reward
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # 3. Eligibility Trace Update: Apply Stage 2 RPE to Stage 1 Choice
        # This connects the final reward back to the first spaceship choice
        q_stage1_mf[a1] += learning_rate * eligibility_lambda * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```