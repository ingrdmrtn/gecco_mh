Here are three new cognitive models that introduce distinct mechanisms for how Obsessive-Compulsive Inventory (OCI) scores might influence decision-making in a two-step task. These models explore concepts of habit formation via model-free dominance, altered learning rates based on outcome valence, and an inability to disengage from non-rewarded actions (perseveration).

### Model 1: Hybrid Learner with OCI-Driven Model-Free Dominance
This model hypothesizes that high OCI scores are associated with a reliance on habitual, model-free control rather than goal-directed, model-based planning. The weighting parameter `w` (which balances model-based and model-free values) is modulated by OCI, such that higher OCI leads to a stronger model-free bias (lower `w`).

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL with OCI-modulated Model-Based/Model-Free Weighting.
    
    Hypothesis: Higher OCI scores are associated with increased reliance on habitual (Model-Free) 
    systems over goal-directed (Model-Based) systems. The mixing parameter 'w' is reduced 
    as OCI increases, pushing the agent towards pure MF control.
    
    Parameters:
    - learning_rate: [0, 1] Update rate for Q-values.
    - beta: [0, 10] Inverse temperature for softmax.
    - w_max: [0, 1] Maximum model-based weight (for OCI=0).
    - oci_suppression: [0, 1] How strongly OCI suppresses the model-based weight.
    """
    learning_rate, beta, w_max, oci_suppression = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate the effective mixing weight w. 
    # Higher OCI reduces w, making the agent more Model-Free.
    # We clip to ensure w stays within [0, 1].
    w = w_max * (1.0 - (current_oci * oci_suppression))
    w = np.clip(w, 0.0, 1.0)

    # Fixed transition matrix for the model-based planner (70/30 common/rare)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)     # Model-free values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for stage 2 (states X, Y)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based Value Calculation: V(S') = max(Q_MF(S', a'))
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value: w * MB + (1-w) * MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Updates ---
        # Stage 2 TD Error (Standard Q-learning)
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 TD Error (SARSA-style update for MF value)
        # Using the value of the state actually reached (q_stage2_mf[state_idx, a2])
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Asymmetric Learning Rates Modulated by OCI
This model proposes that OCI affects how individuals learn from positive versus negative outcomes. Specifically, it tests the hypothesis that individuals with higher OCI scores have a heightened sensitivity to punishment (or lack of reward), leading to a higher learning rate for negative prediction errors (`alpha_neg`) compared to positive ones.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free RL with OCI-modulated Asymmetric Learning Rates.
    
    Hypothesis: OCI is associated with altered sensitivity to outcomes. This model 
    splits the learning rate into alpha_pos (for positive RPEs) and alpha_neg 
    (for negative RPEs). The negative learning rate is boosted by the OCI score, 
    reflecting increased sensitivity to missed rewards or errors.
    
    Parameters:
    - alpha_pos: [0, 1] Learning rate for positive prediction errors.
    - alpha_neg_base: [0, 1] Base learning rate for negative prediction errors.
    - beta: [0, 10] Inverse temperature.
    - oci_neg_boost: [0, 2] How much OCI increases the negative learning rate.
    """
    alpha_pos, alpha_neg_base, beta, oci_neg_boost = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate effective negative learning rate
    alpha_neg = alpha_neg_base + (current_oci * oci_neg_boost)
    alpha_neg = np.clip(alpha_neg, 0.0, 1.0)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy (Pure Model-Free) ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Updates ---
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        if delta_stage2 >= 0:
            q_stage2_mf[state_idx, a2] += alpha_pos * delta_stage2
        else:
            q_stage2_mf[state_idx, a2] += alpha_neg * delta_stage2

        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        if delta_stage1 >= 0:
            q_stage1_mf[a1] += alpha_pos * delta_stage1
        else:
            q_stage1_mf[a1] += alpha_neg * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Driven Inverse Temperature (Exploration/Exploitation Balance)
This model posits that OCI symptoms map onto the exploration-exploitation trade-off. Specifically, high OCI might be associated with rigidity and over-exploitation (high `beta`), reducing stochastic exploration. The inverse temperature `beta` is modeled as a baseline value plus an OCI-dependent increase.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Based RL with OCI-modulated Inverse Temperature (Rigidity).
    
    Hypothesis: High OCI scores lead to more deterministic, rigid choice patterns 
    (reduced exploration). This is modeled by scaling the inverse temperature (beta) 
    upwards as a function of OCI.
    
    Parameters:
    - learning_rate: [0, 1] Update rate for Q-values.
    - beta_base: [0, 10] Baseline inverse temperature.
    - oci_beta_slope: [0, 10] How much OCI increases beta (making choices more rigid).
    - w: [0, 1] Fixed weighting between MB and MF systems.
    """
    learning_rate, beta_base, oci_beta_slope, w = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate effective beta
    # Higher OCI -> Higher Beta -> Sharper softmax (less exploration/more rigidity)
    beta = beta_base + (current_oci * oci_beta_slope)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Updates ---
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```