Here are three new cognitive models that introduce distinct mechanisms for how Obsessive-Compulsive symptoms (OCI) might influence decision-making in the two-step task.

### Model 1: Hybrid Model-Based/Model-Free with OCI-Modulated Mixing Weight
This model tests the hypothesis that high OCI scores are associated with a heavier reliance on habitual (Model-Free) control over goal-directed (Model-Based) control. Instead of a fixed weighting parameter `w`, the balance between systems is a function of the OCI score.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: Hybrid MB/MF with OCI-Modulated Mixing Weight (w).
    
    Hypothesis: Higher OCI scores shift the balance from Model-Based (planning)
    to Model-Free (habitual) control.
    
    Parameters:
    learning_rate: [0, 1] Learning rate for value updates.
    beta: [0, 10] Inverse temperature for softmax.
    w_base: [0, 1] Baseline mixing weight (0=Pure MF, 1=Pure MB) at OCI=0.
    w_oci_slope: [-1, 1] How much OCI changes w. Negative slope implies high OCI -> more Habitual.
    """
    learning_rate, beta, w_base, w_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate mixing weight w based on OCI, clamped between 0 and 1
    w = w_base + (w_oci_slope * oci_score)
    w = np.clip(w, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Q-values
    q_stage1_mf = np.zeros(2)      # Model-free values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for stage 2 (aliens)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based value calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated value (Hybrid)
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        # Stage 2 RPE
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 RPE (TD(0) update for MF system)
        # Using the value of the state we actually arrived at
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Asymmetric Learning Rates Modulated by OCI
This model hypothesizes that OCI relates to how individuals process positive versus negative feedback. Specifically, it tests if high OCI scores lead to "sticky" negative learning (over-weighting punishment/lack of reward) or "sticky" positive learning (compulsive reinforcement). The learning rate splits into positive (`alpha_pos`) and negative (`alpha_neg`), with the negative rate dependent on OCI.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: Dual Learning Rates with OCI-Modulated Negative Learning.
    
    Hypothesis: OCI affects how strongly participants update values after 
    receiving 0 reward (negative prediction error).
    
    Parameters:
    alpha_pos: [0, 1] Learning rate for positive RPEs (reward > expectation).
    alpha_neg_base: [0, 1] Baseline learning rate for negative RPEs.
    alpha_neg_oci_slope: [-1, 1] Modulation of negative learning rate by OCI.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Fixed mixing weight (0=MF, 1=MB).
    """
    alpha_pos, alpha_neg_base, alpha_neg_oci_slope, beta, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective negative learning rate
    alpha_neg = alpha_neg_base + (alpha_neg_oci_slope * oci_score)
    alpha_neg = np.clip(alpha_neg, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        lr_s2 = alpha_pos if delta_stage2 > 0 else alpha_neg
        q_stage2_mf[state_idx, a2] += lr_s2 * delta_stage2
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        lr_s1 = alpha_pos if delta_stage1 > 0 else alpha_neg
        q_stage1_mf[a1] += lr_s1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Inverse Temperature (Exploration/Exploitation) Modulated by OCI
This model posits that the primary effect of OCI is on the exploration-exploitation trade-off. High OCI might be associated with higher `beta` (lower temperature), indicating more deterministic, rigid, or exploitative behavior, whereas low OCI might indicate more stochastic responding. This model uses a standard Hybrid structure but makes `beta` a function of OCI.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Inverse Temperature (Beta).
    
    Hypothesis: OCI scores influence the randomness of choice. High OCI may lead 
    to more deterministic (rigid) choices (high beta), or more random choices 
    (low beta) due to anxiety/uncertainty.
    
    Parameters:
    learning_rate: [0, 1] Learning rate.
    beta_base: [0, 10] Baseline inverse temperature.
    beta_oci_slope: [-5, 5] Effect of OCI on beta.
    w: [0, 1] Fixed mixing weight.
    eligibility_trace: [0, 1] Eligibility trace parameter (lambda) for TD-learning.
    """
    learning_rate, beta_base, beta_oci_slope, w, eligibility_trace = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate Beta modulated by OCI
    beta = beta_base + (beta_oci_slope * oci_score)
    # Ensure beta stays within reasonable bounds (non-negative)
    beta = np.clip(beta, 0.0, 20.0) 

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning with Eligibility Traces (TD(lambda)) ---
        # Stage 2 RPE
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 RPE
        # Standard TD(0) update for Stage 1 MF value
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Eligibility trace update: Second stage reward also updates first stage choice
        # scaled by eligibility trace parameter lambda
        q_stage1_mf[a1] += learning_rate * eligibility_trace * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```