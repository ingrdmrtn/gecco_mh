Here are three new cognitive models that incorporate the OCI score into the decision-making process for the two-step task.

### Model 1: OCI-Driven Choice Stickiness
This model posits that obsessive-compulsive traits drive a tendency to repeat the previous action (perseveration), regardless of the reward outcome. Unlike previous models that might have looked at stickiness as a baseline trait or modulated a base stickiness, this model integrates stickiness into the full hybrid (Model-Based/Model-Free) framework, where the magnitude of the stickiness bonus is directly proportional to the OCI score.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Driven Choice Stickiness.
    
    This model assumes that OCI score drives a "stickiness" or perseveration bonus.
    The agent is more likely to repeat their previous Stage 1 choice, and the strength
    of this tendency is scaled by their OCI score. This is integrated into a full 
    hybrid Model-Based / Model-Free reinforcement learning agent.
    
    Parameters:
    - learning_rate: [0, 1] Update rate for Q-values.
    - beta: [0, 10] Inverse temperature (exploration/exploitation).
    - w: [0, 1] Weighting parameter (0 = Model-Free, 1 = Model-Based).
    - lambda_eligibility: [0, 1] Eligibility trace decay.
    - stickiness_oci_slope: [0, 5] Scaling factor converting OCI to stickiness bonus.
      (Stickiness = stickiness_oci_slope * OCI).
    """
    learning_rate, beta, w, lambda_eligibility, stickiness_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Calculate the stickiness bonus specific to this participant
    stickiness_bonus = stickiness_oci_slope * oci_score
    
    # Track previous choice for stickiness
    prev_a1 = -1

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        # Model-Based Value Calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness to the previously chosen action
        if prev_a1 != -1:
            q_net[prev_a1] += stickiness_bonus
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        # Standard Model-Free choice at Stage 2
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        r = reward[trial]
        
        # Stage 2 Prediction Error
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 Prediction Error (SARSA-like logic for stage 1 value)
        # Note: In standard 2-step, Stage 1 MF value is updated towards Stage 2 value
        # The 'value' of the state arrived at is usually taken as Q_stage2[state, selected_action]
        # or max(Q_stage2[state]). Here we use the selected action (SARSA style) for the TD error
        # driven by the transition.
        
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        
        # Update Stage 1 MF Q-value with eligibility trace from Stage 2 reward
        q_stage1_mf[a1] += learning_rate * delta_stage1 + (learning_rate * lambda_eligibility * delta_stage2)
        
        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Sigmoid Model-Based Weighting
This model investigates the hypothesis that OCI scores predict the balance between Goal-Directed (Model-Based) and Habitual (Model-Free) control. It uses a sigmoid function to map the OCI score to the mixing weight `w`. This formulation respects the mathematical bounds of `w` (0 to 1) while allowing the OCI score to shift the set-point of the agent's strategy along the MB-MF spectrum.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Sigmoid Model-Based Weighting.
    
    This model determines the weighting parameter 'w' (mixing Model-Based and Model-Free values)
    as a sigmoid function of the OCI score. This tests if higher OCI scores relate to a 
    shift towards more habitual (low w) or more goal-directed (high w) control, constrained
    strictly within valid probability bounds [0, 1].
    
    Parameters:
    - learning_rate: [0, 1] Update rate for Q-values.
    - beta: [0, 10] Inverse temperature.
    - lambda_eligibility: [0, 1] Eligibility trace decay.
    - w_logit_intercept: [-5, 5] Base logit for the weighting parameter w.
    - w_oci_slope: [-5, 5] How strongly OCI affects the logit of w.
    """
    learning_rate, beta, lambda_eligibility, w_logit_intercept, w_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Calculate w using sigmoid function
    w_logit = w_logit_intercept + (w_oci_slope * oci_score)
    w = 1.0 / (1.0 + np.exp(-w_logit))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        r = reward[trial]
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1 + (learning_rate * lambda_eligibility * delta_stage2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Surprise Learning (Rare Transition Bias)
This model tests whether individuals with higher OCI scores react differently to "surprise" in the environment structure. Specifically, it modulates the learning rate for the Stage 1 Model-Free update when a **Rare Transition** occurs. High OCI individuals might over-weight these rare events (treating them as significant errors) or under-weight them, affecting how quickly they update their cached values of the spaceships.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Surprise Learning (Rare Transition Bias).
    
    This model assumes that OCI scores modulate the learning rate specifically when
    a Rare Transition occurs (i.e., Spaceship A going to Planet Y, or U to X).
    Standard models use a constant learning rate. This model allows OCI to amplify 
    or dampen the update to the Stage 1 Model-Free values upon encountering
    structural surprise.
    
    Parameters:
    - alpha_base: [0, 1] Baseline learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weighting parameter.
    - lambda_eligibility: [0, 1] Eligibility trace decay.
    - oci_rare_mod: [-1, 5] Multiplier for alpha when transition is rare.
      Effective Alpha (Rare) = alpha_base * (1 + oci_rare_mod * OCI).
    """
    alpha_base, beta, w, lambda_eligibility, oci_rare_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        r = reward[trial]
        
        # Stage 2 update uses base alpha
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += alpha_base * delta_stage2
        
        # Check if transition was Common or Rare
        # Common: (A->X, state 0) or (U->Y, state 1). Indices: (0->0) or (1->1)
        is_common = (a1 == state_idx)
        
        # Determine effective alpha for Stage 1 update
        if not is_common: # Rare transition
            effective_alpha = alpha_base * (1.0 + oci_rare_mod * oci_score)
            # Clip to [0, 1] to maintain stability
            if effective_alpha > 1.0: effective_alpha = 1.0
            if effective_alpha < 0.0: effective_alpha = 0.0
        else:
            effective_alpha = alpha_base
        
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        
        # Update Stage 1 using the effective alpha modulated by surprise and OCI
        q_stage1_mf[a1] += effective_alpha * delta_stage1 + (effective_alpha * lambda_eligibility * delta_stage2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```