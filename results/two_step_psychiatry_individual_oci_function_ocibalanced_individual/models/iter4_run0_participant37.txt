Here are three new cognitive models that incorporate the OCI-R score into the decision-making process in the two-step task.

### Model 1: OCI-Modulated Eligibility Trace (Sarsa($\lambda$))
This model hypothesizes that individuals with higher OCI scores might have a harder time distinguishing the distinct temporal stages of the task, effectively "smearing" credit assignment. Instead of a standard TD(0) update or a pure Model-Based/Model-Free split, this model uses an eligibility trace parameter $\lambda$ that determines how much the Stage 2 reward directly updates the Stage 1 value. The hypothesis is that high OCI leads to higher $\lambda$ (more direct credit assignment to the first action based on the final outcome, bypassing the state transition structure).

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Eligibility Trace (Sarsa-Lambda).

    This model tests the hypothesis that OCI affects the eligibility trace parameter (lambda).
    Higher OCI might lead to 'over-attribution' or stronger direct association between
    Stage 1 choices and Stage 2 rewards, bypassing the intermediate state structure.
    
    The eligibility trace lambda is modeled as:
    lambda = lambda_base + (lambda_oci * oci_score)
    (Clipped to [0, 1]).

    Bounds:
    learning_rate: [0,1]
    beta: [0,10]
    lambda_base: [0,1]
    lambda_oci: [0,1]
    """
    learning_rate, beta, lambda_base, lambda_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate lambda based on OCI
    eligibility_lambda = lambda_base + (lambda_oci * oci_score)
    eligibility_lambda = np.clip(eligibility_lambda, 0.0, 1.0)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        if a1 != -1:
            p_choice_1[trial] = probs_1[a1]
        else:
            p_choice_1[trial] = 1.0

        # Skip missing data
        if a1 == -1 or state[trial] == -1 or action_2[trial] == -1:
            continue

        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[s_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        # Prediction error at stage 2
        delta2 = r - q_stage2[s_idx, a2]
        
        # Prediction error at stage 1 (using Q-value of chosen stage 2 action)
        delta1 = q_stage2[s_idx, a2] - q_stage1[a1]

        # Update Stage 2
        q_stage2[s_idx, a2] += learning_rate * delta2
        
        # Update Stage 1
        # Standard TD(0) part: learning_rate * delta1
        # Eligibility trace part: learning_rate * lambda * delta2
        q_stage1[a1] += learning_rate * (delta1 + eligibility_lambda * delta2)

    eps = 1e-10
    p_choice_1 = np.maximum(p_choice_1, eps)
    p_choice_2 = np.maximum(p_choice_2, eps)
    log_loss = -(np.sum(np.log(p_choice_1)) + np.sum(np.log(p_choice_2)))
    return log_loss
```

### Model 2: OCI-Dependent "Forgetfulness" (Decay Rate)
This model posits that OCI symptoms interfere with the maintenance of value representations over time. Specifically, it introduces a decay parameter that pulls Q-values back toward zero (or a baseline) on every trial. The rate of this decay is modulated by the OCI score. A high decay rate would represent an inability to sustain learned values, potentially leading to repetitive checking or erratic switching behavior often seen in compulsivity.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Dependent Value Decay.

    This model introduces a decay parameter that erodes Q-values toward 0.5 (neutral)
    on every trial for unchosen actions. The hypothesis is that OCI relates to
    the stability of memory traces.
    
    decay_rate = decay_base + (decay_oci * oci_score)
    
    Bounds:
    learning_rate: [0,1]
    beta: [0,10]
    decay_base: [0,1]
    decay_oci: [0,1]
    w_mb: [0,1] (Weight of model-based control)
    """
    learning_rate, beta, decay_base, decay_oci, w_mb = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    decay = decay_base + (decay_oci * oci_score)
    decay = np.clip(decay, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2) + 0.5 # Initialize at neutral
    q_stage2_mf = np.zeros((2, 2)) + 0.5

    for trial in range(n_trials):
        # --- Stage 1 Choice (Hybrid MB/MF) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w_mb * q_stage1_mb) + ((1 - w_mb) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        if a1 != -1:
            p_choice_1[trial] = probs_1[a1]
        else:
            p_choice_1[trial] = 1.0

        if a1 == -1 or state[trial] == -1 or action_2[trial] == -1:
            continue

        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        # Decay unchosen values towards 0.5
        q_stage1_mf = (1 - decay) * q_stage1_mf + decay * 0.5
        q_stage2_mf = (1 - decay) * q_stage2_mf + decay * 0.5

        # Standard Updates
        delta2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta2 # Re-add update to chosen

        delta1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta1 # Re-add update to chosen

    eps = 1e-10
    p_choice_1 = np.maximum(p_choice_1, eps)
    p_choice_2 = np.maximum(p_choice_2, eps)
    log_loss = -(np.sum(np.log(p_choice_1)) + np.sum(np.log(p_choice_2)))
    return log_loss
```

### Model 3: OCI-Modulated Transition Learning Rate
Standard models assume the transition matrix (0.7/0.3) is fixed and known. However, participants might be learning this structure on the fly, or doubting the stability of the transitions. This model proposes that high-OCI individuals might be hypersensitive to "rare" transitions, treating them as evidence that the world structure has changed. Therefore, they update their internal model of the transition probabilities more aggressively based on their OCI score.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Transition Learning.

    This model assumes the agent learns the transition matrix rather than assuming it fixed.
    The rate at which they update their belief about transitions is modulated by OCI.
    High OCI might correspond to 'hyper-updating' the world model upon observing rare transitions,
    reflecting an intolerance of uncertainty or doubt about the stability of the environment.

    Bounds:
    learning_rate: [0,1] (For reward values)
    beta: [0,10]
    lr_trans_base: [0,1] (Base learning rate for transitions)
    lr_trans_oci: [0,1] (Additional learning rate for transitions due to OCI)
    w_mb: [0,1]
    """
    learning_rate, beta, lr_trans_base, lr_trans_oci, w_mb = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    lr_trans = lr_trans_base + (lr_trans_oci * oci_score)
    lr_trans = np.clip(lr_trans, 0.0, 1.0)
    
    # Initialize belief about transition probabilities (starts at true values or 0.5)
    # Here we initialize at 0.5 (max uncertainty) to allow learning to drive differentiation
    # T[a, s] = prob of going to state s given action a
    # We maintain just P(State 0 | Action 0) and P(State 1 | Action 1) for simplicity
    # Assuming symmetry T[0,1] = 1 - T[0,0], etc.
    trans_prob_A_to_X = 0.5 
    trans_prob_B_to_Y = 0.5 

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # Construct current transition matrix from beliefs
        # Action 0 (A) -> [Prob X, Prob Y]
        # Action 1 (B) -> [Prob X, Prob Y]
        curr_trans_matrix = np.array([
            [trans_prob_A_to_X, 1.0 - trans_prob_A_to_X],
            [1.0 - trans_prob_B_to_Y, trans_prob_B_to_Y]
        ])

        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = curr_trans_matrix @ max_q_stage2
        
        q_net = (w_mb * q_stage1_mb) + ((1 - w_mb) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        if a1 != -1:
            p_choice_1[trial] = probs_1[a1]
        else:
            p_choice_1[trial] = 1.0

        if a1 == -1 or state[trial] == -1 or action_2[trial] == -1:
            continue

        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Value Learning ---
        delta2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta2
        
        delta1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta1

        # --- Transition Learning ---
        # If Action 0 was taken
        if a1 == 0:
            # Did we go to state 0? 1 if yes, 0 if no
            outcome = 1.0 if s_idx == 0 else 0.0
            trans_prob_A_to_X += lr_trans * (outcome - trans_prob_A_to_X)
        
        # If Action 1 was taken
        elif a1 == 1:
            # Did we go to state 1? 1 if yes, 0 if no
            outcome = 1.0 if s_idx == 1 else 0.0
            trans_prob_B_to_Y += lr_trans * (outcome - trans_prob_B_to_Y)

    eps = 1e-10
    p_choice_1 = np.maximum(p_choice_1, eps)
    p_choice_2 = np.maximum(p_choice_2, eps)
    log_loss = -(np.sum(np.log(p_choice_1)) + np.sum(np.log(p_choice_2)))
    return log_loss
```