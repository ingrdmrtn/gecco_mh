Here are three new cognitive models that explore different mechanisms for how OCI might influence learning and decision-making in the two-step task. These models focus on how obsessive-compulsive traits might alter the balance between model-based/model-free control, sensitivity to rare transitions, or learning rates.

### Model 1: OCI-Modulated Model-Based Weighting
This model hypothesizes that individuals with higher OCI scores rely more heavily on habit-based (model-free) learning and less on goal-directed (model-based) planning. The mixing weight `w` (where `w=1` is fully model-based and `w=0` is fully model-free) is dynamically adjusted based on the participant's OCI score.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Model-Based Weighting.
    
    This model proposes that the balance between Model-Based (MB) and Model-Free (MF) 
    strategies is modulated by OCI symptoms.
    A base mixing weight is defined, which is then shifted by the OCI score.
    Higher OCI might lead to reduced MB control (or increased, depending on the sign of the modulation).
    
    Parameters:
    learning_rate: [0, 1] Learning rate for value updates.
    beta: [0, 10] Inverse temperature for softmax choice.
    w_base: [0, 1] Baseline weight for Model-Based system (0=MF, 1=MB).
    oci_w_mod: [0, 1] Strength of OCI modulation on w.
    """
    learning_rate, beta, w_base, oci_w_mod = model_parameters
    n_trials = len(action_1)
    oci = oci[0]
  
    # Calculate effective w based on OCI. 
    # We clip to ensure w stays within [0, 1].
    # Hypothesis: High OCI reduces flexibility/MB control -> w decreases.
    w = w_base - (oci * oci_w_mod)
    w = np.clip(w, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combined value: mix of MB and MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        state_idx = int(state[trial])

        # policy for the second choice
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        # Update Stage 2 values (MF)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2
        
        # Update Stage 1 values (MF) using the stage 2 value (SARSA-like)
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Dependent Transition Learning
This model suggests that high OCI might be associated with a rigid internal model of the world or, conversely, hyper-sensitivity to uncertainty in the environment structure. Instead of a fixed transition matrix (0.7/0.3), the agent learns the transition probabilities trial-by-trial. The learning rate for these transitions is modulated by OCI.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Dependent Transition Learning.
    
    This model assumes the agent learns the transition matrix dynamically rather than 
    using the fixed [0.7, 0.3] prior. The rate at which they update their beliefs 
    about spaceship->planet transitions is modulated by OCI.
    High OCI might correspond to faster updating (hyper-responsiveness to rare transitions)
    or slower updating (rigidity).
    
    Parameters:
    lr_value: [0, 1] Learning rate for reward values.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Model-based weight.
    lr_trans_base: [0, 1] Base learning rate for transition probabilities.
    oci_trans_mod: [0, 1] OCI modulation of transition learning rate.
    """
    lr_value, beta, w, lr_trans_base, oci_trans_mod = model_parameters
    n_trials = len(action_1)
    oci = oci[0]
  
    # Initialize transition counts or probabilities
    # We start with a uniform prior or slight bias, but for simplicity here we start 
    # with the true structure but allow it to drift based on experience.
    # To keep it simple: we track the probability P(State 0 | Action 0) and P(State 1 | Action 1).
    # We assume symmetry: P(State 1 | Action 0) = 1 - P(State 0 | Action 0).
    trans_probs = np.array([[0.7, 0.3], [0.3, 0.7]]) 
    
    # Calculate effective transition learning rate
    lr_trans = lr_trans_base + (oci * oci_trans_mod)
    lr_trans = np.clip(lr_trans, 0.0, 1.0)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        
        # Use learned transition matrix for MB calc
        q_stage1_mb = trans_probs @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        action_1_idx = int(action_1[trial])

        # policy for the second choice
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        # Update Values
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += lr_value * delta_stage2
        
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[action_1_idx]
        q_stage1_mf[action_1_idx] += lr_value * delta_stage1
        
        # Update Transition Matrix estimates
        # If action 0 led to state 0: increase P(S0|A0), decrease P(S1|A0)
        # We update the row corresponding to the chosen action
        # Create a one-hot vector for the state observed
        observed_transition = np.zeros(2)
        observed_transition[state_idx] = 1.0
        
        # Update the row for the chosen action towards the observed outcome
        trans_probs[action_1_idx] += lr_trans * (observed_transition - trans_probs[action_1_idx])

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Specific Punishment Sensitivity
This model investigates whether high OCI individuals learn differently from positive vs. negative outcomes (or lack of reward). Specifically, it separates the learning rate for rewards (`lr_rew`) and punishments/omissions (`lr_pun`), and modulates the punishment learning rate by the OCI score. This reflects the hypothesis that compulsive behavior might be driven by an excessive avoidance of negative outcomes or prediction errors.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Specific Punishment Sensitivity.
    
    This model separates learning rates for rewarded trials vs unrewarded (punishment) trials.
    The learning rate for unrewarded trials is modulated by OCI.
    High OCI might lead to 'over-learning' from failures (high punishment sensitivity).
    
    Parameters:
    lr_rew: [0, 1] Learning rate for positive rewards (reward=1).
    lr_pun_base: [0, 1] Base learning rate for zero rewards (reward=0).
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Mixing weight (MB vs MF).
    oci_pun_mod: [0, 1] OCI modulation of punishment learning rate.
    """
    lr_rew, lr_pun_base, beta, w, oci_pun_mod = model_parameters
    n_trials = len(action_1)
    oci = oci[0]
  
    # Calculate effective punishment learning rate
    lr_pun = lr_pun_base + (oci * oci_pun_mod)
    lr_pun = np.clip(lr_pun, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        state_idx = int(state[trial])

        # policy for the second choice
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        # Determine current learning rate based on outcome
        if reward[trial] > 0:
            current_lr = lr_rew
        else:
            current_lr = lr_pun

        # Update Stage 2 values
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += current_lr * delta_stage2
        
        # Update Stage 1 values
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += current_lr * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```