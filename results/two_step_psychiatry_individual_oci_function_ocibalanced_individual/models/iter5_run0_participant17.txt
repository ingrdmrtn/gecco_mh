Here are three new cognitive models that incorporate the OCI score into the decision-making process, distinct from the previously tried combinations.

### Model 1: Hybrid Learner with OCI-Modulated Model-Based Weight (`w`)
This model hypothesizes that high OCI scores reflect a more rigid, habit-driven cognitive style (Model-Free), leading to a reduction in goal-directed (Model-Based) planning. The parameter `w` (weighting between MF and MB systems) is directly scaled down by the OCI score.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: Hybrid Learner with OCI-Modulated Model-Based Weight.
    
    Hypothesis: Higher OCI scores correlate with reduced goal-directed (Model-Based) control
    and increased reliance on habitual (Model-Free) systems. The mixing parameter 'w'
    starts at a baseline and is reduced by the OCI score.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for value updates.
    - beta: [0, 10] Inverse temperature (softmax randomness).
    - w_max: [0, 1] The maximum theoretical model-based weight (when OCI is 0).
    - oci_w_penalty: [0, 1] How much 'w' decreases per unit of OCI.
    """
    learning_rate, beta, w_max, oci_w_penalty = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective w, ensuring it stays within [0, 1]
    # High OCI reduces the contribution of the model-based system
    w = w_max - (oci_w_penalty * oci_score)
    w = np.clip(w, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        s_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        r = reward[trial]
        
        # Stage 2 Update (Model-Free)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 Update (Model-Free only - TD(0))
        # Note: Standard Daw implementation usually updates MB values implicitly via max_q_stage2
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Model-Free Learner with OCI-Driven "Relief" Learning (Asymmetric Learning Rates)
This model hypothesizes that OCI is characterized by anxiety relief. Therefore, avoiding a bad outcome (getting 0 reward) might be learned differently than gaining a reward, or conversely, the "miss" signal is amplified. Here, we test if OCI modulates the learning rate specifically for negative prediction errors (disappointment/safety seeking).

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: Asymmetric Learning with OCI-Modulated Negative Learning Rate.
    
    Hypothesis: Individuals with high OCI are hypersensitive to negative outcomes 
    (or lack of reward) as error signals. The learning rate for negative prediction 
    errors (alpha_neg) is boosted by the OCI score.
    
    Parameters:
    - alpha_pos: [0, 1] Learning rate for positive prediction errors (better than expected).
    - alpha_neg_base: [0, 1] Baseline learning rate for negative prediction errors.
    - alpha_neg_oci_boost: [0, 1] Increase in negative learning rate per unit of OCI.
    - beta: [0, 10] Inverse temperature.
    """
    alpha_pos, alpha_neg_base, alpha_neg_oci_boost, beta = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective negative learning rate
    alpha_neg = alpha_neg_base + (alpha_neg_oci_boost * oci_score)
    alpha_neg = np.clip(alpha_neg, 0.0, 1.0) # Ensure bounds

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy (Pure Model-Free) ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        s_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        r = reward[trial]
        
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        lr_2 = alpha_pos if delta_stage2 > 0 else alpha_neg
        q_stage2_mf[s_idx, a2] += lr_2 * delta_stage2
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        lr_1 = alpha_pos if delta_stage1 > 0 else alpha_neg
        q_stage1_mf[a1] += lr_1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Model-Based Learner with OCI-Modulated Uncertainty/Exploration
This model posits that OCI relates to intolerance of uncertainty. High OCI participants might be less willing to explore probabilistically and instead "freeze" or exploit known values more greedily. We model this by modulating the inverse temperature (`beta`) based on OCI. A higher `beta` means more deterministic (less random) choices.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: Model-Based Learner with OCI-Modulated Determinism (Beta).
    
    Hypothesis: High OCI is associated with intolerance of uncertainty. 
    This manifests as a higher inverse temperature (beta), making choices 
    more deterministic (exploitative) and reducing random exploration.
    
    Parameters:
    - learning_rate: [0, 1] Update rate.
    - beta_base: [0, 5] Baseline inverse temperature.
    - beta_oci_slope: [0, 5] Increase in beta (rigidity) per unit of OCI.
    - w: [0, 1] Weighting between model-based and model-free (fixed).
    """
    learning_rate, beta_base, beta_oci_slope, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Effective Beta: Higher OCI = Higher Beta = More deterministic/rigid
    beta = beta_base + (beta_oci_slope * oci_score)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        s_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        r = reward[trial]
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```