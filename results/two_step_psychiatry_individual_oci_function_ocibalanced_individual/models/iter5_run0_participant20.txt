Here are three new cognitive models exploring different mechanisms for how OCI scores might influence reinforcement learning in the two-step task.

### Model 1: OCI-Modulated Model-Based Weighting (Non-Linear)
This model tests the hypothesis that high OCI scores might lead to a rigid adherence to Model-Based (planning) strategies, but perhaps in a non-linear way (e.g., a threshold effect or exponential increase). Instead of a simple linear slope, we use an exponential scaling factor to link OCI to the weighting parameter `w`.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL where the balance between Model-Based (MB) and Model-Free (MF) control
    is modulated exponentially by the OCI score.
    
    Hypothesis: Higher OCI scores lead to a disproportionately stronger reliance on 
    model-based planning (over-thinking or rigid rule-following) compared to model-free 
    habitual learning.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate for value updates.
    beta: [0, 10] - Inverse temperature for softmax choice.
    w_base: [0, 1] - Baseline MB weighting for a hypothetical 0 OCI score.
    w_oci_scale: [0, 5] - Scaling factor for OCI influence.
    """
    learning_rate, beta, w_base, w_oci_scale = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate w using a sigmoid-like transformation to keep it bounded [0,1]
    # but allowing for non-linear sensitivity to OCI.
    # We use a simplified logistic function centered around base w.
    # If w_oci_scale is high, OCI pushes w strongly towards 1.
    w_raw = w_base + w_oci_scale * oci_score
    # Clip w to ensure it stays valid [0, 1]
    w = np.clip(w_raw, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Weighted mixture of Model-Based and Model-Free values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        qs_stage2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_stage2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # --- Learning ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])

        # TD Error Stage 1 (SARSA-like logic for MF)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

        # TD Error Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Punishment Sensitivity (Asymmetric Learning)
This model hypothesizes that OCI is related to an increased sensitivity to failure or lack of reward (punishment). Instead of modulating the learning rate generally, OCI specifically scales the learning rate when the prediction error is negative (i.e., when an expected reward is not received).

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL with OCI-modulated punishment sensitivity.
    
    Hypothesis: Individuals with higher OCI scores are more sensitive to negative 
    prediction errors (omitted rewards). This is modeled by scaling the learning rate 
    specifically for negative deltas based on the OCI score.
    
    Parameters:
    alpha_pos: [0, 1] - Learning rate for positive prediction errors.
    alpha_neg_base: [0, 1] - Base learning rate for negative prediction errors.
    alpha_neg_oci: [0, 1] - Additional boost to negative learning rate per unit of OCI.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - MB/MF weighting parameter.
    """
    alpha_pos, alpha_neg_base, alpha_neg_oci, beta, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate specific negative learning rate for this participant
    # We clip it to [0,1] to maintain stability
    alpha_neg = np.clip(alpha_neg_base + alpha_neg_oci * oci_score, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        qs_stage2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_stage2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # --- Learning ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])

        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        # Apply asymmetric learning rate
        lr_1 = alpha_pos if delta_stage1 >= 0 else alpha_neg
        q_stage1_mf[a1] += lr_1 * delta_stage1

        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        # Apply asymmetric learning rate
        lr_2 = alpha_pos if delta_stage2 >= 0 else alpha_neg
        q_stage2_mf[state_idx, a2] += lr_2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Choice Stickiness (Perseveration) Decay
This model explores the idea that OCI is related to "stuckness" or perseveration, but focuses on the *decay* of this stickiness. It posits that high OCI individuals have a choice trace that decays slower, meaning past choices influence current behavior for a longer window of time.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL with an OCI-modulated choice trace decay (stickiness memory).
    
    Hypothesis: High OCI relates to difficulty disengaging from past actions.
    Here, we implement a 'choice trace' that accumulates when an action is taken
    and decays on each trial. OCI modulates the decay rate: higher OCI = slower decay
    (longer memory of past choices), leading to persistent behavior.
    
    Parameters:
    learning_rate: [0, 1] - Value learning rate.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - MB/MF weighting.
    stickiness_weight: [0, 5] - How much the choice trace adds to the Q-value.
    decay_oci_mod: [0, 1] - Modulates how much OCI slows down the decay (0=no effect).
    """
    learning_rate, beta, w, stickiness_weight, decay_oci_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Base decay is fixed (e.g., 0.5), OCI reduces the decay rate (making trace last longer)
    # If decay_rate is 1.0, the trace persists perfectly. If 0.0, it vanishes instantly.
    # We construct it such that higher OCI -> higher decay_rate (closer to 1).
    base_decay = 0.2
    decay_rate = np.clip(base_decay + decay_oci_mod * oci_score, 0.0, 0.95)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Choice trace for stage 1 actions
    choice_trace = np.zeros(2)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combine MB, MF, and the Choice Trace (Stickiness)
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf + stickiness_weight * choice_trace
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        qs_stage2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_stage2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # --- Learning & Trace Update ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])

        # Update Choice Trace
        # Decay existing traces
        choice_trace *= decay_rate
        # Add to the current choice
        choice_trace[a1] += 1.0

        # Standard Q-learning updates
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```