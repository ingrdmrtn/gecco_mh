Here are three new cognitive models that incorporate the OCI (Obsessive-Compulsive Inventory) score into the decision-making process in distinct ways. These models explore hypotheses related to habit formation, uncertainty avoidance, and learning rate asymmetries.

### Model 1: Hybrid Model with OCI-Modulated Model-Based/Model-Free Weighting

This model hypothesizes that high OCI scores are associated with a heavier reliance on habitual (Model-Free) control, potentially at the expense of goal-directed (Model-Based) planning. It uses a weighting parameter $w$ that shifts based on the OCI score.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free with OCI-Modulated Weighting.
    
    Hypothesis: Higher OCI scores lead to a stronger reliance on Model-Free (habitual) 
    strategies over Model-Based (goal-directed) strategies. The mixing weight 'w' 
    is calculated as a logistic function of OCI.
    
    Parameters:
    - learning_rate: [0, 1] Update rate for Q-values.
    - beta: [0, 10] Inverse temperature for softmax.
    - w_intercept: [0, 1] Base weighting for Model-Based control (at OCI=0).
    - w_oci_slope: [0, 5] How strongly OCI reduces the Model-Based weight.
    """
    learning_rate, beta, w_intercept, w_oci_slope = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    # Calculate mixing weight w (0 = pure MF, 1 = pure MB)
    # Using a simplified linear clip to ensure bounds [0, 1].
    # Hypothesis: Higher OCI reduces w (pushes towards MF).
    w = w_intercept - (current_oci * w_oci_slope)
    w = np.clip(w, 0.0, 1.0)
    
    # Transition matrix (fixed for this task structure)
    # A->X (0.7), U->Y (0.7) roughly
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2) # Model-free Q-values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Model-free Q-values for stage 2 (aliens)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based Value Calculation: V(state) = max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value: w * MB + (1-w) * MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # --- Stage 2 Policy ---
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # --- Learning Updates ---
        r = reward[trial]
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        
        # TD Error Stage 2
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # TD Error Stage 1 (SARSA-style update for MF)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Asymmetric Learning Rates Modulated by OCI

This model suggests that OCI relates to how individuals process positive versus negative feedback. Specifically, it tests if higher OCI scores lead to "over-learning" from negative outcomes (punishment sensitivity) or rigid updating, represented by modulating the learning rate specifically for prediction errors.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free RL with OCI-Modulated Asymmetric Learning Rates.
    
    Hypothesis: OCI affects how prediction errors are processed. Specifically,
    this model allows the learning rate for negative prediction errors (worse than expected)
    to scale with OCI, reflecting potential hypersensitivity to failure or lack of reward.
    
    Parameters:
    - lr_base: [0, 1] Base learning rate.
    - beta: [0, 10] Inverse temperature.
    - oci_lr_scale: [0, 5] Multiplier for how much OCI boosts learning from negative PEs.
    - eligibility_trace: [0, 1] Decay factor for updating Stage 1 based on Stage 2 outcome.
    """
    lr_base, beta, oci_lr_scale, eligibility_trace = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # --- Updating ---
        r = reward[trial]
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        
        # Calculate Prediction Error at Stage 2
        pe_2 = r - q_stage2[state_idx, a2]
        
        # Determine effective learning rate
        # If PE is negative (disappointment), OCI increases the learning rate
        if pe_2 < 0:
            eff_lr = lr_base * (1.0 + current_oci * oci_lr_scale)
            eff_lr = np.clip(eff_lr, 0, 1) # Ensure bounds
        else:
            eff_lr = lr_base
            
        # Update Stage 2
        q_stage2[state_idx, a2] += eff_lr * pe_2
        
        # Update Stage 1 using eligibility trace (TD(lambda) simplified)
        # We use the PE from stage 2 to update stage 1 directly
        q_stage1[a1] += eff_lr * eligibility_trace * pe_2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Uncertainty-Based Exploration Penalty Modulated by OCI

This model posits that high OCI is linked to intolerance of uncertainty. It tracks the variance (uncertainty) of the Q-values. High OCI participants are modeled to have a "safety bias," actively penalizing actions where the outcome variance is high, preferring "known" quantities even if their mean value is slightly lower.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Mean-Variance RL with OCI-Driven Uncertainty Avoidance.
    
    Hypothesis: OCI is linked to Intolerance of Uncertainty. This model tracks not just 
    expected value (Q), but the variance of rewards (V). The decision policy subtracts 
    uncertainty (V) weighted by OCI, making high-OCI participants avoid uncertain options.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for both Mean and Variance.
    - beta: [0, 10] Inverse temperature.
    - uncertainty_penalty_base: [0, 5] Base aversion to variance.
    - oci_penalty_boost: [0, 5] Additional aversion to variance scaled by OCI.
    """
    learning_rate, beta, uncertainty_penalty_base, oci_penalty_boost = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate total penalty weight
    k_uncertainty = uncertainty_penalty_base + (current_oci * oci_penalty_boost)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q: Expected Mean Reward
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    # V: Expected Variance (Uncertainty) initialized to 1 (high uncertainty initially)
    v_stage1 = np.ones(2)
    v_stage2 = np.ones((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Utility = Q - k * sqrt(V) (Penalty for standard deviation/uncertainty)
        utility_1 = q_stage1 - k_uncertainty * np.sqrt(v_stage1)
        
        exp_u1 = np.exp(beta * utility_1)
        probs_1 = exp_u1 / np.sum(exp_u1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        
        # --- Stage 2 Choice ---
        utility_2 = q_stage2[state_idx] - k_uncertainty * np.sqrt(v_stage2[state_idx])
        
        exp_u2 = np.exp(beta * utility_2)
        probs_2 = exp_u2 / np.sum(exp_u2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # --- Updating ---
        r = reward[trial]
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        
        # Update Stage 2 Mean and Variance
        pe_2 = r - q_stage2[state_idx, a2]
        q_stage2[state_idx, a2] += learning_rate * pe_2
        
        # Variance update: V_new = V_old + lr * ((R - Q_old)^2 - V_old)
        # This approximates the running variance of the reward
        var_error_2 = (pe_2**2) - v_stage2[state_idx, a2]
        v_stage2[state_idx, a2] += learning_rate * var_error_2
        
        # Update Stage 1 (Model-Free TD)
        # Using Stage 2 Q-value as the target for Stage 1
        target_s1 = q_stage2[state_idx, a2]
        pe_1 = target_s1 - q_stage1[a1]
        q_stage1[a1] += learning_rate * pe_1
        
        # Propagate variance estimate to stage 1
        # Simple heuristic: Stage 1 variance tracks Stage 2 variance observed
        var_error_1 = v_stage2[state_idx, a2] - v_stage1[a1]
        v_stage1[a1] += learning_rate * var_error_1
        
        # Ensure variance stays non-negative (numerical stability)
        v_stage1[a1] = max(v_stage1[a1], 0.0001)
        v_stage2[state_idx, a2] = max(v_stage2[state_idx, a2], 0.0001)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```