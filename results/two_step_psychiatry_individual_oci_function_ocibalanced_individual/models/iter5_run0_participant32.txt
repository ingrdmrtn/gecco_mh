Here are 3 new cognitive models based on the provided template and feedback.

### Model 1: Hybrid Model with OCI-Modulated Perseveration
This model expands on your "Best Model" (which was Pure Model-Based) by re-introducing the Model-Free component (creating a Hybrid learner). It tests the hypothesis that OCI-driven perseveration ("stickiness") operates alongside a trade-off between goal-directed (MB) and habitual (MF) systems.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: Hybrid Model (MB/MF) with OCI-Modulated Perseveration.
    
    Hypothesis: Extends the previous best model by adding a Model-Free component.
    It assumes participants trade off between MB and MF systems (controlled by w),
    but OCI specifically drives a 'stickiness' or perseveration bias regardless of 
    the learning strategy.
    
    Parameters:
    learning_rate: [0, 1] Update rate for MF values.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Weighting parameter (0 = Pure MF, 1 = Pure MB).
    pers_base: [-1, 1] Baseline stickiness (positive) or switching (negative).
    pers_oci_slope: [-2, 2] How strongly OCI increases/decreases stickiness.
    """
    learning_rate, beta, w, pers_base, pers_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate perseveration weight based on OCI
    perseveration_weight = pers_base + (pers_oci_slope * oci_score)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Initialize Q-values
    q_stage1_mf = np.zeros(2)      # Model-Free values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Model-Free values for stage 2 (Aliens)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1 

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        
        # 1. Model-Based Value Calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Hybrid Value Calculation
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # 3. Add Perseveration
        if last_action_1 != -1:
            q_net[int(last_action_1)] += perseveration_weight
            
        # 4. Action Selection
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        last_action_1 = action_1[trial]
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Updates ---
        
        # TD(0) update for Stage 1 (using Stage 2 MF value as proxy for future reward)
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1
        
        # TD(0) update for Stage 2 (using actual reward)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model with OCI-Distorted Transition Beliefs
This model investigates if OCI affects the *content* of the model-based planning rather than the weight or stickiness. Specifically, it tests if high OCI leads to distorted beliefs about the transition probabilities (e.g., believing the structure is more deterministic/rigid than it actually is).

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: Hybrid Model with OCI-Distorted Transition Beliefs.
    
    Hypothesis: High OCI relates to an intolerance of uncertainty or desire for control.
    This model posits that OCI modifies the subject's internal model of the transition matrix.
    High OCI might perceive the common transition (0.7) as more certain (closer to 1.0)
    or conversely, more chaotic (closer to 0.5), distorting MB value calculations.
    
    Parameters:
    learning_rate: [0, 1] Update rate.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Weighting parameter.
    trans_distortion: [-0.3, 0.3] Distortion of transition probability scaled by OCI.
                      Positive = believes transitions are more deterministic.
                      Negative = believes transitions are more random.
    """
    learning_rate, beta, w, trans_distortion = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate Subjective Transition Probability
    # Base is 0.7. We add OCI * distortion. 
    # We clip to ensure it stays within a reasonable probability range [0.5, 0.99]
    subjective_p = 0.7 + (trans_distortion * oci_score)
    subjective_p = np.clip(subjective_p, 0.5, 0.99)
    
    # Construct subjective transition matrix
    transition_matrix = np.array([[subjective_p, 1-subjective_p], 
                                  [1-subjective_p, subjective_p]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        
        # MB calculation uses the DISTORTED matrix
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])

        # --- Stage 2 ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model with OCI-Linked "Global Rigidity"
This model proposes that OCI doesn't just affect one parameter, but represents a global "Rigidity" factor that simultaneously degrades goal-directed control (lowering `w`) AND increases motor perseveration. This couples the two mechanisms via a single OCI-sensitivity parameter.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: Hybrid Model with OCI-Linked Global Rigidity.
    
    Hypothesis: OCI represents a global 'Rigidity' phenotype. Instead of testing w 
    or perseveration separately, this model assumes OCI simultaneously:
    1. Reduces Model-Based control (decreases w).
    2. Increases repetitive behavior (increases perseveration).
    These are linked by a single 'oci_rigidity' parameter.
    
    Parameters:
    learning_rate: [0, 1] Update rate.
    beta: [0, 10] Inverse temperature.
    w_base: [0, 1] Baseline model-based weighting (at 0 OCI).
    pers_base: [-1, 1] Baseline stickiness (at 0 OCI).
    oci_rigidity: [0, 2] Factor linking OCI to both lower w and higher stickiness.
    """
    learning_rate, beta, w_base, pers_base, oci_rigidity = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate linked parameters
    # High OCI -> Lower w (more habitual) AND Higher perseveration (more sticky)
    w_curr = w_base - (oci_rigidity * oci_score)
    w_curr = np.clip(w_curr, 0.0, 1.0) # Ensure w stays valid
    
    pers_curr = pers_base + (oci_rigidity * oci_score)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Use the OCI-modified w
        q_net = (w_curr * q_stage1_mb) + ((1 - w_curr) * q_stage1_mf)
        
        # Use the OCI-modified perseveration
        if last_action_1 != -1:
            q_net[int(last_action_1)] += pers_curr
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        last_action_1 = action_1[trial]
        state_idx = int(state[trial])

        # --- Stage 2 ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```