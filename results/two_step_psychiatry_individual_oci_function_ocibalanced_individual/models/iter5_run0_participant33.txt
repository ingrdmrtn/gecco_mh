Here are 3 new cognitive models exploring different mechanisms for how OCI scores might influence decision-making in the two-step task, specifically focusing on the balance between model-based/model-free control and perseveration.

### Model 1: OCI-Modulated Model-Based Weight ($w$)
This model hypothesizes that individuals with higher OCI scores rely more heavily on habit (Model-Free) rather than planning (Model-Based). This aligns with theories of OCD as a disorder of habit or compulsivity, where goal-directed control is diminished. The parameter `w` (weighting between MB and MF) is linearly modulated by OCI.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Model-Based Weight.
    
    Hypothesis: High OCI is associated with reduced model-based control (goal-directed)
    and increased reliance on model-free (habitual) systems.
    The weighting parameter 'w' is modeled as a logistic function of OCI to ensure it stays within [0, 1].
    
    Parameters:
    - learning_rate: [0, 1] Update rate for Q-values.
    - beta: [0, 10] Inverse temperature for softmax.
    - lambda_eligibility: [0, 1] Eligibility trace decay.
    - w_intercept: [0, 10] Base log-odds of using Model-Based control.
    - w_oci_slope: [-5, 5] Effect of OCI on the log-odds of w. Negative slope implies High OCI -> Lower w (more habitual).
    
    w = sigmoid(w_intercept + w_oci_slope * oci)
    """
    learning_rate, beta, lambda_eligibility, w_intercept, w_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate w based on OCI
    # Using logistic function to bound w between 0 and 1
    # We shift the input range for stability
    logit_w = w_intercept + w_oci_slope * oci_score
    w = 1.0 / (1.0 + np.exp(-logit_w))

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2) # Model-free Q-values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Model-free Q-values for stage 2 (also used for MB calculation)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        # Model-Based Value: V_MB(s1) = T(s1, s2) * max(Q_MF(s2))
        max_q_stage2 = np.max(q_stage2_mf, axis=1) # Max value of each state [max(S1), max(S2)]
        q_stage1_mb = transition_matrix @ max_q_stage2 # Expected value based on transition probs
        
        # Integrated Value: Q_net = w * Q_MB + (1-w) * Q_MF
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        # Pure Model-Free at stage 2
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        # Stage 2 RPE
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        # Stage 1 RPE (SARSA-style for MF)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        
        # Update Stage 2
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Update Stage 1 (MF) with eligibility trace
        q_stage1_mf[a1] += learning_rate * delta_stage1 + (learning_rate * lambda_eligibility * delta_stage2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Driven Choice Perseveration (Stickiness)
This model tests the hypothesis that OCI is related to "stickiness" or choice perseverationâ€”a tendency to repeat the previous action regardless of reward history. This is often linked to the repetitive behaviors seen in OCD. Here, the stickiness parameter is directly scaled by the OCI score.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Driven Choice Perseveration.
    
    Hypothesis: High OCI is associated with higher behavioral rigidity or 'stickiness'.
    The model includes a choice autocorrelation bonus (stickiness) added to the Q-values
    in the first stage choice. The magnitude of this bonus is determined by OCI.
    
    Parameters:
    - learning_rate: [0, 1]
    - beta: [0, 10]
    - w: [0, 1] Weight between MB and MF.
    - stickiness_base: [-2, 2] Baseline tendency to repeat choice.
    - stickiness_oci_gain: [0, 5] Additional stickiness per unit of OCI score.
    
    Total Stickiness = stickiness_base + (stickiness_oci_gain * oci)
    """
    learning_rate, beta, w, stickiness_base, stickiness_oci_gain = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective stickiness
    eff_stickiness = stickiness_base + (stickiness_oci_gain * oci_score)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1 # Indicator for previous action

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness bonus
        logits = beta * q_net_stage1
        if last_action_1 != -1:
            logits[last_action_1] += eff_stickiness
            
        # Softmax
        # Subtract max for numerical stability
        logits = logits - np.max(logits)
        exp_q1 = np.exp(logits)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        # Standard TD(1) / Eligibility Trace
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        q_stage1_mf[a1] += learning_rate * delta_stage1 + (learning_rate * delta_stage2) # Lambda=1 assumed here for simplicity given param limit
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Specific Second-Stage Learning Rate
This model proposes that high OCI scores specifically affect how information is updated at the second stage (the direct outcome stage). This could reflect a heightened sensitivity to immediate outcomes or "checking" behavior where the immediate result is over-weighted.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Specific Second-Stage Learning Rate.
    
    Hypothesis: OCI affects the learning rate specifically for the second stage (the immediate reward stage),
    potentially reflecting altered sensitivity to direct feedback vs abstract structure.
    
    Parameters:
    - lr_stage1: [0, 1] Learning rate for stage 1 (abstract choice).
    - lr_stage2_base: [0, 1] Base learning rate for stage 2.
    - lr_stage2_oci_slope: [-1, 1] Modification of stage 2 learning rate by OCI.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Model-based weight.
    
    lr_stage2 = lr_stage2_base + (lr_stage2_oci_slope * oci) (clipped to [0,1])
    """
    lr_stage1, lr_stage2_base, lr_stage2_oci_slope, beta, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate OCI-modulated Stage 2 learning rate
    lr_stage2 = lr_stage2_base + (lr_stage2_oci_slope * oci_score)
    # Clip to valid range [0, 1]
    if lr_stage2 < 0: lr_stage2 = 0.0
    if lr_stage2 > 1: lr_stage2 = 1.0

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        
        # Use OCI-specific learning rate for Stage 2
        q_stage2_mf[s_idx, a2] += lr_stage2 * delta_stage2
        
        # Use standard learning rate for Stage 1
        # Assuming lambda=1 for simplicity to save a parameter
        q_stage1_mf[a1] += lr_stage1 * delta_stage1 + (lr_stage1 * delta_stage2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```