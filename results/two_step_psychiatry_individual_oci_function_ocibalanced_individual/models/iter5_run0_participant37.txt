Here are three new cognitive models that incorporate the OCI score into the decision-making process in the two-step task. These models explore mechanisms distinct from the previous "stickiness" or simple parameter modulation approaches, focusing instead on how OCI might affect the balance between model-based/model-free control, learning from prediction errors, and uncertainty estimation.

### Model 1: OCI-Modulated Model-Based Weighting (Hybrid Model)
This model hypothesizes that individuals with higher OCI scores rely more heavily on model-based (planning) strategies versus model-free (habitual) strategies. This aligns with theories suggesting OCD involves over-engagement of goal-directed control or an inability to switch to efficient habits. The weighting parameter $w$ (mixing coefficient) is a function of the OCI score.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Model-Based Weighting.
    
    This model implements a hybrid reinforcement learning agent where the balance
    between Model-Based (MB) and Model-Free (MF) control is determined by the OCI score.
    Higher OCI leads to a different weighting (w) of the MB system.
    
    Bounds:
    learning_rate: [0,1]
    beta: [0,10]
    w_base: [0,1] - Baseline weight for model-based control (at OCI=0).
    w_oci_scale: [-1,1] - How much OCI score shifts the weight towards MB or MF.
    """
    learning_rate, beta, w_base, w_oci_scale = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate the mixing weight w based on OCI
    # We constrain w to be between 0 (pure MF) and 1 (pure MB)
    w = w_base + (w_oci_scale * oci_score)
    w = np.clip(w, 0.0, 1.0)

    # Fixed transition matrix as per task description (70% common, 30% rare)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2) # Model-free values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for stage 2 (also used for MB planning)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        
        # Model-Based Value Calculation: V_MB(s1) = T * max(Q(s2))
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value: Q_net = w * Q_MB + (1-w) * Q_MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax for Choice 1
        logits_1 = beta * q_net
        exp_q1 = np.exp(logits_1 - np.max(logits_1)) # Numerical stability
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        if a1 != -1:
            p_choice_1[trial] = probs_1[a1]
        else:
            p_choice_1[trial] = 1.0

        # Handle missing data
        if a1 == -1 or state[trial] == -1 or action_2[trial] == -1:
            continue
            
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 2 Policy ---
        # Stage 2 is purely model-free (terminal state)
        logits_2 = beta * q_stage2_mf[s_idx, :]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        
        # 1. Update Stage 2 Q-values (RPE)
        # delta2 = r - Q(s2, a2)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # 2. Update Stage 1 MF Q-values (TD-learning)
        # delta1 = Q(s2, a2) - Q(s1, a1)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    p_choice_1 = np.maximum(p_choice_1, eps)
    p_choice_2 = np.maximum(p_choice_2, eps)
    log_loss = -(np.sum(np.log(p_choice_1)) + np.sum(np.log(p_choice_2)))
    return log_loss
```

### Model 2: OCI-Driven Asymmetric Learning Rates (Perfectionism)
This model posits that high OCI scores (associated with perfectionism and fear of failure) lead to an asymmetry in how positive versus negative outcomes are learned. Specifically, high OCI might amplify learning from negative prediction errors (punishment sensitivity) or dampen learning from rewards (anhedonia/caution).

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Driven Asymmetric Learning Rates.
    
    This model assumes separate learning rates for positive (better than expected)
    and negative (worse than expected) prediction errors. The OCI score modulates
    the learning rate specifically for negative prediction errors (loss sensitivity).
    
    Bounds:
    lr_pos: [0,1] - Learning rate for positive prediction errors.
    lr_neg_base: [0,1] - Baseline learning rate for negative prediction errors.
    lr_neg_oci_slope: [-1,1] - Modulation of negative LR by OCI.
    beta: [0,10]
    w_mb: [0,1] - Fixed weight for model-based control.
    """
    lr_pos, lr_neg_base, lr_neg_oci_slope, beta, w_mb = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate OCI-modulated negative learning rate
    lr_neg = lr_neg_base + (lr_neg_oci_slope * oci_score)
    lr_neg = np.clip(lr_neg, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w_mb * q_stage1_mb + (1 - w_mb) * q_stage1_mf
        
        logits_1 = beta * q_net
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        if a1 != -1:
            p_choice_1[trial] = probs_1[a1]
        else:
            p_choice_1[trial] = 1.0

        if a1 == -1 or state[trial] == -1 or action_2[trial] == -1:
            continue
            
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 2 Choice ---
        logits_2 = beta * q_stage2_mf[s_idx, :]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning with Asymmetry ---
        
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        current_lr2 = lr_pos if delta_stage2 >= 0 else lr_neg
        q_stage2_mf[s_idx, a2] += current_lr2 * delta_stage2
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        current_lr1 = lr_pos if delta_stage1 >= 0 else lr_neg
        q_stage1_mf[a1] += current_lr1 * delta_stage1

    eps = 1e-10
    p_choice_1 = np.maximum(p_choice_1, eps)
    p_choice_2 = np.maximum(p_choice_2, eps)
    log_loss = -(np.sum(np.log(p_choice_1)) + np.sum(np.log(p_choice_2)))
    return log_loss
```

### Model 3: OCI-Modulated Eligibility Trace (Decay Rate)
This model introduces an eligibility trace parameter ($\lambda$) for the update of the first-stage values. In standard TD($\lambda$), $\lambda$ controls how much credit is assigned to the first action based on the second-stage reward. Here, OCI modulates $\lambda$. A high $\lambda$ (closer to 1) means the agent updates stage 1 values directly based on the final reward (ignoring the stage 2 value estimate), effectively collapsing the two steps. This could reflect a compulsion to link outcomes directly to initial choices, bypassing the intermediate structure.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Eligibility Trace (TD-Lambda).
    
    This model uses an eligibility trace (lambda) to update stage 1 values.
    Lambda determines the extent to which the Stage 1 value is updated by the 
    Stage 2 prediction error (direct reinforcement) vs the Stage 1 prediction error.
    OCI modulates this lambda parameter.
    
    Bounds:
    learning_rate: [0,1]
    beta: [0,10]
    lambda_base: [0,1] - Baseline eligibility trace decay.
    lambda_oci_slope: [-1,1] - Effect of OCI on lambda.
    w_mb: [0,1] - Model-based weight.
    """
    learning_rate, beta, lambda_base, lambda_oci_slope, w_mb = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate OCI-modulated lambda
    lam = lambda_base + (lambda_oci_slope * oci_score)
    lam = np.clip(lam, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w_mb * q_stage1_mb + (1 - w_mb) * q_stage1_mf
        
        logits_1 = beta * q_net
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        if a1 != -1:
            p_choice_1[trial] = probs_1[a1]
        else:
            p_choice_1[trial] = 1.0

        if a1 == -1 or state[trial] == -1 or action_2[trial] == -1:
            continue
            
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 2 Choice ---
        logits_2 = beta * q_stage2_mf[s_idx, :]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning with Eligibility Trace ---
        
        # 1. Calculate RPE at Stage 2
        # delta2 = Reward - Q_stage2(state, choice2)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        # 2. Calculate RPE at Stage 1
        # delta1 = Q_stage2(state, choice2) - Q_stage1(choice1)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        
        # 3. Update Stage 2
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # 4. Update Stage 1 using TD(lambda) logic
        # The update combines the immediate step error (delta1) and the 
        # discounted second step error (lambda * delta2)
        # Q1_new = Q1_old + alpha * (delta1 + lambda * delta2)
        q_stage1_mf[a1] += learning_rate * (delta_stage1 + lam * delta_stage2)

    eps = 1e-10
    p_choice_1 = np.maximum(p_choice_1, eps)
    p_choice_2 = np.maximum(p_choice_2, eps)
    log_loss = -(np.sum(np.log(p_choice_1)) + np.sum(np.log(p_choice_2)))
    return log_loss
```