Here are three new cognitive models that incorporate the OCI score to explain decision-making in the two-step task. These models explore different mechanisms: Model-Based/Model-Free weighting modulation, learning rate asymmetry (habit formation), and noise modulation.

### Model 1: OCI-Modulated Model-Based vs. Model-Free Weighting
This model tests the hypothesis that high OCI scores are associated with a rigid reliance on habit (Model-Free) over goal-directed planning (Model-Based), or vice versa. Specifically, it allows the mixing parameter $w$ (which balances MB and MF control) to be a linear function of the OCI score.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free learner where the weighting parameter 'w'
    is modulated by the OCI score.
    
    Hypothesis: High OCI might lead to over-reliance on habits (low w) or 
    excessive planning (high w).
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    w_base: [0, 1] - Baseline weighting for Model-Based control (0=Pure MF, 1=Pure MB)
    w_oci_slope: [-1, 1] - How OCI changes the weighting.
    """
    learning_rate, beta, w_base, w_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective w, clamped between 0 and 1
    w = w_base + (w_oci_slope * oci_score)
    w = np.clip(w, 0.0, 1.0)
    
    # Fixed transition matrix as per task description
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # 2 planets, 2 aliens each

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        
        # Model-Based Value: V(s') = max(Q_stage2(s', :))
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        # Q_MB(s, a) = T(s, a, s') * V(s')
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Value: Q_net = w * Q_MB + (1-w) * Q_MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Decision ---
        s_idx = int(state[trial])
        
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        r = reward[trial]
        
        # Update Stage 2 Q-values (Standard Rescorla-Wagner)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Update Stage 1 MF Q-values (TD(0) learning)
        # The reward for stage 1 is the value of the state reached in stage 2
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Asymmetric Learning Rates Modulated by OCI
This model hypothesizes that high OCI scores relate to how individuals process positive versus negative feedback. Specifically, it proposes that OCI modulates the learning rate for *negative* prediction errors (punishment/omission of reward), potentially reflecting a hypersensitivity to failure or "checking" behavior driven by negative outcomes.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner with asymmetric learning rates for positive and negative
    prediction errors, where the negative learning rate is modulated by OCI.
    
    Hypothesis: High OCI individuals may over-learn from negative outcomes 
    (fear of failure/loss aversion).
    
    Bounds:
    alpha_pos: [0, 1] - Learning rate for positive prediction errors (R > Expected)
    alpha_neg_base: [0, 1] - Base learning rate for negative prediction errors
    beta: [0, 10]
    oci_neg_mod: [0, 2] - Multiplier for OCI effect on negative learning rate
    """
    alpha_pos, alpha_neg_base, beta, oci_neg_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective negative learning rate
    # We clip it to ensure it stays a valid learning rate [0, 1]
    alpha_neg_eff = alpha_neg_base * (1 + oci_neg_mod * oci_score)
    alpha_neg_eff = np.clip(alpha_neg_eff, 0.0, 1.0)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Pure Model-Free setup
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Choice ---
        s_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        r = reward[trial]
        
        # Stage 2 Update
        delta2 = r - q_stage2[s_idx, a2]
        if delta2 >= 0:
            q_stage2[s_idx, a2] += alpha_pos * delta2
        else:
            q_stage2[s_idx, a2] += alpha_neg_eff * delta2
            
        # Stage 1 Update (TD)
        # Using the updated stage 2 value as the target
        delta1 = q_stage2[s_idx, a2] - q_stage1[a1]
        if delta1 >= 0:
            q_stage1[a1] += alpha_pos * delta1
        else:
            q_stage1[a1] += alpha_neg_eff * delta1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Choice Randomness (Inverse Temperature)
This model posits that OCI affects the exploration-exploitation trade-off. High OCI might lead to more deterministic (rigid) behavior, or conversely, more erratic behavior due to anxiety. Here, the inverse temperature parameter $\beta$ is a function of the OCI score. A higher $\beta$ means more deterministic choices (lower noise), while a lower $\beta$ means more random choices.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free learner where the inverse temperature (beta)
    is modulated by the OCI score.
    
    Hypothesis: High OCI relates to choice rigidity (higher beta) or anxiety-driven
    randomness (lower beta).
    
    Bounds:
    learning_rate: [0, 1]
    w: [0, 1] - Mixing weight (fixed for this model to isolate beta effects)
    beta_base: [0, 10] - Baseline inverse temperature
    beta_oci_slope: [-5, 5] - How OCI alters decision noise.
    """
    learning_rate, w, beta_base, beta_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective beta
    # Ensure beta stays non-negative.
    beta_eff = beta_base + (beta_oci_slope * oci_score)
    beta_eff = np.maximum(beta_eff, 0.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 ---
        # MB Calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Weighted Q
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax with OCI-modulated beta
        exp_q1 = np.exp(beta_eff * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 ---
        s_idx = int(state[trial])
        
        exp_q2 = np.exp(beta_eff * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        r = reward[trial]
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```