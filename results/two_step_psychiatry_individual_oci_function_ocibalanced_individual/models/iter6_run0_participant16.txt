Here are 3 new cognitive models based on the provided template and task description, exploring different mechanisms by which Obsessive-Compulsive Inventory (OCI) scores might influence decision-making parameters.

### Model 1: OCI-Modulated Model-Based/Model-Free Trade-off (W)
This model hypothesizes that individuals with higher OCI scores rely more heavily on habit-based (Model-Free) control rather than goal-directed (Model-Based) planning. This is a common hypothesis in computational psychiatry linking compulsivity to habits. The mixing weight `w` is derived from a base parameter and modulated by the OCI score.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Model-Based/Model-Free Trade-off.
    
    Hypothesis: Higher OCI scores are associated with a shift towards habitual (Model-Free) control.
    The mixing weight 'w' (where 1 is fully Model-Based and 0 is fully Model-Free)
    is reduced as OCI increases.

    Parameters:
    lr: [0, 1] Learning rate for value updates.
    beta: [0, 10] Inverse temperature for softmax.
    w_base: [0, 1] Baseline mixing weight (for OCI=0).
    oci_w_slope: [0, 1] Strength of reduction in 'w' due to OCI.
    """
    lr, beta, w_base, oci_w_slope = model_parameters
    n_trials = len(action_1)
    oci = oci[0]
  
    # Fixed transition matrix as per task description
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)      # Model-free values for Stage 1
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for Stage 2 (States X, Y; Actions 0, 1)

    # Calculate effective w based on OCI. Constrain to [0, 1].
    # As OCI increases, w decreases (more habitual).
    w = w_base - (oci * oci_w_slope)
    if w < 0: w = 0
    if w > 1: w = 1

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        # Model-Based Value: Transition * Max(Stage 2 Values)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        # --- Learning ---
        # Update Stage 2 Q-values (Standard Rescorla-Wagner)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += lr * delta_stage2
        
        # Update Stage 1 Model-Free Q-values (TD-learning using Stage 2 value)
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += lr * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Dependent Learning Rate Asymmetry
This model suggests that OCI relates to how individuals process positive versus negative prediction errors. Specifically, high OCI might lead to hypersensitivity to negative outcomes (punishment/lack of reward) or "safety signal" learning. Here, the learning rate for negative prediction errors (when reward is 0) is modulated by OCI.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Dependent Learning Rate Asymmetry.
    
    Hypothesis: OCI modulates sensitivity to negative prediction errors (learning from missing rewards).
    The model splits learning rates into positive (lr_pos) and negative (lr_neg).
    The negative learning rate is a function of OCI.

    Parameters:
    lr_pos: [0, 1] Learning rate for positive prediction errors (RPE > 0).
    lr_neg_base: [0, 1] Base learning rate for negative prediction errors (RPE <= 0).
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Mixing weight (constant).
    oci_neg_scale: [0, 5] Scaling factor for OCI's impact on negative learning rate.
    """
    lr_pos, lr_neg_base, beta, w, oci_neg_scale = model_parameters
    n_trials = len(action_1)
    oci = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    # Calculate effective negative learning rate
    # Higher OCI might amplify learning from failure/omission
    lr_neg = lr_neg_base * (1 + oci * oci_neg_scale)
    # Clip to ensure stability [0, 1]
    if lr_neg > 1: lr_neg = 1.0

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        # --- Learning ---
        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        current_lr = lr_pos if delta_stage2 > 0 else lr_neg
        q_stage2_mf[state_idx, int(action_2[trial])] += current_lr * delta_stage2
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        # Note: Using the same LR logic for stage 1 updates based on stage 2 value differences
        current_lr_s1 = lr_pos if delta_stage1 > 0 else lr_neg
        q_stage1_mf[int(action_1[trial])] += current_lr_s1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Eligibility Trace (Lambda)
This model introduces an eligibility trace parameter ($\lambda$) for Temporal Difference learning. This parameter controls how much credit the first-stage choice gets for the second-stage outcome. The hypothesis is that high OCI might be associated with difficulty in assigning credit over time (or conversely, hyper-binding actions to outcomes), modulated by `oci_lambda_mod`.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Eligibility Trace (Lambda).
    
    Hypothesis: OCI affects the efficiency of credit assignment between stages.
    This model uses TD(lambda) logic where the update of Stage 1 depends on 
    the reward received at Stage 2, mediated by eligibility parameter lambda.
    Lambda is modulated by the OCI score.

    Parameters:
    lr: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Mixing weight.
    lambda_base: [0, 1] Base eligibility trace parameter.
    oci_lambda_mod: [-1, 1] Modulation of lambda by OCI (can increase or decrease).
    """
    lr, beta, w, lambda_base, oci_lambda_mod = model_parameters
    n_trials = len(action_1)
    oci = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    # Calculate effective lambda
    # We use a tanh transform or simple clipping to keep it in [0, 1]
    # Here we use simple linear scaling + clipping
    lambda_param = lambda_base + (oci * oci_lambda_mod)
    if lambda_param < 0: lambda_param = 0.0
    if lambda_param > 1: lambda_param = 1.0

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        # --- Learning (TD-Lambda style) ---
        
        # 1. Prediction error at stage 2 (Reward - Q_stage2)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        
        # 2. Prediction error at stage 1 (Q_stage2 - Q_stage1)
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        
        # Update Stage 2
        q_stage2_mf[state_idx, int(action_2[trial])] += lr * delta_stage2
        
        # Update Stage 1
        # Standard TD(0) update:
        # q_stage1_mf[action_1] += lr * delta_stage1
        # TD(1) / Monte Carlo like update adds the stage 2 error scaled by lambda:
        q_stage1_mf[int(action_1[trial])] += lr * (delta_stage1 + lambda_param * delta_stage2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```