Here are three new cognitive models that introduce different mechanisms for how OCI scores might influence decision-making in this two-step task.

### Model 1: Hybrid Learner with OCI-Modulated Reward Sensitivity
This model hypothesizes that high OCI scores relate to an altered sensitivity to rewards. Rather than changing the learning rate (how fast one learns) or the mixing weight $w$ (model-based vs model-free), this model suggests that individuals with higher OCI might perceive the subjective utility of the reward differently (e.g., dampening the "joy" of success or amplifying the signal), affecting the magnitude of the prediction error.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: Hybrid Learner with OCI-Modulated Reward Sensitivity.

    Hypothesis: OCI score scales the effective reward magnitude. High OCI might
    dampen or amplify the perceived value of the outcome, affecting the size of
    prediction errors in both model-free and model-based updates.

    Parameters:
    - learning_rate: [0, 1] Learning rate for value updates.
    - beta: [0, 10] Inverse temperature for softmax.
    - w: [0, 1] Weighting parameter (0 = pure MF, 1 = pure MB).
    - reward_sens_base: [0, 2] Baseline sensitivity to reward.
    - reward_sens_oci: [-1, 1] Modulation of reward sensitivity by OCI.
    """
    learning_rate, beta, w, reward_sens_base, reward_sens_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective reward sensitivity
    # We clip to ensure sensitivity doesn't go negative and invert the logic weirdly
    r_sens = max(0.0, reward_sens_base + (reward_sens_oci * oci_score))

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based Value
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Choice ---
        s_idx = int(state[trial])
        
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        # Scale the observed reward by the OCI-modulated sensitivity
        effective_reward = reward[trial] * r_sens
        
        # Stage 2 Update (Model-Free)
        delta_stage2 = effective_reward - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 Update (Model-Free TD(1) style)
        # Using the actual stage 2 value (effective reward) to drive stage 1 update
        delta_stage1 = effective_reward - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Model-Based Learner with OCI-driven Transition Learning Rate
Standard models assume the transition matrix is fixed (70/30). However, participants might try to learn the transition probabilities online. This model posits that high OCI (associated with uncertainty intolerance or checking) leads to a higher "state learning rate," meaning these individuals update their internal model of the spaceship-planet transitions more aggressively after rare transitions, making their model-based system more volatile.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: Model-Based Learner with OCI-driven Transition Learning Rate.

    Hypothesis: The transition matrix is not fixed but learned. 
    OCI modulates the learning rate for state transitions (lr_state). 
    Higher OCI leads to faster updating of the internal model of the world 
    (hyper-responsiveness to rare transitions).

    Parameters:
    - learning_rate_reward: [0, 1] Learning rate for reward values (Q-values).
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weighting (0=MF, 1=MB).
    - lr_state_base: [0, 1] Baseline learning rate for transition probabilities.
    - lr_state_oci_slope: [0, 1] Increase in state learning rate per unit OCI.
    """
    learning_rate_reward, beta, w, lr_state_base, lr_state_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate OCI-modulated state learning rate
    lr_state = np.clip(lr_state_base + lr_state_oci_slope * oci_score, 0.0, 1.0)

    # Initialize learned transition matrix (start with uniform or slight prior)
    # Rows: Action (0 or 1), Cols: State (0 or 1)
    # Initializing at 0.5 implies no initial knowledge of the structure
    est_transitions = np.array([[0.5, 0.5], [0.5, 0.5]]) 

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based calculation using DYNAMIC transition matrix
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        
        # MB value = P(S|A) * max Q(S)
        q_stage1_mb = est_transitions @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Choice ---
        s_idx = int(state[trial])
        
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Value Learning ---
        r = reward[trial]
        
        # Update stage 2 values
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate_reward * delta_stage2
        
        # Update stage 1 MF values
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate_reward * delta_stage1
        
        # --- Transition Learning ---
        # Update the estimated probability of reaching state s_idx given action a1
        # We treat the outcome as 1 for the state reached, 0 for the other.
        # This is a simple delta rule on probabilities.
        
        # Create a one-hot vector for the state reached
        state_outcome = np.zeros(2)
        state_outcome[s_idx] = 1.0
        
        # Update row a1
        est_transitions[a1] += lr_state * (state_outcome - est_transitions[a1])
        
        # Normalize to ensure it stays a valid probability distribution
        est_transitions[a1] /= np.sum(est_transitions[a1])

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Pure Model-Free with OCI-modulated Eligibility Trace
This model abandons the Hybrid structure and assumes a pure Model-Free learner (TD-learning). However, it introduces the concept of an eligibility trace ($\lambda$) that connects the second-stage outcome back to the first-stage choice. The hypothesis here is that OCI affects the "credit assignment" efficiency. High OCI might lead to a lower eligibility trace parameter, implying a more fragmented view of the task where the connection between the first choice and the final reward is weaker (or conversely, stronger).

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: Pure Model-Free with OCI-modulated Eligibility Trace (TD-Lambda).

    Hypothesis: The participant uses a pure temporal difference (TD) learning strategy.
    The efficiency of propagating the second-stage reward back to the first-stage choice 
    is controlled by the eligibility trace parameter (lambda). 
    OCI modulates this lambda: high OCI might disrupt the causal link (lower lambda) 
    or enforce rigid chains (higher lambda).

    Parameters:
    - learning_rate: [0, 1]
    - beta: [0, 10]
    - lambda_base: [0, 1] Baseline eligibility trace decay.
    - lambda_oci_slope: [-1, 1] Modulation of lambda by OCI.
    """
    learning_rate, beta, lambda_base, lambda_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate OCI-modulated lambda
    # Lambda controls how much the Stage 2 TD-error updates Stage 1 values directly
    lambda_param = np.clip(lambda_base + lambda_oci_slope * oci_score, 0.0, 1.0)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Choice ---
        s_idx = int(state[trial])
        
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- TD Learning with Eligibility Trace ---
        r = reward[trial]
        
        # 1. Prediction error at Stage 2 (R - Q2)
        delta2 = r - q_stage2[s_idx, a2]
        
        # 2. Prediction error at Stage 1 (Q2 - Q1)
        # Note: In standard TD(lambda), the update for state 1 includes 
        # the immediate error (delta1) plus the discounted error from next step (delta2).
        # Here discount gamma is effectively 1.0 within the trial.
        delta1 = q_stage2[s_idx, a2] - q_stage1[a1]
        
        # Update Stage 2 Q-value
        q_stage2[s_idx, a2] += learning_rate * delta2
        
        # Update Stage 1 Q-value
        # The update is: lr * (delta1 + lambda * delta2)
        # If lambda=0, this is simple TD(0) (Stage 1 only learns from Stage 2 value).
        # If lambda=1, this is TD(1) (Stage 1 learns fully from the final reward).
        q_stage1[a1] += learning_rate * (delta1 + lambda_param * delta2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```