Here are three new cognitive models that incorporate the OCI score to explain individual variability in the two-step decision task. These models focus on different potential mechanisms: altered learning rates based on symptom severity, modulation of the balance between model-based and model-free control, and specific sensitivities to rare transitions.

### Model 1: OCI-Modulated Learning Rate Asymmetry
This model hypothesizes that individuals with higher OCI scores might learn differently from positive versus negative prediction errors. Specifically, high OCI might be associated with a "hyper-learning" from negative outcomes (or lack of reward), reflecting a sensitivity to failure or error often seen in compulsive checking behaviors.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Dual Learning Rate Model with OCI-modulated Negative Learning Rate.
    
    Hypothesis: High OCI scores are associated with an increased learning rate for 
    negative prediction errors (when reward is less than expected), reflecting 
    heightened sensitivity to errors or missed rewards.
    
    Parameters:
    alpha_pos: [0, 1] - Learning rate for positive prediction errors.
    alpha_neg_base: [0, 1] - Base learning rate for negative prediction errors.
    alpha_neg_oci: [0, 1] - Scaling factor for OCI effect on negative learning rate.
    beta: [0, 10] - Inverse temperature for softmax.
    w: [0, 1] - Weighting parameter (0 = pure MF, 1 = pure MB).
    """
    alpha_pos, alpha_neg_base, alpha_neg_oci, beta, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate the effective negative learning rate, bounded between 0 and 1
    # Higher OCI leads to higher learning from negative outcomes.
    alpha_neg = min(1.0, alpha_neg_base + (alpha_neg_oci * oci_score))

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # --- Stage 2 Policy ---
        qs_stage2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_stage2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])

        # --- Updating ---
        # Stage 1 Update (TD(0) - only Model-Free part updates here)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        
        # Apply specific learning rate based on sign of PE
        lr_s1 = alpha_pos if delta_stage1 >= 0 else alpha_neg
        q_stage1_mf[a1] += lr_s1 * delta_stage1

        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        
        # Apply specific learning rate based on sign of PE
        lr_s2 = alpha_pos if delta_stage2 >= 0 else alpha_neg
        q_stage2_mf[state_idx, a2] += lr_s2 * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Model-Based Suppression
This model posits that while all participants have some mix of model-based (MB) and model-free (MF) control, high OCI scores specifically degrade the ability to utilize the model-based system effectively. This could be due to cognitive rigidity or an over-reliance on habit (MF) despite having the knowledge of the transition structure.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Based Suppression by OCI.
    
    Hypothesis: Higher OCI scores suppress the weight (w) given to the model-based 
    controller. This reflects a shift towards habitual (model-free) control in 
    individuals with higher obsessive-compulsive traits.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate.
    beta: [0, 10] - Inverse temperature.
    w_max: [0, 1] - Maximum possible model-based weight (for OCI=0).
    w_decay_oci: [0, 5] - Strength of OCI-dependent suppression of w.
    """
    learning_rate, beta, w_max, w_decay_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate w: it starts at w_max and decays exponentially as OCI increases.
    # If OCI is high, w approaches 0 (pure MF).
    w = w_max * np.exp(-w_decay_oci * oci_score)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combine MB and MF values using the OCI-modulated weight
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # --- Stage 2 Policy ---
        qs_stage2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_stage2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])

        # --- Updating ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Rare Transition Sensitivity
This model investigates if OCI scores correlate with how participants react specifically to *rare* transitions. A key signature of Model-Based RL is adjusting correctly after a rare transition (e.g., if you get a rare transition to the "wrong" planet and get a reward, a MB agent knows *not* to repeat the first action, whereas a MF agent would repeat it). This model introduces a specific parameter `gamma_rare` that modulates the update strength after a rare transition, hypothesizing that high OCI might lead to "over-interpreting" or reacting differently to these rare events.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Rare Transition Sensitivity Model.
    
    Hypothesis: The learning rate is dynamically adjusted based on whether the 
    transition was common or rare. OCI modulates the learning rate specifically 
    following rare transitions, potentially reflecting an inability to discount 
    rare events (or conversely, over-reacting to them).
    
    Parameters:
    lr_common: [0, 1] - Learning rate for common transitions.
    lr_rare_base: [0, 1] - Base learning rate for rare transitions.
    lr_rare_oci_mod: [0, 1] - How much OCI affects learning from rare transitions.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - MB/MF weight.
    """
    lr_common, lr_rare_base, lr_rare_oci_mod, beta, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate learning rate for rare transitions
    # We allow OCI to increase the learning rate on rare transitions
    lr_rare = min(1.0, lr_rare_base + (lr_rare_oci_mod * oci_score))

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # --- Stage 2 Policy ---
        qs_stage2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_stage2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])

        # --- Determine if transition was common or rare ---
        # Common: (Action 0 -> State 0) or (Action 1 -> State 1)
        # Rare:   (Action 0 -> State 1) or (Action 1 -> State 0)
        is_common = (a1 == state_idx)
        
        current_lr = lr_common if is_common else lr_rare

        # --- Updating ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += current_lr * delta_stage1

        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += current_lr * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```