Here are three new cognitive models exploring different mechanisms of how Obsessive-Compulsive Inventory (OCI) scores might influence decision-making in the two-step task.

### Model 1: OCI-Modulated Model-Based/Model-Free Trade-off (w)
This model hypothesizes that OCI symptoms affect the balance between goal-directed (Model-Based) and habitual (Model-Free) control. Specifically, it tests if higher OCI scores lead to a stronger reliance on habits (Model-Free) or rigid goal-directed planning (Model-Based). Instead of a linear relationship, it uses a sigmoid function to bound the mixing weight `w` between 0 and 1, where the inflection is modulated by OCI.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free with OCI-modulated mixing weight (w).
    
    Hypothesis: The balance between Model-Based (MB) and Model-Free (MF) control
    is influenced by OCI. This model calculates the mixing weight 'w' as a 
    function of the OCI score.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for Q-value updates.
    - beta: [0, 10] Inverse temperature (softmax sensitivity).
    - w_base: [0, 1] Baseline mixing weight (0=Pure MF, 1=Pure MB) for low OCI.
    - oci_mod: [-1, 1] Strength and direction of OCI influence on 'w'. 
                 Positive means OCI increases MB control, Negative means OCI increases MF control.
    """
    learning_rate, beta, w_base, oci_mod = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    # Calculate mixing weight w based on OCI. 
    # We clip to ensure it stays within valid bounds [0, 1].
    w = w_base + (current_oci * oci_mod)
    w = np.clip(w, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        
        # --- Stage 1 Policy ---
        # Model-Based Value: Expected value of stage 2 states weighted by transition probs
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Value: Weighted sum of MB and MF values
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Learning ---
        r = reward[trial]
        chosen_s1 = int(action_1[trial])
        chosen_s2 = int(action_2[trial])
        
        # Stage 2 Update (TD)
        delta_stage2 = r - q_stage2_mf[state_idx, chosen_s2]
        q_stage2_mf[state_idx, chosen_s2] += learning_rate * delta_stage2
        
        # Stage 1 Update (TD)
        delta_stage1 = q_stage2_mf[state_idx, chosen_s2] - q_stage1_mf[chosen_s1]
        q_stage1_mf[chosen_s1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Driven Asymmetric Learning Rates (Pos/Neg)
This model posits that individuals with high OCI scores might learn differently from positive versus negative outcomes (e.g., being hyper-sensitive to failure or "missing out"). Here, the OCI score specifically modulates the learning rate for *negative* prediction errors (when outcomes are worse than expected), creating an asymmetry in how value is updated.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free RL with OCI-modulated Asymmetric Learning Rates.
    
    Hypothesis: OCI is associated with altered sensitivity to errors. This model
    allows the learning rate for negative prediction errors (alpha_neg) to diverge
    from the positive learning rate (alpha_pos) based on the OCI score.
    
    Parameters:
    - alpha_pos: [0, 1] Learning rate for positive prediction errors (RPE > 0).
    - alpha_neg_base: [0, 1] Base learning rate for negative prediction errors (RPE < 0).
    - beta: [0, 10] Inverse temperature.
    - oci_neg_scaling: [0, 2] Multiplier for OCI's effect on negative learning rate.
    """
    alpha_pos, alpha_neg_base, beta, oci_neg_scaling = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate effective negative learning rate
    alpha_neg_eff = alpha_neg_base * (1 + (current_oci * oci_neg_scaling))
    alpha_neg_eff = np.clip(alpha_neg_eff, 0.0, 1.0)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Pure Model-Free Q-values
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        r = reward[trial]
        chosen_s1 = int(action_1[trial])
        chosen_s2 = int(action_2[trial])

        # --- Updates with Asymmetric Learning ---
        
        # Stage 2 Update
        delta2 = r - q_stage2[state_idx, chosen_s2]
        lr2 = alpha_pos if delta2 >= 0 else alpha_neg_eff
        q_stage2[state_idx, chosen_s2] += lr2 * delta2
        
        # Stage 1 Update
        delta1 = q_stage2[state_idx, chosen_s2] - q_stage1[chosen_s1]
        lr1 = alpha_pos if delta1 >= 0 else alpha_neg_eff
        q_stage1[chosen_s1] += lr1 * delta1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Choice Stickiness (Perseveration)
This model focuses on the compulsive aspect of OCI. It suggests that high OCI scores lead to "stickiness" or perseverationâ€”a tendency to repeat the previous action regardless of reward history. Unlike Model 2 in the feedback which modified Q-values, this implementation applies stickiness directly as an autocorrelation term in the softmax probability calculation (a more standard implementation in decision neuroscience literature), where the magnitude of this bias is controlled by OCI.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free RL with OCI-modulated Choice Autocorrelation (Stickiness).
    
    Hypothesis: High OCI leads to repetitive behavior (perseveration). This model
    adds a "stickiness" bonus directly to the choice logits of the previously
    selected action, with the strength of this bonus determined by OCI.
    
    Parameters:
    - learning_rate: [0, 1] Speed of value updating.
    - beta: [0, 10] Inverse temperature.
    - stickiness_oci_slope: [0, 5] How much OCI increases the tendency to repeat choices.
    - stickiness_intercept: [-2, 2] Base tendency to repeat (or switch) at OCI=0.
    """
    learning_rate, beta, stickiness_oci_slope, stickiness_intercept = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate stickiness parameter 'phi'
    phi = stickiness_intercept + (current_oci * stickiness_oci_slope)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    prev_action_1 = -1 # Indicator for previous action

    for trial in range(n_trials):

        # --- Stage 1 Choice ---
        # Calculate logits: Beta * Q + Phi * (IsPreviousAction)
        logits_1 = beta * q_stage1
        if prev_action_1 != -1:
            logits_1[prev_action_1] += phi
            
        # Softmax
        # Subtract max for numerical stability
        logits_1 = logits_1 - np.max(logits_1)
        exp_logits_1 = np.exp(logits_1)
        probs_1 = exp_logits_1 / np.sum(exp_logits_1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])

        # --- Stage 2 Choice ---
        # Standard softmax for stage 2 (no stickiness assumed here for simplicity)
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        r = reward[trial]
        chosen_s1 = int(action_1[trial])
        chosen_s2 = int(action_2[trial])

        # --- Updates ---
        delta2 = r - q_stage2[state_idx, chosen_s2]
        q_stage2[state_idx, chosen_s2] += learning_rate * delta2
        
        delta1 = q_stage2[state_idx, chosen_s2] - q_stage1[chosen_s1]
        q_stage1[chosen_s1] += learning_rate * delta1
        
        prev_action_1 = chosen_s1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```