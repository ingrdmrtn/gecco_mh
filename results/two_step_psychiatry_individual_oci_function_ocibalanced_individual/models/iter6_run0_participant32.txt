Here are three new cognitive models implemented as Python functions.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: Hybrid Model with OCI-Modulated Transition Learning.
    
    Hypothesis: High OCI participants may exhibit uncertainty about the environment's structure, 
    leading them to constantly re-learn or adjust their belief about transition probabilities 
    (Dynamic Model-Based), rather than relying on a fixed structural prior.
    
    Parameters:
    lr_reward: [0, 1] Learning rate for Stage 2 reward values.
    beta: [0, 10] Inverse temperature for softmax.
    w: [0, 1] Weighting between MB and MF (0=MF, 1=MB).
    lr_trans_base: [0, 1] Baseline learning rate for transition probabilities.
    lr_trans_oci_slope: [-1, 1] Effect of OCI on transition learning rate.
    """
    lr_reward, beta, w, lr_trans_base, lr_trans_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate transition learning rate specific to this participant
    lr_trans = lr_trans_base + (lr_trans_oci_slope * oci_score)
    lr_trans = np.clip(lr_trans, 0.0, 1.0)

    # Initialize transition matrix (State belief). 
    # Starts flat (0.5) to represent initial uncertainty, or could start at 0.7.
    # Rows: Choice 0 (A), Choice 1 (U). Cols: State 0 (X), State 1 (Y).
    # We track prob of transitioning to State 0.
    trans_prob_to_0 = np.array([0.5, 0.5]) 

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        
        # 1. Model-Based Value Calculation
        # Construct current transition matrix based on beliefs
        # T[0,0] is prob A->X, T[0,1] is prob A->Y (1 - T[0,0])
        curr_trans_matrix = np.zeros((2, 2))
        curr_trans_matrix[0, 0] = trans_prob_to_0[0]
        curr_trans_matrix[0, 1] = 1.0 - trans_prob_to_0[0]
        curr_trans_matrix[1, 0] = trans_prob_to_0[1]
        curr_trans_matrix[1, 1] = 1.0 - trans_prob_to_0[1]

        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = curr_trans_matrix @ max_q_stage2

        # 2. Hybrid Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf

        # 3. Action Selection
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]

        # --- Stage 2 Policy ---
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # --- Updates ---
        
        # 1. Update Transition Beliefs (Dynamic MB)
        chosen_s1 = int(action_1[trial])
        # Did we go to state 0?
        is_state_0 = 1.0 if state_idx == 0 else 0.0
        # Update belief for the chosen spaceship
        trans_prob_to_0[chosen_s1] += lr_trans * (is_state_0 - trans_prob_to_0[chosen_s1])

        # 2. Update MF Values
        # TD(0) update for Stage 1 (driven by Stage 2 value)
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[chosen_s1]
        q_stage1_mf[chosen_s1] += lr_reward * delta_stage1
        
        # TD(0) update for Stage 2 (driven by reward)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += lr_reward * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: Hybrid Model with OCI-Modulated Choice Kernel (Habit).
    
    Hypothesis: OCI relates to strong habit formation. Unlike simple perseveration (repeating 
    last choice), a Choice Kernel tracks the long-term frequency of choices. This model 
    posits that OCI modulates the weight (influence) of this historical choice frequency.
    
    Parameters:
    learning_rate: [0, 1] Learning rate for Q-values and Choice Kernel decay.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Weighting between MB and MF.
    phi_base: [-5, 5] Baseline weight of the Choice Kernel (positive=habit, negative=novelty).
    phi_oci_sens: [-5, 5] Sensitivity of Choice Kernel weight to OCI.
    """
    learning_rate, beta, w, phi_base, phi_oci_sens = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate Habit weight
    phi = phi_base + (phi_oci_sens * oci_score)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Choice Kernel (initialized at 0)
    choice_kernel = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2

        # Combine MB, MF, and Choice Kernel
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf) + (phi * choice_kernel)

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]

        # --- Stage 2 Policy ---
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # --- Updates ---
        chosen_s1 = int(action_1[trial])
        
        # Update Choice Kernel (using same learning rate to conserve parameters)
        # CK accumulates trace of chosen action
        # Vectorized update: decay all, boost chosen
        choice_kernel = (1 - learning_rate) * choice_kernel
        choice_kernel[chosen_s1] += learning_rate * 1.0

        # Standard Hybrid Updates
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[chosen_s1]
        q_stage1_mf[chosen_s1] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: Hybrid Model with OCI-Modulated Learning on Rare Transitions.
    
    Hypothesis: High OCI participants might over-react to 'rare' transitions (anomalies), 
    treating them as significant signals rather than noise in the Model-Free system. 
    This model separates learning rates for common vs rare transitions, with the rare 
    rate dependent on OCI.
    
    Parameters:
    lr_common: [0, 1] Learning rate for common transitions.
    lr_rare_base: [0, 1] Baseline learning rate for rare transitions.
    lr_rare_oci_slope: [-1, 1] Modulation of rare learning rate by OCI.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Weighting between MB and MF.
    """
    lr_common, lr_rare_base, lr_rare_oci_slope, beta, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate rare learning rate
    lr_rare = lr_rare_base + (lr_rare_oci_slope * oci_score)
    lr_rare = np.clip(lr_rare, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2

        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]

        # --- Stage 2 Policy ---
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # --- Updates ---
        chosen_s1 = int(action_1[trial])
        
        # Determine if transition was Common or Rare
        # Common: (Action 0 -> State 0) or (Action 1 -> State 1)
        # Rare: (Action 0 -> State 1) or (Action 1 -> State 0)
        is_common = (chosen_s1 == state_idx)
        
        current_lr = lr_common if is_common else lr_rare

        # MF Update for Stage 1 (uses current_lr determined by transition type)
        # This determines how much the value of the spaceship is updated by the subsequent state value
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[chosen_s1]
        q_stage1_mf[chosen_s1] += current_lr * delta_stage1
        
        # MF Update for Stage 2 (uses common learning rate usually, or we can use the same logic. 
        # Standard convention: The transition surprise affects the *Stage 1* update primarily.
        # We will use lr_common for Stage 2 updates as Stage 2->Reward is not the rare transition).
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += lr_common * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```