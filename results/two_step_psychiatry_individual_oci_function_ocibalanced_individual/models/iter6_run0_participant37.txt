def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Loss-Chasing Stickiness.

    This model integrates Model-Based and Model-Free learning. 
    It hypothesizes that OCI scores predict a specific type of perseveration:
    stickiness to the previous choice specifically when it was *unrewarded* (loss-chasing).
    Unlike general stickiness, this bonus is only applied if the previous trial resulted in 0 coins.
    
    Parameters:
    learning_rate: [0,1] - Update rate for Q-values.
    beta: [0,10] - Inverse temperature for softmax.
    w_mb: [0,1] - Weight of Model-Based values (0=Pure MF, 1=Pure MB).
    stick_base: [0,5] - General tendency to repeat the previous choice.
    stick_loss_oci: [0,5] - Additional stickiness applied only after a loss (0 reward), scaled by OCI.
    """
    learning_rate, beta, w_mb, stick_base, stick_loss_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action

    last_action_1 = -1
    last_reward = -1

    for trial in range(n_trials):
        # 1. Stage 1 Policy
        # Calculate MB values: Q_MB(s1, a) = Sum(P(s2|s1,a) * max(Q(s2, :)))
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combined Net Value
        q_net = w_mb * q_stage1_mb + (1 - w_mb) * q_stage1_mf
        
        # Calculate logits
        logits = beta * q_net
        
        # Apply Stickiness
        if last_action_1 != -1:
            stickiness = stick_base
            # If previous outcome was a loss, add OCI-dependent stickiness
            if last_reward == 0.0:
                stickiness += stick_loss_oci * oci_score
            logits[int(last_action_1)] += stickiness

        # Softmax
        exp_q1 = np.exp(logits - np.max(logits))
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        if a1 != -1:
            p_choice_1[trial] = probs_1[a1]
        else:
            p_choice_1[trial] = 1.0 
            
        state_idx = int(state[trial])
        
        # 2. Stage 2 Policy
        if state_idx != -1:
            logits_2 = beta * q_stage2_mf[state_idx]
            exp_q2 = np.exp(logits_2 - np.max(logits_2))
            probs_2 = exp_q2 / np.sum(exp_q2)
            
            a2 = int(action_2[trial])
            if a2 != -1:
                p_choice_2[trial] = probs_2[a2]
                
                # 3. Learning / Updates
                r = reward[trial]
                
                # TD Error Stage 2
                delta_stage2 = r - q_stage2_mf[state_idx, a2]
                q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
                
                # TD Error Stage 1 (Model-Free update)
                delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
                q_stage1_mf[a1] += learning_rate * delta_stage1
                
                last_action_1 = a1
                last_reward = r
            else:
                 p_choice_2[trial] = 1.0
        else:
             p_choice_2[trial] = 1.0

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Distorted Transition Belief.

    This model posits that OCI scores influence the participant's internal model 
    of the transition structure (Spaceship -> Planet). 
    While the true common transition probability is 0.7, high OCI participants 
    may perceive this as more deterministic (closer to 1.0) or more chaotic 
    (closer to 0.5), affecting their Model-Based value calculation.

    Parameters:
    learning_rate: [0,1] - Update rate.
    beta: [0,10] - Inverse temperature.
    w_mb: [0,1] - Model-Based weight.
    trans_belief_base: [0.5, 1.0] - Baseline belief about common transition probability.
    trans_belief_oci: [-0.5, 0.5] - Modulation of belief by OCI.
    """
    learning_rate, beta, w_mb, trans_belief_base, trans_belief_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate subjective transition matrix
    # P_common is bounded between 0 and 1
    p_common = trans_belief_base + (trans_belief_oci * oci_score)
    p_common = np.clip(p_common, 0.0, 1.0)
    
    # Subjective matrix: Row 0 is Action A (prob X, prob Y), Row 1 is Action U (prob X, prob Y)
    # Action A goes to X (idx 0) with p_common
    # Action U goes to Y (idx 1) with p_common
    subjective_tm = np.array([[p_common, 1-p_common], [1-p_common, p_common]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) 

    for trial in range(n_trials):
        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        
        # Use subjective matrix for MB planning
        q_stage1_mb = subjective_tm @ max_q_stage2
        
        q_net = w_mb * q_stage1_mb + (1 - w_mb) * q_stage1_mf
        
        logits = beta * q_net
        exp_q1 = np.exp(logits - np.max(logits))
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        if a1 != -1:
            p_choice_1[trial] = probs_1[a1]
        else:
            p_choice_1[trial] = 1.0

        state_idx = int(state[trial])
        
        # Stage 2 Policy
        if state_idx != -1:
            logits_2 = beta * q_stage2_mf[state_idx]
            exp_q2 = np.exp(logits_2 - np.max(logits_2))
            probs_2 = exp_q2 / np.sum(exp_q2)
            
            a2 = int(action_2[trial])
            if a2 != -1:
                p_choice_2[trial] = probs_2[a2]
                
                # Updates
                r = reward[trial]
                delta_stage2 = r - q_stage2_mf[state_idx, a2]
                q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
                
                delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
                q_stage1_mf[a1] += learning_rate * delta_stage1
            else:
                 p_choice_2[trial] = 1.0
        else:
            p_choice_2[trial] = 1.0

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: Stage-Specific Exploration with OCI Modulation.

    This model separates the exploration/exploitation trade-off (beta) for the 
    two stages. It hypothesizes that OCI specifically impacts the decision 
    noise at the second stage (choosing the alien), reflecting a local vs. 
    global control difference. For example, high OCI might lead to rigid exploitation
    at the concrete reward stage (Stage 2) while maintaining standard exploration
    at the abstract planning stage (Stage 1).
    
    Parameters:
    learning_rate: [0,1] - Update rate.
    w_mb: [0,1] - Model-Based weight.
    beta_stage1: [0,10] - Inverse temperature for spaceship choice.
    beta_stage2_base: [0,10] - Baseline inverse temperature for alien choice.
    beta_stage2_oci: [-5, 5] - Modulation of stage 2 beta by OCI.
    """
    learning_rate, w_mb, beta_stage1, beta_stage2_base, beta_stage2_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate Stage 2 Beta
    beta_stage2 = beta_stage2_base + (beta_stage2_oci * oci_score)
    beta_stage2 = np.maximum(0.0, beta_stage2) # Ensure non-negative

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) 

    for trial in range(n_trials):
        # Stage 1 Policy (Uses beta_stage1)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w_mb * q_stage1_mb + (1 - w_mb) * q_stage1_mf
        
        logits = beta_stage1 * q_net
        exp_q1 = np.exp(logits - np.max(logits))
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        if a1 != -1:
            p_choice_1[trial] = probs_1[a1]
        else:
            p_choice_1[trial] = 1.0

        state_idx = int(state[trial])
        
        # Stage 2 Policy (Uses OCI-modulated beta_stage2)
        if state_idx != -1:
            logits_2 = beta_stage2 * q_stage2_mf[state_idx]
            exp_q2 = np.exp(logits_2 - np.max(logits_2))
            probs_2 = exp_q2 / np.sum(exp_q2)
            
            a2 = int(action_2[trial])
            if a2 != -1:
                p_choice_2[trial] = probs_2[a2]
                
                # Updates
                r = reward[trial]
                delta_stage2 = r - q_stage2_mf[state_idx, a2]
                q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
                
                delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
                q_stage1_mf[a1] += learning_rate * delta_stage1
            else:
                 p_choice_2[trial] = 1.0
        else:
            p_choice_2[trial] = 1.0

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss