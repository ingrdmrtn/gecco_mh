Here are three new cognitive models exploring different mechanisms of how Obsessive-Compulsive symptoms (OCI) might influence reinforcement learning in a two-step task.

### Model 1: Hybrid Model with OCI-Driven Model-Based Suppression
This model hypothesizes that high OCI scores reflect a reliance on habitual (Model-Free) control over goal-directed (Model-Based) control. Instead of mixing them with a standard `w` parameter, here the `w` (weight of Model-Based control) is explicitly suppressed as OCI increases.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free learner where OCI score suppresses Model-Based control.
    High OCI leads to lower 'w', meaning more reliance on Model-Free (habitual) learning.
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    w_max: [0, 1] - Maximum possible weight for Model-Based control (at OCI=0)
    oci_suppression: [0, 1] - How strongly OCI reduces w
    """
    learning_rate, beta, w_max, oci_suppression = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate w based on OCI: w = w_max * (1 - oci_suppression * OCI)
    # If OCI is high and suppression is high, w approaches 0 (pure Model-Free).
    w = w_max * (1.0 - (oci_suppression * oci_score))
    # Ensure w stays in [0, 1]
    w = np.clip(w, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based Value Calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # --- Updating ---
        r = reward[trial]
        
        # Stage 2 Update (TD)
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 Update (TD)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Asymmetric Learning Rates Modulated by OCI
This model investigates "negativity bias" or "perfectionism" often associated with OC traits. It proposes that individuals with high OCI update their value estimates differently for positive outcomes (coins) vs. negative outcomes (no coins). Specifically, OCI modulates the learning rate for negative prediction errors (disappointment).

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner with asymmetric learning rates.
    OCI modulates the learning rate for negative prediction errors (alpha_neg).
    High OCI might lead to 'over-learning' from failure (or under-learning).
    
    Bounds:
    alpha_pos: [0, 1] - Learning rate for positive prediction errors
    alpha_neg_base: [0, 1] - Base learning rate for negative PE
    beta: [0, 10]
    oci_neg_factor: [0, 2] - Multiplier for OCI effect on negative learning rate
    """
    alpha_pos, alpha_neg_base, beta, oci_neg_factor = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective negative learning rate
    # alpha_neg = alpha_neg_base * (1 + factor * OCI)
    # This allows high OCI to increase sensitivity to negative outcomes.
    alpha_neg = alpha_neg_base * (1.0 + oci_neg_factor * oci_score)
    alpha_neg = np.clip(alpha_neg, 0.0, 1.0)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]

        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # --- Updating ---
        r = reward[trial]

        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        lr_2 = alpha_pos if delta_stage2 > 0 else alpha_neg
        q_stage2_mf[state_idx, a2] += lr_2 * delta_stage2

        # Stage 1 Update
        # Note: We use the updated Q2 value for the TD error calculation
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        lr_1 = alpha_pos if delta_stage1 > 0 else alpha_neg
        q_stage1_mf[a1] += lr_1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Inverse Temperature (Decision Noise)
This model posits that OCI affects the exploration-exploitation trade-off. High OCI might be associated with rigidity (extremely high beta, deterministic choice) or anxiety-driven uncertainty (low beta, random choice). This model allows the `beta` parameter to scale linearly with the OCI score.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner where OCI directly influences decision noise (Inverse Temperature).
    
    Bounds:
    learning_rate: [0, 1]
    beta_intercept: [0, 10] - Beta value when OCI is 0
    beta_slope: [-5, 5] - How much beta changes per unit of OCI (can be negative or positive)
    eligibility_lambda: [0, 1] - Eligibility trace parameter for multi-step update
    """
    learning_rate, beta_intercept, beta_slope, eligibility_lambda = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective beta
    beta = beta_intercept + (beta_slope * oci_score)
    # Ensure beta stays non-negative and within reasonable bounds
    beta = np.clip(beta, 0.0, 20.0) 

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]

        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # --- Updating with Eligibility Trace (TD(lambda)) ---
        r = reward[trial]

        # Stage 2 Prediction Error
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 Prediction Error
        # In TD(lambda) for this task, the Stage 1 update often includes
        # the immediate Stage 2 PE scaled by lambda
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        
        # Standard TD update for stage 1
        q_stage1_mf[a1] += learning_rate * delta_stage1 
        
        # Eligibility trace update: Stage 1 also learns from Stage 2's outcome
        q_stage1_mf[a1] += learning_rate * eligibility_lambda * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```