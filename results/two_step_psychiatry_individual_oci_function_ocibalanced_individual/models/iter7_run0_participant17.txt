Here are three new cognitive models that explore different mechanisms by which Obsessive-Compulsive symptoms (OCI) might influence decision-making in the two-step task, specifically focusing on perseveration, model-based/model-free arbitration, and learning rate asymmetries.

### Model 1: OCI-Modulated Perseveration (Stickiness) on Second Stage
This model hypothesizes that high OCI scores lead to "stuck" behavior not just at the abstract first stage, but specifically at the concrete second stage (choosing aliens). This reflects a compulsion to repeat specific motor actions or choices when faced with the same stimuli (the planets), regardless of reward history.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: Hybrid Learner with OCI-modulated Second-Stage Perseveration.
    
    Hypothesis: Higher OCI scores increase "stickiness" (perseveration) specifically 
    at the second stage (alien choice), reflecting a compulsion to repeat concrete 
    actions given the same stimulus, regardless of the model-based/free trade-off at stage 1.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for value updates.
    - beta: [0, 10] Inverse temperature (softmax sensitivity).
    - w: [0, 1] Weighting parameter (0 = pure Model-Free, 1 = pure Model-Based).
    - stickiness_base: [0, 5] Baseline tendency to repeat the previous choice at stage 2.
    - oci_stickiness_boost: [0, 5] Additional stage 2 stickiness per unit of OCI.
    """
    learning_rate, beta, w, stickiness_base, oci_stickiness_boost = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective stickiness for stage 2 based on OCI
    stickiness_eff = stickiness_base + (oci_stickiness_boost * oci_score)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track last action taken in each state (planet) for stickiness
    last_action_stage2 = np.array([-1, -1]) 

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based Value
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Value
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        s_idx = int(state[trial])

        # --- Stage 2 Policy ---
        # Apply stickiness to the Q-values for decision making only
        q_stage2_decision = q_stage2_mf[s_idx].copy()
        
        if last_action_stage2[s_idx] != -1:
            q_stage2_decision[last_action_stage2[s_idx]] += stickiness_eff
            
        exp_q2 = np.exp(beta * q_stage2_decision)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # Update history
        last_action_stage2[s_idx] = a2

        # --- Learning ---
        r = reward[trial]
        
        # Stage 2 Update (Model-Free)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 Update (Model-Free TD)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Mixing Weight (w) with Inverse Relationship
This model tests the hypothesis that higher OCI scores are associated with a *reduction* in Model-Based control (goal-directed planning) and a reliance on Model-Free (habitual) control. Instead of a linear relationship, this model assumes a baseline `w` which is degraded by OCI severity.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: Hybrid Learner with OCI-Degraded Model-Based Control.
    
    Hypothesis: High OCI participants rely less on the model-based system.
    The mixing weight 'w' (0=MF, 1=MB) starts at a baseline and is reduced
    proportional to the OCI score.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w_base: [0, 1] Baseline model-based weighting for a theoretical 0-OCI participant.
    - oci_w_penalty: [0, 1] Reduction in 'w' per unit of OCI.
    """
    learning_rate, beta, w_base, oci_w_penalty = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    # Calculate w, ensuring it stays within [0, 1] bounds
    w_eff = w_base - (oci_w_penalty * oci_score)
    if w_eff < 0: w_eff = 0
    if w_eff > 1: w_eff = 1

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combine MB and MF values using the OCI-modulated weight
        q_net = w_eff * q_stage1_mb + (1 - w_eff) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        state_idx = int(state[trial])

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
  
        # Update Stage 1 (TD-learning)
        # Note: In standard hybrid models, MF update uses the value of the state reached
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Update Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Learning Rate Asymmetry (Positive vs Negative)
This model posits that OCI affects how participants learn from errors vs. successes. Specifically, it hypothesizes that high OCI is associated with an over-sensitivity to negative outcomes (punishment/lack of reward) or "hyper-correction," leading to a higher learning rate for unrewarded trials compared to rewarded ones.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: Model-Free Learner with OCI-Modulated Negative Learning Rate.
    
    Hypothesis: Participants with high OCI are hypersensitive to prediction errors 
    when outcomes are worse than expected (or zero reward). The learning rate for 
    negative prediction errors (alpha_neg) increases with OCI score.
    
    Parameters:
    - alpha_pos: [0, 1] Learning rate for positive prediction errors.
    - alpha_neg_base: [0, 1] Baseline learning rate for negative prediction errors.
    - alpha_neg_oci_slope: [0, 1] Increase in negative learning rate per unit OCI.
    - beta: [0, 10] Inverse temperature.
    """
    alpha_pos, alpha_neg_base, alpha_neg_oci_slope, beta = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    # Calculate effective negative learning rate
    alpha_neg_eff = alpha_neg_base + (alpha_neg_oci_slope * oci_score)
    if alpha_neg_eff > 1: alpha_neg_eff = 1
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice (Pure Model-Free for simplicity to isolate LR effects)
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        state_idx = int(state[trial])

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
  
        # Update Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        
        lr_2 = alpha_pos if delta_stage2 > 0 else alpha_neg_eff
        q_stage2_mf[state_idx, a2] += lr_2 * delta_stage2
        
        # Update Stage 1
        # Using SARSA-like update (value of state_idx, action_2) to drive stage 1
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        
        lr_1 = alpha_pos if delta_stage1 > 0 else alpha_neg_eff
        q_stage1_mf[a1] += lr_1 * delta_stage1
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```