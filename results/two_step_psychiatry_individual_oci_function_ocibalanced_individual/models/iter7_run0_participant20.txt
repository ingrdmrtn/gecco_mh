Here are three new cognitive models exploring different mechanisms for how obsessive-compulsive symptoms (measured by OCI-R) might influence decision-making in the two-step task.

### Model 1: OCI-Modulated Model-Based/Model-Free Trade-off with Inverse Modulation
This model hypothesizes that high OCI scores might lead to a deficit in goal-directed (model-based) planning, shifting the balance towards habitual (model-free) control. Instead of a linear slope, this model tests if the `w` parameter (weighting of model-based values) is inversely proportional to the OCI score, capped by a scaling factor.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL with OCI-modulated Model-Based weighting (Inverse relationship).
    
    Hypothesis: Higher OCI scores reduce the reliance on the model-based system.
    The weight `w` is calculated as w_max / (1 + oci_scale * oci), suggesting
    that as symptoms increase, the ability to utilize the transition matrix decreases.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate for value updates.
    beta: [0, 10] - Inverse temperature for softmax.
    w_max: [0, 1] - Maximum possible model-based weight (when OCI is 0).
    oci_scale: [0, 10] - Sensitivity of the degradation of w to the OCI score.
    """
    learning_rate, beta, w_max, oci_scale = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective w based on OCI
    w = w_max / (1.0 + oci_scale * oci_score)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid value combining MF and MB
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        state_idx = int(state[trial])

        # policy for the second choice
        qs_stage2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_stage2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])

        # Update stage 1 MF values (SARSA-like)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Update stage 2 MF values
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Eligibility Trace (Lambda)
This model investigates whether OCI scores affect the efficiency of credit assignment between the second stage and the first stage. A parameter $\lambda$ (lambda) controls how much the second-stage reward prediction error updates the first-stage values. This model proposes that higher OCI scores might lead to "over-updating" or stronger chaining of values (higher lambda), reflecting a hyper-connectivity or inability to compartmentalize stages.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free RL with OCI-modulated Eligibility Trace (Lambda).
    
    Hypothesis: OCI affects the eligibility trace parameter (lambda). 
    Higher OCI might lead to stronger 'chaining' of credit assignment back 
    to the first stage, potentially making behavior more rigid or reactive 
    to recent outcomes across stages.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate.
    beta: [0, 10] - Inverse temperature.
    lambda_base: [0, 1] - Baseline eligibility trace parameter.
    lambda_oci_slope: [0, 1] - How much OCI increases the eligibility trace.
    """
    learning_rate, beta, lambda_base, lambda_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective lambda, clamped between 0 and 1
    eligibility_lambda = lambda_base + lambda_oci_slope * oci_score
    eligibility_lambda = np.clip(eligibility_lambda, 0.0, 1.0)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        # policy for the first choice (Pure Model-Free here to isolate lambda effect)
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        state_idx = int(state[trial])

        # policy for the second choice
        qs_stage2 = q_stage2[state_idx]
        exp_q2 = np.exp(beta * qs_stage2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        
        # Prediction error at stage 2
        delta_stage2 = reward[trial] - q_stage2[state_idx, a2]
        
        # Update stage 2
        q_stage2[state_idx, a2] += learning_rate * delta_stage2
        
        # Update stage 1: 
        # Standard TD(0) part: driven by value of state 2
        delta_stage1_td = q_stage2[state_idx, a2] - q_stage1[a1]
        q_stage1[a1] += learning_rate * delta_stage1_td
        
        # Eligibility trace part: Stage 1 also learns directly from Stage 2's RPE
        # scaled by lambda. This connects the final reward outcome back to choice 1.
        q_stage1[a1] += learning_rate * eligibility_lambda * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Exploration (Temperature Scaling)
This model tests the hypothesis that OCI is related to uncertainty tolerance or exploration/exploitation balance. Specifically, it posits that higher OCI scores lead to lower decision noise (higher beta / lower temperature), resulting in more deterministic, rigid behavior (less exploration).

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL with OCI-modulated Inverse Temperature (Beta).
    
    Hypothesis: Higher OCI scores lead to higher Beta (lower temperature),
    resulting in more deterministic (rigid) choices and less exploration.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate.
    w: [0, 1] - Weighting between model-based and model-free.
    beta_base: [0, 10] - Baseline inverse temperature.
    beta_oci_factor: [0, 5] - Multiplicative factor for OCI's effect on beta.
                               Beta = beta_base * (1 + factor * OCI)
    """
    learning_rate, w, beta_base, beta_oci_factor = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Effective beta increases with OCI (more exploitation/rigidity)
    beta_eff = beta_base * (1.0 + beta_oci_factor * oci_score)
    # Cap beta to prevent numerical overflow in exp()
    beta_eff = np.minimum(beta_eff, 20.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_eff * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        state_idx = int(state[trial])

        # policy for the second choice
        qs_stage2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta_eff * qs_stage2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        a1 = int(action_1[trial])
        a2 = int(action_2[trial])

        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```