Here are 3 new cognitive models based on the two-step task, incorporating the OCI score into distinct mechanisms not previously explored in the feedback list.

### Model 1: OCI-modulated Reward Dampening (Subjective Valuation)
This model hypothesizes that high OCI scores are associated with a "feeling of incompleteness" or anhedonia, leading to a dampened subjective perception of the reward. Even when a coin is received (reward = 1), a high-OCI participant might process this as a smaller value (e.g., 0.8), reducing the magnitude of prediction errors and subsequent learning.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-modulated Reward Dampening.
    
    This model posits that OCI scores dampen the subjective valuation of rewards.
    High OCI reduces the effective reward signal entering the prediction error calculation,
    simulating a 'feeling of incompleteness' or reduced satisfaction.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for value updates.
    - beta: [0, 10] Inverse temperature (exploration/exploitation).
    - w: [0, 1] Mixing weight (0 = pure Model-Free, 1 = pure Model-Based).
    - lambda_eligibility: [0, 1] Eligibility trace for Stage 1 updates.
    - oci_dampening: [0, 1] Factor by which OCI reduces reward magnitude.
      Effective Reward = Reward * (1 - (oci_dampening * OCI))
    """
    learning_rate, beta, w, lambda_eligibility, oci_dampening = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    # Fixed transition matrix for Model-Based calculation
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based Value
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Value
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        state_idx = int(state[trial])

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Calculate Effective Reward (Dampened by OCI)
        # We clip the dampener so reward doesn't invert
        damp_factor = oci_dampening * oci_score
        if damp_factor > 0.99: damp_factor = 0.99
        
        effective_reward = reward[trial] * (1.0 - damp_factor)

        # Prediction Errors
        delta_stage2 = effective_reward - q_stage2_mf[state_idx, a2]
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        
        # Update Stage 2 Values
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Update Stage 1 Values (with eligibility trace)
        q_stage1_mf[a1] += learning_rate * delta_stage1 + (learning_rate * lambda_eligibility * delta_stage2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-modulated Transition Uncertainty (Matrix Distortion)
This model suggests that high OCI leads to "doubting" the structural rules of the environment. While the true transition probability is 0.7, a high-OCI participant might perceive the environment as more chaotic or uncertain. This is modeled by flattening the transition matrix used by the Model-Based system towards 0.5 (maximum entropy) proportional to the OCI score.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-modulated Transition Uncertainty.
    
    This model assumes OCI affects the Model-Based system's belief about transition probabilities.
    High OCI participants perceive the transitions as less reliable (closer to random/0.5) 
    than they actually are (0.7), reducing the efficacy of Model-Based planning.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight.
    - lambda_eligibility: [0, 1] Eligibility trace.
    - oci_uncertainty: [0, 1] Degree to which OCI flattens the transition matrix.
      p_common = 0.7 - (oci_uncertainty * OCI * 0.2). 
      If max OCI and max parameter, p_common approaches 0.5.
    """
    learning_rate, beta, w, lambda_eligibility, oci_uncertainty = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    # Dynamic Transition Matrix based on OCI
    # Base common probability is 0.7. 
    # Max distortion reduces it to 0.5 (0.7 - 0.2).
    distortion = oci_uncertainty * oci_score * 0.2
    p_common = 0.7 - distortion
    p_rare = 1.0 - p_common
    
    transition_matrix = np.array([[p_common, p_rare], [p_rare, p_common]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # MB calculation uses the distorted matrix
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        state_idx = int(state[trial])

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        q_stage1_mf[a1] += learning_rate * delta_stage1 + (learning_rate * lambda_eligibility * delta_stage2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-modulated Stage 1 Beta Scaling (Planning Anxiety)
This model separates the decision noise (temperature) for Stage 1 (planning) and Stage 2 (consumption). It hypothesizes that high OCI specifically increases decision noise (lowers beta) during the first stage due to anxiety or indecision about the abstract choice between spaceships, while the second stage (choosing an alien) remains relatively standard.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-modulated Stage 1 Beta Scaling (Planning Anxiety).
    
    This model proposes that OCI specifically degrades the consistency of the 
    first-stage choice (planning), making it more stochastic (lower beta), 
    while the second-stage choice uses a baseline beta. This reflects 'planning anxiety'
    or difficulty committing to a path, common in OCD.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta_base: [0, 10] Baseline inverse temperature (used for Stage 2).
    - w: [0, 1] Mixing weight.
    - lambda_eligibility: [0, 1] Eligibility trace.
    - oci_beta_damp: [0, 1] Factor reducing Beta for Stage 1 based on OCI.
      beta_stage1 = beta_base * (1 - (oci_beta_damp * OCI))
    """
    learning_rate, beta_base, w, lambda_eligibility, oci_beta_damp = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    # Calculate Stage 1 specific Beta
    # High OCI reduces beta1 (more noise/exploration/indecision)
    damp_val = oci_beta_damp * oci_score
    if damp_val > 0.9: damp_val = 0.9
    beta_stage1 = beta_base * (1.0 - damp_val)
    
    # Stage 2 uses base beta
    beta_stage2 = beta_base

    for trial in range(n_trials):
        # --- Stage 1 Choice (Uses beta_stage1) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_stage1 * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        state_idx = int(state[trial])

        # --- Stage 2 Choice (Uses beta_stage2) ---
        exp_q2 = np.exp(beta_stage2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        q_stage1_mf[a1] += learning_rate * delta_stage1 + (learning_rate * lambda_eligibility * delta_stage2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```