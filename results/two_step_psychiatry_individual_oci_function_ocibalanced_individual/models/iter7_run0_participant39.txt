Here are three new cognitive models designed to capture how Obsessive-Compulsive Inventory (OCI) scores might modulate decision-making in the two-step task.

### Model 1: Hybrid Model with OCI-modulated Reward Sensitivity
This model hypothesizes that OCI levels influence the subjective magnitude of rewards. A high OCI score scales the effective reward signal, making the participant more (or less) sensitive to outcomes, which drives the learning updates in both Model-Free and Model-Based components.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model (MB/MF) with OCI-modulated Reward Sensitivity.
    
    This model posits that OCI affects the subjective magnitude of rewards,
    scaling the impact of outcomes on value learning.
    
    Parameters:
    learning_rate: [0, 1] - Rate of updating Q-values.
    beta: [0, 10] - Inverse temperature (exploration/exploitation).
    w: [0, 1] - Weighting between Model-Based (1) and Model-Free (0) control.
    oci_reward_scale: [0, 5] - Scaling factor for reward perception based on OCI.
                              Effective Reward = Reward * (1 + OCI * scale).
    """
    learning_rate, beta, w, oci_reward_scale = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Subjective reward scaling
    # If scale is high, high OCI participants perceive rewards as much larger,
    # leading to stronger reinforcement.
    r_multiplier = 1.0 + (oci_score * oci_reward_scale)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2) # MF values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # MF values for stage 2 (also used for MB calculation)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based Value Calculation
        # V_MB(S1) = T * max(Q_S2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Softmax Policy
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Choice ---
        s_idx = int(state[trial]) # 0 or 1
        
        # Simple Softmax on Stage 2 values
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updating ---
        # Scale the reward
        r_eff = reward[trial] * r_multiplier
        
        # Stage 2 Update (TD)
        # Prediction Error = Reward - Q(S2, A2)
        delta_stage2 = r_eff - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 Update (TD)
        # Using the value of the state we landed in (classic MF)
        # Prediction Error = Q(S2, A2) - Q(S1, A1)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Model-Free with OCI-modulated "Win-Stay" Bias
This model refines the concept of perseveration. Instead of generic stickiness (repeating any action), this model implements a "Win-Stay" bias where the participant is more likely to repeat an action *only if it was rewarded*. OCI modulates the strength of this specific reinforcement bias.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner with OCI-modulated 'Win-Stay' bias.
    
    Unlike generic stickiness (repeating any choice), this model specifically 
    reinforces repeating choices that led to a reward in the previous trial.
    High OCI amplifies this 'Win-Stay' tendency.
    
    Parameters:
    learning_rate: [0, 1] - Rate of updating Q-values.
    beta: [0, 10] - Inverse temperature.
    win_stay_base: [0, 5] - Base bonus added to previously rewarded action.
    oci_win_stay_amp: [0, 5] - Additional bonus proportional to OCI score.
    """
    learning_rate, beta, win_stay_base, oci_win_stay_amp = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective win-stay bonus for this participant
    win_stay_bonus = win_stay_base + (oci_score * oci_win_stay_amp)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    last_reward = 0
    
    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        q_effective = q_stage1_mf.copy()
        
        # Apply Win-Stay Bonus
        if last_action_1 != -1 and last_reward == 1.0:
            q_effective[last_action_1] += win_stay_bonus
            
        exp_q1 = np.exp(beta * q_effective)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Choice ---
        s_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updating ---
        r = reward[trial]
        
        # Update Stage 2
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Update Stage 1
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Store history
        last_action_1 = a1
        last_reward = r

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model with OCI-modulated Transition Uncertainty
This model assumes that high OCI scores correlate with a degraded or "noisier" internal model of the environment. The Model-Based component uses a transition matrix that becomes flatter (closer to 50/50 probability) as OCI increases, representing uncertainty or distrust in the task structure.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model with OCI-modulated Transition Uncertainty.
    
    This model assumes high OCI correlates with a degraded internal model of the 
    environment structure. The Model-Based component uses a transition matrix 
    that becomes "flatter" (closer to 50/50) as OCI increases.
    
    Parameters:
    learning_rate: [0, 1] - Rate of updating Q-values.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - Weighting between Model-Based (1) and Model-Free (0).
    oci_trans_noise: [0, 1] - Degree to which OCI flattens transition probabilities.
                              0 = Perfect model (0.7/0.3). 
                              1 = Max noise allowed (tends toward 0.5/0.5).
    """
    learning_rate, beta, w, oci_trans_noise = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Distort the transition matrix based on OCI
    # Base probability is 0.7. We reduce this towards 0.5 based on OCI.
    # Max reduction range is 0.2 (from 0.7 down to 0.5).
    distortion = oci_score * oci_trans_noise * 0.2
    # Ensure we don't go below 0.5 or flip the model
    prob_common = 0.7 - distortion 
    if prob_common < 0.5: prob_common = 0.5
    
    prob_rare = 1.0 - prob_common
    
    # Subjective transition matrix
    transition_matrix = np.array([[prob_common, prob_rare], 
                                  [prob_rare, prob_common]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based Value (using distorted matrix)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Choice ---
        s_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updating ---
        r = reward[trial]
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```