Here are three new cognitive models implemented as Python functions.

### Model 1: OCI-Modulated "Lapse Rate" (Epsilon-Greedy Mixture)
This model hypothesizes that OCI symptoms are associated with attentional lapses or "noise" in decision-making that is distinct from value-based exploration. While `beta` controls the slope of the softmax curve (value sensitivity), `epsilon` adds a flat probability of random responding (a "floor" to the probabilities), representing moments where the participant acts without consulting their value model (e.g., due to distraction or intrusive thoughts).

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Epsilon-Greedy Noise (Lapse Rate).
    
    This model incorporates a mixture of Softmax (value-based) and Random 
    (epsilon-greedy) choice policies. The weight of the random component (epsilon)
    is modulated by the participant's OCI score.
    
    Hypothesis: High OCI participants may exhibit higher "lapse rates" or 
    random noise in their choices, independent of the learned values.
    
    Parameters:
    lr: [0, 1] Learning rate for value updates.
    beta: [0, 10] Inverse temperature for Softmax (value sensitivity).
    w: [0, 1] Mixing weight (0=Model-Free, 1=Model-Based).
    eps_base: [0, 0.5] Base rate of random lapses.
    oci_eps_mod: [0, 0.5] Modulation of lapse rate by OCI.
    """
    lr, beta, w, eps_base, oci_eps_mod = model_parameters
    n_trials = len(action_1)
    oci = oci[0]
  
    # Calculate epsilon and ensure it stays within bounds [0, 1]
    epsilon = eps_base + (oci * oci_eps_mod)
    epsilon = np.clip(epsilon, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based Value
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax Probability
        exp_q1 = np.exp(beta * q_net)
        probs_softmax_1 = exp_q1 / np.sum(exp_q1)
        
        # Mixture with Uniform Randomness (Lapse)
        probs_1 = (1 - epsilon) * probs_softmax_1 + epsilon * 0.5
        
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_current_state)
        probs_softmax_2 = exp_q2 / np.sum(exp_q2)
        
        # Apply same lapse rate to second stage
        probs_2 = (1 - epsilon) * probs_softmax_2 + epsilon * 0.5
        
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        # --- Updates ---
        # Stage 2 RPE (Model-Free)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += lr * delta_stage2
        
        # Stage 1 RPE (Model-Free)
        # Note: Using TD(0) update for Stage 1 MF
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += lr * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Rewarded Stickiness
This model investigates if OCI symptoms relate specifically to a compulsive "Win-Stay" tendency. Unlike general stickiness (which repeats choices regardless of outcome), this parameter adds a bonus to the previous choice *only* if it resulted in a reward. This captures a specific type of rigid reinforcement processing where success triggers strong repetition compulsion.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Modulated Rewarded Stickiness (Compulsive Win-Stay).
    
    This model adds a "stickiness" bonus to the choice logits at Stage 1, 
    but ONLY if the previous trial was rewarded. The magnitude of this 
    conditional stickiness is modulated by OCI.
    
    Hypothesis: High OCI is associated with a rigid "Win-Stay" strategy, 
    where receiving a coin triggers a compulsive repetition of the starting choice.
    
    Parameters:
    lr: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Mixing weight.
    stick_rew_base: [0, 5] Base stickiness after a reward.
    oci_stick_mod: [0, 5] Modulation of rewarded stickiness by OCI.
    """
    lr, beta, w, stick_rew_base, oci_stick_mod = model_parameters
    n_trials = len(action_1)
    oci = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    # Calculate the effective stickiness for this participant
    stickiness_bonus = stick_rew_base + (oci * oci_stick_mod)
    
    prev_choice_1 = -1
    prev_reward = 0

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Calculate logits
        logits = beta * q_net
        
        # Apply stickiness ONLY if previous trial was rewarded
        if prev_choice_1 != -1 and prev_reward == 1.0:
            logits[prev_choice_1] += stickiness_bonus

        exp_q1 = np.exp(logits)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        # --- Updates ---
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += lr * delta_stage2
        
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += lr * delta_stage1
        
        # Store history for next trial
        prev_choice_1 = int(action_1[trial])
        prev_reward = reward[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Transition Belief (Structural Bias)
This model tests whether OCI affects the internal model of the environment structure. Instead of assuming the participant perfectly knows the 70/30 transition probabilities, this model allows the participant's *belief* about the common transition probability to vary with OCI. High OCI might be associated with doubting the stability of the transitions (belief < 0.7) or over-estimating determinism (belief > 0.7).

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Transition Belief (Structural Model Distortion).
    
    This model posits that OCI affects the Model-Based system's internal map 
    of the world. Specifically, it modulates the assumed probability of the 
    common transition. 
    
    Hypothesis: High OCI participants may possess a distorted causal model, 
    either underestimating the reliability of the spaceship (distrust) or 
    overestimating it (rigidity), affecting how Model-Based values are computed.
    
    Parameters:
    lr: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Mixing weight.
    trans_belief_base: [0.5, 1.0] Base belief about common transition prob (e.g., 0.7).
    oci_belief_mod: [-0.3, 0.3] Modulation of this belief by OCI.
    """
    lr, beta, w, trans_belief_base, oci_belief_mod = model_parameters
    n_trials = len(action_1)
    oci = oci[0]
  
    # Calculate participant's subjective transition probability
    # The real world is 0.7, but the agent might believe otherwise.
    subjective_p = trans_belief_base + (oci * oci_belief_mod)
    subjective_p = np.clip(subjective_p, 0.0, 1.0)
    
    # Construct the internal transition matrix based on this belief
    # Row 0: Space A -> [Common X, Rare Y]
    # Row 1: Space B -> [Rare X, Common Y]
    internal_trans_matrix = np.array([
        [subjective_p, 1 - subjective_p], 
        [1 - subjective_p, subjective_p]
    ])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based Value calculation uses the INTERNAL (subjective) matrix
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = internal_trans_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        # --- Updates ---
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += lr * delta_stage2
        
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += lr * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```