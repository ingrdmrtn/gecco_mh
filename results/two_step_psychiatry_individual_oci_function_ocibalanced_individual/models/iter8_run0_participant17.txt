Here are three new cognitive models implemented as Python functions, exploring different mechanisms for how OCI symptoms might influence decision-making in the two-step task.

### Model 1: Structural Uncertainty Model
This model hypothesizes that high OCI scores correlate with a distrust of the environmental structure. While the "Model-Based" system usually assumes a fixed transition probability (e.g., 70% chance A $\to$ X), this model suggests high-OCI participants treat the transitions as more uncertain (closer to 50/50), effectively degrading the accuracy of their model-based planning without abandoning it entirely.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: Structural Uncertainty Model.
    
    Hypothesis: OCI symptoms degrade the precision of the internal model used 
    for planning. High OCI participants perceive the spaceship transitions 
    as less deterministic (closer to random) than they actually are, 
    diluting the effectiveness of Model-Based control.

    Parameters:
    - learning_rate: [0, 1] Value updating rate.
    - beta: [0, 10] Inverse temperature (choice consistency).
    - w: [0, 1] Weighting between Model-Based (1) and Model-Free (0).
    - oci_uncertainty: [0, 1] How much OCI flattens the transition matrix belief.
      0 = Standard belief (70/30), 1 = High OCI leads to max uncertainty (50/50).
    """
    learning_rate, beta, w, oci_uncertainty = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate the perceived transition probability based on OCI
    # Standard is 0.7. Max uncertainty pushes this towards 0.5.
    # If oci_uncertainty is 1 and oci_score is 1, p_common becomes 0.5.
    distortion = oci_score * oci_uncertainty
    # Clamp distortion to ensure p_common doesn't flip below 0.5
    if distortion > 1.0: distortion = 1.0
    
    p_common = 0.7 - (0.2 * distortion) 
    transition_matrix = np.array([[p_common, 1-p_common], [1-p_common, p_common]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based Value Calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Value
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        s_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        r = reward[trial]
        
        # Stage 2 Update (Model-Free)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 Update (Model-Free TD)
        # Using the value of the state actually arrived at (SARSA-like connection)
        v_stage2_arrival = q_stage2_mf[s_idx, a2]
        delta_stage1 = v_stage2_arrival - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Superstitious Win-Stay Model
This model posits that OCI specifically drives "superstitious" reinforcement. While previous models explored general stickiness, this model suggests that high-OCI individuals are hypersensitive to positive outcomes, forming a compulsive urge to repeat a choice *only* if it was previously rewarded (Win-Stay), regardless of whether that choice was actually optimal in the grand scheme.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: Superstitious Win-Stay Model.
    
    Hypothesis: High OCI scores drive a specific form of perseveration: 
    "Win-Stay". Unlike general stickiness, this compulsion is only triggered 
    by receiving a reward, representing a superstitious belief that repeating 
    the exact action will reproduce the success.

    Parameters:
    - learning_rate: [0, 1]
    - beta: [0, 10]
    - w: [0, 1] Model-Based/Model-Free balance.
    - oci_win_stickiness: [0, 5] Additional value added to the previously 
      chosen action ONLY if the previous trial was rewarded, scaled by OCI.
    """
    learning_rate, beta, w, oci_win_stickiness = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    last_reward = 0

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Apply Superstitious Win-Stickiness
        # If we chose an action last time AND got a reward, boost that action's value
        if last_action_1 != -1 and last_reward == 1.0:
            stickiness_bonus = oci_score * oci_win_stickiness
            q_net[last_action_1] += stickiness_bonus
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # Track history for next trial
        last_action_1 = a1
        
        # --- Stage 2 Policy ---
        s_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        r = reward[trial]
        last_reward = r # Store for next trial's stickiness check
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        v_stage2_arrival = q_stage2_mf[s_idx, a2]
        delta_stage1 = v_stage2_arrival - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Stage-Specific Anxiety Model
This model differentiates between the abstract planning stage (Stage 1) and the concrete reward stage (Stage 2). It hypothesizes that OCI-related anxiety creates noise specifically when facing the immediate outcome (Stage 2), resulting in a dampened inverse temperature ($\beta$) for the second choice, while the first choice remains more stable.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: Stage-Specific Anxiety Model.
    
    Hypothesis: OCI manifests as anxiety-induced exploration or noise specifically 
    at the second stage (where immediate rewards are delivered). High OCI 
    reduces the inverse temperature (beta) at Stage 2 relative to Stage 1, 
    making alien selection more random/erratic compared to spaceship selection.

    Parameters:
    - learning_rate: [0, 1]
    - beta_stage1: [0, 10] Inverse temperature for the first choice (spaceships).
    - w: [0, 1] Model-Based weight.
    - oci_stage2_dampening: [0, 5] Factor by which OCI reduces beta at stage 2.
      Formula: beta_stage2 = beta_stage1 / (1 + oci * dampening).
    """
    learning_rate, beta_stage1, w, oci_stage2_dampening = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate Stage 2 Beta based on OCI
    # If OCI is high, the denominator increases, lowering beta_stage2 (more noise)
    beta_stage2 = beta_stage1 / (1.0 + (oci_score * oci_stage2_dampening))
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy (Uses beta_stage1) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta_stage1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy (Uses OCI-modulated beta_stage2) ---
        s_idx = int(state[trial])
        exp_q2 = np.exp(beta_stage2 * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        r = reward[trial]
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        v_stage2_arrival = q_stage2_mf[s_idx, a2]
        delta_stage1 = v_stage2_arrival - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```