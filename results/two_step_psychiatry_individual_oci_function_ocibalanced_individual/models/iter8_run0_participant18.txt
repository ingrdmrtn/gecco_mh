Here are three new cognitive models based on the provided template and constraints.

### Model 1: OCI-Modulated Learning Rate (Plasticity)
This model tests the hypothesis that OCI scores correlate with cognitive rigidity or plasticity. Specifically, the global learning rate (how quickly both Model-Free stage 1 and stage 2 values are updated) is modulated by the OCI score. A lower learning rate for high OCI might indicate resistance to updating beliefs (rigidity), while a higher rate might indicate hyper-responsiveness.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid learner where the global learning rate is modulated by OCI.
    
    Hypothesis: OCI is associated with cognitive plasticity/rigidity.
    The learning rate is a linear function of the OCI score.
    
    Parameters:
    - lr_base: Baseline learning rate.
    - lr_oci_slope: How OCI affects the learning rate.
    - beta: Inverse temperature (softmax randomness).
    - w: Weighting parameter (0 = pure MF, 1 = pure MB).
    
    Bounds:
    lr_base: [0, 1]
    lr_oci_slope: [-1, 1] (Assuming OCI is normalized or small, resulting LR clipped to [0,1])
    beta: [0, 10]
    w: [0, 1]
    """
    lr_base, lr_oci_slope, beta, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective learning rate
    learning_rate = lr_base + (lr_oci_slope * oci_score)
    learning_rate = np.clip(learning_rate, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Policy for the first choice ---
        # 1. Model-Based Value
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Hybrid Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # 3. Softmax
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Policy for the second choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # --- Updates ---
        # Update Stage 1 MF value (TD-learning using Stage 2 value as proxy)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Update Stage 2 MF value (Direct Reinforcement)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Transition Belief
This model assumes that OCI affects the internal model of the environment's structure. While the true transition probabilities are fixed (70/30), participants with varying OCI levels might perceive the world as more deterministic (e.g., believing the transition is 90/10) or more uncertain (e.g., believing it is 50/50). This specifically alters the Model-Based calculation.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid learner where the internal belief about transition probabilities 
    is modulated by OCI.
    
    Hypothesis: High OCI individuals may have distorted beliefs about 
    environmental uncertainty (e.g., intolerance of uncertainty leading 
    to an assumption of determinism).
    
    Parameters:
    - learning_rate: Learning rate for MF values.
    - beta: Inverse temperature.
    - w: Weighting parameter.
    - prob_base: Baseline belief of the common transition probability (e.g., 0.7).
    - prob_oci: How OCI shifts this belief.
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    w: [0, 1]
    prob_base: [0, 1]
    prob_oci: [-0.5, 0.5]
    """
    learning_rate, beta, w, prob_base, prob_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    # Calculate subjective transition probability
    p_belief = prob_base + (prob_oci * oci_score)
    p_belief = np.clip(p_belief, 0.0, 1.0)
    
    # Construct subjective transition matrix
    transition_matrix = np.array([[p_belief, 1-p_belief], [1-p_belief, p_belief]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Policy for the first choice ---
        # Model-Based calculation uses the SUBJECTIVE transition matrix
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Policy for the second choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # --- Updates ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Memory Decay (Forgetting)
This model introduces a passive forgetting mechanism for the Model-Free values of the *unchosen* options. The rate of this decay is modulated by OCI. This tests if OCI is related to the persistence of value representations (e.g., high OCI individuals might hold onto old values longer, or conversely, have more volatile memory).

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid learner with passive memory decay on unchosen options, 
    modulated by OCI.
    
    Hypothesis: OCI affects how quickly action values decay/are forgotten 
    when not experienced.
    
    Parameters:
    - learning_rate: Learning rate for chosen options.
    - beta: Inverse temperature.
    - w: Weighting parameter.
    - decay_base: Baseline decay rate for unchosen options.
    - decay_oci: How OCI modulates decay.
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    w: [0, 1]
    decay_base: [0, 1]
    decay_oci: [-1, 1]
    """
    learning_rate, beta, w, decay_base, decay_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective decay rate
    decay_rate = decay_base + (decay_oci * oci_score)
    decay_rate = np.clip(decay_rate, 0.0, 1.0)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Policy for the first choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Policy for the second choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # --- Updates ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Decay the unchosen option in Stage 1
        unchosen_a1 = 1 - a1
        q_stage1_mf[unchosen_a1] *= (1.0 - decay_rate)
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Decay the unchosen option in Stage 2 (for the current state)
        unchosen_a2 = 1 - a2
        q_stage2_mf[s_idx, unchosen_a2] *= (1.0 - decay_rate)
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```