Here are three new cognitive models incorporating the OCI score into the decision-making process for the two-step task. These models explore hypotheses related to model-based control, learning rate asymmetry, and uncertainty-driven exploration.

### Model 1: OCI-Modulated Model-Based Weighting (Sigmoid)
This model tests the hypothesis that higher OCI scores lead to a shift in the balance between Model-Based (planning) and Model-Free (habitual) control. Specifically, it uses a sigmoid function to map the OCI score to the mixing weight `w`, allowing for a non-linear relationship where extreme scores might push the participant strongly towards one system.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL with OCI-modulated Model-Based weighting via a sigmoid function.
    
    Hypothesis: The balance between model-based (planning) and model-free (habitual) 
    systems is modulated by OCI severity. This model maps the OCI score to the 
    mixing weight 'w' using a sigmoid transformation centered on a learnable bias,
    allowing for non-linear shifts in strategy based on symptom severity.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate for value updates.
    beta: [0, 10] - Inverse temperature (softmax sensitivity).
    w_bias: [-5, 5] - Bias term shifting the inflection point of the sigmoid for OCI impact.
    w_slope: [-10, 10] - Slope determining the direction and steepness of OCI's effect on 'w'.
    """
    learning_rate, beta, w_bias, w_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Sigmoid mapping of OCI to weight w
    # w approaches 1 (fully MB) or 0 (fully MF) depending on slope and OCI
    w = 1 / (1 + np.exp(-(w_slope * oci_score + w_bias)))

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # Policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])

        # Policy for the second choice
        qs_stage2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_stage2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # Updates
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        
        # SARSA-style update for stage 1 MF
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

        # Prediction error update for stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Punishment Sensitivity
This model investigates whether OCI scores correlate with a differential sensitivity to negative outcomes (lack of reward). It posits that individuals with higher OCI scores might learn more drastically from "punishments" (getting 0 coins), effectively having a higher learning rate for negative prediction errors compared to positive ones.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL with OCI-modulated punishment learning rate.
    
    Hypothesis: High OCI scores relate to increased sensitivity to negative outcomes 
    (or lack of reward). This model splits the learning rate into a base positive 
    learning rate and a negative learning rate that scales with OCI.
    
    Parameters:
    alpha_pos: [0, 1] - Learning rate for positive prediction errors (reward > expectation).
    alpha_neg_base: [0, 1] - Base learning rate for negative prediction errors.
    alpha_neg_oci_scale: [0, 2] - How much OCI increases the learning rate for negative errors.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - Model-based / Model-free mixing weight.
    """
    alpha_pos, alpha_neg_base, alpha_neg_oci_scale, beta, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate the effective negative learning rate, capped at 1.0
    alpha_neg = min(1.0, alpha_neg_base + (alpha_neg_oci_scale * oci_score))

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # Policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])

        # Policy for the second choice
        qs_stage2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_stage2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # Updates
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        
        # Stage 1 MF update
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        lr_1 = alpha_pos if delta_stage1 > 0 else alpha_neg
        q_stage1_mf[a1] += lr_1 * delta_stage1

        # Stage 2 update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        lr_2 = alpha_pos if delta_stage2 > 0 else alpha_neg
        q_stage2_mf[state_idx, a2] += lr_2 * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Driven Inverse Temperature (Exploration/Exploitation)
This model proposes that OCI symptoms affect the randomness of choice behavior (exploration vs. exploitation). It tests if higher OCI scores lead to more deterministic, rigid behavior (higher beta) or more chaotic behavior (lower beta) by scaling the inverse temperature parameter directly with the OCI score.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL with OCI-modulated inverse temperature (Beta).
    
    Hypothesis: OCI symptoms impact the exploration-exploitation trade-off. 
    This model scales the inverse temperature (beta) based on the OCI score.
    Higher OCI might lead to more rigid/deterministic choices (higher beta), 
    or potentially more noise.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate.
    beta_base: [0, 10] - Base inverse temperature.
    beta_oci_scale: [-5, 5] - Scaling factor for OCI's effect on beta.
    w: [0, 1] - Model-based / Model-free mixing weight.
    """
    learning_rate, beta_base, beta_oci_scale, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective beta, ensuring it stays non-negative
    beta_eff = max(0.0, beta_base + (beta_oci_scale * oci_score))

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # Policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_eff * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])

        # Policy for the second choice
        qs_stage2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta_eff * qs_stage2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # Updates
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```