Here are three new cognitive models that introduce different mechanisms for how OCI might influence decision-making in the two-step task. These models explore concepts like habit formation (habitization rate), asymmetric learning rates based on prediction errors, and noise modulation.

### Model 1: Hybrid RL with OCI-Modulated Habitization
This model hypothesizes that high OCI scores correlate with a faster transition from goal-directed (Model-Based) to habitual (Model-Free) control. Instead of a fixed mixing weight $w$, the weight shifts dynamically over trials, with the rate of this shift determined by OCI.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL with OCI-modulated Habitization Rate.
    
    Hypothesis: High OCI accelerates the transition from Model-Based (MB) to 
    Model-Free (MF) control. The mixing weight 'w' starts at pure MB (1.0) and 
    decays toward MF at a rate determined by the OCI score.
    
    Parameters:
    - learning_rate: [0, 1] Value update speed.
    - beta: [0, 10] Inverse temperature.
    - habit_decay_base: [0, 1] Base decay rate of MB control (lower = faster decay).
    - oci_decay_accel: [0, 1] How much OCI accelerates this decay.
    """
    learning_rate, beta, habit_decay_base, oci_decay_accel = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate decay rate: Higher OCI -> lower decay factor -> faster drop to MF
    # Ensure decay_rate stays in [0, 1]
    decay_rate = habit_decay_base * (1.0 - (current_oci * oci_decay_accel))
    decay_rate = np.clip(decay_rate, 0.0, 1.0)
    
    # Initialize mixing weight w (starts fully Model-Based)
    w = 1.0 

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based Value
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Value (weighted sum)
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        r = reward[trial]
        
        # TD Errors
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        
        # Value Updates
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Habitization: w decays over time
        w *= decay_rate

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Model-Free RL with OCI-Modulated Punishment Sensitivity
This model focuses on the asymmetry of learning. It hypothesizes that individuals with higher OCI scores are hypersensitive to "punishment" (lack of reward). The learning rate for negative prediction errors is scaled up by the OCI score, leading to stronger avoidance learning.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free RL with OCI-modulated Punishment Sensitivity.
    
    Hypothesis: High OCI leads to hypersensitivity to negative outcomes (0 reward).
    The learning rate is split into alpha_pos (for positive RPE) and alpha_neg.
    The magnitude of alpha_neg is boosted by the OCI score.
    
    Parameters:
    - alpha_pos: [0, 1] Learning rate for positive prediction errors.
    - alpha_neg_base: [0, 1] Base learning rate for negative prediction errors.
    - beta: [0, 10] Inverse temperature.
    - oci_punish_boost: [0, 2] Multiplier for OCI to increase alpha_neg.
    """
    alpha_pos, alpha_neg_base, beta, oci_punish_boost = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    # Calculate effective negative learning rate
    alpha_neg = alpha_neg_base + (current_oci * oci_punish_boost)
    alpha_neg = np.clip(alpha_neg, 0.0, 1.0)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        r = reward[trial]
        
        # Stage 2 Update
        delta2 = r - q_stage2[state_idx, a2]
        lr2 = alpha_pos if delta2 > 0 else alpha_neg
        q_stage2[state_idx, a2] += lr2 * delta2
        
        # Stage 1 Update
        delta1 = q_stage2[state_idx, a2] - q_stage1[a1]
        lr1 = alpha_pos if delta1 > 0 else alpha_neg
        q_stage1[a1] += lr1 * delta1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid RL with OCI-Driven Inverse Temperature (Noise)
This model posits that OCI affects the exploration/exploitation trade-off globally. Specifically, high OCI might be associated with more rigid, deterministic responding (lower noise/higher beta) due to anxiety or a desire for control.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL with OCI-modulated Inverse Temperature.
    
    Hypothesis: OCI modulates the 'beta' parameter (inverse temperature). 
    Higher OCI leads to higher beta (stiffer, more deterministic choices), 
    reflecting a rigid adherence to current value estimates and reduced exploration.
    
    Parameters:
    - learning_rate: [0, 1] Value update speed.
    - beta_base: [0, 10] Baseline inverse temperature when OCI is 0.
    - oci_beta_stiffness: [0, 5] How much OCI increases beta (making choices more rigid).
    - w: [0, 1] Mixing weight (0=MF, 1=MB).
    """
    learning_rate, beta_base, oci_beta_stiffness, w = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate effective beta
    beta_eff = beta_base + (current_oci * oci_beta_stiffness)
    beta_eff = np.clip(beta_eff, 0.0, 20.0) # Allow it to go slightly higher than base bound

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based Value
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_eff * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta_eff * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        r = reward[trial]
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```