Here are three new cognitive models based on the provided template and constraints.

### Model 1: OCI-Modulated Subjective Reward Magnitude
This model hypothesizes that OCI symptoms alter the subjective valuation of the reward outcomes (the asymptote of learning), rather than the speed of learning. It proposes that high OCI individuals perceive the magnitude of rewards (and losses) more intensely than low OCI individuals, leading to higher Q-values and potentially more extreme behavior.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Subjective Reward Magnitude.
    
    This model assumes that OCI scores scale the subjective magnitude of the reward received.
    Unlike learning rate modulation (which affects speed), this affects the asymptote 
    of the value function (Q-values converge to a higher/lower absolute value).
    
    Parameters:
    - learning_rate: [0, 1] Standard learning rate.
    - beta: [0, 10] Inverse temperature for softmax.
    - w: [0, 1] Weighting parameter (0=Model-Free, 1=Model-Based).
    - lambda_eligibility: [0, 1] Eligibility trace for Stage 1 updates.
    - oci_reward_scale: [0, 2] Scaling factor for reward perception based on OCI.
      Effective Reward = Reward * (1 + oci_reward_scale * OCI).
    """
    learning_rate, beta, w, lambda_eligibility, oci_reward_scale = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combined Q-value
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        state_idx = int(state[trial])

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
  
        # Action value updating
        # Modulate reward based on OCI
        effective_reward = reward[trial] * (1.0 + oci_reward_scale * oci_score)

        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = effective_reward - q_stage2_mf[state_idx, a2]
        
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        q_stage1_mf[a1] += learning_rate * delta_stage1 + (learning_rate * lambda_eligibility * delta_stage2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Specific Stage 1 Rigidity (Beta Asymmetry)
This model suggests that OCI specifically impacts the decision-making "temperature" (randomness vs. determinism) at the planning stage (Stage 1) but not the outcome stage (Stage 2). High OCI is modeled as increased rigidity (higher beta) in the initial choice, reflecting a compulsion to stick to the "best" plan, while the second stage remains standard.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Specific Stage 1 Rigidity (Beta Asymmetry).
    
    This model posits that OCI increases the inverse temperature (beta) specifically
    for the first-stage choice (planning), making high-OCI participants more rigid/deterministic 
    in their spaceship choice, while their second-stage choice remains more exploratory/standard.
    
    Parameters:
    - learning_rate: [0, 1] Standard learning rate.
    - beta_base: [0, 10] Baseline inverse temperature.
    - w: [0, 1] Weighting parameter (0=Model-Free, 1=Model-Based).
    - lambda_eligibility: [0, 1] Eligibility trace.
    - oci_beta_stiffening: [0, 5] How much OCI increases beta for Stage 1.
      Beta_Stage1 = beta_base * (1 + oci_beta_stiffening * OCI).
      Beta_Stage2 = beta_base.
    """
    learning_rate, beta_base, w, lambda_eligibility, oci_beta_stiffening = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Calculate stage-specific betas
    beta_stage1 = beta_base * (1.0 + oci_beta_stiffening * oci_score)
    beta_stage2 = beta_base

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Use stiffened beta for stage 1
        exp_q1 = np.exp(beta_stage1 * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        state_idx = int(state[trial])

        # policy for the second choice
        # Use base beta for stage 2
        exp_q2 = np.exp(beta_stage2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
  
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        q_stage1_mf[a1] += learning_rate * delta_stage1 + (learning_rate * lambda_eligibility * delta_stage2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Amplified Loss Aversion
This model draws on the concept that compulsive symptoms often stem from an excessive fear of negative outcomes. It modifies the reward processing such that negative rewards (losses) are amplified by the OCI score, creating a stronger "avoidance" signal in the Q-value updates, while positive rewards are processed normally.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Amplified Loss Aversion.
    
    This model assumes that OCI modulates the perception of negative rewards (losses).
    If the reward is negative, the effective reward used for the prediction error 
    is amplified (made more negative) by the OCI score. Positive rewards are untouched.
    
    Parameters:
    - learning_rate: [0, 1] Standard learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weighting parameter (0=Model-Free, 1=Model-Based).
    - lambda_eligibility: [0, 1] Eligibility trace.
    - oci_loss_amp: [0, 5] Multiplier for negative rewards.
      If Reward < 0: Effective_Reward = Reward * (1 + oci_loss_amp * OCI).
    """
    learning_rate, beta, w, lambda_eligibility, oci_loss_amp = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        state_idx = int(state[trial])

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
  
        # Action value updating with Loss Aversion
        r = reward[trial]
        if r < 0:
            effective_r = r * (1.0 + oci_loss_amp * oci_score)
        else:
            effective_r = r

        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = effective_r - q_stage2_mf[state_idx, a2]
        
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        q_stage1_mf[a1] += learning_rate * delta_stage1 + (learning_rate * lambda_eligibility * delta_stage2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```