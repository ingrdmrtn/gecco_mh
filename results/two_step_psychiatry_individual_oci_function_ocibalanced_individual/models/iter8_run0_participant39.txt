Here are three new cognitive models for the two-step task, incorporating OCI scores to explain individual variability.

### Model 1: Pure Model-Free TD($\lambda$) with OCI-modulated Eligibility Trace
This model assumes participants rely on a Model-Free strategy but vary in how they assign credit to the first-stage choice based on the second-stage outcome. The eligibility trace parameter $\lambda$ controls this: $\lambda=0$ corresponds to pure TD learning (bootstrapping from stage 2 value), while $\lambda=1$ corresponds to Monte Carlo learning (learning directly from the reward). We hypothesize that OCI modulates this $\lambda$ parameter, potentially reflecting a reliance on concrete outcomes (high $\lambda$) vs. predicted values (low $\lambda$) due to anxiety or compulsivity.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Pure Model-Free TD(lambda) learner where OCI modulates the eligibility trace (lambda).
    
    This model tests if high OCI participants differ in how they assign credit 
    to the first-stage choice (outcomes vs. expectations).
    
    Parameters:
    - learning_rate: [0, 1]
    - beta: [0, 10] (Inverse temperature)
    - stickiness: [0, 5] (Choice perseveration)
    - lambda_base: [0, 1] (Base eligibility trace)
    - lambda_oci_slope: [-1, 1] (Effect of OCI on lambda)
    """
    learning_rate, beta, stickiness, lambda_base, lambda_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate OCI-modulated lambda, clipped between 0 and 1
    lambda_val = lambda_base + (lambda_oci_slope * oci_score)
    lambda_val = np.clip(lambda_val, 0.0, 1.0)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    
    last_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        q_stage1_with_stick = q_stage1_mf.copy()
        if last_action_1 != -1:
            q_stage1_with_stick[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_stage1_with_stick)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        last_action_1 = a1
        
        s_idx = int(state[trial])

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        r = reward[trial]

        # --- Updates ---
        # Stage 2 update (standard Q-learning)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

        # Stage 1 update (TD-Lambda)
        # The update combines the TD error from the transition (stage 1 -> stage 2)
        # and the TD error from the reward (stage 2 -> reward) weighted by lambda.
        # Equivalent to: Q1 += alpha * ( (Q2 - Q1) + lambda * (R - Q2) )
        
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1] # Prediction error at transition
        
        # Combined update:
        # If lambda=0: Q1 updates towards Q2 (Pure TD)
        # If lambda=1: Q1 updates towards R (Monte Carlo)
        update_val = delta_stage1 + lambda_val * delta_stage2
        q_stage1_mf[a1] += learning_rate * update_val

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model with Independent MB/MF Control (OCI-modulated MB Beta)
Instead of a mixing parameter $w$, this model assigns independent inverse temperatures ($\beta$) to the Model-Based (MB) and Model-Free (MF) systems. The MF beta is fixed, but the MB beta is modulated by OCI. This allows OCI to specifically dampen or amplify the influence of the goal-directed system without necessarily affecting the randomness of the habitual system.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid learner with independent Betas for MB and MF systems.
    OCI modulates the Beta (strength) of the Model-Based system specifically.
    
    This tests if high OCI is associated with a specific deficit (or over-reliance)
    in the goal-directed system's control over choice, independent of MF strength.
    
    Parameters:
    - learning_rate: [0, 1]
    - beta_mf: [0, 10] (Inverse temp for Model-Free values)
    - beta_mb_base: [0, 10] (Base inverse temp for Model-Based values)
    - beta_mb_oci_slope: [-5, 5] (Effect of OCI on MB beta)
    - stickiness: [0, 5]
    """
    learning_rate, beta_mf, beta_mb_base, beta_mb_oci_slope, stickiness = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate OCI-modulated MB beta
    beta_mb = beta_mb_base + (beta_mb_oci_slope * oci_score)
    beta_mb = np.clip(beta_mb, 0.0, 20.0) # Allow slightly higher range for strong MB

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):

        # --- Stage 1 Choice ---
        # Model-Based valuation: V(S2) = max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combine logits: beta_mf * Q_mf + beta_mb * Q_mb + stickiness
        logits = (beta_mf * q_stage1_mf) + (beta_mb * q_stage1_mb)
        
        if last_action_1 != -1:
            logits[last_action_1] += stickiness
            
        # Softmax on combined logits
        # Subtract max for numerical stability
        logits = logits - np.max(logits)
        exp_logits = np.exp(logits)
        probs_1 = exp_logits / np.sum(exp_logits)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        last_action_1 = a1
        
        s_idx = int(state[trial])

        # --- Stage 2 Choice ---
        # Standard Softmax on Stage 2 values (using beta_mf as the general noise parameter here)
        exp_q2 = np.exp(beta_mf * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        r = reward[trial]

        # --- Updates ---
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model with OCI-modulated "Surprise Penalty"
This model introduces a mechanism based on "Intolerance of Uncertainty," a common trait in OCD. If a rare transition occurs (e.g., Spaceship A goes to Planet Y), the agent applies a learning penalty to the chosen spaceship, effectively punishing the action that led to the unexpected state, regardless of the subsequent reward. The magnitude of this penalty is modulated by OCI.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid learner with an OCI-modulated 'Surprise Penalty'.
    
    If a rare transition occurs, the model applies a penalty to the Stage 1 choice
    Q-value, simulating Intolerance of Uncertainty.
    
    Parameters:
    - learning_rate: [0, 1]
    - beta: [0, 10]
    - w: [0, 1] (MB/MF mixing weight)
    - penalty_base: [0, 2] (Base penalty for rare transitions)
    - penalty_oci_slope: [-2, 2] (Effect of OCI on penalty magnitude)
    """
    learning_rate, beta, w, penalty_base, penalty_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate penalty
    surprise_penalty = penalty_base + (penalty_oci_slope * oci_score)
    # Penalty typically shouldn't be negative (which would be a bonus), but we allow flexibility
    # We clip to reasonable bounds to prevent instability
    surprise_penalty = np.clip(surprise_penalty, -1.0, 5.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        s_idx = int(state[trial])

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        r = reward[trial]

        # --- Updates ---
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

        # Standard MF update
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Surprise Penalty Update
        # Common transitions: (Action 0 -> State 0) or (Action 1 -> State 1)
        # Rare transitions: (Action 0 -> State 1) or (Action 1 -> State 0)
        is_rare = (a1 == 0 and s_idx == 1) or (a1 == 1 and s_idx == 0)
        
        if is_rare:
            # Apply penalty. We multiply by learning rate to keep scale consistent with updates
            q_stage1_mf[a1] -= learning_rate * surprise_penalty

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```