Here are three new cognitive models based on the Obsessive-Compulsive Inventory (OCI) score.

### Model 1: Subjective Transition Distortion Model
**Hypothesis:** OCI is associated with "doubt" and uncertainty about environmental structure. While the true transition probability is 0.7, high-OCI participants may subjectively "flatten" this probability, trusting the common/rare distinction less. This degrades the accuracy of their Model-Based (MB) calculations.
**Mechanism:** The transition matrix used for MB planning is modulated by OCI. The subjective probability of a common transition ($P_{common}$) starts at 0.7 but is reduced by the OCI score towards 0.5 (randomness).

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: Subjective Transition Distortion Model.
    
    Hypothesis: High OCI scores lead to a "flattening" of the perceived transition matrix.
    Participants with high OCI doubt the stability of the spaceship-planet transitions,
    effectively treating the environment as more random (less predictable) during 
    Model-Based planning.

    Parameters:
    - learning_rate: [0, 1] Standard value update rate.
    - beta: [0, 10] Inverse temperature (choice consistency).
    - w: [0, 1] Weighting parameter (0=Model-Free, 1=Model-Based).
    - distortion_rate: [0, 0.4] How much OCI reduces the subjective probability of 
      common transitions. 
      (Max distortion 0.4 would turn 0.7 into 0.3, effectively reversing the model).
    """
    learning_rate, beta, w, distortion_rate = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate subjective transition probability
    # True is 0.7. OCI reduces this confidence.
    # We clip it so it doesn't go below 0.5 (pure uncertainty) to maintain model logic.
    raw_p = 0.7 - (distortion_rate * oci_score)
    p_common = np.maximum(raw_p, 0.5) 
    p_rare = 1.0 - p_common
    
    # The subject's internal model of the world:
    subjective_trans_matrix = np.array([[p_common, p_rare], [p_rare, p_common]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2) # Model-free Q values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Q values for stage 2 (State x Action)

    for trial in range(n_trials):

        # --- Stage 1 Choice ---
        # 1. Model-Based Value Calculation
        # Bellman equation: Transition * Max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = subjective_trans_matrix @ max_q_stage2

        # 2. Hybrid Value Calculation
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)

        # 3. Action Selection
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Choice ---
        s_idx = int(state[trial])
        
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # --- Learning / Updating ---
        r = reward[trial]

        # Update Stage 2 Q-values (Standard Q-Learning)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

        # Update Stage 1 Model-Free Q-values (TD(0) / SARSA-like update)
        # Using the value of the state actually reached
        v_stage2_arrival = q_stage2_mf[s_idx, a2]
        delta_stage1 = v_stage2_arrival - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Post-Error Rigidity Model
**Hypothesis:** OCI involves a "fear of failure" and perfectionism. When a participant receives a 0 outcome (failure/loss), they become anxious and rigid.
**Mechanism:** Instead of changing how they *learn* (learning rate), OCI modulates how they *act* on the next trial. Specifically, the inverse temperature ($\beta$) increases significantly for the trial immediately following a 0-reward outcome. This represents "clamping down" or "freezing" on their current best guess after an error.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: Post-Error Rigidity Model.
    
    Hypothesis: High OCI is linked to perfectionism and error-sensitivity. 
    Following a trial with 0 reward (perceived as an error), high-OCI participants
    become "rigid" in their decision making. This is modeled as a temporary 
    increase in the inverse temperature (beta), making choices more deterministic
    immediately after a loss.

    Parameters:
    - learning_rate: [0, 1] Value update rate.
    - beta_base: [0, 10] Baseline inverse temperature.
    - w: [0, 1] Weighting (0=MF, 1=MB).
    - post_error_stiffness: [0, 5] Additional beta added on the trial after a 0-reward, scaled by OCI.
    """
    learning_rate, beta_base, w, post_error_stiffness = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Standard transition matrix for MB
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_reward = 1.0 # Initialize assuming no error before trial 1

    for trial in range(n_trials):

        # Determine current Beta based on previous outcome
        current_beta = beta_base
        if last_reward == 0.0:
            # If last trial was a failure, OCI increases rigidity (higher beta)
            current_beta += (post_error_stiffness * oci_score)

        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)

        exp_q1 = np.exp(current_beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Choice ---
        s_idx = int(state[trial])
        
        # We assume rigidity applies to both stages
        exp_q2 = np.exp(current_beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        r = reward[trial]
        last_reward = r # Store for next trial's beta calculation

        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

        v_stage2_arrival = q_stage2_mf[s_idx, a2]
        delta_stage1 = v_stage2_arrival - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Cross-Option Generalization (Leakage) Model
**Hypothesis:** OCI is associated with difficulty separating specific cues (over-generalization of fear/safety). When a participant updates the value of the *chosen* spaceship, a portion of that learning "leaks" to the *unchosen* spaceship.
**Mechanism:** High OCI causes the Q-value update ($\delta$) to be applied partially to the unchosen option as well. This blurs the distinction between the two options (A and U), making it harder for the participant to maintain distinct value representations.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: Cross-Option Generalization (Leakage) Model.
    
    Hypothesis: High OCI leads to "value leakage" or over-generalization. 
    When the participant updates the value of the chosen spaceship (Stage 1), 
    a fraction of this update is also applied to the UNCHOSEN spaceship.
    This reduces the contrast between options (A vs U), representing a confusion
    or inability to segregate outcome contingencies.

    Parameters:
    - learning_rate: [0, 1]
    - beta: [0, 10]
    - w: [0, 1]
    - leakage_factor: [0, 1] The proportion of the learning update that leaks 
      to the unchosen option, scaled by OCI.
    """
    learning_rate, beta, w, leakage_factor = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Choice ---
        s_idx = int(state[trial])
        
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        r = reward[trial]

        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

        # Stage 1 MF Update with LEAKAGE
        v_stage2_arrival = q_stage2_mf[s_idx, a2]
        delta_stage1 = v_stage2_arrival - q_stage1_mf[a1]
        
        # Update chosen option
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Update UNCHOSEN option (Leakage)
        # We calculate the leakage amount based on OCI
        current_leakage = leakage_factor * oci_score
        # Ensure leakage doesn't exceed 1.0 (though param bound handles most cases)
        if current_leakage > 1.0: current_leakage = 1.0
            
        unchosen_a1 = 1 - a1
        # The unchosen option receives a "ghost" update
        q_stage1_mf[unchosen_a1] += learning_rate * delta_stage1 * current_leakage

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```