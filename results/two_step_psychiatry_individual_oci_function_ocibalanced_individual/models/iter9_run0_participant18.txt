Here are three new cognitive models exploring different mechanisms by which OCI scores might modulate reinforcement learning processes in the two-step task.

### Model 1: OCI-Modulated Model-Based Weighting (Logistic)
This model hypothesizes that high OCI scores are associated with a rigid reliance on habitual (model-free) responding, leading to a reduced weight ($w$) on goal-directed (model-based) planning. To ensure the weight stays within valid bounds [0, 1], the OCI modulation is applied through a logistic sigmoid function.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model where the balance between model-based and model-free control (w) 
    is modulated by OCI using a logistic function.
    
    Hypothesis: Higher OCI scores lead to reduced model-based control (lower w),
    representing a bias towards habitual behavior.
    
    Parameters:
    - learning_rate: Learning rate for value updates.
    - beta: Inverse temperature for softmax.
    - w_logit_intercept: Baseline log-odds of being model-based.
    - w_oci_slope: Effect of OCI on the log-odds of w (negative implies OCI reduces MB).
    
    Bounds:
    learning_rate: [0,1]
    beta: [0,10]
    w_logit_intercept: [-5, 5]
    w_oci_slope: [-5, 5]
    """
    learning_rate, beta, w_logit_intercept, w_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    # Calculate w using logistic function to keep it between 0 and 1
    logit_w = w_logit_intercept + (w_oci_slope * oci_score)
    w = 1.0 / (1.0 + np.exp(-logit_w))

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Weighted mixture of Model-Based and Model-Free values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Reward Sensitivity
This model posits that OCI symptoms alter the subjective valuation of rewards. Specifically, individuals with higher OCI scores might perceive the feedback (coins) as more salient (hypersensitivity to feedback) or less salient (blunted response). This is implemented by scaling the reward term $R$ by a factor derived from the OCI score before the prediction error calculation.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model where reward sensitivity is modulated by OCI.
    
    Hypothesis: OCI affects how strongly the agent reacts to the reward outcome.
    The effective reward is R_eff = R * (sensitivity_base + sensitivity_oci * OCI).
    
    Parameters:
    - learning_rate: Learning rate.
    - beta: Inverse temperature.
    - w: Model-based weight.
    - sens_base: Baseline reward sensitivity.
    - sens_oci: Modulation of sensitivity by OCI.
    
    Bounds:
    learning_rate: [0,1]
    beta: [0,10]
    w: [0,1]
    sens_base: [0, 2]
    sens_oci: [-2, 2]
    """
    learning_rate, beta, w, sens_base, sens_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Calculate effective reward sensitivity
    sensitivity = sens_base + (sens_oci * oci_score)
    # Ensure sensitivity doesn't flip the sign of the reward (assume non-negative perception)
    sensitivity = np.maximum(sensitivity, 0.0)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Scale reward by sensitivity parameter
        effective_reward = r * sensitivity
        
        delta_stage2 = effective_reward - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Decay Rate (Forgetting)
This model introduces a memory decay parameter (forgetting rate) for unchosen options. The hypothesis is that OCI symptoms, often characterized by intrusive thoughts or rumination, might interfere with the maintenance of value representations, leading to faster or slower decay of values for states not currently visited.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model with value decay (forgetting) modulated by OCI.
    
    Hypothesis: Unchosen action values decay toward 0. The rate of this decay
    is influenced by OCI severity (e.g., high OCI might retain values longer due to rumination
    or forget faster due to cognitive load).
    
    Parameters:
    - learning_rate: Learning rate for chosen options.
    - beta: Inverse temperature.
    - w: Model-based weight.
    - decay_base: Baseline decay rate [0=no decay, 1=instant forgetting].
    - decay_oci: Modulation of decay rate by OCI.
    
    Bounds:
    learning_rate: [0,1]
    beta: [0,10]
    w: [0,1]
    decay_base: [0,1]
    decay_oci: [-1,1]
    """
    learning_rate, beta, w, decay_base, decay_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Calculate effective decay rate
    decay_rate = decay_base + (decay_oci * oci_score)
    decay_rate = np.clip(decay_rate, 0.0, 1.0) # Ensure it stays a valid rate

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # Update Stage 1
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        # Decay unchosen stage 1
        q_stage1_mf[1-a1] *= (1 - decay_rate)
        
        # Update Stage 2
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        # Decay unchosen stage 2 (in the visited state)
        q_stage2_mf[s_idx, 1-a2] *= (1 - decay_rate)
        # Decay both options in the unvisited state
        q_stage2_mf[1-s_idx, :] *= (1 - decay_rate)
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```