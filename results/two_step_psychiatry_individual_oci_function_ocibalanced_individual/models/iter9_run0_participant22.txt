Here are 3 new cognitive models exploring different mechanisms for how OCI symptoms might interact with reinforcement learning processes, specifically focusing on mixing model-based/model-free control, habit formation, and sensitivity to transitions.

### Model 1: OCI-Modulated Model-Based/Model-Free Mixing
This model tests the hypothesis that higher OCI scores lead to a deficit in goal-directed (Model-Based) control, resulting in a reliance on habitual (Model-Free) systems. Instead of a fixed mixing parameter `w`, `w` is a logistic function of the OCI score.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free RL with OCI-modulated mixing weight.

    Hypothesis: High OCI is associated with reduced Model-Based control (goal-directed)
    and increased reliance on Model-Free (habitual) control. The mixing parameter 'w'
    is determined by the OCI score.

    Parameters:
    - learning_rate: [0, 1] Alpha for value updating.
    - beta: [0, 10] Inverse temperature for softmax.
    - w_base: [0, 1] The baseline mixing weight (0=MF, 1=MB) when OCI is 0.
    - oci_w_decay: [0, 5] The rate at which OCI reduces the model-based weight 'w'.
    """
    learning_rate, beta, w_base, oci_w_decay = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    # Calculate mixing weight w based on OCI. 
    # We constrain w to be between 0 and 1.
    # Higher OCI -> Lower w (Less Model-Based, More Model-Free)
    w = w_base * np.exp(-current_oci * oci_w_decay)
    w = np.clip(w, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)      # Model-free values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for stage 2 (2 states, 2 actions)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based value calculation:
        # Maximize over stage 2 values, then map back to stage 1 via transition matrix
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax policy 1
        logits_1 = beta * q_net
        logits_1 = logits_1 - np.max(logits_1) # stability
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # --- Stage 2 Choice ---
        # Standard Model-Free choice at stage 2
        logits_2 = beta * q_stage2_mf[state_idx]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Learning ---
        # 1. Update Stage 2 Q-values (TD-error)
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # 2. Update Stage 1 Q-values (TD-error using Stage 2 value)
        # Note: Standard TD(0) uses the value of the state reached
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Driven Transition Sensitivity
This model hypothesizes that high OCI individuals are hyper-sensitive to "rare" transitions (prediction errors in the state transition structure), causing them to adjust their behavior more drastically when the transition structure is violated, essentially treating the environment as more volatile.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Based RL with OCI-modulated Transition Learning Rate.
    
    Hypothesis: While the standard task assumes fixed transition probabilities (0.7/0.3),
    participants may actually update their internal model of transitions. High OCI
    participants might be hypersensitive to "rare" transitions, updating their 
    transition matrix faster (or erroneously) after rare events.
    
    Parameters:
    - learning_rate_reward: [0, 1] Learning rate for reward values.
    - beta: [0, 10] Inverse temperature.
    - lr_trans_base: [0, 1] Base learning rate for updating the transition matrix.
    - oci_trans_boost: [0, 5] How much OCI increases sensitivity to transition updates.
    """
    learning_rate_reward, beta, lr_trans_base, oci_trans_boost = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    # Effective transition learning rate scales with OCI
    lr_trans = lr_trans_base * (1 + current_oci * oci_trans_boost)
    lr_trans = np.clip(lr_trans, 0.0, 1.0)
    
    # Initialize transition matrix (subjective belief)
    # Rows: Action 1 (0 or 1), Cols: State (0 or 1)
    # Starts at the true probabilities
    trans_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage2 = np.zeros((2, 2)) # State x Action

    for trial in range(n_trials):
        # --- Stage 1 Choice (Pure Model-Based) ---
        max_q_stage2 = np.max(q_stage2, axis=1)
        
        # MB calculation: Expected value based on current transition belief
        q_stage1_mb = np.zeros(2)
        q_stage1_mb[0] = trans_matrix[0, 0] * max_q_stage2[0] + trans_matrix[0, 1] * max_q_stage2[1]
        q_stage1_mb[1] = trans_matrix[1, 0] * max_q_stage2[0] + trans_matrix[1, 1] * max_q_stage2[1]

        logits_1 = beta * q_stage1_mb
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        a1 = int(action_1[trial])
        s_idx = int(state[trial]) # Observed state

        # --- Stage 2 Choice ---
        logits_2 = beta * q_stage2[s_idx]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Learning ---
        
        # 1. Update Reward Values
        delta = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += learning_rate_reward * delta
        
        # 2. Update Transition Matrix (State Prediction Error)
        # Create a one-hot vector for the observed state
        observed_transition = np.zeros(2)
        observed_transition[s_idx] = 1.0
        
        # Update belief for the chosen action a1
        # Prediction error: observed state - expected state probabilities
        trans_error = observed_transition - trans_matrix[a1]
        trans_matrix[a1] += lr_trans * trans_error
        
        # Normalize to ensure probabilities sum to 1
        trans_matrix[a1] /= np.sum(trans_matrix[a1])

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Linked Avoidance Learning (Asymmetric Learning Rates)
This model posits that OCI is related to enhanced avoidance learning. High OCI individuals might learn more strongly from negative outcomes (lack of reward) than positive outcomes.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free RL with OCI-modulated Asymmetric Learning Rates (Pos/Neg).
    
    Hypothesis: OCI is associated with fear of failure or harm avoidance.
    High OCI individuals may weight negative prediction errors (receiving 0 coins)
    more heavily than positive prediction errors.
    
    Parameters:
    - alpha_pos: [0, 1] Learning rate for positive prediction errors (Reward=1).
    - beta: [0, 10] Inverse temperature.
    - alpha_neg_base: [0, 1] Base learning rate for negative prediction errors (Reward=0).
    - oci_neg_scaling: [0, 5] Multiplier for how much OCI boosts the negative learning rate.
    """
    alpha_pos, beta, alpha_neg_base, oci_neg_scaling = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    # Calculate negative learning rate based on OCI
    # High OCI -> Higher alpha_neg (learns faster from 0 reward)
    alpha_neg = alpha_neg_base * (1 + current_oci * oci_neg_scaling)
    alpha_neg = np.clip(alpha_neg, 0.0, 1.0)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        logits_1 = beta * q_stage1
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        a1 = int(action_1[trial])
        s_idx = int(state[trial])

        # --- Stage 2 Choice ---
        logits_2 = beta * q_stage2[s_idx]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Learning ---
        
        # Define learning rate based on prediction error sign
        # We check the sign of the error for Stage 2 first
        delta2 = r - q_stage2[s_idx, a2]
        
        if delta2 >= 0:
            lr_eff = alpha_pos
        else:
            lr_eff = alpha_neg
            
        q_stage2[s_idx, a2] += lr_eff * delta2
        
        # Update Stage 1
        delta1 = q_stage2[s_idx, a2] - q_stage1[a1]
        
        # We apply the same asymmetry logic to the stage 1 update
        # If the Stage 2 value was lower than expected, it's a negative update
        if delta1 >= 0:
            lr_eff_s1 = alpha_pos
        else:
            lr_eff_s1 = alpha_neg
            
        q_stage1[a1] += lr_eff_s1 * delta1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```