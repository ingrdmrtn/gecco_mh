def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Risk-Sensitive Planning.
    
    This model hypothesizes that high OCI scores lead to 'risk-averse' or 'pessimistic' 
    Model-Based planning. In standard MB planning, the agent values a state by the 
    maximum Q-value available there (assuming optimal future choice). 
    Here, the agent anticipates a mixture of the best (max) and worst (min) outcomes 
    at the second stage, weighted by an OCI-dependent risk parameter.
    
    Parameters:
    - learning_rate: [0, 1] Update rate for Model-Free values.
    - beta: [0, 10] Inverse temperature for softmax choice.
    - w: [0, 1] Weighting between Model-Based (1) and Model-Free (0) control.
    - lambda_eligibility: [0, 1] Eligibility trace for Stage 1 updates.
    - risk_sensitivity: [0, 1] Scaling factor for OCI-induced pessimism.
      The effective risk weight is (risk_sensitivity * OCI).
      V_MB(s) = (1 - risk_weight) * max(Q(s)) + risk_weight * min(Q(s)).
    """
    learning_rate, beta, w, lambda_eligibility, risk_sensitivity = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective risk weight, bounded [0, 1]
    risk_weight = risk_sensitivity * oci_score
    if risk_weight > 1.0: risk_weight = 1.0
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based Valuation with Risk Sensitivity
        max_vals = np.max(q_stage2_mf, axis=1)
        min_vals = np.min(q_stage2_mf, axis=1)
        
        # Valuation of the states (planets) based on risk weight
        v_stage2_mb = (1.0 - risk_weight) * max_vals + risk_weight * min_vals
        
        # Map to Stage 1 actions via transition matrix
        q_stage1_mb = transition_matrix @ v_stage2_mb
        
        # Hybrid Value
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax Choice 1
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        s_idx = int(state[trial])
        
        # Simple Softmax on Stage 2 MF values
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        r = reward[trial]
        
        # Stage 2 RPE
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        # Stage 1 RPE (using Stage 2 value as proxy for reward)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        
        # Update Stage 2
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Update Stage 1 (with eligibility trace)
        q_stage1_mf[a1] += learning_rate * delta_stage1 + (learning_rate * lambda_eligibility * delta_stage2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Distorted Transition Beliefs.
    
    This model proposes that OCI scores correlate with 'Transition Uncertainty' or a 
    distrust in the stability of the environment's structure. High OCI participants 
    perceive the 'common' transition probability (true 0.7) as less certain (closer to 0.5), 
    thereby flattening the transition matrix used in Model-Based planning.
    
    Parameters:
    - learning_rate: [0, 1] Update rate for MF values.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weighting between MB and MF.
    - lambda_eligibility: [0, 1] Eligibility trace.
    - distortion_rate: [0, 1] How much OCI flattens the transition matrix.
      p_perceived = 0.7 - (distortion_rate * OCI * 0.2).
      (Scaled so max OCI ~1.0 can bring 0.7 down to 0.5).
    """
    learning_rate, beta, w, lambda_eligibility, distortion_rate = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate distorted transition probability
    # Base is 0.7. Max distortion brings it to 0.5 (random).
    p_common = 0.7 - (distortion_rate * oci_score * 0.2)
    if p_common < 0.5: p_common = 0.5 
    
    p_rare = 1.0 - p_common
    # Construct perceived transition matrix: [[p, 1-p], [1-p, p]]
    transition_matrix = np.array([[p_common, p_rare], [p_rare, p_common]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        # --- Stage 1 ---
        # MB Calculation using distorted matrix
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 ---
        s_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        r = reward[trial]
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        q_stage1_mf[a1] += learning_rate * delta_stage1 + (learning_rate * lambda_eligibility * delta_stage2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Dependent Value Decay (Fictitious Play/Forgetting).
    
    This model assumes that OCI scores influence the retention of action values 
    for options that were NOT chosen. High OCI might be associated with 
    rapid 'forgetting' or devaluation of the path not taken (or conversely, 
    if the parameter is low, persistence). Here we model it as a passive decay 
    applied to the unchosen Stage 1 action's Q-value on every trial.
    
    Parameters:
    - learning_rate: [0, 1]
    - beta: [0, 10]
    - w: [0, 1]
    - lambda_eligibility: [0, 1]
    - decay_oci_slope: [0, 1] Determines decay rate based on OCI.
      decay = decay_oci_slope * OCI.
      Q(unchosen) <- Q(unchosen) * (1 - decay).
    """
    learning_rate, beta, w, lambda_eligibility, decay_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate decay rate
    decay = decay_oci_slope * oci_score
    if decay > 1.0: decay = 1.0
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        # --- Stage 1 ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 ---
        s_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        r = reward[trial]
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        q_stage1_mf[a1] += learning_rate * delta_stage1 + (learning_rate * lambda_eligibility * delta_stage2)
        
        # Decay the unchosen action in Stage 1
        unchosen_a1 = 1 - a1
        q_stage1_mf[unchosen_a1] *= (1.0 - decay)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss