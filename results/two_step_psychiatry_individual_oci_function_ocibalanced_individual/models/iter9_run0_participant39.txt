Here are three new cognitive models for the two-step task, incorporating OCI scores into the decision-making or learning processes.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model where the Learning Rate is modulated by OCI.
    High OCI scores may lead to either rigidity (low learning rate) or 
    volatility (high learning rate), determining how fast values update.
    Includes fixed stickiness and fixed MB/MF weighting.
    
    Bounds:
    lr_base: [0, 1]
    lr_oci_slope: [-1, 1]
    beta: [0, 10]
    w: [0, 1]
    stickiness: [0, 5]
    """
    lr_base, lr_oci_slope, beta, w, stickiness = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate OCI-modulated learning rate
    learning_rate = lr_base + (lr_oci_slope * oci_score)
    learning_rate = np.clip(learning_rate, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based Value calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Value mixing MB and MF
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Add stickiness bonus to previously chosen action
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        last_action_1 = a1
        
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        # Stage 1 Update (TD) using OCI-modulated learning rate
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Stage 2 Update using OCI-modulated learning rate
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner with OCI-modulated memory decay (forgetting).
    Values for unchosen options decay toward zero on every trial.
    The rate of decay is determined by the OCI score.
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    decay_base: [0, 1]
    decay_oci_slope: [-1, 1]
    """
    learning_rate, beta, decay_base, decay_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate decay rate based on OCI
    decay_rate = decay_base + (decay_oci_slope * oci_score)
    decay_rate = np.clip(decay_rate, 0.0, 1.0)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # 2 states (planets), 2 actions (aliens)

    for trial in range(n_trials):
        # --- Stage 1 Policy (Pure MF) ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Decay unchosen Stage 1 option
        unchosen_a1 = 1 - a1
        q_stage1_mf[unchosen_a1] *= (1.0 - decay_rate)
        
        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Decay unchosen Stage 2 option (in the current state)
        unchosen_a2 = 1 - a2
        q_stage2_mf[state_idx, unchosen_a2] *= (1.0 - decay_rate)
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model where the balance (w) is modulated by OCI, 
    while also including a fixed stickiness parameter.
    This tests if OCI shifts strategy between Goal-Directed (MB) and Habitual (MF)
    control, accounting for simple perseveration.
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    stickiness: [0, 5]
    w_base: [0, 1]
    w_oci_slope: [-1, 1]
    """
    learning_rate, beta, stickiness, w_base, w_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate MB-weight w based on OCI
    w = w_base + (w_oci_slope * oci_score)
    w = np.clip(w, 0.0, 1.0)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Value mixing MB and MF using OCI-modulated weight
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Add fixed stickiness
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        last_action_1 = a1
        
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```