To convert the provided model fitting code (`cognitive_model3`) into simulation code, we need to adapt the logic to generate choices and outcomes based on the probabilities defined in the model, rather than calculating the likelihood of observed data.

Here is the simulation code for the **OCI-modulated Eligibility Trace (TD-Lambda)** model:

```python
import numpy as np

def simulate_model(
    n_trials,
    parameters,
    drift1,
    drift2,
    drift3,
    drift4,
    oci=None):
    """
    Simulates choices and rewards using Model 3: OCI-modulated Eligibility Trace (TD-Lambda).
    
    This model uses Temporal Difference learning with eligibility traces (TD(lambda)).
    The lambda parameter is modulated by the OCI score.

    Parameters:
        n_trials (int): Number of trials to simulate.
        parameters (list): [learning_rate, beta, lambda_base]
        drift1, drift2, drift3, drift4 (np.ndarray): Trial-wise reward probabilities.
        oci (float or None): Obsessive-Compulsive Inventoryâ€“Revised score. If None, defaults to 0.

    Returns:
        stage1_choice (np.ndarray): First-stage choices (0 or 1).
        state2 (np.ndarray): Second-stage states (0 or 1).
        stage2_choice (np.ndarray): Second-stage choices (0 or 1).
        reward (np.ndarray): Rewards (0 or 1).
    """
    
    # Initialize random number generator
    rng = np.random.default_rng()

    # Unpack parameters
    learning_rate, beta, lambda_base = parameters
    
    # Handle OCI input safely
    oci_score = oci if oci is not None else 0.0

    # Calculate effective lambda based on OCI
    lambda_eff = lambda_base + (1.0 - lambda_base) * oci_score
    
    # Initialize storage arrays
    stage1_choice = np.zeros(n_trials, dtype=int)
    state2 = np.zeros(n_trials, dtype=int)
    stage2_choice = np.zeros(n_trials, dtype=int)
    reward = np.zeros(n_trials, dtype=int)
    
    # Transition matrix (fixed for the task)
    # State 0 (Stage 1) -> Choice 0 leads to State 1 (70%) or State 2 (30%)
    # State 0 (Stage 1) -> Choice 1 leads to State 2 (70%) or State 1 (30%)
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Initialize Q-values
    # q_stage1_mf: Q-values for the two actions in Stage 1
    q_stage1_mf = np.zeros(2)
    # q_stage2_mf: Q-values for the two actions in Stage 2 (rows are states, cols are actions)
    # Row 0 corresponds to State 1 (index 0), Row 1 corresponds to State 2 (index 1)
    q_stage2_mf = np.zeros((2, 2))
    
    for t in range(n_trials):
        # Current trial reward probabilities
        # state 0 (idx 0) -> drift1, drift2
        # state 1 (idx 1) -> drift3, drift4
        reward_probs = [[drift1[t], drift2[t]], [drift3[t], drift4[t]]]

        # --- Stage 1 Decision ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = rng.choice([0, 1], p=probs_1)
        
        # --- Transition ---
        # Determine next state based on transition matrix probabilities
        # transition_matrix[a1] gives [prob_state_1, prob_state_2]
        s2_idx = rng.choice([0, 1], p=transition_matrix[a1]) 
        
        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[s2_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = rng.choice([0, 1], p=probs_2)
        
        # --- Reward ---
        r = int(rng.random() < reward_probs[s2_idx][a2])
        
        # --- Learning (TD-Lambda) ---
        
        # 1. Update Stage 1 Q-value based on Stage 2 Q-value (TD error 1)
        delta_1 = q_stage2_mf[s2_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_1

        # 2. Update Stage 2 Q-value based on Reward (TD error 2)
        delta_2 = r - q_stage2_mf[s2_idx, a2]
        q_stage2_mf[s2_idx, a2] += learning_rate * delta_2

        # 3. Eligibility Trace Update: Propagate Stage 2 error back to Stage 1
        q_stage1_mf[a1] += learning_rate * lambda_eff * delta_2
        
        # Store data
        stage1_choice[t] = a1
        state2[t] = s2_idx # Storing 0 or 1 (representing state indices directly)
        stage2_choice[t] = a2
        reward[t] = r

    return stage1_choice, state2, stage2_choice, reward
```