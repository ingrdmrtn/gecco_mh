def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Asymmetric Learning Rate Model.
    
    Hypothesis: OCI score modulates the learning rate specifically for negative prediction errors 
    (disappointments). Positive learning rate is constant, but negative learning rate varies 
    linearly with OCI, reflecting potential negativity bias or rigidity.
    
    Parameters:
    - lr_pos: [0, 1] Learning rate for positive prediction errors.
    - lr_neg_base: [0, 1] Base learning rate for negative prediction errors.
    - lr_neg_oci: [-1, 1] Slope of OCI modulation on negative learning rate.
    - beta: [0, 10] Inverse temperature (softness of choice).
    - w: [0, 1] Mixing weight (0=MF, 1=MB).
    - stickiness: [0, 5] Choice perseveration bonus.
    """
    lr_pos, lr_neg_base, lr_neg_oci, beta, w, stickiness = model_parameters
    n_trials = len(action_1)
    oci_val = oci[0]

    lr_neg = lr_neg_base + lr_neg_oci * oci_val
    lr_neg = np.clip(lr_neg, 0.0, 1.0)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_a1 = -1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        if a1 == -1: 
            prev_a1 = -1
            continue
            
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf

        stick_bonus = np.zeros(2)
        if prev_a1 != -1:
            stick_bonus[prev_a1] = stickiness
            
        exp_q1 = np.exp(beta * (q_net + stick_bonus))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]

        alpha_1 = lr_pos if delta_stage1 > 0 else lr_neg
        q_stage1_mf[a1] += alpha_1 * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]

        alpha_2 = lr_pos if delta_stage2 > 0 else lr_neg
        q_stage2_mf[s_idx, a2] += alpha_2 * delta_stage2

        q_stage1_mf[a1] += alpha_2 * delta_stage2
        
        prev_a1 = a1

    mask = (action_1 != -1)
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss