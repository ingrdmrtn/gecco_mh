def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Based learner with choice perseveration (stickiness) modulated by OCI.
    
    The 'stickiness' parameter captures the tendency to repeat the previous choice.
    We model the effective stickiness as increasing with OCI score.
    
    Parameters:
    lr: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature (shared for both stages).
    stickiness_base: [0, 5] Base tendency to repeat choices.
    """
    lr, beta, stickiness_base = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage2_mf = np.zeros((2, 2)) # Values for aliens


    eff_stickiness = stickiness_base * (1.0 + oci_score)

    last_a1 = -1 # Track previous choice

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2

        q_stage1_augmented = q_stage1_mb.copy()
        if last_a1 != -1:
            q_stage1_augmented[last_a1] += eff_stickiness
            
        exp_q1 = np.exp(beta * q_stage1_augmented)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]


        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_stage2
        
        last_a1 = a1

    eps = 1e-10
    valid_trials = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss