def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Dynamic Perseveration Model.
    Perseveration strength accumulates with consecutive choices of the same option.
    OCI modulates the rate at which this stickiness builds up.
    
    Hypothesis: Higher OCI leads to faster accumulation of stickiness (compulsion)
    with repeated choices, making it harder to switch.
    
    Parameters:
    lr: [0, 1] Learning rate for value updates.
    beta: [0, 10] Inverse temperature (softmax sensitivity).
    w: [0, 1] Weighting between Model-Based (1) and Model-Free (0).
    pers_static: [-2, 2] Base static perseveration bonus (1-back stickiness).
    pers_accum_base: [0, 2] Base rate of stickiness accumulation per consecutive trial.
    pers_accum_oci: [-2, 2] OCI modulation of the accumulation rate.
    """
    lr, beta, w, pers_static, pers_accum_base, pers_accum_oci = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    accum_rate = pers_accum_base + pers_accum_oci * current_oci
    
    q_mf = np.zeros(2)
    q2 = np.zeros((2, 2))
    tm = np.array([[0.7, 0.3], [0.3, 0.7]])

    consecutive_counts = np.zeros(2)
    last_action = -1
    
    log_lik = 0.0
    
    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = reward[t]

        q_mb = tm @ np.max(q2, axis=1)
        q_net = w * q_mb + (1 - w) * q_mf

        if last_action != -1:
            q_net[last_action] += pers_static

        q_net += consecutive_counts * accum_rate

        exp_q = np.exp(beta * q_net)
        probs1 = exp_q / np.sum(exp_q)
        log_lik += np.log(probs1[a1] + 1e-10)

        if a1 == last_action:
            consecutive_counts[a1] += 1
        else:
            consecutive_counts[a1] = 1 # Reset to 1 (current choice)
            consecutive_counts[1-a1] = 0 # Reset other
        last_action = a1

        exp_q2 = np.exp(beta * q2[s])
        probs2 = exp_q2 / np.sum(exp_q2)
        log_lik += np.log(probs2[a2] + 1e-10)

        pe1 = q2[s, a2] - q_mf[a1]
        q_mf[a1] += lr * pe1
        
        pe2 = r - q2[s, a2]
        q2[s, a2] += lr * pe2
        
    return -log_lik