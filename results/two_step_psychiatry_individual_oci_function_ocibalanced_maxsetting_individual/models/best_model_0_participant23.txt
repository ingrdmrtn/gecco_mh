def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Outcome Value Decay.
    
    Hypothesis: OCI modulates the rate at which unvisited outcome values (Stage 2) 
    decay. High OCI might be associated with rapid 'forgetting' or devaluation 
    of unverified options (intolerance of uncertainty), or conversely, rigid maintenance.
    
    Parameters:
    - learning_rate: [0, 1] Update rate for chosen options.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Model-Based weight.
    - stickiness: [0, 5] Perseveration bonus.
    - decay_base: [0, 1] Baseline decay rate for unchosen Stage 2 values.
    - decay_oci: [-1, 1] Modulation of decay rate by OCI.
    """
    learning_rate, beta, w, stickiness, decay_base, decay_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    decay = decay_base + decay_oci * oci_score
    decay = np.clip(decay, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        stick_bonus = np.zeros(2)
        if prev_action_1 != -1:
            stick_bonus[prev_action_1] = stickiness
            
        logits_1 = beta * q_net + stick_bonus
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        logits_2 = beta * q_stage2_mf[s_idx]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2


        for s in range(2):
            for a in range(2):
                if not (s == s_idx and a == a2):
                    q_stage2_mf[s, a] *= (1.0 - decay)
        
        prev_action_1 = a1

    eps = 1e-10
    valid_mask = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss