def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Influenced Value Decay.
    Hypothesis: High OCI is associated with a "tunnel vision" effect where 
    the value of unchosen options decays over time. The rate of this decay 
    is modulated by the OCI score. If the decay is high, the agent forgets 
    the value of the alternative spaceship, making switching less likely 
    if the current one is even marginally positive.
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    w: [0, 1]
    decay_rate_base: [0, 1] (Base rate of decay for unchosen options)
    """
    learning_rate, beta, w, decay_rate_base = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]



    decay_eff = decay_rate_base * oci_score
    decay_eff = min(1.0, max(0.0, decay_eff))

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        if a1 == -1: a1 = 0
        s_idx = int(state[trial])
        if s_idx == -1: s_idx = 0
        a2 = int(action_2[trial])
        if a2 == -1: a2 = 0
        r = reward[trial]
        if r == -1: r = 0

        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]


        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

        unchosen_a2 = 1 - a2
        q_stage2_mf[s_idx, unchosen_a2] *= (1.0 - decay_eff)

        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

        unchosen_a1 = 1 - a1
        q_stage1_mf[unchosen_a1] *= (1.0 - decay_eff)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss