def cognitive_model1_oci_perseveration(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: Hybrid Learner with OCI-Modulated Perseveration.
    
    Hypothesis: OCI score drives the strength of perseveration (stickiness) to the previously chosen spaceship.
    High OCI scores are associated with compulsive repetition. This model posits that the tendency to 
    repeat the previous stage-1 choice, regardless of value, is scaled by the OCI score.
    
    Q_net = w * Q_MB + (1 - w) * Q_MF
    Logits = beta * Q_net + (persev_strength * OCI * I(action == last_action))
    
    Parameters:
    - learning_rate: [0, 1] Update rate for MF values.
    - beta: [0, 10] Inverse temperature (softness of choice).
    - w: [0, 1] Weight of Model-Based control (1 = Pure MB, 0 = Pure MF).
    - persev_strength: [0, 5] Strength of choice stickiness scaled by OCI.
    """
    learning_rate, beta, w, persev_strength = model_parameters
    oci_score = oci[0]
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    
    last_action = -1
    
    for trial in range(n_trials):


        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2

        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf

        logits_1 = beta * q_net
        if last_action != -1:
            logits_1[last_action] += persev_strength * oci_score

        logits_1 = logits_1 - np.max(logits_1) # Numerical stability
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        last_action = action_1[trial]

        state_idx = state[trial]
        logits_2 = beta * q_stage2_mf[state_idx]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]


        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1

        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss