Here are three cognitive models designed to explain the participant's behavior, incorporating their low OCI-R score (0.3) into the learning and decision-making processes.

### Model 1: Hybrid Learner with OCI-modulated Model-Based Weight
This model assumes that the balance between Model-Free (habitual) and Model-Based (planning) control is influenced by the OCI score. Since the participant has a low OCI score (0.3), they likely exhibit a healthy balance or a slight preference for flexible, model-based strategies compared to high-OCI individuals who might be more habitual. The `w` parameter (mixing weight) is modulated by OCI.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL model where the balance between Model-Based (MB) and Model-Free (MF) 
    systems is modulated by the OCI score.
    
    The low OCI score (0.3) implies less reliance on rigid habits (MF) and potentially 
    intact or higher reliance on goal-directed planning (MB).
    
    Parameters:
    lr: [0, 1] Learning rate for value updates.
    beta_1: [0, 10] Inverse temperature for stage 1 choice.
    beta_2: [0, 10] Inverse temperature for stage 2 choice.
    w_base: [0, 1] Baseline weight for Model-Based control.
    """
    lr, beta_1, beta_2, w_base = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Transition matrix (fixed structure of the task)
    # 0.7 probability of common transition, 0.3 rare
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)      # Model-free values for spaceships (A/U)
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for aliens at planets (X/Y)
    
    # Calculate effective mixing weight w. 
    # High OCI often correlates with deficits in MB control (lower w).
    # Since OCI is low here, we model w as decreasing slightly as OCI increases.
    # We constrain w to be between 0 and 1.
    w = w_base * (1.0 - oci_score) 
    w = np.clip(w, 0.0, 1.0)

    for trial in range(n_trials):
        # Skip missing data
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy (Hybrid MB/MF) ---
        # Calculate Model-Based values: V(state) = max(Q_stage2)
        # Q_MB = Transition_Matrix * V(state)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combined value: w * Q_MB + (1-w) * Q_MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy (Simple MF) ---
        exp_q2 = np.exp(beta_2 * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Stage 2 TD Error (RPE)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_stage2
        
        # Stage 1 TD Error (SARSA-style update for MF)
        # Using the value of the state actually reached (q_stage2_mf[s_idx, a2])
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1

    eps = 1e-10
    # Filter out 0 probabilities from missing trials before log
    valid_trials = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```

### Model 2: OCI-Driven Perseveration (Stickiness) Model
This model hypothesizes that even with low OCI, there is a baseline tendency to repeat actions (perseveration), and this tendency is scaled by the OCI score. A low OCI score (0.3) implies lower "stickiness" or compulsion to repeat the previous choice regardless of reward, but it is still a factor.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Based learner with choice perseveration (stickiness) modulated by OCI.
    
    The 'stickiness' parameter captures the tendency to repeat the previous choice.
    We model the effective stickiness as increasing with OCI score.
    
    Parameters:
    lr: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature (shared for both stages).
    stickiness_base: [0, 5] Base tendency to repeat choices.
    """
    lr, beta, stickiness_base = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage2_mf = np.zeros((2, 2)) # Values for aliens
    
    # Effective stickiness increases with OCI
    # For this participant (OCI=0.3), stickiness will be closer to 0 than high OCI.
    eff_stickiness = stickiness_base * (1.0 + oci_score)

    last_a1 = -1 # Track previous choice

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy (Pure Model-Based + Stickiness) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Add stickiness bonus to the Q-values
        q_stage1_augmented = q_stage1_mb.copy()
        if last_a1 != -1:
            q_stage1_augmented[last_a1] += eff_stickiness
            
        exp_q1 = np.exp(beta * q_stage1_augmented)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy (Simple MF) ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Only updating stage 2 values as stage 1 is purely computed from them (MB)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_stage2
        
        last_a1 = a1

    eps = 1e-10
    valid_trials = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Learning Rate Asymmetry
This model suggests that OCI status affects how participants learn from positive versus negative prediction errors. High OCI is sometimes associated with over-learning from negative outcomes (avoidance). Since this participant has low OCI (0.3), this model tests if they have a more balanced learning rate or a bias toward positive learning (`lr_pos`) versus negative (`lr_neg`), scaled by their OCI score.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Reinforcement learning model with asymmetric learning rates for positive 
    and negative prediction errors. The asymmetry is influenced by the OCI score.
    
    We hypothesize that higher OCI might amplify learning from negative errors.
    Since OCI is low (0.3), the bias might be minimal or skewed towards positive.
    
    Parameters:
    lr_base: [0, 1] Base learning rate.
    beta: [0, 10] Inverse temperature.
    bias_param: [0, 1] Parameter controlling the spread between pos/neg learning.
    """
    lr_base, beta, bias_param = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Simple Model-Free Q-values for both stages
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    # Define learning rates based on OCI
    # If OCI is high, lr_neg gets a boost. If OCI is low, lr_pos might be dominant.
    # We construct them such that they center around lr_base.
    # bias_param scales the impact of OCI on this split.
    
    # Low OCI (0.3) -> smaller boost to negative learning, potentially balanced.
    lr_pos = lr_base 
    lr_neg = lr_base * (1.0 + bias_param * oci_score)
    
    # Clip to ensure bounds
    lr_pos = np.clip(lr_pos, 0.0, 1.0)
    lr_neg = np.clip(lr_neg, 0.0, 1.0)

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue

        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy (MF) ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy (MF) ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates with Asymmetric Learning Rates ---
        
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        current_lr = lr_pos if delta_stage2 >= 0 else lr_neg
        q_stage2_mf[s_idx, a2] += current_lr * delta_stage2
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        current_lr = lr_pos if delta_stage1 >= 0 else lr_neg
        q_stage1_mf[a1] += current_lr * delta_stage1

    eps = 1e-10
    valid_trials = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```