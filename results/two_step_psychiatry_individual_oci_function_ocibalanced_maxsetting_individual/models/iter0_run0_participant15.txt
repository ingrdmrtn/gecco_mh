Here are three cognitive models based on the participant's data and OCI score.

### Model 1: OCI-Modulated Model-Based/Model-Free Hybrid
This model hypothesizes that the balance between goal-directed (Model-Based) and habitual (Model-Free) control is mediated by the participant's OCI score. High OCI scores are often associated with reduced model-based control (compulsivity/habits). The mixing weight `w` is calculated dynamically based on the OCI score.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free Reinforcement Learning model where the mixing weight (w)
    is modulated by the OCI score.
    
    Hypothesis: Higher OCI scores lead to reduced Model-Based control (lower w).
    
    Parameters:
    - learning_rate: [0, 1] Rate at which Q-values update.
    - beta: [0, 10] Inverse temperature for softmax (exploration/exploitation).
    - w_intercept: [0, 1] Base mixing weight for Model-Based control.
    - w_slope: [0, 1] Sensitivity of the mixing weight to OCI score.
    
    w = w_intercept - (w_slope * oci)
    Q_net = w * Q_MB + (1-w) * Q_MF
    """
    learning_rate, beta, w_intercept, w_slope = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate mixing weight w constrained to [0, 1]
    # Higher OCI reduces w (less Model-Based)
    w = w_intercept - (w_slope * current_oci)
    w = np.clip(w, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Q-values initialization
    q_stage1_mf = np.zeros(2)      # Model-free values for Stage 1 (Spaceships)
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for Stage 2 (Aliens)
    
    log_likelihood = 0.0
    eps = 1e-10

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Handle missing/invalid data (e.g., trial 93)
        if a1 < 0 or s_idx < 0 or a2 < 0:
            continue

        # --- Stage 1 Policy ---
        # Model-Based Value: Planning using transition matrix and max Stage 2 values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Value: Weighted mix of MB and MF
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax choice probability for Stage 1
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        # Accumulate log loss for action 1
        log_likelihood += np.log(probs_1[a1] + eps)

        # --- Stage 2 Policy ---
        # Simple Model-Free decision at Stage 2
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        # Accumulate log loss for action 2
        log_likelihood += np.log(probs_2[a2] + eps)

        # --- Updating ---
        # Prediction Error Stage 1 (SARSA-style for the MF component)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Prediction Error Stage 2
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Eligibility Trace: The stage 2 outcome also updates stage 1 choice (TD(1))
        # This reinforces the full chain MF path
        q_stage1_mf[a1] += learning_rate * delta_stage2

    return -log_likelihood
```

### Model 2: OCI-Modulated Perseveration (Stickiness)
This model assumes the participant follows a standard hybrid strategy, but their tendency to perseverate (repeat the previous choice regardless of outcome) is driven by their OCI score. Compulsive symptoms are often linked to repetitive behaviors ("stickiness").

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model with OCI-modulated Perseveration (Stickiness).
    
    Hypothesis: The tendency to repeat the previous stage 1 choice (perseveration)
    increases with OCI score.
    
    Parameters:
    - learning_rate: [0, 1]
    - beta: [0, 10]
    - w: [0, 1] Fixed mixing weight for MB/MF.
    - stick_base: [0, 5] Base level of perseveration.
    - stick_oci_sens: [0, 5] Additional perseveration per unit of OCI.
    
    Stickiness Bonus = stick_base + (stick_oci_sens * oci)
    """
    learning_rate, beta, w, stick_base, stick_oci_sens = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate stickiness magnitude
    stickiness = stick_base + (stick_oci_sens * current_oci)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    log_likelihood = 0.0
    eps = 1e-10

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        if a1 < 0 or s_idx < 0 or a2 < 0:
            continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness bonus to the previously chosen action
        q_augmented = q_net.copy()
        if last_action_1 != -1:
            q_augmented[last_action_1] += stickiness
        
        exp_q1 = np.exp(beta * q_augmented)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        log_likelihood += np.log(probs_1[a1] + eps)

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        log_likelihood += np.log(probs_2[a2] + eps)

        # --- Updating ---
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Eligibility Trace
        q_stage1_mf[a1] += learning_rate * delta_stage2
        
        # Update history
        last_action_1 = a1

    return -log_likelihood
```

### Model 3: OCI-Modulated Eligibility Trace (Lambda)
This model explores the idea that OCI affects the *structure* of learning rather than just the decision weights. Specifically, it modulates the eligibility trace parameter ($\lambda$). A higher lambda means the outcome at Stage 2 has a stronger direct effect on the value of the Stage 1 choice, bypassing the model-based map. This represents a more "reflexive" or habit-forming learning style associated with compulsivity.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model where the Eligibility Trace (lambda) is modulated by OCI.
    
    Hypothesis: Higher OCI leads to a higher lambda, meaning outcomes at the end
    of the trial strongly reinforce the starting choice directly (Model-Free), 
    ignoring the transition structure.
    
    Parameters:
    - learning_rate: [0, 1]
    - beta: [0, 10]
    - w: [0, 1] Mixing weight.
    - lambda_base: [0, 1] Base eligibility trace.
    - lambda_oci_sens: [0, 1] Sensitivity of lambda to OCI.
    
    lambda = lambda_base + (lambda_oci_sens * oci) (clipped to [0,1])
    """
    learning_rate, beta, w, lambda_base, lambda_oci_sens = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate lambda parameter
    lambda_param = lambda_base + (lambda_oci_sens * current_oci)
    lambda_param = np.clip(lambda_param, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    log_likelihood = 0.0
    eps = 1e-10

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        if a1 < 0 or s_idx < 0 or a2 < 0:
            continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        log_likelihood += np.log(probs_1[a1] + eps)

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        log_likelihood += np.log(probs_2[a2] + eps)

        # --- Updating ---
        # Stage 1 MF Update (TD(0) part)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Stage 2 MF Update
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Eligibility Trace Update (TD(lambda) part)
        # Updates Stage 1 value based on Stage 2 RPE, scaled by lambda
        q_stage1_mf[a1] += learning_rate * lambda_param * delta_stage2

    return -log_likelihood
```