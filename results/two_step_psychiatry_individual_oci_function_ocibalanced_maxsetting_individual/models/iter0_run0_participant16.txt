Here are three cognitive models designed to explain the participant's behavior, incorporating their OCI score into the decision-making process.

### Model 1: Hybrid Learner with OCI-modulated Model-Based Weight
This model assumes that the balance between Model-Based (planning) and Model-Free (habitual) control is influenced by the participant's OCI score. A higher OCI score might reflect a tendency towards more rigid, habitual behavior (Model-Free) or, conversely, over-planning (Model-Based). Here, we test the hypothesis that OCI modulates the mixing weight `w`, shifting the balance between the two systems.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free learner where the mixing weight 'w' is 
    modulated by the OCI score.
    
    Bounds:
    learning_rate: [0, 1]
    beta_1: [0, 10]
    beta_2: [0, 10]
    w_base: [0, 1]
    oci_sensitivity: [-1, 1]
    """
    learning_rate, beta_1, beta_2, w_base, oci_sensitivity = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate the mixing weight based on OCI
    # We use a sigmoid-like transformation to keep w between 0 and 1
    # If oci_sensitivity is positive, high OCI increases Model-Based control.
    # If negative, high OCI increases Model-Free control.
    w_logit = np.log(w_base / (1 - w_base + 1e-10)) + oci_sensitivity * current_oci
    w = 1 / (1 + np.exp(-w_logit))

    # Fixed transition matrix (70% common, 30% rare)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)       # Model-free values for stage 1 (Spaceships)
    q_stage2_mf = np.zeros((2, 2))  # Model-free values for stage 2 (Aliens)

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        
        # Model-Based valuation: V(state) = max(Q_stage2)
        # Q_MB(action) = Transition_Matrix * V(states)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix[0] * max_q_stage2[0] + transition_matrix[1] * max_q_stage2[1] # Simplified for 2 actions
        q_stage1_mb = transition_matrix @ max_q_stage2

        # Hybrid valuation: Q_net = w * Q_MB + (1-w) * Q_MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax for Stage 1
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        # Store probability of the chosen action (handling missing data if -1)
        if action_1[trial] != -1:
            p_choice_1[trial] = probs_1[int(action_1[trial])]
        else:
            p_choice_1[trial] = 1.0 # Ignore missing trials
            continue

        # --- Stage 2 Decision ---
        state_idx = int(state[trial])
        
        # Standard Model-Free Q-learning for Stage 2
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        if action_2[trial] != -1:
            p_choice_2[trial] = probs_2[int(action_2[trial])]
            
            # --- Learning Updates ---
            
            # Prediction Errors
            # Stage 2 PE: Reward - Expected
            pe_2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
            
            # Stage 1 PE: Value of state reached - Expected
            pe_1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
            
            # Update Stage 2 Q-values
            q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * pe_2
            
            # Update Stage 1 Q-values (TD(0))
            q_stage1_mf[int(action_1[trial])] += learning_rate * pe_1
            
            # Note: In a full TD(lambda) model, eligibility traces would connect 
            # the stage 2 reward back to stage 1. Here we use a simplified update 
            # structure common in these tasks.
        else:
            p_choice_2[trial] = 1.0

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Driven Perseveration (Stickiness)
High OCI scores are often associated with repetitive behaviors or "stickiness" (perseveration). This model introduces a choice autocorrelation parameter (stickiness) that biases the participant to repeat the previous Stage 1 choice. The magnitude of this stickiness is scaled by the participant's OCI score.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner with OCI-modulated choice stickiness (perseveration).
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    stickiness_base: [0, 5]
    oci_sticky_mod: [0, 5]
    """
    learning_rate, beta, stickiness_base, oci_sticky_mod = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    # Calculate effective stickiness
    # Higher OCI leads to higher tendency to repeat the previous choice
    effective_stickiness = stickiness_base + (oci_sticky_mod * current_oci)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        
        # Add stickiness bonus to the Q-values before softmax
        q_stage1_modified = q_stage1.copy()
        if last_action_1 != -1:
            q_stage1_modified[int(last_action_1)] += effective_stickiness
            
        exp_q1 = np.exp(beta * q_stage1_modified)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        if action_1[trial] != -1:
            p_choice_1[trial] = probs_1[int(action_1[trial])]
            last_action_1 = action_1[trial]
        else:
            p_choice_1[trial] = 1.0
            last_action_1 = -1
            continue

        # --- Stage 2 Decision ---
        state_idx = int(state[trial])
        
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        if action_2[trial] != -1:
            p_choice_2[trial] = probs_2[int(action_2[trial])]
            
            # --- Updates ---
            # SARSA / TD update logic
            
            # Stage 2 Update
            pe_2 = reward[trial] - q_stage2[state_idx, int(action_2[trial])]
            q_stage2[state_idx, int(action_2[trial])] += learning_rate * pe_2
            
            # Stage 1 Update
            # Using the value of the state reached (max Q of stage 2) as target
            target_val = np.max(q_stage2[state_idx])
            pe_1 = target_val - q_stage1[int(action_1[trial])]
            q_stage1[int(action_1[trial])] += learning_rate * pe_1
            
        else:
            p_choice_2[trial] = 1.0

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Learning Rate Asymmetry
This model hypothesizes that OCI affects how participants learn from positive versus negative feedback. It implements separate learning rates for positive prediction errors (better than expected) and negative prediction errors (worse than expected). The OCI score scales the learning rate specifically for negative outcomes, testing if high OCI leads to hypersensitivity (or hyposensitivity) to failure/lack of reward.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner with asymmetric learning rates for positive/negative PEs.
    The negative learning rate is modulated by OCI.
    
    Bounds:
    alpha_pos: [0, 1]
    alpha_neg_base: [0, 1]
    beta: [0, 10]
    oci_neg_mod: [-1, 1]
    eligibility_trace: [0, 1]
    """
    alpha_pos, alpha_neg_base, beta, oci_neg_mod, eligibility_trace = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    # Calculate effective negative learning rate
    # We clip to ensure it stays within [0, 1]
    alpha_neg_eff = alpha_neg_base + (oci_neg_mod * current_oci)
    alpha_neg_eff = np.clip(alpha_neg_eff, 0.0, 1.0)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        if action_1[trial] != -1:
            p_choice_1[trial] = probs_1[int(action_1[trial])]
        else:
            p_choice_1[trial] = 1.0
            continue

        # --- Stage 2 Decision ---
        state_idx = int(state[trial])
        
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        if action_2[trial] != -1:
            p_choice_2[trial] = probs_2[int(action_2[trial])]
            
            # --- Updates ---
            
            # Stage 2 Prediction Error
            pe_2 = reward[trial] - q_stage2[state_idx, int(action_2[trial])]
            
            # Select learning rate based on sign of PE
            lr_2 = alpha_pos if pe_2 >= 0 else alpha_neg_eff
            
            # Update Stage 2
            q_stage2[state_idx, int(action_2[trial])] += lr_2 * pe_2
            
            # Stage 1 Prediction Error (TD(1) / Eligibility Trace style)
            # PE_1 is driven by the Stage 2 PE, scaled by eligibility trace
            # This allows the reward at the end to directly influence Stage 1 choice
            
            # First, immediate PE for Stage 1 (transition to state)
            # Usually 0 in this task setup unless states have intrinsic value, 
            # but we use the value of the chosen stage 2 option as the proxy.
            pe_1 = q_stage2[state_idx, int(action_2[trial])] - q_stage1[int(action_1[trial])]
            
            # Combined update for Stage 1 using eligibility trace logic
            # We effectively backpropagate the final outcome
            total_pe_1 = pe_1 + (eligibility_trace * pe_2)
            
            lr_1 = alpha_pos if total_pe_1 >= 0 else alpha_neg_eff
            
            q_stage1[int(action_1[trial])] += lr_1 * total_pe_1
            
        else:
            p_choice_2[trial] = 1.0

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```