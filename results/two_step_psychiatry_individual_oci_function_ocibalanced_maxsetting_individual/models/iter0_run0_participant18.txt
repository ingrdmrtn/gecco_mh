Here are three cognitive models formulated as Python functions. Each model incorporates the OCI-R score to test a specific hypothesis about how obsessive-compulsive symptoms influence decision-making mechanisms in the two-step task.

### Model 1: OCI-Modulated Model-Based Weight
This model tests the hypothesis that higher obsessive-compulsive symptoms are associated with a deficit in Goal-Directed (Model-Based) control, leading to a greater reliance on Habitual (Model-Free) control. The mixing parameter $w$ is scaled down by the OCI score.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Model-Based Weight (Habit Hypothesis).
    Hypothesis: Higher OCI scores reduce the weight of the Model-Based system (w),
    shifting control towards the Model-Free (habitual) system.
    
    Parameters:
    - lr: Learning rate [0,1]
    - beta: Inverse temperature (softmax sensitivity) [0,10]
    - w_param: Base Model-Based weight parameter [0,1]
    - lambda_val: Eligibility trace decay [0,1]
    - stickiness: Choice perseveration parameter [0,5]
    """
    lr, beta, w_param, lambda_val, stickiness = model_parameters
    oci_score = oci[0]
    
    # Effective w is reduced by OCI score. 
    # If OCI is high, w becomes small (dominance of Model-Free).
    w = w_param * (1.0 - oci_score)
    
    n_trials = len(action_1)
    # Transition probabilities: A(0)->X(0) is 0.7, U(1)->Y(1) is 0.7
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Initialize Q-values
    q_mf_s1 = np.zeros(2) 
    q_mf_s2 = np.zeros((2, 2)) # States x Actions
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1
    
    for t in range(n_trials):
        # Skip invalid data
        if action_1[t] == -1:
            p_choice_1[t] = 1.0
            p_choice_2[t] = 1.0
            continue
            
        a1 = int(action_1[t])
        s2 = int(state[t])
        a2 = int(action_2[t])
        r = reward[t]
        
        # --- Stage 1 Policy ---
        # Model-Based: V(s') = max Q(s', a')
        max_q_s2 = np.max(q_mf_s2, axis=1)
        q_mb_s1 = transition_matrix @ max_q_s2
        
        # Net Value: weighted mixture
        q_net = w * q_mb_s1 + (1 - w) * q_mf_s1
        
        # Softmax with Stickiness
        logits_1 = beta * q_net
        if prev_a1 != -1:
            logits_1[prev_a1] += stickiness
            
        exp_1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_1 / np.sum(exp_1)
        p_choice_1[t] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        # Standard Model-Free Softmax
        logits_2 = beta * q_mf_s2[s2]
        exp_2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_2 / np.sum(exp_2)
        p_choice_2[t] = probs_2[a2]
        
        # --- Updates ---
        # Stage 1 TD Error (SARSA-style): Q(s2, a2) - Q(s1, a1)
        delta_1 = q_mf_s2[s2, a2] - q_mf_s1[a1]
        q_mf_s1[a1] += lr * delta_1
        
        # Stage 2 TD Error: Reward - Q(s2, a2)
        delta_2 = r - q_mf_s2[s2, a2]
        q_mf_s2[s2, a2] += lr * delta_2
        
        # Eligibility Trace: Propagate Stage 2 error back to Stage 1 choice
        q_mf_s1[a1] += lr * lambda_val * delta_2
        
        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Perseveration
This model tests the hypothesis that OCI is linked to "compulsive" repetition of actions. Here, the OCI score amplifies the stickiness parameter, increasing the tendency to repeat the previous Stage 1 choice regardless of reward history.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Modulated Perseveration (Compulsivity Hypothesis).
    Hypothesis: Higher OCI scores increase the tendency to repetitively choose 
    the same Stage 1 action (stickiness), independent of value.
    
    Parameters:
    - lr: Learning rate [0,1]
    - beta: Inverse temperature [0,10]
    - w: Model-Based weight [0,1]
    - lambda_val: Eligibility trace decay [0,1]
    - stick_base: Base perseveration parameter [0,5]
    """
    lr, beta, w, lambda_val, stick_base = model_parameters
    oci_score = oci[0]
    
    # Effective stickiness is amplified by OCI
    stickiness = stick_base * (1.0 + oci_score)
    
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_mf_s1 = np.zeros(2) 
    q_mf_s2 = np.zeros((2, 2)) 
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1
    
    for t in range(n_trials):
        if action_1[t] == -1:
            p_choice_1[t] = 1.0
            p_choice_2[t] = 1.0
            continue
            
        a1 = int(action_1[t])
        s2 = int(state[t])
        a2 = int(action_2[t])
        r = reward[t]
        
        # --- Stage 1 ---
        max_q_s2 = np.max(q_mf_s2, axis=1)
        q_mb_s1 = transition_matrix @ max_q_s2
        
        q_net = w * q_mb_s1 + (1 - w) * q_mf_s1
        
        logits_1 = beta * q_net
        # Apply OCI-enhanced stickiness
        if prev_a1 != -1:
            logits_1[prev_a1] += stickiness
            
        exp_1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_1 / np.sum(exp_1)
        p_choice_1[t] = probs_1[a1]
        
        # --- Stage 2 ---
        logits_2 = beta * q_mf_s2[s2]
        exp_2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_2 / np.sum(exp_2)
        p_choice_2[t] = probs_2[a2]
        
        # --- Updates ---
        delta_1 = q_mf_s2[s2, a2] - q_mf_s1[a1]
        q_mf_s1[a1] += lr * delta_1
        
        delta_2 = r - q_mf_s2[s2, a2]
        q_mf_s2[s2, a2] += lr * delta_2
        
        q_mf_s1[a1] += lr * lambda_val * delta_2
        
        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Punishment Sensitivity
This model tests the hypothesis that OCI is related to anxiety and an over-sensitivity to negative outcomes (missing gold coins). The learning rate is increased specifically for trials where the reward is 0, implying "over-learning" from failure.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Punishment Sensitivity (Anxiety Hypothesis).
    Hypothesis: Higher OCI scores lead to higher sensitivity to negative feedback (0 reward).
    The learning rate is boosted by the OCI score when reward is 0.
    
    Parameters:
    - lr_base: Base learning rate for positive rewards [0,1]
    - beta: Inverse temperature [0,10]
    - w: Model-Based weight [0,1]
    - lambda_val: Eligibility trace decay [0,1]
    - stickiness: Choice perseveration [0,5]
    """
    lr_base, beta, w, lambda_val, stickiness = model_parameters
    oci_score = oci[0]
    
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_mf_s1 = np.zeros(2) 
    q_mf_s2 = np.zeros((2, 2)) 
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1
    
    for t in range(n_trials):
        if action_1[t] == -1:
            p_choice_1[t] = 1.0
            p_choice_2[t] = 1.0
            continue
            
        a1 = int(action_1[t])
        s2 = int(state[t])
        a2 = int(action_2[t])
        r = reward[t]
        
        # --- Stage 1 ---
        max_q_s2 = np.max(q_mf_s2, axis=1)
        q_mb_s1 = transition_matrix @ max_q_s2
        
        q_net = w * q_mb_s1 + (1 - w) * q_mf_s1
        
        logits_1 = beta * q_net
        if prev_a1 != -1:
            logits_1[prev_a1] += stickiness
            
        exp_1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_1 / np.sum(exp_1)
        p_choice_1[t] = probs_1[a1]
        
        # --- Stage 2 ---
        logits_2 = beta * q_mf_s2[s2]
        exp_2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_2 / np.sum(exp_2)
        p_choice_2[t] = probs_2[a2]
        
        # --- Determine Learning Rate ---
        # If reward is 0 (loss/punishment), increase LR based on OCI
        if r == 0:
            current_lr = min(1.0, lr_base * (1.0 + oci_score))
        else:
            current_lr = lr_base
            
        # --- Updates ---
        delta_1 = q_mf_s2[s2, a2] - q_mf_s1[a1]
        q_mf_s1[a1] += current_lr * delta_1
        
        delta_2 = r - q_mf_s2[s2, a2]
        q_mf_s2[s2, a2] += current_lr * delta_2
        
        q_mf_s1[a1] += current_lr * lambda_val * delta_2
        
        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```