Here are three cognitive models designed to explain the participant's behavior in the two-step task, incorporating their OCI score (0.45, medium symptoms) into the learning and decision-making processes.

### Model 1: OCI-Modulated Model-Based/Model-Free Balance
This model hypothesizes that the balance between Model-Based (planning) and Model-Free (habitual) control is influenced by the OCI score. A common finding is that compulsivity relates to a deficit in goal-directed (model-based) control or an over-reliance on habits. Here, the mixing weight `w` is modulated by the OCI score.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Model-Based/Model-Free Balance.
    
    This model assumes that the trade-off between Model-Based (MB) and Model-Free (MF) 
    strategies in the first stage is influenced by the participant's OCI score.
    Higher OCI might shift behavior towards MF (habitual) control.
    
    Parameters:
    - learning_rate: [0, 1] Rate at which Q-values are updated.
    - beta: [0, 10] Inverse temperature for softmax choice (exploration/exploitation).
    - w_base: [0, 1] Base weighting for Model-Based control (0 = pure MF, 1 = pure MB).
    - w_oci_slope: [0, 1] Scaling factor for how much OCI reduces MB control.
    """
    learning_rate, beta, w_base, w_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate mixing weight w based on OCI. 
    # We hypothesize OCI reduces model-based control.
    # Clip w to be between 0 and 1.
    w = w_base - (w_oci_slope * oci_score)
    if w < 0: w = 0
    if w > 1: w = 1

    # Transition matrix: A->X (0.7), U->Y (0.7) usually
    # Indices: [Action, State] -> [0,0] is A->X
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2) # MF values for stage 1 actions
    q_stage2_mf = np.zeros((2, 2)) # Values for stage 2 states (planets) and actions (aliens)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])
        
        if a1 == -1: # Skip missing trials
            continue

        # --- Stage 1 Policy ---
        # Model-Based value calculation: V(s') = max_a' Q(s', a')
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Q-value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax for Stage 1
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        # Standard Softmax on Stage 2 Q-values
        exp_q2 = np.exp(beta * q_stage2_mf[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        # SARSA / Q-learning updates
        
        # Stage 1 MF update (using Stage 2 value as proxy for prediction error, simplified)
        # Note: Standard TD(0) or eligibility traces could be used, here we use simple TD
        # Prediction error using the value of the state actually reached
        # Value of state s2 is max(Q(s2, :))
        v_s2 = np.max(q_stage2_mf[s2])
        delta_stage1 = v_s2 - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Stage 2 MF update
        delta_stage2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += learning_rate * delta_stage2

    eps = 1e-10
    # Filter out missing trials (p=0) before log
    valid_trials = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```

### Model 2: OCI-influenced Perseveration (Stickiness)
Individuals with obsessive-compulsive traits often exhibit repetitive behaviors or "stickiness" to previous choices regardless of reward. This model adds a perseveration parameter (stickiness) that is directly scaled by the OCI score.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-influenced Perseveration (Stickiness).
    
    This model assumes a pure Model-Free learner but adds a choice stickiness 
    parameter (perseveration) that scales with OCI score.
    High OCI -> Higher tendency to repeat the previous Stage 1 choice.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - stickiness_base: [0, 5] Base tendency to repeat previous choice.
    - stickiness_oci: [0, 5] Additional stickiness scaled by OCI.
    """
    learning_rate, beta, stickiness_base, stickiness_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective stickiness
    eff_stickiness = stickiness_base + (stickiness_oci * oci_score)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    last_a1 = -1 # Track previous stage 1 choice

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])
        
        if a1 == -1:
            last_a1 = -1
            continue

        # --- Stage 1 Policy ---
        # Add stickiness bonus to the Q-value of the previously chosen action
        q_stage1_modified = q_stage1.copy()
        if last_a1 != -1:
            q_stage1_modified[last_a1] += eff_stickiness
            
        exp_q1 = np.exp(beta * q_stage1_modified)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        # Direct reinforcement of Stage 1 choice based on final reward (Model-Free)
        # Standard TD(1) or direct update logic often used in these simple models
        delta_stage1 = r - q_stage1[a1]
        q_stage1[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2[s2, a2]
        q_stage2[s2, a2] += learning_rate * delta_stage2
        
        last_a1 = a1

    eps = 1e-10
    valid_trials = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Second Stage Learning Rate
This model posits that OCI symptoms affect how strongly participants update their beliefs after outcomes, specifically at the second stage (the direct outcome of the alien choice). It allows for separate learning rates for the first stage (navigation) and second stage (reward), where the second stage learning rate is modulated by OCI. This reflects potential hypersensitivity or hyposensitivity to feedback in OC traits.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Second Stage Learning Rate.
    
    This model separates learning rates for Stage 1 (navigation/transition) and 
    Stage 2 (reward outcome). The Stage 2 learning rate is modulated by OCI, 
    testing if symptom severity impacts feedback sensitivity.
    
    Parameters:
    - lr_stage1: [0, 1] Learning rate for Stage 1 transitions/values.
    - lr_stage2_base: [0, 1] Base learning rate for Stage 2.
    - lr_oci_mod: [0, 1] Modulation factor for Stage 2 LR by OCI.
    - beta: [0, 10] Inverse temperature.
    - lambda_eligibility: [0, 1] Eligibility trace parameter connecting Stage 2 reward to Stage 1.
    """
    lr_stage1, lr_stage2_base, lr_oci_mod, beta, lambda_eligibility = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Modulate Stage 2 learning rate
    # We allow OCI to either increase or decrease sensitivity depending on the fit of lr_oci_mod
    # We define it as: lr2 = base + (mod * (oci - 0.5)) to allow bidirectional effect roughly centered
    # But to keep within bounds for this specific task structure:
    # Let's assume OCI increases learning rate (hypersensitivity to error)
    lr_stage2 = lr_stage2_base + (lr_oci_mod * oci_score)
    if lr_stage2 > 1: lr_stage2 = 1.0
    if lr_stage2 < 0: lr_stage2 = 0.0
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])
        
        if a1 == -1:
            continue

        # --- Stage 1 Policy ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        # Temporal Difference Learning with Eligibility Trace (lambda)
        
        # 1. Prediction Error at Stage 1 (Transition)
        # Value of arriving at state s2 is max(Q(s2, :))
        v_s2 = np.max(q_stage2[s2])
        delta_1 = v_s2 - q_stage1[a1]
        
        # Update Stage 1 based on transition
        q_stage1[a1] += lr_stage1 * delta_1
        
        # 2. Prediction Error at Stage 2 (Reward)
        delta_2 = r - q_stage2[s2, a2]
        
        # Update Stage 2
        q_stage2[s2, a2] += lr_stage2 * delta_2
        
        # 3. Update Stage 1 again based on Stage 2 outcome (eligibility trace)
        # This connects the final reward back to the first choice
        q_stage1[a1] += lr_stage1 * lambda_eligibility * delta_2

    eps = 1e-10
    valid_trials = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```