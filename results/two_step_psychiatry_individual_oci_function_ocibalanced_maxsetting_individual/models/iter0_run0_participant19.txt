Here are three cognitive models formulated as Python functions. These models integrate the Obsessive-Compulsive Inventory (OCI) score to explain the participant's decision-making process, specifically focusing on the balance between goal-directed and habitual control, perseveration, and decision rigidity.

### Model 1: OCI-Modulated Hybrid Control
**Hypothesis:** High OCI scores are associated with a shift away from model-based (goal-directed) control towards model-free (habitual) control. This model posits that the mixing weight $w$ (which governs the balance between Model-Based and Model-Free values) is negatively modulated by the participant's OCI score.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Hybrid Control.
    
    This model assumes a hybrid Model-Based (MB) and Model-Free (MF) reinforcement learning process.
    The core hypothesis is that higher OCI scores reduce the reliance on the Model-Based system.
    The mixing weight 'w' is scaled down by the OCI score.
    
    Parameters:
    - learning_rate: [0, 1] Rate at which Q-values are updated.
    - beta: [0, 10] Inverse temperature for softmax (exploration/exploitation balance).
    - w_base: [0, 1] Baseline weighting for Model-Based control (before OCI modulation).
    """
    learning_rate, beta, w_base = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Hypothesis: Effective w decreases as OCI increases (shift to habit/model-free)
    # We clip to ensure it stays within [0, 1]
    w_effective = w_base * (1.0 - oci_score)
    w_effective = np.clip(w_effective, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)      # Model-free values for stage 1 (Spaceships)
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for stage 2 (Aliens at Planets)

    for trial in range(n_trials):
        # --- Stage 1: Choice ---
        # Model-Based Value Calculation: Transition * max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value: Weighted sum of MB and MF
        q_net = w_effective * q_stage1_mb + (1 - w_effective) * q_stage1_mf
        
        # Softmax Policy Stage 1
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        curr_state = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 2: Choice ---
        # Standard Softmax on Stage 2 Q-values
        exp_q2 = np.exp(beta * q_stage2_mf[curr_state, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning / Updates ---
        # Stage 1 TD Error (SARSA-style update for MF)
        delta_stage1 = q_stage2_mf[curr_state, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Stage 2 TD Error
        delta_stage2 = r - q_stage2_mf[curr_state, a2]
        q_stage2_mf[curr_state, a2] += learning_rate * delta_stage2
        
        # Note: Additional TD(1) eligibility trace update to Stage 1 could be added here,
        # but we stick to the provided template structure of simple TD(0) steps.

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Driven Perseveration
**Hypothesis:** High OCI scores are linked to compulsivity and repetition ("stickiness"). This model adds a "stickiness" bonus to the previously chosen action in Stage 1. The magnitude of this stickiness is directly determined by the OCI score, explaining the long streaks of repeated choices observed in the participant data.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Driven Perseveration.
    
    This model includes a 'stickiness' parameter that encourages repeating the 
    previous Stage 1 choice. The magnitude of this stickiness is scaled by the OCI score,
    reflecting the hypothesis that higher compulsivity leads to higher motor perseveration.
    
    Parameters:
    - learning_rate: [0, 1] Rate at which Q-values are updated.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weighting between Model-Based and Model-Free values.
    - stick_sensitivity: [0, 5] How strongly OCI scales the perseveration bonus.
    """
    learning_rate, beta, w, stick_sensitivity = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Stickiness magnitude depends on OCI
    stickiness_bonus = stick_sensitivity * oci_score

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1 # Initialize with no previous action

    for trial in range(n_trials):
        # --- Stage 1: Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness bonus to the Q-value of the previously chosen action
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness_bonus
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # Update tracker
        last_action_1 = int(action_1[trial])
        
        curr_state = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 2: Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[curr_state, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        delta_stage1 = q_stage2_mf[curr_state, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[curr_state, a2]
        q_stage2_mf[curr_state, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Decision Rigidity (Beta)
**Hypothesis:** High OCI scores may result in "rigid" decision-making, characterized by a high inverse temperature ($\beta$). This implies that as OCI increases, the participant becomes more deterministic (exploiting the value difference more sharply) and less likely to explore or choose noisily. This model scales $\beta$ based on the OCI score.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Decision Rigidity.
    
    This model posits that OCI affects the 'temperature' of the softmax decision rule.
    Higher OCI scores lead to a higher effective beta (inverse temperature), 
    resulting in more rigid/deterministic choices (less exploration).
    
    Parameters:
    - learning_rate: [0, 1] Rate at which Q-values are updated.
    - beta_base: [0, 10] Baseline inverse temperature.
    - w: [0, 1] Weighting between Model-Based and Model-Free values.
    """
    learning_rate, beta_base, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Hypothesis: Effective beta increases with OCI (more rigid/deterministic)
    # We use a multiplier: 1.0 + OCI. 
    # If OCI is 0.6, Beta becomes 1.6x the base, making choices sharper.
    beta_effective = beta_base * (1.0 + oci_score)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1: Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Use OCI-modulated Beta
        exp_q1 = np.exp(beta_effective * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        curr_state = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 2: Choice ---
        # Also apply rigidity to stage 2
        exp_q2 = np.exp(beta_effective * q_stage2_mf[curr_state, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        delta_stage1 = q_stage2_mf[curr_state, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[curr_state, a2]
        q_stage2_mf[curr_state, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```