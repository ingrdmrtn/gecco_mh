Here are three cognitive models that capture different ways the OCI score might influence decision-making in the two-step task, specifically focusing on the balance between model-based (planning) and model-free (habitual) reinforcement learning systems.

### Cognitive Model 1: OCI-Modulated Model-Based/Model-Free Balance
This model hypothesizes that the OCI score modulates the mixing weight (`w`) between model-based and model-free systems. Higher OCI scores (indicating higher compulsivity) are often associated with a reliance on habitual (model-free) control over goal-directed (model-based) planning.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL model where the balance between Model-Based (MB) and Model-Free (MF) 
    control is modulated by the OCI score.
    
    Hypothesis: High OCI leads to more Model-Free (habitual) behavior.
    
    Parameters:
    lr: Learning rate for value updates [0, 1]
    beta: Inverse temperature for softmax choice [0, 10]
    w_base: Baseline mixing weight (0=pure MF, 1=pure MB) [0, 1]
    w_oci_scale: Sensitivity of mixing weight to OCI score [-1, 1] (Mapped to [0,1] for optimization)
    """
    lr, beta, w_base, w_oci_scale = model_parameters
    
    # Rescale w_oci_scale from [0, 1] to [-1, 1] for flexibility
    w_oci_scale_transformed = (w_oci_scale * 2) - 1
    
    n_trials = len(action_1)
    current_oci = oci[0]

    # Calculate effective mixing weight w
    # We clip to ensure it stays within [0, 1]
    # If w_oci_scale is negative, higher OCI reduces w (more MF)
    w = w_base + (w_oci_scale_transformed * current_oci)
    w = np.clip(w, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2) # Model-free values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for stage 2 (2 states, 2 actions)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        # Model-Based Value: Transition * Max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Value: w * MB + (1-w) * MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        # Standard model-free choice based on Q values of current state
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # --- Learning ---
        # Stage 2 RPE
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_stage2
        
        # Stage 1 RPE (TD(0) update)
        # Using the value of the state actually reached
        v_stage2_state = np.max(q_stage2_mf[s_idx]) 
        delta_stage1 = v_stage2_state - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2: OCI-Dependent Perseveration
This model posits that OCI scores relate to "stickiness" or choice perseveration. Individuals with higher compulsivity might be more prone to repeating their previous action regardless of the reward outcome, reflecting a rigid behavioral pattern.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Based/Model-Free hybrid with a perseveration (stickiness) parameter
    that is modulated by the OCI score.
    
    Hypothesis: High OCI leads to higher perseveration (repeating the last choice).
    
    Parameters:
    lr: Learning rate [0, 1]
    beta: Inverse temperature [0, 10]
    w: Mixing weight (0=MF, 1=MB) [0, 1]
    persev_base: Baseline perseveration bonus [0, 5] (Scaled internally)
    persev_oci: OCI scaling factor for perseveration [0, 5] (Scaled internally)
    """
    lr, beta, w, persev_base_param, persev_oci_param = model_parameters
    
    # Scale params to useful ranges
    persev_base = persev_base_param
    persev_oci = persev_oci_param 
    
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate effective perseveration bonus
    # Higher OCI -> Higher tendency to repeat previous choice
    effective_persev = persev_base + (persev_oci * current_oci)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1 # Initialize as -1 (no previous action)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Calculate net Q-value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add perseveration bonus to the Q-values before softmax
        q_net_with_stickiness = q_net.copy()
        if last_action_1 != -1:
            q_net_with_stickiness[last_action_1] += effective_persev
            
        exp_q1 = np.exp(beta * q_net_with_stickiness)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Store action for next trial's perseveration
        last_action_1 = a1

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # --- Learning ---
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_stage2
        
        # TD(1) / SARSA-like update for stage 1 using stage 2 Q-value
        # Note: Using Q(s', a') here for temporal difference
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3: OCI-Modulated Learning Rate Asymmetry
This model explores the idea that high OCI scores might relate to an asymmetry in how positive versus negative prediction errors are processed. Specifically, it tests if OCI modulates the learning rate for negative prediction errors (punishment/lack of reward), potentially reflecting enhanced sensitivity to failure or "checking" behavior.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Reinforcement Learning model with separate learning rates for positive 
    and negative prediction errors, where the negative learning rate is 
    modulated by OCI.
    
    Hypothesis: High OCI individuals learn differently from lack of reward (0 coins).
    
    Parameters:
    lr_pos: Learning rate for positive RPE (Reward = 1) [0, 1]
    lr_neg_base: Baseline learning rate for negative RPE (Reward = 0) [0, 1]
    lr_neg_oci: Effect of OCI on negative learning rate [-1, 1] (Mapped to [0,1])
    beta: Inverse temperature [0, 10]
    w: Mixing weight [0, 1]
    """
    lr_pos, lr_neg_base, lr_neg_oci_param, beta, w = model_parameters
    
    # Map param to [-1, 1]
    lr_neg_oci_scale = (lr_neg_oci_param * 2) - 1
    
    n_trials = len(action_1)
    current_oci = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # --- Learning ---
        # Calculate Stage 2 RPE
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        # Determine effective learning rate based on sign of RPE
        if delta_stage2 >= 0:
            eff_lr = lr_pos
        else:
            # Modulate negative learning rate by OCI
            # High OCI might increase sensitivity to negative outcomes
            raw_neg_lr = lr_neg_base + (lr_neg_oci_scale * current_oci)
            eff_lr = np.clip(raw_neg_lr, 0.0, 1.0)
            
        q_stage2_mf[s_idx, a2] += eff_lr * delta_stage2
        
        # Stage 1 Update (using stage 2 value)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        
        # Apply same logic to Stage 1 LR
        if delta_stage1 >= 0:
            eff_lr_s1 = lr_pos
        else:
            raw_neg_lr_s1 = lr_neg_base + (lr_neg_oci_scale * current_oci)
            eff_lr_s1 = np.clip(raw_neg_lr_s1, 0.0, 1.0)

        q_stage1_mf[a1] += eff_lr_s1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```