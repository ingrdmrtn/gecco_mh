Here are three cognitive models based on the two-step task structure, incorporating the participant's OCI score to modulate specific cognitive mechanisms.

### Cognitive Model 1: OCI-Modulated Model-Based Weighting
This model tests the hypothesis that obsessive-compulsive traits affect the balance between Goal-Directed (Model-Based) and Habitual (Model-Free) control. High compulsivity is often associated with a deficit in model-based control (reliance on habits). Here, the OCI score shifts the mixing weight $w$.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free RL where OCI modulates the mixing weight (w).
    
    Hypothesis: Higher OCI scores reduce the reliance on the Model-Based system 
    (lower w), leading to more habitual behavior.
    
    Parameters:
    - learning_rate: [0, 1] Update rate for Q-values.
    - beta: [0, 10] Inverse temperature (softmax randomness).
    - w_base: [0, 1] Baseline weight for Model-Based control.
    - w_oci_sens: [0, 1] Sensitivity of w to the OCI score.
      w_effective = w_base - (w_oci_sens * oci)
    
    Args:
        action_1: Array of stage 1 choices (0 or 1).
        state: Array of stage 2 states/planets (0 or 1).
        action_2: Array of stage 2 choices (0 or 1).
        reward: Array of rewards received (0 or 1).
        oci: Array containing the participant's OCI score (scalar).
        model_parameters: List of [learning_rate, beta, w_base, w_oci_sens].
        
    Returns:
        Negative log-likelihood of the observed choices.
    """
    learning_rate, beta, w_base, w_oci_sens = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective MB-weighting based on OCI
    # We clip to ensure it stays within [0, 1]
    # Hypothesis: Higher OCI -> Lower w (more MF/Habitual)
    w_effective = w_base - (w_oci_sens * oci_score)
    w_effective = np.clip(w_effective, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Q-tables
    q_stage1_mf = np.zeros(2)      # Model-Free values for Stage 1
    q_stage2_mf = np.zeros((2, 2)) # Model-Free values for Stage 2 (State x Action)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        # Handle missing data/timeouts
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])

        # --- Stage 1 Policy ---
        # 1. Model-Based Value: Transition Matrix * Max(Stage 2 Values)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Integrated Value: Mix MB and MF based on OCI-modulated weight
        q_net_stage1 = (w_effective * q_stage1_mb) + ((1 - w_effective) * q_stage1_mf)
        
        # 3. Softmax Choice Probability
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        # Standard Softmax on Stage 2 Q-values
        exp_q2 = np.exp(beta * q_stage2_mf[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        # Prediction Error Stage 1 (TD-0 using Stage 2 value)
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Prediction Error Stage 2 (Reward prediction error)
        delta_stage2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += learning_rate * delta_stage2
        
    # Calculate Negative Log Likelihood
    # Filter out 0.0 probabilities from skipped trials to avoid log(0)
    mask = (action_1 != -1)
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    
    return log_loss
```

### Cognitive Model 2: OCI-Modulated Perseveration
This model tests the hypothesis that compulsivity manifests as "stickiness" or perseveration (repeating the previous choice regardless of outcome). The OCI score scales the perseveration bonus added to the previously chosen action.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model with Choice Perseveration modulated by OCI.
    
    Hypothesis: Higher OCI scores increase 'stickiness' (repetition of 
    previous Stage 1 choices), regardless of reward history.
    
    Parameters:
    - learning_rate: [0, 1]
    - beta: [0, 10]
    - w: [0, 1] Fixed mixing weight between MB and MF.
    - p_base: [0, 5] Baseline perseveration bonus.
    - p_oci_factor: [0, 5] How strongly OCI increases perseveration.
      perseveration = p_base + (p_oci_factor * oci)
    
    Args:
        action_1, state, action_2, reward, oci, model_parameters: Standard inputs.
        
    Returns:
        Negative log-likelihood.
    """
    learning_rate, beta, w, p_base, p_oci_factor = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate OCI-dependent perseveration parameter
    perseveration_strength = p_base + (p_oci_factor * oci_score)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_choice_1 = -1 # Initialize with no previous choice

    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Q-value
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Add Perseveration Bonus to the logits (before exponentiation)
        logits = beta * q_net
        if last_choice_1 != -1:
            logits[last_choice_1] += perseveration_strength
            
        exp_q1 = np.exp(logits)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += learning_rate * delta_stage2
        
        # Track choice for next trial
        last_choice_1 = a1

    mask = (action_1 != -1)
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss
```

### Cognitive Model 3: OCI-Modulated Learning Rigidity
This model tests the hypothesis that OCI relates to cognitive inflexibility or rigidity in updating beliefs. Here, the OCI score acts as a dampener on the learning rate, making high-OCI participants slower to update their value estimates in response to new information.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model with Learning Rate Rigidity modulated by OCI.
    
    Hypothesis: Higher OCI scores lead to 'cognitive rigidity', represented 
    by a reduced learning rate. The participant updates values more slowly 
    as OCI increases.
    
    Parameters:
    - lr_base: [0, 1] Base learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight.
    - lambda_elig: [0, 1] Eligibility trace (how much Reward updates Stage 1).
    - rigidity_factor: [0, 1] How much OCI dampens the learning rate.
      lr_effective = lr_base * (1 - (rigidity_factor * oci))
    
    Args:
        action_1, state, action_2, reward, oci, model_parameters: Standard inputs.
        
    Returns:
        Negative log-likelihood.
    """
    lr_base, beta, w, lambda_elig, rigidity_factor = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective learning rate
    # Higher OCI -> Higher dampening -> Lower effective LR
    lr_effective = lr_base * (1.0 - (rigidity_factor * oci_score))
    lr_effective = np.clip(lr_effective, 0.01, 1.0) # Ensure minimal learning
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        # 1. Update Stage 1 MF based on Stage 2 Value (TD-0)
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr_effective * delta_stage1
        
        # 2. Update Stage 2 MF based on Reward
        delta_stage2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += lr_effective * delta_stage2
        
        # 3. Eligibility Trace: Update Stage 1 MF based on final Reward (TD-1 approximation)
        # This allows Stage 1 to learn directly from the final outcome
        q_stage1_mf[a1] += lr_effective * lambda_elig * delta_stage2

    mask = (action_1 != -1)
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss
```