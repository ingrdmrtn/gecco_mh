Here are three cognitive models designed to capture the behavior of the participant, incorporating their specific OCI score and the observed choice patterns.

### Model 1: Hybrid Learner with OCI-Modulated Model-Based Weight
This model assumes the participant uses a mixture of model-free (habitual) and model-based (planning) strategies. The core hypothesis here is that the balance between these systems is influenced by the OCI score. Specifically, higher OCI scores might correlate with a rigid adherence to one system (e.g., model-based control or "over-thinking") or a deficit in the other. Given the participant's score is in the medium-high range (0.617), this model tests if `w` (the weight of model-based control) is a function of OCI.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free Reinforcement Learning.
    
    The weight 'w' determining the trade-off between Model-Based (MB) and Model-Free (MF) 
    values is modulated by the OCI score.
    
    Parameters:
    lr : [0, 1] Learning rate for value updates.
    beta_1 : [0, 10] Inverse temperature for stage 1 choice.
    beta_2 : [0, 10] Inverse temperature for stage 2 choice.
    w_base : [0, 1] Baseline weight for model-based control.
    w_oci : [-1, 1] Coefficient for how OCI affects the MB weight.
    """
    lr, beta_1, beta_2, w_base, w_oci = model_parameters
    n_trials = len(action_1)
    # Cast inputs to integers for indexing
    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    
    # Calculate the effective mixing weight w based on OCI
    # We clip w to be between 0 and 1
    # oci is an array, take the first element (scalar for the participant)
    current_oci = oci[0]
    w = w_base + w_oci * current_oci
    w = np.clip(w, 0.0, 1.0)

    # Fixed transition matrix as per task description
    # A (0) -> X (0) is common (0.7), U (1) -> Y (1) is common (0.7)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Initialize Q-values
    q_stage1_mf = np.zeros(2)       # Model-free values for stage 1 (2 spaceships)
    q_stage2_mf = np.zeros((2, 2))  # Model-free values for stage 2 (2 planets x 2 aliens)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Calculate Model-Based values
        # MB value is the expected value of the best action at stage 2, weighted by transition probs
        max_q_stage2 = np.max(q_stage2_mf, axis=1) # Max value for each planet
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid value: combination of MF and MB
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax selection for Stage 1
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Choice ---
        state_idx = state[trial] # The planet arrived at
        
        # Softmax selection for Stage 2 (purely model-free based on alien values)
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning Updates ---
        
        # Prediction errors
        # Note: In standard TD(1) or similar, we might update stage 1 based on stage 2 value.
        # Here we implement a simple SARSA-like or Q-learning update.
        
        # Stage 2 update (Alien values)
        # RPE at second stage
        rpe_2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr * rpe_2
        
        # Stage 1 update (Spaceship values - Model Free)
        # We use the value of the state actually reached (q_stage2_mf[state_idx, ...]) 
        # as the target for the first stage update.
        # This is a TD(0) update.
        # Note: Some models use the max of stage 2 (Q-learning) or the chosen value (SARSA).
        # We use the updated value of the chosen alien to drive the MF update for the spaceship.
        target_stage1 = q_stage2_mf[state_idx, action_2[trial]]
        rpe_1 = target_stage1 - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr * rpe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Perseveration with OCI-Driven Stickiness
This model posits that the participant exhibits "stickiness" or perseverationâ€”a tendency to repeat the previous choice regardless of reward. This is often observed in compulsive behaviors. The magnitude of this perseveration is modeled as a function of the OCI score. A high OCI score might lead to higher choice repetition (stickiness).

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free Learner with OCI-Dependent Choice Perseveration (Stickiness).
    
    The 'stickiness' parameter adds a bonus to the previously chosen action, 
    making repetition more likely. This bonus is scaled by the OCI score.
    
    Parameters:
    lr : [0, 1] Learning rate.
    beta : [0, 10] Inverse temperature (shared for both stages).
    stickiness_base : [0, 5] Base tendency to repeat choices.
    stickiness_oci : [0, 5] Additional stickiness scaled by OCI.
    """
    lr, beta, stickiness_base, stickiness_oci = model_parameters
    n_trials = len(action_1)
    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    
    current_oci = oci[0]
    
    # Calculate total stickiness
    # This value is added to the Q-value of the action taken in the previous trial
    phi = stickiness_base + stickiness_oci * current_oci
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_action_1 = -1 # No previous action for first trial
    
    for trial in range(n_trials):
        
        # --- Stage 1 Choice ---
        # Add stickiness bonus to Q-values temporarily for decision making
        q_stage1_modified = q_stage1.copy()
        if prev_action_1 != -1:
            q_stage1_modified[prev_action_1] += phi
            
        exp_q1 = np.exp(beta * q_stage1_modified)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Update previous action tracker
        prev_action_1 = action_1[trial]
        
        # --- Stage 2 Choice ---
        state_idx = state[trial]
        # We assume stickiness applies primarily to the top-level choice (spaceship),
        # but standard RL applies Q-values here.
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning ---
        # Standard TD learning
        
        # Stage 2 RPE
        rpe_2 = reward[trial] - q_stage2[state_idx, action_2[trial]]
        q_stage2[state_idx, action_2[trial]] += lr * rpe_2
        
        # Stage 1 RPE
        # Using the value of the state reached (best available Q at stage 2) as target
        # This is a Q-learning style update for stage 1 based on stage 2 values
        max_q2_next = np.max(q_stage2[state_idx])
        rpe_1 = max_q2_next - q_stage1[action_1[trial]]
        q_stage1[action_1[trial]] += lr * rpe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Separate Learning Rates for Positive/Negative Outcomes
This model suggests that the participant learns differently from wins (rewards) versus losses (omissions), and that the OCI score influences the learning rate for negative outcomes (losses). Individuals with high compulsivity might be hyper-sensitive to failure or lack of reward, potentially leading to a higher learning rate for losses (`lr_neg`) compared to wins.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Dual Learning Rate Model (Risk/Loss Sensitivity).
    
    Learning rate splits into alpha_pos (for rewards) and alpha_neg (for omissions).
    The alpha_neg parameter is modulated by OCI, hypothesizing that OCI affects 
    sensitivity to negative feedback (zero coins).
    
    Parameters:
    lr_pos : [0, 1] Learning rate for positive prediction errors (reward > expectation).
    lr_neg_base : [0, 1] Base learning rate for negative prediction errors.
    lr_neg_oci : [-1, 1] Modulation of negative learning rate by OCI.
    beta : [0, 10] Inverse temperature.
    """
    lr_pos, lr_neg_base, lr_neg_oci, beta = model_parameters
    n_trials = len(action_1)
    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    
    current_oci = oci[0]
    
    # Calculate effective negative learning rate
    lr_neg = lr_neg_base + lr_neg_oci * current_oci
    lr_neg = np.clip(lr_neg, 0.0, 1.0)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Choice ---
        state_idx = state[trial]
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning ---
        
        # Stage 2 Update
        rpe_2 = reward[trial] - q_stage2[state_idx, action_2[trial]]
        if rpe_2 >= 0:
            q_stage2[state_idx, action_2[trial]] += lr_pos * rpe_2
        else:
            q_stage2[state_idx, action_2[trial]] += lr_neg * rpe_2
            
        # Stage 1 Update
        # Using simple TD(0) where stage 1 value tracks stage 2 value
        target_val = q_stage2[state_idx, action_2[trial]]
        rpe_1 = target_val - q_stage1[action_1[trial]]
        
        if rpe_1 >= 0:
            q_stage1[action_1[trial]] += lr_pos * rpe_1
        else:
            q_stage1[action_1[trial]] += lr_neg * rpe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```