Here are three cognitive models that analyze the participant's behavior in the two-step task, incorporating their Obsessive-Compulsive Inventory (OCI) score to modulate specific decision-making parameters.

### Model 1: OCI-Modulated Model-Based/Model-Free Balance
This model hypothesizes that the balance between goal-directed (Model-Based) and habitual (Model-Free) control is influenced by the participant's OCI score. High compulsivity is often associated with a reliance on habits (Model-Free) over flexible planning (Model-Based). Here, the mixing weight `w` is a logistic function of the OCI score.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free learning where the mixing weight 'w'
    is modulated by the OCI score.
    
    Hypothesis: Higher OCI scores shift reliance towards Model-Free (habitual) 
    strategies or Model-Based (planning) strategies.
    
    Parameters:
    lr: [0, 1] Learning rate for value updates.
    beta: [0, 10] Inverse temperature (softmax sensitivity).
    w_base: [-5, 5] Base logit for the mixing weight.
    w_oci_slope: [-5, 5] How strongly OCI affects the mixing weight.
                 w = sigmoid(w_base + w_oci_slope * OCI)
    """
    lr, beta, w_base, w_oci_slope = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate mixing weight w (bounded 0 to 1 via sigmoid)
    # w -> 1 means pure Model-Based, w -> 0 means pure Model-Free
    logit_w = w_base + w_oci_slope * current_oci
    w = 1.0 / (1.0 + np.exp(-logit_w))
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)      # Model-free values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for stage 2 (2 states, 2 actions)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # 1. Calculate Model-Based values (Bellman equation using transition matrix)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Combine MB and MF values based on OCI-derived weight
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # 3. Softmax choice probability
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial]) # Planet X (0) or Y (1)

        # --- Stage 2 Policy ---
        # Standard Softmax on Stage 2 Q-values
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Value Updating ---
        # Prediction error for Stage 1 (TD-0)
        # We update Stage 1 MF value based on the value of the state reached
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += lr * delta_stage1
        
        # Prediction error for Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += lr * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Perseveration
The participant data shows significant "stickiness" (repeating the same spaceship choice for long blocks). This model includes a perseveration parameter `p` that adds a bonus to the previously chosen action. The magnitude of this perseveration is modulated by the OCI score, testing the hypothesis that compulsive symptoms correlate with repetitive behavior or "stuckness."

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid model with Choice Perseveration (Stickiness) modulated by OCI.
    
    Hypothesis: OCI score predicts the tendency to repeat the previous Stage 1 choice,
    independent of reward (compulsive repetition).
    
    Parameters:
    lr: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Weight for Model-Based (vs Model-Free) values.
    pers_base: [-2, 2] Baseline perseveration bonus.
    pers_oci_slope: [-2, 2] Effect of OCI on perseveration. 
                    Total Pers = pers_base + pers_oci_slope * OCI
    """
    lr, beta, w, pers_base, pers_oci_slope = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate perseveration bonus
    pers_val = pers_base + pers_oci_slope * current_oci
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1 # No previous action for first trial

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Q-value mixing
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add perseveration bonus to the net Q-values
        # We create a temporary Q vector for decision making
        q_decision = q_net.copy()
        if prev_action_1 != -1:
            q_decision[prev_action_1] += pers_val
            
        exp_q1 = np.exp(beta * q_decision)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Value Updating ---
        # Update Stage 1 MF
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += lr * delta_stage1
        
        # Update Stage 2 MF
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += lr * delta_stage2
        
        # Store action for next trial's perseveration
        prev_action_1 = int(action_1[trial])

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Punishment Sensitivity
This model explores the idea that high OCI scores might correlate with altered sensitivity to negative outcomes (missing a coin). It uses separate learning rates for positive outcomes (finding gold) and negative outcomes (no gold). The OCI score specifically modulates the "negative" learning rate, testing if compulsive individuals over- or under-weight failures.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid model with Asymmetric Learning Rates modulated by OCI.
    
    Hypothesis: OCI affects how strongly the participant learns from 
    negative outcomes (0 coins) compared to positive ones.
    
    Parameters:
    lr_pos: [0, 1] Learning rate for positive rewards (1 coin).
    lr_neg_base: [0, 1] Base learning rate for negative rewards (0 coins).
    lr_neg_oci: [-1, 1] Modulation of negative learning rate by OCI.
                lr_neg = clip(lr_neg_base + lr_neg_oci * OCI, 0, 1)
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Weight for Model-Based values.
    """
    lr_pos, lr_neg_base, lr_neg_oci, beta, w = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate effective negative learning rate, bounded [0,1]
    raw_lr_neg = lr_neg_base + lr_neg_oci * current_oci
    lr_neg = 0.0 if raw_lr_neg < 0 else (1.0 if raw_lr_neg > 1 else raw_lr_neg)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Value Updating ---
        # Determine which learning rate to use based on the outcome of this trial
        current_reward = reward[trial]
        if current_reward > 0.5: # Reward is 1
            current_lr = lr_pos
        else: # Reward is 0
            current_lr = lr_neg
            
        # Update Stage 1 MF
        # Note: We use the same current_lr for the Stage 1 update here for consistency,
        # though some models separate Stage 1 and Stage 2 LRs.
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += current_lr * delta_stage1
        
        # Update Stage 2 MF
        delta_stage2 = current_reward - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += current_lr * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```