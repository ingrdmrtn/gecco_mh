Here are the 3 cognitive models based on the participant's data and OCI score.

### Cognitive Model 1: OCI-Modulated Perseveration Model
This model hypothesizes that the participant's Obsessive-Compulsive symptoms (OCI) primarily drive **perseveration** (stickiness) in the first stage. Individuals with higher compulsivity scores may be more prone to repetitive behaviors regardless of reward history. The model uses a standard hybrid Model-Based/Model-Free architecture, but adds a "stickiness" bonus to the previously chosen spaceship, the magnitude of which is scaled directly by the OCI score.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free model where choice perseverance (stickiness)
    is modulated by the OCI score.
    
    Hypothesis: Higher OCI scores lead to higher 'sticky' behavior (repeating 
    Stage 1 choices) regardless of value.
    
    Parameters:
    - learning_rate: [0,1] Rate of value updating.
    - beta_1: [0,10] Inverse temperature for Stage 1.
    - beta_2: [0,10] Inverse temperature for Stage 2.
    - w: [0,1] Weighting parameter (0=Model-Free, 1=Model-Based).
    - stick_sensitivity: [0,5] Scaling factor for OCI-driven stickiness.
    """
    learning_rate, beta_1, beta_2, w, stick_sensitivity = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective stickiness based on OCI
    effective_stickiness = stick_sensitivity * oci_score
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-tables
    q_stage1_mf = np.zeros(2) # Model-free values for Stage 1
    q_stage2_mf = np.zeros((2, 2)) # Values for Stage 2 (State x Action)
    
    prev_action_1 = -1

    for trial in range(n_trials):
        # Handle missing data
        if action_1[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 0.5 # Placeholder
            p_choice_2[trial] = 0.5 # Placeholder
            continue

        # --- Policy for Choice 1 ---
        # Model-Based Value: Expected max value of Stage 2 weighted by transition probs
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Value: Weighted sum of MB and MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add OCI-driven stickiness to the previous action
        if prev_action_1 != -1:
            q_net[int(prev_action_1)] += effective_stickiness
            
        # Softmax Choice 1
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        # Store probability of the observed action
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        state_idx = int(state[trial])

        # --- Policy for Choice 2 ---
        # Standard Softmax on Stage 2 values
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
  
        # --- Updates ---
        # TD(0) update for Stage 1 (Model-Free)
        # Prediction Error: Value of state 2 (Q2 chosen) - Value of state 1
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # TD(0) update for Stage 2
        # Prediction Error: Reward - Value of state 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Update tracker
        prev_action_1 = a1

    eps = 1e-10
    # Filter out 0.0 probabilities from missing trials to avoid log(0) errors
    # (Though logic above sets them to 0.5, ensuring safety)
    valid_trials = (action_1 != -1) & (action_2 != -1)
    
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```

### Cognitive Model 2: OCI-Interpolated Mixing Weight
This model hypothesizes that the balance between goal-directed (Model-Based) and habitual (Model-Free) control is a function of the OCI score. It defines two baseline weights: one for low-symptom individuals and one for high-symptom individuals. The participant's actual mixing weight `w` is a linear interpolation between these two parameters based on their specific OCI score. This tests if higher compulsivity shifts the participant towards habitual (MF) control.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid model where the Model-Based/Model-Free weighting (w) is determined 
    by interpolating between a 'low-symptom' weight and a 'high-symptom' weight
    based on the participant's OCI score.
    
    Hypothesis: OCI score predicts the reliance on Model-Based strategies.
    
    Parameters:
    - learning_rate: [0,1] Rate of value updating.
    - beta_1: [0,10] Inverse temperature for Stage 1.
    - beta_2: [0,10] Inverse temperature for Stage 2.
    - w_low_oci: [0,1] Mixing weight for hypothetical OCI=0.
    - w_high_oci: [0,1] Mixing weight for hypothetical OCI=1.
    """
    learning_rate, beta_1, beta_2, w_low_oci, w_high_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Linearly interpolate w based on OCI
    # w represents the weight of Model-Based control
    w = w_low_oci * (1 - oci_score) + w_high_oci * oci_score
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        if action_1[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        # --- Policy for Choice 1 ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combine using the OCI-derived weight
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        state_idx = int(state[trial])

        # --- Policy for Choice 2 ---
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
  
        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    valid_trials = (action_1 != -1) & (action_2 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```

### Cognitive Model 3: OCI-Asymmetric Learning Rates
This model suggests that OCI affects how participants update their beliefs, specifically creating an asymmetry between learning from rewards versus non-rewards (positive vs. negative prediction errors). The OCI score modulates the learning rate specifically for negative outcomes (or lack of reward), reflecting a potential "rigidity" or "hypersensitivity" to failure common in compulsive phenotypes.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid model where the Learning Rate is split into positive and negative
    components, and the negative learning rate is modulated by OCI.
    
    Hypothesis: High OCI participants may learn differently from failures 
    (0 coins) than successes (1 coin), potentially exhibiting rigidity (low alpha)
    or hypersensitivity (high alpha) to negative outcomes.
    
    Parameters:
    - lr_pos: [0,1] Learning rate for positive prediction errors.
    - lr_neg_base: [0,1] Base learning rate for negative prediction errors.
    - lr_neg_oci_scale: [0,1] How much OCI scales the negative learning rate.
    - beta_1: [0,10] Inverse temperature for Stage 1.
    - beta_2: [0,10] Inverse temperature for Stage 2.
    - w: [0,1] Model-Based weight.
    """
    lr_pos, lr_neg_base, lr_neg_oci_scale, beta_1, beta_2, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        if action_1[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        # --- Policy for Choice 1 ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        state_idx = int(state[trial])

        # --- Policy for Choice 2 ---
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
  
        # --- Updates ---
        
        # Calculate Effective Learning Rate for Stage 1
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        
        # If PE is negative, modulate LR by OCI; otherwise use standard positive LR
        if delta_stage1 < 0:
            # Scale base negative LR by OCI (e.g., higher OCI reduces/increases impact)
            # We model it such that higher OCI *modifies* the base negative rate.
            # We clamp between 0 and 1.
            current_lr = lr_neg_base * (1.0 + lr_neg_oci_scale * oci_score * (1 if lr_neg_base < 0.5 else -1))
            # Simplified for stability: Weighted average towards the scale param
            current_lr = lr_neg_base * (1 - oci_score) + lr_neg_oci_scale * oci_score
        else:
            current_lr = lr_pos
            
        q_stage1_mf[a1] += current_lr * delta_stage1
        
        # Calculate Effective Learning Rate for Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        
        if delta_stage2 < 0:
            current_lr = lr_neg_base * (1 - oci_score) + lr_neg_oci_scale * oci_score
        else:
            current_lr = lr_pos
            
        q_stage2_mf[state_idx, a2] += current_lr * delta_stage2
        

    eps = 1e-10
    valid_trials = (action_1 != -1) & (action_2 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```