Here are three cognitive models formulated as Python functions. They represent different hypotheses about how Obsessive-Compulsive symptoms (OCI) influence decision-making in the two-step task.

### Model 1: The "OCI-Modulated Perseveration" Model
**Hypothesis:** This model hypothesizes that OCI scores correlate with "stickiness" or perseveration (repeating the previous Stage 1 choice regardless of reward). High OCI leads to a stronger urge to repeat actions, overriding value-based learning.
**Mechanism:** A standard Hybrid (Model-Based + Model-Free) reinforcement learning model, but with an additional "stickiness" parameter in the softmax function that is scaled linearly by the participant's OCI score.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free RL with OCI-modulated perseveration.
    
    Hypothesis: Higher OCI scores lead to higher perseveration (stickiness) 
    on the first-stage choice.
    
    Parameters:
    - learning_rate: [0, 1] Rate of value updating.
    - beta: [0, 10] Inverse temperature (exploration/exploitation).
    - w: [0, 1] Mixing weight (0 = pure Model-Free, 1 = pure Model-Based).
    - stick_base: [0, 5] Baseline perseveration.
    - stick_oci: [0, 5] Additional perseveration scaled by OCI score.
    """
    learning_rate, beta, w, stick_base, stick_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Transition matrix: A(0)->X(0) is 0.7, A(0)->Y(1) is 0.3
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)      # Model-free Q-values for Stage 1
    q_stage2_mf = np.zeros((2, 2)) # Model-free Q-values for Stage 2 (2 states, 2 actions)
    
    prev_action_1 = -1

    for trial in range(n_trials):
        # Handle missing data
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- STAGE 1 POLICY ---
        # 1. Calculate Model-Based values: V_MB = T * max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Mix MB and MF values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # 3. Add Stickiness (Perseveration)
        # Stickiness bonus is added to the Q-value of the previously chosen action
        stickiness_bonus = np.zeros(2)
        if prev_action_1 != -1:
            # Total stickiness = base + (scaling_factor * OCI)
            total_stick = stick_base + (stick_oci * oci_score)
            stickiness_bonus[prev_action_1] = total_stick
            
        # 4. Softmax Probability
        logits = beta * q_net + stickiness_bonus
        # Numerical stability trick
        logits = logits - np.max(logits)
        exp_q = np.exp(logits)
        probs_1 = exp_q / np.sum(exp_q)
        
        p_choice_1[trial] = probs_1[a1]

        # --- STAGE 2 POLICY ---
        # Standard Softmax on Stage 2 Q-values
        logits_2 = beta * q_stage2_mf[s_idx]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        p_choice_2[trial] = probs_2[a2]

        # --- UPDATING ---
        # SARSA / TD(1) style updates
        
        # Stage 1 MF update (TD)
        # Prediction Error using Stage 2 Q-value as proxy for value of state
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Stage 2 MF update
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Store action for next trial's stickiness
        prev_action_1 = a1

    # Calculate Negative Log Likelihood
    eps = 1e-10
    # Mask out 0 probabilities from skipped trials to avoid log(0)
    valid_trials = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_trials] + eps)))
    
    return log_loss
```

### Model 2: The "OCI-Biased Habit" Model
**Hypothesis:** This model hypothesizes that OCI correlates with a deficit in Model-Based control, leading to a reliance on Model-Free (habitual) strategies.
**Mechanism:** The mixing weight parameter $w$ (which balances Model-Based vs. Model-Free control) is not a free parameter, but is directly constrained by the OCI score. Higher OCI forces $w$ towards 0 (pure Model-Free).
**Form:** $w_{effective} = w_{max} \times (1 - OCI)$.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL where OCI score constrains the Model-Based weight.
    
    Hypothesis: High OCI is associated with deficits in Model-Based control 
    (compulsivity/habit). w is calculated as w_max * (1 - OCI).
    
    Parameters:
    - learning_rate: [0, 1]
    - beta: [0, 10]
    - w_max: [0, 1] The maximum model-based weight a participant with 0 OCI would have.
    """
    learning_rate, beta, w_max = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective weight based on OCI
    # If OCI is 0, w = w_max. If OCI is 1, w = 0 (Pure Habit).
    w = w_max * (1.0 - oci_score)
    # Ensure bounds
    if w < 0: w = 0.0
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue

        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- STAGE 1 POLICY ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Mix using the OCI-derived weight
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits = beta * q_net
        logits = logits - np.max(logits)
        exp_q = np.exp(logits)
        probs_1 = exp_q / np.sum(exp_q)
        p_choice_1[trial] = probs_1[a1]

        # --- STAGE 2 POLICY ---
        logits_2 = beta * q_stage2_mf[s_idx]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- UPDATING ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    valid_trials = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```

### Model 3: The "OCI-Rigidity" Model
**Hypothesis:** This model suggests that OCI relates to behavioral rigidity and intolerance of uncertainty. High OCI participants may exhibit higher "exploitation" or determinism in their choices, regardless of whether they are using a Model-Based or Model-Free strategy.
**Mechanism:** The inverse temperature parameter $\beta$ is modulated by OCI. $\beta_{effective} = \beta_{base} + (\beta_{oci} \times OCI)$. A higher OCI results in a "sharper" softmax, making choices more deterministic (less random exploration).

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL where OCI score increases the inverse temperature (beta).
    
    Hypothesis: High OCI leads to rigid, deterministic behavior (reduced exploration).
    Beta = beta_base + (beta_oci_scale * OCI).
    
    Parameters:
    - learning_rate: [0, 1]
    - beta_base: [0, 10] Baseline inverse temperature.
    - beta_oci_scale: [0, 10] Scaling factor for OCI's effect on beta.
    - w: [0, 1] Mixing weight.
    """
    learning_rate, beta_base, beta_oci_scale, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective Beta
    # Higher OCI -> Higher Beta -> More deterministic (less exploration)
    beta_eff = beta_base + (beta_oci_scale * oci_score)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue

        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- STAGE 1 POLICY ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Use OCI-modulated Beta
        logits = beta_eff * q_net
        logits = logits - np.max(logits)
        exp_q = np.exp(logits)
        probs_1 = exp_q / np.sum(exp_q)
        p_choice_1[trial] = probs_1[a1]

        # --- STAGE 2 POLICY ---
        logits_2 = beta_eff * q_stage2_mf[s_idx]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- UPDATING ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    valid_trials = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```