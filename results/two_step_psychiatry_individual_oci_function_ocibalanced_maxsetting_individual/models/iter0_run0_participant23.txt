Here are the three cognitive models based on the participant's data and OCI-R score.

### Analysis of Participant Behavior and OCI Score
The participant has an OCI-R score of **0.366**, which falls into the **medium** range (0.34 - 0.66). This suggests a moderate level of compulsivity or rigidity, but perhaps not extreme.

In the data, we observe periods of sticking to one choice (e.g., choosing spaceship 0 consistently from trial 25-30, then again later), even when rewards are intermittent. This suggests a potential "stickiness" or habit formation. The standard two-step task analysis distinguishes between Model-Based (MB) and Model-Free (MF) control.
*   **Model-Free (MF):** Repeats actions that were recently rewarded.
*   **Model-Based (MB):** Uses the transition structure (Spaceship A -> Planet X) to plan.

Given the "medium" OCI score, we might hypothesize that OCI modulates the balance between these two systems (the `omega` parameter usually controls the weighting between MB and MF) or affects the learning rate (perseveration).

### Model 1: OCI-Weighted Hybrid Model (The "Balanced" Hypothesis)
This model hypothesizes that the OCI score directly influences the trade-off parameter `omega` (w). A higher OCI might correlate with more habitual (Model-Free) control, or conversely, rigid rule-following (Model-Based). Since the score is "medium", this model allows the data to determine if this specific individual leans more MB or MF relative to their symptom score.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free RL with OCI-modulated weighting.
    
    This model assumes the balance between Model-Based (planning) and Model-Free (habit)
    systems is influenced by the OCI score.
    
    Parameters:
    lr: [0, 1] Learning rate for value updates.
    beta: [0, 10] Inverse temperature for softmax.
    w_base: [0, 1] Base weighting for Model-Based control (0 = pure MF, 1 = pure MB).
    w_oci_mod: [0, 1] Strength of OCI modulation on the weighting parameter.
    """
    lr, beta, w_base, w_oci_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0] # Scalar OCI score

    # Calculate effective omega (w) bounded between 0 and 1
    # We hypothesize OCI might shift the balance. 
    # If w_oci_mod is positive, higher OCI pushes w towards 1 or 0 depending on the fit.
    # Here we model it as an additive shift to the base weight.
    # We use a sigmoid-like transformation or simple clipping to keep it in [0,1].
    # Let's use a simple linear shift clipped.
    w = w_base + (w_oci_mod * (oci_score - 0.5)) 
    w = np.clip(w, 0.0, 1.0)

    # Transition matrix (fixed for this task structure: A->X (0->0) is common)
    # 0->0 (0.7), 0->1 (0.3), 1->0 (0.3), 1->1 (0.7)
    # Note: State 0 is Planet X, State 1 is Planet Y. Action 0 is Spaceship A, Action 1 is Spaceship U.
    # Usually A->X is common (0.7).
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    # Stage 1: Model-Free values for spaceships
    q_stage1_mf = np.zeros(2) + 0.5
    # Stage 2: Model-Free values for aliens (State x Action)
    q_stage2_mf = np.zeros((2, 2)) + 0.5 

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        if a1 == -1: continue # Skip missing data
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])

        # --- Stage 1 Decision ---
        # Model-Based Value: V(s') = max_a Q(s', a)
        max_q_stage2 = np.max(q_stage2_mf, axis=1) # Max value of each planet
        q_stage1_mb = transition_matrix @ max_q_stage2 # Expected value of spaceships based on transitions
        
        # Integrated Value: Q_net = w * Q_MB + (1-w) * Q_MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax choice 1
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Decision ---
        # Standard Model-Free choice at the second stage
        exp_q2 = np.exp(beta * q_stage2_mf[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        # Stage 2 Update (TD error)
        # Prediction error at second stage
        delta_stage2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += lr * delta_stage2
        
        # Stage 1 Update (TD(0))
        # Note: In pure MF, we update Stage 1 based on Stage 2 value (SARSA or Q-learning)
        # or based on the reward received at the end. 
        # Standard formulation uses the Stage 2 state-value as the target.
        # delta_stage1 = Q(s2, a2) - Q(s1, a1)
        delta_stage1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        # Additional eligibility trace update for stage 1 based on stage 2 reward (TD(1))
        # Often simplified in these models to just the standard update above, 
        # but sometimes includes a direct reward update: q1 += lr * lambda * delta2.
        # We will stick to the template's implication of simple TD updates.

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Driven Choice Stickiness (Perseveration)
Individuals with compulsivity often exhibit "stickiness" or perseverationâ€”repeating the previous choice regardless of the outcome. This model introduces a "stickiness" parameter that is directly scaled by the OCI score. A higher OCI score increases the bonus added to the previously chosen action, making the participant more resistant to switching even after negative feedback.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free RL with OCI-modulated Choice Stickiness (Perseveration).
    
    This model ignores the Model-Based component to focus on habit. 
    It adds a 'stickiness' bonus to the Q-value of the previously chosen action.
    The magnitude of this stickiness is scaled by the OCI score.
    
    Parameters:
    lr: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    stickiness_base: [0, 5] Base tendency to repeat the last choice.
    stickiness_oci: [0, 5] Additional stickiness scaled by OCI score.
    """
    lr, beta, stickiness_base, stickiness_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Total stickiness parameter
    # People with high OCI might be 'stickier'
    kappa = stickiness_base + (stickiness_oci * oci_score)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2) + 0.5
    q_stage2 = np.zeros((2, 2)) + 0.5
    
    last_action_1 = -1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        if a1 == -1: 
            last_action_1 = -1
            continue
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])

        # --- Stage 1 Choice ---
        # Add stickiness bonus to the Q-values for decision making (but not learning)
        q_net_1 = q_stage1.copy()
        if last_action_1 != -1:
            q_net_1[last_action_1] += kappa

        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Choice ---
        # Standard MF choice
        exp_q2 = np.exp(beta * q_stage2[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        # Update Stage 2
        delta2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += lr * delta2
        
        # Update Stage 1 (SARSA-style TD(0) update driven by value of next state)
        # Using the value of the state reached (Planet)
        # V(s') = max Q(s', a') is a common proxy, or just Q(s', chosen_a')
        # Here we use Q(s, a2) as the target for Stage 1
        delta1 = q_stage2[s, a2] - q_stage1[a1]
        q_stage1[a1] += lr * delta1
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Learning Rate Asymmetry
This model posits that OCI affects how participants learn from positive versus negative prediction errors. Compulsive individuals might be hypersensitive to punishment (loss of reward) or rigid in the face of reward. This model splits the learning rate into `lr_pos` (for positive RPEs) and `lr_neg` (for negative RPEs), and uses the OCI score to skew the learning rate towards negative updating (or positive, depending on the fit).

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Based/Model-Free Hybrid with OCI-modulated Learning Rate Asymmetry.
    
    This model tests if OCI affects the ratio of learning from positive vs negative outcomes.
    It assumes a fixed MB/MF balance (w=0.5 for simplicity to save parameters) but
    allows OCI to scale the learning rate for negative prediction errors specifically.
    
    Parameters:
    lr_base: [0, 1] Base learning rate.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Weighting between MB and MF.
    k_oci: [0, 5] Multiplier for negative learning rate based on OCI.
           If k_oci > 1, higher OCI leads to stronger learning from disappointment.
    """
    lr_base, beta, w, k_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2) + 0.5
    q_stage2_mf = np.zeros((2, 2)) + 0.5

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        if a1 == -1: continue
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])

        # --- Stage 1 Policy ---
        # MB Calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Q
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning with Asymmetry ---
        
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[s, a2]
        
        # Determine effective learning rate for Stage 2
        if delta_stage2 >= 0:
            eff_lr = lr_base
        else:
            # OCI modulates sensitivity to negative outcomes
            # We constrain the multiplier so lr doesn't explode > 1 easily
            eff_lr = lr_base * (1.0 + (k_oci * oci_score))
            eff_lr = np.clip(eff_lr, 0.0, 1.0)
            
        q_stage2_mf[s, a2] += eff_lr * delta_stage2
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        
        # Apply same asymmetry logic to Stage 1
        if delta_stage1 >= 0:
            eff_lr_1 = lr_base
        else:
            eff_lr_1 = lr_base * (1.0 + (k_oci * oci_score))
            eff_lr_1 = np.clip(eff_lr_1, 0.0, 1.0)

        q_stage1_mf[a1] += eff_lr_1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```