Here are three cognitive models designed to explain the participant's behavior in the two-step task, incorporating their OCI (Obsessive-Compulsive Inventory) score to explain individual variability.

### Model 1: Hybrid MB/MF with OCI-Modulated Perseveration
**Hypothesis:** Individuals with higher OCI scores often exhibit "stickiness" or compulsivity in their choices, repeating previous actions regardless of the reward outcome. This model modifies a standard Hybrid Model-Based/Model-Free learner by adding a "perseveration bonus" to the Stage 1 choice. The magnitude of this bonus is determined by the participant's OCI score.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free learner where OCI modulates choice perseveration.
    
    Hypothesis: High OCI leads to higher motor perseveration (repeating stage 1 choice),
    independent of value-based learning.
    
    Parameters:
    - learning_rate: [0, 1] Speed of value updating.
    - beta: [0, 10] Inverse temperature (exploration/exploitation).
    - w: [0, 1] Weighting between Model-Based (1) and Model-Free (0).
    - persev_base: [0, 5] Baseline tendency to repeat the previous choice.
    - persev_oci_slope: [0, 5] How strongly OCI increases perseveration.
    """
    learning_rate, beta, w, persev_base, persev_oci_slope = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]
    
    # Calculate effective perseveration bonus based on OCI
    # A higher OCI score increases the tendency to repeat the last action
    perseveration_weight = persev_base + (persev_oci_slope * participant_oci)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Q-values initialization
    q_stage1_mf = np.zeros(2)      # Model-free values for Stage 1 (Spaceships)
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for Stage 2 (Aliens)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_action_1 = -1

    for trial in range(n_trials):
        # Handle missing data
        if action_1[trial] == -1 or action_2[trial] == -1:
            continue

        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- STAGE 1 POLICY ---
        # 1. Calculate Model-Based values: V(state) = max(Q_stage2(state, :))
        # Q_MB = Transition_Matrix * V_States
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Combine MB and MF values
        q_net_stage1 = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # 3. Add Perseveration Bonus
        if prev_action_1 != -1:
            q_net_stage1[prev_action_1] += perseveration_weight

        # 4. Softmax probability
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- STAGE 2 POLICY ---
        # Standard softmax on Stage 2 Q-values
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- UPDATES ---
        # Update Stage 1 MF (TD-0 style using Stage 2 value as proxy for reward)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Update Stage 2 MF (Standard Q-learning)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Store action for next trial's perseveration
        prev_action_1 = a1

    eps = 1e-10
    # Sum logs, masking out 0s from skipped trials
    log_lik_1 = np.sum(np.log(p_choice_1[p_choice_1 > 0] + eps))
    log_lik_2 = np.sum(np.log(p_choice_2[p_choice_2 > 0] + eps))
    
    return -(log_lik_1 + log_lik_2)
```

### Model 2: OCI-Dependent Goal-Directedness (Variable w)
**Hypothesis:** OCI scores correlate with the balance between habitual (Model-Free) and goal-directed (Model-Based) control. This model does not treat the mixing parameter `w` as a static free parameter. Instead, `w` is a logistic function of the OCI score. This tests if higher OCI symptoms drive the participant towards more Model-Free (habitual) behavior (lower `w`) or Model-Based behavior (higher `w`).

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid model where the balance between Model-Based and Model-Free (w)
    is determined by the OCI score.
    
    Hypothesis: OCI predicts the reliance on Model-Based strategies.
    
    Parameters:
    - learning_rate: [0, 1]
    - beta: [0, 10]
    - w_intercept: [-5, 5] Base logit for MB weighting.
    - w_oci_slope: [-5, 5] How OCI shifts the MB/MF balance. 
      (Negative slope implies High OCI -> More Model-Free/Habitual).
    """
    learning_rate, beta, w_intercept, w_oci_slope = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]

    # Calculate w (weight) dynamically based on OCI using a sigmoid function
    # w will be bounded between 0 (Pure MF) and 1 (Pure MB)
    w_logit = w_intercept + (w_oci_slope * participant_oci)
    w = 1.0 / (1.0 + np.exp(-w_logit))

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        if action_1[trial] == -1 or action_2[trial] == -1:
            continue

        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- STAGE 1 CHOICE ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Mix MB and MF based on OCI-derived w
        q_net_stage1 = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- STAGE 2 CHOICE ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- UPDATES ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_lik_1 = np.sum(np.log(p_choice_1[p_choice_1 > 0] + eps))
    log_lik_2 = np.sum(np.log(p_choice_2[p_choice_2 > 0] + eps))
    
    return -(log_lik_1 + log_lik_2)
```

### Model 3: OCI-Modulated Belief Rigidity (Learning Rate)
**Hypothesis:** High OCI scores are associated with cognitive rigidityâ€”a difficulty in updating beliefs in response to new information. This model posits that the learning rate ($\alpha$) is not constant across individuals but is dampened by the OCI score. A higher OCI results in a lower effective learning rate, causing the participant to hold onto old value estimates longer.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid model where OCI dampens the learning rate (Rigidity Hypothesis).
    
    Hypothesis: High OCI participants are more rigid, resulting in a lower 
    effective learning rate (slower updating of values).
    
    Parameters:
    - lr_base: [0, 1] Baseline learning rate for a theoretical OCI of 0.
    - lr_dampening: [0, 1] Factor by which OCI reduces the learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] MB/MF mixing weight.
    """
    lr_base, lr_dampening, beta, w = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]

    # Calculate effective learning rate
    # As OCI increases, effective_lr decreases (Rigidity)
    # effective_lr = lr_base * (1 - (dampening * OCI))
    # We clip to ensure it stays non-negative
    effective_lr = lr_base * (1.0 - (lr_dampening * participant_oci))
    effective_lr = np.maximum(effective_lr, 0.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        if action_1[trial] == -1 or action_2[trial] == -1:
            continue

        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- STAGE 1 CHOICE ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- STAGE 2 CHOICE ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- UPDATES (Using effective_lr) ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += effective_lr * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += effective_lr * delta_stage2

    eps = 1e-10
    log_lik_1 = np.sum(np.log(p_choice_1[p_choice_1 > 0] + eps))
    log_lik_2 = np.sum(np.log(p_choice_2[p_choice_2 > 0] + eps))
    
    return -(log_lik_1 + log_lik_2)
```