Here are three cognitive models designed to explain the participant's behavior in the two-step task, incorporating their "medium" OCI score (0.3833).

### Model 1: Hybrid Learner with OCI-Modulated Model-Based Weight
This model assumes that the balance between Model-Based (planning) and Model-Free (habit) learning is influenced by the participant's obsessive-compulsive traits. The parameter `w` (mixing weight) is dynamically scaled by the OCI score, testing the hypothesis that OCI symptoms affect reliance on the transition structure of the task.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL model where the balance between Model-Based (MB) and Model-Free (MF) 
    control is modulated by the OCI score.
    
    Parameters:
    - learning_rate: [0, 1] Update rate for Q-values.
    - beta: [0, 10] Inverse temperature for softmax choice.
    - w_base: [0, 1] Baseline weight for Model-Based control (0=Pure MF, 1=Pure MB).
    - oci_mod: [0, 1] Strength of OCI score's influence on the mixing weight w.
    """
    learning_rate, beta, w_base, oci_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective weight w. We clamp it between 0 and 1.
    # Hypothesis: Higher OCI might push w away from the baseline (either rigid habits or over-planning).
    # Here we model it as an additive effect.
    w = w_base + (oci_score * oci_mod)
    if w > 1.0: w = 1.0
    if w < 0.0: w = 0.0

    # Fixed transition matrix (70% common, 30% rare) as described in task
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)      # Model-free values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for stage 2 (2 planets x 2 aliens)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based value calculation: V(state) = max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid value
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Softmax policy Stage 1
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        # Store probability of the observed action
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # Transition to state (planet)
        s_idx = int(state[trial])
        
        # --- Stage 2 Choice ---
        # Only Model-Free learning applies at the second stage
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        r = reward[trial]
        
        # Update Stage 2 Q-values (SARSA/Q-learning equivalent here since terminal)
        # PE = Reward - Expectation
        pe_2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * pe_2
        
        # Update Stage 1 Q-values (TD-learning)
        # Using the value of the state actually reached (SARSA-like for the transition)
        # Note: Standard two-step usually updates MF stage 1 based on Stage 2 value
        pe_1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * pe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Perseveration Model with OCI-Driven Stickiness
This model posits that the "medium" OCI score relates to perseveration or "stickiness"â€”a tendency to repeat the previous choice regardless of reward. The OCI score scales the `stickiness` parameter, making the participant more likely to repeat actions if their symptoms are higher.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner with choice perseveration (stickiness) scaled by OCI.
    
    Parameters:
    - learning_rate: [0, 1] Update rate for Q-values.
    - beta: [0, 10] Inverse temperature.
    - stick_base: [0, 5] Base parameter for choice repetition bonus.
    - oci_stick_scale: [0, 5] How much OCI amplifies the stickiness.
    """
    learning_rate, beta, stick_base, oci_stick_scale = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective stickiness
    # Stickiness is added to the Q-value of the previously chosen action
    stickiness = stick_base + (oci_score * oci_stick_scale)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    last_action_1 = -1 # Initialize as invalid index

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Add stickiness bonus to Q-values for decision making only
        q_net_1 = q_stage1.copy()
        if last_action_1 != -1:
            q_net_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        last_action_1 = a1 # Update for next trial
        s_idx = int(state[trial])

        # --- Stage 2 Choice ---
        # No stickiness modeled for stage 2 in this variant to isolate stage 1 habit
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        r = reward[trial]
        
        # Update Stage 2
        pe_2 = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += learning_rate * pe_2
        
        # Update Stage 1 (TD(0))
        # Update using the value of the second stage choice
        pe_1 = q_stage2[s_idx, a2] - q_stage1[a1]
        q_stage1[a1] += learning_rate * pe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Separate Learning Rates for Reward/Punishment (OCI-Biased)
This model investigates if OCI symptoms relate to differential sensitivity to positive outcomes (coins) versus negative outcomes (no coins). The OCI score is used to bias the learning rate specifically for negative outcomes (zero reward), testing if higher symptomology leads to over-learning from failure.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner with separate learning rates for positive and negative rewards.
    The negative learning rate is modulated by OCI.
    
    Parameters:
    - alpha_pos: [0, 1] Learning rate for rewarded trials (coin).
    - alpha_neg_base: [0, 1] Base learning rate for unrewarded trials (no coin).
    - oci_sens: [0, 1] Multiplier for OCI impact on negative learning rate.
    - beta: [0, 10] Inverse temperature.
    """
    alpha_pos, alpha_neg_base, oci_sens, beta = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective negative learning rate
    # If OCI is high, alpha_neg might increase (hypersensitivity to failure)
    alpha_neg = alpha_neg_base + (oci_score * oci_sens)
    if alpha_neg > 1.0: alpha_neg = 1.0
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        s_idx = int(state[trial])

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        r = reward[trial]
        
        # Determine which learning rate to use
        if r > 0:
            lr = alpha_pos
        else:
            lr = alpha_neg
        
        # Update Stage 2
        pe_2 = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += lr * pe_2
        
        # Update Stage 1
        # Using the value of the chosen stage 2 option to update stage 1
        pe_1 = q_stage2[s_idx, a2] - q_stage1[a1]
        q_stage1[a1] += lr * pe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```