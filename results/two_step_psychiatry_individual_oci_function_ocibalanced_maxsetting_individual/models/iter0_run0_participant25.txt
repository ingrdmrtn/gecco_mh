Here are three cognitive models implemented as Python functions. These models hypothesize different ways the participant's "medium" Obsessive-Compulsive Inventory (OCI) score influences their decision-making process, particularly regarding the balance between goal-directed planning (Model-Based) and habitual repetition (Model-Free/Perseveration).

### Model 1: OCI-Modulated Model-Based Weighting
This model tests the hypothesis that higher OCI scores correlate with a reduction in Model-Based (planning) control. The mixing parameter $w$ (which governs the trade-off between Model-Based and Model-Free values) is negatively modulated by the OCI score. A participant with higher OCI is assumed to rely more on Model-Free (habitual) values.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free learner where the balance (w) is modulated by OCI.
    
    Hypothesis: Higher OCI scores reduce the reliance on Model-Based planning (w),
    leading to more habitual (Model-Free) choices.
    
    Parameters:
    - learning_rate: [0, 1] Rate of updating Q-values.
    - beta: [0, 10] Inverse temperature (softness of choice).
    - w_base: [0, 1] Baseline weight for Model-Based control.
    - w_oci_damp: [0, 1] How strongly OCI reduces the w parameter. 
      Effective w = w_base * (1 - w_oci_damp * oci).
    """
    learning_rate, beta, w_base, w_oci_damp = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate effective mixing weight w based on OCI
    # If OCI is high and w_oci_damp is high, w decreases (more Model-Free)
    w = w_base * (1.0 - (w_oci_damp * current_oci))
    w = np.clip(w, 0.0, 1.0)

    # Transition matrix: A(0)->X(0) is 0.7, U(1)->Y(1) is 0.7
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Q-values
    q_stage1_mf = np.zeros(2)       # Model-free values for spaceships (0, 1)
    q_stage2_mf = np.zeros((2, 2))  # Model-free values for aliens (Planet 0/1, Alien 0/1)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])

        # --- Stage 1 Decision ---
        # 1. Calculate Model-Based values: V_MB(s1) = T * max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Integrate MB and MF values
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # 3. Softmax Choice Probability
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Decision ---
        # Standard Softmax on Stage 2 Q-values
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning Updates ---
        # Update Stage 2 MF values (Prediction Error 2)
        pe_2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * pe_2
        
        # Update Stage 1 MF values (Prediction Error 1)
        # Using simple TD(0) where the value of the state reached (max Q2) is the target
        # Note: Using max_q_stage2 from *before* the update (standard Q-learning) 
        # or after (SARSA) is a choice; here we use the value of the state actually reached.
        target_stage1 = q_stage2_mf[s_idx, a2] # SARSA-like connection
        pe_1 = target_stage1 - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * pe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Driven Perseveration
This model addresses the "sticky" behavior observed in the participant data (long streaks of choosing the same spaceship). It hypothesizes that OCI scores relate to "stickiness" or perseveration. A specific parameter adds a bonus to the previously chosen action, and the magnitude of this bonus is scaled by the OCI score.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model with OCI-modulated Perseveration (Stickiness).
    
    Hypothesis: The participant's OCI score drives a tendency to repeat 
    the previous Stage 1 choice regardless of reward (compulsive repetition).
    
    Parameters:
    - learning_rate: [0, 1]
    - beta: [0, 10]
    - w: [0, 1] Mixing weight (fixed).
    - pers_base: [0, 5] Baseline perseveration bonus.
    - pers_oci_sens: [0, 5] Additional perseveration scaled by OCI.
      Total Bonus = pers_base + (pers_oci_sens * oci).
    """
    learning_rate, beta, w, pers_base, pers_oci_sens = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate Perseveration Bonus
    pers_bonus = pers_base + (pers_oci_sens * current_oci)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1 # Initialize previous action placeholder

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])

        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Calculate net value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add perseveration bonus to the logits of the previous action
        logits = beta * q_net
        if prev_a1 != -1:
            logits[prev_a1] += pers_bonus
            
        # Softmax
        exp_q1 = np.exp(logits - np.max(logits)) # Subtract max for stability
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Stage 2 Update
        pe_2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * pe_2
        
        # Stage 1 Update (TD)
        target = q_stage2_mf[s_idx, a2]
        pe_1 = target - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * pe_1
        
        # Store action for next trial's perseveration
        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Learning Rate
This model hypothesizes that OCI impacts the *rate* at which the participant updates their internal models (habit formation). Specifically, it posits that higher OCI might lead to "rigidity" (lower learning rate, ignoring new evidence) or "hyper-sensitivity" (higher learning rate). The effective learning rate is a function of the base rate and the OCI score.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model with OCI-modulated Learning Rate.
    
    Hypothesis: OCI affects the plasticity of the system. Higher OCI might
    suppress the learning rate (rigidity) or enhance it.
    
    Parameters:
    - lr_base: [0, 1] Base learning rate.
    - lr_oci_mod: [0, 1] Modulation factor.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] MB/MF mixing weight.
    - direction: [0, 1] Binary switch (treated as continuous 0-1). 
      If < 0.5: High OCI reduces LR (Rigidity).
      If >= 0.5: High OCI increases LR (Anxiety/Hyper-learning).
    """
    lr_base, lr_oci_mod, beta, w, direction = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Determine effective learning rate
    if direction < 0.5:
        # Rigidity hypothesis: OCI reduces learning rate
        effective_lr = lr_base * (1.0 - (lr_oci_mod * current_oci))
    else:
        # Hyper-learning hypothesis: OCI increases learning rate
        effective_lr = lr_base + (lr_oci_mod * current_oci * (1.0 - lr_base))
        
    effective_lr = np.clip(effective_lr, 0.01, 1.0) # Ensure non-zero

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])

        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates using OCI-modulated Learning Rate ---
        # Stage 2
        pe_2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += effective_lr * pe_2
        
        # Stage 1
        # Using a Lambda=1 approach (full propagation of reward) for robustness
        # Combined with TD(0) from Stage 2 value
        target = q_stage2_mf[s_idx, a2]
        pe_1 = target - q_stage1_mf[a1]
        q_stage1_mf[a1] += effective_lr * pe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```