Here are three cognitive models analyzing the participant's behavior in the two-step task. Each model incorporates the OCI score to explain variations in decision-making strategies, specifically focusing on the balance between goal-directed (Model-Based) and habitual (Model-Free) control, as well as perseveration and learning sensitivity.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Model-Based/Model-Free Hybrid.
    
    Hypothesis: 
    Participants with higher OCI scores exhibit reduced reliance on Model-Based (MB) 
    planning and increased reliance on Model-Free (MF) habits. The mixing weight 'w' 
    is dynamically scaled by the OCI score.
    
    Parameters:
    - learning_rate: [0, 1] Rate at which Q-values are updated.
    - beta: [0, 10] Inverse temperature for softmax choice (exploration/exploitation).
    - w_base: [0, 1] Baseline weight for Model-Based control.
    """
    learning_rate, beta, w_base = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # OCI Modulation: Higher OCI reduces the effective weight of MB planning.
    # We clip to ensure w remains in [0, 1].
    w_effective = w_base * (1.0 - oci_score)
    w_effective = np.clip(w_effective, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)      # Model-Free values for Stage 1 (Spaceships)
    q_stage2_mf = np.zeros((2, 2)) # Model-Free values for Stage 2 (Aliens at Planets)

    log_likelihood = 0.0
    eps = 1e-10

    for trial in range(n_trials):
        # Handle missing data (participant data contains -1 for missed trials)
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        # Calculate Model-Based values: Transition * Max(Stage 2 Values)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value: Mix MB and MF based on OCI-modulated weight
        q_net = w_effective * q_stage1_mb + (1 - w_effective) * q_stage1_mf
        
        # Softmax for Stage 1
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Accumulate Log Loss
        log_likelihood += np.log(p_choice_1[trial] + eps)

        # --- Stage 2 Policy ---
        # Standard Softmax on Stage 2 MF values for the current state
        exp_q2 = np.exp(beta * q_stage2_mf[s2, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        log_likelihood += np.log(p_choice_2[trial] + eps)
  
        # --- Updates ---
        # TD Error Stage 1 (SARSA-style update using value of chosen stage 2 action)
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # TD Error Stage 2
        delta_stage2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += learning_rate * delta_stage2
        
    return -log_likelihood


def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: Hybrid Model with OCI-Driven Perseveration.
    
    Hypothesis:
    Obsessive-compulsive traits are associated with "stickiness" or perseveration 
    (repeating the previous choice regardless of reward). This model adds a 
    stickiness bonus to the Stage 1 choice, scaled by the OCI score.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weighting between MB and MF (constant in this model).
    - stick_base: [0, 5] Baseline perseveration bonus.
    """
    learning_rate, beta, w, stick_base = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # OCI Modulation: Higher OCI increases choice stickiness.
    stickiness = stick_base * (1.0 + oci_score)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    log_likelihood = 0.0
    eps = 1e-10
    
    # Track previous choice for perseveration
    last_action_1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1:
            last_action_1 = -1 # Reset memory on missed trial
            continue
            
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add Stickiness Bonus
        logits = beta * q_net
        if last_action_1 != -1:
            logits[last_action_1] += stickiness
            
        exp_q1 = np.exp(logits - np.max(logits)) # Subtract max for stability
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        log_likelihood += np.log(probs_1[a1] + eps)

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s2, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        log_likelihood += np.log(probs_2[a2] + eps)
  
        # --- Updates ---
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += learning_rate * delta_stage2
        
        # Update history
        last_action_1 = a1

    return -log_likelihood


def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: Hybrid Model with OCI-Modulated Plasticity (Learning Rate).
    
    Hypothesis:
    OCI scores correlate with altered error correction sensitivity. This model 
    posits that the learning rate (plasticity) is scaled by the OCI score, 
    making the participant potentially more volatile or reactive to recent outcomes 
    if OCI is high.
    
    Parameters:
    - lr_base: [0, 1] Baseline learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weighting between MB and MF.
    """
    lr_base, beta, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # OCI Modulation: Learning rate scales with OCI.
    # We maintain bounds [0, 1].
    # Assumption: Higher OCI -> Higher reactivity to prediction errors.
    learning_rate = lr_base * (0.5 + oci_score)
    learning_rate = np.clip(learning_rate, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    log_likelihood = 0.0
    eps = 1e-10

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_likelihood += np.log(probs_1[a1] + eps)

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s2, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_likelihood += np.log(probs_2[a2] + eps)
  
        # --- Updates ---
        # Note: Using the OCI-modulated learning rate for both stages
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += learning_rate * delta_stage2

    return -log_likelihood
```