Here are three cognitive models designed to explain the participant's behavior, incorporating their medium OCI score into the decision-making process.

### Model 1: Hybrid Model with OCI-modulated Mixing
This model assumes that decision-making is a hybrid of Model-Based (MB) and Model-Free (MF) reinforcement learning. The core hypothesis is that the OCI score influences the balance (mixing weight `w`) between these two systems. A score of 0.35 (medium) suggests a potential imbalance or specific weighting between habitual (MF) and goal-directed (MB) control.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid reinforcement learning model where the balance between Model-Based (MB)
    and Model-Free (MF) control is modulated by the OCI score.
    
    The mixing weight 'w' is a logistic function of the OCI score, allowing
    symptom severity to shift the strategy.
    
    Parameters:
    - learning_rate: Update rate for Q-values [0, 1]
    - beta_stage1: Inverse temperature for stage 1 choice [0, 10]
    - beta_stage2: Inverse temperature for stage 2 choice [0, 10]
    - w_intercept: Base mixing weight parameter before OCI modulation [0, 1]
    - w_slope: Sensitivity of mixing weight to OCI score [0, 10]
    """
    learning_rate, beta_stage1, beta_stage2, w_intercept, w_slope = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate mixing weight w based on OCI
    # w represents the weight of Model-Based control (0 = Pure MF, 1 = Pure MB)
    w = 1 / (1 + np.exp(-(w_intercept + w_slope * (current_oci - 0.5))))
    
    # Transition matrix (fixed as per task structure: A->X (0->0) common, U->Y (1->1) common)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Initialize Q-values
    q_stage1_mf = np.zeros(2)      # MF values for stage 1 choices (Spaceships)
    q_stage2_mf = np.zeros((2, 2)) # MF values for stage 2 choices (Aliens per Planet)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Skip trials with missing data (indicated by -1)
        if a1 == -1 or s_idx == -1 or a2 == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # --- Stage 1 Policy ---
        # Model-Based Value: Expected max value of next stage weighted by transition probs
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Value: Weighted sum of MB and MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax for Stage 1
        exp_q1 = np.exp(beta_stage1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        # Standard Softmax on Stage 2 Q-values
        exp_q2 = np.exp(beta_stage2 * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        # Stage 2 Update (standard TD)
        prediction_error_2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * prediction_error_2
        
        # Stage 1 Update (TD(0) using Stage 2 value)
        # Note: In pure hybrid models, MF Q1 is often updated by the value of the state reached
        prediction_error_1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * prediction_error_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Driven Perseveration Model
This model hypothesizes that OCI symptoms manifest as "stickiness" or perseverationâ€”a tendency to repeat the previous action regardless of reward history. The `perseveration` parameter is added to the Q-values, and its magnitude is scaled directly by the OCI score. This reflects the repetitive behaviors often seen in obsessive-compulsive spectrum disorders.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free reinforcement learning with choice perseveration (stickiness)
    modulated by the OCI score.
    
    The OCI score scales the 'perseveration_strength', making high-OCI participants
    more likely to repeat their previous Stage 1 choice regardless of outcome.
    
    Parameters:
    - learning_rate: Update rate for Q-values [0, 1]
    - beta: Inverse temperature for both stages [0, 10]
    - perseveration_base: Baseline tendency to repeat choice [0, 5]
    - oci_sensitivity: How strongly OCI amplifies perseveration [0, 5]
    """
    learning_rate, beta, perseveration_base, oci_sensitivity = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate effective perseveration bonus
    pers_weight = perseveration_base + (oci_sensitivity * current_oci)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        if a1 == -1 or s_idx == -1 or a2 == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            last_action_1 = -1
            continue

        # --- Stage 1 Policy ---
        # Add perseveration bonus to the previously chosen action
        q_net_stage1 = q_stage1.copy()
        if last_action_1 != -1:
            q_net_stage1[last_action_1] += pers_weight
            
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        # SARSA / TD(1) style update logic often used in these tasks
        # Update Stage 2
        dt_2 = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += learning_rate * dt_2
        
        # Update Stage 1 based on Stage 2 value (TD0)
        # Using the value of the chosen stage 2 state
        val_stage2 = q_stage2[s_idx, a2]
        dt_1 = val_stage2 - q_stage1[a1]
        q_stage1[a1] += learning_rate * dt_1
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Learning Rate Asymmetry
This model suggests that OCI scores affect how participants learn from positive versus negative prediction errors. It implements separate learning rates for positive (`alpha_pos`) and negative (`alpha_neg`) outcomes. The OCI score is used to skew this ratio, testing the hypothesis that OCI is associated with hyper-sensitivity to negative feedback (punishment) or "safety" signals.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Reinforcement learning model with asymmetric learning rates for positive and
    negative prediction errors, modulated by OCI.
    
    The model tests if OCI score alters the balance between learning from reward
    vs. lack of reward (or punishment).
    
    Parameters:
    - base_lr: Base learning rate [0, 1]
    - beta: Inverse temperature [0, 10]
    - oci_neg_bias: How much OCI increases learning from negative errors [0, 1]
    - lambda_eligibility: Eligibility trace decay (0=TD0, 1=Monte Carlo) [0, 1]
    """
    base_lr, beta, oci_neg_bias, lambda_eligibility = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Define asymmetric learning rates based on OCI
    # Higher OCI -> Higher learning rate for negative prediction errors
    alpha_pos = base_lr
    alpha_neg = base_lr + (oci_neg_bias * current_oci)
    
    # Cap alpha_neg at 1.0 to maintain stability
    if alpha_neg > 1.0: alpha_neg = 1.0
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        if a1 == -1 or s_idx == -1 or a2 == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # --- Stage 1 Policy ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning with Eligibility Traces & Asymmetry ---
        
        # Stage 2 PE
        pe2 = r - q_stage2[s_idx, a2]
        lr2 = alpha_pos if pe2 >= 0 else alpha_neg
        q_stage2[s_idx, a2] += lr2 * pe2
        
        # Stage 1 PE
        # Using the updated Stage 2 value as the target for Stage 1
        target_stage1 = q_stage2[s_idx, a2]
        pe1 = target_stage1 - q_stage1[a1]
        lr1 = alpha_pos if pe1 >= 0 else alpha_neg
        
        # Eligibility trace update: 
        # The Stage 1 choice gets credit for the Stage 2 outcome scaled by lambda
        q_stage1[a1] += lr1 * pe1 * lambda_eligibility 
        
        # Standard TD(0) component for immediate transition value (usually 0 reward at stage 1)
        # Simplified here to just the eligibility trace backpropagation for the two-step task context

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```