Here are three cognitive models designed to explain the participant's behavior in the two-step task, incorporating the OCI-R score as a modulator of learning and decision-making parameters.

### Model 1: Hybrid Learner with OCI-Modulated Mixing
This model assumes that the participant uses a hybrid of Model-Based (MB) and Model-Free (MF) reinforcement learning. The core hypothesis is that the balance between these two systems (the mixing weight `w`) is influenced by the participant's obsessive-compulsive traits. Research often suggests compulsivity relates to habit formation (MF), so higher OCI might skew the weight towards MF or alter how the two are integrated. Here, we model `w` as a logistic function of the baseline mixing parameter and the OCI score.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free learner where the mixing weight (w) is modulated by OCI.
    
    This model posits that the trade-off between goal-directed (MB) and habitual (MF) 
    control is influenced by obsessive-compulsive traits.
    
    Parameters:
    lr (learning_rate): [0, 1] Learning rate for value updates.
    beta: [0, 10] Inverse temperature for softmax choice.
    w_base: [0, 1] Baseline mixing weight (0 = pure MF, 1 = pure MB).
    oci_sens: [0, 5] Sensitivity of the mixing weight to the OCI score.
    """
    lr, beta, w_base, oci_sens = model_parameters
    n_trials = len(action_1)
    # Extract scalar OCI
    oci_score = oci[0]
    
    # Calculate effective mixing weight w. 
    # We allow OCI to shift the baseline w. A simple linear shift clamped to [0,1].
    # Hypothesis: Higher OCI might reduce MB control (reduce w).
    w = w_base - (oci_sens * oci_score)
    # Clamp w to be between 0 and 1
    if w < 0: w = 0.0
    if w > 1: w = 1.0

    # Fixed transition matrix (A->X=0.7, U->Y=0.7)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)      # Model-free values for stage 1 (A vs U)
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for stage 2 (Planet X: W/S, Planet Y: P/H)

    for trial in range(n_trials):
        # --- STAGE 1 CHOICE ---
        # Model-Based value calculation: V(S2) = max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax policy Stage 1
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # --- STAGE 2 CHOICE ---
        state_idx = int(state[trial]) # 0 for Planet X, 1 for Planet Y
        
        # Softmax policy Stage 2 (purely model-free based on terminal rewards)
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- UPDATES ---
        # Prediction errors
        # Stage 1 MF update (TD(0)) - driven by value of state 2
        # Note: In standard 2-step, Stage 1 MF is often updated by Q(s2, a2).
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # SARSA-style update for Stage 1 MF
        # The value of the chosen second stage action acts as the target for the first stage
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        # Stage 2 MF update - driven by reward
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += lr * delta_stage2
        
        # Often in hybrid models, the MB system uses the MF Q-values at stage 2 as the estimates of state value.
        # This is implicitly handled because q_stage1_mb is calculated from q_stage2_mf.

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Driven Perseveration (Stickiness)
This model focuses on the "stickiness" or choice perseveration aspect of behavior. Compulsive symptoms are often characterized by repetitive behaviors or difficulty switching sets. Here, we model a pure Model-Free learner (often sufficient if MB is weak) but add a perseveration parameter `k` that is directly scaled by the OCI score. A higher OCI score leads to stronger repetition of the previous Stage 1 choice, regardless of reward history.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner with OCI-modulated choice perseveration (stickiness).
    
    This model tests if OCI scores predict the tendency to repeat the previous 
    Stage 1 choice (perseveration), independent of reward learning.
    
    Parameters:
    lr: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    persev_base: [-5, 5] Baseline perseveration bonus (can be negative for alternation).
    oci_persev_mod: [0, 5] How much OCI increases perseveration.
    """
    lr, beta, persev_base, oci_persev_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective perseveration parameter
    # Higher OCI -> Higher tendency to stick to previous choice
    k = persev_base + (oci_persev_mod * oci_score)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    last_action_1 = -1 # Indicator for previous action

    for trial in range(n_trials):
        # --- STAGE 1 CHOICE ---
        # Add stickiness bonus to the Q-values before softmax
        q_net = q_stage1.copy()
        if last_action_1 != -1:
            q_net[last_action_1] += k
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # Update last action tracker
        last_action_1 = int(action_1[trial])

        # --- STAGE 2 CHOICE ---
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # --- UPDATES (TD-0 / SARSA) ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Stage 1 update
        delta_1 = q_stage2[state_idx, a2] - q_stage1[a1]
        q_stage1[a1] += lr * delta_1
        
        # Stage 2 update
        delta_2 = r - q_stage2[state_idx, a2]
        q_stage2[state_idx, a2] += lr * delta_2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Reward Sensitivity (Learning Rate)
This model investigates if OCI affects the plasticity of value updating. It proposes that individuals with different OCI levels might update their beliefs faster or slower in response to prediction errors. Specifically, we model the learning rate `alpha` as a function of OCI. This captures the hypothesis that high OCI might be associated with rigid beliefs (low learning rate) or hyper-reactivity to failure (high learning rate).

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Based learner where the learning rate is modulated by OCI.
    
    This model assumes the participant is primarily Model-Based (aware of transitions),
    but their rate of updating values based on rewards (plasticity) depends on OCI.
    
    Parameters:
    lr_base: [0, 1] Baseline learning rate.
    beta: [0, 10] Inverse temperature.
    oci_lr_mod: [-1, 1] Modulation of learning rate by OCI.
    """
    lr_base, beta, oci_lr_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective learning rate
    # We use a logistic-like or clamped linear transformation to keep it in [0,1]
    # Here we use simple linear scaling with clamping
    lr = lr_base + (oci_lr_mod * oci_score)
    if lr < 0: lr = 0.01 # prevent 0 to allow some learning
    if lr > 1: lr = 1.0
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # In a pure MB model, we still track stage 2 values (terminal reward probabilities)
    # Stage 1 values are computed on the fly.
    q_stage2 = np.zeros((2, 2)) # Values of aliens

    for trial in range(n_trials):
        # --- STAGE 1 CHOICE (Model-Based) ---
        # V(state) = max_a Q(state, a)
        max_q_stage2 = np.max(q_stage2, axis=1)
        # Q_MB(action) = Sum(P(s'|a) * V(s'))
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # --- STAGE 2 CHOICE ---
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- UPDATE ---
        # Only Stage 2 values need updating in a pure MB model (Stage 1 is derived)
        a2 = int(action_2[trial])
        r = reward[trial]
        
        delta = r - q_stage2[state_idx, a2]
        q_stage2[state_idx, a2] += lr * delta

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```