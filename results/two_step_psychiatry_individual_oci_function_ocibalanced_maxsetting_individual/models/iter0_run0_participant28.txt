Here are the three cognitive models implemented as Python functions. They represent different hypotheses about how obsessive-compulsive symptoms (OCI) might influence decision-making processes in the two-step task.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free model where OCI modulates the mixing weight 'w'.
    
    This model hypothesizes that the balance between goal-directed (Model-Based) and 
    habitual (Model-Free) control is affected by the severity of obsessive-compulsive 
    symptoms. The parameter 'w' mixes the two value estimates.
    
    Bounds:
    learning_rate: [0,1]
    beta: [0,10]
    w_base: [0,1]
    w_oci_mod: [0,1]
    """
    learning_rate, beta, w_base, w_oci_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # OCI modulation of the mixing weight w.
    # We map w_oci_mod from [0,1] to a range [-1, 1] to allow OCI to either increase 
    # or decrease the weight of Model-Based control relative to the baseline.
    w_effect = (w_oci_mod - 0.5) * 2.0 * oci_score
    w = w_base + w_effect
    w = np.clip(w, 0.0, 1.0) # Ensure w remains a valid probability weight

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combine MB and MF values using the OCI-modulated weight
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # Updates (SARSA-like TD(0) for Stage 1)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Updates for Stage 2
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model with Choice Perseveration modulated by OCI.
    
    This model includes a 'stickiness' or perseveration parameter 'p' that biases 
    the participant to repeat their previous Stage 1 choice. This tendency is 
    hypothesized to be modulated by OCI score (e.g., higher compulsivity leading 
    to higher repetition).
    
    Bounds:
    learning_rate: [0,1]
    beta: [0,10]
    w: [0,1]
    p_base: [0,1] (Internal scale 0 to 5)
    p_oci_mod: [0,1] (Modulation strength)
    """
    learning_rate, beta, w, p_base, p_oci_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective perseveration p based on OCI
    # Base p is scaled to be potentially strong (0 to 5)
    p_val = p_base * 5.0
    # Modulation allows OCI to increase or decrease stickiness
    p_change = (p_oci_mod - 0.5) * 5.0 * oci_score
    p = p_val + p_change
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Calculate logits and add perseveration bonus
        logits = beta * q_net
        if last_action_1 != -1:
            logits[last_action_1] += p
            
        # Use stable softmax
        exp_q1 = np.exp(logits - np.max(logits))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # Updates
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model where OCI modulates the Learning Rate.
    
    This model tests the hypothesis that OCI affects the speed of value updating.
    For example, high OCI might be associated with rigidity (low learning rate)
    or hyper-sensitivity to errors (high learning rate).
    
    Bounds:
    lr_base: [0,1]
    lr_oci_mod: [0,1]
    beta: [0,10]
    w: [0,1]
    """
    lr_base, lr_oci_mod, beta, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective learning rate based on OCI
    lr_change = (lr_oci_mod - 0.5) * 2.0 * oci_score
    lr = lr_base + lr_change
    lr = np.clip(lr, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # Updates
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```