Here are the three cognitive models based on the participant's data and OCI score.

### Model 1: OCI-Modulated Model-Based Weight
This model hypothesizes that the participant's "medium" OCI score is associated with a deficit in Model-Based (goal-directed) control, leading to a greater reliance on Model-Free (habitual) learning. The mixing weight $w$ is scaled down by the OCI score. The model also includes a stickiness parameter to account for the participant's observed perseveration.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model where OCI modulates the balance between systems.
    Hypothesis: Higher OCI scores reduce the weight (w) of the Model-Based system,
    leading to more Model-Free behavior.
    
    Parameters:
    learning_rate: [0, 1] Learning rate for value updates.
    beta: [0, 10] Inverse temperature (softness of choice).
    w_param: [0, 1] Base mixing weight for Model-Based system (scaled by OCI).
    stickiness: [0, 5] Bonus for repeating the previous Stage 1 choice.
    lambda_decay: [0, 1] Eligibility trace decay (0=TD, 1=Monte Carlo).
    """
    learning_rate, beta, w_param, stickiness, lambda_decay = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # OCI Modulation: Higher OCI -> Lower w (less Model-Based)
    w = w_param * (1.0 - oci_score)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_a1 = -1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        state_idx = int(state[trial])
        a2 = int(action_2[trial])
        
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combine MB and MF values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add Stickiness
        if prev_a1 != -1:
            q_net[prev_a1] += stickiness
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Eligibility Trace: Update Stage 1 based on Stage 2 outcome
        q_stage1_mf[a1] += learning_rate * lambda_decay * delta_stage2
        
        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Stickiness
This model hypothesizes that the participant's OCI score relates to compulsivity, manifesting as increased "stickiness" or perseveration (repeating the same choice regardless of outcome). The stickiness parameter is amplified by the OCI score.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model where OCI modulates choice stickiness.
    Hypothesis: Higher OCI scores increase perseveration (stickiness),
    reflecting compulsive repetition.
    
    Parameters:
    learning_rate: [0, 1] Learning rate for value updates.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Mixing weight (0=MF, 1=MB).
    stick_base: [0, 5] Base stickiness parameter (scaled up by OCI).
    lambda_decay: [0, 1] Eligibility trace decay.
    """
    learning_rate, beta, w, stick_base, lambda_decay = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # OCI Modulation: Higher OCI -> Higher Stickiness
    stickiness = stick_base * (1.0 + oci_score)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_a1 = -1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        state_idx = int(state[trial])
        a2 = int(action_2[trial])

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if prev_a1 != -1:
            q_net[prev_a1] += stickiness
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        q_stage1_mf[a1] += learning_rate * lambda_decay * delta_stage2
        
        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Choice Rigidity (Beta)
This model hypothesizes that OCI is associated with rigidity or anxiety-driven exploitation. Higher OCI scores increase the inverse temperature parameter ($\beta$), making choices more deterministic (sharper probabilities) and less exploratory.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model where OCI modulates the inverse temperature (beta).
    Hypothesis: Higher OCI scores lead to more rigid/deterministic choices
    (higher beta), reducing random exploration.
    
    Parameters:
    learning_rate: [0, 1] Learning rate.
    beta_base: [0, 10] Base inverse temperature (scaled up by OCI).
    w: [0, 1] Mixing weight.
    stickiness: [0, 5] Choice perseveration bonus.
    lambda_decay: [0, 1] Eligibility trace decay.
    """
    learning_rate, beta_base, w, stickiness, lambda_decay = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # OCI Modulation: Higher OCI -> Higher Beta (more rigid)
    beta = beta_base * (1.0 + oci_score)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_a1 = -1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        state_idx = int(state[trial])
        a2 = int(action_2[trial])

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if prev_a1 != -1:
            q_net[prev_a1] += stickiness
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        q_stage1_mf[a1] += learning_rate * lambda_decay * delta_stage2
        
        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```