Here are three cognitive models that incorporate the OCI (Obsessive-Compulsive Inventory) score into the decision-making process. The participant has a very high OCI score (0.93), suggesting high compulsivity.

### Model 1: OCI-Modulated Model-Based/Model-Free Balance
This model hypothesizes that high OCI scores relate to a deficit in goal-directed (Model-Based) control and an over-reliance on habitual (Model-Free) control. The mixing parameter `w` (which balances MB and MF systems) is directly modulated by the OCI score. A higher OCI reduces the influence of the Model-Based system.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free Reinforcement Learning where the balance (w)
    between systems is modulated by the OCI score.
    
    Hypothesis: Higher OCI scores lead to reduced Model-Based control (lower w).
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    w_base: [0, 1] - Baseline mixing parameter
    w_oci_slope: [0, 1] - Sensitivity to OCI score reducing MB control
    """
    learning_rate, beta, w_base, w_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective mixing weight w. 
    # We clip it to [0, 1]. Higher OCI reduces w (less Model-Based).
    w = w_base - (w_oci_slope * oci_score)
    w = np.clip(w, 0.0, 1.0)
    
    # Transition matrix (fixed for this task structure)
    # A -> X (0.7), A -> Y (0.3); U -> Y (0.7), U -> X (0.3)
    # Indices: [Action, State] -> [0,0] is A->X, [0,1] is A->Y, etc.
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)      # Model-Free Stage 1 values
    q_stage2_mf = np.zeros((2, 2)) # Model-Free Stage 2 values (State x Action)
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        # Model-Based valuation: V_MB(s) = max_a Q_MF(s, a)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        # Bellman equation using known transition structure
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid valuation
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax choice 1
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        # Standard Model-Free choice at Stage 2
        exp_q2 = np.exp(beta * q_stage2_mf[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        # Stage 2 RPE
        delta_stage2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += learning_rate * delta_stage2
        
        # Stage 1 RPE (TD(0))
        # Note: In standard hybrid models, MF stage 1 is updated via TD error 
        # using the value of the state actually reached.
        # V(s') = max(Q(s', a'))
        v_next_state = np.max(q_stage2_mf[s]) 
        delta_stage1 = v_next_state - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Based Stickiness (Perseveration)
This model posits that high OCI scores manifest as behavioral rigidity or "stickiness." Regardless of reward outcomes, high OCI participants are more likely to repeat their previous Stage 1 choice. The OCI score scales a stickiness parameter added to the softmax function.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free Reinforcement Learning with Choice Stickiness modulated by OCI.
    
    Hypothesis: High OCI participants exhibit higher choice perseveration (stickiness),
    repeating the previous action regardless of the outcome.
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    stickiness_base: [0, 5] - General tendency to repeat choices
    stickiness_oci: [0, 5] - Additional stickiness driven by OCI score
    """
    learning_rate, beta, stickiness_base, stickiness_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective stickiness
    eff_stickiness = stickiness_base + (stickiness_oci * oci_score)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    # Track previous choice for stickiness (initialize with -1 for no previous choice)
    prev_a1 = -1 

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        # Calculate logits: Q-value + Stickiness bonus
        logits = beta * q_stage1.copy()
        
        if prev_a1 != -1:
            logits[prev_a1] += eff_stickiness
            
        exp_q1 = np.exp(logits)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        # SARSA-like update for Stage 1 (using Q of stage 2 choice)
        # This is a pure Model-Free approach
        delta_stage1 = q_stage2[s, a2] - q_stage1[a1]
        q_stage1[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += learning_rate * delta_stage2
        
        # Update previous action
        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Second-Stage Learning Rate
This model suggests that OCI affects how quickly participants update their values based on immediate rewards. High compulsivity might relate to over-sensitivity to immediate feedback (or conversely, rigidity). Here, we model the learning rate at the second stage (where the reward is received) as a function of the OCI score.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Pure Model-Based RL where the learning rate for the second stage (reward stage)
    is modulated by the OCI score.
    
    Hypothesis: OCI affects the plasticity of value updating in response to direct rewards.
    
    Bounds:
    alpha_base: [0, 1] - Base learning rate
    alpha_oci_mod: [0, 1] - Modulation factor. Can increase or decrease alpha based on OCI.
                            We implement it as alpha = alpha_base * (1 + alpha_oci_mod * OCI)
                            normalized to stay within bounds.
    beta: [0, 10]
    """
    alpha_base, alpha_oci_mod, beta = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective learning rate
    # We allow the OCI score to scale the learning rate up.
    # We clip at 1.0 to ensure stability.
    learning_rate = alpha_base * (1 + alpha_oci_mod * oci_score)
    learning_rate = np.clip(learning_rate, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Only need Stage 2 Q-values for a Pure Model-Based agent
    # The agent calculates Stage 1 values on the fly using the transition matrix
    q_stage2 = np.zeros((2, 2)) 

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy (Pure Model-Based) ---
        # V_MB(s) = max_a Q(s, a)
        max_q_stage2 = np.max(q_stage2, axis=1)
        # Q_MB(a) = sum P(s|a) * V_MB(s)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        # Only update Stage 2 values based on reward
        delta_stage2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += learning_rate * delta_stage2
        
        # No Stage 1 learning required for pure MB; it is derived analytically

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```