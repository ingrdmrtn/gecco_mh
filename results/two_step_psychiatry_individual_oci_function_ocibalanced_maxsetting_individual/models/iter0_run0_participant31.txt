Here are three cognitive models designed to capture the behavior of the participant, particularly focusing on how their high OCI-R score (0.733, indicating high obsessive-compulsive symptoms) might influence their learning and decision-making.

### Model 1: The "Habitual Reliance" Model
**Hypothesis:** High OCI scores are often associated with a reliance on habit-based (Model-Free) learning over goal-directed (Model-Based) planning. This model posits that the mixing weight `w` (balancing MB and MF control) is directly modulated by the OCI score. A higher OCI reduces `w`, pushing the agent towards pure model-free behavior.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated MB/MF Balance.
    
    This model assumes that the balance between Model-Based (MB) and Model-Free (MF) 
    control is influenced by the participant's OCI score. High OCI is hypothesized 
    to reduce goal-directed (MB) control, favoring habitual (MF) responding.
    
    The mixing weight `w` is calculated as: w = w_base * (1 - oci)
    
    Parameters:
    learning_rate: [0, 1] - Rate at which Q-values are updated.
    beta: [0, 10] - Inverse temperature for softmax choice.
    w_base: [0, 1] - Baseline mixing weight (before OCI modulation).
    """
    learning_rate, beta, w_base = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0] 
    
    # OCI modulation: Higher OCI reduces 'w', reducing MB influence.
    w = w_base * (1.0 - current_oci)
    
    # Transition matrix (fixed for this task structure)
    # A->X (0.7), A->Y (0.3), U->X (0.3), U->Y (0.7)
    # Mapping: A=0, U=1; X=0, Y=1
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Initialize Q-values
    q_stage1_mf = np.zeros(2)      # Model-free values for stage 1 (A vs U)
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for stage 2 (Aliens at Planet X/Y)

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        
        # Model-Based calculation: V(S2) * Transition
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Q-value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax policy Stage 1
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        # Record probability of the chosen action
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Decision ---
        
        s_idx = int(state[trial]) # 0 for X, 1 for Y
        
        # Standard Q-learning for Stage 2
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning Updates ---
        
        r = reward[trial]
        
        # Prediction errors
        # Stage 2 PE: Reward - Expectation at Stage 2
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        # Stage 1 PE: Value of state reached - Expectation at Stage 1
        # Note: Using SARSA-style update (value of chosen S2 action) for MF chain
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        
        # Update Q-values
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: The "Stickiness" Model
**Hypothesis:** Individuals with high OCI often exhibit repetitive behaviors or "stickiness" (perseveration), regardless of reward outcomes. This model introduces a choice autocorrelation parameter (`stickiness`) that is amplified by the OCI score. The higher the OCI, the more the previous choice biases the current choice.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Amplified Perseveration (Stickiness).
    
    This model implements a standard Model-Free Q-learner (SARSA) but adds a 
    'stickiness' bonus to the previously chosen action at Stage 1. 
    The magnitude of this stickiness is scaled by the OCI score.
    
    Stickiness bonus = stick_param * (1 + oci)
    
    Parameters:
    learning_rate: [0, 1] - Learning rate.
    beta: [0, 10] - Inverse temperature.
    stick_param: [0, 5] - Base magnitude of choice perseveration.
    """
    learning_rate, beta, stick_param = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # OCI amplifies the tendency to repeat the previous choice
    effective_stickiness = stick_param * (1.0 + current_oci)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1 # Indicator for no previous action

    for trial in range(n_trials):
        
        # --- Stage 1 Decision ---
        
        # Calculate logits (Q-values + stickiness bonus)
        logits_1 = q_stage1_mf.copy()
        
        if last_action_1 != -1:
            logits_1[last_action_1] += effective_stickiness
            
        exp_q1 = np.exp(beta * logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        last_action_1 = a1 # Update for next trial
        
        # --- Stage 2 Decision ---
        
        s_idx = int(state[trial])
        
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning Updates ---
        
        r = reward[trial]
        
        # TD Errors (Model-Free SARSA)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: The "Punishment Sensitivity" Model
**Hypothesis:** High OCI is sometimes linked to an over-sensitivity to negative outcomes (or lack of reward) rather than positive ones. This model splits the learning rate into two components: one for positive prediction errors (`lr_pos`) and one for negative prediction errors (`lr_neg`). The OCI score modulates the negative learning rate, making the agent update values more drastically when expectations are not met (0 coins).

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Enhanced Negative Learning Rate.
    
    This model assumes separate learning rates for positive and negative prediction errors.
    The OCI score specifically scales the learning rate for negative prediction errors 
    (disappointment), reflecting a heightened sensitivity to failure or loss.
    
    lr_neg_effective = lr_neg_base * (1 + oci) (clipped at 1.0)
    
    Parameters:
    lr_pos: [0, 1] - Learning rate for positive prediction errors (R > Q).
    lr_neg_base: [0, 1] - Base learning rate for negative prediction errors (R < Q).
    beta: [0, 10] - Inverse temperature.
    """
    lr_pos, lr_neg_base, beta = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # OCI increases sensitivity to negative outcomes
    lr_neg_effective = min(1.0, lr_neg_base * (1.0 + current_oci))
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Pure Model-Based Agent logic for Stage 1 value estimation, 
    # but with split learning rates at Stage 2
    q_stage2_mf = np.zeros((2, 2)) 

    for trial in range(n_trials):
        
        # --- Stage 1 Decision (Pure Model-Based for this hypothesis) ---
        
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Decision ---
        
        s_idx = int(state[trial])
        
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning Updates ---
        
        r = reward[trial]
        
        # Only Stage 2 values are learned directly here to feed into MB planner
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        if delta_stage2 >= 0:
            q_stage2_mf[s_idx, a2] += lr_pos * delta_stage2
        else:
            q_stage2_mf[s_idx, a2] += lr_neg_effective * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```