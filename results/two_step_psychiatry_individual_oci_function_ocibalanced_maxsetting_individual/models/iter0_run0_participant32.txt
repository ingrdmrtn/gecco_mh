Here are three cognitive models designed to explain the participant's behavior, incorporating their high OCI-R score (0.833).

### Model 1: Hybrid MB/MF with OCI-modulated Model-Based Weighting
This model hypothesizes that high OCI scores correlate with a rigid adherence to a Model-Based (planning) strategy, potentially at the expense of flexible Model-Free learning. The `w` parameter (mixing weight) is modulated by the OCI score, where higher OCI pushes the agent towards pure model-based planning (calculating expected value based on transition probabilities) rather than simple habit formation.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free Reinforcement Learning.
    The mixing weight 'w' is modulated by OCI, where higher OCI leads to 
    stronger Model-Based control (planning).
    
    Bounds:
    learning_rate: [0,1]
    beta: [0,10]
    w_base: [0,1]
    w_oci_scale: [0,1]
    """
    learning_rate, beta, w_base, w_oci_scale = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate mixing weight w based on OCI. 
    # High OCI increases w (more Model-Based). 
    # We clip to ensure it stays valid [0, 1].
    w = w_base + (w_oci_scale * oci_score)
    if w > 1.0: w = 1.0
    if w < 0.0: w = 0.0

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)      # Model-free values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for stage 2 (Aliens)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy (Hybrid) ---
        # Model-Based Value: Transition * max(Stage 2 Values)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Value
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Softmax for Choice 1
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        # Standard softmax on Stage 2 Q-values
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning Updates ---
        # Stage 2 update (TD)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 update (TD)
        # Note: In standard hybrid models, MF updates via TD(1) or TD(0). 
        # Here we use TD(0) based on the state reached.
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Perseveration Model with OCI-Driven Stickiness
This model posits that high OCI scores relate to "stickiness" or perseverationâ€”a tendency to repeat the previous action regardless of the reward outcome. This captures the repetitive behavior often seen in compulsive symptomatology. The `stickiness` parameter is directly scaled by the OCI score.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free RL with OCI-modulated Perseveration (Stickiness).
    Higher OCI leads to higher stickiness (tendency to repeat previous choice).
    
    Bounds:
    learning_rate: [0,1]
    beta: [0,10]
    stickiness_base: [0,5]
    stickiness_oci: [0,5]
    """
    learning_rate, beta, stickiness_base, stickiness_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Stickiness increases with OCI
    stickiness = stickiness_base + (stickiness_oci * oci_score)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    last_action_1 = -1 # Indicator for previous action

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        # Add stickiness bonus to the previously chosen action
        q_net_1 = np.copy(q_stage1)
        if last_action_1 != -1:
            q_net_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Update tracker for next trial
        last_action_1 = a1

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning Updates ---
        # SARSA-style / TD updates
        delta_stage2 = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += learning_rate * delta_stage2
        
        # Simple TD(0) update for stage 1 based on value of stage 2 state
        # Using the max of the next state (Q-learning style)
        delta_stage1 = np.max(q_stage2[s_idx]) - q_stage1[a1]
        q_stage1[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Separate Learning Rates for Positive/Negative Prediction Errors (OCI-modulated)
This model investigates if OCI affects how participants learn from positive versus negative outcomes. High OCI might be associated with hypersensitivity to negative outcomes (avoidance) or positive outcomes (checking). Here, the OCI score shifts the balance between `alpha_pos` (learning from reward) and `alpha_neg` (learning from lack of reward).

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free RL with Asymmetric Learning Rates modulated by OCI.
    OCI influences the ratio between learning from positive vs negative prediction errors.
    
    Bounds:
    alpha_base: [0,1]
    alpha_bias_oci: [0,1]
    beta: [0,10]
    """
    alpha_base, alpha_bias_oci, beta = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # We define two learning rates.
    # High OCI increases sensitivity to negative prediction errors (alpha_neg)
    # and decreases sensitivity to positive ones, or vice versa depending on fit.
    # Here we model it such that OCI creates a split from the base rate.
    
    # Ensure bounds [0,1]
    alpha_pos = alpha_base - (alpha_bias_oci * oci_score * 0.5)
    alpha_neg = alpha_base + (alpha_bias_oci * oci_score * 0.5)
    
    alpha_pos = np.clip(alpha_pos, 0.001, 0.999)
    alpha_neg = np.clip(alpha_neg, 0.001, 0.999)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning Updates ---
        
        # Stage 2 Update
        delta_stage2 = r - q_stage2[s_idx, a2]
        if delta_stage2 >= 0:
            q_stage2[s_idx, a2] += alpha_pos * delta_stage2
        else:
            q_stage2[s_idx, a2] += alpha_neg * delta_stage2
            
        # Stage 1 Update
        # Using the value of the chosen stage 2 state
        val_stage2 = np.max(q_stage2[s_idx])
        delta_stage1 = val_stage2 - q_stage1[a1]
        
        if delta_stage1 >= 0:
            q_stage1[a1] += alpha_pos * delta_stage1
        else:
            q_stage1[a1] += alpha_neg * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```