Here are three cognitive models formulated as Python functions. They incorporate the Obsessive-Compulsive Inventory (OCI) score to modulate specific cognitive parameters, hypothesizing different mechanisms for how obsessive-compulsive traits influence decision-making (e.g., through compulsive stickiness, reduced model-based planning, or hypersensitivity to negative outcomes).

### Model 1: OCI-Modulated Compulsive Stickiness
This model hypothesizes that high OCI scores lead to "compulsive" repetition of choices (perseveration). It implements a standard hybrid learner (mixing Model-Based and Model-Free values) but adds a "stickiness" bonus to the previously chosen spaceship. The magnitude of this stickiness is directly scaled by the participant's OCI score, predicting that the high-scoring participant (0.83) will exhibit significant inertia in their Stage 1 choices.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model with OCI-modulated Choice Stickiness.
    
    Hypothesis: Participants with high OCI scores exhibit "compulsive" perseveration.
    The previous choice receives a value bonus proportional to the OCI score.
    
    Parameters:
    - learning_rate: [0, 1] Rate of updating Q-values.
    - beta: [0, 10] Inverse temperature for softmax (exploration/exploitation).
    - w: [0, 1] Weight of Model-Based values (0=Pure MF, 1=Pure MB).
    - stickiness_sensitivity: [0, 5] How strongly OCI scales the tendency to repeat choices.
    """
    learning_rate, beta, w, stickiness_sensitivity = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Transition matrix for Model-Based planning (A->X, U->Y mostly)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)      # Model-Free values for Stage 1 (Spaceships)
    q_stage2_mf = np.zeros((2, 2)) # Model-Free values for Stage 2 (Aliens per Planet)
    
    # Track previous choice for stickiness
    last_action_1 = -1 

    for trial in range(n_trials):
        # --- STAGE 1 POLICY ---
        # 1. Calculate Model-Based values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Calculate Net Value (Hybrid)
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # 3. Add OCI-modulated Stickiness
        # If OCI is high, the bonus for repeating the last action is larger.
        stickiness_bonus = np.zeros(2)
        if last_action_1 != -1:
            stickiness_bonus[int(last_action_1)] = stickiness_sensitivity * oci_score
        
        # 4. Softmax
        exp_q1 = np.exp(beta * (q_net + stickiness_bonus))
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # Record state and action for updates
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- STAGE 2 POLICY ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- UPDATING ---
        # Update Stage 1 MF values (SARSA-like update using Stage 2 Q-value)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Update Stage 2 MF values (Reward Prediction Error)
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Update history
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Impaired Model-Based Control
This model hypothesizes that obsessive-compulsive symptoms interfere with goal-directed (Model-Based) planning, leading to a reliance on habitual (Model-Free) control. The mixing weight parameter $w$ (which balances MB and MF systems) is not free but is constrained by the OCI score. Specifically, higher OCI reduces the effective weight of the Model-Based system.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model where OCI score inhibits Model-Based control.
    
    Hypothesis: High OCI scores correlate with deficits in goal-directed planning.
    The effective mixing weight (w_eff) is reduced as OCI increases.
    
    Parameters:
    - learning_rate: [0, 1] Rate of updating Q-values.
    - beta: [0, 10] Inverse temperature.
    - w_base: [0, 1] Baseline Model-Based weight for a participant with 0 OCI.
    """
    learning_rate, beta, w_base = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    # Calculate effective weight based on OCI
    # If OCI is 0, w_eff = w_base.
    # If OCI is 1, w_eff = 0 (Purely Model-Free/Habitual).
    w_eff = w_base * (1.0 - oci_score)

    for trial in range(n_trials):
        # --- STAGE 1 POLICY ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combine using the OCI-attenuated weight
        q_net = (w_eff * q_stage1_mb) + ((1 - w_eff) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- STAGE 2 POLICY ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # --- UPDATING ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Enhanced Punishment Sensitivity
This model hypothesizes that high OCI scores are associated with anxiety and an over-sensitivity to failure (lack of reward). In this model, the learning rate is asymmetric. When the prediction error is negative (expected a reward but got none), the learning rate is boosted by the OCI score. This explains rapid switching behavior after non-rewarded trials.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model with OCI-modulated Punishment Sensitivity.
    
    Hypothesis: High OCI leads to faster unlearning (higher learning rate) 
    when outcomes are worse than expected (Negative Prediction Error).
    
    Parameters:
    - lr_base: [0, 1] Baseline learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight (static).
    - punish_mult: [0, 5] Multiplier for how much OCI boosts LR during negative errors.
    """
    lr_base, beta, w, punish_mult = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- STAGE 1 POLICY ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- STAGE 2 POLICY ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # --- UPDATING ---
        # Determine effective learning rate based on Prediction Error sign
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        # If delta is negative (disappointment), boost LR by OCI factor
        lr_1 = lr_base
        if delta_stage1 < 0:
            lr_1 = min(1.0, lr_base * (1.0 + punish_mult * oci_score))
            
        q_stage1_mf[a1] += lr_1 * delta_stage1
        
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        # If delta is negative (no reward when expected), boost LR by OCI factor
        lr_2 = lr_base
        if delta_stage2 < 0:
            lr_2 = min(1.0, lr_base * (1.0 + punish_mult * oci_score))
            
        q_stage2_mf[state_idx, a2] += lr_2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```