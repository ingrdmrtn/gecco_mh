Here are three cognitive models formulated as Python functions. They are designed to analyze the two-step decision-making task, specifically incorporating the Obsessive-Compulsive Inventory (OCI) score to explain individual variability in model-based/model-free balance, perseveration (stickiness), and learning rigidity.

### Model 1: OCI-Modulated Hybrid Model
This model hypothesizes that the balance between Model-Based (goal-directed) and Model-Free (habitual) control is influenced by the participant's OCI score. High OCI scores are often associated with deficits in goal-directed control or over-reliance on habits. Here, the mixing weight `w` is dynamically calculated based on the OCI score.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Hybrid Model.
    
    This model assumes the participant uses a hybrid of Model-Based (MB) and Model-Free (MF) 
    strategies. The mixing weight 'w' (where 1 is fully MB and 0 is fully MF) is not fixed 
    but is a function of the participant's OCI score. This allows the model to test if 
    obsessive-compulsive symptoms shift the strategy towards habit formation (MF).
    
    Parameters:
    - learning_rate: [0,1] Learning rate for value updates.
    - beta: [0,10] Inverse temperature for softmax (exploration/exploitation).
    - w_base: [0,1] Baseline mixing weight.
    - w_oci_shift: [0,1] Parameter determining how OCI shifts 'w'. 
                   Values < 0.5 imply OCI reduces MB usage; > 0.5 imply OCI increases it.
    - lambd: [0,1] Eligibility trace parameter connecting stage 2 outcome to stage 1 choice.
    """
    learning_rate, beta, w_base, w_oci_shift, lambd = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate the effective mixing weight w based on OCI
    # Map w_oci_shift from [0, 1] to [-1, 1] to allow directional modulation
    shift_magnitude = (w_oci_shift - 0.5) * 2.0
    w = w_base + shift_magnitude * oci_score
    w = np.clip(w, 0.0, 1.0)
    
    # Transition matrix (fixed for this task structure)
    # Spaceship A (0) -> Planet X (0) w/ 0.7
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Initialize Q-values
    q_stage1_mf = np.zeros(2)       # Model-Free values for Stage 1 (Spaceships)
    q_stage2_mf = np.zeros((2, 2))  # Model-Free values for Stage 2 (Planets -> Aliens)
    
    log_loss = 0.0
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Skip missing data
        if a1 < 0 or s2 < 0 or a2 < 0 or r < 0:
            continue
            
        # --- Stage 1 Policy ---
        # Model-Based Value: Expected max value of next stage based on transition prob
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Value (Hybrid)
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax Choice Probability
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / (np.sum(exp_q1) + 1e-10)
        log_loss -= np.log(probs_1[a1] + 1e-10)
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s2])
        probs_2 = exp_q2 / (np.sum(exp_q2) + 1e-10)
        log_loss -= np.log(probs_2[a2] + 1e-10)
        
        # --- Value Updates ---
        # 1. Stage 1 MF Update (TD(0) prediction error from Stage 2 transition)
        delta_1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_1
        
        # 2. Stage 2 MF Update (Reward prediction error)
        delta_2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += learning_rate * delta_2
        
        # 3. Eligibility Trace (Update Stage 1 value based on Stage 2 reward)
        q_stage1_mf[a1] += learning_rate * lambd * delta_2
        
    return log_loss
```

### Model 2: OCI-Modulated Stickiness Model
This model focuses on "stickiness" or perseverationâ€”the tendency to repeat the previous choice regardless of reward. High OCI scores are hypothesized to correlate with repetitive behaviors or compulsions. Here, the OCI score scales the stickiness bonus added to the previously chosen action.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Stickiness Model.
    
    This model incorporates a choice perseveration ('stickiness') parameter that is modulated 
    by the OCI score. It tests the hypothesis that participants with higher obsessive-compulsive 
    symptoms are more prone to repetitively choosing the same spaceship, regardless of the 
    outcome (reward or transition).
    
    Parameters:
    - learning_rate: [0,1] Learning rate.
    - beta: [0,10] Inverse temperature.
    - w: [0,1] Fixed mixing weight for MB/MF balance.
    - stick_base: [0,1] Baseline stickiness independent of OCI.
    - stick_oci: [0,1] Additional stickiness scaling factor applied to the OCI score.
    - lambd: [0,1] Eligibility trace.
    """
    learning_rate, beta, w, stick_base, stick_oci, lambd = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate stickiness bonus
    # We scale by 5.0 to allow the optimizer to reach high stickiness values within [0,1] param bounds
    stickiness_bonus = (stick_base + stick_oci * oci_score) * 5.0
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    log_loss = 0.0
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        if a1 < 0 or s2 < 0 or a2 < 0 or r < 0:
            last_action_1 = -1 # Reset memory on missing data
            continue
            
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Calculate logits (values before softmax)
        logits = beta * q_net
        
        # Add stickiness bonus to the previously chosen action
        if last_action_1 != -1:
            logits[last_action_1] += stickiness_bonus
            
        exp_q1 = np.exp(logits)
        probs_1 = exp_q1 / (np.sum(exp_q1) + 1e-10)
        log_loss -= np.log(probs_1[a1] + 1e-10)
        
        # Update history
        last_action_1 = a1
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s2])
        probs_2 = exp_q2 / (np.sum(exp_q2) + 1e-10)
        log_loss -= np.log(probs_2[a2] + 1e-10)
        
        # --- Value Updates ---
        delta_1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_1
        
        delta_2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += learning_rate * delta_2
        
        q_stage1_mf[a1] += learning_rate * lambd * delta_2
        
    return log_loss
```

### Model 3: OCI-Modulated Learning Rigidity Model
This model hypothesizes that OCI relates to the flexibility of belief updating. High OCI might be associated with "cognitive rigidity" (lower learning rate) or hyper-correction (higher learning rate). The learning rate $\alpha$ is modulated by the OCI score.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Learning Rigidity Model.
    
    This model proposes that the Obsessive-Compulsive trait influences the rate at which 
    participants update their value estimates (learning rate). A linear relationship is 
    modeled where the base learning rate is shifted up or down based on the OCI score.
    
    Parameters:
    - lr_base: [0,1] Baseline learning rate.
    - lr_oci_mod: [0,1] Modifier for learning rate based on OCI. 
                  (Mapped internally to shift the rate positive or negative).
    - beta: [0,10] Inverse temperature.
    - w: [0,1] Mixing weight.
    - lambd: [0,1] Eligibility trace.
    """
    lr_base, lr_oci_mod, beta, w, lambd = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective learning rate
    # Map lr_oci_mod from [0, 1] to [-0.5, 0.5]
    # This allows OCI to either dampen (rigidity) or accelerate learning
    lr_shift = lr_oci_mod - 0.5
    learning_rate = lr_base + lr_shift * oci_score
    learning_rate = np.clip(learning_rate, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    log_loss = 0.0
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        if a1 < 0 or s2 < 0 or a2 < 0 or r < 0:
            continue
            
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / (np.sum(exp_q1) + 1e-10)
        log_loss -= np.log(probs_1[a1] + 1e-10)
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s2])
        probs_2 = exp_q2 / (np.sum(exp_q2) + 1e-10)
        log_loss -= np.log(probs_2[a2] + 1e-10)
        
        # --- Value Updates (Using OCI-modulated learning rate) ---
        delta_1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_1
        
        delta_2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += learning_rate * delta_2
        
        q_stage1_mf[a1] += learning_rate * lambd * delta_2
        
    return log_loss
```