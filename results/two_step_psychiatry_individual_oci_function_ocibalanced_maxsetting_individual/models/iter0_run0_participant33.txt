Here are three cognitive models designed to capture the behavior of the participant, specifically integrating their high OCI-R score into the decision-making process.

### Analysis of Participant Data
The participant has an OCI-R score of ~0.72, which is classified as **high obsessive-compulsive symptoms**.
*   **Behavioral Observation:** Looking at the trial data, the participant shows a strong tendency to stick with one option (Spaceship 0) for long streaks, even when rewards are intermittent. Later in the task (around trial 160+), they switch to Spaceship 1 and stick with it.
*   **Hypothesis:** High OCI is often associated with habit formation or a reliance on Model-Free (habitual) learning over Model-Based (goal-directed) planning. They might also exhibit "stickiness" (perseveration) regardless of reward history.

### Model 1: OCI-Modulated Model-Based/Model-Free Hybrid
This model assumes that the balance between Model-Based (MB) and Model-Free (MF) control is directly influenced by the OCI score. Higher OCI scores shift the weight towards Model-Free (habitual) control.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free Reinforcement Learning with OCI-modulated mixing.
    
    Hypothesis: High OCI scores bias the agent towards Model-Free (habitual) control 
    and away from Model-Based (goal-directed) planning.
    
    Parameters:
    learning_rate: [0, 1] - Rate at which Q-values are updated.
    beta: [0, 10] - Inverse temperature for softmax action selection.
    w_base: [0, 1] - Baseline weighting for Model-Based control (0 = pure MF, 1 = pure MB).
    """
    learning_rate, beta, w_base = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # OCI modulation: High OCI reduces the effective weight of Model-Based control.
    # If OCI is high (near 1), w becomes smaller (more MF).
    # If OCI is low (near 0), w stays closer to w_base.
    w = w_base * (1.0 - oci_score) 
    
    # Transition matrix (fixed structure of the task)
    # 0 -> 0 (70%), 0 -> 1 (30%)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)      # MF values for stage 1 options
    q_stage2_mf = np.zeros((2, 2)) # MF values for stage 2 (state, action) pairs

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        if a1 == -1: # Skip missing trials
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # --- Stage 1 Policy ---
        # Calculate Model-Based values: T * max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Q-value: weighted sum of MB and MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        # Standard Softmax on Stage 2 Q-values
        exp_q2 = np.exp(beta * q_stage2_mf[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning Updates ---
        # Stage 2 Update (TD)
        delta_stage2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += learning_rate * delta_stage2
        
        # Stage 1 Update (TD using Stage 2 value as proxy for reward)
        # Note: In pure MF, we update Q1 based on Q2 of the state actually reached
        delta_stage1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Driven Perseveration (Stickiness)
This model posits that high OCI leads to "stickiness" or perseveration. The participant is more likely to repeat their previous Stage 1 choice regardless of the outcome, and the strength of this repetition bias is scaled by their OCI score.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Reinforcement Learning with OCI-modulated Choice Perseveration.
    
    Hypothesis: High OCI scores increase the 'stickiness' parameter, causing the 
    agent to repeat the previous Stage 1 action regardless of reward history.
    
    Parameters:
    learning_rate: [0, 1] - Update rate for Q-values.
    beta: [0, 10] - Inverse temperature.
    pers_base: [0, 5] - Base perseveration bonus added to the previously chosen action.
    """
    learning_rate, beta, pers_base = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # OCI modulation: Effective perseveration increases with OCI score.
    # A high OCI score amplifies the tendency to repeat actions.
    w_pers = pers_base * (1.0 + oci_score)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    last_action_1 = -1 # Initialize variable to track previous choice

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        if a1 == -1: 
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # --- Stage 1 Policy ---
        # Add perseveration bonus to Q-values before softmax
        q_effective = q_stage1.copy()
        if last_action_1 != -1:
            q_effective[last_action_1] += w_pers
            
        exp_q1 = np.exp(beta * q_effective)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning Updates ---
        # Standard SARSA/Q-learning approach
        delta_stage2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += learning_rate * delta_stage2
        
        # Update Stage 1 based on the value of the state reached (Model-Free)
        delta_stage1 = q_stage2[s, a2] - q_stage1[a1]
        q_stage1[a1] += learning_rate * delta_stage1
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Reward Sensitivity (Anxiety/Avoidance)
This model suggests that high OCI (often comorbid with anxiety) might alter how rewards and punishments (lack of reward) are perceived. Specifically, high OCI might make the agent less sensitive to positive rewards (anhedonia or focus on error avoidance) or overly sensitive to prediction errors, effectively scaling the learning rate based on OCI.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free RL with OCI-modulated Learning Rate.
    
    Hypothesis: OCI acts as a modulator on learning speed. High OCI might result in 
    rigid beliefs (low learning rate) or hyper-correction (high learning rate). 
    Here we model it such that high OCI dampens the learning rate, leading to 
    rigidity/habit.
    
    Parameters:
    alpha_base: [0, 1] - Baseline learning rate.
    beta: [0, 10] - Inverse temperature.
    eligibility: [0, 1] - Eligibility trace (lambda) connecting Stage 2 outcome to Stage 1 choice.
    """
    alpha_base, beta, eligibility = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # OCI modulation: High OCI reduces the effective learning rate.
    # This simulates "rigidity" - the agent is slow to update values based on new evidence.
    # We clip to ensure it stays positive.
    effective_alpha = alpha_base * (1.0 - (0.5 * oci_score))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        if a1 == -1: 
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # --- Stage 1 Policy ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning Updates ---
        # Stage 2 Prediction Error
        delta_2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += effective_alpha * delta_2
        
        # Stage 1 Update: 
        # We use an eligibility trace approach (lambda) to update Stage 1 based on Stage 2 outcome.
        # This is a pure Model-Free update mechanism.
        # The update is a mix of the immediate transition value and the final reward prediction error.
        
        # 1. Update based on transition (TD(0) for stage 1)
        delta_1 = q_stage2[s, a2] - q_stage1[a1]
        q_stage1[a1] += effective_alpha * delta_1
        
        # 2. Update based on final reward (Eligibility trace)
        # If eligibility is high, Stage 1 Q-value is strongly driven by the final reward r
        q_stage1[a1] += effective_alpha * eligibility * delta_2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```