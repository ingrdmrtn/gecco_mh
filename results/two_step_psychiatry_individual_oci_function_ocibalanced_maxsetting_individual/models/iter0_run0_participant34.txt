Here are three cognitive models designed to explain the participant's behavior, incorporating their high OCI-R score (0.9) into the decision-making logic.

### Participant Analysis
The participant has a very high OCI-R score (0.9), placing them in the "high obsessive-compulsive symptoms" category. Looking at the data:
1.  **Stickiness/Repetition:** There are long streaks of repeating the same first-stage choice (e.g., choosing Spaceship 1.0 from trial 2 to 18, and again from 22 to 50). This suggests a high degree of choice perseveration or "stickiness."
2.  **Habitual vs. Goal-Directed:** High OCI scores are often associated in literature with a bias toward habitual (Model-Free) control over goal-directed (Model-Based) control. The participant seems less sensitive to the transition structure (rare vs. common) and more sensitive to recent rewards or simple repetition.
3.  **Second Stage Behavior:** The second stage choices also show some repetition, but are constrained by which planet is reached.

### Model 1: OCI-Modulated Mixture Model (Model-Based/Model-Free Hybrid)
This model hypothesizes that the balance between Model-Based (planning) and Model-Free (habit) systems is directly modulated by the OCI score. A high OCI score pushes the weight `w` towards 0 (pure Model-Free), ignoring the transition structure.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free RL where the mixing weight is modulated by OCI.
    
    The participant has a high OCI score (0.9), suggesting a deficit in goal-directed 
    (Model-Based) control. This model links OCI to the mixing parameter `w`.
    
    Parameters:
    learning_rate: [0, 1] Rate at which Q-values are updated.
    beta: [0, 10] Inverse temperature for softmax action selection.
    w_base: [0, 1] Baseline mixing weight (1 = MB, 0 = MF).
    oci_sensitivity: [0, 1] How strongly OCI reduces the model-based weight.
    """
    learning_rate, beta, w_base, oci_sensitivity = model_parameters
    n_trials = len(action_1)
    # Extract scalar OCI
    oci_score = oci[0]
    
    # Calculate effective mixing weight w. 
    # High OCI reduces w, making the agent more Model-Free.
    # We clip to ensure it stays in [0, 1].
    w = np.clip(w_base - (oci_sensitivity * oci_score), 0.0, 1.0)
    
    # Fixed transition matrix (A->X, U->Y commonly)
    # Indices: 0->A, 1->U; 0->X, 1->Y
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2) # Model-free values for stage 1 (A, U)
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for stage 2 (Planet X, Y -> Alien 0, 1)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based Value: Q_MB = T * max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value: Q_net = w * Q_MB + (1-w) * Q_MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # --- Stage 2 Choice ---
        state_idx = int(state[trial]) # Planet 0 or 1
        
        # Standard Model-Free choice at stage 2
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Learning ---
        # Stage 2 RPE
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2
        
        # Stage 1 RPE (SARSA-style update for MF)
        # Using the value of the state actually reached
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Driven Choice Stickiness (Perseveration)
The participant data shows long streaks of choosing the same spaceship (e.g., trials 2-18). This model assumes the core mechanism is purely Model-Free (common in high OCI), but adds a "stickiness" parameter that is amplified by the OCI score. High OCI leads to rigid repetition of the previous action, regardless of reward.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Pure Model-Free RL with Choice Stickiness modulated by OCI.
    
    This model assumes high OCI leads to perseveration (repeating the last choice),
    modeled as a 'stickiness' bonus added to the Q-value of the previously chosen action.
    
    Parameters:
    learning_rate: [0, 1] Update rate for Q-values.
    beta: [0, 10] Inverse temperature.
    stickiness_base: [0, 5] Base bonus for repeating an action.
    oci_stickiness_mult: [0, 5] Additional stickiness multiplier based on OCI score.
    """
    learning_rate, beta, stickiness_base, oci_stickiness_mult = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate total stickiness parameter
    # A high OCI score increases the tendency to repeat the previous choice.
    choice_stickiness = stickiness_base + (oci_stickiness_mult * oci_score)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track previous choice (initialize to -1 or handle first trial separately)
    last_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Add stickiness bonus to the Q-values before softmax
        q_augmented = q_stage1_mf.copy()
        if last_action_1 != -1:
            q_augmented[int(last_action_1)] += choice_stickiness
            
        exp_q1 = np.exp(beta * q_augmented)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # Update last action
        last_action_1 = action_1[trial]
        
        # --- Stage 2 Choice ---
        state_idx = int(state[trial])
        
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Learning ---
        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2
        
        # Stage 1 Update (TD-0)
        # Using the max value of the next state (Q-learning style) or value of chosen (SARSA)
        # Here we use the value of the chosen stage 2 action
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Influenced Learning Rate Decay (Rigidity)
Compulsivity is often characterized by rigidity and an inability to update behavior in response to changing feedback. This model posits that high OCI scores dampen the learning rate (`alpha`). A high OCI participant learns slower, making their behavior resistant to recent negative outcomes (explaining why they stick to a spaceship even after getting 0 coins).

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Based/Model-Free Hybrid where Learning Rate is dampened by OCI.
    
    High OCI is modeled here as 'rigidity': the learning rate is reduced 
    proportional to the OCI score, making the agent slow to update values
    based on new reward information.
    
    Parameters:
    alpha_base: [0, 1] Base learning rate.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Mixing weight (fixed parameter, not modulated).
    oci_dampening: [0, 1] Factor by which OCI reduces the learning rate.
    """
    alpha_base, beta, w, oci_dampening = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Effective learning rate: alpha = base * (1 - (oci * dampening))
    # If OCI is high and dampening is high, learning rate becomes very small.
    learning_rate = alpha_base * (1.0 - (oci_score * oci_dampening))
    learning_rate = np.clip(learning_rate, 0.01, 1.0) # Ensure it doesn't hit 0 completely
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # --- Stage 2 Choice ---
        state_idx = int(state[trial])
        
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Learning ---
        # Update Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2
        
        # Update Stage 1
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```