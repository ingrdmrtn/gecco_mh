Here are three cognitive models developed based on the participant's data and OCI score.

### Model 1: OCI-Modulated Perseveration (Stickiness)
This model hypothesizes that the OCI score drives "choice stickiness" or perseveration. High OCI scores are associated with compulsive repetition. In this model, the tendency to repeat the previous Stage 1 action is explicitly scaled by the participant's OCI score.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model where choice stickiness (perseveration) is modulated by OCI.
    
    Hypothesis:
    High OCI scores lead to higher perseveration (stickiness) on the first-stage choice,
    independent of reward history.
    
    Parameters:
    learning_rate: [0,1] - Rate of value updating.
    beta: [0,10] - Inverse temperature (exploitation vs exploration).
    w: [0,1] - Weighting between Model-Based (1) and Model-Free (0).
    lambda_elig: [0,1] - Eligibility trace decay (credit assignment to stage 1).
    stick_oci: [0,5] - Sensitivity of stickiness to OCI score.
    
    Total parameters: 5
    """
    learning_rate, beta, w, lambda_elig, stick_oci = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate stickiness magnitude based on OCI
    # High OCI -> Higher tendency to repeat last action
    stickiness_magnitude = stick_oci * current_oci

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2) # Model-free values for Stage 1
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for Stage 2 (State x Action)
    
    last_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        
        # 1. Model-Based Value Calculation
        # Max value of Stage 2 states derived from MF values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        # Expected value of Stage 1 actions based on transition matrix
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Net Value Calculation
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # 3. Add Stickiness
        # If we took an action previously, add a bonus to that action's value
        if last_action_1 != -1:
            q_net_stage1[last_action_1] += stickiness_magnitude

        # 4. Softmax Choice Probability
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # Record state and action
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        
        # --- Stage 2 Policy ---
        # Standard Softmax on Stage 2 MF values
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Value Updating ---
        
        # Prediction errors
        # Delta 1: Difference between Stage 2 value and Stage 1 value (SARSA-style)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        
        # Delta 2: Difference between received reward and Stage 2 value
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        
        # Update Stage 1 MF values
        # Uses eligibility trace (lambda) to pass Stage 2 outcome back to Stage 1
        q_stage1_mf[a1] += learning_rate * (delta_stage1 + lambda_elig * delta_stage2)
        
        # Update Stage 2 MF values
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Suppressed Model-Based Control
This model hypothesizes that high obsessive-compulsive symptoms are associated with a deficit in goal-directed (Model-Based) control. Instead of `w` being a free parameter, `w` is constrained to be a function of the OCI score. A high OCI score suppresses `w`, forcing the agent to rely more on Model-Free (habitual) values.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model where the MB/MF balance (w) is suppressed by OCI.
    
    Hypothesis:
    High OCI scores reflect a deficit in Model-Based control. 
    The weighting parameter 'w' is dynamically calculated: w = w_max * (1 - OCI).
    Participants with OCI ~ 0.9 will have near-zero w (Pure Model-Free).
    
    Parameters:
    learning_rate: [0,1] - Rate of value updating.
    beta: [0,10] - Inverse temperature.
    w_max: [0,1] - The maximum possible Model-Based weight (for an OCI of 0).
    lambda_elig: [0,1] - Eligibility trace decay.
    
    Total parameters: 4
    """
    learning_rate, beta, w_max, lambda_elig = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate effective weight w based on OCI
    # If OCI is 0.9, w becomes w_max * 0.1 (Very low MB influence)
    w_effective = w_max * (1.0 - current_oci)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Use w_effective calculated from OCI
        q_net_stage1 = w_effective * q_stage1_mb + (1 - w_effective) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # --- Value Updating ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        
        # Standard hybrid update with eligibility trace
        q_stage1_mf[a1] += learning_rate * (delta_stage1 + lambda_elig * delta_stage2)
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Induced Rigid Learning (Inverse Learning Rate)
This model hypothesizes that OCI relates to cognitive rigidity. Specifically, high OCI scores dampen the learning rate, making the participant slower to update their values in response to new information (both rewards and lack thereof). This explains long streaks of behavior even after outcomes change.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model where OCI reduces the effective learning rate (Rigidity).
    
    Hypothesis:
    High OCI scores result in cognitive rigidity, modeled as a reduction in the
    learning rate. The agent is slow to update beliefs.
    alpha_effective = alpha_base / (1 + rigidity_factor * OCI).
    
    Parameters:
    alpha_base: [0,1] - Baseline learning rate for OCI=0.
    beta: [0,10] - Inverse temperature.
    w: [0,1] - MB/MF weighting.
    lambda_elig: [0,1] - Eligibility trace.
    rigidity_factor: [0,10] - How strongly OCI dampens the learning rate.
    
    Total parameters: 5
    """
    alpha_base, beta, w, lambda_elig, rigidity_factor = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate effective learning rate
    # High OCI + High rigidity factor -> Low effective alpha (slow unlearning of habits)
    learning_rate = alpha_base / (1.0 + rigidity_factor * current_oci)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # --- Value Updating ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        
        # Update using the OCI-dampened learning rate
        q_stage1_mf[a1] += learning_rate * (delta_stage1 + lambda_elig * delta_stage2)
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```