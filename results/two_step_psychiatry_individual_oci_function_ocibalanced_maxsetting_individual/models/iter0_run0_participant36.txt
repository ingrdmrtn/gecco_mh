Here are three cognitive models designed to capture the behavior of the participant, specifically integrating their high OCI-R score (0.783) which suggests high obsessive-compulsive traits.

### Model 1: OCI-Modulated Model-Based/Model-Free Balance
This model hypothesizes that high OCI scores correlate with a rigid reliance on Model-Free (habitual) learning over Model-Based (goal-directed) planning. The `w` parameter (mixing weight) is directly modulated by the OCI score, pushing the participant towards habitual behavior as OCI increases.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Hybrid Learner.
    Hypothesis: High OCI participants exhibit reduced model-based control (goal-directed)
    and increased reliance on model-free (habitual) learning. The mixing parameter 'w'
    is effectively reduced by the OCI score.

    Parameters:
    - learning_rate: [0, 1] Update rate for Q-values.
    - beta: [0, 10] Inverse temperature for softmax choice.
    - w_base: [0, 1] Baseline mixing weight (1=Model-Based, 0=Model-Free).
    - oci_penalty: [0, 1] Strength of OCI reduction on model-based control.
    """
    learning_rate, beta, w_base, oci_penalty = model_parameters
    n_trials = len(action_1)
    # Ensure inputs are integers for indexing
    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    
    # Calculate effective w based on OCI (higher OCI reduces Model-Based influence)
    # We clip to ensure it stays in [0, 1]
    w_effective = w_base * (1.0 - (oci[0] * oci_penalty))
    w_effective = np.clip(w_effective, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)      # Model-free values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for stage 2 (states x actions)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based value calculation: transition * max(stage2_values)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid value: weighted sum of MB and MF
        q_hybrid = (w_effective * q_stage1_mb) + ((1 - w_effective) * q_stage1_mf)
        
        # Softmax choice 1
        exp_q1 = np.exp(beta * q_hybrid)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        a1 = action_1[trial]

        # --- Stage 2 Policy ---
        # Standard model-free choice at stage 2
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]

        # --- Learning ---
        # Stage 2 update (TD(0))
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 update (TD(1) / SARSA-like logic for MF)
        # Using the value of the state actually reached
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Enhanced Perseveration (Stickiness)
This model posits that obsessive-compulsive traits manifest as "stickiness" or perseverationâ€”a tendency to repeat the previous choice regardless of reward outcome. The OCI score scales a stickiness parameter, making high-OCI participants significantly more likely to repeat their last action.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Enhanced Perseveration.
    Hypothesis: High OCI is associated with behavioral rigidity (stickiness).
    The model adds a choice autocorrelation bonus (stickiness) to the Q-values,
    where the magnitude of this bonus is scaled by the OCI score.

    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - stickiness_base: [0, 5] Base tendency to repeat choices.
    - oci_stickiness_gain: [0, 5] Additional stickiness multiplier based on OCI.
    """
    learning_rate, beta, stickiness_base, oci_stickiness_gain = model_parameters
    n_trials = len(action_1)
    
    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    
    # Calculate total stickiness: Base + (OCI * Gain)
    total_stickiness = stickiness_base + (oci[0] * oci_stickiness_gain)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track previous choice (initialize to -1 for no previous choice)
    last_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Pure Model-Based calculation for the base value (simplified for this model focus)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_net = transition_matrix @ max_q_stage2
        
        # Add stickiness bonus to the Q-value of the previously chosen action
        q_with_stickiness = q_net.copy()
        if last_action_1 != -1:
            q_with_stickiness[last_action_1] += total_stickiness

        exp_q1 = np.exp(beta * q_with_stickiness)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        a1 = action_1[trial]
        last_action_1 = a1 # Update history

        # --- Stage 2 Policy ---
        # Standard choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]

        # --- Learning ---
        # Standard TD updates
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Note: In this specific model formulation, we aren't maintaining a separate 
        # MF Q-value for stage 1 to isolate the MB+Stickiness effect, but usually
        # one might update MF here too. We focus on Stage 2 value propagation.
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Driven Learning Rate Asymmetry (Loss Aversion)
This model suggests that high obsessive-compulsive symptoms relate to an altered sensitivity to negative outcomes (or lack of reward). Here, OCI modulates the learning rate specifically for unrewarded trials (negative prediction errors), making the participant learn differently from failures compared to successes.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Driven Learning Rate Asymmetry.
    Hypothesis: High OCI participants may be hyper-sensitive (or hypo-sensitive) 
    to prediction errors when rewards are not received.
    We split learning rates into alpha_pos (reward) and alpha_neg (no reward),
    where alpha_neg is modulated by OCI.

    Parameters:
    - alpha_pos: [0, 1] Learning rate for positive prediction errors (Reward=1).
    - alpha_neg_base: [0, 1] Base learning rate for negative prediction errors.
    - beta: [0, 10] Inverse temperature.
    - oci_neg_mod: [0, 1] Factor by which OCI modifies negative learning rate.
    """
    alpha_pos, alpha_neg_base, beta, oci_neg_mod = model_parameters
    n_trials = len(action_1)
    
    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    
    # Calculate effective negative learning rate
    # If oci_neg_mod is high, OCI increases sensitivity to negative outcomes
    alpha_neg_effective = alpha_neg_base + (oci[0] * oci_neg_mod)
    alpha_neg_effective = np.clip(alpha_neg_effective, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Simple Model-Free learner for Stage 1 in this hypothesis
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        a1 = action_1[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]

        # --- Learning ---
        # Determine which learning rate to use based on prediction error sign
        # We look at the Stage 2 outcome
        pe_stage2 = r - q_stage2_mf[state_idx, a2]
        
        if pe_stage2 >= 0:
            lr = alpha_pos
        else:
            lr = alpha_neg_effective
            
        q_stage2_mf[state_idx, a2] += lr * pe_stage2
        
        # Stage 1 Update
        # Using the same asymmetric logic for the Stage 1 update
        pe_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        
        if pe_stage1 >= 0:
            lr_s1 = alpha_pos
        else:
            lr_s1 = alpha_neg_effective

        q_stage1_mf[a1] += lr_s1 * pe_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```