Here are three cognitive models formulated as Python functions. They are based on the standard reinforcement learning framework for the two-step task (Daw et al., 2011), modified to incorporate the OCI score to explain individual variability in the balance between model-based/model-free control, perseveration, and learning rigidity.

### Cognitive Model 1: OCI-Modulated Hybrid Control
This model assumes that the participant uses a hybrid of Model-Based (MB) and Model-Free (MF) reinforcement learning. The core hypothesis here is that high OCI scores correlate with a deficit in Model-Based control (goal-directed planning) and a reliance on Model-Free (habitual) control. The mixing weight $w$ is dynamically adjusted by the OCI score.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free RL where OCI modulates the mixing weight 'w'.
    
    Hypothesis: Higher OCI scores reduce the weight of Model-Based planning (w),
    leading to more habitual (Model-Free) behavior.
    
    Parameters:
    - learning_rate: [0, 1] Rate of value updating.
    - beta: [0, 10] Inverse temperature (exploration/exploitation).
    - lambda_coeff: [0, 1] Eligibility trace (how much Stage 2 reward updates Stage 1).
    - w_base: [0, 1] Baseline model-based weight for a person with 0 OCI.
    - w_oci_penalty: [0, 1] Reduction in MB weight proportional to OCI score.
    """
    learning_rate, beta, lambda_coeff, w_base, w_oci_penalty = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective mixing weight w based on OCI
    # High OCI reduces w, pushing behavior towards Model-Free
    w = w_base - (w_oci_penalty * oci_score)
    w = np.clip(w, 0.0, 1.0) # Ensure w stays valid
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)       # Model-Free values for Stage 1 (Spaceships)
    q_stage2_mf = np.zeros((2, 2))  # Model-Free values for Stage 2 (Aliens)

    for trial in range(n_trials):
        # --- STAGE 1 POLICY ---
        # Model-Based value calculation: Transition * max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value: w * MB + (1-w) * MF
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Softmax Choice 1
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial]) # Planet arrived at
        
        # --- STAGE 2 POLICY ---
        # Standard Model-Free choice at Stage 2
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- UPDATES ---
        # Prediction Errors
        # Delta 1: Difference between Stage 2 Value and Stage 1 Expectation
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        
        # Delta 2: Difference between Reward and Stage 2 Expectation
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        
        # Update Stage 1 MF (TD(lambda) update)
        # Update with immediate PE + eligibility trace of second stage PE
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1 + learning_rate * lambda_coeff * delta_stage2
        
        # Update Stage 2 MF
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2: OCI-Driven Perseveration
This model posits that high OCI scores drive "compulsive" repetition of actions (stickiness/perseveration), regardless of reward history. It uses a hybrid RL structure but adds a perseveration bonus to the previously chosen action in Stage 1. The magnitude of this bonus is scaled by the OCI score.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL model with OCI-modulated Perseveration (Stickiness).
    
    Hypothesis: High OCI leads to 'stuck' behavior. The participant receives a 
    'bonus' to the value of the previously chosen spaceship, making switching harder.
    The magnitude of this bonus increases with OCI.
    
    Parameters:
    - learning_rate: [0, 1]
    - beta: [0, 10]
    - w: [0, 1] Mixing weight (fixed for this model, focus is on p).
    - p_base: [0, 5] Base perseveration parameter.
    - p_oci_scale: [0, 5] How strongly OCI amplifies perseveration.
    """
    learning_rate, beta, w, p_base, p_oci_scale = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective perseveration strength
    perseveration_strength = p_base + (p_oci_scale * oci_score)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1 # Initialize previous action

    for trial in range(n_trials):
        # --- STAGE 1 POLICY ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Add perseveration bonus to the logits (not the Q-value itself)
        logits = beta * q_net
        if prev_action_1 != -1:
            logits[int(prev_action_1)] += perseveration_strength
            
        # Softmax
        # Subtract max for numerical stability
        logits = logits - np.max(logits)
        probs_1 = np.exp(logits) / np.sum(np.exp(logits))
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # Store action for next trial's perseveration
        prev_action_1 = action_1[trial]
        state_idx = int(state[trial])

        # --- STAGE 2 POLICY ---
        logits_2 = beta * q_stage2_mf[state_idx]
        logits_2 = logits_2 - np.max(logits_2)
        probs_2 = np.exp(logits_2) / np.sum(np.exp(logits_2))
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- UPDATES ---
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        
        # Assuming lambda=1 (full eligibility) for parsimony in this specific model variation
        # to save a parameter for the p_oci_scale
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1 + learning_rate * delta_stage2
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3: OCI-Modulated Learning Rigidity
This model suggests that high OCI scores correlate with "cognitive rigidity" manifested as a reduced learning rate. High OCI participants may form beliefs and update them very slowly in the face of new evidence (low alpha), or conversely, be hyper-reactive. This model tests the hypothesis that OCI scales the learning rate $\alpha$.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL model where OCI modulates the Learning Rate.
    
    Hypothesis: OCI affects cognitive flexibility. High OCI leads to lower 
    learning rates (rigidity), making the participant slower to update values 
    based on prediction errors.
    
    Parameters:
    - lr_base: [0, 1] Base learning rate.
    - lr_oci_factor: [0, 1] Factor reducing learning rate by OCI.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight.
    - lambda_coeff: [0, 1] Eligibility trace.
    """
    lr_base, lr_oci_factor, beta, w, lambda_coeff = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective learning rate
    # lr = base * (1 - factor * oci). If factor is high and OCI is high, LR becomes very small.
    learning_rate = lr_base * (1.0 - (lr_oci_factor * oci_score))
    learning_rate = np.clip(learning_rate, 0.001, 1.0) # Prevent 0 learning
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- STAGE 1 POLICY ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])

        # --- STAGE 2 POLICY ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- UPDATES ---
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        
        # Update using the OCI-modulated learning rate
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1 + learning_rate * lambda_coeff * delta_stage2
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```