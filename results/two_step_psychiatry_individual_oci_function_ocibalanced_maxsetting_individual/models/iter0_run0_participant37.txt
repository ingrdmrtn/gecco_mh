Here are three cognitive models designed to capture the behavior of the participant, specifically integrating their high OCI-R score (0.733) into the decision-making logic.

### Model 1: The "Compulsive Habit" Model
**Hypothesis:** High OCI scores are associated with an over-reliance on Model-Free (habitual) learning and a deficit in Model-Based (goal-directed) planning. This model posits that the mixing weight `w` (balance between MB and MF) is directly modulated by the OCI score, where higher OCI forces a lower `w` (more habit).

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: Compulsive Habit Model.
    Hypothesis: High OCI reduces the weight of model-based control (w), leading to 
    habitual, model-free dominance. The OCI score acts as a scaling factor on w.
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    w_baseline: [0, 1] (Base mixing weight before OCI modulation)
    """
    learning_rate, beta, w_baseline = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # OCI modulation: High OCI reduces w towards 0 (pure MF).
    # We assume w_baseline is the potential w, and OCI acts as a penalty.
    # If OCI is 0, w = w_baseline. If OCI is 1, w is significantly reduced.
    w = w_baseline * (1.0 - oci_score)

    # Fixed transition structure for Model-Based
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Initialize values
    q_stage1_mf = np.zeros(2)  # Model-free Q-values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Model-free Q-values for stage 2 (2 states, 2 actions)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # 1. Calculate Model-Based values (Bellman equation using transition matrix)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Integrate MB and MF values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # 3. Softmax selection
        # Convert -1/1 actions to 0/1 indices for array access if necessary, 
        # but data seems to be 0.0/1.0 or -1.0. Assuming 0 and 1 indices based on template.
        # Data check: The provided data uses -1.0, 0.0, 1.0. 
        # Standardize inputs to integers 0 and 1.
        a1 = int(action_1[trial])
        if a1 == -1: a1 = 0 # Handle potential missing/pre-trial data artifact if present
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Choice ---
        s_idx = int(state[trial])
        if s_idx == -1: s_idx = 0
        
        a2 = int(action_2[trial])
        if a2 == -1: a2 = 0
        
        # Stage 2 is purely model-free Q-learning
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        r = reward[trial]
        if r == -1: r = 0 # Handle non-reward trials if coded as -1
        
        # SARSA / Q-learning updates
        # Stage 2 Update (TD error)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 Update (TD error using Stage 2 value)
        # Using the value of the state actually reached (standard TD(0))
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: The "Anxious Stickiness" Model
**Hypothesis:** High OCI is often comorbid with anxiety and avoidance of uncertainty. This model suggests that high OCI leads to increased "stickiness" or choice perseveration (repeating the previous action regardless of reward), representing a compulsion to maintain the status quo. The OCI score scales a `stickiness` parameter added to the Q-values.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: Anxious Stickiness Model.
    Hypothesis: High OCI leads to repetitive behavior (perseveration). 
    The OCI score amplifies a stickiness parameter that biases the agent 
    to repeat the previous Stage 1 choice.
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    w: [0, 1] (Mixing weight)
    stickiness_base: [0, 5] (Base tendency to repeat)
    """
    learning_rate, beta, w, stickiness_base = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # OCI modulation: High OCI amplifies stickiness.
    stickiness_param = stickiness_base * (1.0 + oci_score)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1 # Initialize previous action

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        if a1 == -1: a1 = 0
        s_idx = int(state[trial])
        if s_idx == -1: s_idx = 0
        a2 = int(action_2[trial])
        if a2 == -1: a2 = 0
        r = reward[trial]
        if r == -1: r = 0

        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness bonus to the previously chosen action
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness_param
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Update last action for next trial
        last_action_1 = a1

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        # Standard TD updates
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: The "Rigid Learning" Model
**Hypothesis:** OCI is linked to cognitive inflexibility. This model proposes that high OCI scores result in a lower learning rate (`alpha`), making the participant slower to update their beliefs when contingencies change (e.g., when coin probabilities drift). The OCI score acts as a damper on the learning rate.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: Rigid Learning Model.
    Hypothesis: High OCI is associated with cognitive rigidity. 
    The OCI score dampens the learning rate, making the agent less responsive 
    to recent prediction errors (slower to unlearn habits).
    
    Bounds:
    alpha_base: [0, 1] (Maximum learning rate)
    beta: [0, 10]
    w: [0, 1]
    eligibility_lambda: [0, 1] (Eligibility trace for direct reinforcement)
    """
    alpha_base, beta, w, eligibility_lambda = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # OCI modulation: Learning rate decreases as OCI increases.
    # We use an exponential decay factor based on OCI.
    learning_rate = alpha_base * np.exp(-oci_score)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        if a1 == -1: a1 = 0
        s_idx = int(state[trial])
        if s_idx == -1: s_idx = 0
        a2 = int(action_2[trial])
        if a2 == -1: a2 = 0
        r = reward[trial]
        if r == -1: r = 0

        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        # Stage 2 update
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 update (TD(1) / Eligibility Trace style logic)
        # Instead of just updating based on stage 2 value (TD(0)), we also allow
        # the final reward to directly influence stage 1 choice via lambda.
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        
        # Standard TD update
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Additional direct reinforcement from reward (Eligibility Trace approximation)
        # This reinforces the stage 1 choice based on the final outcome directly
        q_stage1_mf[a1] += learning_rate * eligibility_lambda * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```