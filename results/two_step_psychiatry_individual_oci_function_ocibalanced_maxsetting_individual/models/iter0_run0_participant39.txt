Here are three cognitive models designed to capture the behavior of the participant, particularly focusing on how their high OCI-R score (0.717) might influence the balance between model-based (goal-directed) and model-free (habitual) control.

### Model 1: OCI-Modulated Model-Based/Model-Free Hybrid
This model hypothesizes that high OCI scores correlate with a deficit in model-based control or an over-reliance on habitual (model-free) control. The `w` parameter (mixing weight) is dynamically adjusted by the OCI score. A high OCI score reduces the contribution of the model-based system (`w`), making the agent more habitual.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free RL with OCI-modulated mixing weight.
    
    The balance between Model-Based (planning) and Model-Free (habit) systems
    is determined by parameter 'w'. This model posits that high OCI scores
    reduce the reliance on the model-based system (reduced w).

    Parameters:
    - learning_rate: [0, 1] Speed of value updating.
    - beta: [0, 10] Inverse temperature for softmax (choice consistency).
    - w_base: [0, 1] Baseline mixing weight (0=pure MF, 1=pure MB).
    - oci_penalty: [0, 1] How much OCI score reduces the model-based weight.
    """
    learning_rate, beta, w_base, oci_penalty = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]
    
    # Calculate effective mixing weight based on OCI
    # High OCI reduces w, pushing behavior towards Model-Free
    w = w_base * (1.0 - (participant_oci * oci_penalty))
    w = np.clip(w, 0.0, 1.0)

    # Fixed transition matrix (A->X=0.7, U->Y=0.7)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Initialize Q-values
    q_stage1_mf = np.zeros(2)      # Model-free values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for stage 2 (State x Action)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        # Model-Based Value: T * max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Value: w * MB + (1-w) * MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        # Standard Softmax on Stage 2 Q-values
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        # Stage 2 update (TD error)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 update (TD error using Stage 2 value)
        # Note: In hybrid models, MF system updates based on transition to stage 2
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-driven Stickiness (Perseveration)
This model assumes that obsessive-compulsive traits manifest as "stickiness" or perseverationâ€”a tendency to repeat the previous choice regardless of reward history. Here, the OCI score scales a choice autocorrelation parameter (`stickiness`), making the agent more likely to repeat the previous Stage 1 action if their OCI is high.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free RL with OCI-modulated Choice Stickiness.
    
    This model assumes high OCI leads to repetitive behavior (perseveration).
    The 'stickiness' parameter adds a bonus to the previously chosen action,
    scaled directly by the participant's OCI score.

    Parameters:
    - learning_rate: [0, 1] Speed of value updating.
    - beta: [0, 10] Inverse temperature.
    - stick_base: [0, 5] Base level of choice perseveration.
    - stick_oci_sens: [0, 5] Sensitivity of stickiness to OCI score.
    """
    learning_rate, beta, stick_base, stick_oci_sens = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]
    
    # Calculate effective stickiness
    # Stickiness increases with OCI score
    effective_stickiness = stick_base + (participant_oci * stick_oci_sens)

    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Track previous choice (initialized to -1 for first trial)
    prev_a1 = -1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        # Add stickiness bonus to Q-values before softmax
        q_augmented = q_stage1.copy()
        if prev_a1 != -1:
            q_augmented[prev_a1] += effective_stickiness
            
        exp_q1 = np.exp(beta * q_augmented)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        # SARSA-style / TD(0) updates
        # Update Stage 2
        delta_stage2 = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += learning_rate * delta_stage2
        
        # Update Stage 1 based on value of state arrived at
        # Using the value of the chosen stage 2 action as the target
        delta_stage1 = q_stage2[s_idx, a2] - q_stage1[a1]
        q_stage1[a1] += learning_rate * delta_stage1
        
        # Update memory
        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-influenced Learning Rate Asymmetry
This model posits that OCI affects how the participant learns from positive versus negative prediction errors. Specifically, it tests if high OCI leads to hypersensitivity to punishment (negative prediction errors) or reward (positive prediction errors). The learning rate is split into `alpha_pos` and `alpha_neg`, where the balance is shifted by the OCI score.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free RL with OCI-modulated Asymmetric Learning Rates.
    
    This model investigates if OCI affects sensitivity to positive vs negative
    outcomes. OCI shifts the balance between learning from positive errors
    (alpha_pos) and negative errors (alpha_neg).

    Parameters:
    - alpha_base: [0, 1] Baseline learning rate.
    - beta: [0, 10] Inverse temperature.
    - oci_bias: [0, 1] How much OCI biases learning towards negative errors.
      (If bias is high, OCI increases alpha_neg and decreases alpha_pos).
    - lambda_eligibility: [0, 1] Eligibility trace decay (connects stage 2 outcome to stage 1).
    """
    alpha_base, beta, oci_bias, lambda_eligibility = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]

    # Modulation: High OCI + High Bias -> Higher learning from negative prediction errors
    # We maintain bounds [0, 1]
    alpha_pos = alpha_base * (1.0 - (participant_oci * oci_bias))
    alpha_neg = alpha_base * (1.0 + (participant_oci * oci_bias))
    
    alpha_pos = np.clip(alpha_pos, 0.0, 1.0)
    alpha_neg = np.clip(alpha_neg, 0.0, 1.0)

    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        
        # Stage 2 Prediction Error
        pe_2 = r - q_stage2[s_idx, a2]
        
        # Select learning rate based on sign of PE
        lr_2 = alpha_pos if pe_2 > 0 else alpha_neg
        q_stage2[s_idx, a2] += lr_2 * pe_2
        
        # Stage 1 Prediction Error
        # TD(1) / Eligibility trace style update logic for Stage 1
        # The Stage 1 value is updated by the Stage 2 PE, scaled by lambda
        # This is a common simplification in two-step tasks to represent model-free chains
        lr_1 = alpha_pos if pe_2 > 0 else alpha_neg
        q_stage1[a1] += lr_1 * lambda_eligibility * pe_2
        
        # Note: We also usually update Stage 1 based on the transition to Stage 2 (PE_1),
        # but pure TD(1) often focuses on the final reward. 
        # Here we add the immediate transition update as well for completeness.
        pe_1 = q_stage2[s_idx, a2] - q_stage1[a1] # (Old value of stage 2 before update)
        # Using base alpha for transition structure learning to isolate reward sensitivity
        q_stage1[a1] += alpha_base * (1 - lambda_eligibility) * pe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```