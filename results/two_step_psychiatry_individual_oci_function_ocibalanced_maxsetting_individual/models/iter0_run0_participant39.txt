Here are the three cognitive models designed to explain the participant's behavior, incorporating their OCI score into the decision-making mechanics.

### Cognitive Model 1: OCI-Modulated Model-Based Weighting
This model hypothesizes that the balance between Goal-Directed (Model-Based) and Habitual (Model-Free) control is a function of the participant's obsessive-compulsive symptoms. High OCI scores are often associated with a reliance on habitual control. Here, the mixing weight `w` is not a fixed parameter but is derived linearly from the OCI score.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free model where the mixing weight (w) is modulated by OCI.
    
    Hypothesis:
    High OCI scores may correlate with a reduced reliance on Model-Based (goal-directed) 
    planning. The weight 'w' is calculated as a logistic function of OCI.
    
    Parameters:
    learning_rate: [0, 1] - Rate of value updating.
    beta: [0, 10] - Inverse temperature (exploration/exploitation balance).
    lambda_eligibility: [0, 1] - Eligibility trace (how much Stage 2 outcome updates Stage 1).
    w_slope: [-5, 5] - Sensitivity of MB-weight to OCI.
    w_intercept: [-5, 5] - Baseline MB-weight logit.
    """
    learning_rate, beta, lambda_eligibility, w_slope, w_intercept = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate mixing weight w based on OCI (bounded 0 to 1 via sigmoid)
    # w represents the weight of Model-Based control.
    w_logit = w_slope * oci_score + w_intercept
    w = 1.0 / (1.0 + np.exp(-w_logit))
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)      # Model-Free values for Stage 1 (Spaceships)
    q_stage2_mf = np.zeros((2, 2)) # Model-Free values for Stage 2 (Aliens | Planet)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Policy for Choice 1 ---
        # Model-Based Value: Transition * Max(Stage 2 Values)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Value: Weighted sum of MB and MF
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax Choice 1
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Policy for Choice 2 ---
        # Standard Softmax on Stage 2 MF values
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Value Updating ---
        # Prediction Errors
        # Delta 1: Difference between expected value of chosen spaceship and value of state arrived at
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        
        # Delta 2: Difference between expected value of chosen alien and actual reward
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        # Update Stage 1 MF (TD(lambda) update)
        # Updates based on immediate transition (delta1) and eventual reward (delta2 * lambda)
        q_stage1_mf[a1] += learning_rate * (delta_stage1 + lambda_eligibility * delta_stage2)
        
        # Update Stage 2 MF
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2: OCI-Driven Perseveration
This model hypothesizes that high OCI scores correlate with "stickiness" or perseveration (compulsive repetition of choices), regardless of the reward outcome. The model adds a perseveration bonus to the previously chosen action, the magnitude of which is scaled by the OCI score.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid model with OCI-scaled Perseveration (Stickiness).
    
    Hypothesis:
    Participants with high OCI scores exhibit higher choice stickiness (compulsive repetition).
    A bonus is added to the Q-value of the previously chosen spaceship.
    The magnitude of this bonus is determined by 'persev_oci_scale'.
    
    Parameters:
    learning_rate: [0, 1]
    beta: [0, 10]
    w: [0, 1] - Fixed mixing weight for MB/MF.
    persev_base: [-2, 2] - Baseline stickiness.
    persev_oci_scale: [-5, 5] - How much OCI increases stickiness.
    """
    learning_rate, beta, w, persev_base, persev_oci_scale = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective perseveration bonus
    perseveration_bonus = persev_base + (persev_oci_scale * oci_score)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1 # No previous action for first trial

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Policy for Choice 1 ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combined Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add Perseveration Bonus to the previously chosen action
        if last_action_1 != -1:
            q_net[last_action_1] += perseveration_bonus
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Store action for next trial's perseveration
        last_action_1 = a1

        # --- Policy for Choice 2 ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Value Updating ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        # Using lambda=1.0 implicit assumption for this model to save params
        q_stage1_mf[a1] += learning_rate * (delta_stage1 + delta_stage2) 
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3: OCI-Modulated Inverse Temperature (Rigidity)
This model hypothesizes that OCI affects the "rigidity" or determinism of the decision policy. High OCI scores might lead to a higher inverse temperature (`beta`), resulting in less exploration and more "greedy" or rigid exploitation of current beliefs, explaining the long runs of identical choices seen in the data.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model where OCI scales the Inverse Temperature (Beta).
    
    Hypothesis:
    High OCI relates to behavioral rigidity. This is modeled by scaling the 
    softmax beta parameter. Higher OCI -> Higher Beta -> More deterministic/rigid choices.
    
    Parameters:
    learning_rate: [0, 1]
    beta_base: [0, 10] - Baseline inverse temperature.
    beta_oci_slope: [-5, 5] - Scaling factor for OCI's effect on Beta.
    w: [0, 1] - MB/MF mixing weight.
    lambda_eligibility: [0, 1] - Eligibility trace.
    """
    learning_rate, beta_base, beta_oci_slope, w, lambda_eligibility = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective Beta
    # We ensure beta stays positive. 
    # If slope is positive, high OCI = high rigidity.
    beta_eff = beta_base * (1.0 + beta_oci_slope * oci_score)
    beta_eff = max(0.0, beta_eff) 
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Policy for Choice 1 ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Use effective beta derived from OCI
        exp_q1 = np.exp(beta_eff * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Policy for Choice 2 ---
        exp_q2 = np.exp(beta_eff * q_stage2_mf[s_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Value Updating ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        q_stage1_mf[a1] += learning_rate * (delta_stage1 + lambda_eligibility * delta_stage2)
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```