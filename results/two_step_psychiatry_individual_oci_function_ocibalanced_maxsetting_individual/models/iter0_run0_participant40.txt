Here are three cognitive models designed to explain the participant's behavior, incorporating their OCI score to account for individual variability in decision-making strategies.

### Cognitive Model 1: Hybrid Model with OCI-Modulated Stickiness
This model hypothesizes that the participant's high OCI score drives compulsive repetition of choices ("stickiness"). The stickiness parameter, which adds a bonus to the previously chosen action, is modeled as a linear function of the OCI score. This accounts for the extreme perseveration observed in the data (e.g., choosing Spaceship 0 for over 100 consecutive trials).

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model where choice stickiness is modulated by OCI score.
    High OCI is hypothesized to increase perseveration (stickiness).

    Parameters:
    - learning_rate: [0, 1] Rate of value updating.
    - beta: [0, 10] Inverse temperature (softness of choice).
    - w: [0, 1] Mixing weight (0 = pure MF, 1 = pure MB).
    - stick_base: [0, 5] Baseline stickiness independent of OCI.
    - stick_oci: [0, 5] Additional stickiness scaled by OCI score.
    - lambda_eligibility: [0, 1] Eligibility trace decay (TD(lambda)).
    """
    learning_rate, beta, w, stick_base, stick_oci, lambda_eligibility = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate effective stickiness based on OCI
    # OCI is ~0.77 (high), likely increasing the tendency to repeat choices.
    stickiness = stick_base + stick_oci * current_oci

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2) # MF values for Stage 1 (Spaceships)
    q_stage2_mf = np.zeros((2, 2)) # MF values for Stage 2 (Planets -> Aliens)
    
    prev_a1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # Model-Based Value: Expected max value of next stage using transition matrix
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Value: Weighted sum of MB and MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add Stickiness to the previously chosen action
        logits = beta * q_net
        if prev_a1 != -1:
            logits[prev_a1] += stickiness
            
        # Softmax Choice Probability
        # subtract max for numerical stability
        exp_q1 = np.exp(logits - np.max(logits))
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # --- Stage 2 Decision ---
        state_idx = int(state[trial])
        
        # Standard Softmax on Stage 2 MF values
        logits_2 = beta * q_stage2_mf[state_idx, :]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Value Updating ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Stage 1 TD Error (SARSA-like connection to Stage 2 value)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Stage 2 TD Error (Reward Prediction Error)
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Eligibility Trace: Update Stage 1 value based on Stage 2 outcome
        q_stage1_mf[a1] += learning_rate * lambda_eligibility * delta_stage2
        
        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2: Hybrid Model with OCI-Modulated Mixing Weight
This model tests the hypothesis that higher OCI scores correlate with a deficit in Model-Based control (goal-directed behavior) and a reliance on Model-Free (habitual) control. The mixing weight $w$ is modeled as a sigmoid function of the OCI score, allowing the balance between MB and MF systems to vary with symptom severity.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid model where the Model-Based/Model-Free balance (w) is modulated by OCI.
    High OCI is hypothesized to reduce w (more habitual/MF).
    
    Parameters:
    - learning_rate: [0, 1]
    - beta: [0, 10]
    - w_intercept: [0, 10] Intercept for the sigmoid function of w.
    - w_slope: [0, 10] Slope for OCI effect on w (scaled down internally).
    - stickiness: [0, 5] Choice stickiness.
    - lambda_eligibility: [0, 1]
    """
    learning_rate, beta, w_intercept, w_slope, stickiness, lambda_eligibility = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Map w_intercept and w_slope to a probability w via sigmoid.
    # We subtract 5 from parameters to allow negative slopes/intercepts from [0,10] inputs.
    # w = sigmoid(intercept + slope * oci)
    eff_intercept = w_intercept - 5.0
    eff_slope = w_slope - 5.0
    w_logit = eff_intercept + eff_slope * current_oci
    w = 1.0 / (1.0 + np.exp(-w_logit))
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_a1 = -1

    for trial in range(n_trials):
        # --- Stage 1 ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits = beta * q_net
        if prev_a1 != -1:
            logits[prev_a1] += stickiness
            
        exp_q1 = np.exp(logits - np.max(logits))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # --- Stage 2 ---
        state_idx = int(state[trial])
        logits_2 = beta * q_stage2_mf[state_idx, :]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Updates ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        q_stage1_mf[a1] += learning_rate * lambda_eligibility * delta_stage2
        
        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3: Hybrid Model with OCI-Modulated Determinism (Beta)
This model posits that OCI affects the "temperature" of decision-making. High OCI individuals may exhibit more rigid (deterministic) behavior, characterized by a higher inverse temperature parameter ($\beta$). This model allows the randomness of choice to vary based on the participant's obsessive-compulsive symptom score.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid model where the inverse temperature (beta) is modulated by OCI.
    High OCI is hypothesized to increase beta (more rigid/deterministic choice).

    Parameters:
    - learning_rate: [0, 1]
    - beta_base: [0, 5] Baseline beta.
    - beta_oci: [0, 5] Additional beta scaled by OCI.
    - w: [0, 1] Mixing weight.
    - stickiness: [0, 5]
    - lambda_eligibility: [0, 1]
    """
    learning_rate, beta_base, beta_oci, w, stickiness, lambda_eligibility = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate effective beta
    # Total beta = base + oci_factor * oci
    beta = beta_base + beta_oci * current_oci
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_a1 = -1

    for trial in range(n_trials):
        # --- Stage 1 ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Apply OCI-modulated beta
        logits = beta * q_net
        if prev_a1 != -1:
            logits[prev_a1] += stickiness
            
        exp_q1 = np.exp(logits - np.max(logits))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # --- Stage 2 ---
        state_idx = int(state[trial])
        logits_2 = beta * q_stage2_mf[state_idx, :]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Updates ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        q_stage1_mf[a1] += learning_rate * lambda_eligibility * delta_stage2
        
        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```