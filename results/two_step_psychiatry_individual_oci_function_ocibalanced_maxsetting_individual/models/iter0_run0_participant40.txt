Here are three cognitive models designed to explain the participant's behavior, incorporating their high OCI score (0.766).

### Model 1: Hybrid Learner with OCI-modulated Model-Based Weight
This model assumes that individuals with high compulsive symptoms (high OCI) might rely more heavily on habitual (model-free) responding rather than goal-directed (model-based) planning, or vice versa. Given the literature suggesting compulsivity relates to habit formation, this model uses the OCI score to determine the balance `w` between Model-Based (MB) and Model-Free (MF) systems. A high OCI score shifts the weight towards MF (habitual) control.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free learner where the mixing weight 'w' is 
    modulated by the OCI score.
    
    Hypothesis: High OCI (compulsivity) leads to reduced model-based control 
    (lower w), relying more on habits.
    
    Bounds:
    learning_rate: [0,1]
    beta_1: [0,10]
    beta_2: [0,10]
    w_base: [0,1]
    """
    learning_rate, beta_1, beta_2, w_base = model_parameters
    n_trials = len(action_1)
    
    # Transform inputs to integers for indexing
    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    reward = reward.astype(int)
    oci_score = oci[0]

    # Fixed transition matrix (A->X mostly, U->Y mostly)
    # 0 -> 0 (70%), 0 -> 1 (30%)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Initialize Q-values
    q_mf_stage1 = np.zeros(2)       # Model-free values for stage 1
    q_mb_stage1 = np.zeros(2)       # Model-based values for stage 1
    q_stage2 = np.zeros((2, 2))     # Values for stage 2 (State x Action)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Calculate OCI-modulated weight. 
    # If OCI is high, effective_w decreases (more habit).
    # We clip to ensure it stays in [0, 1].
    effective_w = np.clip(w_base * (1.0 - oci_score), 0.0, 1.0)

    for t in range(n_trials):
        # --- Stage 1 Decision ---
        # Calculate Model-Based values: T * max(Q_stage2)
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_mb_stage1 = transition_matrix @ max_q_stage2
        
        # Combined value
        q_net_stage1 = (effective_w * q_mb_stage1) + ((1 - effective_w) * q_mf_stage1)
        
        # Softmax choice 1
        logits_1 = beta_1 * q_net_stage1
        # prevent overflow
        logits_1 = logits_1 - np.max(logits_1)
        probs_1 = np.exp(logits_1) / np.sum(np.exp(logits_1))
        
        p_choice_1[t] = probs_1[action_1[t]]
        
        # --- Stage 2 Decision ---
        # Current state index
        s_idx = state[t]
        
        # Softmax choice 2 based on stage 2 Q-values
        logits_2 = beta_2 * q_stage2[s_idx]
        logits_2 = logits_2 - np.max(logits_2)
        probs_2 = np.exp(logits_2) / np.sum(np.exp(logits_2))
        
        p_choice_2[t] = probs_2[action_2[t]]
        
        # --- Learning ---
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]
        
        # Prediction errors
        # Stage 2 PE
        pe_2 = r - q_stage2[s_idx, a2]
        
        # Stage 1 PE (SARSA-style update for MF)
        # Using the value of the state actually reached
        pe_1 = q_stage2[s_idx, a2] - q_mf_stage1[a1]
        
        # Updates
        q_stage2[s_idx, a2] += learning_rate * pe_2
        q_mf_stage1[a1] += learning_rate * pe_1
        
        # Eligibility trace: Stage 1 MF values also updated by Stage 2 reward (TD-lambda=1 logic simplified)
        q_mf_stage1[a1] += learning_rate * pe_2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Driven Perseveration (Stickiness)
This model posits that high OCI scores relate to "stickiness" or perseverationâ€”a tendency to repeat the previous choice regardless of the reward outcome. The OCI score is used to scale a choice autocorrelation parameter (`perseveration`), making the participant more likely to repeat `action_1` if their OCI is high.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner with OCI-modulated choice perseveration (stickiness).
    
    Hypothesis: High OCI leads to higher choice repetition (stickiness) 
    in the first stage, regardless of reward history.
    
    Bounds:
    learning_rate: [0,1]
    beta: [0,10]
    stickiness_factor: [0,5]
    """
    learning_rate, beta, stickiness_factor = model_parameters
    n_trials = len(action_1)
    
    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    reward = reward.astype(int)
    oci_score = oci[0]

    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Calculate perseveration bonus based on OCI
    # Higher OCI = Higher tendency to repeat last action
    perseveration_weight = stickiness_factor * oci_score

    last_action_1 = -1 # Indicator for no previous action

    for t in range(n_trials):
        # --- Stage 1 Decision ---
        # Base Q-values
        logits_1 = beta * q_stage1.copy()
        
        # Add stickiness bonus to the previously chosen action
        if last_action_1 != -1:
            logits_1[last_action_1] += perseveration_weight
            
        logits_1 = logits_1 - np.max(logits_1)
        probs_1 = np.exp(logits_1) / np.sum(np.exp(logits_1))
        
        p_choice_1[t] = probs_1[action_1[t]]
        
        # Store action for next trial's stickiness
        last_action_1 = action_1[t]
        
        # --- Stage 2 Decision ---
        s_idx = state[t]
        logits_2 = beta * q_stage2[s_idx]
        logits_2 = logits_2 - np.max(logits_2)
        probs_2 = np.exp(logits_2) / np.sum(np.exp(logits_2))
        
        p_choice_2[t] = probs_2[action_2[t]]
        
        # --- Learning (Standard TD) ---
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]
        
        # Stage 2 update
        pe_2 = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += learning_rate * pe_2
        
        # Stage 1 update (TD(1) / Direct reinforcement)
        # Updating Stage 1 value directly from the reward received at end of trial
        pe_1 = r - q_stage1[a1]
        q_stage1[a1] += learning_rate * pe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Learning Rate Asymmetry
This model suggests that OCI affects how participants learn from positive versus negative feedback. It implements separate learning rates for positive prediction errors (learning from success) and negative prediction errors (learning from failure). The OCI score creates a bias: high OCI individuals might be hypersensitive to negative feedback (errors).

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model with Asymmetric Learning Rates modulated by OCI.
    
    Hypothesis: High OCI participants are more sensitive to negative outcomes 
    (negative prediction errors) than positive ones. The OCI score amplifies
    the learning rate for negative PE (alpha_neg).
    
    Bounds:
    alpha_pos: [0,1]
    base_alpha_neg: [0,1]
    beta: [0,10]
    mix_mb: [0,1]
    """
    alpha_pos, base_alpha_neg, beta, mix_mb = model_parameters
    n_trials = len(action_1)
    
    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    reward = reward.astype(int)
    oci_score = oci[0]

    # Transition matrix for the MB component
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    q_mf_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Calculate effective negative learning rate
    # If OCI is high, alpha_neg is boosted, bounded at 1.0
    alpha_neg = np.clip(base_alpha_neg + (0.5 * oci_score), 0.0, 1.0)

    for t in range(n_trials):
        # --- Stage 1 Decision ---
        # Model-Based Component
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_mb_stage1 = transition_matrix @ max_q_stage2
        
        # Weighted mixture of MB and MF
        q_net = (mix_mb * q_mb_stage1) + ((1 - mix_mb) * q_mf_stage1)
        
        logits_1 = beta * q_net
        logits_1 = logits_1 - np.max(logits_1)
        probs_1 = np.exp(logits_1) / np.sum(np.exp(logits_1))
        p_choice_1[t] = probs_1[action_1[t]]
        
        # --- Stage 2 Decision ---
        s_idx = state[t]
        logits_2 = beta * q_stage2[s_idx]
        logits_2 = logits_2 - np.max(logits_2)
        probs_2 = np.exp(logits_2) / np.sum(np.exp(logits_2))
        p_choice_2[t] = probs_2[action_2[t]]
        
        # --- Learning ---
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]
        
        # Stage 2 Update
        pe_2 = r - q_stage2[s_idx, a2]
        lr_2 = alpha_pos if pe_2 >= 0 else alpha_neg
        q_stage2[s_idx, a2] += lr_2 * pe_2
        
        # Stage 1 Update (TD-lambda=1 / Direct Reinforcement)
        # Using the final reward to update stage 1 MF value
        pe_1 = r - q_mf_stage1[a1]
        lr_1 = alpha_pos if pe_1 >= 0 else alpha_neg
        q_mf_stage1[a1] += lr_1 * pe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```