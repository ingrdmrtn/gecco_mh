Here are the three cognitive models based on the participant data and the OCI score.

### Cognitive Model 1: OCI-Modulated Model-Based/Model-Free Balance
This model hypothesizes that the balance between model-based (planning) and model-free (habitual) learning is directly influenced by the participant's OCI score. High OCI scores are often associated with rigid, habitual behavior. Therefore, this model uses the OCI score to weight the mixing parameter `w` towards model-free control, reducing the influence of the transition structure on decision-making.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    This model assumes a hybrid reinforcement learning agent where the balance 
    between Model-Based (MB) and Model-Free (MF) control is modulated by the OCI score.
    
    A base mixing weight `w_base` is modified by the OCI score. Higher OCI scores, 
    associated with compulsivity, are hypothesized here to reduce model-based planning
    (reducing w), leading to more habitual (model-free) choices.
    
    Parameters:
    lr: [0, 1] Learning rate for value updates.
    beta: [0, 10] Inverse temperature for softmax action selection.
    w_base: [0, 1] Base weight for Model-Based control (before OCI modulation).
    """
    lr, beta, w_base = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0] # Scalar value
    
    # OCI modulation: High OCI reduces the effective weight of model-based control.
    # We clip to ensure w stays in [0, 1].
    # If OCI is 1.0 (high), w becomes smaller (more MF). If OCI is 0, w is w_base.
    w = w_base * (1.0 - 0.5 * oci_score) 
    w = np.clip(w, 0.0, 1.0)

    # Fixed transition matrix for Model-Based evaluation
    # Spaceship A (0) -> Planet X (0) w/ 0.7 probability
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_mf = np.zeros((2, 2)) # Stage 1 MF values: [Spaceship A, Spaceship U] (Not strictly used in pure MB/MF hybrid usually, but used for TD)
    q_mb = np.zeros(2)      # Stage 1 MB values
    q_stage2 = np.zeros((2, 2)) # Stage 2 values: [Planet X, Planet Y] x [Alien 1, Alien 2]
    
    # Note: For standard hybrid models, Stage 1 Q-values are a mix.
    # We maintain a separate MF value for Stage 1 choices.
    q_stage1_mf = np.zeros(2)

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        
        # 1. Model-Based Value Calculation: V(Planet) = max(Q(Planet, Alien))
        max_q_stage2 = np.max(q_stage2, axis=1) # Max value for each planet
        q_mb = transition_matrix @ max_q_stage2 # Expected value based on transitions
        
        # 2. Integrated Q-value
        q_net = w * q_mb + (1 - w) * q_stage1_mf
        
        # Softmax for Stage 1
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        # Record probability of the chosen action
        # Action 1 is in 0.0/1.0 format, cast to int
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Decision ---
        
        s_curr = int(state[trial]) # Planet arrived at
        
        # Softmax for Stage 2 (pure MF based on Q_stage2)
        exp_q2 = np.exp(beta * q_stage2[s_curr])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial]) # Alien chosen
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning Updates ---
        
        r = reward[trial]
        
        # 1. Stage 2 Update (TD)
        # Update Q(Planet, Alien)
        pe_2 = r - q_stage2[s_curr, a2]
        q_stage2[s_curr, a2] += lr * pe_2
        
        # 2. Stage 1 MF Update (TD(1) / Sarsa-like)
        # We use the value of the state we actually arrived at (or the Q value of the second choice)
        # Standard Daw et al. 2011 uses Q(s2, a2) to update Q(s1, a1)
        pe_1 = q_stage2[s_curr, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * pe_1
        
        # Note: In some versions, there's an eligibility trace lambda. 
        # Here we assume simple TD chaining for simplicity within parameter limit.

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2: OCI-Driven Perseveration (Stickiness)
This model posits that high OCI scores manifest as "stickiness" or perseverationâ€”a tendency to repeat the previous choice regardless of reward history. Instead of modulating the complex model-based/model-free trade-off, the OCI score here directly scales a choice autocorrelation parameter (`stickiness`). A high OCI score increases the bonus added to the previously chosen action, making the agent resistant to switching even after losses.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    This model assumes a Model-Free learner with choice perseveration (stickiness).
    The magnitude of the stickiness is scaled by the OCI score.
    
    Hypothesis: High OCI participants are more likely to repeat their previous 
    Stage 1 choice regardless of the outcome (compulsive repetition).
    
    Parameters:
    lr: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    persev_base: [0, 5] Base perseveration parameter.
    """
    lr, beta, persev_base = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Effective stickiness increases with OCI
    # If OCI is high, the urge to repeat the last action is stronger.
    stickiness = persev_base * (1.0 + oci_score)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    last_action_1 = -1 # Initialize with no previous action

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        
        # Calculate logits: Q-value + Stickiness
        logits_1 = beta * q_stage1.copy()
        
        if last_action_1 != -1:
            logits_1[last_action_1] += stickiness
            
        # Softmax
        # Stability trick: subtract max to avoid overflow
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # Update tracker
        last_action_1 = a1
        
        # --- Stage 2 Decision ---
        
        s_curr = int(state[trial])
        
        # Standard softmax for stage 2
        exp_q2 = np.exp(beta * q_stage2[s_curr])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning Updates ---
        
        r = reward[trial]
        
        # Update Stage 2
        pe_2 = r - q_stage2[s_curr, a2]
        q_stage2[s_curr, a2] += lr * pe_2
        
        # Update Stage 1 (SARSA-style update using Stage 2 Q-value)
        pe_1 = q_stage2[s_curr, a2] - q_stage1[a1]
        q_stage1[a1] += lr * pe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3: OCI-Modulated Learning Rate Asymmetry
This model investigates if OCI affects how participants process positive versus negative feedback. It suggests that individuals with high OCI might be hypersensitive to punishment (losses) or hyposensitive to rewards, leading to different learning rates for positive and negative prediction errors. The OCI score acts as a ratio modifier, skewing the learning rate for negative prediction errors relative to positive ones.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    This model implements separate learning rates for positive and negative 
    prediction errors (dual-learning rate model).
    
    The OCI score modulates the ratio between learning from positive (lr_pos)
    and negative (lr_neg) outcomes.
    
    Hypothesis: High OCI might lead to 'safety behavior' or altered sensitivity 
    to lack of reward (0 coins). Here, OCI increases the learning rate for 
    negative outcomes (0 reward) relative to the base rate, reflecting 
    heightened error monitoring or anxiety about errors.
    
    Parameters:
    lr_base: [0, 1] Base learning rate for positive outcomes (Reward = 1).
    beta: [0, 10] Inverse temperature.
    neg_bias: [0, 5] Multiplier for negative learning rate scaling by OCI.
    """
    lr_base, beta, neg_bias = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective learning rates
    # lr_pos is the base rate.
    # lr_neg is modulated by OCI. If OCI is high, lr_neg increases.
    lr_pos = lr_base
    lr_neg = lr_base * (1.0 + neg_bias * oci_score)
    
    # Clip to valid range [0, 1]
    lr_neg = np.clip(lr_neg, 0.0, 1.0)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        
        # Softmax
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Decision ---
        
        s_curr = int(state[trial])
        
        exp_q2 = np.exp(beta * q_stage2[s_curr])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning Updates ---
        
        r = reward[trial]
        
        # Determine which learning rate to use for Stage 2 update
        pe_2 = r - q_stage2[s_curr, a2]
        curr_lr = lr_pos if pe_2 >= 0 else lr_neg
        
        q_stage2[s_curr, a2] += curr_lr * pe_2
        
        # Stage 1 Update
        # We use the updated Q-value from stage 2 to drive stage 1
        # This is effectively a TD(0) update
        pe_1 = q_stage2[s_curr, a2] - q_stage1[a1]
        
        # Use same LR logic for Stage 1 PE
        curr_lr_1 = lr_pos if pe_1 >= 0 else lr_neg
        q_stage1[a1] += curr_lr_1 * pe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```