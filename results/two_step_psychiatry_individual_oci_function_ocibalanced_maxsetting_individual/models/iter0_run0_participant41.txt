Here are three cognitive models formulated as Python functions. These models hypothesize different mechanisms by which Obsessive-Compulsive symptoms (OCI) might influence decision-making in the two-step task, specifically focusing on the balance between goal-directed and habitual control, perseveration, and learning rigidity.

### Cognitive Model 1: OCI-Modulated Hybrid Model
This model posits that high OCI scores are associated with a shift from Model-Based (goal-directed) to Model-Free (habitual) control. The weighting parameter `w` controls the balance between these systems, and `w` is reduced as the OCI score increases.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based / Model-Free RL model where OCI score modulates the balance (w).
    High OCI reduces the weight of the Model-Based system, leading to more habitual behavior.
    
    Parameters:
    learning_rate: [0,1] - Update rate for Q-values.
    beta: [0,10] - Inverse temperature for softmax.
    w_max: [0,1] - Maximum weight for Model-Based system (when OCI is 0).
    elig_lambda: [0,1] - Eligibility trace decay for Model-Free updates.
    """
    learning_rate, beta, w_max, elig_lambda = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # OCI modulation: w decreases as OCI increases, suppressing Model-Based control
    w = w_max * (1.0 - oci_score)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])
        
        # Handle invalid data (e.g., missing trials)
        if a2 == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # Policy for the first choice
        # Calculate Model-Based value
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combine MB and MF values based on OCI-modulated weight
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # Action value updating
        # Stage 1 TD error
        delta_stage1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Stage 2 TD error
        delta_stage2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += learning_rate * delta_stage2
        
        # Eligibility trace: update stage 1 with stage 2 prediction error
        q_stage1_mf[a1] += learning_rate * elig_lambda * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2: OCI-Driven Stickiness Model
This model assumes the participant operates primarily via a Model-Free system but exhibits "stickiness" or choice perseveration. The degree of stickiness is hypothesized to be a function of the OCI score, reflecting the compulsive repetition of actions often seen in high OCI individuals.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free RL model with OCI-modulated perseverance (stickiness).
    High OCI increases the tendency to repeat the previous Stage 1 choice regardless of value.
    
    Parameters:
    learning_rate: [0,1] - Update rate for Q-values.
    beta: [0,10] - Inverse temperature for softmax.
    stick_base: [0,5] - Baseline stickiness parameter.
    stick_oci: [0,5] - Additional stickiness scaling with OCI score.
    """
    learning_rate, beta, stick_base, stick_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Stickiness modulation: Total stickiness increases with OCI
    stickiness = stick_base + stick_oci * oci_score
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action = -1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])
        
        if a2 == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            last_action = a1
            continue

        # Policy for the first choice
        q_net = q_stage1_mf.copy()
        # Add stickiness bonus to the previously chosen action
        if last_action != -1:
            q_net[last_action] += stickiness
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # Action value updating
        delta_stage1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += learning_rate * delta_stage2
        
        # Implicit full eligibility trace (lambda=1) for strong MF connection
        q_stage1_mf[a1] += learning_rate * delta_stage2
        
        last_action = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3: OCI-Rigidity Model
This model hypothesizes that high OCI scores correlate with behavioral rigidity and reduced cognitive plasticity. This is modeled by dampening the learning rate based on the OCI score, making it harder for the agent to update values in response to new information.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free RL model where Learning Rate is modulated by OCI.
    High OCI is hypothesized to result in reduced plasticity (lower learning rate),
    causing rigidity in behavior updating.
    
    Parameters:
    lr_base: [0,1] - Base learning rate.
    beta: [0,10] - Inverse temperature.
    oci_damp: [0,1] - Dampening factor; higher values reduce learning rate more strongly with OCI.
    elig_lambda: [0,1] - Eligibility trace decay.
    """
    lr_base, beta, oci_damp, elig_lambda = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Learning rate modulation: Effective LR decreases as OCI increases
    learning_rate = lr_base * (1.0 - oci_damp * oci_score)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])
        
        if a2 == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # Policy for the first choice
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # Action value updating
        delta_stage1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += learning_rate * delta_stage2
        
        q_stage1_mf[a1] += learning_rate * elig_lambda * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```