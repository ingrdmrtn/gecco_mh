Here are three cognitive models designed to capture the behavior of a participant with high obsessive-compulsive symptoms (OCI score ~0.97) in a two-step decision task.

### Cognitive Model 1: High OCI as "Sticky" Model-Based Control
This model hypothesizes that high OCI scores correlate with a rigid adherence to a Model-Based (planning) strategy, potentially making the participant less sensitive to recent prediction errors (lower learning rates) or more "sticky" in their choices regardless of outcomes. In this specific formulation, the OCI score modulates the mixing weight `w` between Model-Based (MB) and Model-Free (MF) systems. A high OCI pushes `w` towards 1 (pure MB), reflecting a compulsive reliance on the known transition structure rather than flexible habit learning.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-modulated Model-Based Weighting.
    High OCI scores increase the weight (w) of the Model-Based system, reflecting 
    compulsive adherence to the 'correct' model structure over habit learning.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for value updates.
    - beta: [0, 10] Inverse temperature for softmax.
    - w_base: [0, 1] Baseline mixing weight (0=MF, 1=MB).
    - stickiness: [0, 5] Tendency to repeat the previous Stage 1 choice.
    """
    learning_rate, beta, w_base, stickiness = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # OCI modulates the mixing weight towards 1 (Model-Based)
    # We constrain the final w to be between 0 and 1
    w = w_base + (1.0 - w_base) * oci_score
    w = np.clip(w, 0, 1)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2) # Model-Free Stage 1 values
    q_stage2_mf = np.zeros((2, 2)) # Model-Free Stage 2 values (also used for MB calculation)
    
    prev_choice_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based Value Calculation: V_MB = T * max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value: Q_net = w * Q_MB + (1-w) * Q_MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness bonus to the previously chosen action
        choice_logits = beta * q_net
        if prev_choice_1 != -1:
            choice_logits[prev_choice_1] += stickiness
            
        exp_q1 = np.exp(choice_logits - np.max(choice_logits)) # Stability trick
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        # Store probability of observed action
        act1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[act1]
        
        # Record choice for next trial stickiness
        prev_choice_1 = act1
        
        # --- Stage 2 Policy ---
        state_idx = int(state[trial])
        act2 = int(action_2[trial])
        
        # Standard softmax on Stage 2 values
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[act2]
        
        # --- Learning Updates ---
        r = reward[trial]
        
        # Stage 2 TD Error (RPE)
        delta_stage2 = r - q_stage2_mf[state_idx, act2]
        q_stage2_mf[state_idx, act2] += learning_rate * delta_stage2
        
        # Stage 1 TD Error (SARSA-style update for MF)
        # Using the value of the state actually reached (Q_stage2 of chosen option)
        delta_stage1 = q_stage2_mf[state_idx, act2] - q_stage1_mf[act1]
        q_stage1_mf[act1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2: OCI-Driven "Perseveration"
This model posits that high OCI leads to high choice perseveration (repetitive behavior), a hallmark of compulsivity. Instead of modulating the sophisticated Model-Based/Model-Free balance, the OCI score directly scales the `stickiness` parameter. A participant with a high OCI score will have a very high tendency to repeat their previous Stage 1 choice, regardless of whether it was rewarded or not, overriding value-based learning.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-driven Perseveration.
    The OCI score scales the 'stickiness' parameter, reflecting a compulsive 
    repetition of choices independent of reward outcomes.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight (0=MF, 1=MB).
    - stick_base: [0, 5] Baseline stickiness.
    - stick_oci_mult: [0, 5] Multiplier for OCI impact on stickiness.
    """
    learning_rate, beta, w, stick_base, stick_oci_mult = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Stickiness is heavily influenced by OCI
    stickiness = stick_base + (stick_oci_mult * oci_score)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_choice_1 = -1

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        choice_logits = beta * q_net
        
        # Apply OCI-modulated stickiness
        if prev_choice_1 != -1:
            choice_logits[prev_choice_1] += stickiness
            
        exp_q1 = np.exp(choice_logits - np.max(choice_logits))
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        act1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[act1]
        prev_choice_1 = act1
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        act2 = int(action_2[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[act2]
  
        # --- Learning ---
        r = reward[trial]
        
        delta_stage2 = r - q_stage2_mf[state_idx, act2]
        q_stage2_mf[state_idx, act2] += learning_rate * delta_stage2
        
        # Note: Lambda=1 (eligibility trace) is often assumed in these tasks for MF
        # Here we use standard one-step update for simplicity but consistent with template
        delta_stage1 = q_stage2_mf[state_idx, act2] - q_stage1_mf[act1]
        q_stage1_mf[act1] += learning_rate * delta_stage1
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3: OCI-Modulated Punishment Sensitivity
This model suggests that high OCI is associated with an increased sensitivity to negative outcomes (or lack of reward), akin to "fear of failure" or checking behavior. Here, the OCI score creates an asymmetry in the learning rate. High OCI participants might learn much faster from losses (0 reward) than gains (1 reward), or specifically amplify the negative prediction error, causing them to switch strategies anxiously after failures.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Punishment Sensitivity.
    High OCI scores increase the learning rate specifically for negative prediction errors 
    (when reward is 0), reflecting anxiety or hyper-responsiveness to failure.
    
    Parameters:
    - alpha_pos: [0, 1] Learning rate for positive prediction errors (RPE > 0).
    - alpha_neg_base: [0, 1] Baseline learning rate for negative prediction errors.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight.
    """
    alpha_pos, alpha_neg_base, beta, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # High OCI amplifies learning from negative outcomes
    # We allow alpha_neg to potentially exceed alpha_pos significantly
    alpha_neg = alpha_neg_base + (0.5 * oci_score) # Scaling factor 
    alpha_neg = np.clip(alpha_neg, 0, 1)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        act1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[act1]
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        act2 = int(action_2[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[act2]
  
        # --- Learning ---
        r = reward[trial]
        
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[state_idx, act2]
        lr_2 = alpha_pos if delta_stage2 > 0 else alpha_neg
        q_stage2_mf[state_idx, act2] += lr_2 * delta_stage2
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, act2] - q_stage1_mf[act1]
        lr_1 = alpha_pos if delta_stage1 > 0 else alpha_neg
        q_stage1_mf[act1] += lr_1 * delta_stage1
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```