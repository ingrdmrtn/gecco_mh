Here are three cognitive models designed to explain the participant's behavior, incorporating their high OCI-R score (0.867) into the decision-making mechanics.

### Model 1: Hybrid Learner with OCI-Modulated Model-Based Control
This model hypothesizes that high OCI scores (compulsivity) might lead to a rigid adherence to a Model-Based strategy, or conversely, a deficit in switching between Model-Based (MB) and Model-Free (MF) control. Given the high OCI score (0.867), this model posits that the mixing weight `w` (which balances MB and MF systems) is directly influenced by the OCI score. Specifically, higher OCI might drive a more rigid, calculated (MB) approach, potentially reducing reliance on simple habit (MF).

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free learner where the mixing weight 'w' is modulated by OCI.
    
    The participant has a high OCI score (0.867), suggesting high compulsivity.
    This model tests if high OCI leads to a stronger bias towards Model-Based planning (w -> 1)
    or Model-Free habit (w -> 0). The base mixing weight is adjusted by the OCI score.

    Parameters:
    lr: [0, 1] Learning rate for value updates.
    beta: [0, 10] Inverse temperature for softmax choice.
    w_base: [0, 1] Base mixing weight between MB (1) and MF (0).
    oci_sens: [0, 1] Sensitivity of the mixing weight to the OCI score.
    """
    lr, beta, w_base, oci_sens = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0] # Scalar value
    
    # Calculate effective mixing weight w. 
    # We constrain it to [0, 1] using a sigmoid-like transformation or simple clipping.
    # Here we assume OCI pushes w towards 1 (more MB) or 0 (more MF) based on sign, 
    # but since parameters are positive, we model it as an additive factor.
    # If OCI is high, w increases (more model-based/rigid planning).
    w = w_base + (oci_sens * participant_oci)
    if w > 1.0: w = 1.0
    
    # Fixed transition matrix (A->X=0.7, U->Y=0.7)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2) # MF values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # MF values for stage 2 (2 planets, 2 aliens)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based Value: T * max(Q2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Value: w * MB + (1-w) * MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        # Store probability of observed action
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Choice ---
        s_curr = int(state[trial]) # 0 or 1
        
        # Stage 2 is purely Model-Free (no further steps)
        exp_q2 = np.exp(beta * q_stage2_mf[s_curr])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        r = reward[trial]
        
        # Update Stage 2 Q-values (Standard TD)
        # Q2(s, a2) += lr * (r - Q2(s, a2))
        pe_2 = r - q_stage2_mf[s_curr, a2]
        q_stage2_mf[s_curr, a2] += lr * pe_2
        
        # Update Stage 1 MF Q-values (TD(1) / SARSA-like)
        # Q1(a1) += lr * (Q2(s, a2) - Q1(a1)) + lr * lambda * pe_2 
        # Simplified: standard TD update using the value of the state reached
        # Note: In standard Daw model, Stage 1 MF is updated via Q(s2, a2)
        pe_1 = q_stage2_mf[s_curr, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * pe_1
        
        # Note: We also apply the stage 2 prediction error to stage 1 (eligibility trace = 1)
        q_stage1_mf[a1] += lr * pe_2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Driven Perseveration (Stickiness)
High OCI scores are often associated with repetitive behaviors or "stickiness" (perseveration). This model modifies the standard hybrid model by adding a perseveration parameter (stickiness) to the first-stage choice. Crucially, the magnitude of this stickiness is scaled by the participant's OCI score. The hypothesis is that this participant (OCI > 0.66) will exhibit a much stronger tendency to repeat the previous Stage 1 choice regardless of reward outcomes.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid learner with choice perseveration (stickiness) scaled by OCI.
    
    The high OCI score (0.867) is hypothesized to increase 'stickiness' 
    (tendency to repeat the previous Stage 1 action), representing compulsive repetition.
    
    Parameters:
    lr: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Mixing weight (fixed parameter, not OCI modulated here to isolate stickiness).
    stickiness_base: [0, 5] Base tendency to repeat choice.
    oci_sticky_mod: [0, 5] Additional stickiness multiplier based on OCI.
    """
    lr, beta, w, stickiness_base, oci_sticky_mod = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]

    # Calculate total stickiness bonus
    # Effective stickiness = Base + (Mod * OCI)
    # High OCI -> Higher bonus for repeating the previous action
    stickiness_bonus = stickiness_base + (oci_sticky_mod * participant_oci)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1 # No previous action for first trial

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness to the Q-values before softmax
        q_net_sticky = q_net.copy()
        if prev_a1 != -1:
            q_net_sticky[prev_a1] += stickiness_bonus
            
        exp_q1 = np.exp(beta * q_net_sticky)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Choice ---
        s_curr = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[s_curr])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        r = reward[trial]
        
        # Stage 2 Update
        pe_2 = r - q_stage2_mf[s_curr, a2]
        q_stage2_mf[s_curr, a2] += lr * pe_2
        
        # Stage 1 MF Update (TD(1))
        pe_1 = q_stage2_mf[s_curr, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * pe_1
        q_stage1_mf[a1] += lr * pe_2
        
        # Update previous action
        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Learning Rate Asymmetry
This model investigates if the participant's high OCI score affects how they learn from positive versus negative prediction errors. Compulsive individuals might be hyper-sensitive to errors (negative outcomes) or punishment. Here, we split the learning rate into positive (`lr_pos`) and negative (`lr_neg`) components, but we constrain the *negative* learning rate to be a function of the OCI score. This tests if the high OCI score drives faster learning/avoidance after non-rewarded trials.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Pure Model-Free learner where learning rates for negative prediction errors 
    are modulated by OCI.
    
    Hypothesis: High OCI (0.867) implies anxiety/compulsivity, potentially leading 
    to over-reaction to lack of reward (0 outcomes). The learning rate for 
    negative PEs (lr_neg) is boosted by the OCI score.
    
    Parameters:
    lr_pos: [0, 1] Learning rate for positive prediction errors (Reward > Expectation).
    lr_neg_base: [0, 1] Base learning rate for negative prediction errors.
    oci_neg_mod: [0, 1] Multiplier for OCI impact on negative learning rate.
    beta: [0, 10] Inverse temperature.
    """
    lr_pos, lr_neg_base, oci_neg_mod, beta = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]
    
    # Calculate effective negative learning rate
    # lr_neg = Base + (Mod * OCI)
    lr_neg = lr_neg_base + (oci_neg_mod * participant_oci)
    if lr_neg > 1.0: lr_neg = 1.0

    # We use a pure Model-Free setup (w=0 implicitly) to isolate the learning rate effect.
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        # --- Stage 1 Choice (Pure MF) ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Choice ---
        s_curr = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2[s_curr])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        r = reward[trial]
        
        # Stage 2 PE
        pe_2 = r - q_stage2[s_curr, a2]
        
        # Select LR based on sign of PE
        if pe_2 >= 0:
            eff_lr_2 = lr_pos
        else:
            eff_lr_2 = lr_neg
            
        q_stage2[s_curr, a2] += eff_lr_2 * pe_2
        
        # Stage 1 PE (TD(1) logic)
        # PE1 is driven by the value of the state reached plus the second stage PE
        pe_1 = q_stage2[s_curr, a2] - q_stage1[a1] # This is technically part of the update
        
        # For simplicity in this specific MF model, we just propagate the total error back
        # Total error for Stage 1 = (Reward + Q_stage2_future - Q_stage2_current) + (Q_stage2_current - Q_stage1)
        # Simplified to: Reward - Q_stage1 (ignoring discounting for this task usually)
        # But to keep consistent with Two-Step logic, we usually sum the PEs.
        
        total_pe_1 = pe_1 + pe_2 # TD(1) style
        
        if total_pe_1 >= 0:
            eff_lr_1 = lr_pos
        else:
            eff_lr_1 = lr_neg
            
        q_stage1[a1] += eff_lr_1 * total_pe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```