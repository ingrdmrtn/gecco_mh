Here are three cognitive models that incorporate the OCI (Obsessive-Compulsive Inventory) score into the decision-making process. The participant has a high OCI score (0.7), suggesting they fall into the "high obsessive-compulsive symptoms" category.

The models focus on how this high OCI score might influence the balance between Model-Based (planning) and Model-Free (habitual) control, or affect learning rates and stickiness.

### Cognitive Model 1: OCI-Modulated MB/MF Balance
This model hypothesizes that high OCI symptoms correlate with a rigidity or reliance on habits (Model-Free) over flexible planning (Model-Based), or vice versa. Here, the `w` parameter (mixing weight) is directly modulated by the OCI score. A base mixing weight is adjusted by the OCI score to determine the final balance.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-modulated Model-Based/Model-Free Balance.
    
    This model assumes that the OCI score influences the balance (w) between 
    Model-Based (goal-directed) and Model-Free (habitual) control.
    
    Parameters:
    lr: [0, 1] Learning rate for value updates.
    beta: [0, 10] Inverse temperature for softmax choice.
    w_base: [0, 1] Base weight for Model-Based control (before OCI modulation).
    oci_sensitivity: [0, 1] Strength of OCI influence on the mixing weight.
    """
    lr, beta, w_base, oci_sensitivity = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective mixing weight w based on OCI
    # If oci_sensitivity is positive, higher OCI shifts w. 
    # We constrain w to be between 0 and 1.
    # Hypothesis: High OCI might reduce MB control (w becomes smaller).
    # Formula: w = w_base * (1 - oci_sensitivity * oci_score)
    w = w_base * (1.0 - (oci_sensitivity * oci_score))
    w = np.clip(w, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2) # Model-free values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for stage 2 (2 planets, 2 aliens)

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # Model-Based calculation: V(state) = max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Q-value: weighted sum of MB and MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        # Store probability of the chosen action
        # Inputs are floats, convert to int for indexing
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # Transition to state (planet)
        s_idx = int(state[trial])

        # --- Stage 2 Decision ---
        # Standard Model-Free choice at Stage 2
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning / Updating ---
        r = reward[trial]
        
        # Prediction errors
        # Stage 2 RPE
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        # Stage 1 RPE (SARSA-like: using Q_stage2 of chosen action)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        
        # Update values
        q_stage1_mf[a1] += lr * delta_stage1
        q_stage2_mf[s_idx, a2] += lr * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2: OCI-Driven Perseveration (Stickiness)
High OCI scores often relate to repetitive behaviors or difficulty switching sets. This model introduces a "stickiness" parameter that biases the agent to repeat the previous Stage 1 choice. The magnitude of this stickiness is scaled by the OCI score.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-driven Perseveration (Stickiness).
    
    This model assumes that OCI symptoms drive repetitive behavior (stickiness).
    The choice at Stage 1 is biased by the previous choice, and the strength 
    of this bias is determined by the OCI score.
    
    Parameters:
    lr: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Mixing weight (MB vs MF).
    k_oci: [0, 5] Stickiness scaling factor applied to OCI score.
    """
    lr, beta, w, k_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate stickiness bonus based on OCI
    stickiness_bonus = k_oci * oci_score

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1 # Initialize previous action placeholder

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combined value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness bonus to the previously chosen action
        # We modify the Q-values temporarily for softmax calculation
        q_net_stick = q_net.copy()
        if last_action_1 != -1:
            q_net_stick[last_action_1] += stickiness_bonus

        exp_q1 = np.exp(beta * q_net_stick)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # Update tracker for next trial
        last_action_1 = a1
        
        s_idx = int(state[trial])

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        r = reward[trial]
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        
        q_stage1_mf[a1] += lr * delta_stage1
        q_stage2_mf[s_idx, a2] += lr * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3: OCI-Specific Negative Learning Rate
This model posits that individuals with high OCI symptoms might be hypersensitive to negative feedback (omission of reward) or "mistakes." Instead of a single learning rate, this model splits learning into positive (reward) and negative (no reward) updates. The negative learning rate is specifically modulated by the OCI score, suggesting that high OCI leads to stronger avoidance learning or stronger reaction to failure.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Specific Negative Learning Rate (Asymmetric Learning).
    
    This model assumes high OCI leads to hypersensitivity to negative outcomes.
    The learning rate for negative prediction errors (lr_neg) is scaled by OCI.
    
    Parameters:
    lr_pos: [0, 1] Learning rate for positive prediction errors (Reward = 1).
    lr_neg_base: [0, 1] Base learning rate for negative prediction errors.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Mixing weight.
    oci_amp: [0, 5] Amplification factor for negative learning based on OCI.
    """
    lr_pos, lr_neg_base, beta, w, oci_amp = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective negative learning rate
    # Bounded at 1.0 to prevent instability
    lr_neg = lr_neg_base * (1.0 + oci_amp * oci_score)
    lr_neg = np.clip(lr_neg, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        s_idx = int(state[trial])

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning with Asymmetric Rates ---
        r = reward[trial]
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        
        # Apply specific learning rate based on sign of prediction error
        # Note: In this task, reward is 0 or 1.
        # If Reward=1, PE is usually positive (unless Q > 1, unlikely).
        # If Reward=0, PE is negative.
        # We simplify by using reward outcome to pick LR, as is common in these models.
        
        current_lr = lr_pos if r > 0 else lr_neg
        
        q_stage1_mf[a1] += current_lr * delta_stage1
        q_stage2_mf[s_idx, a2] += current_lr * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```