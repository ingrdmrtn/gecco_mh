Here are three new cognitive models based on the provided template and participant data. These models explore different mechanisms for how OCI might modulate learning and decision-making, specifically focusing on the balance between model-based/model-free control, learning rates, and exploration.

### Model 1: Hybrid Learner with OCI-modulated Mixing Weight
This model assumes agents use a hybrid of Model-Based (MB) and Model-Free (MF) strategies. The core hypothesis is that OCI symptoms might relate to the reliance on habit (MF) versus goal-directed (MB) planning. Since the participant has a low OCI score (0.3), this model tests if the mixing weight `w` (where w=1 is pure MB, w=0 is pure MF) is modulated by OCI.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free learner where the mixing weight is modulated by OCI.
    
    Hypothesis: The balance between goal-directed (MB) and habitual (MF) control is
    influenced by OCI traits. We model the mixing weight 'w' as a logistic function
    of a base parameter shifted by OCI.
    
    Parameters:
    learning_rate: [0, 1] Learning rate for value updates.
    beta: [0, 10] Inverse temperature for softmax choice.
    w_slope: [0, 5] Sensitivity of the mixing weight to OCI.
    w_intercept: [0, 1] Base mixing weight before OCI modulation.
    """
    learning_rate, beta, w_slope, w_intercept = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate mixing weight w bounded between 0 and 1
    # We use a simple linear modulation clipped to [0,1]
    w = w_intercept + w_slope * (oci_score - 0.5) 
    w = np.clip(w, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)      # MF values for spaceships
    q_stage2_mf = np.zeros((2, 2)) # MF values for aliens/planets

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        # Model-Based Value: T * max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value: w * MB + (1-w) * MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updating ---
        # Prediction error for Stage 1 (MF only)
        # Note: In standard hybrid models, MF Q1 is updated via TD(0) or TD(1). 
        # Here we use a simple TD(1)-like update where r propagates back.
        # But to fit the template structure of separate updates:
        
        # Update Stage 2 values (MF)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Update Stage 1 values (MF) using the reward directly (TD(1) style for simplicity here)
        # or using the stage 2 value (TD(0)). Let's use TD(1) as it's robust.
        delta_stage1 = r - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    valid_trials = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```

### Model 2: Model-Based with OCI-modulated Learning Rate (Stress/Hyper-learning)
This model posits that OCI affects the *rate* of learning rather than the strategy. High OCI might lead to "over-updating" or hyper-vigilance to feedback (high learning rate), or rigidity (low learning rate). Given the low OCI score here, this model allows the learning rate to scale relative to the OCI score. It assumes a purely Model-Based strategy for stage 1 planning, as this is often the dominant strategy in healthy controls, but allows the valuation of the second stage to fluctuate based on `lr`.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Pure Model-Based learner where the learning rate is modulated by OCI.
    
    Hypothesis: OCI score impacts the speed of belief updating (learning rate).
    We model the effective learning rate as a base rate multiplied by an OCI factor.
    
    Parameters:
    lr_base: [0, 1] Base learning rate.
    beta: [0, 10] Inverse temperature.
    lr_mod: [0, 2] Modulation factor for OCI. lr = lr_base * (1 + lr_mod * OCI).
    """
    lr_base, beta, lr_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Effective learning rate calculation
    learning_rate = lr_base * (1.0 + lr_mod * oci_score)
    learning_rate = np.clip(learning_rate, 0.0, 1.0) # Ensure bounds

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Only need Stage 2 values for a pure MB agent to calculate expected value
    q_stage2_mf = np.zeros((2, 2)) 

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy (Pure Model-Based) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updating ---
        # Update Stage 2 values based on reward
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # No Stage 1 MF update needed for pure MB

    eps = 1e-10
    valid_trials = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```

### Model 3: Model-Based with OCI-modulated Inverse Temperature (Exploration/Anxiety)
This model investigates the relationship between OCI and decision noise (exploration). It is often hypothesized that anxiety or compulsivity (related to OCI) might lead to more deterministic (rigid) behavior (higher beta) or conversely, erratic behavior under uncertainty. Here, we model `beta` (inverse temperature) as a function of OCI. The model uses a standard Model-Based planning approach.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Based learner where the inverse temperature (beta) is modulated by OCI.
    
    Hypothesis: OCI scores relate to decision rigidity or noise. A higher OCI might
    imply higher exploitation (higher beta) or higher noise.
    
    Parameters:
    learning_rate: [0, 1] Learning rate.
    beta_base: [0, 10] Base inverse temperature.
    beta_oci_slope: [-5, 5] How OCI changes beta. beta = beta_base + slope * OCI.
    """
    learning_rate, beta_base, beta_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective beta
    beta = beta_base + beta_oci_slope * oci_score
    beta = np.maximum(beta, 0.0) # Ensure non-negative beta

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy (Model-Based) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updating ---
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    valid_trials = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```