Here are three cognitive models that explain the participant's behavior in the two-step task, incorporating the OCI-R score to modulate specific cognitive mechanisms.

### Model 1: Hybrid Learner with OCI-Modulated Model-Based Weight
This model hypothesizes that the balance between Goal-Directed (Model-Based) and Habitual (Model-Free) control is a function of the participant's obsessiveness. Higher OCI scores may shift the participant towards a more habitual (Model-Free) strategy or a less flexible Model-Based strategy.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Reinforcement Learning model where the weighting parameter 'w'
    (arbitration between Model-Based and Model-Free values) is modulated by OCI.
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    w_base: [0, 1]
    w_oci_mod: [-1, 1]
    """
    learning_rate, beta, w_base, w_oci_mod = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
  
    # Calculate effective weight w based on OCI
    w = w_base + (w_oci_mod * current_oci)
    w = np.clip(w, 0.0, 1.0)

    # Fixed transition matrix as per task structure (70% common, 30% rare)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # 2 states (Planets), 2 actions (Aliens)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based Value: Bellman equation using known transitions and max Stage 2 values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value: Weighted sum of MB and MF values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        if action_1[trial] != -1:
            p_choice_1[trial] = probs_1[int(action_1[trial])]
        else:
            p_choice_1[trial] = 1.0
            continue # Skip update if data is missing
            
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        if action_2[trial] != -1:
            a2 = int(action_2[trial])
            p_choice_2[trial] = probs_2[a2]
            
            # --- Updates ---
            # Stage 1 MF Update (TD(0))
            delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
            q_stage1_mf[a1] += learning_rate * delta_stage1
            
            # Stage 2 MF Update
            delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
            q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
            
            # Note: Model-Based values update implicitly because they depend on q_stage2_mf
            
        else:
            p_choice_2[trial] = 1.0

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Learner with OCI-Modulated Stickiness
This model investigates whether OCI is linked to "stickiness" or perseveration (the tendency to repeat the previous choice regardless of value). It adds a bonus to the previously chosen action's value, with the magnitude of this bonus determined by the OCI score.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid model with a choice stickiness (perseveration) parameter that is
    modulated by OCI. Stickiness adds a bonus to the logits of the repeated action.
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    w: [0, 1]
    stick_base: [0, 5]
    stick_oci_mod: [-1, 1]
    """
    learning_rate, beta, w, stick_base, stick_oci_mod = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
  
    # Calculate effective stickiness
    stickiness = stick_base + (stick_oci_mod * current_oci)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Calculate logits
        logits = beta * q_net
        
        # Add stickiness bonus if this isn't the first trial and previous choice exists
        if trial > 0 and action_1[trial-1] != -1:
            prev_a1 = int(action_1[trial-1])
            logits[prev_a1] += stickiness
            
        exp_q1 = np.exp(logits - np.max(logits)) # Subtract max for stability
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        if action_1[trial] != -1:
            p_choice_1[trial] = probs_1[int(action_1[trial])]
        else:
            p_choice_1[trial] = 1.0
            continue
            
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # --- Stage 2 Policy ---
        # No stickiness modeled for stage 2 (alien choice) to keep model focused
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        if action_2[trial] != -1:
            a2 = int(action_2[trial])
            p_choice_2[trial] = probs_2[a2]
            
            # --- Updates ---
            delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
            q_stage1_mf[a1] += learning_rate * delta_stage1
            
            delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
            q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
            
        else:
            p_choice_2[trial] = 1.0

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Model-Free Learner with OCI-Modulated Eligibility Trace
This model assumes a pure Model-Free strategy (no transition matrix usage) but focuses on temporal credit assignment. The "eligibility trace" (lambda) determines how strongly the second-stage reward updates the first-stage choice. The OCI score modulates this parameter, testing if compulsivity relates to how efficiently distant outcomes reinforce initial actions.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner where the eligibility trace (lambda) is modulated by OCI.
    This controls the extent to which the Stage 2 outcome reinforces the Stage 1 choice.
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    lambda_base: [0, 1]
    lambda_oci_mod: [-1, 1]
    """
    learning_rate, beta, lambda_base, lambda_oci_mod = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
  
    # Calculate effective eligibility trace
    eligibility_trace = lambda_base + (lambda_oci_mod * current_oci)
    eligibility_trace = np.clip(eligibility_trace, 0.0, 1.0)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy (Pure Model-Free) ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        if action_1[trial] != -1:
            p_choice_1[trial] = probs_1[int(action_1[trial])]
        else:
            p_choice_1[trial] = 1.0
            continue
            
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        if action_2[trial] != -1:
            a2 = int(action_2[trial])
            p_choice_2[trial] = probs_2[a2]
            
            # --- Updates (SARSA(lambda) style) ---
            
            # 1. Prediction error at Stage 1 (TD(0) update)
            # The value of the chosen state (Q2) acts as the reward for stage 1
            delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
            q_stage1_mf[a1] += learning_rate * delta_stage1
            
            # 2. Prediction error at Stage 2
            delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
            q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
            
            # 3. Eligibility Trace update
            # The Stage 2 prediction error also updates Stage 1 value, scaled by lambda
            q_stage1_mf[a1] += learning_rate * eligibility_trace * delta_stage2
            
        else:
            p_choice_2[trial] = 1.0

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```