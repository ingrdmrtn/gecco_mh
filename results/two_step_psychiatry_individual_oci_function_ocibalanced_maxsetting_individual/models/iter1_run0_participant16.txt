Here are three new cognitive models for the two-step task, incorporating the OCI score into different mechanisms of the reinforcement learning process.

### Model 1: OCI-Modulated Learning Rate
This model hypothesizes that the severity of OCI symptoms affects the global volatility of belief updating. It interpolates the learning rate between a low and high value based on the OCI score.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model where OCI modulates the Learning Rate (Alpha).
    Hypothesis: OCI severity correlates with the speed of belief updating (volatility).
    
    Parameters:
    - alpha_low: [0, 1] Learning rate when OCI is 0.
    - alpha_high: [0, 1] Learning rate when OCI is 1.
    - beta_1: [0, 10] Inverse temperature Stage 1.
    - beta_2: [0, 10] Inverse temperature Stage 2.
    - w: [0, 1] Mixing weight (0=MF, 1=MB).
    - stickiness_param: [0, 1] Transformed to [-5, 5] for choice persistence.
    """
    alpha_low, alpha_high, beta_1, beta_2, w, stickiness_param = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Interpolate learning rate based on OCI
    learning_rate = alpha_low * (1 - current_oci) + alpha_high * current_oci
    
    # Transform stickiness to range [-5, 5]
    stickiness = (stickiness_param - 0.5) * 10.0
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # state x action
    
    last_action_1 = -1
    log_likelihood = 0.0
    eps = 1e-10

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_likelihood += np.log(probs_1[a1] + eps)
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta_2 * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_likelihood += np.log(probs_2[a2] + eps)
        
        # --- Updates ---
        # Stage 2 RPE
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 MF Update (TD(0))
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        last_action_1 = a1

    return -log_likelihood
```

### Model 2: OCI-Modulated Eligibility Trace (Lambda)
This model hypothesizes that OCI symptoms affect the credit assignment processâ€”specifically, how strongly the final outcome (reward) reinforces the initial choice (spaceship) via the eligibility trace parameter $\lambda$.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model where OCI modulates the Eligibility Trace (Lambda).
    Hypothesis: OCI affects how much the Stage 2 RPE directly updates the Stage 1 choice value.
    
    Parameters:
    - learning_rate: [0, 1] Base learning rate.
    - beta_1: [0, 10] Inverse temperature Stage 1.
    - beta_2: [0, 10] Inverse temperature Stage 2.
    - w: [0, 1] Mixing weight.
    - stickiness_param: [0, 1] Transformed to [-5, 5].
    - lambda_low: [0, 1] Eligibility trace when OCI is 0.
    - lambda_high: [0, 1] Eligibility trace when OCI is 1.
    """
    learning_rate, beta_1, beta_2, w, stickiness_param, lambda_low, lambda_high = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Interpolate lambda based on OCI
    lam = lambda_low * (1 - current_oci) + lambda_high * current_oci
    stickiness = (stickiness_param - 0.5) * 10.0
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    log_likelihood = 0.0
    eps = 1e-10

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_likelihood += np.log(probs_1[a1] + eps)
        
        # Stage 2 Policy
        exp_q2 = np.exp(beta_2 * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_likelihood += np.log(probs_2[a2] + eps)
        
        # Updates
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        # Stage 1 Update: TD(0) + TD(1) component weighted by lambda
        q_stage1_mf[a1] += learning_rate * (q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]) + \
                           learning_rate * lam * delta_stage2
                           
        # Stage 2 Update
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        last_action_1 = a1

    return -log_likelihood
```

### Model 3: OCI-Modulated Asymmetric Learning
This model hypothesizes that OCI creates a bias in learning from negative outcomes (disappointment/failure) versus positive outcomes. It uses separate learning rates for positive and negative prediction errors, with the negative rate modulated by OCI.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model with Asymmetric Learning Rates modulated by OCI.
    Hypothesis: OCI determines the sensitivity to negative prediction errors (learning from failure).
    
    Parameters:
    - lr_pos: [0, 1] Learning rate for positive RPEs.
    - lr_neg_low: [0, 1] Learning rate for negative RPEs when OCI is 0.
    - lr_neg_high: [0, 1] Learning rate for negative RPEs when OCI is 1.
    - beta_1: [0, 10] Stage 1 beta.
    - beta_2: [0, 10] Stage 2 beta.
    - w: [0, 1] Mixing weight.
    - stickiness_param: [0, 1] Transformed to [-5, 5].
    """
    lr_pos, lr_neg_low, lr_neg_high, beta_1, beta_2, w, stickiness_param = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Interpolate negative learning rate based on OCI
    lr_neg = lr_neg_low * (1 - current_oci) + lr_neg_high * current_oci
    stickiness = (stickiness_param - 0.5) * 10.0
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    log_likelihood = 0.0
    eps = 1e-10

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Stage 1 Choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_likelihood += np.log(probs_1[a1] + eps)
        
        # Stage 2 Choice
        exp_q2 = np.exp(beta_2 * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_likelihood += np.log(probs_2[a2] + eps)
        
        # Updates with asymmetric learning rates
        
        # Stage 2 RPE
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        alpha_2 = lr_pos if delta_stage2 >= 0 else lr_neg
        q_stage2_mf[s_idx, a2] += alpha_2 * delta_stage2
        
        # Stage 1 RPE (TD(0))
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        alpha_1 = lr_pos if delta_stage1 >= 0 else lr_neg
        q_stage1_mf[a1] += alpha_1 * delta_stage1
        
        last_action_1 = a1

    return -log_likelihood
```