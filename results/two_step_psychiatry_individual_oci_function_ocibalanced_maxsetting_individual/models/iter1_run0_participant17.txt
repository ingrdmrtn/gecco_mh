Here are three new cognitive models exploring different mechanisms for how OCI scores might influence reinforcement learning in this two-step task.

### Model 1: Balanced MB/MF with OCI-Modulated Mixing Weight
This model hypothesizes that obsessive-compulsive traits influence the balance between Model-Based (goal-directed) and Model-Free (habitual) control. Specifically, it tests if higher OCI scores shift the mixing weight `w` towards one system or the other.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Balanced Model-Based/Model-Free Hybrid with OCI-modulated mixing weight.
    
    This model posits that the balance between goal-directed (MB) and habitual (MF)
    control is a function of the participant's OCI score. The mixing weight 'w'
    is not a free parameter but is derived from a base weight and the OCI score.
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    w_base: [0, 1] - Baseline mixing weight (MB contribution).
    oci_sensitivity: [0, 1] - How strongly OCI pulls 'w' away from baseline.
    """
    learning_rate, beta, w_base, oci_sensitivity = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate mixing weight w based on OCI
    # If oci_sensitivity is high, w is pulled strongly by the OCI score.
    # We model this as a linear shift: w = w_base + oci_sensitivity * (oci_score - 0.5)
    # The 0.5 centers the effect around the middle of the potential OCI range.
    # We clip w to be between 0 and 1.
    w_raw = w_base + oci_sensitivity * (oci_score - 0.5)
    w = np.clip(w_raw, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Weighted mixture of MB and MF values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        s1 = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[s1])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # --- Learning Updates ---
        r = reward[trial]
        
        # MF Stage 1 update (SARSA-like using Stage 2 value)
        delta_stage1 = q_stage2_mf[s1, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] = q_stage1_mf[a1] + learning_rate * delta_stage1

        # MF Stage 2 update
        delta_stage2 = r - q_stage2_mf[s1, a2]
        q_stage2_mf[s1, a2] = q_stage2_mf[s1, a2] + learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Driven Learning Rate Asymmetry
This model suggests that OCI symptoms might relate to how participants learn from positive versus negative outcomes. Specifically, it allows the learning rate for negative prediction errors (disappointments) to differ from positive ones, with the magnitude of this difference scaled by the OCI score.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free RL with OCI-modulated Learning Rate Asymmetry.
    
    This model tests if OCI scores correlate with an imbalance in learning from 
    positive vs. negative prediction errors. The learning rate is split into 
    alpha_pos and alpha_neg, where the split is determined by an asymmetry 
    parameter scaled by the OCI score.
    
    Bounds:
    alpha_base: [0, 1] - Base learning rate.
    beta: [0, 10] - Inverse temperature.
    asymmetry_scale: [0, 1] - How much OCI scales the difference between alpha+ and alpha-.
    """
    alpha_base, beta, asymmetry_scale = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate positive and negative learning rates
    # If asymmetry_scale is high and OCI is high, alpha_neg becomes larger than alpha_pos
    # This simulates a "negativity bias" or over-sensitivity to failure common in anxiety/OCD.
    bias = asymmetry_scale * oci_score
    alpha_pos = np.clip(alpha_base - bias, 0.01, 1.0)
    alpha_neg = np.clip(alpha_base + bias, 0.01, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Policy (Pure MF for simplicity to isolate learning rate effect) ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        s1 = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[s1])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # --- Learning Updates ---
        r = reward[trial]
        
        # MF Stage 1 update
        delta_stage1 = q_stage2_mf[s1, a2] - q_stage1_mf[a1]
        lr_1 = alpha_pos if delta_stage1 >= 0 else alpha_neg
        q_stage1_mf[a1] = q_stage1_mf[a1] + lr_1 * delta_stage1

        # MF Stage 2 update
        delta_stage2 = r - q_stage2_mf[s1, a2]
        lr_2 = alpha_pos if delta_stage2 >= 0 else alpha_neg
        q_stage2_mf[s1, a2] = q_stage2_mf[s1, a2] + lr_2 * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Eligibility Trace (Lambda)
This model investigates if OCI scores influence the "eligibility trace" parameter ($\lambda$). In reinforcement learning, $\lambda$ controls how much credit for a reward is assigned to the first-stage choice versus the second-stage choice. A higher $\lambda$ links the outcome more strongly back to the initial decision.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    TD(lambda) Model with OCI-modulated eligibility trace.
    
    This model assumes the participant uses Temporal Difference learning with 
    eligibility traces (TD(lambda)). The decay parameter lambda is not fixed 
    but is modulated by the OCI score. This tests if higher OCI relates to 
    attributing outcomes more strongly to distant past actions (high lambda).
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    lambda_base: [0, 1] - Base eligibility trace decay.
    lambda_oci_mod: [-1, 1] - Modulation factor for OCI on lambda.
    """
    learning_rate, beta, lambda_base, lambda_oci_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate lambda based on OCI
    # We allow lambda to increase or decrease with OCI based on the sign of lambda_oci_mod
    lambda_param = lambda_base + (lambda_oci_mod * oci_score)
    lambda_param = np.clip(lambda_param, 0.0, 1.0)

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        s1 = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[s1])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # --- Learning Updates (TD(lambda) logic) ---
        r = reward[trial]
        
        # Prediction error at stage 1
        # Note: In standard TD(lambda) for this task, the stage 1 value is updated 
        # based on stage 2 value, and then stage 2 PE propagates back.
        
        # 1. Update Stage 1 based on transition to Stage 2
        delta_1 = q_stage2_mf[s1, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] = q_stage1_mf[a1] + learning_rate * delta_1
        
        # 2. Update Stage 2 based on reward
        delta_2 = r - q_stage2_mf[s1, a2]
        q_stage2_mf[s1, a2] = q_stage2_mf[s1, a2] + learning_rate * delta_2
        
        # 3. Eligibility trace update: Propagate stage 2 prediction error back to stage 1
        # The strength of this update depends on lambda_param
        q_stage1_mf[a1] = q_stage1_mf[a1] + learning_rate * lambda_param * delta_2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```