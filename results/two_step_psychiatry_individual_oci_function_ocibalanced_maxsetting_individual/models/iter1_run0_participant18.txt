Here are three cognitive models designed to explain the participant's behavior, incorporating their OCI-R score into the decision-making mechanisms.

### Model 1: OCI-Modulated Post-Error Perseveration
This model hypothesizes that higher OCI scores lead to increased "stickiness" or perseveration specifically after a negative outcome (frustrative non-reward). It posits that the participant is more likely to compulsively repeat a choice if it failed to yield a reward, attempting to "correct" the outcome.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Post-Error Perseveration.
    Hypothesis: Higher OCI scores lead to increased stickiness (repetition of stage 1 choice)
    specifically after a non-rewarded trial (frustrative non-reward).
    
    Parameters:
    - lr: Learning rate [0,1]
    - beta: Inverse temperature [0,10]
    - w: Model-based weight [0,1]
    - lambda_val: Eligibility trace [0,1]
    - stickiness: Base perseveration parameter [0,5]
    """
    lr, beta, w, lambda_val, stickiness = model_parameters
    oci_score = oci[0]
    n_trials = len(action_1)
    
    # Transition matrix: Row 0 -> [0.7, 0.3], Row 1 -> [0.3, 0.7]
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_mf_s1 = np.zeros(2)
    q_mf_s2 = np.zeros((2, 2)) # state, action
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1
    prev_reward = 0 
    
    for t in range(n_trials):
        # Skip missing data
        if action_1[t] == -1:
            p_choice_1[t] = 1.0
            p_choice_2[t] = 1.0
            continue
            
        a1 = int(action_1[t])
        s2 = int(state[t])
        a2 = int(action_2[t])
        r = reward[t]
        
        # --- Stage 1 Choice Probability ---
        # Model-Based Value
        max_q_s2 = np.max(q_mf_s2, axis=1)
        q_mb_s1 = transition_matrix @ max_q_s2
        
        # Net Value
        q_net_s1 = w * q_mb_s1 + (1 - w) * q_mf_s1
        
        logits_1 = beta * q_net_s1
        
        # Apply Stickiness
        if prev_a1 != -1:
            # Calculate effective stickiness
            # If previous reward was 0, OCI amplifies stickiness
            if prev_reward == 0:
                eff_stick = stickiness * (1.0 + oci_score)
            else:
                eff_stick = stickiness
            
            logits_1[prev_a1] += eff_stick
            
        # Softmax Stage 1
        exp_1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_1 / np.sum(exp_1)
        p_choice_1[t] = probs_1[a1]
        
        # --- Stage 2 Choice Probability ---
        logits_2 = beta * q_mf_s2[s2]
        exp_2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_2 / np.sum(exp_2)
        p_choice_2[t] = probs_2[a2]
        
        # --- Learning Updates ---
        # Stage 1 PE
        delta_1 = q_mf_s2[s2, a2] - q_mf_s1[a1]
        q_mf_s1[a1] += lr * delta_1
        
        # Stage 2 PE
        delta_2 = r - q_mf_s2[s2, a2]
        q_mf_s2[s2, a2] += lr * delta_2
        
        # Eligibility Trace Update for Stage 1
        q_mf_s1[a1] += lr * lambda_val * delta_2
        
        prev_a1 = a1
        prev_reward = r

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Rare Transition Deficit
This model suggests that high OCI scores relate to intolerance of uncertainty or structural violations. When a "rare" transition occurs (e.g., Spaceship A going to Planet Y), the participant's trust in the Model-Based system is temporarily shaken. On the subsequent trial, the weight of Model-Based control ($w$) is reduced proportional to their OCI score, causing a reversion to habitual (Model-Free) control.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Modulated Rare Transition Deficit.
    Hypothesis: High OCI scores relate to intolerance of uncertainty. When a rare transition 
    occurs (outcome contradicts the model), the Model-Based weight is suppressed on the 
    subsequent trial, reverting to Model-Free habits.
    
    Parameters:
    - lr: Learning rate [0,1]
    - beta: Inverse temperature [0,10]
    - w_base: Base Model-based weight [0,1]
    - lambda_val: Eligibility trace [0,1]
    - stickiness: Perseveration parameter [0,5]
    """
    lr, beta, w_base, lambda_val, stickiness = model_parameters
    oci_score = oci[0]
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_mf_s1 = np.zeros(2)
    q_mf_s2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1
    prev_s2 = -1
    
    for t in range(n_trials):
        if action_1[t] == -1:
            p_choice_1[t] = 1.0
            p_choice_2[t] = 1.0
            continue
            
        a1 = int(action_1[t])
        s2 = int(state[t])
        a2 = int(action_2[t])
        r = reward[t]
        
        # Determine effective w based on previous transition
        current_w = w_base
        if prev_a1 != -1 and prev_s2 != -1:
            # Check if previous transition was rare
            # Common: 0->0, 1->1. Rare: 0->1, 1->0.
            if prev_a1 != prev_s2:
                # Rare transition occurred
                # Suppress MB weight by OCI score
                current_w = w_base * (1.0 - oci_score)
        
        # --- Stage 1 Choice ---
        max_q_s2 = np.max(q_mf_s2, axis=1)
        q_mb_s1 = transition_matrix @ max_q_s2
        
        q_net_s1 = current_w * q_mb_s1 + (1 - current_w) * q_mf_s1
        
        logits_1 = beta * q_net_s1
        if prev_a1 != -1:
            logits_1[prev_a1] += stickiness
            
        exp_1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_1 / np.sum(exp_1)
        p_choice_1[t] = probs_1[a1]
        
        # --- Stage 2 Choice ---
        logits_2 = beta * q_mf_s2[s2]
        exp_2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_2 / np.sum(exp_2)
        p_choice_2[t] = probs_2[a2]
        
        # --- Updates ---
        delta_1 = q_mf_s2[s2, a2] - q_mf_s1[a1]
        q_mf_s1[a1] += lr * delta_1
        
        delta_2 = r - q_mf_s2[s2, a2]
        q_mf_s2[s2, a2] += lr * delta_2
        
        q_mf_s1[a1] += lr * lambda_val * delta_2
        
        prev_a1 = a1
        prev_s2 = s2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Blunted Structural Knowledge
This model hypothesizes that OCI is associated with doubt or "blunted" structural knowledge. Instead of using the true transition matrix (70/30) for Model-Based planning, the participant's internal model is flattened towards randomness (50/50) proportional to their OCI score. This reduces the distinctiveness of the Model-Based value signal.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Blunted Structural Knowledge.
    Hypothesis: OCI is associated with doubt or uncertainty about environmental structure.
    The participant's internal model of the transition matrix is 'flattened' towards 
    randomness (0.5/0.5) proportional to their OCI score, reducing the efficacy of 
    Model-Based planning.
    
    Parameters:
    - lr: Learning rate [0,1]
    - beta: Inverse temperature [0,10]
    - w: Model-based weight [0,1]
    - lambda_val: Eligibility trace [0,1]
    - stickiness: Perseveration parameter [0,5]
    """
    lr, beta, w, lambda_val, stickiness = model_parameters
    oci_score = oci[0]
    n_trials = len(action_1)
    
    # True Transition Matrix
    true_transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Subjective Transition Matrix modulated by OCI
    # As OCI increases, matrix moves towards [[0.5, 0.5], [0.5, 0.5]]
    # Formula: T_subj = T_true * (1 - oci) + 0.5 * oci
    # If OCI=0, T_subj = T_true. If OCI=1, T_subj = 0.5 everywhere.
    subjective_matrix = true_transition_matrix * (1.0 - oci_score) + 0.5 * oci_score
    
    q_mf_s1 = np.zeros(2)
    q_mf_s2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1
    
    for t in range(n_trials):
        if action_1[t] == -1:
            p_choice_1[t] = 1.0
            p_choice_2[t] = 1.0
            continue
            
        a1 = int(action_1[t])
        s2 = int(state[t])
        a2 = int(action_2[t])
        r = reward[t]
        
        # --- Stage 1 Choice ---
        # Use Subjective Matrix for MB calculation
        max_q_s2 = np.max(q_mf_s2, axis=1)
        q_mb_s1 = subjective_matrix @ max_q_s2
        
        q_net_s1 = w * q_mb_s1 + (1 - w) * q_mf_s1
        
        logits_1 = beta * q_net_s1
        if prev_a1 != -1:
            logits_1[prev_a1] += stickiness
            
        exp_1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_1 / np.sum(exp_1)
        p_choice_1[t] = probs_1[a1]
        
        # --- Stage 2 Choice ---
        logits_2 = beta * q_mf_s2[s2]
        exp_2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_2 / np.sum(exp_2)
        p_choice_2[t] = probs_2[a2]
        
        # --- Updates ---
        delta_1 = q_mf_s2[s2, a2] - q_mf_s1[a1]
        q_mf_s1[a1] += lr * delta_1
        
        delta_2 = r - q_mf_s2[s2, a2]
        q_mf_s2[s2, a2] += lr * delta_2
        
        q_mf_s1[a1] += lr * lambda_val * delta_2
        
        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```