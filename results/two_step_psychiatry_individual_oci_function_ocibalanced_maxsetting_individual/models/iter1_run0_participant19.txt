Here are three new cognitive models that incorporate the OCI score to explain the participant's decision-making behavior, specifically addressing the balance between goal-directedness, choice rigidity, and feedback sensitivity.

### Model 1: OCI-Modulated Model-Based Deficit
This model tests the hypothesis that higher obsessive-compulsive symptoms (OCI) are associated with a reduction in Model-Based (goal-directed) control. Instead of a static mixing weight `w`, the weight is dynamically reduced by the OCI score. A high OCI score effectively "taxes" the Model-Based system, pushing the participant towards Model-Free (habitual) control.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Model-Based Deficit.
    
    Hypothesis: Higher OCI scores reduce the weight of Model-Based control (w),
    shifting the participant towards Model-Free (habitual) behavior.
    
    Parameters:
    - learning_rate: [0, 1] Update rate for Q-values.
    - beta: [0, 10] Inverse temperature (choice consistency).
    - w_base: [0, 1] Baseline Model-Based weight (for OCI=0).
    - oci_penalty: [0, 1] How strongly OCI reduces the Model-Based weight.
    """
    learning_rate, beta, w_base, oci_penalty = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective w: Baseline minus a penalty proportional to OCI
    # We clip it to ensure it stays between 0 and 1.
    w_effective = np.clip(w_base - (oci_penalty * oci_score), 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combine MB and MF values using the OCI-modulated weight
        q_net = w_effective * q_stage1_mb + (1 - w_effective) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # Update Stage 1 (TD(1) logic roughly, or simply TD(0) on Q2)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Update Stage 2
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Driven Rigid Exploration (Beta Scaling)
This model posits that OCI affects the "temperature" of decision-making. The participant data shows long streaks of selecting the same spaceship (e.g., trials 11-21, 123-140). This suggests high rigidity or low exploration. Here, the OCI score scales the inverse temperature parameter (`beta`). Higher OCI leads to a higher effective beta, resulting in more deterministic (rigid) choices and less random exploration.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Driven Rigid Exploration.
    
    Hypothesis: OCI increases decision rigidity. The inverse temperature (beta)
    is scaled up by the OCI score, making choices more deterministic and 
    reducing exploration (explaining the long streaks in the data).
    
    Parameters:
    - learning_rate: [0, 1] Update rate for Q-values.
    - beta_base: [0, 10] Baseline inverse temperature.
    - w: [0, 1] Mixing weight between Model-Based and Model-Free.
    - oci_rigidity: [0, 5] Scaling factor for how OCI increases beta.
    """
    learning_rate, beta_base, w, oci_rigidity = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Effective Beta increases with OCI
    beta_effective = beta_base * (1.0 + (oci_rigidity * oci_score))

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Use the OCI-modulated beta
        exp_q1 = np.exp(beta_effective * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta_effective * q_stage2_mf[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Negative Feedback Sensitivity
This model explores the idea that compulsivity involves altered learning from failure. The model introduces separate learning dynamics for positive and negative prediction errors. Specifically, the OCI score dampens the learning rate for *negative* prediction errors. If `oci_dampening` is high, the participant ignores "misses" (0 coins), leading to perseveration on a choice despite a lack of reward, which aligns with the observed "sticky" behavior in the dataset.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Negative Feedback Sensitivity.
    
    Hypothesis: High OCI participants may be less sensitive to negative prediction 
    errors (omission of reward), causing them to persist in a choice despite 
    failures. OCI scales the learning rate specifically when delta < 0.
    
    Parameters:
    - lr_base: [0, 1] Base learning rate for positive updates.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight.
    - oci_neg_mod: [0, 1] Factor by which OCI dampens learning from negative errors.
    """
    lr_base, beta, w, oci_neg_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # Update Stage 1
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        
        # Determine learning rate based on sign of prediction error
        if delta_stage1 >= 0:
            eff_lr_1 = lr_base
        else:
            # If error is negative, reduce LR based on OCI
            eff_lr_1 = lr_base * (1.0 - (oci_neg_mod * oci_score))
            
        q_stage1_mf[a1] += eff_lr_1 * delta_stage1
        
        # Update Stage 2
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        
        if delta_stage2 >= 0:
            eff_lr_2 = lr_base
        else:
            eff_lr_2 = lr_base * (1.0 - (oci_neg_mod * oci_score))
            
        q_stage2_mf[state_idx, a2] += eff_lr_2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```