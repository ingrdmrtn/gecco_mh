Here are three cognitive models incorporating OCI scores into the two-step decision-making process.

### Model 1: Asymmetric Learning Rates Modulated by OCI
This model hypothesizes that individuals with higher OCI scores exhibit a "negativity bias" in their learning. They may be more sensitive to negative prediction errors (omission of reward) compared to positive ones, leading to faster updating when outcomes are worse than expected.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model with Asymmetric Learning Rates modulated by OCI.
    
    Hypothesis: Participants with higher OCI scores exhibit a negativity bias,
    learning more rapidly from negative prediction errors (omission of reward)
    than from positive ones.
    
    Parameters:
    - lr_base: [0, 1] Base learning rate for positive prediction errors.
    - neg_bias_oci: [0, 5] Scaling factor for negative PE learning rate based on OCI.
      lr_neg = lr_base * (1 + neg_bias_oci * oci) (clipped to 1.0).
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight (0=MF, 1=MB).
    - p_stick: [0, 5] Choice perseveration bonus.
    
    Args:
        action_1, state, action_2, reward, oci, model_parameters
        
    Returns:
        float: Negative log-likelihood.
    """
    lr_base, neg_bias_oci, beta, w, p_stick = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate negative learning rate
    lr_neg = lr_base * (1.0 + neg_bias_oci * oci_score)
    if lr_neg > 1.0: lr_neg = 1.0
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_choice_1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])
        
        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        logits_1 = beta * q_net
        
        if last_choice_1 != -1:
            logits_1[last_choice_1] += p_stick
            
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Stage 2 Policy
        logits_2 = beta * q_stage2_mf[s2]
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # Updates
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[s2, a2]
        
        alpha_1 = lr_neg if delta_stage1 < 0 else lr_base
        q_stage1_mf[a1] += alpha_1 * delta_stage1
        
        alpha_2 = lr_neg if delta_stage2 < 0 else lr_base
        q_stage2_mf[s2, a2] += alpha_2 * delta_stage2
        
        last_choice_1 = a1

    mask = (action_1 != -1)
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss
```

### Model 2: Eligibility Trace Modulated by OCI
This model hypothesizes that OCI modulates the "eligibility trace" ($\lambda$). Higher OCI scores increase $\lambda$, causing the final reward outcome to more strongly reinforce the first-stage choice directly. This represents a stronger "habitual" smearing of value from the outcome back to the start, bypassing the model-based state structure.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model with Eligibility Trace modulated by OCI.
    
    Hypothesis: OCI modulates the eligibility trace (lambda). High OCI scores
    increase the eligibility trace, causing the second-stage outcome to 
    more strongly reinforce the first-stage choice directly (habitual smearing),
    bypassing the model-based state structure.
    
    Parameters:
    - lr: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight.
    - lambda_base: [0, 1] Base eligibility trace.
    - lambda_oci_factor: [0, 1] Factor increasing lambda with OCI.
      lambda = clip(lambda_base + lambda_oci_factor * oci, 0, 1)
    - p_stick: [0, 5] Perseveration bonus.
    
    Args:
        action_1, state, action_2, reward, oci, model_parameters
        
    Returns:
        float: Negative log-likelihood.
    """
    lr, beta, w, lambda_base, lambda_oci_factor, p_stick = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    lam = lambda_base + lambda_oci_factor * oci_score
    if lam > 1.0: lam = 1.0
    if lam < 0.0: lam = 0.0
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_choice_1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])
        
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        logits_1 = beta * q_net
        if last_choice_1 != -1:
            logits_1[last_choice_1] += p_stick
            
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        logits_2 = beta * q_stage2_mf[s2]
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # Updates with eligibility trace for Stage 1 MF
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[s2, a2]
        
        # Stage 1 MF update includes fraction of Stage 2 error (lambda)
        q_stage1_mf[a1] += lr * (delta_stage1 + lam * delta_stage2)
        
        # Stage 2 MF update
        q_stage2_mf[s2, a2] += lr * delta_stage2
        
        last_choice_1 = a1

    mask = (action_1 != -1)
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss
```

### Model 3: Inverse Temperature (Beta) Modulated by OCI
This model posits that OCI scores correlate with decision-making rigidity or intolerance of uncertainty. Higher OCI scores are hypothesized to lead to a higher inverse temperature ($\beta$), resulting in more deterministic (exploitative) choices and reduced exploration.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model with Inverse Temperature (Beta) modulated by OCI.
    
    Hypothesis: OCI scores correlate with rigidity and intolerance of uncertainty.
    Higher OCI scores lead to a higher inverse temperature (beta), resulting in 
    more deterministic (exploitative) choices and less random exploration.
    
    Parameters:
    - lr: [0, 1] Learning rate.
    - beta_base: [0, 10] Base inverse temperature.
    - beta_oci_slope: [0, 10] Sensitivity of beta to OCI. 
      beta = beta_base + beta_oci_slope * oci.
    - w: [0, 1] Mixing weight.
    - p_stick: [0, 5] Perseveration bonus.
    
    Args:
        action_1, state, action_2, reward, oci, model_parameters
        
    Returns:
        float: Negative log-likelihood.
    """
    lr, beta_base, beta_oci_slope, w, p_stick = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    beta = beta_base + beta_oci_slope * oci_score
    # Ensure beta is non-negative
    if beta < 0: beta = 0
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_choice_1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])
        
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        logits_1 = beta * q_net
        if last_choice_1 != -1:
            logits_1[last_choice_1] += p_stick
            
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        logits_2 = beta * q_stage2_mf[s2]
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += lr * delta_stage2
        
        last_choice_1 = a1

    mask = (action_1 != -1)
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss
```