Here are three new cognitive models exploring different mechanisms for how OCI scores might influence learning and decision-making in this two-step task.

### Model 1: OCI-Modulated Eligibility Traces
This model hypothesizes that OCI symptoms affect how credit is assigned to past actions. Specifically, it tests if higher OCI scores lead to stronger "eligibility traces" (lambda), meaning the outcome at the second stage (reward) has a stronger direct reinforcement effect on the first-stage choice (Model-Free learning), potentially bypassing the model-based structure.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model with Eligibility Traces (Lambda) modulated by OCI.
    
    Hypothesis: OCI affects the eligibility trace parameter (lambda), influencing how much 
    the second-stage reward directly reinforces the first-stage choice. 
    Higher OCI might lead to higher lambda (more direct model-free reinforcement of stage 1).

    Parameters:
    learning_rate: [0,1] - Update rate for Q-values.
    beta: [0,10] - Inverse temperature for softmax choice.
    lambda_base: [0,1] - Baseline eligibility trace decay parameter.
    lambda_oci_scale: [0,1] - Scaling factor for OCI's effect on lambda.
    
    Lambda is calculated as: lambda = lambda_base + lambda_oci_scale * oci
    (bounded between 0 and 1).
    """
    learning_rate, beta, lambda_base, lambda_oci_scale = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]

    # Calculate lambda based on OCI
    eligibility_lambda = lambda_base + lambda_oci_scale * participant_oci
    # Bound lambda between 0 and 1
    if eligibility_lambda > 1.0: eligibility_lambda = 1.0
    if eligibility_lambda < 0.0: eligibility_lambda = 0.0

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2) 
    q_stage2_mf = np.zeros((2, 2)) 

    for trial in range(n_trials):
        # Skip missing data
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy (Pure Model-Free for this model to isolate lambda effect) ---
        # Note: Standard TD(lambda) often mixes with MB, but here we focus on MF dynamics
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Prediction error at stage 2 (Reward - Q_stage2)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        # Prediction error at stage 1 (Q_stage2 - Q_stage1)
        # In SARSA/Q-learning, this is the TD error for the first step
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]

        # Update Stage 2 values
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Update Stage 1 values
        # The update includes the immediate TD error (delta_stage1)
        # PLUS the eligibility trace of the stage 2 error (eligibility_lambda * delta_stage2)
        q_stage1_mf[a1] += learning_rate * (delta_stage1 + eligibility_lambda * delta_stage2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Choice Stickiness
This model posits that OCI scores relate to behavioral rigidity or "stickiness" (perseveration). Instead of affecting learning rates, the OCI score modulates a choice autocorrelation parameter, making the participant more likely to repeat their previous Stage 1 choice regardless of the outcome.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model with OCI-modulated Choice Stickiness (Perseveration).
    
    Hypothesis: OCI correlates with repetitive behavior. This model adds a 'stickiness' 
    bonus to the previously chosen action at Stage 1. The magnitude of this bonus 
    is determined by the OCI score.

    Parameters:
    learning_rate: [0,1] - Update rate for Q-values.
    beta: [0,10] - Inverse temperature.
    w: [0,1] - Mixing weight for Model-Based (1) vs Model-Free (0).
    stickiness_oci_param: [0,5] - How much OCI scales the stickiness bonus.
    
    The stickiness bonus added to the Q-value of the previous action is:
    bonus = stickiness_oci_param * oci
    """
    learning_rate, beta, w, stickiness_oci_param = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]
    
    # Stickiness magnitude based on OCI
    stickiness_mag = stickiness_oci_param * participant_oci

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2) 
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness bonus to the previously chosen action
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness_mag

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        last_action_1 = a1 # Update for next trial

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Second-Stage Learning Rate
This model suggests that OCI impacts the volatility of belief updating specifically at the second stage (the specific aliens). High OCI might be associated with over-sensitivity to recent feedback (high learning rate) or rigidity (low learning rate). Here we model the learning rate as a function of OCI.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model with OCI-modulated Learning Rate.
    
    Hypothesis: The learning rate itself is a function of the OCI score.
    This model assumes a baseline learning rate which is modified by the OCI score,
    testing if symptom severity correlates with faster or slower updating of value estimates.

    Parameters:
    lr_base: [0,1] - Base learning rate.
    lr_oci_slope: [0,1] - Slope of OCI effect on learning rate.
    beta: [0,10] - Inverse temperature.
    w: [0,1] - Mixing weight (MB vs MF).
    
    The effective learning rate is: alpha = lr_base + lr_oci_slope * (oci - 0.5)
    (Centered roughly on the scale to allow increase or decrease, bounded [0,1]).
    """
    lr_base, lr_oci_slope, beta, w = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]

    # Calculate effective learning rate
    # We center OCI around 0.5 so the slope can increase or decrease LR relative to base
    # If slope is positive, higher OCI -> higher learning rate.
    learning_rate = lr_base + lr_oci_slope * (participant_oci - 0.5)
    
    if learning_rate < 0.01: learning_rate = 0.01
    if learning_rate > 1.0: learning_rate = 1.0

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2) 
    q_stage2_mf = np.zeros((2, 2)) 

    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates using OCI-modulated learning rate ---
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```