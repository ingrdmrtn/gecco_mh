Here are three new cognitive models exploring different mechanisms for how OCI scores might influence decision-making in this two-step task.

### Model 1: Hybrid Learner with OCI-Modulated Model-Based Weight
This model posits that the balance between model-based (planning) and model-free (habitual) control is influenced by the OCI score. Specifically, it tests the hypothesis that higher OCI scores might lead to either a deficit in model-based control (relying more on habits) or an over-reliance on it.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Learner with OCI-Modulated Model-Based Weight.
    
    This model combines Model-Based (MB) and Model-Free (MF) value estimation.
    The mixing weight 'w' determines the contribution of MB vs MF values.
    Here, 'w' is a linear function of the OCI score.
    
    Parameters:
    lr : [0, 1] Learning rate for value updates.
    beta : [0, 10] Inverse temperature for softmax choice.
    w_base : [0, 1] Base weight for model-based control (when OCI is 0).
    w_oci : [-1, 1] Modulation of weight by OCI score.
    """
    lr, beta, w_base, w_oci = model_parameters
    n_trials = len(action_1)
    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    
    current_oci = oci[0]
    
    # Calculate the mixing weight w, constrained to [0, 1]
    # w = 1 means purely Model-Based, w = 0 means purely Model-Free
    w = w_base + w_oci * current_oci
    w = np.clip(w, 0.0, 1.0)
    
    # Fixed transition matrix for the task structure
    # Spaceship 0 -> Planet 0 (70%), Planet 1 (30%)
    # Spaceship 1 -> Planet 0 (30%), Planet 1 (70%)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_mf_stage1 = np.zeros(2)
    q_mf_stage2 = np.zeros((2, 2)) # state x action
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        
        # Model-Based Value Calculation: V_MB(s1, a1) = Sum T(s1, a1, s2) * max(Q_MF(s2, a2))
        # We use the max of the stage 2 MF values as the value of the second stage states
        v_stage2_max = np.max(q_mf_stage2, axis=1)
        q_mb_stage1 = transition_matrix @ v_stage2_max
        
        # Hybrid Value: Q_net = w * Q_MB + (1-w) * Q_MF
        q_net_stage1 = w * q_mb_stage1 + (1 - w) * q_mf_stage1
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Transition ---
        state_idx = state[trial]
        
        # --- Stage 2 Decision ---
        # Stage 2 is purely model-free in this task context (no further transitions)
        exp_q2 = np.exp(beta * q_mf_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning ---
        
        # Update Stage 2 MF values
        # RPE2 = Reward - Q_MF_Stage2(s, a)
        rpe_2 = reward[trial] - q_mf_stage2[state_idx, action_2[trial]]
        q_mf_stage2[state_idx, action_2[trial]] += lr * rpe_2
        
        # Update Stage 1 MF values (SARSA-like update using Q-value of chosen stage 2 action)
        # Often in hybrid models, stage 1 MF is updated via TD(0) or TD(lambda)
        # Here we use the value of the state reached (or the Q value of action taken)
        # Standard TD(0): RPE1 = Q_MF_Stage2(s', a') - Q_MF_Stage1(s, a)
        rpe_1 = q_mf_stage2[state_idx, action_2[trial]] - q_mf_stage1[action_1[trial]]
        q_mf_stage1[action_1[trial]] += lr * rpe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Model-Free Learner with OCI-Dependent Learning Rates
This model suggests that OCI affects how strongly participants update their beliefs based on prediction errors. Specifically, it separates learning rates for positive (better than expected) and negative (worse than expected) outcomes, with the OCI score modulating the learning rate specifically for negative outcomes (loss sensitivity or rigidity).

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free Learner with OCI-Dependent Asymmetric Learning Rates.
    
    This model assumes separate learning rates for positive and negative prediction errors.
    The learning rate for negative prediction errors (lr_neg) is modulated by the OCI score.
    This tests if high OCI leads to hyper-sensitivity or insensitivity to negative feedback.
    
    Parameters:
    lr_pos : [0, 1] Learning rate for positive prediction errors.
    lr_neg_base : [0, 1] Base learning rate for negative prediction errors.
    lr_neg_oci : [-1, 1] Modulation of negative learning rate by OCI.
    beta : [0, 10] Inverse temperature.
    """
    lr_pos, lr_neg_base, lr_neg_oci, beta = model_parameters
    n_trials = len(action_1)
    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    
    current_oci = oci[0]
    
    # Calculate effective negative learning rate
    lr_neg = lr_neg_base + lr_neg_oci * current_oci
    lr_neg = np.clip(lr_neg, 0.0, 1.0)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning ---
        
        # Stage 2 Update
        rpe_2 = reward[trial] - q_stage2[state_idx, action_2[trial]]
        if rpe_2 >= 0:
            q_stage2[state_idx, action_2[trial]] += lr_pos * rpe_2
        else:
            q_stage2[state_idx, action_2[trial]] += lr_neg * rpe_2
            
        # Stage 1 Update
        # Using the updated Stage 2 value for the TD target (SARSA-like logic)
        rpe_1 = q_stage2[state_idx, action_2[trial]] - q_stage1[action_1[trial]]
        if rpe_1 >= 0:
            q_stage1[action_1[trial]] += lr_pos * rpe_1
        else:
            q_stage1[action_1[trial]] += lr_neg * rpe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Model-Based Learner with OCI-Dependent Decay
This model assumes a Model-Based strategy but introduces a memory decay parameter on the values. The hypothesis is that OCI might relate to the stability of value representations over time. High OCI might be associated with lower decay (rigidity/persistence of value) or higher decay (instability).

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Based Learner with OCI-Dependent Value Decay.
    
    This model assumes the agent builds a model of the task (transition matrix is known/fixed).
    However, the stored values (Q-values at stage 2) decay over time toward 0 (forgetting).
    The rate of decay is modulated by the OCI score.
    
    Parameters:
    lr : [0, 1] Learning rate.
    beta : [0, 10] Inverse temperature.
    decay_base : [0, 1] Base decay rate (1 = no decay, 0 = full reset).
    decay_oci : [-1, 1] Modulation of decay rate by OCI.
    """
    lr, beta, decay_base, decay_oci = model_parameters
    n_trials = len(action_1)
    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    
    current_oci = oci[0]
    
    # Calculate effective decay rate
    # decay factor is applied to unchosen options or all options
    decay = decay_base + decay_oci * current_oci
    decay = np.clip(decay, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Q-values for the second stage (aliens)
    q_stage2 = np.zeros((2, 2)) 
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        
        # --- Stage 1 Choice (Model-Based) ---
        # Agent computes expected value of stage 1 actions based on current stage 2 values
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning ---
        # Update chosen option
        rpe = reward[trial] - q_stage2[state_idx, action_2[trial]]
        q_stage2[state_idx, action_2[trial]] += lr * rpe
        
        # --- Decay ---
        # Decay all values towards 0 (or some baseline) for the next trial
        # This represents forgetting or instability of value representations
        q_stage2 *= decay

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```