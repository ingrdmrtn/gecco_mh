Here are three new cognitive models based on the two-step task structure and the participant's OCI profile. These models explore different ways OCI symptoms might interact with reinforcement learning, specifically focusing on the balance between model-based and model-free control, and learning rates.

### Model 1: Hybrid Model with OCI-Modulated Model-Based Weight
This model posits that the balance between goal-directed (Model-Based) and habitual (Model-Free) control is influenced by the participant's OCI score. Specifically, it tests the hypothesis that higher compulsivity might be associated with a reliance on habit (Model-Free) or a rigid adherence to a model, modulating the mixing weight `w`.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL with OCI-modulated Model-Based Weight.
    
    This model mixes Model-Based (MB) and Model-Free (MF) values.
    The mixing weight 'w' determines the dominance of the MB system.
    This weight is modulated by the OCI score, allowing compulsivity to shift
    the balance between goal-directed and habitual control.
    
    Parameters:
    lr: [0, 1] Learning rate for value updates.
    beta: [0, 10] Inverse temperature for softmax choice.
    w_base: [0, 1] Base weight for Model-Based control (0=MF, 1=MB).
    w_oci_slope: [-1, 1] How OCI score adjusts the MB weight. 
                 Positive means higher OCI -> more MB. Negative means higher OCI -> more MF.
    """
    lr, beta, w_base, w_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate the effective weight w, bounded between 0 and 1
    w = w_base + (w_oci_slope * oci_score)
    w = np.clip(w, 0.0, 1.0)
    
    # Transition matrix (fixed for this task structure)
    # 70% common, 30% rare
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_mf = np.zeros(2) + 0.5        # Stage 1 MF values
    q_mb = np.zeros(2) + 0.5        # Stage 1 MB values
    q_stage2 = np.zeros((2, 2)) + 0.5 # Stage 2 values (shared by MB and MF logic)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        if a1 == -1: continue
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])

        # --- Stage 1 Policy ---
        # Model-Based valuation for Stage 1:
        # V_MB(s1) = sum(P(s2|s1) * max(Q_stage2(s2, a)))
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_mb = transition_matrix @ max_q_stage2
        
        # Combined value: Q_net = w * Q_MB + (1-w) * Q_MF
        q_net = w * q_mb + (1 - w) * q_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        # Standard softmax on stage 2 Q-values
        exp_q2 = np.exp(beta * q_stage2[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Stage 2 update (TD(0))
        # Q2(s, a2) <- Q2(s, a2) + lr * (r - Q2(s, a2))
        delta2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += lr * delta2
        
        # Stage 1 MF update (TD(1) logic roughly, simplified for 2-step)
        # We update Q_MF(a1) using the Q-value of the state we actually arrived at
        # Q_MF(a1) <- Q_MF(a1) + lr * (Q2(s, a2) - Q_MF(a1))
        # Note: Often in hybrid models, stage 1 MF is updated by the stage 2 value
        delta1 = q_stage2[s, a2] - q_mf[a1]
        q_mf[a1] += lr * delta1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Learning Rate Asymmetry
This model investigates if OCI symptoms relate to how participants process positive versus negative feedback. It hypothesizes that individuals with different OCI scores might learn differently from rewards (coins) versus omissions (no coins), reflecting altered sensitivity to success or failure.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free RL with OCI-modulated Learning Rate Asymmetry.
    
    This model allows for different learning rates for positive prediction errors (rewards)
    and negative prediction errors (omissions). The degree of this asymmetry is
    modulated by the OCI score.
    
    Parameters:
    lr_base: [0, 1] Base learning rate.
    beta: [0, 10] Inverse temperature.
    asymmetry_param: [-1, 1] Bias towards positive or negative learning.
    oci_mod: [0, 5] Strength of OCI modulation on the asymmetry.
    """
    lr_base, beta, asymmetry_param, oci_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective learning rates
    # We define a bias 'b' modulated by OCI
    # If b > 0, we learn more from positive PEs. If b < 0, more from negative PEs.
    bias = asymmetry_param + (oci_mod * oci_score)
    
    # We apply the bias to split lr into lr_pos and lr_neg
    # Using a sigmoid-like scaling or simple clamping to keep in [0,1]
    # Here we use simple additive modification centered on lr_base
    lr_pos = np.clip(lr_base + 0.5 * bias, 0.0, 1.0)
    lr_neg = np.clip(lr_base - 0.5 * bias, 0.0, 1.0)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2) + 0.5
    q_stage2 = np.zeros((2, 2)) + 0.5

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        if a1 == -1: continue
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])

        # Stage 1 Choice
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2 Choice
        exp_q2 = np.exp(beta * q_stage2[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Stage 2 Update
        delta2 = r - q_stage2[s, a2]
        if delta2 >= 0:
            q_stage2[s, a2] += lr_pos * delta2
        else:
            q_stage2[s, a2] += lr_neg * delta2
            
        # Stage 1 Update
        # Using the updated Q2 value as the target (SARSA-like connection)
        delta1 = q_stage2[s, a2] - q_stage1[a1]
        if delta1 >= 0:
            q_stage1[a1] += lr_pos * delta1
        else:
            q_stage1[a1] += lr_neg * delta1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Eligibility Trace (Lambda)
This model explores the temporal credit assignment problem. The parameter `lambda` controls how much the Stage 1 choice is updated based on the Stage 2 outcome. An OCI-modulated lambda tests if compulsivity affects the ability to link distal outcomes (rewards at stage 2) back to initial choices (stage 1 actions).

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free RL with OCI-modulated Eligibility Trace (Lambda).
    
    The parameter lambda controls the strength of the link between the Stage 2 outcome
    and the Stage 1 choice update. 
    Lambda = 0: Stage 1 only learns from Stage 2 state value (TD(0)).
    Lambda = 1: Stage 1 learns directly from the reward (Monte Carlo / TD(1)).
    
    OCI modulates this lambda, testing if compulsivity affects credit assignment across time steps.
    
    Parameters:
    lr: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    lambda_base: [0, 1] Base eligibility trace decay parameter.
    lambda_oci_slope: [-1, 1] OCI modulation of lambda.
    """
    lr, beta, lambda_base, lambda_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective lambda
    lam = lambda_base + (lambda_oci_slope * oci_score)
    lam = np.clip(lam, 0.0, 1.0)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2) + 0.5
    q_stage2 = np.zeros((2, 2)) + 0.5
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        if a1 == -1: continue
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])

        # Stage 1 Choice
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2 Choice
        exp_q2 = np.exp(beta * q_stage2[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Prediction Errors
        # PE for Stage 1: difference between Value of State 2 and Value of Choice 1
        delta1 = q_stage2[s, a2] - q_stage1[a1]
        
        # PE for Stage 2: difference between Reward and Value of Choice 2
        delta2 = r - q_stage2[s, a2]
        
        # Updates
        # Stage 2 update is standard
        q_stage2[s, a2] += lr * delta2
        
        # Stage 1 update includes the eligibility trace
        # Q1(a1) <- Q1(a1) + lr * (delta1 + lambda * delta2)
        # If lambda is 1, it effectively pushes the full reward error back to stage 1
        q_stage1[a1] += lr * (delta1 + lam * delta2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```