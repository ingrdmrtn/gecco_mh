Here are three new cognitive models that explore different mechanisms for how Obsessive-Compulsive Inventory (OCI) scores might influence decision-making in this two-step task.

### Model 1: Hybrid RL with OCI-Modulated Model-Based Weighting
This model tests the hypothesis that higher OCI scores correlate with a stronger reliance on habitual (model-free) control over goal-directed (model-based) planning. It implements a standard hybrid reinforcement learning model where the mixing parameter `w` (which balances model-based and model-free values) is a function of the OCI score. Specifically, higher OCI scores reduce the weight of model-based planning.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL model where the balance between Model-Based (MB) and Model-Free (MF) 
    control is modulated by the OCI score.
    
    Hypothesis: Higher OCI scores lead to reduced model-based control (lower w).
    
    Parameters:
    learning_rate: [0, 1] - Learning rate for value updates.
    beta: [0, 10] - Inverse temperature for softmax choice.
    w_intercept: [0, 1] - Baseline weighting for Model-Based values.
    oci_slope: [0, 1] - Strength of OCI reduction on MB weighting.
    """
    learning_rate, beta, w_intercept, oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate mixing weight w based on OCI
    # We clip to ensure w stays in [0, 1]
    # Interpretation: w = 1 is pure MB, w = 0 is pure MF
    w = w_intercept - (oci_slope * oci_score)
    w = np.clip(w, 0.0, 1.0)

    # Transition matrix (fixed for this task structure)
    # 0 -> 0 (Common), 0 -> 1 (Rare) | 1 -> 1 (Common), 1 -> 0 (Rare)
    # Typically: A(0) -> X(0) (0.7), U(1) -> Y(1) (0.7)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_mf = np.zeros((2, 2))  # Stage 1 MF values (2 actions) -> This is actually usually just 2 values, but let's stick to standard Q(s,a)
    q_mb = np.zeros(2)       # Stage 1 MB values
    q_stage2 = np.zeros((2, 2)) # Stage 2 values (2 states, 2 actions)
    
    # For stage 1 MF, we only have 1 state, so just vector of size 2
    q_stage1_mf = np.zeros(2)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        
        # --- Stage 1 Policy ---
        # Calculate Model-Based values for Stage 1
        # V_MB(s1, a) = sum(P(s2|s1,a) * max(Q(s2, a')))
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_mb = transition_matrix @ max_q_stage2
        
        # Combined value
        q_net = w * q_mb + (1 - w) * q_stage1_mf
        
        # Softmax choice 1
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        if a1 != -1:
            p_choice_1[trial] = probs_1[a1]
        else:
            p_choice_1[trial] = 1.0
            
        s_idx = int(state[trial])
        
        # --- Stage 2 Policy ---
        if s_idx != -1:
            # Standard softmax on stage 2 values
            exp_q2 = np.exp(beta * q_stage2[s_idx])
            probs_2 = exp_q2 / np.sum(exp_q2)
            
            a2 = int(action_2[trial])
            if a2 != -1:
                p_choice_2[trial] = probs_2[a2]
                
                r = reward[trial]
                
                # --- Learning ---
                # Update Stage 2 Q-values (TD(0))
                # Q2(s2, a2) += alpha * (r - Q2(s2, a2))
                delta2 = r - q_stage2[s_idx, a2]
                q_stage2[s_idx, a2] += learning_rate * delta2
                
                # Update Stage 1 MF Q-values (TD(1) / SARSA-like)
                # Note: In standard Daw 2011, Stage 1 MF is updated via Q(s2, a2) (TD(0) of stage 1) 
                # and then potentially eligibility traces.
                # Simplified MF update using the reward directly (TD(1) approximation often used)
                # or using the value of the second stage choice. 
                # Using standard TD(1) assumption: Update Q1 based on final reward
                if a1 != -1:
                    delta1 = r - q_stage1_mf[a1]
                    q_stage1_mf[a1] += learning_rate * delta1
            else:
                p_choice_2[trial] = 1.0
        else:
            p_choice_2[trial] = 1.0

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Model-Free RL with OCI-Dependent Learning Rate Asymmetry
This model investigates if OCI scores relate to a bias in learning from positive versus negative prediction errors. It proposes that individuals with higher OCI scores might be hyper-sensitive to negative outcomes (or "missed" rewards) or overly driven by positive reinforcement. The model splits the learning rate into positive (`alpha_pos`) and negative (`alpha_neg`) components, where the balance between them is shifted by the OCI score.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free RL where the learning rate for positive vs negative prediction errors
    is modulated by OCI.
    
    Hypothesis: High OCI might lead to higher sensitivity to negative prediction errors 
    (learning more from lack of reward).
    
    Parameters:
    lr_base: [0, 1] - Base learning rate.
    beta: [0, 10] - Inverse temperature.
    neg_bias_base: [0, 1] - Base bias towards negative learning rates.
    oci_mod: [0, 1] - How much OCI increases the negative learning rate bias.
    """
    lr_base, beta, neg_bias_base, oci_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate asymmetrical learning rates
    # Bias factor: higher means stronger learning from negative PE
    bias = neg_bias_base + (oci_mod * oci_score)
    # Ensure bias doesn't break math, though strictly it's a ratio modifier here
    
    # We define:
    # alpha_pos = lr_base
    # alpha_neg = lr_base * (1 + bias)  -- clipped at 1
    
    alpha_pos = lr_base
    alpha_neg = np.clip(lr_base * (1.0 + bias), 0.0, 1.0)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        
        # Stage 1 Choice
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        if a1 != -1:
            p_choice_1[trial] = probs_1[a1]
        else:
            p_choice_1[trial] = 1.0
            
        s_idx = int(state[trial])
        
        # Stage 2 Choice
        if s_idx != -1:
            exp_q2 = np.exp(beta * q_stage2[s_idx])
            probs_2 = exp_q2 / np.sum(exp_q2)
            
            a2 = int(action_2[trial])
            if a2 != -1:
                p_choice_2[trial] = probs_2[a2]
                
                r = reward[trial]
                
                # --- Learning Stage 2 ---
                delta2 = r - q_stage2[s_idx, a2]
                curr_alpha = alpha_pos if delta2 >= 0 else alpha_neg
                q_stage2[s_idx, a2] += curr_alpha * delta2
                
                # --- Learning Stage 1 ---
                # Using simple TD(1) update from reward
                if a1 != -1:
                    delta1 = r - q_stage1[a1]
                    curr_alpha_1 = alpha_pos if delta1 >= 0 else alpha_neg
                    q_stage1[a1] += curr_alpha_1 * delta1
            else:
                p_choice_2[trial] = 1.0
        else:
            p_choice_2[trial] = 1.0

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Model-Based RL with OCI-Driven "Relief" Exploration
This model posits that OCI relates to uncertainty avoidance or a specific type of exploration. Instead of standard Softmax exploration, this model uses an "Uncertainty Bonus" (or penalty) added to the Q-values. The magnitude of this bonus is determined by the OCI score. This captures the idea that individuals with higher OCI might stick to known safe options (uncertainty penalty) or compulsively check uncertain options (uncertainty bonus).

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Based RL with an Uncertainty Bonus/Penalty modulated by OCI.
    
    Hypothesis: OCI score affects how participants handle uncertainty. 
    They might avoid uncertain options (penalty) or seek them (bonus).
    We track a 'counter' for how long ago an option was chosen to proxy uncertainty.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate.
    beta: [0, 10] - Inverse temperature.
    uncert_base: [0, 1] - Base weight for uncertainty (exploration bonus).
    oci_uncert_impact: [0, 1] - How OCI modifies the uncertainty weight.
    """
    learning_rate, beta, uncert_base, oci_uncert_impact = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate uncertainty weight phi
    # If oci_uncert_impact is positive, high OCI increases the bonus (exploration/checking).
    # If the interaction implies avoidance, the fitted parameter might be negative conceptually, 
    # but here we model it as an additive effect to the base.
    phi = uncert_base + (oci_uncert_impact * oci_score)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage2 = np.zeros((2, 2))
    q_stage1_mf = np.zeros(2) # Purely for tracking average reward
    
    # Uncertainty trackers: count trials since last chosen
    # Initialize with 0
    counts_s1 = np.zeros(2) 
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        
        # --- Stage 1 Policy ---
        # 1. Calculate MB values
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_mb = transition_matrix @ max_q_stage2
        
        # 2. Calculate Uncertainty Bonus
        # Simple counter-based bonus: more trials unchosen -> higher bonus
        # We normalize counts to keep values reasonable relative to Q (0-1)
        bonus = np.sqrt(counts_s1) # Square root dampens the growth
        
        # Effective Q
        q_net = q_mb + (phi * bonus)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        if a1 != -1:
            p_choice_1[trial] = probs_1[a1]
            
            # Update counts
            counts_s1 += 1 # Increment all
            counts_s1[a1] = 0 # Reset chosen
        else:
            p_choice_1[trial] = 1.0
        
        s_idx = int(state[trial])
        
        # --- Stage 2 Policy ---
        if s_idx != -1:
            exp_q2 = np.exp(beta * q_stage2[s_idx])
            probs_2 = exp_q2 / np.sum(exp_q2)
            
            a2 = int(action_2[trial])
            if a2 != -1:
                p_choice_2[trial] = probs_2[a2]
                
                r = reward[trial]
                
                # Update Q2
                delta2 = r - q_stage2[s_idx, a2]
                q_stage2[s_idx, a2] += learning_rate * delta2
                
            else:
                p_choice_2[trial] = 1.0
        else:
            p_choice_2[trial] = 1.0

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```