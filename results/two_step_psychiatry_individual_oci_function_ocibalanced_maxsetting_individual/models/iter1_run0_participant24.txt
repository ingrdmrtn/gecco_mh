Here are three cognitive models that incorporate the OCI-R score to explain participant behavior in the two-step task.

### Model 1: OCI-Modulated Asymmetric Learning Rates
This model tests the hypothesis that OCI scores relate to differential sensitivity to positive versus negative prediction errors. High OCI (compulsivity/anxiety) might lead to hyper-sensitivity to negative outcomes (missing a reward), resulting in a different learning rate for negative prediction errors compared to positive ones.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Asymmetric Learning Rates.
    
    Hypothesis: Participants with higher OCI scores may exhibit differential sensitivity 
    to positive versus negative prediction errors. Specifically, this model allows the 
    learning rate for negative prediction errors (disappointments) to be modulated by OCI,
    reflecting a potential bias in how failures are processed.
    
    Parameters:
    - beta: [0, 10] Inverse temperature (exploration/exploitation trade-off).
    - w: [0, 1] Weighting between Model-Based (1) and Model-Free (0).
    - lr_pos: [0, 1] Learning rate for positive prediction errors (reward > expectation).
    - lr_neg_base: [0, 1] Baseline learning rate for negative prediction errors.
    - lr_neg_oci_slope: [-1, 1] How strongly OCI modulates the negative learning rate.
    """
    beta, w, lr_pos, lr_neg_base, lr_neg_oci_slope = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]
    
    # Calculate OCI-modulated negative learning rate
    # We clip to [0, 1] to ensure stability
    lr_neg = lr_neg_base + (lr_neg_oci_slope * participant_oci)
    lr_neg = np.clip(lr_neg, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)      # Model-free values for Stage 1
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for Stage 2
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        # Handle missing or invalid trials
        if np.isnan(action_1[trial]) or action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Stage 1 Policy ---
        # Model-Based value: Transition * Max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net value mixing MB and MF
        q_net_s1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_s1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        # Stage 1 Update (TD(0))
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        alpha_1 = lr_pos if delta_stage1 >= 0 else lr_neg
        q_stage1_mf[a1] += alpha_1 * delta_stage1
        
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        alpha_2 = lr_pos if delta_stage2 >= 0 else lr_neg
        q_stage2_mf[s_idx, a2] += alpha_2 * delta_stage2
        
    eps = 1e-10
    # Only sum log-likelihoods for valid trials
    valid_mask_1 = p_choice_1 > 0
    valid_mask_2 = p_choice_2 > 0
    
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask_1] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_mask_2] + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Exploration (Beta)
This model tests the hypothesis that OCI relates to the degree of determinism or rigidity in behavior. Higher OCI scores might lead to a higher inverse temperature ($\beta$), indicating a stronger preference for exploiting high-value options and reduced random exploration.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Modulated Inverse Temperature (Beta).
    
    Hypothesis: OCI scores correlate with the rigidity of choice behavior.
    High OCI indicates compulsive tendencies which may manifest as reduced exploration 
    (higher Beta), causing the participant to stick strictly to the perceived best option.
    
    Parameters:
    - learning_rate: [0, 1] Speed of value updating.
    - w: [0, 1] Weighting between Model-Based (1) and Model-Free (0).
    - beta_base: [0, 10] Baseline inverse temperature.
    - beta_oci_slope: [-5, 5] Effect of OCI on beta. Positive slope = more rigid with high OCI.
    """
    learning_rate, w, beta_base, beta_oci_slope = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]
    
    # Calculate OCI-modulated Beta
    beta = beta_base + (beta_oci_slope * participant_oci)
    beta = np.clip(beta, 0.0, 10.0) # Clip to standard bounds
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        if np.isnan(action_1[trial]) or action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_s1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_s1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
    eps = 1e-10
    valid_mask_1 = p_choice_1 > 0
    valid_mask_2 = p_choice_2 > 0
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask_1] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_mask_2] + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Stage 2 Stickiness
This model posits that compulsivity manifests as "checking" behavior or habit persistence specifically at the second stage (the aliens). This is distinct from general perseveration; it is context-dependent stickiness: "When I am at Planet X, I tend to repeat the alien I chose last time I was at Planet X."

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Stage 2 Stickiness (Contextual Habit).
    
    Hypothesis: OCI reflects compulsive tendencies that manifest as "stickiness" 
    to specific choices within a specific context (Stage 2 aliens). This models 
    'checking' behavior: a tendency to repeat the last choice made in this specific state,
    modulated by the OCI score.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Model-based weight.
    - st2_stick_base: [0, 5] Baseline stickiness bonus for Stage 2 choices.
    - st2_stick_oci_slope: [-5, 5] Effect of OCI on Stage 2 stickiness.
    """
    learning_rate, beta, w, st2_stick_base, st2_stick_oci_slope = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]
    
    # Calculate stickiness bonus
    stickiness = st2_stick_base + (st2_stick_oci_slope * participant_oci)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track last choice made in each state (planet) for Stage 2
    # Initialize with -1 (no previous choice)
    last_choice_stage2 = np.array([-1, -1]) 
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        if np.isnan(action_1[trial]) or action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_s1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_s1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        # Add stickiness bonus to the Q-values used for choice (but not for learning)
        q_stage2_choice = q_stage2_mf[s_idx].copy()
        
        # If we have visited this state before, add stickiness to the previously chosen action
        if last_choice_stage2[s_idx] != -1:
            q_stage2_choice[last_choice_stage2[s_idx]] += stickiness
            
        exp_q2 = np.exp(beta * q_stage2_choice)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # Update stickiness tracker for this state
        last_choice_stage2[s_idx] = a2
        
        # --- Updates ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
    eps = 1e-10
    valid_mask_1 = p_choice_1 > 0
    valid_mask_2 = p_choice_2 > 0
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask_1] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_mask_2] + eps)))
    return log_loss
```