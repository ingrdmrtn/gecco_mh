Here are three new cognitive models based on the provided participant data and task description.

### Model 1: Hybrid Learner with OCI-Modulated Mixing
This model assumes the participant uses a hybrid strategy combining Model-Based (MB) and Model-Free (MF) learning. The key hypothesis is that the balance between these two systems is influenced by their OCI score. A higher OCI score might lead to a more rigid, habit-based (Model-Free) strategy, while a lower score might allow for more flexible Model-Based planning. The mixing weight `w` is derived from a base parameter and the OCI score.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free learner where the mixing weight (w) is modulated by OCI.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for Q-value updates.
    - beta: [0, 10] Inverse temperature for softmax choice.
    - w_intercept: [0, 1] Base weight for Model-Based control (when OCI is 0).
    - w_slope: [-1, 1] How OCI score adjusts the weight. Negative slope implies higher OCI -> less MB.
    """
    learning_rate, beta, w_intercept, w_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate mixing weight w, bounded between 0 and 1.
    # w = 1 implies pure Model-Based, w = 0 implies pure Model-Free.
    w = w_intercept + (w_slope * oci_score)
    w = np.clip(w, 0.0, 1.0)

    # Fixed transition matrix as described in the task (70% common, 30% rare)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_mf = np.zeros((2, 2))  # Stage 1 MF values (2 actions) - Note: Usually MF stage 1 is just Q(s1, a)
    # However, standard hybrid models often track Q_MF(s1, a) separately. 
    # Let's simplify: q_mf_stage1 stores Q(action1)
    q_mf_stage1 = np.zeros(2)
    
    q_stage2 = np.zeros((2, 2)) # Stage 2 values for (Planet X/Y, Alien 1/2)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based Value: V_MB(s1) = T * max(Q_stage2)
        max_q_stage2 = np.max(q_stage2, axis=1) # Max value for each planet
        q_mb_stage1 = transition_matrix @ max_q_stage2
        
        # Hybrid Value: Q_net = w * Q_MB + (1-w) * Q_MF
        q_net_stage1 = w * q_mb_stage1 + (1 - w) * q_mf_stage1
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Choice ---
        s_idx = int(state[trial]) # Planet arrived at
        
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        r = reward[trial]
        
        # Update Stage 2 Q-values (Standard TD)
        pe_2 = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += learning_rate * pe_2
        
        # Update Stage 1 MF Q-values (TD(1) / SARSA-like update using Q_stage2)
        # The agent updates Q_MF(a1) based on the value of the state they actually landed in.
        # This is the hallmark of MF learning: it's sensitive to the outcome (planet) but ignores transition probs.
        pe_1 = q_stage2[s_idx, a2] - q_mf_stage1[a1]
        q_mf_stage1[a1] += learning_rate * pe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Model-Free with OCI-Dependent Eligibility Traces
This model proposes that OCI symptoms relate to how outcomes are credited to past actions (eligibility traces). A higher OCI score might correlate with "over-thinking" or attributing current outcomes too strongly to distal causes (high lambda), or conversely, a fragmented focus (low lambda). Here, the decay rate `lambda` for the eligibility trace is a function of OCI.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner (TD-Lambda) where the eligibility trace parameter (lambda) 
    is modulated by OCI.
    
    Parameters:
    - learning_rate: [0, 1] Update rate.
    - beta: [0, 10] Inverse temperature.
    - lambda_base: [0, 1] Base eligibility trace decay.
    - oci_lambda_mod: [-1, 1] Modulation of lambda by OCI.
    """
    learning_rate, beta, lambda_base, oci_lambda_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective lambda, bounded [0, 1]
    lam = lambda_base + (oci_score * oci_lambda_mod)
    lam = np.clip(lam, 0.0, 1.0)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Choice ---
        s_idx = int(state[trial])
        
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        r = reward[trial]
        
        # Prediction Error at Stage 2
        # PE2 = Reward - Q_Stage2(State, Action2)
        delta_2 = r - q_stage2[s_idx, a2]
        
        # Prediction Error at Stage 1
        # PE1 = Q_Stage2(State, Action2) - Q_Stage1(Action1)
        delta_1 = q_stage2[s_idx, a2] - q_stage1[a1]
        
        # Update Stage 2
        q_stage2[s_idx, a2] += learning_rate * delta_2
        
        # Update Stage 1
        # In TD(lambda), the update for the first stage includes the immediate PE (delta_1)
        # PLUS a portion of the second stage PE (delta_2) scaled by lambda.
        # If lambda=0, this is pure TD (Q-learning-ish). If lambda=1, it's Monte Carlo.
        q_stage1[a1] += learning_rate * (delta_1 + lam * delta_2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Asymmetric Learning Rates Modulated by OCI
This model investigates if OCI affects sensitivity to positive vs. negative prediction errors differently. Individuals with high OCI might be more sensitive to "failures" (negative prediction errors) or less sensitive to rewards. The model uses separate learning rates for positive and negative updates, where the ratio or magnitude is shifted by the OCI score.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner with asymmetric learning rates for positive and negative 
    prediction errors. The 'negative' learning rate is scaled by OCI.
    
    Parameters:
    - alpha_pos: [0, 1] Learning rate for positive prediction errors.
    - alpha_neg_base: [0, 1] Base learning rate for negative prediction errors.
    - oci_neg_scale: [0, 5] Multiplier for OCI to increase/decrease sensitivity to negative PEs.
    - beta: [0, 10] Inverse temperature.
    """
    alpha_pos, alpha_neg_base, oci_neg_scale, beta = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective negative learning rate
    # We assume OCI might amplify the impact of negative outcomes (fear of failure/error).
    alpha_neg = alpha_neg_base * (1 + oci_score * oci_neg_scale)
    alpha_neg = np.clip(alpha_neg, 0.0, 1.0)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Choice ---
        s_idx = int(state[trial])
        
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        r = reward[trial]
        
        # Stage 2 Update
        pe_2 = r - q_stage2[s_idx, a2]
        lr_2 = alpha_pos if pe_2 > 0 else alpha_neg
        q_stage2[s_idx, a2] += lr_2 * pe_2
        
        # Stage 1 Update
        # Using simple SARSA/TD(0) style update from Stage 2 value
        pe_1 = q_stage2[s_idx, a2] - q_stage1[a1]
        lr_1 = alpha_pos if pe_1 > 0 else alpha_neg
        q_stage1[a1] += lr_1 * pe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```