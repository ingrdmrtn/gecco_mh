Here are the three cognitive models based on the participant's data and OCI scores.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Driven Perseveration (Stickiness).
    
    Hypothesis:
    Obsessive-compulsive symptoms (OCI) are associated with repetitive behaviors and
    habitual rigidity. This model posits that choice 'stickiness' (the tendency to 
    repeat the previous first-stage choice regardless of outcome) is directly 
    proportional to the OCI score.
    
    Parameters:
    - lr: [0, 1] Learning rate for value updates.
    - beta: [0, 10] Inverse temperature (exploration/exploitation balance).
    - w: [0, 1] Weighting parameter for Model-Based (1) vs Model-Free (0) control.
    - stick_k: [0, 5] Scaling factor linking OCI to stickiness magnitude.
    """
    lr, beta, w, stick_k = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Stickiness is derived from OCI score
    stickiness = stick_k * oci_score
    
    # Transition matrix (fixed structure for the task)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1
    log_likelihood = 0.0
    eps = 1e-10

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        # Model-Based Value: Expected max Q(s2) weighted by transition probs
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Apply OCI-driven stickiness to the previously chosen action
        if prev_action_1 != -1:
            q_net[prev_action_1] += stickiness
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_likelihood += np.log(probs_1[a1] + eps)

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s2, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_likelihood += np.log(probs_2[a2] + eps)

        # --- Updates ---
        # Prediction error for Stage 1 (SARSA-like)
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        # Prediction error for Stage 2
        delta_stage2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += lr * delta_stage2
        
        prev_action_1 = a1

    return -log_likelihood


def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Modulated Asymmetric Learning.
    
    Hypothesis:
    Anxiety and compulsivity often involve altered sensitivity to negative outcomes 
    (or lack of reward). This model suggests that the learning rate is asymmetric, 
    and the degree of asymmetry is modulated by the OCI score. Specifically, 
    learning from non-rewarded trials (0 coins) is scaled by OCI.
    
    Parameters:
    - lr_base: [0, 1] Base learning rate for rewarded trials.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] MB/MF weighting.
    - asym_factor: [0, 1] Factor scaling the reduction of learning rate for 
                   unrewarded trials based on OCI.
    """
    lr_base, beta, w, asym_factor = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate asymmetric learning rates
    # lr_pos is used when reward is 1
    lr_pos = lr_base
    # lr_neg is used when reward is 0; modulated by OCI
    # Higher OCI -> lower learning rate from failure (ignoring safety signals/rigidity)
    lr_neg = lr_base * (1.0 - asym_factor * oci_score)
    lr_neg = np.clip(lr_neg, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    log_likelihood = 0.0
    eps = 1e-10

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Determine current learning rate based on outcome
        current_lr = lr_pos if r == 1.0 else lr_neg

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_likelihood += np.log(probs_1[a1] + eps)

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s2, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_likelihood += np.log(probs_2[a2] + eps)

        # --- Updates ---
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += current_lr * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += current_lr * delta_stage2

    return -log_likelihood


def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Eligibility Trace (Credit Assignment).
    
    Hypothesis:
    This model investigates whether OCI scores impact the ability to assign credit 
    across stages (eligibility trace). It posits that high OCI scores impair the 
    direct backpropagation of the second-stage reward prediction error to the 
    first-stage choice (reduced eligibility trace), leading to more disjointed learning.
    
    Parameters:
    - lr: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] MB/MF weighting.
    - lambda_factor: [0, 1] Baseline eligibility trace factor, scaled down by OCI.
    """
    lr, beta, w, lambda_factor = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Eligibility trace lambda is modulated by OCI
    # Higher OCI -> Lower lambda (worse credit assignment)
    lambda_val = lambda_factor * (1.0 - oci_score)
    lambda_val = np.clip(lambda_val, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    log_likelihood = 0.0
    eps = 1e-10

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_likelihood += np.log(probs_1[a1] + eps)

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s2, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_likelihood += np.log(probs_2[a2] + eps)

        # --- Updates ---
        # Stage 1 update includes direct TD error (delta1) AND 
        # the eligibility trace of the Stage 2 error (lambda * delta2)
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        
        delta_stage2 = r - q_stage2_mf[s2, a2]
        
        # Update Stage 1 MF value with both errors
        q_stage1_mf[a1] += lr * delta_stage1 + lr * lambda_val * delta_stage2
        
        # Update Stage 2 MF value
        q_stage2_mf[s2, a2] += lr * delta_stage2

    return -log_likelihood
```