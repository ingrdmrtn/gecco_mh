Here are three new cognitive models that incorporate the OCI score into the decision-making process in different ways, exploring mechanisms like habit formation, learning rate modulation, and mixing model-based/model-free strategies.

### Model 1: OCI-Modulated Habit Strength (Perseverance)
This model hypothesizes that individuals with higher OCI scores exhibit stronger "stickiness" or perseverance in their choices, regardless of reward. This reflects the repetitive nature often associated with OC symptoms. The OCI score scales a perseverance parameter added to the Q-values.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Habit Strength (Perseverance)
    
    This model posits that the OCI score modulates a 'perseverance' bonus.
    Higher OCI scores lead to a stronger tendency to repeat the previous Stage 1 action,
    regardless of the outcome (habitual responding).
    
    Parameters:
    - learning_rate: [0, 1] Standard learning rate for Q-value updates.
    - beta: [0, 10] Inverse temperature for softmax.
    - w: [0, 1] Weighting parameter between Model-Based (1) and Model-Free (0) control.
    - perseverance_base: [0, 5] Base strength of choice perseverance.
    """
    learning_rate, beta, w, perseverance_base = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Scale perseverance by OCI score. Higher OCI -> stronger habit/stickiness.
    # We normalize OCI slightly or just use it as a multiplier.
    # Since OCI is roughly 0-1 range (max score is usually higher but normalized here presumably),
    # we treat it as a direct scalar.
    effective_perseverance = perseverance_base * (1.0 + oci_score)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track the previous action for perseverance
    last_action_1 = -1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        if a1 == -1: continue # Skip missing data

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Q-value (Hybrid Model)
        q_integrated = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add perseverance bonus if a previous action exists
        q_final = q_integrated.copy()
        if last_action_1 != -1:
            q_final[last_action_1] += effective_perseverance

        exp_q1 = np.exp(beta * q_final)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        state_idx = int(state[trial])
        if state_idx == -1: continue

        # --- Stage 2 Policy ---
        a2 = int(action_2[trial])
        if a2 == -1: continue

        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning Updates ---
        r = reward[trial]
        
        # Stage 2 Update (TD)
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 Update (TD) - Model Free path
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Update history
        last_action_1 = a1

    eps = 1e-10
    valid_trials = (action_1 != -1) & (state != -1) & (action_2 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```

### Model 2: OCI-Based Learning Rate Asymmetry (Rigidity)
This model hypothesizes that OCI relates to cognitive rigidity, specifically in how beliefs are updated. It suggests that individuals with higher OCI scores might be more resistant to changing their value estimates (lower learning rate) or might weigh positive and negative prediction errors differently. Here, we model OCI as dampening the learning rate, making the agent "stuck" in its initial or early beliefs.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Based Learning Rate Asymmetry (Rigidity)
    
    Hypothesis: Higher OCI scores are associated with cognitive rigidity.
    This is modeled as a reduction in the learning rate, making the participant
    slower to update values in response to new information.
    
    Parameters:
    - lr_base: [0, 1] Base learning rate.
    - beta: [0, 10] Inverse temperature.
    - lambda_eligibility: [0, 1] Eligibility trace decay (allowing Stage 2 reward to update Stage 1 directly).
    - w: [0, 1] Weight for Model-Based control.
    """
    lr_base, beta, lambda_eligibility, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Modulate learning rate: Higher OCI -> Lower effective learning rate (more rigid)
    # We use a simple decay factor: lr = base / (1 + OCI)
    effective_lr = lr_base / (1.0 + oci_score)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        if a1 == -1: continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_integrated = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_integrated)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        state_idx = int(state[trial])
        if state_idx == -1: continue

        # --- Stage 2 Policy ---
        a2 = int(action_2[trial])
        if a2 == -1: continue

        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning Updates ---
        r = reward[trial]
        
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += effective_lr * delta_stage2
        
        # Stage 1 Update (TD)
        # Note: In standard TD(0), update is based on Stage 2 Q-value. 
        # With eligibility traces (TD(lambda)), the reward also propagates back.
        # Here we implement a simplified TD(1)-like update via lambda parameter.
        
        # Standard TD error for Stage 1
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += effective_lr * delta_stage1
        
        # Eligibility trace update: allowing the final reward outcome to directly influence Stage 1
        # This effectively mixes TD(0) and Monte Carlo updates.
        q_stage1_mf[a1] += effective_lr * lambda_eligibility * delta_stage2

    eps = 1e-10
    valid_trials = (action_1 != -1) & (state != -1) & (action_2 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```

### Model 3: OCI-Driven Model-Based vs Model-Free Balance
This model proposes that the balance between goal-directed (Model-Based) and habitual (Model-Free) control is directly influenced by the OCI score. Specifically, high OCI might be associated with an over-reliance on habitual control (Model-Free) or a deficit in flexible Model-Based planning.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Driven Model-Based vs Model-Free Balance
    
    Hypothesis: The weighting parameter 'w' (mixing Model-Based and Model-Free values)
    is not a fixed free parameter, but is a function of the OCI score.
    Higher OCI leads to lower 'w' (more Model-Free/Habitual behavior).
    
    Parameters:
    - learning_rate: [0, 1]
    - beta: [0, 10]
    - w_base: [0, 1] The baseline model-based weight for a theoretical 0-OCI participant.
    - decay_rate: [0, 1] How quickly model-based control decays as OCI increases.
    """
    learning_rate, beta, w_base, decay_rate = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective w based on OCI.
    # w = w_base * exp(-decay * OCI)
    # If OCI is high, w becomes small (Model Free dominance).
    effective_w = w_base * np.exp(-decay_rate * oci_score)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        if a1 == -1: continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Mix using the OCI-derived weight
        q_integrated = effective_w * q_stage1_mb + (1 - effective_w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_integrated)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        state_idx = int(state[trial])
        if state_idx == -1: continue

        # --- Stage 2 Policy ---
        a2 = int(action_2[trial])
        if a2 == -1: continue

        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning Updates ---
        r = reward[trial]
        
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    valid_trials = (action_1 != -1) & (state != -1) & (action_2 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```