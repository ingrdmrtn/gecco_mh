Here are three new cognitive models that explore different mechanisms for how Obsessive-Compulsive symptoms (OCI) might influence decision-making in this two-step task.

### Model 1: Hybrid RL with OCI-modulated Mixing Weight
This model assumes participants use a mixture of Model-Based (planning) and Model-Free (habitual) strategies. The core hypothesis here is that higher OCI scores might be associated with a reliance on habitual (Model-Free) control, perhaps due to cognitive rigidity or an over-reliance on safety signals rather than flexible planning. The parameter `w` controls the balance, and OCI shifts this balance.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based / Model-Free RL with OCI-modulated mixing weight.
    
    Hypothesis: OCI score influences the balance between Model-Based (planning)
    and Model-Free (habitual) control. High OCI might lead to more habitual behavior.
    
    Parameters:
    - learning_rate: Update rate for Q-values [0, 1]
    - beta: Inverse temperature for softmax choice [0, 10]
    - w_base: Baseline mixing weight (0=Pure MF, 1=Pure MB) [0, 1]
    - w_oci_shift: How much OCI shifts the weight towards MF (negative) or MB (positive) [-1, 1]
    """
    learning_rate, beta, w_base, w_oci_shift = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    # Calculate mixing weight w, bounded [0, 1]
    # If w_oci_shift is negative, higher OCI reduces w (more Model-Free)
    w = w_base + (w_oci_shift * current_oci)
    w = np.clip(w, 0.0, 1.0)

    # Fixed transition matrix for Model-Based (A->X=0.7, U->Y=0.7)
    # Rows: Spaceship 0, Spaceship 1. Cols: Planet 0, Planet 1
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2 = np.zeros((2, 2)) # State (Planet), Action (Alien)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Handle missing data
        if a1 == -1 or s_idx == -1 or a2 == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # --- Stage 1 Policy ---
        # Model-Based Value: V(s') = max(Q_stage2(s', a'))
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        # Stage 2 Update (TD)
        # Prediction error: R - Q(s, a2)
        pe_2 = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += learning_rate * pe_2
        
        # Stage 1 MF Update (TD)
        # Prediction error: Q(s, a2) - Q(a1) (SARSA-like connection to stage 2 value)
        pe_1 = q_stage2[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * pe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Asymmetric Learning Rates based on OCI
This model proposes that high OCI scores relate to an asymmetry in how positive versus negative prediction errors are processed. Specifically, individuals with obsessive-compulsive traits might be hyper-sensitive to "errors" or lack of reward (negative prediction errors) while learning less from successes, or vice versa.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free RL with OCI-modulated asymmetric learning rates.
    
    Hypothesis: OCI affects the ratio between learning from positive prediction errors 
    (rewards) vs negative prediction errors (omissions). 
    
    Parameters:
    - lr_base: Base learning rate [0, 1]
    - beta: Inverse temperature [0, 10]
    - asymmetry_param: How much OCI biases learning towards positive or negative PE [-1, 1]
    - decay: Passive forgetting rate for unchosen options [0, 1]
    """
    lr_base, beta, asymmetry_param, decay = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    # Calculate separate learning rates for positive (pos) and negative (neg) PEs
    # If asymmetry_param > 0 and OCI is high: Learn more from rewards (lr_pos > lr_neg)
    # If asymmetry_param < 0 and OCI is high: Learn more from punishments/omissions (lr_neg > lr_pos)
    # We apply a modulation factor around 1.0
    
    mod = 1.0 + (asymmetry_param * current_oci)
    # Ensure modulation doesn't flip signs or explode
    mod = np.clip(mod, 0.1, 2.0) 
    
    lr_pos = np.clip(lr_base * mod, 0, 1)
    lr_neg = np.clip(lr_base * (1/mod), 0, 1)

    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        if a1 == -1 or s_idx == -1 or a2 == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # Stage 1 Choice
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2 Choice
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Learning Stage 2
        pe2 = r - q_stage2[s_idx, a2]
        effective_lr2 = lr_pos if pe2 >= 0 else lr_neg
        q_stage2[s_idx, a2] += effective_lr2 * pe2
        
        # Passive decay for unchosen stage 2 alien in current state
        unchosen_a2 = 1 - a2
        q_stage2[s_idx, unchosen_a2] *= (1 - decay)

        # Learning Stage 1
        # Use Stage 2 Q-value as proxy for reward in TD(0)
        pe1 = q_stage2[s_idx, a2] - q_stage1[a1]
        effective_lr1 = lr_pos if pe1 >= 0 else lr_neg
        q_stage1[a1] += effective_lr1 * pe1
        
        # Passive decay for unchosen stage 1 spaceship
        unchosen_a1 = 1 - a1
        q_stage1[unchosen_a1] *= (1 - decay)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Uncertainty-Driven Exploration Modulated by OCI
This model posits that OCI relates to intolerance of uncertainty. In this model, the "uncertainty" of an option (tracked roughly by how long it has been since it was last chosen) adds a bonus or penalty to the Q-value. High OCI participants might be "uncertainty avoidant" (penalizing unexplored options) or engage in "checking" behavior (uncertainty seeking).

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free RL with Uncertainty Bonus/Penalty modulated by OCI.
    
    Hypothesis: OCI scores correlate with how uncertainty (time since last selection)
    affects choice. High OCI might lead to uncertainty avoidance (negative parameter)
    or checking behaviors (positive parameter).
    
    Parameters:
    - learning_rate: [0, 1]
    - beta: [0, 10]
    - phi_base: Base exploration bonus parameter [-1, 1]
    - phi_oci_slope: How OCI changes the exploration bonus [-2, 2]
    """
    learning_rate, beta, phi_base, phi_oci_slope = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    # Calculate exploration sensitivity
    # Positive phi -> Novelty bonus (explore)
    # Negative phi -> Uncertainty aversion (exploit/avoid)
    phi = phi_base + (phi_oci_slope * current_oci)

    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    # Track "days since last chosen" as a proxy for uncertainty
    last_chosen_stage1 = np.zeros(2) 
    last_chosen_stage2 = np.zeros((2, 2)) 
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        if a1 == -1 or s_idx == -1 or a2 == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # --- Stage 1 Decision ---
        # Add uncertainty bonus: phi * sqrt(time_since_last_choice)
        # We use a simple counter for time
        uncertainty_1 = np.sqrt(last_chosen_stage1)
        q_net_1 = q_stage1 + (phi * uncertainty_1)
        
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Update counters
        last_chosen_stage1 += 1
        last_chosen_stage1[a1] = 0

        # --- Stage 2 Decision ---
        uncertainty_2 = np.sqrt(last_chosen_stage2[s_idx])
        q_net_2 = q_stage2[s_idx] + (phi * uncertainty_2)
        
        exp_q2 = np.exp(beta * q_net_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # Update counters
        last_chosen_stage2[s_idx] += 1
        last_chosen_stage2[s_idx, a2] = 0

        # --- Learning ---
        # Standard TD updates
        pe2 = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += learning_rate * pe2
        
        pe1 = q_stage2[s_idx, a2] - q_stage1[a1]
        q_stage1[a1] += learning_rate * pe1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```