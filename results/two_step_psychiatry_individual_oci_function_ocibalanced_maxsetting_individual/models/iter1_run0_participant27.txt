Here are three cognitive models based on the participant data and OCI score.

### Model 1: Stage-Specific Learning with OCI Modulation
This model separates learning rates for the first stage (spaceship choice) and second stage (alien choice). It hypothesizes that the participant's obsessive-compulsive symptoms (OCI) specifically impair the flexibility of the high-level planning (Stage 1) update, making it slower or less efficient, while the second stage (direct reward learning) remains distinct.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model with separate learning rates for Stage 1 and Stage 2.
    OCI modulates the Stage 1 learning rate, hypothesizing that symptoms affect 
    the update of the high-level choice policy.

    Parameters:
    lr_1_base: [0,1] - Base learning rate for Stage 1 (MF).
    lr_2: [0,1] - Learning rate for Stage 2.
    beta: [0,10] - Inverse temperature for both stages.
    w: [0,1] - Weighting between Model-Based and Model-Free values.
    lam: [0,1] - Eligibility trace (lambda).
    stickiness: [0,5] - Choice perseverance bonus.
    """
    lr_1_base, lr_2, beta, w, lam, stickiness = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # OCI modulation: Higher OCI reduces Stage 1 learning rate (rigidity/slow updating)
    # We clamp the modulation to ensure lr remains valid.
    lr_1 = lr_1_base * (1.0 - 0.5 * oci_score) 

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    
    last_action_1 = -1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Handle missing data
        if a1 == -1 or s_idx == -1 or a2 == -1:
            p_choice_1[trial] = 0.5 # Assume uniform if missing
            p_choice_2[trial] = 0.5
            continue

        # --- Stage 1 Choice ---
        # Model-Based Value
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Stickiness
        if last_action_1 != -1:
            q_net_1[last_action_1] += stickiness
            
        # Softmax Stage 1
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Choice ---
        # Softmax Stage 2
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        # Stage 1 PE
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr_1 * delta_stage1
        
        # Stage 2 PE
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr_2 * delta_stage2
        
        # Eligibility Trace Update (Stage 1 update from Stage 2 R)
        q_stage1_mf[a1] += lr_1 * lam * delta_stage2
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Trace-Based Stickiness with OCI Modulation
This model implements a "Choice Trace" (or habit strength) that decays over time, rather than just repeating the immediately previous action. This accounts for the long streaks of identical choices observed in the data. The OCI score modulates the amplitude of this trace-based stickiness, testing the hypothesis that compulsive symptoms increase the strength of habit formation.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model with decaying choice trace stickiness.
    Stickiness is based on an exponential moving average of past choices (Trace).
    OCI scales the impact of this trace on future choices.

    Parameters:
    lr: [0,1] - Learning rate.
    beta: [0,10] - Inverse temperature.
    w: [0,1] - Weighting between MB and MF.
    lam: [0,1] - Eligibility trace.
    decay: [0,1] - Decay rate of the choice trace (habit memory).
    stick_base: [0,5] - Base magnitude of stickiness.
    """
    lr, beta, w, lam, decay, stick_base = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # OCI increases the weight of habitual stickiness
    stickiness_amp = stick_base * (1.0 + oci_score)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Choice trace for Stage 1 actions (0 and 1)
    choice_trace = np.zeros(2)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        if a1 == -1 or s_idx == -1 or a2 == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add trace-based stickiness
        q_net_1 += stickiness_amp * choice_trace
            
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        # Update Choice Trace
        choice_trace *= decay
        choice_trace[a1] += 1.0
        
        # MF Updates
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_stage2
        
        q_stage1_mf[a1] += lr * lam * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Dual-Beta with OCI Modulation
This model proposes that the participant exhibits different levels of decision noise (exploration/exploitation) in the two stages. It uses two inverse temperature parameters (`beta_1` and `beta_2`). The OCI score modulates `beta_1`, testing the hypothesis that obsessive-compulsive traits lead to increased rigidity (higher beta) specifically in the primary spaceship choice, potentially overriding value-based trade-offs.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model with separate inverse temperatures (beta) for Stage 1 and Stage 2.
    OCI modulates Beta 1, hypothesizing that symptoms lead to more deterministic (rigid)
    behavior in the first stage choice.

    Parameters:
    lr: [0,1] - Learning rate.
    beta_1_base: [0,10] - Base inverse temperature for Stage 1.
    beta_2: [0,10] - Inverse temperature for Stage 2.
    w: [0,1] - Weighting between MB and MF.
    lam: [0,1] - Eligibility trace.
    stickiness: [0,5] - Simple choice perseverance.
    """
    lr, beta_1_base, beta_2, w, lam, stickiness = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # OCI increases rigidity (higher beta) in stage 1
    beta_1 = beta_1_base * (1.0 + oci_score)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        if a1 == -1 or s_idx == -1 or a2 == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_action_1 != -1:
            q_net_1[last_action_1] += stickiness
            
        # Use Beta 1
        exp_q1 = np.exp(beta_1 * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Choice ---
        # Use Beta 2
        exp_q2 = np.exp(beta_2 * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_stage2
        
        q_stage1_mf[a1] += lr * lam * delta_stage2
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```