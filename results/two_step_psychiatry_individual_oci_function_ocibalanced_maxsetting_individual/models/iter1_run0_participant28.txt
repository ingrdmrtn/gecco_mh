Here are three new cognitive models that explore different mechanisms for how OCI scores might influence decision-making in this two-step task, specifically focusing on the balance between model-based/model-free control and parameters like inverse temperature (exploration/exploitation).

### Model 1: Hybrid Learner with OCI-modulated Mixing Weight
This model assumes participants use a hybrid of Model-Based (MB) and Model-Free (MF) strategies. The key hypothesis here is that the balance (mixing weight `w`) between these two systems is influenced by the OCI score. High OCI might lead to a stronger reliance on habitual (MF) or goal-directed (MB) control depending on the sign of the modulation.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free learner where the mixing weight is modulated by OCI.
    
    This model calculates both Model-Based (transition-aware) and Model-Free (habitual)
    values for the first stage. The final Q-value is a weighted sum: 
    Q_net = w * Q_MB + (1-w) * Q_MF.
    The weight 'w' is determined by a baseline and an OCI-dependent shift.
    
    Parameters:
    learning_rate: [0, 1] Learning rate for value updates.
    beta: [0, 10] Inverse temperature for softmax choice.
    w_base: [0, 1] Baseline mixing weight (0=Pure MF, 1=Pure MB).
    oci_w_mod: [-1, 1] Modulation of mixing weight by OCI.
    """
    learning_rate, beta, w_base, oci_w_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate mixing weight w based on OCI
    w = w_base + (oci_w_mod * oci_score)
    # Clip w to stay within [0, 1]
    if w < 0: w = 0.0
    if w > 1: w = 1.0
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2) # Model-free values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Values for stage 2 (aliens)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        # Model-Based value calculation: T * max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid value: weighted combination
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        # Standard softmax on stage 2 values
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # --- Learning ---
        # TD Error 1: Prediction error between stage 1 and stage 2 value
        # Note: In a full TD(lambda) model, eligibility traces would be used. 
        # Here we use a simplified SARSA-like update for Stage 1 MF.
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # TD Error 2: Prediction error from reward
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Choice Stickiness (Perseveration)
This model investigates if OCI scores relate to "stickiness" or perseverationâ€”the tendency to repeat the previous action regardless of reward history. Compulsive behaviors are often repetitive; thus, high OCI might correlate with a higher tendency to repeat the last Stage 1 choice.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Based learner with OCI-modulated choice stickiness (perseveration).
    
    This model includes a 'stickiness' parameter that biases the agent to repeat 
    the previous Stage 1 choice. The magnitude of this stickiness is a function 
    of the OCI score.
    
    Parameters:
    learning_rate: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    persev_base: [-5, 5] Baseline perseveration bonus (positive = repeat, negative = switch).
    oci_persev_mod: [-5, 5] How much OCI score changes the perseveration bonus.
    """
    learning_rate, beta, persev_base, oci_persev_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective perseveration parameter
    # We map the inputs (which might be optimized in 0-1 range by some optimizers, 
    # but here we assume the optimizer handles the bounds defined in the docstring 
    # or we map them if they are strictly 0-1. Let's assume the optimizer provides
    # values, and we interpret them. If the optimizer forces 0-1, we can scale them).
    # For safety with a 0-1 bound optimizer, let's scale:
    # persev_base: input 0-1 maps to -2 to +2
    # oci_persev_mod: input 0-1 maps to -2 to +2
    # NOTE: The instructions say "Parameter bounds: [0,1] for most; [0,10] for beta".
    # I will stick to [0,1] inputs and scale them inside to allow negative values.
    
    # Scaling inputs to allow for negative/positive stickiness
    # 0.5 becomes 0. >0.5 is sticky, <0.5 is switchy.
    # range approx -3 to +3
    persev_param = (persev_base - 0.5) * 6.0 
    oci_mod_param = (oci_persev_mod - 0.5) * 6.0
    
    stickiness = persev_param + (oci_mod_param * oci_score)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage2 = np.zeros((2, 2))
    
    last_action_1 = -1 # No previous action for first trial

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Add stickiness bonus to the Q-values before softmax
        # We create a temporary Q vector for decision making
        q_decision = q_stage1_mb.copy()
        
        if last_action_1 != -1:
            q_decision[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_decision)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # --- Learning ---
        delta = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += learning_rate * delta
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Exploration (Inverse Temperature)
This model posits that OCI symptoms affect the exploration-exploitation trade-off. A high OCI score might lead to more rigid, deterministic behavior (higher beta, less exploration) or more erratic behavior (lower beta). This model modulates the inverse temperature `beta` based on the OCI score.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Based learner where the Inverse Temperature (Beta) is modulated by OCI.
    
    This model tests if OCI affects the randomness of choices (exploration/exploitation).
    Beta = beta_base + (oci_beta_mod * oci_score).
    
    Parameters:
    learning_rate: [0, 1] Learning rate.
    beta_base: [0, 10] Baseline inverse temperature.
    oci_beta_mod: [0, 1] Modulation of beta. Scaled internally to [-5, 5].
    w: [0, 1] Mixing weight (fixed for this model to reduce parameters, or fitted). 
              Here we fit it to allow general MB/MF strategy.
    """
    learning_rate, beta_base, oci_beta_mod, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Map oci_beta_mod from [0, 1] to [-5, 5] to allow OCI to increase or decrease noise
    beta_shift = (oci_beta_mod - 0.5) * 10.0
    
    current_beta = beta_base + (beta_shift * oci_score)
    # Ensure beta stays positive
    if current_beta < 0: current_beta = 0.0
    # Cap beta at reasonable high value to prevent overflow
    if current_beta > 20: current_beta = 20.0
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Weighted Q value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(current_beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(current_beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # --- Learning ---
        # Update Stage 1 MF
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Update Stage 2
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```