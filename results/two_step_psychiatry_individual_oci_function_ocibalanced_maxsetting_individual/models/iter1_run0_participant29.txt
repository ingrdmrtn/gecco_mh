Here are three cognitive models that incorporate the OCI (Obsessive-Compulsive Inventory) score to explain the participant's decision-making behavior, specifically focusing on the high degree of repetition (stickiness) and potential resistance to negative feedback observed in the data.

### Model 1: OCI-Modulated Stickiness (Compulsive Perseveration)
This model hypothesizes that the "stickiness" or choice perseveration parameter is not constant but is driven by the participant's OCI score. The data shows extreme repetition (e.g., choosing Spaceship 0 for 58 consecutive trials), suggesting that higher obsessive-compulsive traits might lead to a stronger urge to repeat the previous action regardless of the reward value.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model where OCI modulates the Stickiness (Perseveration) parameter.
    Hypothesis: Participants with higher OCI scores exhibit higher choice perseveration
    (compulsive repetition), modeled as a base stickiness plus an OCI-dependent boost.
    
    Parameters:
    learning_rate: [0, 1] Learning rate for value updates.
    beta: [0, 10] Inverse temperature for softmax.
    w: [0, 1] Mixing weight (0=MF, 1=MB).
    stick_base: [0, 5] Baseline choice stickiness.
    stick_oci_sens: [0, 5] Sensitivity of stickiness to the OCI score.
    """
    learning_rate, beta, w, stick_base, stick_oci_sens = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Stickiness is enhanced by OCI score
    stickiness = stick_base + (stick_oci_sens * oci_score)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_a1 = -1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        state_idx = int(state[trial])
        a2 = int(action_2[trial])

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Apply OCI-modulated stickiness
        if prev_a1 != -1:
            q_net[prev_a1] += stickiness
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Updates
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Note: No lambda eligibility trace in this model to isolate stickiness effects
        
        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated MB/MF Balance (Habitual Reliance)
This model hypothesizes that OCI scores correlate with a shift from Model-Based (goal-directed) to Model-Free (habitual) control. The participant's repetitive behavior might reflect a reliance on simple cached values (MF) rather than the transition structure (MB). Here, the mixing weight $w$ is down-weighted by the OCI score.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model where OCI modulates the mixing weight 'w'.
    Hypothesis: Higher OCI scores reduce Model-Based control (goal-directedness)
    and increase reliance on Model-Free (habitual) control.
    
    Parameters:
    learning_rate: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    w_base: [0, 1] Base mixing weight (assuming OCI=0).
    stickiness: [0, 5] Choice perseveration bonus.
    lambda_decay: [0, 1] Eligibility trace decay for Stage 1 MF updates.
    """
    learning_rate, beta, w_base, stickiness, lambda_decay = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Effective w decreases as OCI increases (Shift towards MF)
    w = w_base * (1.0 - oci_score)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_a1 = -1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        state_idx = int(state[trial])
        a2 = int(action_2[trial])

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Mix using OCI-adjusted w
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if prev_a1 != -1:
            q_net[prev_a1] += stickiness
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Updates
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Eligibility trace update
        q_stage1_mf[a1] += learning_rate * lambda_decay * delta_stage2
        
        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Asymmetric Learning (Rigidity to Failure)
The participant data shows persistence in choosing Spaceship 0 even after receiving 0 coins (negative prediction errors). This model proposes that higher OCI scores lead to "cognitive rigidity" specifically by dampening the learning rate for negative outcomes. This makes the participant slower to unlearn a behavior when it fails.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Asymmetric Learning Rate model where OCI dampens negative learning.
    Hypothesis: Higher OCI scores lead to rigidity/persistence by reducing 
    the learning rate specifically when outcomes are worse than expected (negative delta).
    
    Parameters:
    lr_pos: [0, 1] Learning rate for positive prediction errors.
    lr_neg_base: [0, 1] Base learning rate for negative prediction errors.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Mixing weight.
    stickiness: [0, 5] Choice perseveration.
    """
    lr_pos, lr_neg_base, beta, w, stickiness = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # OCI reduces the effective negative learning rate (ignoring failures)
    # If OCI is high, lr_neg becomes small, preserving old values despite 0 reward.
    lr_neg = lr_neg_base * (1.0 - oci_score)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_a1 = -1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        state_idx = int(state[trial])
        a2 = int(action_2[trial])

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if prev_a1 != -1:
            q_net[prev_a1] += stickiness
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Updates with Asymmetric Learning Rates
        
        # Stage 1 MF Update
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        lr_1 = lr_pos if delta_stage1 >= 0 else lr_neg
        q_stage1_mf[a1] += lr_1 * delta_stage1
        
        # Stage 2 MF Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        lr_2 = lr_pos if delta_stage2 >= 0 else lr_neg
        q_stage2_mf[state_idx, a2] += lr_2 * delta_stage2
        
        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```