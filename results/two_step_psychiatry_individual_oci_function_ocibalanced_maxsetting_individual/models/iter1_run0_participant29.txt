Here are three new cognitive models exploring different mechanisms for how OCI scores might influence decision-making in the two-step task, specifically focusing on how obsessive-compulsive traits might alter learning rates, reward sensitivity, or the balance between goal-directed and habitual control.

### Cognitive Model 1: OCI-Modulated Model-Based Weight
This model tests the hypothesis that higher OCI scores, often associated with rigidity and habit formation, might lead to a reduction in model-based (goal-directed) planning. Here, the mixing weight `w` (which balances Model-Based and Model-Free control) is a function of the OCI score.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Model-Based Weight.
    
    Hypothesis: Higher OCI scores reduce the reliance on model-based (goal-directed) 
    strategies in favor of model-free (habitual) ones.
    
    The mixing weight 'w' is calculated as a logistic function of the OCI score,
    constrained between 0 (pure MF) and 1 (pure MB).
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for value updates.
    - beta: [0, 10] Inverse temperature (softness of choice).
    - w_intercept: [0, 10] Base log-odds of using MB strategy.
    - w_slope: [-10, 10] How strongly OCI affects the MB weight (negative = less MB with high OCI).
    """
    learning_rate, beta, w_intercept, w_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate mixing weight w using a sigmoid function to keep it in [0, 1]
    # If w_slope is negative, higher OCI leads to lower w (more Model-Free)
    w_logit = w_intercept + (w_slope * oci_score)
    w = 1 / (1 + np.exp(-w_logit))

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        # Model-Based Value: Expected value of best option in next state
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        # Stage 2 Update (TD)
        delta_stage2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += learning_rate * delta_stage2
        
        # Stage 1 Update (TD using stage 2 value)
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2: OCI-Specific Learning Rates (Positive vs Negative)
This model investigates if OCI traits lead to asymmetric learning from positive versus negative outcomes. It proposes that individuals with higher OCI might be hyper-sensitive to errors (loss aversion/negative feedback) or rigid in updating beliefs. The model splits the learning rate into two, modulated by OCI.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Specific Asymmetric Learning Rates.
    
    Hypothesis: OCI scores differentially impact how participants learn from 
    rewards (positive prediction errors) versus lack of rewards (negative prediction errors).
    
    Parameters:
    - alpha_base: [0, 1] Baseline learning rate.
    - alpha_oci_pos: [-1, 1] Adjustment to learning rate for positive outcomes based on OCI.
    - alpha_oci_neg: [-1, 1] Adjustment to learning rate for negative outcomes based on OCI.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Fixed MB/MF mixing weight.
    """
    alpha_base, alpha_oci_pos, alpha_oci_neg, beta, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective learning rates, clipped to [0, 1]
    lr_pos = np.clip(alpha_base + (alpha_oci_pos * oci_score), 0.0, 1.0)
    lr_neg = np.clip(alpha_base + (alpha_oci_neg * oci_score), 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        # Determine which learning rate to use based on Prediction Error sign? 
        # Or simply based on Reward presence (simplified for this task where R is 0 or 1)
        current_lr = lr_pos if r > 0 else lr_neg

        delta_stage2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += current_lr * delta_stage2
        
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += current_lr * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3: OCI-Driven Inverse Temperature (Exploration/Exploitation)
This model posits that OCI affects the "randomness" or determinism of choices. High OCI is often linked to rigid behavior. This model tests if higher OCI scores lead to a higher `beta` (inverse temperature), resulting in more exploitative (deterministic) choices and less exploration, regardless of the learning strategy.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Driven Inverse Temperature.
    
    Hypothesis: OCI scores influence the exploration-exploitation balance. 
    High OCI may lead to more rigid, deterministic choices (higher beta),
    while low OCI allows for more noise/exploration.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta_base: [0, 10] Base inverse temperature.
    - beta_oci_scale: [-5, 5] Scaling factor for OCI's effect on beta.
    - w: [0, 1] MB/MF mixing weight.
    - eligibility: [0, 1] Eligibility trace (lambda) for Stage 1 update.
    """
    learning_rate, beta_base, beta_oci_scale, w, eligibility = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Beta must be non-negative. 
    # If beta_oci_scale is positive, high OCI -> high Beta (rigid/exploitative).
    beta = np.maximum(0.0, beta_base + (beta_oci_scale * oci_score))

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Use the OCI-modulated beta
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += learning_rate * delta_stage2
        
        # Stage 1 Update with Eligibility Trace
        # This updates Stage 1 Q-values based on the Stage 2 RPE, weighted by eligibility
        q_stage1_mf[a1] += learning_rate * eligibility * delta_stage2
        
        # Standard Stage 1 TD update (transition from Stage 1 to Stage 2)
        # Note: In standard TD(1) or lambda models, the update above covers the full path.
        # But here we often mix TD(0) for stage 1 and TD(1) via eligibility.
        # Simple TD(0) component:
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * (1 - eligibility) * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```