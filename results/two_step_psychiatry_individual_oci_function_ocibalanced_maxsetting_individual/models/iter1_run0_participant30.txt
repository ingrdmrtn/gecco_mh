Here are the three proposed cognitive models.

### Model 1
```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model with OCI-Dependent Stickiness.
    
    Hypothesis: High OCI scores correlate with increased perseveration (stickiness) in choice behavior.
    While the Model-Based/Model-Free weighting (w) is constant, the tendency to repeat the previous 
    stage-1 choice is scaled by the participant's OCI score.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for value updates.
    - beta: [0, 10] Inverse temperature for softmax.
    - w: [0, 1] Mixing weight for Model-Based system (0=MF, 1=MB).
    - stick_oci_scale: [0, 5] Scaling factor for stickiness based on OCI. 
      Effective stickiness = stick_oci_scale * OCI.
    """
    learning_rate, beta, w, stick_oci_scale = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Stickiness is directly proportional to OCI
    stickiness = stick_oci_scale * oci_score
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Initialize Q-values
    q_stage1_mf = np.zeros(2) + 0.5
    q_stage2_mf = np.zeros((2, 2)) + 0.5
    
    log_loss = 0.0
    eps = 1e-10
    
    prev_a1 = -1

    for trial in range(n_trials):
        # Skip missing data
        if action_1[trial] == -1:
            prev_a1 = -1
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        # Model-Based Value Calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Q-value mixing MB and MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness bonus to the previously chosen action
        if prev_a1 != -1:
            q_net[prev_a1] += stickiness
            
        # Softmax choice probability
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + eps)

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[a2] + eps)

        # --- Learning ---
        # Stage 1 Update (TD-like)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        prev_a1 = a1

    return log_loss
```

### Model 2
```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model with OCI-Modulated Negative Learning Rate.
    
    Hypothesis: Participants with high OCI symptoms may exhibit hypersensitivity to negative outcomes 
    (punishment or lack of reward). This model allows the learning rate for negative prediction errors 
    to be scaled up by the OCI score, while positive updates use the base learning rate.
    
    Parameters:
    - learning_rate: [0, 1] Base learning rate for positive prediction errors.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight for Model-Based system.
    - neg_lr_scale: [0, 5] Multiplier for OCI effect on negative learning rate.
      lr_neg = learning_rate * (1 + neg_lr_scale * OCI).
    """
    learning_rate, beta, w, neg_lr_scale = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Define asymmetric learning rates
    # lr_pos is the base rate.
    # lr_neg is boosted by OCI, capped at 1.0 for stability.
    lr_pos = learning_rate
    lr_neg_raw = learning_rate * (1.0 + neg_lr_scale * oci_score)
    lr_neg = min(1.0, lr_neg_raw)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2) + 0.5
    q_stage2_mf = np.zeros((2, 2)) + 0.5
    
    log_loss = 0.0
    eps = 1e-10

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + eps)

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[a2] + eps)

        # --- Learning ---
        # Stage 1 Update with asymmetry
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        eff_lr_1 = lr_neg if delta_stage1 < 0 else lr_pos
        q_stage1_mf[a1] += eff_lr_1 * delta_stage1
        
        # Stage 2 Update with asymmetry
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        eff_lr_2 = lr_neg if delta_stage2 < 0 else lr_pos
        q_stage2_mf[s_idx, a2] += eff_lr_2 * delta_stage2

    return log_loss
```

### Model 3
```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model with OCI-Suppressed MB Weight and Baseline Stickiness.
    
    Hypothesis: Builds on the previous best model where OCI suppresses Model-Based control, 
    but adds a constant stickiness parameter. This tests whether the data is best explained 
    by a combination of OCI-induced habituality (low MB) and general choice inertia (stickiness),
    rather than OCI driving stickiness directly.
    
    Parameters:
    - learning_rate: [0, 1]
    - beta: [0, 10]
    - w_max: [0, 1] The maximum MB weight (realized when OCI=0).
    - stick: [0, 5] General stickiness parameter applied to the previous choice (independent of OCI).
    """
    learning_rate, beta, w_max, stick = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # OCI suppresses the Model-Based weight
    w_effective = w_max * (1.0 - oci_score)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2) + 0.5
    q_stage2_mf = np.zeros((2, 2)) + 0.5
    
    log_loss = 0.0
    eps = 1e-10
    
    prev_a1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1:
            prev_a1 = -1
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w_effective * q_stage1_mb + (1 - w_effective) * q_stage1_mf
        
        # Add baseline stickiness
        if prev_a1 != -1:
            q_net[prev_a1] += stick
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + eps)

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[a2] + eps)

        # --- Learning ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        prev_a1 = a1

    return log_loss
```