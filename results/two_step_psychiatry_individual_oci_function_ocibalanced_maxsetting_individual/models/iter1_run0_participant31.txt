Here are the three proposed cognitive models.

### Model 1: OCI-Modulated Inverse Temperature (Beta)
This model hypothesizes that the participant's obsessive-compulsive symptoms (OCI) affect the randomness of their choices (exploration vs. exploitation). A high OCI score modulates the inverse temperature parameter ($\beta$), potentially leading to more rigid (higher $\beta$) or more erratic (lower $\beta$) behavior compared to the baseline.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Inverse Temperature (Beta) Model.
    
    This model assumes that the level of obsessive-compulsive symptoms (OCI)
    modulates the exploration-exploitation balance (beta). 
    
    Parameters:
    - learning_rate: [0,1] The learning rate for value updates.
    - w: [0,1] The weight between Model-Based (1) and Model-Free (0) control.
    - beta_base: [0,10] Base inverse temperature.
    - beta_oci_param: [0,1] Effect of OCI on beta (mapped to [-5, 5]).
    """
    learning_rate, w, beta_base, beta_oci_param = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Map beta_oci_param [0,1] to range [-5, 5]
    # Effective beta = base + (modifier * oci)
    beta_modifier = (beta_oci_param * 10.0) - 5.0
    beta_val = beta_base + (beta_modifier * oci_score)
    beta_val = np.maximum(0.0, beta_val) # Ensure beta is non-negative
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax with modulated beta
        logits_1 = beta_val * (q_net_1 - np.max(q_net_1))
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Decision ---
        q_vals_2 = q_stage2_mf[s_idx]
        logits_2 = beta_val * (q_vals_2 - np.max(q_vals_2))
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Stage 1 Update (TD0)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Learning Rate Asymmetry
This model allows for different learning rates for positive (better than expected) and negative (worse than expected) prediction errors. The ratio between the negative and positive learning rates is modulated by the OCI score, testing if high-OCI participants are hypersensitive to punishment (losses) or rewards.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Learning Rate Asymmetry Model.
    
    This model allows for different learning rates for positive and negative
    prediction errors (PE). The OCI score modulates the ratio of the 
    negative learning rate relative to the positive learning rate.
    
    Parameters:
    - lr_base: [0,1] Base learning rate (used for positive PE).
    - beta: [0,10] Inverse temperature.
    - w: [0,1] Weight for MB vs MF.
    - ratio_base: [0,1] Base ratio of neg/pos LR (mapped to [0, 2]).
    - ratio_oci: [0,1] Effect of OCI on the ratio (mapped to [-1, 1]).
    """
    lr_base, beta, w, ratio_base, ratio_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate asymmetry ratio
    # ratio_base maps 0-1 to 0-2. ratio_oci maps 0-1 to -1 to +1.
    ratio = (ratio_base * 2.0) + ((ratio_oci * 2.0 - 1.0) * oci_score)
    ratio = np.maximum(0.0, ratio)
    
    lr_pos = lr_base
    lr_neg = lr_base * ratio
    # Cap learning rate at 1.0 for stability
    lr_neg = np.minimum(1.0, lr_neg)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * (q_net_1 - np.max(q_net_1)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Decision ---
        q_vals_2 = q_stage2_mf[s_idx]
        exp_q2 = np.exp(beta * (q_vals_2 - np.max(q_vals_2)))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Apply asymmetry to both stages based on the sign of the prediction error
        
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        curr_lr_1 = lr_pos if delta_stage1 >= 0 else lr_neg
        q_stage1_mf[a1] += curr_lr_1 * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        curr_lr_2 = lr_pos if delta_stage2 >= 0 else lr_neg
        q_stage2_mf[s_idx, a2] += curr_lr_2 * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Eligibility Trace (Lambda)
This model introduces an eligibility trace parameter, $\lambda$, which controls how effectively the reward obtained in Stage 2 reinforces the choice made in Stage 1. This implements a TD($\lambda$) update. The OCI score modulates $\lambda$, testing if high OCI leads to stronger (or weaker) direct reinforcement of the initial spaceship choice by the final outcome.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Eligibility Trace (Lambda) Model.
    
    This model introduces an eligibility trace parameter (lambda) that controls
    how much the Stage 1 value is updated by the final reward outcome directly
    (via the eligibility trace of the first choice). OCI modulates lambda.
    
    Parameters:
    - learning_rate: [0,1] Update rate.
    - beta: [0,10] Inverse temperature.
    - w: [0,1] Weight for MB vs MF.
    - lam_base: [0,1] Base eligibility trace value.
    - lam_oci: [0,1] Effect of OCI on lambda (mapped to [-1, 1]).
    """
    learning_rate, beta, w, lam_base, lam_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate lambda
    # Map lam_oci [0,1] to range [-1, 1]
    lam_val = lam_base + ((lam_oci * 2.0 - 1.0) * oci_score)
    lam_val = np.clip(lam_val, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * (q_net_1 - np.max(q_net_1)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Decision ---
        q_vals_2 = q_stage2_mf[s_idx]
        exp_q2 = np.exp(beta * (q_vals_2 - np.max(q_vals_2)))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        # TD(lambda) update for Stage 1:
        # The Stage 1 choice eligibility trace allows it to be updated by delta_stage2 scaled by lambda.
        # Update = alpha * (delta1 + lambda * delta2)
        q_stage1_mf[a1] += learning_rate * (delta_stage1 + lam_val * delta_stage2)
        
        # TD(0) update for Stage 2
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```