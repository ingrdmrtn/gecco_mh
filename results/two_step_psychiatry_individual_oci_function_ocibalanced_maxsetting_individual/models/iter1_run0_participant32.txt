Here are three cognitive models formulated as Python functions. These models incorporate the OCI score to explain specific behavioral tendencies associated with obsessive-compulsive symptoms, such as deficits in model-based control, hypersensitivity to punishment, and decision rigidity.

### Model 1: Hybrid Model with OCI-Modulated Model-Based Deficit
This model tests the hypothesis that high OCI scores are associated with a deficit in goal-directed (Model-Based) control, leading to a reliance on the habitual (Model-Free) system. The mixing weight $w$ is constrained by the OCI score.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model with OCI-Modulated Model-Based Deficit.
    
    Hypothesis: Participants with high OCI scores exhibit reduced Model-Based (MB) control.
    The effective mixing weight 'w' is attenuated by the OCI score, forcing high-OCI
    participants towards Model-Free (MF) behavior.
    
    w_effective = w_param * (1 - oci_score)
    
    Parameters:
    - learning_rate: [0, 1] Rate of updating Q-values.
    - beta: [0, 10] Inverse temperature for softmax.
    - w_param: [0, 1] Base weight parameter for MB system.
    - stickiness: [0, 5] Bonus added to the Q-value of the previously chosen spaceship.
    """
    learning_rate, beta, w_param, stickiness = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective weight: High OCI reduces the contribution of the MB system
    w_eff = w_param * (1.0 - oci_score)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)      # Model-Free values for Stage 1
    q_stage2_mf = np.zeros((2, 2)) # Model-Free values for Stage 2 (State x Action)
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based valuation: V(s') = max Q(s', a')
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net value mixing MB and MF
        q_net = (w_eff * q_stage1_mb) + ((1 - w_eff) * q_stage1_mf)
        
        # Stickiness bonus
        stick_bonus = np.zeros(2)
        if last_action_1 != -1:
            stick_bonus[int(last_action_1)] = stickiness
            
        exp_q1 = np.exp(beta * (q_net + stick_bonus))
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        r = reward[trial]
        
        # TD(0) update for Stage 1 MF
        delta_1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_1
        
        # TD(0) update for Stage 2 MF
        delta_2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_2
        
        last_action_1 = a1
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Pure Model-Free with OCI-Enhanced Punishment Sensitivity
This model assumes the participant relies on a Model-Free strategy but learns asymmetrically. High OCI is hypothesized to correlate with a "fear of failure," causing stronger learning (larger updates) from negative prediction errors (disappointments/omission of reward) compared to positive ones.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Pure Model-Free Model with OCI-Enhanced Punishment Learning.
    
    Hypothesis: Participants with high OCI are hypersensitive to worse-than-expected outcomes.
    The learning rate for negative prediction errors is boosted by the OCI score.
    
    If delta < 0: alpha = learning_rate * (1 + punish_factor * oci_score)
    If delta > 0: alpha = learning_rate
    
    Parameters:
    - learning_rate: [0, 1] Base learning rate for positive prediction errors.
    - beta: [0, 10] Inverse temperature.
    - punish_factor: [0, 5] Scaling factor for OCI-based punishment sensitivity.
    - stickiness: [0, 5] Choice stickiness.
    """
    learning_rate, beta, punish_factor, stickiness = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Policy (Pure MF + Stickiness) ---
        stick_bonus = np.zeros(2)
        if last_action_1 != -1:
            stick_bonus[int(last_action_1)] = stickiness
            
        exp_q1 = np.exp(beta * (q_stage1_mf + stick_bonus))
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        r = reward[trial]
        
        # Stage 1 Update
        delta_1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        if delta_1 < 0:
            # Boost learning rate for disappointment
            lr_1 = min(learning_rate * (1 + punish_factor * oci_score), 1.0)
        else:
            lr_1 = learning_rate
        q_stage1_mf[a1] += lr_1 * delta_1
        
        # Stage 2 Update
        delta_2 = r - q_stage2_mf[s_idx, a2]
        if delta_2 < 0:
            # Boost learning rate for disappointment
            lr_2 = min(learning_rate * (1 + punish_factor * oci_score), 1.0)
        else:
            lr_2 = learning_rate
        q_stage2_mf[s_idx, a2] += lr_2 * delta_2
        
        last_action_1 = a1
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model with OCI-Modulated Rigidity
This model posits that OCI drives decision rigidity or compulsivity. Instead of affecting learning, OCI scales the inverse temperature ($\beta$) of the softmax function. High OCI leads to sharper, more deterministic choices (higher $\beta$), while low OCI allows for more exploration.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model with OCI-Modulated Rigidity (Inverse Temperature).
    
    Hypothesis: OCI acts as a driver for decision rigidity (compulsivity).
    The inverse temperature (beta) is scaled by OCI, leading to more deterministic 
    exploitation for high-OCI participants.
    
    beta_effective = beta_base * (1 + oci_rigidity * oci_score)
    
    Parameters:
    - learning_rate: [0, 1] Rate of updating Q-values.
    - beta_base: [0, 10] Base inverse temperature.
    - w: [0, 1] Model-Based weight.
    - stickiness: [0, 5] Choice stickiness.
    - oci_rigidity: [0, 5] Scaling factor for Beta based on OCI.
    """
    learning_rate, beta_base, w, stickiness, oci_rigidity = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective beta: High OCI increases rigidity
    beta_eff = beta_base * (1 + oci_rigidity * oci_score)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        stick_bonus = np.zeros(2)
        if last_action_1 != -1:
            stick_bonus[int(last_action_1)] = stickiness
            
        exp_q1 = np.exp(beta_eff * (q_net + stick_bonus))
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        
        exp_q2 = np.exp(beta_eff * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        r = reward[trial]
        
        delta_1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_1
        
        delta_2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_2
        
        last_action_1 = a1
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```