Here are three new cognitive models that explore different mechanisms for how OCI scores might influence decision-making in this two-step task.

### Model 1: Hybrid RL with OCI-modulated Model-Based Weighting
This model tests the hypothesis that higher OCI scores correlate with a stronger reliance on habitual (model-free) control at the expense of goal-directed (model-based) planning. It assumes that individuals with high compulsivity might be more "stuck" in habit-like responding. The mixing parameter `w` determines the balance between model-based (MB) and model-free (MF) values, and this balance is modulated by the OCI score.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free RL with OCI-modulated Weighting.

    Hypothesis: Higher OCI scores reduce the weight of model-based planning (w),
    leading to more habitual (model-free) behavior.

    Parameters:
    learning_rate: [0, 1] - Update rate for Q-values.
    beta: [0, 10] - Inverse temperature for softmax.
    w_max: [0, 1] - Maximum weight for model-based control (when OCI is 0).
    """
    learning_rate, beta, w_max = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate mixing weight w: High OCI reduces MB influence.
    # If OCI is 0, w = w_max. If OCI is 1, w approaches 0.
    w = w_max * (1.0 - oci_score)

    # Transition matrix (fixed for this task structure)
    # Row 0: Spaceship 0 -> [Planet 0 (0.7), Planet 1 (0.3)]
    # Row 1: Spaceship 1 -> [Planet 0 (0.3), Planet 1 (0.7)]
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Q-values
    q_mf_stage1 = np.zeros(2)      # Model-free Stage 1 values
    q_mf_stage2 = np.zeros((2, 2)) # Model-free Stage 2 values (also used for MB calculation)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        # Calculate Model-Based values: V_MB(s1) = Sum(P(s2|s1) * max_a2 Q(s2, a2))
        max_q_stage2 = np.max(q_mf_stage2, axis=1)
        q_mb_stage1 = transition_matrix @ max_q_stage2

        # Combine MB and MF values
        q_net_stage1 = w * q_mb_stage1 + (1 - w) * q_mf_stage1

        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_mf_stage2[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Stage 2 update (TD)
        delta_stage2 = r - q_mf_stage2[s, a2]
        q_mf_stage2[s, a2] += learning_rate * delta_stage2

        # Stage 1 update (TD - Model Free)
        # Note: In standard hybrid models, MF S1 is updated by S2 value or reward.
        # Here we use the standard TD(0) from the state value of the second stage.
        delta_stage1 = q_mf_stage2[s, a2] - q_mf_stage1[a1]
        q_mf_stage1[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Learning Rate Asymmetry
This model hypothesizes that high OCI scores are associated with an over-sensitivity to negative outcomes (or lack of reward). It implements separate learning rates for positive and negative prediction errors (learning from reward vs. learning from no reward), where the balance between these rates is shifted by the OCI score.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Dual Learning Rate Model with OCI-modulated Sensitivity.

    Hypothesis: High OCI individuals learn differently from positive vs negative outcomes.
    Specifically, OCI increases the learning rate for negative outcomes (alpha_neg)
    relative to a base rate.

    Parameters:
    alpha_base: [0, 1] - Base learning rate.
    beta: [0, 10] - Inverse temperature.
    neg_bias: [0, 1] - How much OCI enhances learning from non-reward (0).
    """
    alpha_base, beta, neg_bias = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # alpha_pos is the base rate
    alpha_pos = alpha_base
    
    # alpha_neg is boosted by OCI. 
    # If OCI is high, they update more drastically on failure.
    # We clip to ensure it stays in [0, 1].
    alpha_neg = min(1.0, alpha_base + (neg_bias * oci_score))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy (Model-Free only for simplicity) ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        
        # Stage 2 Update
        delta_stage2 = r - q_stage2[s, a2]
        if delta_stage2 >= 0:
            q_stage2[s, a2] += alpha_pos * delta_stage2
        else:
            q_stage2[s, a2] += alpha_neg * delta_stage2

        # Stage 1 Update
        delta_stage1 = q_stage2[s, a2] - q_stage1[a1]
        if delta_stage1 >= 0:
            q_stage1[a1] += alpha_pos * delta_stage1
        else:
            q_stage1[a1] += alpha_neg * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Randomness (Inverse Temperature)
This model tests the hypothesis that high OCI scores are related to decision noise or exploration/exploitation balance. It posits that higher compulsion might paradoxically lead to more erratic choices (lower beta) if the participant doubts their internal model, or more rigid choices (higher beta) if they are anxious. Here, we model `beta` as a function of OCI.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Reinforcement Learning with OCI-modulated Inverse Temperature.

    Hypothesis: OCI scores impact the exploration-exploitation trade-off.
    We model beta as a linear function of OCI: beta = beta_0 + beta_slope * OCI.
    This allows the model to find if OCI makes behavior more deterministic (positive slope)
    or more noisy (negative slope).

    Parameters:
    learning_rate: [0, 1] - Update rate.
    beta_0: [0, 10] - Baseline inverse temperature.
    beta_slope: [0, 10] - How much OCI scales beta.
    """
    learning_rate, beta_0, beta_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective beta
    # We constrain effective beta to be non-negative.
    # If beta_slope is high, high OCI -> very deterministic choices.
    beta_eff = beta_0 + (beta_slope * oci_score)
    if beta_eff < 0:
        beta_eff = 0.0

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        exp_q1 = np.exp(beta_eff * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta_eff * q_stage2[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Stage 2
        delta_stage2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += learning_rate * delta_stage2

        # Stage 1
        delta_stage1 = q_stage2[s, a2] - q_stage1[a1]
        q_stage1[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```