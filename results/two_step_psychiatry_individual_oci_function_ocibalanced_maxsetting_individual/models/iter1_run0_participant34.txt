Here are three new cognitive models that explore different mechanisms for how OCI symptoms might influence decision-making in this task, specifically focusing on the interplay between model-based/model-free control and habit formation.

### Model 1: Hybrid Model with OCI-Modulated Model-Based Weight
This model tests the hypothesis that high OCI scores are associated with a deficit in model-based control (goal-directed planning) in favor of model-free (habitual) control. The weighting parameter `w` determines the balance between these two systems, and this `w` is dampened by the OCI score.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free RL with OCI-modulated Weighting.
    
    Hypothesis: High OCI reduces the influence of the Model-Based system (w),
    leading to more reliance on Model-Free values.
    
    Parameters:
    learning_rate: [0, 1] Update rate for Q-values.
    beta: [0, 10] Inverse temperature for softmax.
    w_max: [0, 1] Maximum weight for model-based values (when OCI is 0).
    oci_w_penalty: [0, 1] Reduction in 'w' proportional to OCI score.
    """
    learning_rate, beta, w_max, oci_w_penalty = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate the mixing weight w based on OCI
    # Higher OCI reduces w, pushing behavior towards Model-Free (w=0)
    w = w_max * (1.0 - (oci_score * oci_w_penalty))
    w = np.clip(w, 0.0, 1.0) # Ensure w stays valid

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2) # Model-free Q-values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Model-free Q-values for stage 2

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # Model-Based Value Calculation: V_MB(s1) = T * max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value: Q_net = w * Q_MB + (1-w) * Q_MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # --- Stage 2 Decision ---
        state_idx = int(state[trial])
        
        # Standard Model-Free choice at Stage 2
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # --- Learning ---
        # Update Stage 2 Q-values (SARSA / Q-learning are equivalent here as terminal state)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2
        
        # Update Stage 1 Model-Free Q-values (TD(0))
        # Note: In standard hybrid models, MF updates via the stage 2 Q-value or reward
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Perseveration with Inverse Temperature Scaling
This model posits that high OCI symptoms lead to rigid behavior not just through simple stickiness, but by effectively increasing the inverse temperature (`beta`) specifically for the first-stage choice. This makes the participant more deterministic and less exploratory based on their current value estimates.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free RL with OCI-Driven Determinism (Beta Scaling).
    
    Hypothesis: High OCI increases the 'beta' (inverse temperature) parameter,
    making choices more deterministic/rigid and less sensitive to small value differences
    or exploration.
    
    Parameters:
    learning_rate: [0, 1] Update rate for Q-values.
    beta_base: [0, 10] Baseline inverse temperature.
    oci_beta_scale: [0, 5] Multiplier for beta based on OCI.
    stickiness: [0, 5] General choice stickiness (independent of OCI).
    """
    learning_rate, beta_base, oci_beta_scale, stickiness = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Effective beta is scaled up by OCI
    beta_eff = beta_base * (1.0 + oci_score * oci_beta_scale)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        q_augmented = q_stage1_mf.copy()
        if last_action_1 != -1:
            q_augmented[int(last_action_1)] += stickiness
            
        exp_q1 = np.exp(beta_eff * q_augmented)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        last_action_1 = action_1[trial]
        
        # --- Stage 2 Decision ---
        state_idx = int(state[trial])
        
        # Stage 2 usually has less stickiness/rigidity in literature, 
        # but we use the same beta_eff for consistency of the "rigidity" hypothesis.
        exp_q2 = np.exp(beta_eff * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # --- Learning ---
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Learning Rate Asymmetry (Positive vs Negative)
This model investigates if OCI affects how participants learn from positive versus negative prediction errors. It proposes that high OCI might lead to hypersensitivity to negative outcomes (or failure to learn from positive ones), causing anxiety-driven avoidance or safety behaviors.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Dual Learning Rate Model-Free RL with OCI Modulation on Negative Learning.
    
    Hypothesis: OCI modulates the learning rate specifically for negative prediction errors (alpha_neg).
    High OCI might lead to over-learning from disappointments (or under-learning),
    affecting how values are updated.
    
    Parameters:
    alpha_pos: [0, 1] Learning rate for positive prediction errors.
    alpha_neg_base: [0, 1] Base learning rate for negative prediction errors.
    oci_alpha_neg_mod: [0, 1] Modulation of negative learning rate by OCI.
    beta: [0, 10] Inverse temperature.
    """
    alpha_pos, alpha_neg_base, oci_alpha_neg_mod, beta = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective negative learning rate
    # We model it such that OCI increases sensitivity to negative errors
    alpha_neg = alpha_neg_base + (oci_score * oci_alpha_neg_mod)
    alpha_neg = np.clip(alpha_neg, 0.0, 1.0)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # --- Stage 2 Decision ---
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # --- Learning ---
        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        lr_2 = alpha_pos if delta_stage2 > 0 else alpha_neg
        q_stage2_mf[state_idx, int(action_2[trial])] += lr_2 * delta_stage2
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        lr_1 = alpha_pos if delta_stage1 > 0 else alpha_neg
        q_stage1_mf[int(action_1[trial])] += lr_1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```