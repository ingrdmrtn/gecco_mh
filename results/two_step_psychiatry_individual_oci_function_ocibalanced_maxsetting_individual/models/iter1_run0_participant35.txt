Here are three new cognitive models designed to capture different potential mechanisms by which obsessive-compulsive symptoms (OCI) might influence decision-making in this two-step task.

### Model 1: Stickiness Modulation by OCI
This model hypothesizes that high OCI scores lead to increased "stickiness" or choice perseveration. Rather than just affecting the balance between model-based and model-free control, OCI might drive a compulsion to repeat previous actions regardless of reward history. Here, the stickiness parameter is directly scaled by the OCI score.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Based/Model-Free Hybrid with OCI-modulated Stickiness.
    
    Hypothesis: Participants with high OCI scores exhibit higher choice perseveration 
    (stickiness), repeating the same stage 1 choice regardless of outcomes.
    The stickiness parameter is defined as: stickiness = stickiness_base + stickiness_oci * oci_score.
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    w: [0, 1] (Mixing weight: 1=Model-Based, 0=Model-Free)
    stickiness_base: [0, 5]
    stickiness_oci: [0, 5]
    """
    learning_rate, beta, w, stickiness_base, stickiness_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective stickiness based on OCI
    stickiness = stickiness_base + (stickiness_oci * oci_score)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_mf1 = np.zeros(2)      # Model-free values for stage 1
    q_mb1 = np.zeros(2)      # Model-based values for stage 1
    q_mf2 = np.zeros((2, 2)) # Model-free values for stage 2 (2 states, 2 actions)
    
    # Track the previous choice for stickiness
    prev_choice_1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue

        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        # Calculate Model-Based values
        max_q_mf2 = np.max(q_mf2, axis=1)
        q_mb1 = transition_matrix @ max_q_mf2
        
        # Combined value
        q_net = w * q_mb1 + (1 - w) * q_mf1
        
        # Add stickiness bonus to the previously chosen action
        stickiness_bonus = np.zeros(2)
        if prev_choice_1 != -1:
            stickiness_bonus[prev_choice_1] = stickiness
            
        exp_q1 = np.exp(beta * (q_net + stickiness_bonus))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_mf2[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updating ---
        # Stage 2 update (TD)
        delta_2 = r - q_mf2[s2, a2]
        q_mf2[s2, a2] += learning_rate * delta_2

        # Stage 1 update (TD-lambda with lambda=1 for simplicity, connecting outcome to stage 1)
        # Standard SARSA/Q-learning update for stage 1
        delta_1 = q_mf2[s2, a2] - q_mf1[a1]
        q_mf1[a1] += learning_rate * delta_1
        
        # Eligibility trace update (outcomes reinforce stage 1 choice)
        q_mf1[a1] += learning_rate * delta_2
        
        prev_choice_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Learning Rates (Positive vs Negative)
This model posits that OCI affects how participants learn from errors versus rewards. High OCI might be associated with a fear of negative outcomes or perfectionism, potentially leading to different learning rates for positive prediction errors (gains) versus negative prediction errors (omissions).

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Asymmetric Learning Rates Modulated by OCI.
    
    Hypothesis: OCI affects sensitivity to feedback. High OCI participants may
    learn differently from positive rewards (alpha_pos) vs lack of rewards (alpha_neg).
    Here, the negative learning rate is boosted by OCI score, reflecting hyper-awareness of errors.
    
    alpha_neg_effective = alpha_neg_base + (alpha_boost * oci)
    
    Bounds:
    alpha_pos: [0, 1]
    alpha_neg_base: [0, 1]
    alpha_boost: [0, 1] (How much OCI increases learning from negative outcomes)
    beta: [0, 10]
    w: [0, 1]
    """
    alpha_pos, alpha_neg_base, alpha_boost, beta, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective negative learning rate
    alpha_neg = alpha_neg_base + (alpha_boost * oci_score)
    # Clip to valid range [0, 1]
    if alpha_neg > 1.0: alpha_neg = 1.0

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_mf1 = np.zeros(2)
    q_mb1 = np.zeros(2)
    q_mf2 = np.zeros((2, 2))

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue

        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_mf2 = np.max(q_mf2, axis=1)
        q_mb1 = transition_matrix @ max_q_mf2
        
        q_net = w * q_mb1 + (1 - w) * q_mf1
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_mf2[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updating with Asymmetric Learning Rates ---
        
        # Determine which learning rate to use based on prediction error sign
        # We calculate PE first
        pe_2 = r - q_mf2[s2, a2]
        lr_2 = alpha_pos if pe_2 >= 0 else alpha_neg
        
        q_mf2[s2, a2] += lr_2 * pe_2

        # Stage 1 MF update
        pe_1 = q_mf2[s2, a2] - q_mf1[a1]
        lr_1 = alpha_pos if pe_1 >= 0 else alpha_neg
        q_mf1[a1] += lr_1 * pe_1
        
        # Direct reinforcement of stage 1 from stage 2 outcome
        # Using the same PE as stage 2
        q_mf1[a1] += lr_2 * pe_2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Driven Decay of Unchosen Options
This model explores the idea that high OCI is associated with memory biases or anxiety about unchosen paths. Specifically, this model implements a decay mechanism where the value of unchosen actions decays back to 0 (forgetting). The rate of this forgetting is modulated by the OCI score, hypothesizing that high OCI individuals might have "stickier" memories or conversely, might worry more about current failures and forget alternatives faster.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Based/Model-Free Hybrid with OCI-Modulated Forgetting.
    
    Hypothesis: The values of unchosen actions decay over time. 
    The rate of decay (forgetting) is modulated by OCI.
    decay_rate = decay_base + (decay_oci * oci)
    
    If decay_oci is positive, high OCI implies faster forgetting of unchosen options,
    potentially leading to more rigid behavior based on recent reinforced choices.
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    w: [0, 1]
    decay_base: [0, 1]
    decay_oci: [-1, 1] (Can increase or decrease decay)
    """
    learning_rate, beta, w, decay_base, decay_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    decay = decay_base + (decay_oci * oci_score)
    # Clip decay to [0, 1]
    decay = max(0.0, min(1.0, decay))

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_mf1 = np.zeros(2)
    q_mb1 = np.zeros(2)
    q_mf2 = np.zeros((2, 2))

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue

        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_mf2 = np.max(q_mf2, axis=1)
        q_mb1 = transition_matrix @ max_q_mf2
        
        q_net = w * q_mb1 + (1 - w) * q_mf1
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_mf2[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updating ---
        # Standard updates for chosen options
        delta_2 = r - q_mf2[s2, a2]
        q_mf2[s2, a2] += learning_rate * delta_2

        delta_1 = q_mf2[s2, a2] - q_mf1[a1]
        q_mf1[a1] += learning_rate * delta_1
        q_mf1[a1] += learning_rate * delta_2

        # --- Decay for Unchosen Options ---
        # Stage 1 unchosen
        unchosen_a1 = 1 - a1
        q_mf1[unchosen_a1] *= (1 - decay)
        
        # Stage 2 unchosen (in the visited state)
        unchosen_a2 = 1 - a2
        q_mf2[s2, unchosen_a2] *= (1 - decay)
        
        # Note: We do not decay the unvisited state in stage 2 in this simple model,
        # though one could argue for that as well.

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```