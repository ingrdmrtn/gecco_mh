Here are three new cognitive models for the two-step task, incorporating the OCI-R score to explain individual variability in decision-making strategies.

### Cognitive Model 1: OCI-Modulated Model-Based/Model-Free Trade-off
This model hypothesizes that high OCI scores are associated with a deficit in goal-directed (model-based) control, leading to a heavier reliance on habitual (model-free) strategies. The mixing weight `w` between model-based and model-free values is directly modulated by the OCI score.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated MB/MF Trade-off.
    Hypothesis: Higher OCI scores reduce the weight of model-based planning (w),
    biasing the agent towards model-free (habitual) control.
    
    w_raw is the baseline mixing weight. The effective w is reduced by the OCI score.
    effective_w = w_raw * (1 - oci_sensitivity * oci_score)
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    w_raw: [0, 1] (Baseline Model-Based weight)
    oci_sensitivity: [0, 1] (How much OCI dampens MB control)
    """
    learning_rate, beta, w_raw, oci_sensitivity = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective mixing weight based on OCI
    # High OCI reduces w, making behavior more Model-Free
    w = w_raw * (1.0 - oci_sensitivity * oci_score)
    # Clip w to ensure it stays valid
    w = np.clip(w, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        if a1 == -1: a1 = 0
        s_idx = int(state[trial])
        if s_idx == -1: s_idx = 0
        a2 = int(action_2[trial])
        if a2 == -1: a2 = 0
        r = reward[trial]
        if r == -1: r = 0

        # Stage 1 Policy: Hybrid MB/MF
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2 Policy: Pure MF
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Updates
        # Stage 2 update (TD(0))
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 update (TD(1) logic simplified for this structure usually involves eligibility traces, 
        # but here we follow the standard hybrid structure where MF Q1 is updated by Stage 2 Q-values)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2: OCI-Driven Learning Rate Asymmetry
This model suggests that high OCI scores are linked to altered sensitivity to feedback, specifically an over-sensitivity to punishment (or lack of reward) versus reward. This reflects a "fear of failure" or perfectionism aspect of OCI, where negative outcomes drive learning more strongly than positive ones.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Driven Learning Rate Asymmetry.
    Hypothesis: OCI affects the balance between learning from positive vs negative prediction errors.
    High OCI individuals might be more sensitive to negative outcomes (punishment sensitivity).
    
    alpha_pos = alpha_base
    alpha_neg = alpha_base * (1 + oci_scale * oci_score)
    
    Bounds:
    alpha_base: [0, 1]
    beta: [0, 10]
    w: [0, 1]
    oci_scale: [0, 5] (Scales the asymmetry based on OCI)
    """
    alpha_base, beta, w, oci_scale = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Define asymmetric learning rates
    alpha_pos = alpha_base
    # If OCI is high, alpha_neg becomes larger than alpha_pos
    alpha_neg = alpha_base * (1.0 + oci_scale * oci_score)
    alpha_neg = np.clip(alpha_neg, 0.0, 1.0) # Ensure bound

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        if a1 == -1: a1 = 0
        s_idx = int(state[trial])
        if s_idx == -1: s_idx = 0
        a2 = int(action_2[trial])
        if a2 == -1: a2 = 0
        r = reward[trial]
        if r == -1: r = 0

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Stage 2 Update with asymmetry
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        lr_2 = alpha_pos if delta_stage2 > 0 else alpha_neg
        q_stage2_mf[s_idx, a2] += lr_2 * delta_stage2
        
        # Stage 1 Update with asymmetry
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        lr_1 = alpha_pos if delta_stage1 > 0 else alpha_neg
        q_stage1_mf[a1] += lr_1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3: OCI-Dependent Inverse Temperature (Exploration/Exploitation)
This model posits that OCI scores influence the randomness of choice (exploration vs. exploitation). High OCI might lead to rigid, deterministic behavior (high beta), reducing exploration even when uncertainty is high.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Dependent Inverse Temperature.
    Hypothesis: OCI modulates the exploration-exploitation balance.
    High OCI leads to more deterministic (rigid) choices, modeled by increasing beta.
    
    effective_beta = beta_base * (1 + oci_stiffness * oci_score)
    
    Bounds:
    learning_rate: [0, 1]
    beta_base: [0, 10]
    w: [0, 1]
    oci_stiffness: [0, 5] (How much OCI increases rigidity)
    """
    learning_rate, beta_base, w, oci_stiffness = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective beta
    beta = beta_base * (1.0 + oci_stiffness * oci_score)
    # Cap beta to prevent numerical overflows
    beta = np.clip(beta, 0.0, 20.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        if a1 == -1: a1 = 0
        s_idx = int(state[trial])
        if s_idx == -1: s_idx = 0
        a2 = int(action_2[trial])
        if a2 == -1: a2 = 0
        r = reward[trial]
        if r == -1: r = 0

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Updates
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```