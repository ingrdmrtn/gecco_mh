Here are three cognitive models that incorporate the OCI score into the decision-making process for the two-step task.

### Model 1: Hybrid MB/MF with OCI-Scaled Stickiness
This model posits that the participant uses a hybrid Model-Based (MB) and Model-Free (MF) strategy, but their tendency to perseverate (stickiness) is directly driven by their OCI score. Higher OCI leads to stronger repetition of the previous choice, regardless of the reward outcome. This tests if the high OCI score explains the participant's "runs" of identical choices better than a standard stickiness parameter.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model where OCI scales Choice Stickiness.
    
    This model combines Model-Based and Model-Free reinforcement learning.
    The influence of the previous choice (stickiness) is proportional to the OCI score.
    High OCI leads to higher stickiness (perseveration).
    
    Parameters:
    - learning_rate: [0,1] Update rate for MF values.
    - beta_1: [0,10] Inverse temperature for Stage 1 choices.
    - beta_2: [0,10] Inverse temperature for Stage 2 choices.
    - lam: [0,1] Eligibility trace parameter (weight of Stage 2 reward on Stage 1 choice).
    - w: [0,1] Mixing weight for Model-Based (1) vs Model-Free (0).
    - stick_factor: [0,10] Scaling factor for stickiness. Stickiness = stick_factor * OCI.
    """
    learning_rate, beta_1, beta_2, lam, w, stick_factor = model_parameters
    oci_score = oci[0]
    n_trials = len(action_1)
    
    # Stickiness is modulated by OCI
    stickiness = stick_factor * oci_score
    
    # Initialize Q-values
    # Stage 1: 2 actions
    q_stage1_mf = np.zeros(2)
    # Stage 2: 2 states (planets) x 2 actions (aliens)
    q_stage2_mf = np.zeros((2, 2))
    
    # Fixed transition matrix for MB: A(0)->X(0) 0.7, U(1)->Y(1) 0.7
    trans_probs = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    prev_action_1 = -1
    log_loss = 0.0
    eps = 1e-10
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Skip missing trials
        if a1 < 0 or s_idx < 0 or a2 < 0:
            continue
            
        # --- Stage 1 Choice ---
        # Model-Based Values
        # Max Q value for each state in stage 2
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = trans_probs @ max_q_stage2
        
        # Net Q-values
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness
        if prev_action_1 != -1:
            q_net_1[prev_action_1] += stickiness
            
        # Softmax
        exp_q1 = np.exp(beta_1 * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + eps)
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta_2 * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[a2] + eps)
        
        # --- Updates ---
        # MF Stage 1 RPE
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # MF Stage 2 RPE
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Eligibility trace update for Stage 1
        q_stage1_mf[a1] += learning_rate * lam * delta_stage2
        
        prev_action_1 = a1
        
    return log_loss
```

### Model 2: Hybrid MB/MF with OCI-Determined Weight
This model hypothesizes that the balance between Goal-Directed (MB) and Habitual (MF) control is determined by the OCI score. Specifically, higher OCI scores are assumed to reduce the weight `w` of the Model-Based component, making the agent more habitual. This tests if symptom severity directly predicts the cognitive strategy mix.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model where OCI modulates the MB/MF Balance (w).
    
    The mixing weight 'w' is determined by the OCI score.
    Higher OCI scores reduce 'w', making the agent more Model-Free (habitual).
    w = w_factor * (1 - OCI).
    
    Parameters:
    - learning_rate: [0,1] Update rate.
    - beta_1: [0,10] Inverse temperature Stage 1.
    - beta_2: [0,10] Inverse temperature Stage 2.
    - lam: [0,1] Eligibility trace decay.
    - w_factor: [0,1] Base mixing weight scaling. Effective w = w_factor * (1 - OCI).
    - stickiness: [0,10] Constant choice stickiness bonus.
    """
    learning_rate, beta_1, beta_2, lam, w_factor, stickiness = model_parameters
    oci_score = oci[0]
    n_trials = len(action_1)
    
    # Calculate effective w based on OCI
    # If OCI is high, w becomes small (MF dominant)
    w = w_factor * (1.0 - oci_score)
    # Ensure w is within [0,1] just in case OCI > 1 (though likely [0,1])
    w = np.clip(w, 0.0, 1.0)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    trans_probs = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    prev_action_1 = -1
    log_loss = 0.0
    eps = 1e-10
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        if a1 < 0 or s_idx < 0 or a2 < 0:
            continue
            
        # Stage 1
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = trans_probs @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if prev_action_1 != -1:
            q_net_1[prev_action_1] += stickiness
            
        exp_q1 = np.exp(beta_1 * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + eps)
        
        # Stage 2
        exp_q2 = np.exp(beta_2 * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[a2] + eps)
        
        # Updates
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        q_stage1_mf[a1] += learning_rate * lam * delta_stage2
        
        prev_action_1 = a1
        
    return log_loss
```

### Model 3: Pure MF with OCI-Driven Rigidity
This model assumes a Pure Model-Free strategy (given the participant's likely deficit in MB control) but links the OCI score to "rigidity" or deterministic behavior. Specifically, the OCI score scales the inverse temperature (`beta_1`), making the policy sharper. A high OCI leads to a high beta, meaning the participant is more likely to exploit the current best option (or stuck option) rather than explore, explaining the long runs of identical choices.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Pure Model-Free model where OCI modulates Inverse Temperature (Rigidity).
    
    High OCI increases the inverse temperature (beta_1), leading to more 
    deterministic/rigid choices in the first stage.
    beta_1 = beta_base * (1 + OCI).
    
    Parameters:
    - learning_rate: [0,1] Update rate.
    - beta_base: [0,10] Base inverse temperature for Stage 1.
    - beta_2: [0,10] Inverse temperature Stage 2.
    - lam: [0,1] Eligibility trace decay.
    - stickiness: [0,10] Choice stickiness bonus.
    """
    learning_rate, beta_base, beta_2, lam, stickiness = model_parameters
    oci_score = oci[0]
    n_trials = len(action_1)
    
    # Beta 1 is modulated by OCI
    # If OCI is high, beta_1 is higher (more rigid)
    beta_1 = beta_base * (1.0 + oci_score)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1
    log_loss = 0.0
    eps = 1e-10
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        if a1 < 0 or s_idx < 0 or a2 < 0:
            continue
            
        # Stage 1 (Pure MF)
        q_net_1 = q_stage1_mf.copy()
        
        if prev_action_1 != -1:
            q_net_1[prev_action_1] += stickiness
            
        exp_q1 = np.exp(beta_1 * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + eps)
        
        # Stage 2
        exp_q2 = np.exp(beta_2 * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[a2] + eps)
        
        # Updates
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        q_stage1_mf[a1] += learning_rate * lam * delta_stage2
        
        prev_action_1 = a1
        
    return log_loss
```