Here are 3 new cognitive models based on the participant's data and OCI score.

### Cognitive Model 1: Hybrid Learner with OCI-modulated Mixing Weight
This model posits that the participant uses a mixture of Model-Based (planning using the transition structure) and Model-Free (learning from direct reward) strategies. The key hypothesis here is that high OCI scores might correlate with a rigid reliance on habitual (model-free) control over goal-directed (model-based) planning. The mixing weight `w` determines the balance, where `w=1` is fully model-based and `w=0` is fully model-free. We model `w` as being modulated by the OCI score, potentially reducing the contribution of the model-based system as OCI increases.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based / Model-Free learner where the mixing weight is modulated by OCI.
    High OCI might shift the balance towards Model-Free (habitual) or Model-Based (rigid planning).
    
    Parameters:
    learning_rate: [0,1] - Update rate for Q-values.
    beta: [0,10] - Inverse temperature for softmax.
    w_base: [0,1] - Baseline mixing weight (0=MF, 1=MB).
    oci_shift: [0,1] - Magnitude of OCI's influence on the mixing weight.
    """
    learning_rate, beta, w_base, oci_shift = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective mixing weight w
    # We hypothesize OCI might reduce model-based control (shift towards 0)
    # w is clamped between 0 and 1
    w = w_base - (oci_shift * oci_score)
    if w < 0: w = 0
    if w > 1: w = 1

    # Fixed transition matrix for MB planning: A->X (0->0) is 0.7, etc.
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State (Planet) x Action (Alien)
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Choice ---
        # Model-Based Value: Q_MB(s1, a1) = Sum(P(s2|s1,a1) * max_a2 Q_MF(s2, a2))
        max_q_stage2 = np.max(q_stage2_mf, axis=1) # Max value for each planet
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_hybrid = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax
        exp_q1 = np.exp(beta * q_hybrid)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Choice ---
        # Standard Model-Free Q-learning at stage 2
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        # Stage 2 update (TD error)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 update (TD error using Stage 2 value)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2: Model-Free with OCI-Driven Asymmetric Learning Rates
This model explores the idea that high OCI scores might relate to altered sensitivity to feedback, specifically an asymmetry between learning from positive versus negative outcomes (rewards vs. omissions). Individuals with compulsive traits might be hyper-sensitive to errors or lack of reward (negative prediction errors) or conversely, very driven by success. Here, we split the learning rate into `alpha_pos` and `alpha_neg`, and we let the OCI score scale the `alpha_neg` parameter, hypothesizing that high OCI leads to "over-learning" from failures (getting 0 coins).

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner with asymmetric learning rates for positive/negative prediction errors.
    The learning rate for negative errors (alpha_neg) is scaled by OCI.
    
    Parameters:
    alpha_pos: [0,1] - Learning rate for positive prediction errors (R > Q).
    alpha_neg_base: [0,1] - Base learning rate for negative prediction errors (R < Q).
    beta: [0,10] - Inverse temperature.
    oci_sens: [0,5] - Scaling factor for OCI on negative learning rate.
    """
    alpha_pos, alpha_neg_base, beta, oci_sens = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective negative learning rate
    # High OCI increases sensitivity to negative outcomes
    alpha_neg = alpha_neg_base * (1.0 + oci_sens * oci_score)
    if alpha_neg > 1.0: alpha_neg = 1.0

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2)) # State x Action
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        # Stage 2 Update
        pe2 = r - q_stage2[s_idx, a2]
        lr2 = alpha_pos if pe2 >= 0 else alpha_neg
        q_stage2[s_idx, a2] += lr2 * pe2
        
        # Stage 1 Update
        # Using SARSA-like update: driving stage 1 by stage 2 value
        pe1 = q_stage2[s_idx, a2] - q_stage1[a1]
        lr1 = alpha_pos if pe1 >= 0 else alpha_neg
        q_stage1[a1] += lr1 * pe1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3: Model-Based with OCI-Modulated Eligibility Trace
This model introduces an eligibility trace parameter `lambda`. In standard Q-learning (lambda=0), the first stage is updated based on the value of the second stage state. In TD(1) or Monte Carlo (lambda=1), the first stage is updated based directly on the final reward. An intermediate `lambda` blends these. This model hypothesizes that OCI affects the "credit assignment" process. High OCI might lead to a more direct association between the initial choice and the final outcome (ignoring the intermediate structure), effectively increasing `lambda`.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner with eligibility traces (TD(lambda)).
    The decay parameter lambda is modulated by OCI.
    
    Parameters:
    learning_rate: [0,1] - Update rate.
    beta: [0,10] - Inverse temperature.
    lambda_base: [0,1] - Base eligibility trace decay parameter.
    oci_lambda_boost: [0,1] - How much OCI increases the eligibility trace.
    """
    learning_rate, beta, lambda_base, oci_lambda_boost = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective lambda
    # High OCI might increase 'obsessive' linking of start to end, ignoring transition structure
    lam = lambda_base + (oci_lambda_boost * oci_score)
    if lam > 1.0: lam = 1.0

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning with Eligibility Traces ---
        # 1. Prediction error at stage 2 (Reward - Q_stage2)
        delta2 = r - q_stage2[s_idx, a2]
        
        # 2. Prediction error at stage 1 (Q_stage2 - Q_stage1)
        delta1 = q_stage2[s_idx, a2] - q_stage1[a1]
        
        # Update Stage 2
        q_stage2[s_idx, a2] += learning_rate * delta2
        
        # Update Stage 1
        # In TD(lambda), the update for the first state accumulates the immediate error (delta1)
        # plus a discounted portion of the subsequent error (delta2).
        # Update = alpha * (delta1 + lambda * delta2)
        q_stage1[a1] += learning_rate * (delta1 + lam * delta2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```