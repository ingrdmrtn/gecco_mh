Here are three new cognitive models that explore different mechanisms for how Obsessive-Compulsive Inventory (OCI) scores might influence learning and decision-making in this two-step task.

### Model 1: Hybrid RL with OCI-modulated Model-Based Weight
This model hypothesizes that high OCI scores correlate with a rigid reliance on habitual (model-free) control, reducing the influence of goal-directed (model-based) planning. The parameter `w` (mixing weight) determines the balance between model-based and model-free values. Here, `w` is derived from a base level and modulated negatively by the OCI score, suggesting that higher compulsion leads to less flexible, model-based reasoning.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL with OCI-modulated Model-Based Weight.

    This model blends Model-Based (MB) and Model-Free (MF) learning.
    The mixing weight 'w' is modulated by OCI:
    w_effective = w_base - (oci * oci_w_penalty)
    Higher OCI reduces 'w', shifting control toward the Model-Free system.

    Parameters:
    - learning_rate: [0, 1] Learning rate for value updates.
    - beta: [0, 10] Inverse temperature for softmax.
    - w_base: [0, 1] Base weight for Model-Based control (before OCI modulation).
    - oci_w_penalty: [0, 1] How much OCI reduces the Model-Based weight.
    """
    learning_rate, beta, w_base, oci_w_penalty = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]

    # Calculate effective mixing weight w, clamped between 0 and 1
    w_effective = w_base - (participant_oci * oci_w_penalty)
    w_effective = np.clip(w_effective, 0.0, 1.0)

    # Transition matrix (fixed for this task structure)
    # Row 0: Spaceship A (0) -> 0.7 to Planet 0, 0.3 to Planet 1
    # Row 1: Spaceship U (1) -> 0.3 to Planet 0, 0.7 to Planet 1
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # 2 planets, 2 aliens

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        # 1. Calculate Model-Based values
        # Max value of each planet
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2

        # 2. Combine MB and MF values
        q_net = (w_effective * q_stage1_mb) + ((1 - w_effective) * q_stage1_mf)

        # 3. Softmax
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        # Update Stage 2 MF values (TD error)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

        # Update Stage 1 MF values (TD error using Stage 2 value as target)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Asymmetric Learning Rate Modulated by OCI
This model proposes that high OCI scores are associated with a hypersensitivity to negative outcomes (or lack of reward). Instead of a single learning rate, the model uses separate rates for positive and negative prediction errors. The OCI score specifically amplifies the learning rate for negative prediction errors (`alpha_neg`), reflecting a tendency to over-adjust behavior following failure or omitted reward.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free RL with OCI-modulated Asymmetric Learning Rates.
    
    This model posits that OCI increases sensitivity to negative outcomes.
    There is a base learning rate for positive prediction errors (alpha_pos).
    The learning rate for negative prediction errors is:
    alpha_neg = alpha_base + (oci * oci_neg_sens)
    
    Parameters:
    - alpha_pos: [0, 1] Learning rate for positive prediction errors (reward > expectation).
    - beta: [0, 10] Inverse temperature.
    - alpha_base_neg: [0, 1] Base learning rate for negative prediction errors.
    - oci_neg_sens: [0, 1] Sensitivity of negative learning rate to OCI.
    """
    alpha_pos, beta, alpha_base_neg, oci_neg_sens = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]

    # Calculate negative learning rate based on OCI
    alpha_neg_effective = alpha_base_neg + (participant_oci * oci_neg_sens)
    alpha_neg_effective = np.clip(alpha_neg_effective, 0.0, 1.0)

    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Stage 1 Choice
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2 Choice
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Update Stage 2
        delta_stage2 = r - q_stage2[s_idx, a2]
        if delta_stage2 >= 0:
            q_stage2[s_idx, a2] += alpha_pos * delta_stage2
        else:
            q_stage2[s_idx, a2] += alpha_neg_effective * delta_stage2

        # Update Stage 1
        delta_stage1 = q_stage2[s_idx, a2] - q_stage1[a1]
        if delta_stage1 >= 0:
            q_stage1[a1] += alpha_pos * delta_stage1
        else:
            q_stage1[a1] += alpha_neg_effective * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Inverse Temperature Modulation (Exploration/Exploitation)
This model investigates the relationship between OCI and decision noise (exploration vs. exploitation). It hypothesizes that individuals with higher OCI scores may exhibit more deterministic (less noisy) behavior, sticking rigidly to the option they currently perceive as best. The inverse temperature parameter `beta` is modulated by OCI: a higher OCI leads to a higher `beta`, resulting in a steeper softmax curve and less exploration.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free RL with OCI-modulated Inverse Temperature (Beta).

    This model tests if OCI affects the exploration-exploitation balance.
    High OCI is hypothesized to increase Beta (reduce exploration/noise).
    Beta_effective = beta_base + (oci * oci_beta_boost)

    Parameters:
    - learning_rate: [0, 1] Value update speed.
    - beta_base: [0, 10] Baseline inverse temperature.
    - oci_beta_boost: [0, 10] Increase in beta per unit of OCI.
    - lambda_decay: [0, 1] Eligibility trace decay for temporal consistency.
    """
    learning_rate, beta_base, oci_beta_boost, lambda_decay = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]

    # Calculate effective beta
    beta_effective = beta_base + (participant_oci * oci_beta_boost)
    # No strict upper clip needed for beta, but keep positive
    beta_effective = np.maximum(beta_effective, 0.0)

    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Stage 1 Choice
        exp_q1 = np.exp(beta_effective * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2 Choice
        exp_q2 = np.exp(beta_effective * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Update Stage 2
        delta_stage2 = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += learning_rate * delta_stage2

        # Update Stage 1 using eligibility trace (lambda) logic
        # Standard TD(0) updates stage 1 based on stage 2 value, 
        # but TD(lambda) connects reward directly to stage 1.
        # Simplified implementation: Update Q1 with a mix of Stage 2 value and Reward
        
        # Standard TD error for Stage 1
        delta_stage1_td = q_stage2[s_idx, a2] - q_stage1[a1]
        
        # We also have the direct reward prediction error from the end of trial
        # effectively passing the stage 2 RPE back to stage 1 scaled by lambda
        q_stage1[a1] += learning_rate * delta_stage1_td + (learning_rate * lambda_decay * delta_stage2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```