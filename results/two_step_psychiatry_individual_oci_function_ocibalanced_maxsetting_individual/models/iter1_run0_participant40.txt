Here are three new cognitive models for the two-step task, incorporating the OCI score into the learning and decision-making mechanisms in distinct ways.

### Model 1: OCI-Modulated Learning Rigidity
This model tests the hypothesis that high obsessive-compulsive symptoms are associated with cognitive rigidity, modeled as a reduction in the learning rate. The effective learning rate is scaled down by the OCI score, making high-OCI participants slower to update their value estimates in response to new information.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model where the learning rate is negatively modulated by OCI.
    High OCI scores lead to a lower effective learning rate (cognitive rigidity).

    Parameters:
    - lr_base: [0, 1] Baseline learning rate.
    - lr_oci_damp: [0, 1] Factor by which OCI reduces the learning rate.
    - beta: [0, 10] Inverse temperature for softmax.
    - w: [0, 1] Mixing weight (0 = pure MF, 1 = pure MB).
    - stickiness: [0, 5] Choice stickiness (perseveration).
    - lambda_eligibility: [0, 1] Eligibility trace decay.
    """
    lr_base, lr_oci_damp, beta, w, stickiness, lambda_eligibility = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    # Effective learning rate decreases as OCI increases
    learning_rate = lr_base * (1.0 - lr_oci_damp * current_oci)
    learning_rate = np.clip(learning_rate, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_a1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net
        if prev_a1 != -1:
            logits_1[prev_a1] += stickiness
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        state_idx = int(state[trial])

        # --- Stage 2 Choice ---
        logits_2 = beta * q_stage2_mf[state_idx, :]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        r = reward[trial]

        # --- Updating ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Eligibility trace update for Stage 1 based on Stage 2 outcome
        q_stage1_mf[a1] += learning_rate * lambda_eligibility * delta_stage2
        
        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Reward Asymmetry
This model hypothesizes that OCI symptoms affect how participants learn from negative outcomes (omission of reward) versus positive outcomes. Specifically, it proposes that high OCI leads to reduced learning from failures (reward = 0), potentially explaining perseverative behaviors.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model with asymmetric learning rates modulated by OCI.
    High OCI reduces the learning rate specifically for unrewarded trials (0 coins),
    modeling a reduced sensitivity to negative feedback or failure.

    Parameters:
    - lr: [0, 1] Base learning rate (used for positive rewards).
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight.
    - stickiness: [0, 5] Choice stickiness.
    - lambda_eligibility: [0, 1] Eligibility trace decay.
    - neg_lr_oci_penalty: [0, 1] Reduction in learning rate for 0-reward trials scaled by OCI.
    """
    lr, beta, w, stickiness, lambda_eligibility, neg_lr_oci_penalty = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    # Calculate learning rates for positive (1) and negative (0) rewards
    lr_pos = lr
    lr_neg = lr * (1.0 - neg_lr_oci_penalty * current_oci)
    lr_neg = np.clip(lr_neg, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_a1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net
        if prev_a1 != -1:
            logits_1[prev_a1] += stickiness
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        state_idx = int(state[trial])

        # --- Stage 2 Choice ---
        logits_2 = beta * q_stage2_mf[state_idx, :]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        r = reward[trial]

        # Determine effective learning rate based on reward outcome
        current_lr = lr_pos if r == 1.0 else lr_neg

        # --- Updating ---
        # Stage 1 update (consistency) uses base LR as it's not a reward outcome
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        # Stage 2 update (reward) uses asymmetric LR
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += current_lr * delta_stage2
        
        # Eligibility trace uses the reward-based LR
        q_stage1_mf[a1] += current_lr * lambda_eligibility * delta_stage2
        
        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Distorted Transition Beliefs
This model proposes that high OCI scores correlate with a distrust of the environmental structure (e.g., intolerance of uncertainty). The model-based system's belief about the transition matrix is "flattened" towards randomness (0.5) as OCI increases, representing a degraded internal model of the task structure.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model where OCI distorts the Model-Based transition probabilities.
    High OCI reduces the subjective probability of the common transition,
    modeling a distrust of the task structure or higher perceived uncertainty.

    Parameters:
    - lr: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight.
    - stickiness: [0, 5] Choice stickiness.
    - lambda_eligibility: [0, 1] Eligibility trace decay.
    - mb_distortion_oci: [0, 1] Degree to which OCI flattens transition beliefs towards 0.5.
    """
    lr, beta, w, stickiness, lambda_eligibility, mb_distortion_oci = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    # Distort the transition probability: 
    # If distortion is high and OCI is high, p_common moves from 0.7 towards 0.5.
    # Max distortion (param=1, oci=1) approx: 0.7 - 0.2 = 0.5.
    p_common = 0.7 - (0.2 * mb_distortion_oci * current_oci)
    p_common = np.clip(p_common, 0.5, 0.7)
    
    # Subjective transition matrix used for MB calculations
    transition_matrix = np.array([[p_common, 1-p_common], [1-p_common, p_common]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_a1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        
        # MB value calculation uses the distorted transition matrix
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net
        if prev_a1 != -1:
            logits_1[prev_a1] += stickiness
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        state_idx = int(state[trial])

        # --- Stage 2 Choice ---
        logits_2 = beta * q_stage2_mf[state_idx, :]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        r = reward[trial]

        # --- Updating ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += lr * delta_stage2
        
        q_stage1_mf[a1] += lr * lambda_eligibility * delta_stage2
        
        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```