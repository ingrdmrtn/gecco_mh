Here are 3 new cognitive models in Python, designed to capture different potential mechanisms of decision-making in a high-OCI participant.

### Model 1: Hybrid Learner with OCI-modulated Model-Based Weight
This model tests the hypothesis that high OCI scores correlate with a stronger reliance on a rigid, model-based system (computing transitions) versus a habitual model-free system. It implements a classic hybrid reinforcement learning agent where the weighting parameter `w` (mixing model-based and model-free values) is modulated by the OCI score.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free learner where OCI score modulates the mixing weight 'w'.
    
    Hypothesis: Participants with high OCI might over-rely on the model-based structure 
    (planning) or conversely be stuck in habits. Here we test if OCI scales the 
    model-based weight directly.
    
    Bounds:
    learning_rate: [0,1]
    beta: [0,10]
    w_scale: [0, 1]  # scales how much OCI affects the MB/MF balance
    """
    learning_rate, beta, w_scale = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    reward = reward.astype(int)

    # Fixed transition matrix for Model-Based (70% common, 30% rare)
    # Row: Spaceship (A=0, B=1), Col: Planet (X=0, Y=1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_mf_stage1 = np.zeros(2)
    q_mf_stage2 = np.zeros((2, 2)) # State x Action
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Calculate mixing weight w based on OCI. 
    # w = 0 is pure Model-Free, w = 1 is pure Model-Based.
    # We constrain w to be within [0, 1].
    # Hypothesis: Higher OCI -> Higher w (more rigid/rule-based).
    w = w_scale * oci_score
    w = np.clip(w, 0.0, 1.0)

    for t in range(n_trials):
        
        # --- Stage 1 Decision ---
        
        # Model-Based Value Calculation
        # V_MB(s1) = sum(P(s2|s1) * max(Q_MF(s2, :)))
        max_q_stage2 = np.max(q_mf_stage2, axis=1)
        q_mb_stage1 = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net_stage1 = w * q_mb_stage1 + (1 - w) * q_mf_stage1
        
        # Softmax Choice 1
        logits_1 = beta * q_net_stage1
        logits_1 = logits_1 - np.max(logits_1)
        probs_1 = np.exp(logits_1) / np.sum(np.exp(logits_1))
        p_choice_1[t] = probs_1[action_1[t]]
        
        # --- Stage 2 Decision ---
        
        s_idx = state[t] # The planet we arrived at
        
        # Softmax Choice 2 (Pure Model-Free at this stage)
        logits_2 = beta * q_mf_stage2[s_idx]
        logits_2 = logits_2 - np.max(logits_2)
        probs_2 = np.exp(logits_2) / np.sum(np.exp(logits_2))
        p_choice_2[t] = probs_2[action_2[t]]
        
        # --- Learning ---
        
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]
        
        # Update Stage 2 Q-values (TD(0))
        delta_2 = r - q_mf_stage2[s_idx, a2]
        q_mf_stage2[s_idx, a2] += learning_rate * delta_2
        
        # Update Stage 1 MF Q-values (TD(1) / Direct reinforcement)
        # Using the reward directly for simplicity in this hybrid formulation 
        # (often called SARSA or Q-learning update at stage 1)
        delta_1 = r - q_mf_stage1[a1]
        q_mf_stage1[a1] += learning_rate * delta_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Model-Free Learner with OCI-Modulated Loss Aversion
This model hypothesizes that high OCI is associated with an increased sensitivity to negative outcomes (failure to get gold). Instead of a single learning rate, this model splits learning into positive (`alpha_pos`) and negative (`alpha_neg`) updates. The negative learning rate is scaled by the OCI score, suggesting that high-OCI individuals might over-learn from missing rewards (0 coins).

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner with asymmetric learning rates, where OCI modulates 
    sensitivity to negative prediction errors (loss/no reward).
    
    Hypothesis: High OCI participants are more sensitive to 'failure' (0 reward),
    causing them to adjust values more drastically after non-rewarded trials.
    
    Bounds:
    alpha_pos: [0,1]
    base_alpha_neg: [0,1]
    beta: [0,10]
    oci_sensitivity: [0, 5]
    """
    alpha_pos, base_alpha_neg, beta, oci_sensitivity = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    reward = reward.astype(int)

    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Calculate effective negative learning rate
    # We clip it to [0,1] to ensure stability
    alpha_neg = base_alpha_neg * (1 + oci_sensitivity * oci_score)
    alpha_neg = np.clip(alpha_neg, 0.0, 1.0)

    for t in range(n_trials):
        
        # --- Stage 1 Choice ---
        logits_1 = beta * q_stage1
        logits_1 = logits_1 - np.max(logits_1)
        probs_1 = np.exp(logits_1) / np.sum(np.exp(logits_1))
        p_choice_1[t] = probs_1[action_1[t]]
        
        # --- Stage 2 Choice ---
        s_idx = state[t]
        logits_2 = beta * q_stage2[s_idx]
        logits_2 = logits_2 - np.max(logits_2)
        probs_2 = np.exp(logits_2) / np.sum(np.exp(logits_2))
        p_choice_2[t] = probs_2[action_2[t]]
        
        # --- Learning ---
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]
        
        # Stage 2 Update
        pe_2 = r - q_stage2[s_idx, a2]
        if pe_2 >= 0:
            q_stage2[s_idx, a2] += alpha_pos * pe_2
        else:
            q_stage2[s_idx, a2] += alpha_neg * pe_2
            
        # Stage 1 Update
        # Using simple TD(1)-like update (reward based) for MF
        pe_1 = r - q_stage1[a1]
        if pe_1 >= 0:
            q_stage1[a1] += alpha_pos * pe_1
        else:
            q_stage1[a1] += alpha_neg * pe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Model-Based Learner with OCI-Driven Inverse Temperature (Anxiety-Induced Noise)
This model posits that while the participant uses a sophisticated Model-Based strategy (planning based on transition probabilities), the OCI score negatively impacts their decision consistency. High OCI (often comorbid with anxiety) might introduce "noise" into the decision process, effectively lowering the inverse temperature `beta`.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Pure Model-Based learner where OCI modulates the exploration/exploitation 
    parameter (beta).
    
    Hypothesis: High OCI leads to more erratic or anxious behavior, effectively 
    lowering the beta (inverse temperature), making choices noisier despite 
    having a model of the task.
    
    Bounds:
    learning_rate: [0,1]
    base_beta: [0,10]
    oci_noise_factor: [0, 5]
    """
    learning_rate, base_beta, oci_noise_factor = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    reward = reward.astype(int)
    
    # Transition matrix (70/30)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    # We only track Stage 2 values, Stage 1 is derived
    q_stage2 = np.zeros((2, 2)) 
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Adjust beta based on OCI
    # Higher OCI -> Higher noise -> Lower effective beta
    # effective_beta = base_beta / (1 + factor * oci)
    effective_beta = base_beta / (1.0 + oci_noise_factor * oci_score)
    # Ensure beta doesn't go negative or explode
    effective_beta = np.clip(effective_beta, 0.0, 20.0) 

    for t in range(n_trials):
        
        # --- Stage 1 Decision (Model-Based) ---
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_mb_stage1 = transition_matrix @ max_q_stage2
        
        logits_1 = effective_beta * q_mb_stage1
        logits_1 = logits_1 - np.max(logits_1)
        probs_1 = np.exp(logits_1) / np.sum(np.exp(logits_1))
        p_choice_1[t] = probs_1[action_1[t]]
        
        # --- Stage 2 Decision ---
        s_idx = state[t]
        logits_2 = effective_beta * q_stage2[s_idx]
        logits_2 = logits_2 - np.max(logits_2)
        probs_2 = np.exp(logits_2) / np.sum(np.exp(logits_2))
        p_choice_2[t] = probs_2[action_2[t]]
        
        # --- Learning ---
        a2 = action_2[t]
        r = reward[t]
        
        # Only update Stage 2 values (Stage 1 is computed via the model)
        delta = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += learning_rate * delta

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```