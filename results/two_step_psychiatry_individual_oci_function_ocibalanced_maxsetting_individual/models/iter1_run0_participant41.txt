Here are three cognitive models expressed as Python functions. They utilize the Obsessive-Compulsive Inventory (OCI) score to modulate different aspects of the decision-making process, specifically the balance between Model-Based/Model-Free control, the decision noise (rigidity), and perseverance (stickiness).

### Model 1: Hybrid Model with OCI-Modulated Model-Based Weight
This model assumes that high OCI scores reflect a deficit in goal-directed (Model-Based) control, shifting the participant towards habitual (Model-Free) behavior. The weighting parameter $w$ is reduced as the OCI score increases.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL model where OCI score reduces the weight of Model-Based (MB) planning.
    High OCI leads to lower 'w', resulting in more Model-Free (habitual) behavior.

    Parameters:
    learning_rate: [0,1] - Update rate for Q-values.
    beta: [0,10] - Inverse temperature for softmax.
    w_max: [0,1] - Maximum weight for Model-Based values (at OCI=0).
    w_oci_penalty: [0,1] - Reduction in MB weight proportional to OCI score.
    """
    learning_rate, beta, w_max, w_oci_penalty = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate mixing weight w based on OCI
    # w decreases as OCI increases. Clipped at 0.
    w = max(0.0, w_max * (1.0 - (oci_score * w_oci_penalty)))

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Mix MB and MF values
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        state_idx = s

        # policy for the second choice
        if a2 == -1:
            # Handle missing data
            p_choice_2[trial] = 1.0
            continue
            
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # ACTION VALUE UPDATING
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        
        # Update Stage 2 MF values
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Update Stage 1 MF values (TD(1) update assumption for hybrid models)
        q_stage1_mf[a1] += learning_rate * (delta_stage1 + delta_stage2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model with OCI-Modulated Rigidity (Beta)
This model posits that high OCI scores result in "rigidity" or a lack of exploration. Instead of affecting the learning strategy, OCI increases the inverse temperature ($\beta$), making choices more deterministic and less sensitive to small value differences.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL model where OCI score increases the inverse temperature (beta).
    High OCI leads to higher beta, causing more rigid/deterministic choices (less exploration).

    Parameters:
    learning_rate: [0,1] - Update rate for Q-values.
    beta_base: [0,10] - Baseline inverse temperature.
    w: [0,1] - Fixed weight for Model-Based values.
    beta_oci_slope: [0,10] - Increase in beta proportional to OCI score.
    """
    learning_rate, beta_base, w, beta_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective beta based on OCI
    current_beta = beta_base + (beta_oci_slope * oci_score)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Mix MB and MF values
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(current_beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        state_idx = s

        # policy for the second choice
        if a2 == -1:
            p_choice_2[trial] = 1.0
            continue

        exp_q2 = np.exp(current_beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # ACTION VALUE UPDATING
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        
        # Update Stage 2 MF values
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Update Stage 1 MF values
        q_stage1_mf[a1] += learning_rate * (delta_stage1 + delta_stage2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model with OCI-Driven Stickiness
This model combines the Hybrid (MB/MF) architecture with a specific OCI-driven stickiness mechanism. Unlike the "best model so far" which was purely Model-Free, this model tests if the participant uses *some* model-based planning ($w$), but is overridden by a compulsion to repeat the previous action (stickiness) driven specifically by their OCI score.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL model with OCI-modulated choice stickiness (perseveration).
    Combines Model-Based planning with a compulsion to repeat choices driven by OCI.
    
    Parameters:
    learning_rate: [0,1] - Update rate for Q-values.
    beta: [0,10] - Inverse temperature for softmax.
    w: [0,1] - Weight for Model-Based values.
    stick_oci: [0,5] - Stickiness bonus added to previous choice, scaled by OCI.
    """
    learning_rate, beta, w, stick_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Stickiness is purely a function of OCI in this model variant
    stickiness_val = stick_oci * oci_score

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action = -1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Mix MB and MF values
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Apply OCI-driven stickiness
        if last_action != -1:
            q_net_stage1[last_action] += stickiness_val
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        state_idx = s

        # policy for the second choice
        if a2 == -1:
            p_choice_2[trial] = 1.0
            last_action = a1
            continue

        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # ACTION VALUE UPDATING
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        
        # Update Stage 2 MF values
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Update Stage 1 MF values
        q_stage1_mf[a1] += learning_rate * (delta_stage1 + delta_stage2)
        
        last_action = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```