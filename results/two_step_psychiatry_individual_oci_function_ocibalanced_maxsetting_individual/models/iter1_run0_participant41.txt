Here are three new cognitive models designed to explain the behavior of a participant with high obsessive-compulsive symptoms (OCI score = 1.0) in a two-step decision task.

### Cognitive Model 1: Model-Based/Model-Free Hybrid with OCI-Modulated Mixing
This model hypothesizes that high OCI scores correlate with a rigid reliance on habit (Model-Free) over goal-directed planning (Model-Based). Instead of a fixed mixing weight `w`, the balance between the two systems is modulated by the OCI score. A higher OCI score pushes the agent towards pure Model-Free control.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based / Model-Free learner where the weighting (w) is modulated by OCI.
    
    Hypothesis: High OCI participants exhibit a deficit in goal-directed (Model-Based) control,
    relying more heavily on habitual (Model-Free) systems.
    
    The mixing weight 'w' determines the balance:
    Q_net = w * Q_MB + (1-w) * Q_MF
    
    Here, w is calculated as: w_base * (1 - oci_score)
    Since this participant has OCI=1.0, this effectively tests a pure MF model against
    the possibility that lower OCI would have allowed MB control.
    
    Parameters:
    learning_rate: [0, 1] Learning rate for value updates.
    beta: [0, 10] Inverse temperature for softmax choice.
    w_base: [0, 1] Base weight for Model-Based control (before OCI reduction).
    """
    learning_rate, beta, w_base = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective weight. High OCI reduces MB influence.
    # If OCI is 1.0, w becomes 0 (Pure MF).
    w = w_base * (1.0 - oci_score)
    
    # Transition probabilities (fixed for this task structure)
    # 0 -> 0 (70%), 0 -> 1 (30%) etc.
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_curr = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        # Model-Based Value Calculation: V(s') = max_a Q(s', a)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Q-value
        q_integrated = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax choice 1
        logits_1 = beta * q_integrated
        logits_1 = logits_1 - np.max(logits_1) # stability
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        # Standard MF choice at stage 2
        logits_2 = beta * q_stage2_mf[s_curr]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        # Stage 2 update (TD)
        pe_2 = r - q_stage2_mf[s_curr, a2]
        q_stage2_mf[s_curr, a2] += learning_rate * pe_2
        
        # Stage 1 update (TD)
        # Note: In standard hybrid models, stage 1 MF is updated via TD(0) or TD(1).
        # We use TD(1) logic: Q1 updates towards Q2 outcome.
        pe_1 = q_stage2_mf[s_curr, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * pe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2: Loss Aversion Amplified by OCI
This model hypothesizes that high OCI is associated with an increased sensitivity to negative outcomes (punishment or lack of reward). The participant learns differently from positive prediction errors versus negative prediction errors, with the OCI score amplifying the learning rate specifically for negative events.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner with Asymmetric Learning Rates modulated by OCI.
    
    Hypothesis: High OCI participants are hyper-sensitive to "failure" (0 reward).
    They update their value estimates more drastically when outcomes are worse than expected.
    
    Parameters:
    lr_pos: [0, 1] Learning rate for positive prediction errors (better than expected).
    lr_neg_base: [0, 1] Base learning rate for negative prediction errors.
    beta: [0, 10] Inverse temperature.
    amp_factor: [0, 5] How much OCI amplifies the negative learning rate.
    """
    lr_pos, lr_neg_base, beta, amp_factor = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective negative learning rate
    # High OCI increases the weight of negative experiences.
    # We clip at 1.0 to ensure stability.
    lr_neg = min(1.0, lr_neg_base * (1.0 + amp_factor * oci_score))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_curr = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Choice ---
        logits_1 = beta * q_stage1
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Choice ---
        logits_2 = beta * q_stage2[s_curr]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        # Stage 2 PE
        pe_2 = r - q_stage2[s_curr, a2]
        
        # Apply asymmetric learning rates
        lr_2 = lr_pos if pe_2 >= 0 else lr_neg
        q_stage2[s_curr, a2] += lr_2 * pe_2
        
        # Stage 1 PE
        pe_1 = q_stage2[s_curr, a2] - q_stage1[a1]
        
        # Apply asymmetric learning rates
        lr_1 = lr_pos if pe_1 >= 0 else lr_neg
        q_stage1[a1] += lr_1 * pe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3: Uncertainty-Driven Exploration Suppression (OCI)
This model posits that high OCI participants are averse to uncertainty. Instead of standard softmax exploration, their exploration is dampened by the OCI score. As OCI increases, the `beta` (inverse temperature) effectively increases, making choices more deterministic (exploitative) and less exploratory, reflecting a compulsive need for control and predictability.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner where OCI suppresses exploration (increases exploitation).
    
    Hypothesis: High OCI is linked to intolerance of uncertainty. 
    The participant becomes 'stiffer' in their choices, exploring less than a 
    low-OCI participant.
    
    The effective inverse temperature (beta_eff) is modulated:
    beta_eff = beta_base * (1 + stiff_param * oci)
    
    Parameters:
    learning_rate: [0, 1] Standard learning rate.
    beta_base: [0, 10] Base inverse temperature.
    stiff_param: [0, 5] Stiffness parameter scaling the effect of OCI on beta.
    """
    learning_rate, beta_base, stiff_param = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective beta
    # Higher OCI -> Higher Beta -> More deterministic choices (less exploration)
    beta_eff = beta_base * (1.0 + stiff_param * oci_score)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_curr = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Choice ---
        logits_1 = beta_eff * q_stage1
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Choice ---
        logits_2 = beta_eff * q_stage2[s_curr]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        # Stage 2 PE
        pe_2 = r - q_stage2[s_curr, a2]
        q_stage2[s_curr, a2] += learning_rate * pe_2
        
        # Stage 1 PE
        pe_1 = q_stage2[s_curr, a2] - q_stage1[a1]
        q_stage1[a1] += learning_rate * pe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```