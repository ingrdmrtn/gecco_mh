Here are three new cognitive models based on the participant's data and OCI score.

### Cognitive Model 1: OCI-Modulated Perseveration
This model hypothesizes that the high OCI score drives **choice perseveration** (stickiness). High OCI participants may exhibit compulsive repetition of actions regardless of reward history. The "stickiness" bonus added to the previously chosen action is scaled directly by the OCI score.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Perseveration Model.
    
    This model assumes that OCI score drives 'stickiness' or perseveration 
    in the first-stage choice. High OCI leads to a higher tendency to repeat 
    the previous spaceship choice, independent of the value calculated by 
    Model-Based or Model-Free systems.
    
    Parameters:
    - learning_rate: [0,1] Rate of updating value estimates.
    - beta: [0,10] Inverse temperature for softmax (choice consistency).
    - w: [0,1] Weight for Model-Based control (0=Pure MF, 1=Pure MB).
    - stick_scale: [0,5] Scaling factor for OCI-driven stickiness. 
                   Stickiness bonus = stick_scale * OCI.
    """
    learning_rate, beta, w, stick_scale = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate stickiness magnitude based on OCI
    stickiness_bonus = stick_scale * oci_score

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Calculate logits (beta * Q)
        logits_1 = beta * q_net_stage1
        
        # Add stickiness to the previously chosen action (if not first trial)
        if trial > 0:
            prev_a1 = int(action_1[trial-1])
            logits_1[prev_a1] += stickiness_bonus
            
        # Softmax for Stage 1
        exp_q1 = np.exp(logits_1 - np.max(logits_1)) # Subtract max for stability
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        state_idx = int(state[trial])
        
        # --- Stage 2 Policy ---
        q_s2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * q_s2 - np.max(beta * q_s2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Value Updates ---
        # Update Stage 1 MF value using Stage 2 value (TD error 1)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Update Stage 2 MF value using Reward (TD error 2)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Eligibility trace: Update Stage 1 MF value using Stage 2 RPE
        q_stage1_mf[a1] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2: OCI-Modulated Punishment Sensitivity
This model hypothesizes that OCI is linked to **anxiety and avoidance**, resulting in altered sensitivity to negative outcomes (omission of reward). The learning rate for negative prediction errors (losses) is amplified by the OCI score, making high-OCI participants react more strongly when they fail to get a coin.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Punishment Sensitivity Model.
    
    This model posits that OCI modulates the learning rate specifically for 
    negative prediction errors (losses). High OCI participants are assumed 
    to be more sensitive to 'punishment' (0 coins), updating their values 
    more aggressively after a loss.
    
    Parameters:
    - lr_base: [0,1] Base learning rate for positive prediction errors.
    - beta: [0,10] Inverse temperature.
    - w: [0,1] Weight for Model-Based control.
    - loss_sens: [0,5] Multiplier for negative prediction errors sensitivity.
                 lr_neg = lr_base * (1 + loss_sens * OCI).
    """
    lr_base, beta, w, loss_sens = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate negative learning rate, clipped to 1.0 for stability
    lr_neg = np.clip(lr_base * (1.0 + loss_sens * oci_score), 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1 - np.max(beta * q_net_stage1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        state_idx = int(state[trial])
        
        # --- Stage 2 Policy ---
        q_s2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * q_s2 - np.max(beta * q_s2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Value Updates with Asymmetric Learning Rate ---
        
        # Delta 1: Stage 1 update
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        lr_1 = lr_base if delta_stage1 >= 0 else lr_neg
        q_stage1_mf[a1] += lr_1 * delta_stage1
        
        # Delta 2: Stage 2 update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        lr_2 = lr_base if delta_stage2 >= 0 else lr_neg
        q_stage2_mf[state_idx, a2] += lr_2 * delta_stage2
        
        # Eligibility trace
        # We use the learning rate associated with the outcome (delta_stage2)
        q_stage1_mf[a1] += lr_2 * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3: OCI-Modulated Decision Rigidity
This model hypothesizes that OCI relates to **behavioral rigidity**. Rather than affecting the learning process or weighting, OCI modulates the inverse temperature (`beta`) of the softmax function. High OCI leads to a higher effective beta, resulting in more deterministic and less exploratory choices.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Decision Rigidity Model.
    
    This model posits that OCI affects the inverse temperature (beta) of the softmax function.
    High OCI scores lead to higher beta values, resulting in more deterministic (rigid) behavior 
    and reduced exploration (exploitation of current best options).
    
    Parameters:
    - learning_rate: [0,1] Value update rate.
    - beta_base: [0,10] Base inverse temperature.
    - w: [0,1] Weight for Model-Based control.
    - rigidity_scale: [0,5] Scaling factor for OCI's effect on beta.
                      beta_effective = beta_base * (1 + rigidity_scale * OCI).
    """
    learning_rate, beta_base, w, rigidity_scale = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective beta based on OCI
    beta_eff = beta_base * (1.0 + rigidity_scale * oci_score)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Use beta_eff instead of raw beta
        exp_q1 = np.exp(beta_eff * q_net_stage1 - np.max(beta_eff * q_net_stage1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        state_idx = int(state[trial])
        
        # --- Stage 2 Policy ---
        q_s2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta_eff * q_s2 - np.max(beta_eff * q_s2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Value Updates ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        q_stage1_mf[a1] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```