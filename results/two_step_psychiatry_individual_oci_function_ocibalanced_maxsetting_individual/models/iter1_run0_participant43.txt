Here are three cognitive models formulated as Python functions. These models incorporate the OCI (Obsessive-Compulsive Inventory) score to explain the participant's decision-making process, focusing on mechanisms relevant to compulsivity, such as perseveration, deficits in model-based control, and altered sensitivity to outcomes.

```python
def cognitive_model1_oci_perseveration(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: Hybrid Learner with OCI-Modulated Perseveration.
    
    Hypothesis: OCI score drives the strength of perseveration (stickiness) to the previously chosen spaceship.
    High OCI scores are associated with compulsive repetition. This model posits that the tendency to 
    repeat the previous stage-1 choice, regardless of value, is scaled by the OCI score.
    
    Q_net = w * Q_MB + (1 - w) * Q_MF
    Logits = beta * Q_net + (persev_strength * OCI * I(action == last_action))
    
    Parameters:
    - learning_rate: [0, 1] Update rate for MF values.
    - beta: [0, 10] Inverse temperature (softness of choice).
    - w: [0, 1] Weight of Model-Based control (1 = Pure MB, 0 = Pure MF).
    - persev_strength: [0, 5] Strength of choice stickiness scaled by OCI.
    """
    learning_rate, beta, w, persev_strength = model_parameters
    oci_score = oci[0]
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Ensure inputs are integers for indexing
    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    
    last_action = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based Value: Transition * max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Net Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Calculate Logits with OCI-scaled Perseveration Bonus
        logits_1 = beta * q_net
        if last_action != -1:
            logits_1[last_action] += persev_strength * oci_score
            
        # Softmax for Stage 1
        logits_1 = logits_1 - np.max(logits_1) # Numerical stability
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Update last action
        last_action = action_1[trial]
        
        # --- Stage 2 Policy ---
        state_idx = state[trial]
        logits_2 = beta * q_stage2_mf[state_idx]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        # Stage 1 Update (SARSA-like for MF part)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2_oci_mixing_scaling(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: Variable MB-MF Balance Modulated by OCI.
    
    Hypothesis: High OCI scores reflect a deficit in goal-directed (Model-Based) control, 
    leading to a reliance on habitual (Model-Free) control.
    Instead of a fixed mixing weight w, the effective weight w_eff is the baseline MB capacity (w_max)
    attenuated by the OCI score.
    
    w_eff = w_max * (1 - OCI)
    
    Parameters:
    - learning_rate: [0, 1] Update rate.
    - beta: [0, 10] Inverse temperature.
    - w_max: [0, 1] Maximum Model-Based weight (achieved if OCI=0).
    """
    learning_rate, beta, w_max = model_parameters
    oci_score = oci[0]
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    
    # Calculate effective w based on OCI
    # If OCI is 1, w_eff becomes 0 (Pure MF). If OCI is 0, w_eff is w_max.
    w_eff = w_max * (1.0 - oci_score)
    w_eff = np.clip(w_eff, 0.0, 1.0)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Mix MB and MF using the OCI-attenuated weight
        q_net = w_eff * q_stage1_mb + (1 - w_eff) * q_stage1_mf
        
        logits_1 = beta * q_net
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Policy ---
        state_idx = state[trial]
        logits_2 = beta * q_stage2_mf[state_idx]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3_oci_loss_sensitivity(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: Hybrid Learner with OCI-Modulated Loss Sensitivity.
    
    Hypothesis: High OCI individuals may exhibit perfectionism or anxiety, causing them to 
    perceive the absence of a reward (0 coins) as a significant failure or punishment.
    This model modifies the reward function such that 0 coins is treated as a negative reward,
    scaled by the OCI score.
    
    Effective Reward = Reward (if 1) else -(loss_sens * OCI)
    
    Parameters:
    - learning_rate: [0, 1] Update rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight for MB/MF.
    - loss_sens: [0, 5] Sensitivity to 'loss' (0 coins), scaled by OCI.
    """
    learning_rate, beta, w, loss_sens = model_parameters
    oci_score = oci[0]
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Policy ---
        state_idx = state[trial]
        logits_2 = beta * q_stage2_mf[state_idx]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        # Calculate effective reward based on OCI and loss sensitivity
        r_eff = reward[trial]
        if r_eff == 0.0:
            r_eff = -1.0 * loss_sens * oci_score
            
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Update stage 2 values with effective reward
        delta_stage2 = r_eff - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```