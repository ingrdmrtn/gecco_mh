Here are three new cognitive models exploring different mechanisms for how Obsessive-Compulsive symptoms (OCI) might influence decision-making in the two-step task.

### Model 1: OCI-Modulated Model-Based vs. Model-Free Weighting
This model hypothesizes that high OCI scores reflect a reliance on habitual (Model-Free) control over goal-directed (Model-Based) planning. Individuals with high compulsivity often show deficits in goal-directed control. Here, the mixing weight `w` (which balances Model-Based and Model-Free values) is directly modulated by the OCI score. Higher OCI reduces `w`, pushing the agent towards Model-Free behavior.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid learner where the balance between Model-Based (MB) and Model-Free (MF) 
    control is modulated by OCI.
    
    Hypothesis: High OCI leads to reduced Model-Based control (lower w).
    
    Parameters:
    lr: [0, 1] Learning rate for value updates.
    beta: [0, 10] Inverse temperature for softmax choice.
    w_base: [0, 1] Baseline mixing weight (proportion of MB) for a hypothetical OCI=0.
    oci_w_penalty: [0, 1] Reduction in MB weight proportional to OCI score.
    
    The effective w is calculated as: w = clip(w_base - (oci_w_penalty * oci), 0, 1)
    """
    lr, beta, w_base, oci_w_penalty = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]

    # Calculate effective mixing weight based on OCI
    w = w_base - (oci_w_penalty * participant_oci)
    # Ensure w stays within [0, 1] bounds
    if w < 0: w = 0.0
    if w > 1: w = 1.0

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based Value Calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Value (Hybrid)
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax Policy Stage 1
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Choice ---
        s_curr = int(state[trial])
        
        # Softmax Policy Stage 2
        exp_q2 = np.exp(beta * q_stage2_mf[s_curr])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        r = reward[trial]

        # Stage 2 Update (TD)
        delta_stage2 = r - q_stage2_mf[s_curr, a2]
        q_stage2_mf[s_curr, a2] += lr * delta_stage2

        # Stage 1 Update (TD-lambda / eligibility trace logic for MF)
        # Note: In standard 2-step models, MF Q1 is updated by Stage 2 prediction error 
        # plus the transition prediction error (delta_stage1).
        
        delta_stage1 = q_stage2_mf[s_curr, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1 # Update based on transition
        q_stage1_mf[a1] += lr * delta_stage2 # Update based on final outcome (eligibility trace)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Punishment Sensitivity
This model investigates whether high OCI is associated with an increased sensitivity to negative outcomes (punishment). Anxious or compulsive individuals may over-weight failures (0 reward) compared to successes. Here, the learning rate is split into positive (`lr_pos`) and negative (`lr_neg`) components, where the negative learning rate is amplified by the OCI score.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner with separate learning rates for positive and negative outcomes.
    The learning rate for negative outcomes (zero reward) is modulated by OCI.
    
    Hypothesis: High OCI leads to hypersensitivity to punishment/failure (0 reward),
    resulting in faster updating (unlearning) after non-rewarded trials.
    
    Parameters:
    lr_pos: [0, 1] Learning rate for rewarded trials (reward = 1).
    lr_neg_base: [0, 1] Base learning rate for unrewarded trials (reward = 0).
    oci_neg_scale: [0, 5] Multiplier for OCI to increase lr_neg.
    beta: [0, 10] Inverse temperature.
    
    effective_lr_neg = clip(lr_neg_base * (1 + oci_neg_scale * oci), 0, 1)
    """
    lr_pos, lr_neg_base, oci_neg_scale, beta = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]

    # Calculate effective negative learning rate
    lr_neg = lr_neg_base * (1 + oci_neg_scale * participant_oci)
    if lr_neg > 1.0: lr_neg = 1.0
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2)) # State x Action
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Pure Model-Free choice at Stage 1 for this specific hypothesis
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Choice ---
        s_curr = int(state[trial])
        
        exp_q2 = np.exp(beta * q_stage2[s_curr])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        r = reward[trial]
        
        # Select learning rate based on outcome
        current_lr = lr_pos if r > 0 else lr_neg

        # Stage 2 Update
        delta_stage2 = r - q_stage2[s_curr, a2]
        q_stage2[s_curr, a2] += current_lr * delta_stage2

        # Stage 1 Update
        # Using standard SARSA/Q-learning temporal difference for Stage 1
        delta_stage1 = q_stage2[s_curr, a2] - q_stage1[a1]
        q_stage1[a1] += current_lr * delta_stage1
        q_stage1[a1] += current_lr * delta_stage2 # Eligibility trace

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Inverse Temperature (Exploration/Exploitation)
This model posits that OCI affects the decision noise or the exploration-exploitation balance. High anxiety or compulsivity might lead to more rigid, deterministic behavior (high beta, low exploration) once a preference is formed, or conversely, more erratic behavior. Here, we model `beta` as a function of OCI.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid learner where the inverse temperature (beta) is modulated by OCI.
    
    Hypothesis: OCI affects the randomness of choice. High OCI might lead to 
    more deterministic (rigid) choices (higher beta) or more noisy choices.
    This model allows the data to determine the direction via the oci_beta_mod parameter.
    
    Parameters:
    lr: [0, 1] Learning rate.
    w: [0, 1] Mixing weight (fixed).
    beta_base: [0, 10] Baseline inverse temperature.
    oci_beta_mod: [-5, 5] Modification to beta based on OCI. 
                  (Can be negative to reduce beta, positive to increase).
    
    effective_beta = clip(beta_base + (oci_beta_mod * oci), 0, 10)
    """
    lr, w, beta_base, oci_beta_mod = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]

    # Calculate effective beta
    beta = beta_base + (oci_beta_mod * participant_oci)
    # Clip beta to reasonable bounds [0, 20] (slightly wider than base to allow range)
    if beta < 0: beta = 0.0
    if beta > 20: beta = 20.0 # Upper bound guardrail

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Choice ---
        s_curr = int(state[trial])
        
        exp_q2 = np.exp(beta * q_stage2_mf[s_curr])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        r = reward[trial]

        delta_stage2 = r - q_stage2_mf[s_curr, a2]
        q_stage2_mf[s_curr, a2] += lr * delta_stage2

        delta_stage1 = q_stage2_mf[s_curr, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        q_stage1_mf[a1] += lr * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```