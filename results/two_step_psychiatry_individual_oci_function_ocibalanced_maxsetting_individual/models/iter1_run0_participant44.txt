Here are three new cognitive models inspired by the participant's data and OCI score. The participant has a high OCI score (0.7) and shows signs of repetitive choice behavior (perseveration) and potentially rigid learning.

### Cognitive Model 1: OCI-Modulated Model-Based/Model-Free Balance
This model hypothesizes that high OCI scores correlate with a reliance on habit-based (Model-Free) control over goal-directed (Model-Based) control. Instead of a static mixing weight `w`, the balance between MB and MF systems is dynamically shifted by the OCI score. A base weight parameter is adjusted downward by the OCI score, suggesting that higher compulsivity reduces model-based planning.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated MB/MF Balance.
    
    Hypothesis: Higher OCI scores reduce the influence of the Model-Based (MB) system
    and increase reliance on the Model-Free (MF) system. The mixing weight `w` is 
    calculated as a base weight minus an OCI-dependent penalty.
    
    Parameters:
    learning_rate: [0, 1] Standard learning rate for Q-value updates.
    beta: [0, 10] Inverse temperature for softmax.
    w_max: [0, 1] Maximum possible Model-Based weight (when OCI is 0).
    oci_penalty: [0, 1] Strength of OCI reduction on MB weight.
    """
    learning_rate, beta, w_max, oci_penalty = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective weight: w = w_max - (penalty * oci)
    # We clip it to be between 0 and 1.
    w = np.clip(w_max - (oci_penalty * oci_score), 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based value calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combined value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        s_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        r = reward[trial]
        
        # SARSA / TD(1) style update for stage 1 based on stage 2 value
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Standard Q-learning for stage 2
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2: OCI-Driven Learning Rate Asymmetry
This model suggests that OCI relates to how individuals process positive versus negative outcomes. Specifically, individuals with high compulsivity might be hyper-sensitive to errors (punishment/lack of reward) or rigid in maintaining beliefs despite evidence. Here, we model this as separate learning rates for positive prediction errors (better than expected) and negative prediction errors (worse than expected), where the ratio or magnitude is modulated by the OCI score.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Driven Learning Rate Asymmetry.
    
    Hypothesis: OCI symptoms affect how participants learn from positive vs negative
    prediction errors. A base learning rate is used, but OCI amplifies learning 
    from negative outcomes (loss/omission of reward), reflecting a fear of failure 
    or error sensitivity often seen in OCD.
    
    Parameters:
    lr_base: [0, 1] Base learning rate for positive prediction errors.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Mixing weight (MB vs MF).
    oci_neg_boost: [0, 5] Multiplier for OCI to boost learning from negative errors.
    """
    lr_base, beta, w, oci_neg_boost = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective learning rates
    # Positive errors use base rate
    alpha_pos = lr_base 
    # Negative errors use base rate + boost from OCI
    alpha_neg = np.clip(lr_base + (oci_neg_boost * oci_score * 0.5), 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        s_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        r = reward[trial]
        
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        lr_2 = alpha_pos if delta_stage2 > 0 else alpha_neg
        q_stage2_mf[s_idx, a2] += lr_2 * delta_stage2
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        lr_1 = alpha_pos if delta_stage1 > 0 else alpha_neg
        q_stage1_mf[a1] += lr_1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3: OCI-Scaled Uncertainty Avoidance (Exploration Penalty)
This model posits that high OCI scores are associated with an intolerance of uncertainty. In reinforcement learning terms, uncertainty often drives exploration (trying options with variable outcomes). A high OCI individual might *avoid* exploration, effectively having a higher inverse temperature (beta) specifically modulated by their OCI score. They "exploit" (stick to what is known) more rigidly than low OCI individuals.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Scaled Uncertainty Avoidance (Exploration Penalty).
    
    Hypothesis: High OCI leads to "Intolerance of Uncertainty," resulting in 
    reduced exploration. This is modeled by scaling the inverse temperature (beta)
    with the OCI score. Higher OCI leads to higher beta (more deterministic/rigid 
    choices) and less random exploration.
    
    Parameters:
    lr: [0, 1] Learning rate.
    beta_base: [0, 10] Baseline inverse temperature.
    w: [0, 1] Mixing weight (MB vs MF).
    oci_rigidity: [0, 5] Scaling factor: how much OCI increases beta.
    """
    lr, beta_base, w, oci_rigidity = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Effective beta is base + boost from OCI
    # Higher beta = sharper probability distribution = less exploration
    beta_effective = beta_base + (oci_rigidity * oci_score * 2.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Use the OCI-modified beta
        exp_q1 = np.exp(beta_effective * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        s_idx = int(state[trial])
        exp_q2 = np.exp(beta_effective * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        r = reward[trial]
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_stage2
        
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```