Here are three new cognitive models based on the participant's low OCI score (0.3) and the provided data patterns. The data shows periods of sticking to one spaceship, followed by switching, suggesting a balance between model-based planning and simple habit, potentially influenced by OCI.

### Model 1: Hybrid Learner with OCI-modulated Mixing Weight
This model assumes the participant uses a hybrid of Model-Based (MB) and Model-Free (MF) strategies. The OCI score modulates the mixing weight `w`, reflecting the hypothesis that obsessive-compulsive traits might influence the reliance on rigid (MF) vs. flexible (MB) control. Since the score is low (0.3), this model tests if `w` is derived dynamically from OCI rather than being a static parameter.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free learner where the mixing weight w is a function of OCI.
    
    The mixing weight w determines the balance between Model-Based (planning) and 
    Model-Free (habit) values. w is modeled as a logistic function of OCI, allowing
    symptom severity to shift the balance.
    
    Parameters:
    learning_rate: [0, 1] Learning rate for value updates.
    beta: [0, 10] Inverse temperature for softmax choice.
    w_scale: [0, 10] Scaling factor for OCI influence on mixing weight.
    """
    learning_rate, beta, w_scale = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate mixing weight w based on OCI (bounded 0 to 1 via sigmoid-like logic or clipping)
    # A simple linear mapping clipped to [0,1] is often robust.
    # Here we assume w = w_scale * OCI, clipped.
    w = np.clip(w_scale * oci_score, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # Values for aliens (state, action)

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Stage 1 Choice: Hybrid of MB and MF
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_hybrid = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_hybrid)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2 Choice: Pure MF
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Learning
        # Stage 1 MF update (SARSA-like using stage 2 value)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Stage 2 MF update
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    valid_trials = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```

### Model 2: Model-Based with OCI-modulated Eligibility Trace
This model introduces an eligibility trace parameter `lambda` that is modulated by the OCI score. An eligibility trace connects the second-stage reward back to the first-stage choice. A higher eligibility trace implies stronger direct reinforcement of the first-stage action based on the final outcome (ignoring the transition structure), which is characteristic of model-free or habit-based learning.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Based learner with an eligibility trace (lambda) modulated by OCI.
    
    The eligibility trace determines how much credit the first-stage action gets
    from the second-stage reward directly. 
    lambda_eff = lambda_base + lambda_oci * OCI
    
    Parameters:
    lr: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    lambda_base: [0, 1] Base eligibility trace.
    lambda_oci: [0, 1] Sensitivity of eligibility trace to OCI.
    """
    lr, beta, lambda_base, lambda_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective lambda, clipped to [0, 1]
    lambda_eff = np.clip(lambda_base + lambda_oci * oci_score, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2) # Directly updated via trace
    q_stage2 = np.zeros((2, 2)) 

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Stage 1 Choice: Mixture of MB (via transition) and direct value
        # Here we define the policy based on a simple weighted sum implicitly handled 
        # by how we update q_stage1. 
        # However, standard MB uses transition matrix. Let's use pure MB for value estimation
        # but allow the q_stage1 variable to accumulate direct reinforcement via lambda.
        
        max_q_stage2 = np.max(q_stage2, axis=1)
        mb_value = transition_matrix @ max_q_stage2
        
        # Combined value: The agent considers both the computed MB value 
        # and the cached value q_stage1 which holds the trace-updated history.
        # We average them for the decision variable.
        q_integrated = 0.5 * mb_value + 0.5 * q_stage1
        
        exp_q1 = np.exp(beta * q_integrated)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2 Choice
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Update Stage 2
        delta2 = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += lr * delta2
        
        # Update Stage 1 via Eligibility Trace
        # The update is driven by the stage 2 prediction error, scaled by lambda
        q_stage1[a1] += lr * lambda_eff * delta2

    eps = 1e-10
    valid_trials = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```

### Model 3: Asymmetric Learning Rate Model
This model posits that the participant might learn differently from positive versus negative prediction errors, and this asymmetry is influenced by their OCI score. This is relevant for checking if low-OCI participants are more sensitive to rewards (positive RPE) or punishments/omissions (negative RPE).

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner with asymmetric learning rates for positive and negative RPEs.
    The OCI score scales the learning rate for negative prediction errors (punishment/omission).
    
    Parameters:
    lr_pos: [0, 1] Learning rate for positive prediction errors.
    lr_neg_base: [0, 1] Base learning rate for negative prediction errors.
    beta: [0, 10] Inverse temperature.
    """
    lr_pos, lr_neg_base, beta = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # OCI modulates sensitivity to negative outcomes
    # Hyp: Higher OCI might mean more sensitivity to lack of reward (perfectionism/checking)
    lr_neg = np.clip(lr_neg_base * (1.0 + oci_score), 0.0, 1.0)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Stage 1 Choice (Pure MF for simplicity to isolate LR effects)
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2 Choice
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Stage 2 Update with Asymmetric LR
        delta2 = r - q_stage2[s_idx, a2]
        if delta2 >= 0:
            q_stage2[s_idx, a2] += lr_pos * delta2
        else:
            q_stage2[s_idx, a2] += lr_neg * delta2
            
        # Stage 1 Update (TD(0))
        # Using the value of the state arrived at (max Q) as the target
        target_stage1 = np.max(q_stage2[s_idx])
        delta1 = target_stage1 - q_stage1[a1]
        
        if delta1 >= 0:
            q_stage1[a1] += lr_pos * delta1
        else:
            q_stage1[a1] += lr_neg * delta1

    eps = 1e-10
    valid_trials = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```