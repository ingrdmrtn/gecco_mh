Here are three new cognitive models that incorporate the OCI score into different mechanisms of the decision-making process: **Asymmetric Learning** (learning from failure vs. success), **Anxiety-Rigidity** (modulation of exploration), and **Memory Decay** (forgetting of unchosen options).

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Asymmetric Learning Model.
    
    Hypothesis: OCI (related to anxiety and compulsivity) modulates the sensitivity to 
    negative prediction errors. High OCI leads to stronger learning from worse-than-expected 
    outcomes (omission of reward), reflecting a "fear of failure" or error-correction bias.
    
    Parameters:
    - learning_rate: [0, 1] Base learning rate for positive prediction errors (Success).
    - asymmetry: [0, 10] Scaling factor for negative prediction errors based on OCI.
      lr_neg = learning_rate * (1 + asymmetry * oci)
    - beta: [0, 10] Inverse temperature for softmax.
    - w: [0, 1] Mixing weight (0=MF, 1=MB).
    """
    learning_rate, asymmetry, beta, w = model_parameters
    oci_val = oci[0]
    
    # Define learning rates based on valence
    lr_pos = learning_rate
    lr_neg = learning_rate * (1.0 + asymmetry * oci_val)
    # Clamp to ensure stability
    if lr_neg > 1.0: lr_neg = 1.0
    
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        if a1 == -1: continue
        
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # Stage 1 Update (TD0)
        delta_1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        alpha_1 = lr_pos if delta_1 >= 0 else lr_neg
        q_stage1_mf[a1] += alpha_1 * delta_1
        
        # Stage 2 Update (Reward)
        delta_2 = r - q_stage2_mf[s_idx, a2]
        alpha_2 = lr_pos if delta_2 >= 0 else lr_neg
        q_stage2_mf[s_idx, a2] += alpha_2 * delta_2
        
        # Stage 1 Update from Stage 2 Reward (Eligibility Trace)
        q_stage1_mf[a1] += alpha_2 * delta_2

    mask = (action_1 != -1)
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Rigidity Model (Beta Modulation).
    
    Hypothesis: High OCI is associated with rigid, deterministic behavior and intolerance 
    of uncertainty. This is modeled by modulating the inverse temperature (beta) with the 
    OCI score. Higher OCI leads to higher beta (exploitation/rigidity), while lower OCI 
    allows for more exploration.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta_base: [0, 10] Baseline inverse temperature.
    - beta_scale: [0, 10] Scaling factor for OCI influence on beta.
      beta_effective = beta_base * (1 + beta_scale * oci)
    - w: [0, 1] Mixing weight (0=MF, 1=MB).
    """
    learning_rate, beta_base, beta_scale, w = model_parameters
    oci_val = oci[0]
    
    # Calculate effective beta
    beta_eff = beta_base * (1.0 + beta_scale * oci_val)
    
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        if a1 == -1: continue
            
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_eff * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Stage 2 Policy
        exp_q2 = np.exp(beta_eff * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # Updates
        delta_1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_1
        
        delta_2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_2
        
        q_stage1_mf[a1] += learning_rate * delta_2

    mask = (action_1 != -1)
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Memory Decay Model.
    
    Hypothesis: OCI scores impact how value representations are maintained in working memory.
    This model introduces a decay parameter for unchosen options, modulated by OCI.
    High OCI may lead to faster 'forgetting' or devaluation of unvisited states (attentional narrowing),
    or conversely, better maintenance depending on the fitted parameter.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for chosen options.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight.
    - decay_rate: [0, 1] Factor determining how much OCI drives decay.
      decay = decay_rate * oci
      Unchosen values are multiplied by (1 - decay).
    """
    learning_rate, beta, w, decay_rate = model_parameters
    oci_val = oci[0]
    
    # Decay proportional to OCI
    decay = decay_rate * oci_val
    if decay > 1.0: decay = 1.0
    
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        if a1 == -1: continue
            
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # Updates
        # Stage 1 Chosen
        delta_1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_1
        
        # Stage 1 Unchosen Decay
        unchosen_a1 = 1 - a1
        q_stage1_mf[unchosen_a1] *= (1.0 - decay)
        
        # Stage 2 Chosen
        delta_2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_2
        
        # Stage 2 Unchosen Decay (within the current state)
        unchosen_a2 = 1 - a2
        q_stage2_mf[s_idx, unchosen_a2] *= (1.0 - decay)
        
        # Stage 1 Update from Stage 2 Reward
        q_stage1_mf[a1] += learning_rate * delta_2

    mask = (action_1 != -1)
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss
```