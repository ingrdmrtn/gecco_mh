Here are three new cognitive models that incorporate OCI scores to explain participant behavior in the two-step task.

### Cognitive Model 1: Subjective Transition Belief Model
**Hypothesis**: Participants with varying OCI symptoms may perceive the structure of the world (transition probabilities) differently. While the true transition matrix is fixed (0.7 probability), high OCI might correlate with "uncertainty intolerance" (perceiving transitions as more random, closer to 0.5) or "rigidity" (perceiving them as deterministic, closer to 1.0). This model allows the OCI score to modulate the *perceived* transition probability used in the Model-Based (MB) calculation.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Subjective Transition Belief Model.
    OCI modulates the perceived transition probability used in Model-Based updates.
    
    Parameters:
    - lr: [0, 1] Learning rate for Model-Free values.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - w: [0, 1] Mixing weight (0=MF, 1=MB).
    - stickiness_param: [0, 1] Transformed to [-5, 5] for choice persistence.
    - p_low: [0, 1] Perceived transition probability when OCI is 0.
    - p_high: [0, 1] Perceived transition probability when OCI is 1.
    """
    lr, beta_1, beta_2, w, stickiness_param, p_low, p_high = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    # Calculate subjective transition probability based on OCI
    p_trans = p_low * (1 - current_oci) + p_high * current_oci
    # Ensure p_trans stays within reasonable probability bounds
    p_trans = np.clip(p_trans, 0.01, 0.99)
    
    # Subjective transition matrix: [[p, 1-p], [1-p, p]]
    # Assumes A->X (0->0) is the dominant diagonal if p > 0.5
    transition_matrix = np.array([[p_trans, 1 - p_trans], [1 - p_trans, p_trans]])

    stickiness = (stickiness_param - 0.5) * 10.0
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # state x action
    
    last_action_1 = -1
    log_likelihood = 0.0
    eps = 1e-10

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        # Model-Based Value: Expected max value of next stage using subjective transitions
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Apply Stickiness
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_likelihood += np.log(probs_1[a1] + eps)

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta_2 * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_likelihood += np.log(probs_2[a2] + eps)

        # --- Updates ---
        # Standard Q-learning for Stage 2
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_stage2

        # TD-learning for Stage 1 MF (using Stage 2 Q-value as proxy for value of state)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        last_action_1 = a1

    return -log_likelihood
```

### Cognitive Model 2: Stage-Specific Learning Rate Modulation
**Hypothesis**: OCI symptoms may differentially affect the formation of habits (Stage 1 associations) versus direct reward learning (Stage 2 associations). This model separates the learning rates for Stage 1 and Stage 2, with the Stage 1 learning rate being modulated by OCI. High OCI might lead to faster habit formation (higher Stage 1 LR) relative to goal value updating.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Stage-Specific Learning Rate Model.
    OCI modulates the learning rate for Stage 1 (Habit) specifically, 
    while Stage 2 (Goal/Reward) learning rate is fixed.
    
    Parameters:
    - lr_s2: [0, 1] Base learning rate for Stage 2 (Direct Reward).
    - lr_s1_low: [0, 1] Learning rate for Stage 1 when OCI is 0.
    - lr_s1_high: [0, 1] Learning rate for Stage 1 when OCI is 1.
    - beta_1: [0, 10] Inverse temperature Stage 1.
    - beta_2: [0, 10] Inverse temperature Stage 2.
    - w: [0, 1] Mixing weight (0=MF, 1=MB).
    - stickiness_param: [0, 1] Transformed to [-5, 5] for choice persistence.
    """
    lr_s2, lr_s1_low, lr_s1_high, beta_1, beta_2, w, stickiness_param = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    # Calculate Stage 1 learning rate based on OCI
    lr_s1 = lr_s1_low * (1 - current_oci) + lr_s1_high * current_oci
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    stickiness = (stickiness_param - 0.5) * 10.0
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    log_likelihood = 0.0
    eps = 1e-10

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_likelihood += np.log(probs_1[a1] + eps)

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta_2 * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_likelihood += np.log(probs_2[a2] + eps)

        # --- Updates ---
        # Update Stage 2 using fixed base learning rate
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr_s2 * delta_stage2

        # Update Stage 1 using OCI-modulated learning rate
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr_s1 * delta_stage1
        
        last_action_1 = a1

    return -log_likelihood
```

### Cognitive Model 3: OCI-Modulated Fictive Learning
**Hypothesis**: High OCI is often associated with "checking" behavior and counterfactual thinking ("What if I had chosen the other option?"). This model introduces "Fictive Learning" (or counterfactual updating) for the unchosen alien in Stage 2. The strength of this update—assuming the unchosen option would have yielded the opposite outcome—is modulated by OCI.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Fictive (Counterfactual) Learning Model.
    OCI modulates the degree to which the participant updates the unchosen 
    option in Stage 2, assuming a counterfactual reward (1 - actual_reward).
    
    Parameters:
    - lr: [0, 1] Base learning rate.
    - beta_1: [0, 10] Inverse temperature Stage 1.
    - beta_2: [0, 10] Inverse temperature Stage 2.
    - w: [0, 1] Mixing weight.
    - stickiness_param: [0, 1] Transformed to [-5, 5].
    - fictive_low: [0, 1] Weight of fictive update when OCI is 0.
    - fictive_high: [0, 1] Weight of fictive update when OCI is 1.
    """
    lr, beta_1, beta_2, w, stickiness_param, fictive_low, fictive_high = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    # Calculate fictive weighting based on OCI
    fictive_weight = fictive_low * (1 - current_oci) + fictive_high * current_oci
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    stickiness = (stickiness_param - 0.5) * 10.0
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    log_likelihood = 0.0
    eps = 1e-10

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_likelihood += np.log(probs_1[a1] + eps)

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta_2 * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_likelihood += np.log(probs_2[a2] + eps)

        # --- Updates ---
        # 1. Update Chosen Option (Stage 2)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_stage2
        
        # 2. Update Unchosen Option (Fictive Learning)
        unchosen_a2 = 1 - a2
        # Counterfactual assumption: if chosen got r, unchosen might have got 1-r
        # (Note: In independent reward tasks this is a bias, but a common one).
        fictive_reward = 1.0 - r
        delta_fictive = fictive_reward - q_stage2_mf[s_idx, unchosen_a2]
        # Apply update scaled by fictive_weight
        q_stage2_mf[s_idx, unchosen_a2] += lr * fictive_weight * delta_fictive

        # 3. Update Stage 1
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        last_action_1 = a1

    return -log_likelihood
```