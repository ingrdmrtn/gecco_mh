Here are three new cognitive models designed to capture the behavior of the participant, incorporating the OCI score into the learning and decision-making mechanisms.

### Model 1: Hybrid Learner with OCI-Modulated Model-Based Weight (`w`)
This model assumes the participant uses a hybrid of model-free (MF) and model-based (MB) reinforcement learning. The core hypothesis is that high OCI scores might relate to a reliance on "habitual" (model-free) vs. "goal-directed" (model-based) control. Here, the weighting parameter `w` (which balances MF and MB values) is modulated by the OCI score. A high OCI might push the participant towards more rigid, model-free habits or conversely, towards over-thinking (model-based).

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free learner where the weighting parameter 'w'
    is modulated by the OCI score.
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    w_base: [0, 1]
    w_oci_slope: [-1, 1]
    """
    learning_rate, beta, w_base, w_oci_slope = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    # Calculate effective weight w based on OCI
    # w = 1 is pure Model-Based, w = 0 is pure Model-Free
    w = w_base + (w_oci_slope * current_oci)
    w = np.clip(w, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_mf = np.zeros((2, 2))      # Stage 2 Model-Free values (aliens)
    q_mb = np.zeros((2, 2))      # Stage 1 Model-Based values (spaceships)
    q_stage1_mf = np.zeros(2)    # Stage 1 Model-Free values (spaceships)

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        
        # Calculate Model-Based values for Stage 1
        # V_MB(s1) = sum(P(s2|s1) * max(Q_MF(s2, a2)))
        # We use q_mf from stage 2 as the value of the states for MB calculation
        max_q_stage2 = np.max(q_mf, axis=1)
        q_mb_val = transition_matrix @ max_q_stage2
        
        # Combine MF and MB values
        q_net = w * q_mb_val + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        if action_1[trial] != -1:
            a1 = int(action_1[trial])
            p_choice_1[trial] = probs_1[a1]
        else:
            p_choice_1[trial] = 1.0
            continue # Skip learning if data is missing

        # --- Stage 2 Decision ---
        s_idx = int(state[trial]) # The planet arrived at
        
        exp_q2 = np.exp(beta * q_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        if action_2[trial] != -1:
            a2 = int(action_2[trial])
            p_choice_2[trial] = probs_2[a2]
            
            r = reward[trial]
            
            # --- Learning ---
            
            # 1. Update Stage 2 MF values (TD(0))
            # Q_MF(s2, a2) += alpha * (r - Q_MF(s2, a2))
            pe_2 = r - q_mf[s_idx, a2]
            q_mf[s_idx, a2] += learning_rate * pe_2
            
            # 2. Update Stage 1 MF values (TD(1) / SARSA-like)
            # We use the Stage 2 value *before* update (or after, depending on convention; 
            # standard TD(1) often uses the reward directly if lambda=1, 
            # but in hybrid models we often just update Q_MF(s1) towards Q_MF(s2))
            # Here we implement a simple TD update for Stage 1 MF based on the value of the chosen stage 2 option
            # This is effectively SARSA(0) for the first step if we consider the transition
            pe_1 = q_mf[s_idx, a2] - q_stage1_mf[a1]
            q_stage1_mf[a1] += learning_rate * pe_1
            
        else:
            p_choice_2[trial] = 1.0

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Perseveration (Stickiness) Modulated by OCI
This model focuses on the "stickiness" or perseveration bias often seen in decision-making. High OCI might lead to repetitive behaviors or "getting stuck" on a choice regardless of reward history. This model adds a stickiness bonus to the Q-values of the previously chosen action, where the magnitude of this bonus is determined by the OCI score. It uses a pure Model-Free framework to isolate this effect.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner with a stickiness (perseveration) bonus modulated by OCI.
    Stickiness is added to the Q-value of the previously chosen action.
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    stickiness_base: [-5, 5]
    stickiness_oci_mod: [-5, 5]
    """
    learning_rate, beta, stickiness_base, stickiness_oci_mod = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate effective stickiness
    eff_stickiness = stickiness_base + (stickiness_oci_mod * current_oci)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    prev_a1 = -1 # Track previous action for stickiness

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        q_net_1 = q_stage1.copy()
        
        # Add stickiness bonus if a previous action exists
        if prev_a1 != -1:
            q_net_1[prev_a1] += eff_stickiness
            
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        if action_1[trial] != -1:
            a1 = int(action_1[trial])
            p_choice_1[trial] = probs_1[a1]
            prev_a1 = a1 # Update previous action
        else:
            p_choice_1[trial] = 1.0
            prev_a1 = -1
            continue

        # --- Stage 2 Decision ---
        s_idx = int(state[trial])
        
        # Note: Stickiness is usually applied to the top-level choice in this task structure,
        # but could be applied to stage 2. Here we apply only to Stage 1 for simplicity/interpretability.
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        if action_2[trial] != -1:
            a2 = int(action_2[trial])
            p_choice_2[trial] = probs_2[a2]
            
            r = reward[trial]
            
            # --- Learning (Simple TD) ---
            
            # Stage 2 Update
            pe_2 = r - q_stage2[s_idx, a2]
            q_stage2[s_idx, a2] += learning_rate * pe_2
            
            # Stage 1 Update (TD(1) style, using reward directly or via stage 2 value)
            # Here we use the standard Q-learning update for stage 1 based on stage 2 value
            pe_1 = q_stage2[s_idx, a2] - q_stage1[a1]
            q_stage1[a1] += learning_rate * pe_1
            
        else:
            p_choice_2[trial] = 1.0

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Reward Sensitivity (Beta)
This model posits that OCI affects the exploration-exploitation trade-off, specifically the inverse temperature parameter `beta`. A high OCI might be associated with anxiety or a strong desire to avoid uncertainty, leading to higher `beta` (more deterministic/exploitative choice behavior) or potentially lower `beta` (more random switching due to distress). We model `beta` as a function of the OCI score.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner where the inverse temperature (beta) is modulated by OCI.
    This tests if OCI affects the randomness/determinism of choices.
    
    Bounds:
    learning_rate: [0, 1]
    beta_base: [0, 10]
    beta_oci_mod: [-5, 5]
    eligibility_lambda: [0, 1]
    """
    learning_rate, beta_base, beta_oci_mod, eligibility_lambda = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate effective beta
    beta_eff = beta_base + (beta_oci_mod * current_oci)
    # Ensure beta stays within reasonable positive bounds
    beta_eff = np.clip(beta_eff, 0.0, 20.0) 
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        exp_q1 = np.exp(beta_eff * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        if action_1[trial] != -1:
            a1 = int(action_1[trial])
            p_choice_1[trial] = probs_1[a1]
        else:
            p_choice_1[trial] = 1.0
            continue

        # --- Stage 2 Decision ---
        s_idx = int(state[trial])
        
        exp_q2 = np.exp(beta_eff * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        if action_2[trial] != -1:
            a2 = int(action_2[trial])
            p_choice_2[trial] = probs_2[a2]
            
            r = reward[trial]
            
            # --- Learning ---
            
            # Stage 2 Update
            pe_2 = r - q_stage2[s_idx, a2]
            q_stage2[s_idx, a2] += learning_rate * pe_2
            
            # Stage 1 Update with eligibility trace (lambda)
            # This blends TD(0) and TD(1)
            # TD(1) target would be Reward
            # TD(0) target would be Q_stage2
            # Here we implement a simplified eligibility trace logic often used in these tasks:
            # Q1(a1) <- Q1(a1) + alpha * (Q2(s2,a2) - Q1(a1) + lambda * (r - Q2(s2,a2)))
            # Which simplifies to updating based on a mix of the stage 2 value and the reward prediction error
            
            # Prediction error at stage 1 (using stage 2 value as proxy for reward)
            pe_1 = q_stage2[s_idx, a2] - q_stage1[a1]
            
            # The 'lambda' term brings the reward information back to stage 1
            # If lambda=1, we update Q1 based fully on the reward outcome (Monte Carlo / TD(1))
            # If lambda=0, we update Q1 based only on the value of the state we arrived at (TD(0))
            combined_pe = pe_1 + eligibility_lambda * pe_2
            
            q_stage1[a1] += learning_rate * combined_pe
            
        else:
            p_choice_2[trial] = 1.0

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```