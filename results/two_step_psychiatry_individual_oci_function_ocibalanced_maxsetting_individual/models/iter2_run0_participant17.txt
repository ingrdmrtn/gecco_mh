Here are three new cognitive models exploring different mechanisms for how OCI scores might influence decision-making in this task.

### Model 1: OCI-Modulated Model-Based/Model-Free Trade-off
This model hypothesizes that obsessive-compulsive traits (OCI) influence the balance between goal-directed (Model-Based) and habitual (Model-Free) control. Specifically, higher OCI might lead to a stronger reliance on habitual, model-free strategies, reducing the weight `w` placed on the model-based system.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free RL with OCI-modulated mixing weight.
    
    Hypothesis: OCI score directly modulates the trade-off parameter 'w'. 
    Higher OCI leads to lower 'w' (more Model-Free/Habitual), while lower OCI 
    leads to higher 'w' (more Model-Based/Goal-Directed).
    
    Parameters:
    learning_rate: [0, 1] - Learning rate for value updates.
    beta: [0, 10] - Inverse temperature for softmax.
    w_base: [0, 1] - Baseline mixing weight (tendency towards MB).
    oci_impact: [0, 1] - How strongly OCI reduces the MB weight.
    """
    learning_rate, beta, w_base, oci_impact = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate the effective mixing weight w
    # We assume OCI reduces Model-Based control.
    # We clip to ensure w stays in [0, 1].
    w = w_base - (oci_score * oci_impact)
    if w < 0: w = 0
    if w > 1: w = 1
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2) # Model-free Q-values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Model-free Q-values for stage 2

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based valuation: V(s') = max_a Q(s', a)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        # Q_MB(s, a) = T(s, a, s') * V(s')
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Q-value: w * Q_MB + (1-w) * Q_MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax choice 1
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # --- Stage 2 Policy ---
        # Standard Model-Free choice at stage 2
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Updates ---
        # SARSA-style update for stage 1 MF values
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] = q_stage1_mf[a1] + learning_rate * delta_stage1
        
        # Reward prediction error for stage 2
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] = q_stage2_mf[state_idx, a2] + learning_rate * delta_stage2
        
        # Note: Eligibility traces (lambda) are often used to update stage 1 
        # based on stage 2 reward, but here we stick to the simpler structure 
        # to focus on the 'w' modulation mechanism.

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Driven Asymmetric Learning Rates
This model suggests that OCI affects how participants learn from positive versus negative prediction errors. Individuals with higher compulsivity might be hyper-sensitive to errors (loss avoidance) or rigid in their beliefs. Here, we model this as the OCI score scaling the learning rate specifically for negative prediction errors (disappointments), leading to potential asymmetry.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free RL with OCI-modulated Asymmetric Learning Rates.
    
    Hypothesis: OCI affects sensitivity to negative outcomes. 
    The learning rate for negative prediction errors is scaled by the OCI score.
    
    Parameters:
    alpha_pos: [0, 1] - Learning rate for positive prediction errors.
    alpha_neg_base: [0, 1] - Base learning rate for negative prediction errors.
    beta: [0, 10] - Inverse temperature.
    oci_sens: [0, 5] - Multiplier for OCI to increase/decrease negative learning rate.
    """
    alpha_pos, alpha_neg_base, beta, oci_sens = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective negative learning rate
    # Higher OCI might amplify the reaction to negative outcomes (or dampen it).
    alpha_neg = alpha_neg_base * (1 + oci_score * oci_sens)
    # Clip to valid range [0, 1]
    if alpha_neg > 1.0: alpha_neg = 1.0
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Pure Model-Free formulation for simplicity to isolate learning rate effects
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Updates ---
        # Stage 2 Update
        delta2 = r - q_stage2[state_idx, a2]
        lr2 = alpha_pos if delta2 > 0 else alpha_neg
        q_stage2[state_idx, a2] += lr2 * delta2
        
        # Stage 1 Update (TD-0 for simplicity)
        # Using the value of the state arrived at (max Q or actual Q)
        # Here we use the value of the chosen action in stage 2 as the target
        delta1 = q_stage2[state_idx, a2] - q_stage1[a1]
        lr1 = alpha_pos if delta1 > 0 else alpha_neg
        q_stage1[a1] += lr1 * delta1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Inverse Temperature (Exploration/Exploitation)
This model posits that OCI traits influence the randomness of choice behavior (exploration vs. exploitation). Higher OCI might manifest as more rigid, deterministic behavior (high beta), or conversely, as high uncertainty/anxiety leading to erratic switching. This model scales the inverse temperature `beta` based on the OCI score.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model with OCI-modulated Inverse Temperature (Beta).
    
    Hypothesis: OCI score influences the exploration-exploitation trade-off.
    The effective beta is a base beta plus a modulation term based on OCI.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate.
    beta_base: [0, 10] - Baseline inverse temperature.
    w: [0, 1] - Fixed mixing weight for MB/MF.
    oci_beta_mod: [0, 10] - Scaling factor for OCI's effect on beta.
    """
    learning_rate, beta_base, w, oci_beta_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Effective Beta
    # We assume OCI increases rigidity (higher beta = more exploitation)
    beta_eff = beta_base + (oci_score * oci_beta_mod)
    # Ensure beta doesn't explode, though softmax handles large values okay, 
    # we keep it within reason for the optimizer.
    if beta_eff > 20: beta_eff = 20
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_eff * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # --- Stage 2 ---
        exp_q2 = np.exp(beta_eff * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```