Here are three cognitive models implemented as Python functions, designed to explain the participant's behavior in the two-step task, incorporating their OCI score.

### Model 1: Dual Learning Rates with OCI-Modulated Stickiness
This model posits that the participant learns the value of the abstract spaceships (Stage 1) and the concrete aliens (Stage 2) at different rates. The "stickiness" (tendency to repeat choices) is modulated by the OCI score, testing the hypothesis that compulsive symptoms drive choice perseveration.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Dual Learning Rate Model with OCI-Modulated Stickiness.
    
    Hypothesis: The participant updates Stage 1 (Spaceship) and Stage 2 (Alien) values
    at different rates. The tendency to repeat choices (stickiness) is modulated by 
    the OCI score, reflecting compulsive perseveration.
    
    Parameters:
    lr_1: [0, 1] - Learning rate for Stage 1 (Spaceships).
    lr_2: [0, 1] - Learning rate for Stage 2 (Aliens).
    beta: [0, 10] - Inverse temperature (randomness).
    w: [0, 1] - Weighting between Model-Based (1) and Model-Free (0).
    stick_base: [-5, 5] - Baseline choice stickiness/perseveration.
    stick_oci: [-5, 5] - Modulation of stickiness by OCI score.
    """
    lr_1, lr_2, beta, w, stick_base, stick_oci = model_parameters
    n_trials = len(action_1)
    oci_val = oci[0]
    
    # Calculate effective stickiness based on OCI
    stickiness = stick_base + (stick_oci * oci_val)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2) # Model-free values for spaceships
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for aliens (State x Action)
    
    last_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # Model-Based Value: Expected max value of next stage based on transition probs
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Calculate logits with stickiness
        logits_1 = beta * q_net
        if last_action_1 != -1:
            logits_1[last_action_1] += stickiness
            
        # Softmax Probability
        exp_q1 = np.exp(logits_1 - np.max(logits_1)) # Subtract max for stability
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        last_action_1 = int(action_1[trial])
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])

        # --- Stage 2 Decision ---
        # Standard Softmax on Stage 2 Q-values
        logits_2 = beta * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        # Update Stage 1 MF value using Stage 2 Q-value (TD(0))
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr_1 * delta_stage1
        
        # Update Stage 2 MF value using Reward
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += lr_2 * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Eligibility Trace (Lambda)
This model introduces an eligibility trace parameter ($\lambda$) that controls how much the final reward directly reinforces the first-stage choice. The OCI score modulates this parameter, testing if compulsivity relates to a more "habitual" stamping-in of actions by outcomes (high $\lambda$) or a more sequential value estimation (low $\lambda$).

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model with OCI-Modulated Eligibility Trace (Lambda).
    
    Hypothesis: OCI scores influence the eligibility trace (lambda), which determines
    how strongly the final outcome directly updates the first-stage value.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate for value updates.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - Weighting between Model-Based (1) and Model-Free (0).
    stickiness: [-5, 5] - Choice perseveration bonus.
    lambda_base: [0, 1] - Baseline eligibility trace.
    lambda_oci: [-1, 1] - Modulation of lambda by OCI score.
    """
    learning_rate, beta, w, stickiness, lambda_base, lambda_oci = model_parameters
    n_trials = len(action_1)
    oci_val = oci[0]
    
    # Calculate lambda and clip to valid range [0, 1]
    lambda_param = lambda_base + (lambda_oci * oci_val)
    lambda_param = np.clip(lambda_param, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net
        if last_action_1 != -1:
            logits_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        last_action_1 = int(action_1[trial])
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])

        # --- Stage 2 Decision ---
        logits_2 = beta * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        # Stage 1 TD error (based on Stage 2 Q-value)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Stage 2 TD error (based on Reward)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Eligibility Trace Update: Propagate Stage 2 error back to Stage 1
        q_stage1_mf[a1] += learning_rate * lambda_param * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Model-Based Weight with Stickiness
This model investigates whether OCI scores predict the balance between Model-Based (goal-directed) and Model-Free (habitual) control (`w`), while explicitly accounting for stickiness. Previous attempts explored `w` modulation without stickiness; however, given the participant's highly repetitive data, stickiness is essential to isolate the specific effect of OCI on the planning strategy.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model with OCI-Modulated MB/MF Weight (w) and Constant Stickiness.
    
    Hypothesis: OCI scores predict the reliance on Model-Based vs Model-Free strategies.
    Stickiness is included as a constant parameter to account for the participant's
    perseveration, allowing 'w' to capture the planning aspect more accurately.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate.
    beta: [0, 10] - Inverse temperature.
    w_base: [0, 1] - Baseline MB weighting.
    w_oci: [-1, 1] - Modulation of w by OCI score.
    stickiness: [-5, 5] - Choice perseveration bonus.
    """
    learning_rate, beta, w_base, w_oci, stickiness = model_parameters
    n_trials = len(action_1)
    oci_val = oci[0]
    
    # Calculate w and clip to [0, 1]
    w = w_base + (w_oci * oci_val)
    w = np.clip(w, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net
        if last_action_1 != -1:
            logits_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        last_action_1 = int(action_1[trial])
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])

        # --- Stage 2 Decision ---
        logits_2 = beta * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```