Here are three new cognitive models exploring different mechanisms for how Obsessive-Compulsive Inventory (OCI) scores might influence decision-making in this two-step task.

### Model 1: OCI-Modulated Model-Based/Model-Free Balance
This model hypothesizes that OCI symptoms affect the balance between goal-directed (Model-Based) and habitual (Model-Free) control. Specifically, it tests if higher OCI scores lead to a stronger reliance on Model-Based planning (perhaps due to overthinking or a desire for control) or Model-Free habits. The mixing parameter `w` is a linear function of the OCI score.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated MB/MF Balance.
    
    This model assumes a hybrid learner where the weight (w) determining the trade-off 
    between Model-Based (MB) and Model-Free (MF) values is modulated by the OCI score.
    w = w_base + (w_oci * oci_score). 
    w is clamped between 0 (pure MF) and 1 (pure MB).
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for value updates.
    - beta: [0, 10] Inverse temperature for softmax choice.
    - w_base: [0, 1] Base weight for Model-Based control.
    - w_oci: [-1, 1] Modulation of MB weight by OCI score.
    """
    learning_rate, beta, w_base, w_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective weight w, clamped between 0 and 1
    w = w_base + (w_oci * oci_score)
    w = np.clip(w, 0.0, 1.0)
    
    # Transition matrix (fixed structure for the task: A->X commonly, U->Y commonly)
    # Rows: Choice (0=A, 1=U), Cols: State (0=X, 1=Y)
    # Assuming standard task probabilities 0.7 common, 0.3 rare
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)      # MF Q-values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # MF Q-values for stage 2 (State x Action)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])
        
        if a1 == -1: # Skip missing trials
            continue

        # --- Stage 1 Policy ---
        # Model-Based Value Calculation: V_MB(s1, a1) = Sum(P(s2|s1,a1) * max_a2 Q(s2, a2))
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value: Q_net = w * Q_MB + (1-w) * Q_MF
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        # Standard Model-Free choice at Stage 2
        exp_q2 = np.exp(beta * q_stage2_mf[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Q_MF(Stage 1) update using TD(0) to the Stage 2 value
        # Note: Standard hybrid models often update MF stage 1 with the value of the chosen stage 2 state/action
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Q_MF(Stage 2) update using reward
        delta_stage2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += learning_rate * delta_stage2
        
        # Eligibility trace for Stage 1 (TD(1) logic often used, but simplified here to direct update for clarity in this specific model structure unless lambda is requested. 
        # Standard hybrid models update Q_MF(S1) again with the stage 2 RPE:
        q_stage1_mf[a1] += learning_rate * delta_stage2 # TD(1) style update assuming lambda=1 for simplicity in this 4-param model

    eps = 1e-10
    valid_trials = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```

### Model 2: OCI-Dependent Learning Rates for Positive vs. Negative Outcomes
This model investigates if OCI scores correlate with an asymmetry in how participants learn from rewards versus omissions (lack of reward). It proposes that higher OCI might be associated with hypersensitivity to failure (negative prediction errors) or success, modulating the learning rate specifically for negative outcomes relative to positive ones.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Dependent Asymmetric Learning Rates.
    
    This model allows for different learning rates for positive prediction errors (Reward=1)
    and negative prediction errors (Reward=0). The learning rate for negative outcomes
    is modulated by the OCI score, testing if OCI relates to sensitivity to 'failure'.
    
    Parameters:
    - lr_pos: [0, 1] Learning rate for positive outcomes (R=1).
    - lr_neg_base: [0, 1] Base learning rate for negative outcomes (R=0).
    - lr_neg_oci: [-1, 1] Modulation of negative learning rate by OCI.
    - beta: [0, 10] Inverse temperature.
    """
    lr_pos, lr_neg_base, lr_neg_oci, beta = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective negative learning rate
    lr_neg = lr_neg_base + (lr_neg_oci * oci_score)
    lr_neg = np.clip(lr_neg, 0.0, 1.0)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Pure Model-Free Q-values
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])
        
        if a1 == -1:
            continue

        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Determine which learning rate to use based on the final outcome
        # Note: In standard TD, the RPE sign determines this, but here we use the raw reward outcome
        # as a proxy for "good" vs "bad" trial outcome to modulate learning.
        current_lr = lr_pos if r == 1 else lr_neg

        # Update Stage 2
        delta_stage2 = r - q_stage2[s2, a2]
        q_stage2[s2, a2] += current_lr * delta_stage2
        
        # Update Stage 1
        # Using simple SARSA-like update: Q1(a1) <- Q1(a1) + lr * (Q2(s2, a2) - Q1(a1))
        # Then the second stage RPE is also often passed back. 
        # Here we use a direct update from the final reward to Stage 1 for simplicity (Direct Reinforcement)
        delta_total = r - q_stage1[a1]
        q_stage1[a1] += current_lr * delta_total

    eps = 1e-10
    valid_trials = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```

### Model 3: OCI-Scaled Inverse Temperature (Exploration/Exploitation)
This model posits that OCI symptoms affect the overall randomness of behavior (decision noise). High OCI might lead to more rigid, deterministic choices (high beta) or, conversely, more uncertainty and noise (low beta). This model makes the inverse temperature parameter `beta` a function of the OCI score.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Scaled Inverse Temperature (Beta).
    
    This model assumes a standard Model-Based learner (with fixed partial MF influence via lambda=0 for simplicity 
    or just pure MB logic, but here we implement a hybrid structure where w is fixed at 0.5 to isolate beta effects).
    The key feature is that the inverse temperature (beta) is modulated by OCI.
    beta = beta_base * exp(beta_oci * oci_score).
    Using exponential to ensure beta stays positive.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta_base: [0, 10] Base inverse temperature.
    - beta_oci: [-2, 2] Scaling factor for OCI influence on beta.
    - w_mix: [0, 1] Fixed mixing weight between MB and MF (estimated param).
    """
    learning_rate, beta_base, beta_oci, w_mix = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective beta
    # If beta_oci is positive, higher OCI -> higher beta (more exploitation/rigidity)
    # If beta_oci is negative, higher OCI -> lower beta (more exploration/noise)
    beta = beta_base * np.exp(beta_oci * oci_score)
    # Cap beta to prevent overflow
    beta = np.clip(beta, 0.0, 20.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])
        
        if a1 == -1:
            continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w_mix * q_stage1_mb + (1 - w_mix) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Standard TD updates
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += learning_rate * delta_stage2
        
        # Second stage reward propagating to first stage MF Q-values
        q_stage1_mf[a1] += learning_rate * delta_stage2

    eps = 1e-10
    valid_trials = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```