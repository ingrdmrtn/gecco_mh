Here are three new cognitive models that explore different mechanisms for how OCI symptoms might influence decision-making in the two-step task, specifically focusing on how uncertainty, habit formation, and learning rates might be modulated.

### Cognitive Model 1: OCI-Modulated Habit Strength (Model-Free Dominance)
This model hypothesizes that higher OCI scores lead to a stronger reliance on Model-Free (habitual) control over Model-Based (goal-directed) control. Instead of a fixed mixing weight `w`, the weight is dynamically determined by the OCI score. Specifically, higher OCI reduces `w` (the weight of the model-based system), pushing the participant towards habitual repetition of rewarded actions regardless of transition structure.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model with OCI-modulated mixing weight (w).
    
    Hypothesis: Higher OCI scores are associated with reduced Model-Based control 
    (lower w) and increased Model-Free reliance (habitual behavior).
    
    Parameters:
    learning_rate: [0, 1] - Update rate for Q-values
    beta: [0, 10] - Inverse temperature for softmax
    w_base: [0, 1] - Baseline mixing weight for low OCI
    w_oci_penalty: [0, 1] - Reduction in model-based weight proportional to OCI
    """
    learning_rate, beta, w_base, w_oci_penalty = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
  
    # Calculate effective w. Higher OCI reduces w (less MB, more MF).
    # We clip to ensure it stays in [0, 1].
    w = np.clip(w_base - (w_oci_penalty * current_oci), 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State (Planet) x Action (Alien)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Policy for the first choice (Stage 1) ---
        # Model-Based Value: V_MB(s1) = T * max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Value: w * MB + (1-w) * MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Policy for the second choice (Stage 2) ---
        # Standard Softmax on Stage 2 Q-values
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # --- Learning Updates ---
        # SARSA(0) / TD(0) update for Stage 1 MF
        # Note: In standard two-step, Stage 1 MF is often updated by Q(s2, a2)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # TD error for Stage 2
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2: OCI-Modulated Second-Stage Learning Rate
This model hypothesizes that OCI symptoms affect how participants learn from direct outcomes at the second stage (the aliens). Specifically, high OCI might be associated with "over-learning" from recent feedback (high learning rate) or rigidity (low learning rate). Here, we test if the learning rate `alpha` is a function of the OCI score, allowing the model to capture either hyper-sensitivity or insensitivity to reward prediction errors at the terminal state.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model where the learning rate is modulated by OCI.
    
    Hypothesis: The rate at which value estimates are updated (learning rate) 
    depends on the severity of OCI symptoms.
    
    Parameters:
    lr_base: [0, 1] - Baseline learning rate
    lr_oci_slope: [-1, 1] - How OCI changes LR (can increase or decrease)
                            (Modeled as [0, 1] parameter but shifted/scaled inside)
    beta: [0, 10] - Inverse temperature
    w: [0, 1] - Mixing weight (fixed)
    """
    lr_base, lr_oci_param, beta, w = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Map lr_oci_param from [0, 1] to [-0.5, 0.5] to allow positive or negative modulation
    lr_oci_slope = lr_oci_param - 0.5 
    
    # Calculate effective learning rate
    learning_rate = np.clip(lr_base + (lr_oci_slope * current_oci), 0.01, 1.0)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # --- Updates ---
        # Update Stage 1 MF
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Update Stage 2
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3: OCI-Modulated Choice Stickiness (Perseveration) on Stage 2
While previous feedback explored Stage 1 stickiness, this model explores the idea that OCI-related compulsivity might manifest as repetitive behavior specifically at the *second* stage (choosing the alien). This "local stickiness" implies that once a participant picks an alien, they are compelled to pick it again, regardless of reward, and the strength of this compulsion is driven by their OCI score.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model with OCI-modulated perseveration on Stage 2 (Alien choice).
    
    Hypothesis: OCI manifests as repetitive selection of the specific terminal option (Alien),
    making the participant 'stick' to the previous alien choice within that planet.
    
    Parameters:
    learning_rate: [0, 1]
    beta: [0, 10]
    w: [0, 1]
    stickiness_oci: [0, 5] - Bonus added to the previously chosen alien Q-value, scaled by OCI.
    """
    learning_rate, beta, w, stickiness_oci_param = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Stickiness bonus is purely a function of OCI here
    stickiness_bonus = stickiness_oci_param * current_oci
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track last choice for each state (planet)
    last_action_2_in_state = [-1, -1] 

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Choice with Local Stickiness ---
        q_stage2_effective = q_stage2_mf[s_idx].copy()
        
        # If we have visited this planet before, add bonus to the previously chosen alien
        if last_action_2_in_state[s_idx] != -1:
            prev_a2 = last_action_2_in_state[s_idx]
            q_stage2_effective[prev_a2] += stickiness_bonus
        
        exp_q2 = np.exp(beta * q_stage2_effective)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # Update history
        last_action_2_in_state[s_idx] = a2
  
        # --- Updates ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```