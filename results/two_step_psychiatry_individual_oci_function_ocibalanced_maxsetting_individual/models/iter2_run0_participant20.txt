Here are the three cognitive models implemented as Python functions.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Decay of Unchosen Options (Memory Interference).
    
    Hypothesis: Participants with higher OCI scores may experience greater interference 
    or difficulty maintaining value representations for unchosen options (or conversely, 
    ruminate on them). This model posits that the value of unchosen actions decays 
    towards zero at a rate modulated by the OCI score.
    
    Parameters:
    - lr: [0, 1] Learning rate for prediction errors.
    - beta: [0, 10] Inverse temperature for softmax choice.
    - w: [0, 1] Mixing weight between Model-Based (1) and Model-Free (0).
    - stick: [0, 5] Perseveration bonus for repeating the previous Stage 1 choice.
    - decay_base: [0, 1] Base decay rate for unchosen action values.
    - decay_oci_sens: [0, 5] Sensitivity of decay rate to OCI.
      decay_rate = decay_base * (1 + decay_oci_sens * oci), clipped to 1.0.
      
    Args:
        action_1, state, action_2, reward, oci, model_parameters
        
    Returns:
        float: Negative log-likelihood of the observed choices.
    """
    lr, beta, w, stick, decay_base, decay_oci_sens = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate OCI-modulated decay rate
    decay_rate = decay_base * (1.0 + decay_oci_sens * oci_score)
    if decay_rate > 1.0: decay_rate = 1.0
    
    # Transition matrix for Model-Based control
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Initialize Q-values
    q_stage1_mf = np.zeros(2)      # Model-Free values for Stage 1
    q_stage2_mf = np.zeros((2, 2)) # Model-Free values for Stage 2 (2 states, 2 actions)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_choice_1 = -1

    for trial in range(n_trials):
        # Handle missing data
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])

        # --- Stage 1 Policy ---
        # Model-Based Value Calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Value (Weighted Average)
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Logits and Stickiness
        logits_1 = beta * q_net
        if last_choice_1 != -1:
            logits_1[last_choice_1] += stick
            
        # Softmax Probability
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        logits_2 = beta * q_stage2_mf[s2]
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Value Updates ---
        # Prediction Errors
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[s2, a2]
        
        # Update Chosen Actions
        q_stage1_mf[a1] += lr * delta_stage1
        q_stage2_mf[s2, a2] += lr * delta_stage2
        
        # Decay Unchosen Actions
        # Stage 1 unchosen
        unchosen_a1 = 1 - a1
        q_stage1_mf[unchosen_a1] *= (1.0 - decay_rate)
        
        # Stage 2 unchosen (in the visited state)
        unchosen_a2 = 1 - a2
        q_stage2_mf[s2, unchosen_a2] *= (1.0 - decay_rate)
        
        last_choice_1 = a1

    mask = (action_1 != -1)
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Outcome-Dependent Arbitration (Dynamic w).
    
    Hypothesis: The balance between Goal-Directed (MB) and Habitual (MF) control 
    is dynamic and sensitive to recent outcomes. High OCI participants may exhibit 
    exaggerated shifts in strategy following rewards vs. omissions (e.g., retreating 
    to habits/MF after failure, or vice versa).
    
    Parameters:
    - lr: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w_base: [0, 1] Baseline mixing weight.
    - w_shift_oci: [0, 5] Magnitude of w shift based on previous reward, scaled by OCI.
      w_trial = w_base + w_shift_oci * oci * (1 if prev_reward else -1).
    - stick: [0, 5] Choice stickiness.
    
    Args:
        action_1, state, action_2, reward, oci, model_parameters
        
    Returns:
        float: Negative log-likelihood.
    """
    lr, beta, w_base, w_shift_oci, stick = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_choice_1 = -1
    last_reward = 1 # Initialize with optimism/neutrality

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])

        # Calculate Dynamic w based on previous outcome and OCI
        # If last trial rewarded: shift w up (towards MB? or just shift).
        # If last trial unrewarded: shift w down.
        direction = 1.0 if last_reward == 1 else -1.0
        w_dynamic = w_base + (w_shift_oci * oci_score * direction)
        
        # Clip w to [0, 1]
        if w_dynamic > 1.0: w_dynamic = 1.0
        if w_dynamic < 0.0: w_dynamic = 0.0

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w_dynamic * q_stage1_mb + (1 - w_dynamic) * q_stage1_mf
        
        logits_1 = beta * q_net
        if last_choice_1 != -1:
            logits_1[last_choice_1] += stick
            
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        logits_2 = beta * q_stage2_mf[s2]
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Value Updates ---
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[s2, a2]
        
        q_stage1_mf[a1] += lr * delta_stage1
        q_stage2_mf[s2, a2] += lr * delta_stage2
        
        last_choice_1 = a1
        last_reward = r

    mask = (action_1 != -1)
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Differential Learning Rates (Stage 1 vs Stage 2).
    
    Hypothesis: OCI affects the learning mechanism for immediate proximal outcomes (Stage 2)
    differently than for distal outcomes (Stage 1). High OCI might be associated with 
    hyper-learning or over-sensitivity to the immediate alien feedback (Stage 2) relative 
    to the spaceship choice (Stage 1).
    
    Parameters:
    - lr_base: [0, 1] Base learning rate (applied to Stage 1).
    - lr_2_oci_mult: [0, 5] Multiplier for Stage 2 learning rate based on OCI.
      lr_stage2 = lr_base * (1 + lr_2_oci_mult * oci), clipped to 1.0.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight.
    - stick: [0, 5] Choice stickiness.
    
    Args:
        action_1, state, action_2, reward, oci, model_parameters
        
    Returns:
        float: Negative log-likelihood.
    """
    lr_base, lr_2_oci_mult, beta, w, stick = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Define separate learning rates
    lr_1 = lr_base
    lr_2 = lr_base * (1.0 + lr_2_oci_mult * oci_score)
    if lr_2 > 1.0: lr_2 = 1.0

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_choice_1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net
        if last_choice_1 != -1:
            logits_1[last_choice_1] += stick
            
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        logits_2 = beta * q_stage2_mf[s2]
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Value Updates ---
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[s2, a2]
        
        # Use different learning rates for the two stages
        q_stage1_mf[a1] += lr_1 * delta_stage1
        q_stage2_mf[s2, a2] += lr_2 * delta_stage2
        
        last_choice_1 = a1

    mask = (action_1 != -1)
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss
```