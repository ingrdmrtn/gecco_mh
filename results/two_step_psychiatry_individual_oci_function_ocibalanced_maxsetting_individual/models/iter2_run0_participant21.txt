Here are the 3 new cognitive models based on the participant's data and OCI score.

### Cognitive Model 1: OCI-Modulated Eligibility Trace with Perseveration
This model hypothesizes that the OCI score predicts the strength of "eligibility traces" ($\lambda$). A higher eligibility trace means the outcome at the second stage (reward) has a stronger direct reinforcement effect on the first-stage choice, facilitating the formation of habitual action chains. This models the "compulsive" aspect as a tendency to form stronger sequential habits. It also includes a fixed perseveration (stickiness) parameter to account for the participant's strong tendency to repeat choices.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: Eligibility Trace (Lambda) modulated by OCI.
    
    Hypothesis: OCI score predicts the strength of the eligibility trace (lambda),
    determining how strongly Stage 2 outcomes reinforce Stage 1 choices (habit chaining).
    Includes fixed choice perseveration.

    Parameters:
    lr: [0, 1] Learning rate for value updates.
    beta: [0, 10] Inverse temperature (softness of choice).
    w: [0, 1] Weight for Model-Based values (0=MF, 1=MB).
    pers: [-2, 2] Fixed perseveration bonus for repeating Stage 1 choice.
    lambda_base: [0, 1] Baseline eligibility trace parameter.
    lambda_oci_slope: [-1, 1] Effect of OCI on lambda.
                      lambda = clip(lambda_base + lambda_oci_slope * OCI, 0, 1).
    """
    lr, beta, w, pers, lambda_base, lambda_oci_slope = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate OCI-modulated lambda
    lambda_val = lambda_base + lambda_oci_slope * current_oci
    lambda_val = np.clip(lambda_val, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Apply perseveration
        if prev_action_1 != -1:
            q_net[prev_action_1] += pers
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # --- Stage 2 Decision ---
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Updates ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Stage 1 TD error (transition)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        # Stage 2 TD error (outcome)
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += lr * delta_stage2
        
        # Eligibility trace update: Stage 2 error reinforces Stage 1 choice
        q_stage1_mf[a1] += lr * lambda_val * delta_stage2
        
        prev_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2: OCI-Modulated Stage 2 Learning Rate
This model proposes that OCI specifically affects the learning rate in the second stage (the bandit task), while the learning rate for the first stage (structure/transition learning) remains separate. High OCI might correlate with either hyper-sensitivity or rigidity in updating values based on immediate reward probability changes (Stage 2), distinct from learning the transition structure (Stage 1).

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: Stage 2 Learning Rate modulated by OCI.
    
    Hypothesis: OCI affects the learning rate specifically for the second stage 
    (reward probability estimation), distinct from Stage 1 transition learning.
    
    Parameters:
    lr_s1: [0, 1] Learning rate for Stage 1.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Weight for Model-Based values.
    pers: [-2, 2] Fixed perseveration bonus.
    lr_s2_base: [0, 1] Baseline Stage 2 learning rate.
    lr_s2_oci_slope: [-1, 1] Effect of OCI on Stage 2 learning rate.
                     lr_s2 = clip(lr_s2_base + lr_s2_oci_slope * OCI, 0, 1).
    """
    lr_s1, beta, w, pers, lr_s2_base, lr_s2_oci_slope = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate OCI-modulated Stage 2 learning rate
    lr_s2 = lr_s2_base + lr_s2_oci_slope * current_oci
    lr_s2 = np.clip(lr_s2, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if prev_action_1 != -1:
            q_net[prev_action_1] += pers
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # --- Stage 2 Decision ---
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Updates ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Stage 1 Update (using lr_s1)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr_s1 * delta_stage1
        
        # Stage 2 Update (using OCI-modulated lr_s2)
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += lr_s2 * delta_stage2
        
        prev_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3: OCI-Modulated Positive Learning Rate
This model hypothesizes that OCI modulates sensitivity to *positive* prediction errors (rewards) specifically. While a previous model tested negative modulation (punishment sensitivity), this model tests if higher OCI relates to over-learning from success (reinforcing rituals) or under-learning from it. It separates the learning rate for positive errors ($LR_{pos}$) and negative errors ($LR_{neg}$), with OCI modulating only $LR_{pos}$.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: Positive Learning Rate modulated by OCI.
    
    Hypothesis: OCI modulates the learning rate specifically for positive prediction errors
    (rewards), while learning from negative errors (omissions) is fixed.
    
    Parameters:
    lr_neg: [0, 1] Learning rate for negative prediction errors.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Weight for Model-Based values.
    pers: [-2, 2] Fixed perseveration bonus.
    lr_pos_base: [0, 1] Baseline positive learning rate.
    lr_pos_oci_slope: [-1, 1] Effect of OCI on positive learning rate.
                      lr_pos = clip(lr_pos_base + lr_pos_oci_slope * OCI, 0, 1).
    """
    lr_neg, beta, w, pers, lr_pos_base, lr_pos_oci_slope = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate OCI-modulated positive learning rate
    lr_pos = lr_pos_base + lr_pos_oci_slope * current_oci
    lr_pos = np.clip(lr_pos, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if prev_action_1 != -1:
            q_net[prev_action_1] += pers
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # --- Stage 2 Decision ---
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Updates ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        lr_s1 = lr_pos if delta_stage1 > 0 else lr_neg
        q_stage1_mf[a1] += lr_s1 * delta_stage1
        
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        lr_s2 = lr_pos if delta_stage2 > 0 else lr_neg
        q_stage2_mf[state_idx, a2] += lr_s2 * delta_stage2
        
        prev_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```