Here are three new cognitive models that explore different mechanisms for how OCI scores might influence decision-making in this two-step task.

### Model 1: Hybrid Learner with OCI-modulated Model-Based Weighting
This model tests the hypothesis that higher OCI scores correlate with a stronger reliance on habitual (model-free) control over goal-directed (model-based) control. It implements a standard hybrid reinforcement learning model where the mixing parameter `w` (weight of model-based values) is inversely modulated by the OCI score.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free Learner where OCI modulates the balance (w).
    
    Hypothesis: Higher OCI scores lead to reduced model-based control (lower w).
    
    Parameters:
    lr : [0, 1] Learning rate for value updates.
    beta : [0, 10] Inverse temperature for softmax.
    w_base : [0, 1] Base weight for model-based values.
    w_oci_mod : [0, 1] Strength of OCI reduction on model-based weight.
                 Effective w = w_base * (1 - w_oci_mod * oci)
    """
    lr, beta, w_base, w_oci_mod = model_parameters
    n_trials = len(action_1)
    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    current_oci = oci[0]

    # Calculate effective weight w. 
    # If OCI is high, w decreases, shifting towards model-free.
    w = w_base * (1.0 - w_oci_mod * current_oci)
    # Clamp w to [0, 1] to ensure validity
    w = np.clip(w, 0.0, 1.0)
    
    # Transition matrix (fixed for this task structure)
    # A->X (0->0) is 0.7, A->Y (0->1) is 0.3, etc.
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_mf_stage1 = np.zeros(2)
    q_mf_stage2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        
        # Model-Based Value: V(s') = max(Q_MF(s', a'))
        v_stage2 = np.max(q_mf_stage2, axis=1)
        q_mb_stage1 = transition_matrix @ v_stage2
        
        # Hybrid Value
        q_net_stage1 = w * q_mb_stage1 + (1 - w) * q_mf_stage1
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Decision ---
        state_idx = state[trial]
        
        exp_q2 = np.exp(beta * q_mf_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning ---
        
        # Stage 2 Update (TD)
        # RPE at stage 2
        delta_2 = reward[trial] - q_mf_stage2[state_idx, action_2[trial]]
        q_mf_stage2[state_idx, action_2[trial]] += lr * delta_2
        
        # Stage 1 Update (TD)
        # We use the value of the chosen stage 2 state as the target for stage 1 MF
        # Note: In pure TD(0), target is V(s'). Here we approximate with Q(s', a')
        delta_1 = q_mf_stage2[state_idx, action_2[trial]] - q_mf_stage1[action_1[trial]]
        q_mf_stage1[action_1[trial]] += lr * delta_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Asymmetric Learning Rates based on OCI
This model investigates if OCI scores relate to an asymmetry in how positive versus negative outcomes are processed. Specifically, it tests if higher OCI is associated with a "safety bias" where negative outcomes (receiving 0 coins) drive learning more strongly than positive outcomes (receiving 1 coin), potentially reflecting an avoidance of failure.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free Learner with OCI-modulated Asymmetric Learning Rates.
    
    Hypothesis: OCI modulates the ratio between learning from positive (lr_pos)
    and negative (lr_neg) prediction errors.
    
    Parameters:
    lr_base : [0, 1] Base learning rate.
    beta : [0, 10] Inverse temperature.
    bias_oci : [0, 1] Parameter controlling how much OCI skews learning towards negative outcomes.
                lr_pos = lr_base * (1 - bias_oci * oci)
                lr_neg = lr_base * (1 + bias_oci * oci) (clipped to 1)
    """
    lr_base, beta, bias_oci = model_parameters
    n_trials = len(action_1)
    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    current_oci = oci[0]

    # Calculate asymmetric learning rates
    # If bias_oci is high and OCI is high:
    # lr_pos decreases (less sensitivity to reward)
    # lr_neg increases (more sensitivity to omission/punishment)
    lr_pos = np.clip(lr_base * (1.0 - bias_oci * current_oci), 0.0, 1.0)
    lr_neg = np.clip(lr_base * (1.0 + bias_oci * current_oci), 0.0, 1.0)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Choice ---
        state_idx = state[trial]
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning Stage 2 ---
        rpe_2 = reward[trial] - q_stage2[state_idx, action_2[trial]]
        if rpe_2 >= 0:
            q_stage2[state_idx, action_2[trial]] += lr_pos * rpe_2
        else:
            q_stage2[state_idx, action_2[trial]] += lr_neg * rpe_2
            
        # --- Learning Stage 1 ---
        # Using simple TD(0) update
        rpe_1 = q_stage2[state_idx, action_2[trial]] - q_stage1[action_1[trial]]
        if rpe_1 >= 0:
            q_stage1[action_1[trial]] += lr_pos * rpe_1
        else:
            q_stage1[action_1[trial]] += lr_neg * rpe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Driven Exploration Suppression
This model posits that OCI relates to rigidity or a lack of exploration. Instead of modifying learning rates or the model-based weight, here OCI directly modifies the softmax temperature (beta). Higher OCI leads to a higher beta (lower temperature), resulting in more deterministic, "exploitative" choices and less random exploration.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free Learner where OCI suppresses exploration (increases Beta).
    
    Hypothesis: High OCI participants are more rigid/deterministic.
    
    Parameters:
    lr : [0, 1] Learning rate.
    beta_base : [0, 10] Base inverse temperature.
    beta_oci_boost : [0, 5] Additional beta added per unit of OCI.
                     Effective Beta = beta_base + beta_oci_boost * oci
    eligibility_trace : [0, 1] Lambda parameter for TD(lambda) to link stages better.
    """
    lr, beta_base, beta_oci_boost, eligibility_trace = model_parameters
    n_trials = len(action_1)
    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    current_oci = oci[0]

    # Effective Beta: Higher OCI -> Higher Beta -> More deterministic (less exploration)
    beta = beta_base + beta_oci_boost * current_oci
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Choice ---
        state_idx = state[trial]
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning (TD with eligibility trace) ---
        
        # 1. Update Stage 1 based on Stage 2 value (Temporal Difference)
        # RPE_1 = V(s') - Q(s,a)
        # We use Q(s', a') as proxy for V(s')
        target_stage1 = q_stage2[state_idx, action_2[trial]]
        rpe_1 = target_stage1 - q_stage1[action_1[trial]]
        q_stage1[action_1[trial]] += lr * rpe_1
        
        # 2. Update Stage 2 based on Reward
        rpe_2 = reward[trial] - q_stage2[state_idx, action_2[trial]]
        q_stage2[state_idx, action_2[trial]] += lr * rpe_2
        
        # 3. Eligibility Trace: Update Stage 1 again based on Stage 2 outcome
        # This links the final reward back to the first choice
        q_stage1[action_1[trial]] += lr * eligibility_trace * rpe_2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```