Here are three cognitive models formulated as Python functions, designed to explain the participant's behavior in the two-step task, specifically incorporating their OCI score.

### Model 1: Differential Learning Rate Model
This model hypothesizes that the participant's OCI score differentially affects how they learn from immediate outcomes (Stage 2) versus predictive cues (Stage 1). High OCI may correlate with a hyper-sensitivity or specific learning rate for the direct stimulus-reward contingency (aliens), distinct from the learning rate used for state prediction updates.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Dual Learning Rate Model modulated by OCI.
    
    Hypothesis: OCI scores modulate the learning rate specifically for Stage 2 
    (direct outcome), reflecting altered sensitivity to immediate feedback 
    compared to the planning stage (Stage 1).
    
    Parameters:
    - lr_1: [0,1] Learning rate for Stage 1 (Model-Free).
    - lr_2_base: [0,1] Base learning rate for Stage 2.
    - lr_2_oci_slope: [-1,1] Modulation of Stage 2 learning rate by OCI.
    - beta: [0,10] Inverse temperature parameter.
    - w: [0,1] Weighting between Model-Based (1) and Model-Free (0).
    """
    lr_1, lr_2_base, lr_2_oci_slope, beta, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate OCI-modulated learning rate for stage 2
    # We clip to ensure the learning rate remains valid [0, 1]
    lr_2 = lr_2_base + lr_2_oci_slope * oci_score
    lr_2 = np.clip(lr_2, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # Skip invalid trials (missing data)
        if action_1[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # --- Policy for Choice 1 ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax policy 1
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Policy for Choice 2 ---
        # Softmax policy 2 based on MF values for the current state
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Stage 1 MF update using lr_1
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr_1 * delta_stage1
        
        # Stage 2 MF update using OCI-modulated lr_2
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += lr_2 * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Decay Model
This model tests the hypothesis that OCI relates to the "forgetting" or decay of value for unchosen options. High OCI might be associated with doubt or compulsive checking, represented here as a faster (or slower) decay of information about actions not recently taken.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Value Decay Model modulated by OCI.
    
    Hypothesis: OCI score influences the rate at which the values of unchosen 
    actions decay (forgetting). High OCI may lead to faster decay (uncertainty) 
    or slower decay (rigidity).
    
    Parameters:
    - learning_rate: [0,1] Learning rate for chosen actions.
    - beta: [0,10] Inverse temperature.
    - w: [0,1] Model-based weight.
    - decay_base: [0,1] Base decay rate for unchosen options.
    - decay_oci_slope: [-1,1] Modulation of decay rate by OCI.
    """
    learning_rate, beta, w, decay_base, decay_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate OCI-modulated decay rate
    decay_rate = decay_base + decay_oci_slope * oci_score
    decay_rate = np.clip(decay_rate, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        if action_1[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # --- Policy for Choice 1 ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Policy for Choice 2 ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Standard updates for chosen actions
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # --- Decay for Unchosen Actions ---
        # Decay unchosen Stage 1 value
        q_stage1_mf[1 - a1] *= (1.0 - decay_rate)
        
        # Decay unchosen Stage 2 value (in the current state)
        q_stage2_mf[state_idx, 1 - a2] *= (1.0 - decay_rate)
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Win-Stay Lose-Shift
This model proposes that high OCI scores might correlate with a reliance on a simple heuristic strategy—Win-Stay Lose-Shift (WSLS)—over or alongside complex value estimation. The strength of this heuristic bias is modulated by the OCI score.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model with OCI-modulated Win-Stay/Lose-Shift (WSLS) heuristic.
    
    Hypothesis: OCI score correlates with the strength of a reactive WSLS 
    strategy (repeating rewarded choices, switching from unrewarded ones) 
    that biases the Stage 1 choice.
    
    Parameters:
    - learning_rate: [0,1] Learning rate for value updates.
    - beta: [0,10] Inverse temperature.
    - w: [0,1] Model-based weight.
    - wsls_base: [0,5] Base strength of the WSLS bias.
    - wsls_oci_slope: [-5,5] Modulation of WSLS strength by OCI.
    """
    learning_rate, beta, w, wsls_base, wsls_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate OCI-modulated WSLS strength
    wsls_strength = wsls_base + wsls_oci_slope * oci_score
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_a1 = -1
    prev_reward = 0.0

    for trial in range(n_trials):
        if action_1[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            # Reset history on invalid trial to avoid carrying over noise
            prev_a1 = -1 
            continue

        # --- Policy for Choice 1 ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Apply WSLS bias to q_net
        if prev_a1 != -1:
            # If previous reward was 1 (Win), add bonus to previous action (Stay)
            # If previous reward was 0 (Lose), subtract bonus from previous action (Shift)
            # Mapping reward 0/1 to sign -1/+1
            sign = 2 * prev_reward - 1 
            q_net[int(prev_a1)] += wsls_strength * sign
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Policy for Choice 2 ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Update history
        prev_a1 = a1
        prev_reward = r
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```