Here are 3 new cognitive models based on the participant's data and OCI score.

### Model 1: OCI-Modulated Model-Based Weight (Mixing Parameter)
This model tests the hypothesis that higher OCI scores correlate with a rigid reliance on Model-Based (habitual/rule-based) planning or, conversely, a deficit in it. Instead of a static mixing weight `w`, this model makes `w` a function of the OCI score. Given the participant's medium-high OCI (0.6) and the task nature, this model explores if symptom severity shifts the balance between goal-directed (MB) and habitual (MF) control.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model where the mixing weight w is a logistic function of OCI.
    
    Hypothesis: The balance between Model-Based and Model-Free control is determined
    by the OCI score. 
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    w_slope: [-5, 5] (Slope of OCI effect on w)
    w_intercept: [-5, 5] (Baseline log-odds for w)
    """
    learning_rate, beta, w_slope, w_intercept = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate mixing weight w using a sigmoid function to keep it in [0, 1]
    # w represents the weight of Model-Based values
    w_logit = w_intercept + w_slope * current_oci
    w = 1 / (1 + np.exp(-w_logit))

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
             p_choice_1[trial] = 0.5
             p_choice_2[trial] = 0.5
             continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combined value: w * MB + (1-w) * MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        act1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[act1]

        # --- Stage 2 Policy ---
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        act2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[act2]

        # --- Updates ---
        # Update Stage 2 Q-values (Model-Free)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, act2]
        q_stage2_mf[state_idx, act2] += learning_rate * delta_stage2
        
        # Update Stage 1 Q-values (Model-Free) using SARSA-like error from Stage 2 value
        delta_stage1 = q_stage2_mf[state_idx, act2] - q_stage1_mf[act1]
        q_stage1_mf[act1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Learning Rate Asymmetry (Positive vs Negative)
This model posits that OCI symptoms might affect how participants learn from positive versus negative prediction errors. Individuals with high compulsivity might be hyper-sensitive to errors (negative feedback) or rigid in the face of rewards. Here, the OCI score shifts the learning rate specifically for negative prediction errors, creating an asymmetry.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free model with OCI-modulated asymmetric learning rates.
    
    Hypothesis: OCI score modulates sensitivity to negative prediction errors (punishment/omission),
    causing a divergence between learning from positive outcomes (alpha_pos) and negative outcomes (alpha_neg).
    
    Bounds:
    lr_base: [0, 1] (Base learning rate for positive errors)
    beta: [0, 10]
    neg_lr_scale: [0, 5] (Multiplier for negative learning rate based on OCI)
    """
    lr_base, beta, neg_lr_scale = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate specific learning rates
    # Alpha positive is the base rate
    alpha_pos = lr_base
    # Alpha negative is scaled by OCI. Higher OCI -> Higher sensitivity to negative outcomes?
    # We clip to [0, 1] to ensure stability.
    alpha_neg = np.clip(lr_base * (1 + neg_lr_scale * current_oci), 0.0, 1.0)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
             p_choice_1[trial] = 0.5
             p_choice_2[trial] = 0.5
             continue
        
        # --- Stage 1 Policy (Pure Model-Free for simplicity of mechanism isolation) ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        act1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[act1]

        # --- Stage 2 Policy ---
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        act2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[act2]

        # --- Updates ---
        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2[state_idx, act2]
        lr_2 = alpha_pos if delta_stage2 >= 0 else alpha_neg
        q_stage2[state_idx, act2] += lr_2 * delta_stage2
        
        # Stage 1 Update
        delta_stage1 = q_stage2[state_idx, act2] - q_stage1[act1]
        lr_1 = alpha_pos if delta_stage1 >= 0 else alpha_neg
        q_stage1[act1] += lr_1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Scaled Eligibility Trace (Lambda)
This model introduces an eligibility trace parameter ($\lambda$) that determines how much the Stage 1 choice is reinforced by the Stage 2 outcome directly. Instead of a fixed lambda, this model scales lambda by the OCI score. This tests if higher compulsivity leads to "long-range" credit assignment (linking the spaceship choice directly to the coin) or a more fragmented, step-by-step update.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free TD(lambda) model where the eligibility trace parameter lambda
    is scaled by OCI.
    
    Hypothesis: OCI modulates the extent to which the Stage 2 outcome directly 
    reinforces the Stage 1 choice (eligibility trace).
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    lambda_base: [0, 1]
    lambda_mod: [-1, 1] (Modulation factor of OCI on lambda)
    """
    learning_rate, beta, lambda_base, lambda_mod = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    # Calculate lambda. If lambda_mod is positive, higher OCI = higher lambda.
    # Clip to ensure it stays valid [0, 1].
    eligibility_lambda = np.clip(lambda_base + (lambda_mod * current_oci), 0.0, 1.0)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
             p_choice_1[trial] = 0.5
             p_choice_2[trial] = 0.5
             continue

        # --- Stage 1 Policy ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        act1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[act1]

        # --- Stage 2 Policy ---
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        act2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[act2]

        # --- Updates ---
        # Stage 2 Prediction Error
        delta_stage2 = reward[trial] - q_stage2[state_idx, act2]
        
        # Stage 1 Prediction Error (Standard TD(0) part)
        delta_stage1 = q_stage2[state_idx, act2] - q_stage1[act1]
        
        # Update Stage 2
        q_stage2[state_idx, act2] += learning_rate * delta_stage2
        
        # Update Stage 1:
        # 1. Direct update from Stage 1 error
        # 2. Eligibility trace update from Stage 2 error (scaled by lambda)
        q_stage1[act1] += learning_rate * (delta_stage1 + eligibility_lambda * delta_stage2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```