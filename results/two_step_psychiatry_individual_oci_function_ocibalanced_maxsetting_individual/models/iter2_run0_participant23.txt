Here are the three cognitive models based on the participant's data and OCI score.

### Cognitive Model 1: Valence-Dependent Learning Rate
This model hypothesizes that the participant's OCI score modulates their sensitivity to negative prediction errors (punishments or omission of reward). Participants with higher OCI scores may update their values differently when outcomes are worse than expected, reflecting an altered error-monitoring mechanism often associated with obsessive-compulsive traits.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Valence-Dependent Learning Rate Model.
    
    Hypothesis: OCI modulates the learning rate specifically for negative prediction errors 
    (outcomes worse than expected). High OCI may lead to different updating dynamics for 
    losses compared to gains.
    
    Parameters:
    - lr_pos: [0, 1] Learning rate for positive prediction errors (better than expected).
    - lr_neg_base: [0, 10] Base parameter for the negative learning rate sigmoid.
    - lr_neg_oci: [-10, 10] OCI slope for the negative learning rate sigmoid.
    - beta: [0, 10] Inverse temperature (exploration/exploitation balance).
    - w: [0, 1] Weight mixing Model-Based (1) and Model-Free (0) systems.
    - stickiness: [0, 5] Perseveration bonus for repeating the previous Stage 1 choice.
    """
    lr_pos, lr_neg_base, lr_neg_oci, beta, w, stickiness = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate OCI-modulated negative learning rate via sigmoid
    lr_neg = 1.0 / (1.0 + np.exp(-(lr_neg_base + lr_neg_oci * oci_score)))
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    
    prev_action_1 = -1
    
    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Stickiness
        stick_bonus = np.zeros(2)
        if prev_action_1 != -1:
            stick_bonus[prev_action_1] = stickiness
            
        logits_1 = beta * q_net + stick_bonus
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        logits_2 = beta * q_stage2_mf[s_idx]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Value Updates with Valence-Dependent Learning Rate ---
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        alpha_1 = lr_pos if delta_stage1 > 0 else lr_neg
        q_stage1_mf[a1] += alpha_1 * delta_stage1
        
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        alpha_2 = lr_pos if delta_stage2 > 0 else lr_neg
        q_stage2_mf[s_idx, a2] += alpha_2 * delta_stage2
        
        prev_action_1 = a1

    eps = 1e-10
    valid_trials = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```

### Cognitive Model 2: OCI-Modulated Habit Trace Decay
This model proposes that high OCI scores are associated with "stickier" habits, modeled as a slower decay of a choice trace. Unlike simple stickiness (repeating the last action), a habit trace accumulates over time and persists, explaining the long blocks of repeated choices observed in the participant data.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Habit Trace Decay Model.
    
    Hypothesis: OCI modulates the decay rate of a 'habit trace'. Higher OCI scores may be 
    associated with slower decay (stronger habits), causing perseveration over longer 
    blocks of trials rather than just immediate repetition.
    
    Parameters:
    - learning_rate: [0, 1] Value learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Model-based weight.
    - habit_scale: [0, 5] Strength of the habit trace influence on choice.
    - decay_base: [0, 10] Base parameter for the decay rate sigmoid.
    - decay_oci: [-10, 10] OCI slope for the decay rate sigmoid.
    """
    learning_rate, beta, w, habit_scale, decay_base, decay_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate decay rate: close to 1 means slow forgetting (strong habit), close to 0 means fast forgetting
    decay_rate = 1.0 / (1.0 + np.exp(-(decay_base + decay_oci * oci_score)))
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Habit trace for Stage 1 actions
    habit_trace = np.zeros(2)
    
    for trial in range(n_trials):
        if action_1[trial] == -1:
            # Optionally decay trace even on missed trials, but here we skip for simplicity
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Combine value with habit trace
        logits_1 = beta * q_net + habit_scale * habit_trace
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        logits_2 = beta * q_stage2_mf[s_idx]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        
        # Update Habit Trace
        habit_trace = habit_trace * decay_rate
        habit_trace[a1] += 1.0
        
        # Update Values
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    valid_trials = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```

### Cognitive Model 3: Stage-Specific Exploration
This model posits that OCI symptoms manifest differently during the planning phase (Stage 1) versus the outcome phase (Stage 2). Specifically, it allows the OCI score to modulate the randomness of decisions (inverse temperature `beta`) in the second stage, testing if high-OCI participants are more or less deterministic when selecting aliens.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Stage-Specific Exploration Model.
    
    Hypothesis: OCI modulates the inverse temperature (beta) specifically for Stage 2 
    decisions. This tests if OCI affects the precision of choices closer to the reward 
    differently than abstract planning choices.
    
    Parameters:
    - learning_rate: [0, 1] Value learning rate.
    - beta_1: [0, 10] Inverse temperature for Stage 1 (Spaceship choice).
    - w: [0, 1] Model-based weight.
    - stickiness: [0, 5] Perseveration for Stage 1.
    - beta_2_base: [0, 10] Base parameter for Stage 2 inverse temperature.
    - beta_2_oci: [-5, 5] OCI scaling factor for Stage 2 inverse temperature.
    """
    learning_rate, beta_1, w, stickiness, beta_2_base, beta_2_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate Stage 2 beta, ensuring it stays non-negative
    beta_2 = np.maximum(0.0, beta_2_base + beta_2_oci * oci_score)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1
    
    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Stage 1 Policy (uses beta_1) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        stick_bonus = np.zeros(2)
        if prev_action_1 != -1:
            stick_bonus[prev_action_1] = stickiness
            
        logits_1 = beta_1 * q_net + stick_bonus
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy (uses OCI-modulated beta_2) ---
        logits_2 = beta_2 * q_stage2_mf[s_idx]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        prev_action_1 = a1

    eps = 1e-10
    valid_trials = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```