Here are three new cognitive models that explore different mechanisms by which OCI scores might influence decision-making in this task, focusing on the balance between model-based/model-free control and learning rates.

### Model 1: Hybrid Model with OCI-modulated Model-Based Weight
This model posits that individuals with different OCI scores rely differently on Model-Based (planning) vs. Model-Free (habit) strategies. Specifically, higher OCI might lead to a more rigid, habit-like strategy (lower `w`) or potentially a more deliberative one. Here, the weighting parameter `w` is a linear function of the OCI score.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL model where the balance between Model-Based (MB) and Model-Free (MF) 
    control is modulated by the OCI score.
    
    The weight `w` determines the mix: Q_net = w * Q_MB + (1-w) * Q_MF.
    w is calculated as: w = w_base + (w_oci_slope * OCI).
    We clip w to [0, 1] to ensure valid probabilities.
    
    Parameters:
    learning_rate: [0, 1] Learning rate for MF updates.
    beta: [0, 10] Inverse temperature for softmax.
    w_base: [0, 1] Baseline model-based weight.
    w_oci_slope: [-1, 1] How OCI score changes the weight w.
    """
    learning_rate, beta, w_base, w_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate w based on OCI and clip to valid range [0, 1]
    w = w_base + (w_oci_slope * oci_score)
    w = np.clip(w, 0.0, 1.0)
  
    # Transition matrix (fixed for this task structure usually)
    # A -> X (0.7), A -> Y (0.3); U -> X (0.3), U -> Y (0.7)
    # Rows: Action (0, 1), Cols: State (0, 1)
    # Note: State 0 is usually Planet X, State 1 is Planet Y.
    # But usually transition matrix is defined as T[action, next_state].
    # Let's assume standard task structure: 
    # Action 0 -> State 0 (0.7), State 1 (0.3)
    # Action 1 -> State 0 (0.3), State 1 (0.7)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2) + 0.5
    q_stage2_mf = np.zeros((2, 2)) + 0.5

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        if a1 == -1: continue # Skip missing data
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])

        # --- Stage 1 Policy ---
        # Model-Based Value Calculation
        # Max value of next stage states
        max_q_stage2 = np.max(q_stage2_mf, axis=1) 
        # Bellman equation using known transition matrix
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        # Standard Model-Free choice at second stage
        exp_q2 = np.exp(beta * q_stage2_mf[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # --- Learning ---
        # TD Error Stage 2
        delta_stage2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += learning_rate * delta_stage2
        
        # TD Error Stage 1 (SARSA-like update using stage 2 value)
        # Note: In standard Daw 2011, Q_MF(s1, a1) is updated with Q_MF(s2, a2) or r?
        # Usually standard TD(0):
        delta_stage1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Eligibility trace (lambda) is often used, but here we stick to TD(0) 
        # implicit in the template structure provided, or just update stage 1 with stage 2 value.

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Learning Rate Asymmetry (Positive vs Negative)
This model investigates if OCI scores relate to an asymmetry in how people learn from positive versus negative prediction errors. For example, high OCI might be associated with over-learning from failure (negative RPEs) or under-learning from success.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free RL where learning rates for positive and negative prediction errors 
    are modulated differently by the OCI score.
    
    lr_pos = lr_base
    lr_neg = lr_base + (oci_sensitivity * OCI)
    
    This tests if OCI is linked to a specific sensitivity to negative outcomes 
    (or lack of reward), creating an imbalance in updating.
    
    Parameters:
    lr_base: [0, 1] Base learning rate for positive errors.
    beta: [0, 10] Inverse temperature.
    oci_sensitivity: [-1, 1] How much OCI adds to the negative learning rate.
    w_mb: [0, 1] Fixed weight for Model-Based control (to account for basic planning).
    """
    lr_base, beta, oci_sensitivity, w_mb = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate separate learning rates
    lr_pos = lr_base
    lr_neg = lr_base + (oci_sensitivity * oci_score)
    
    # Clip to valid range
    lr_pos = np.clip(lr_pos, 0.0, 1.0)
    lr_neg = np.clip(lr_neg, 0.0, 1.0)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2) + 0.5
    q_stage2_mf = np.zeros((2, 2)) + 0.5

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        if a1 == -1: continue
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])

        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Q value
        q_net = w_mb * q_stage1_mb + (1 - w_mb) * q_stage1_mf[a1] # Note: vector addition
        
        # Re-calculating full vector for softmax
        q_net_vec = w_mb * q_stage1_mb + (1 - w_mb) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_vec)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # --- Updates ---
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[s, a2]
        eff_lr2 = lr_pos if delta_stage2 > 0 else lr_neg
        q_stage2_mf[s, a2] += eff_lr2 * delta_stage2
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        eff_lr1 = lr_pos if delta_stage1 > 0 else lr_neg
        q_stage1_mf[a1] += eff_lr1 * delta_stage1
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Decay Rate (Forgetting)
This model tests the hypothesis that OCI relates to memory persistence or "forgetting" of value estimates. High OCI might involve obsessive retention of value (low decay) or potentially volatile updating. This model introduces a decay parameter that pulls Q-values back to 0.5 (uncertainty) on unchosen options, scaled by OCI.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free RL with OCI-modulated value decay (forgetting).
    
    Unchosen options decay towards 0.5. The rate of decay is determined by OCI.
    High OCI might mean 'stuck' values (low decay) or rapid forgetting.
    
    decay_rate = decay_base + (decay_oci_slope * OCI)
    
    Parameters:
    learning_rate: [0, 1]
    beta: [0, 10]
    decay_base: [0, 1] Base decay rate for unchosen options.
    decay_oci_slope: [-1, 1] How OCI modifies decay.
    stickiness: [0, 5] Simple choice stickiness (unmodulated).
    """
    learning_rate, beta, decay_base, decay_oci_slope, stickiness = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate decay rate
    decay = decay_base + (decay_oci_slope * oci_score)
    decay = np.clip(decay, 0.0, 1.0)
  
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2) + 0.5
    q_stage2_mf = np.zeros((2, 2)) + 0.5
    
    last_action_1 = -1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        if a1 == -1: 
            last_action_1 = -1
            continue
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])

        # --- Stage 1 Choice ---
        q_net_1 = q_stage1_mf.copy()
        if last_action_1 != -1:
            q_net_1[last_action_1] += stickiness # Simple constant stickiness

        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # --- Updates ---
        
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += learning_rate * delta_stage2
        
        # Decay unchosen stage 2 option in the current state
        unchosen_a2 = 1 - a2
        q_stage2_mf[s, unchosen_a2] = (1 - decay) * q_stage2_mf[s, unchosen_a2] + (decay * 0.5)
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Decay unchosen stage 1 option
        unchosen_a1 = 1 - a1
        q_stage1_mf[unchosen_a1] = (1 - decay) * q_stage1_mf[unchosen_a1] + (decay * 0.5)
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```