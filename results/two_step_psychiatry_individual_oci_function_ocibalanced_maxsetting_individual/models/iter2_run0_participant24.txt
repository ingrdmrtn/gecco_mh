Here are three new cognitive models exploring different mechanisms for how OCI scores might influence decision-making in this two-step task, focusing on mixing model-based/model-free strategies and learning rate asymmetries.

### Model 1: OCI-Modulated Model-Based vs Model-Free Mixture
This model hypothesizes that individuals with higher OCI scores might rely more on habitual (Model-Free) control rather than goal-directed (Model-Based) planning, or vice versa. The mixing weight `w` is dynamic based on OCI.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based / Model-Free RL with OCI-modulated mixing weight.
    
    This model assumes behavior is a mixture of Model-Based (planning using transition matrix)
    and Model-Free (TD learning) strategies. The balance between these strategies (w)
    is determined by the OCI score.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate for value updates.
    beta: [0, 10] - Inverse temperature for softmax choice.
    w_base: [0, 1] - Baseline weight for Model-Based control (0=Pure MF, 1=Pure MB).
    oci_mb_shift: [-1, 1] - How OCI shifts the balance. Positive = High OCI is more Model-Based.
    """
    learning_rate, beta, w_base, oci_mb_shift = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate mixing weight w, bounded between 0 and 1
    # If oci_mb_shift is positive, high OCI increases MB reliance.
    w = w_base + (oci_mb_shift * oci_score)
    w = np.clip(w, 0.0, 1.0)

    # Fixed transition matrix for the task (A->X, U->Y commonly)
    # A=0, U=1; X=0, Y=1.
    # A (0) -> X (0) with 0.7
    # U (1) -> Y (1) with 0.7
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_mf = np.zeros((2, 2)) # Stage 1 MF values: [spaceship] (unused directly here but standard MF)
                            # Actually standard hybrid models split Q_MF(s1, a) and Q_MB(s1, a).
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # Stage 2 values: [planet, alien]

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Choice ---
        # Model-Based Value: T * max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1) # Max value for each planet
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        if a1 != -1:
            p_choice_1[trial] = probs_1[a1]
        else:
            p_choice_1[trial] = 1.0
            
        # --- Stage 2 Choice ---
        if s_idx != -1:
            exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
            probs_2 = exp_q2 / np.sum(exp_q2)
            
            if a2 != -1:
                p_choice_2[trial] = probs_2[a2]
                
                # --- Learning ---
                # Update Stage 2 MF values (Standard Q-learning)
                # Prediction error at stage 2
                delta2 = r - q_stage2_mf[s_idx, a2]
                q_stage2_mf[s_idx, a2] += learning_rate * delta2
                
                # Update Stage 1 MF values (TD-learning)
                # We use the value of the state we actually arrived at (q_stage2_mf[s_idx, a2] pre-update or max post? usually max or actual)
                # SARSA-style or Q-learning style for stage 1? 
                # Standard Daw 2011 uses Q(s2, a2) for the update target
                if a1 != -1:
                    delta1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
                    q_stage1_mf[a1] += learning_rate * delta1
                    
                    # Note: Eligibility traces often used to update stage 1 with stage 2 reward directly,
                    # but simple TD(0) updates stage 1 based on stage 2 value.
                    # A more complex model adds lambda. Here we keep it simple.
            else:
                 p_choice_2[trial] = 1.0
        else:
            p_choice_2[trial] = 1.0

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Driven Punishment Sensitivity
This model hypothesizes that OCI is related to an increased sensitivity to negative outcomes (missing rewards). Instead of a single learning rate, the model uses separate learning rates for positive (reward=1) and negative (reward=0) prediction errors, where the negative learning rate is modulated by OCI.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free RL with OCI-modulated Punishment Sensitivity.
    
    This model posits that OCI scores specifically affect how strongly participants
    learn from lack of reward (punishment/omission).
    
    Parameters:
    lr_pos: [0, 1] - Learning rate for positive prediction errors (Reward = 1).
    lr_neg_base: [0, 1] - Base learning rate for negative prediction errors (Reward = 0).
    beta: [0, 10] - Inverse temperature.
    oci_punishment_boost: [0, 2] - Multiplier for OCI to increase learning from negative outcomes.
    """
    lr_pos, lr_neg_base, beta, oci_punishment_boost = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate the effective negative learning rate
    # Higher OCI -> Higher lr_neg (bounded at 1.0)
    lr_neg = lr_neg_base + (oci_punishment_boost * oci_score)
    lr_neg = np.clip(lr_neg, 0.0, 1.0)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        
        # Stage 1 Choice
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        if a1 != -1:
            p_choice_1[trial] = probs_1[a1]
        else:
            p_choice_1[trial] = 1.0

        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Stage 2 Choice
        if s_idx != -1:
            exp_q2 = np.exp(beta * q_stage2[s_idx])
            probs_2 = exp_q2 / np.sum(exp_q2)
            
            if a2 != -1:
                p_choice_2[trial] = probs_2[a2]
                
                # --- Learning ---
                # Stage 2 Update
                delta2 = r - q_stage2[s_idx, a2]
                
                # Choose learning rate based on sign of prediction error or reward?
                # Usually based on delta sign, but here reward is binary 0/1.
                # If Reward=1, delta is positive (unless Q>1). If Reward=0, delta is negative.
                current_lr = lr_pos if delta2 >= 0 else lr_neg
                
                q_stage2[s_idx, a2] += current_lr * delta2
                
                # Stage 1 Update
                if a1 != -1:
                    # Update Stage 1 based on Stage 2 value (TD chain)
                    delta1 = q_stage2[s_idx, a2] - q_stage1[a1]
                    # We apply the same asymmetry logic to stage 1 updates
                    current_lr_s1 = lr_pos if delta1 >= 0 else lr_neg
                    q_stage1[a1] += current_lr_s1 * delta1
            else:
                p_choice_2[trial] = 1.0
        else:
            p_choice_2[trial] = 1.0

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Choice Consistency (Inverse Temperature)
This model suggests that OCI affects the "noise" or "determinism" in decision-making. High OCI might lead to more rigid, deterministic choices (higher beta), or conversely, more uncertainty.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free RL with OCI-modulated Inverse Temperature (Beta).
    
    This model tests if OCI scores correlate with the exploration/exploitation trade-off.
    Instead of modifying the learning process, OCI modifies the softmax temperature.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate.
    beta_base: [0, 10] - Baseline inverse temperature.
    oci_beta_slope: [-5, 5] - How OCI changes beta. Positive = High OCI is more deterministic.
    decay: [0, 1] - Forget rate for unchosen options (decay to 0).
    """
    learning_rate, beta_base, oci_beta_slope, decay = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective Beta
    # Bounded to be non-negative
    beta_eff = beta_base + (oci_beta_slope * oci_score)
    beta_eff = max(0.0, beta_eff)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        
        # Stage 1 Choice
        exp_q1 = np.exp(beta_eff * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        if a1 != -1:
            p_choice_1[trial] = probs_1[a1]
        else:
            p_choice_1[trial] = 1.0

        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Stage 2 Choice
        if s_idx != -1:
            exp_q2 = np.exp(beta_eff * q_stage2[s_idx])
            probs_2 = exp_q2 / np.sum(exp_q2)
            
            if a2 != -1:
                p_choice_2[trial] = probs_2[a2]
                
                # --- Learning ---
                # Stage 2 Update
                delta2 = r - q_stage2[s_idx, a2]
                q_stage2[s_idx, a2] += learning_rate * delta2
                
                # Decay unchosen stage 2 option
                unchosen_a2 = 1 - a2
                q_stage2[s_idx, unchosen_a2] *= (1 - decay)

                # Stage 1 Update
                if a1 != -1:
                    delta1 = q_stage2[s_idx, a2] - q_stage1[a1]
                    q_stage1[a1] += learning_rate * delta1
                    
                    # Decay unchosen stage 1 option
                    unchosen_a1 = 1 - a1
                    q_stage1[unchosen_a1] *= (1 - decay)
            else:
                p_choice_2[trial] = 1.0
        else:
            p_choice_2[trial] = 1.0

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```