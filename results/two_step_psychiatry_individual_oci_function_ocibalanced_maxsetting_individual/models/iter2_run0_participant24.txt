Here are three cognitive models represented as Python functions. They introduce mechanisms distinct from the previously tested combinations, focusing on eligibility traces, stage-specific learning rates, and reward-contingent perseveration.

### Model 1: Eligibility Trace (Lambda) Modulation
This model introduces an eligibility trace parameter ($\lambda$) to the Model-Free system. This parameter controls how much the credit for the reward at the second stage is assigned back to the choice made at the first stage. The hypothesis is that OCI symptoms modulate this "backward" learning connectionâ€”high OCI might lead to stronger habit formation (higher $\lambda$) linking outcomes to distal choices, or conversely, a disconnect between stages.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    MB/MF Hybrid with OCI-modulated Eligibility Traces (Lambda).
    
    Hypothesis: OCI affects the efficiency of credit assignment (lambda) in the 
    Model-Free system. High OCI may increase the 'habitual' link between 
    Stage 2 outcomes and Stage 1 choices.
    
    Parameters:
    - learning_rate: [0, 1] General learning rate for value updates.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weight for Model-Based system (1=MB, 0=MF).
    - lambda_base: [0, 1] Baseline eligibility trace decay.
    - lambda_oci_slope: [-1, 1] Effect of OCI on lambda.
    """
    learning_rate, beta, w, lambda_base, lambda_oci_slope = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]
    
    # Calculate OCI-modulated lambda and clip to [0, 1]
    lambda_val = lambda_base + (lambda_oci_slope * participant_oci)
    lambda_val = np.clip(lambda_val, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)      
    q_stage2_mf = np.zeros((2, 2)) 
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        # Skip missing data
        if action_1[trial] == -1 or action_2[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Stage 1 Prediction Error
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        
        # Stage 2 Prediction Error
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        # Update Stage 1 MF (with eligibility trace connecting stage 2 reward to stage 1)
        # Q(s1,a1) += alpha * delta1 + alpha * lambda * delta2
        q_stage1_mf[a1] += learning_rate * delta_stage1 + learning_rate * lambda_val * delta2
        
        # Update Stage 2 MF
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_lik_1 = np.sum(np.log(p_choice_1[p_choice_1 > 0] + eps))
    log_lik_2 = np.sum(np.log(p_choice_2[p_choice_2 > 0] + eps))
    
    return -(log_lik_1 + log_lik_2)
```

### Model 2: Stage-Specific Learning Rates
Standard models often assume a single learning rate for both the high-level planning stage (Stage 1) and the low-level bandit stage (Stage 2). This model separates them. The hypothesis is that OCI symptoms specifically impact the learning rate of the complex, first-stage decision-making process ($LR_{stage1}$), while the simpler sensory-motor association ($LR_{stage2}$) remains independent of OCI.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    MB/MF Hybrid with Separate Stage 1 and Stage 2 Learning Rates.
    
    Hypothesis: OCI specifically modulates the learning rate for Stage 1 (Spaceships),
    reflecting differences in updating high-level state-action values, while 
    Stage 2 (Aliens) learning remains independent.
    
    Parameters:
    - lr_stage2: [0, 1] Learning rate for Stage 2 (Aliens).
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weight for Model-Based system.
    - lr_stage1_base: [0, 1] Baseline learning rate for Stage 1.
    - lr_stage1_oci_slope: [-1, 1] Effect of OCI on Stage 1 learning rate.
    """
    lr_stage2, beta, w, lr_stage1_base, lr_stage1_oci_slope = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]
    
    # Calculate OCI-modulated Stage 1 learning rate
    lr_stage1 = lr_stage1_base + (lr_stage1_oci_slope * participant_oci)
    lr_stage1 = np.clip(lr_stage1, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)      
    q_stage2_mf = np.zeros((2, 2)) 
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        if action_1[trial] == -1 or action_2[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Update Stage 1 MF using Stage 1 specific learning rate
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr_stage1 * delta_stage1
        
        # Update Stage 2 MF using Stage 2 specific learning rate
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr_stage2 * delta_stage2

    eps = 1e-10
    log_lik_1 = np.sum(np.log(p_choice_1[p_choice_1 > 0] + eps))
    log_lik_2 = np.sum(np.log(p_choice_2[p_choice_2 > 0] + eps))
    
    return -(log_lik_1 + log_lik_2)
```

### Model 3: Reward-Dependent Perseveration (Win-Stay)
The previous best model explored *general* perseveration (stickiness). This model refines that by exploring *Reward-Dependent* perseveration (Win-Stay). It hypothesizes that OCI symptoms do not just cause random repetition, but specifically amplify the tendency to repeat a Stage 1 choice *only if* the previous trial resulted in a reward. This captures a potential "reinforcement stickiness" distinct from motor stickiness.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    MB/MF Hybrid with OCI-modulated Win-Stay Perseveration.
    
    Hypothesis: OCI modulates 'Win-Stay' behavior. Unlike general perseveration,
    this bias is only applied to the previous Stage 1 choice if the previous
    trial resulted in a reward, suggesting a compulsion driven by positive reinforcement.
    
    Parameters:
    - learning_rate: [0, 1] Speed of value updating.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weight for Model-Based system.
    - winstay_base: [0, 5] Baseline bonus added to previous choice if rewarded.
    - winstay_oci_slope: [-5, 5] Effect of OCI on the Win-Stay bonus.
    """
    learning_rate, beta, w, winstay_base, winstay_oci_slope = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]
    
    # Calculate Win-Stay bonus
    winstay_bonus = winstay_base + (winstay_oci_slope * participant_oci)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)      
    q_stage2_mf = np.zeros((2, 2)) 
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_action_1 = -1
    prev_reward = 0.0
    
    for trial in range(n_trials):
        if action_1[trial] == -1 or action_2[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Apply Win-Stay bonus: Only add if previous action exists AND previous trial was rewarded
        if prev_action_1 != -1 and prev_reward > 0:
            q_net_stage1[prev_action_1] += winstay_bonus
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        prev_action_1 = a1
        prev_reward = r

    eps = 1e-10
    log_lik_1 = np.sum(np.log(p_choice_1[p_choice_1 > 0] + eps))
    log_lik_2 = np.sum(np.log(p_choice_2[p_choice_2 > 0] + eps))
    
    return -(log_lik_1 + log_lik_2)
```