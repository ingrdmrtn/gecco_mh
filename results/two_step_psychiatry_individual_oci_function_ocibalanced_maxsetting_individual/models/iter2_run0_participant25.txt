Here are three cognitive models expressed as Python functions.

### Model 1: OCI-Modulated Eligibility Trace (Lambda)
This model hypothesizes that OCI symptoms influence the "eligibility trace" ($\lambda$) in the reinforcement learning process. A higher (or lower) $\lambda$ changes how much the final outcome (Stage 2 reward) directly reinforces the Stage 1 choice, bypassing the Stage 2 value estimation. This reflects a potential alteration in credit assignment or "chasing" outcomes associated with compulsive traits.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model with OCI-modulated Eligibility Trace (Lambda).
    
    Hypothesis: OCI affects the eligibility trace parameter (lambda), which controls 
    how much the Stage 2 prediction error propagates directly to the Stage 1 update.
    This reflects the degree to which the final outcome reinforces the initial choice.

    Parameters:
    - learning_rate: [0, 1] Standard learning rate.
    - beta: [0, 10] Inverse temperature for softmax.
    - w: [0, 1] Mixing weight (0=MF, 1=MB).
    - lam_base: [0, 1] Baseline eligibility trace.
    - lam_oci_slope: [-1, 1] Slope of OCI modulation on lambda.
      Effective lambda = clip(lam_base + lam_oci_slope * oci, 0, 1).
    """
    learning_rate, beta, w, lam_base, lam_oci_slope = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate effective lambda and clip to valid range [0, 1]
    lam = lam_base + (lam_oci_slope * current_oci)
    lam = np.clip(lam, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])

        # --- Stage 1 Policy ---
        # Model-Based Value: Transition * Max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax Choice 1
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        # Standard Softmax on Stage 2 MF values
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Stage 1 PE (based on Stage 2 Value)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        
        # Stage 2 PE (based on Reward)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        # Update Stage 2 Q-values
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Update Stage 1 Q-values with Eligibility Trace
        # Q1 += alpha * (delta1 + lambda * delta2)
        q_stage1_mf[a1] += learning_rate * (delta_stage1 + lam * delta_stage2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Loss Aversion
This model hypothesizes that high OCI scores correlate with "loss aversion" or a heightened sensitivity to failure. Specifically, it treats the absence of a reward (0 coins) not as a neutral event, but as a negative outcome (loss). The magnitude of this subjective loss is scaled by the OCI score.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model with OCI-modulated Loss Aversion.
    
    Hypothesis: Participants with higher OCI perceive the lack of reward (0 coins)
    as a subjective loss (negative reward). The magnitude of this loss is 
    determined by the OCI score.
    
    Parameters:
    - learning_rate: [0, 1]
    - beta: [0, 10]
    - w: [0, 1]
    - loss_base: [0, 5] Base magnitude of negative reward for 0 outcome.
    - loss_oci_sens: [0, 5] Additional loss magnitude scaled by OCI.
      Effective Reward (if r=0) = -1 * (loss_base + loss_oci_sens * oci).
    """
    learning_rate, beta, w, loss_base, loss_oci_sens = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate loss magnitude
    loss_magnitude = loss_base + (loss_oci_sens * current_oci)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        raw_r = int(reward[trial])
        
        # Apply Loss Aversion Logic
        if raw_r == 0:
            effective_r = -1.0 * loss_magnitude
        else:
            effective_r = float(raw_r)

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Note: Using effective_r for updates
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = effective_r - q_stage2_mf[s_idx, a2]
        
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Stage 2 Perseveration
Previous successful models identified perseveration (stickiness) in Stage 1. This model proposes that OCI-driven compulsivity also manifests as "motor perseveration" in Stage 2 (choosing the same alien/button), regardless of the planet state. This captures repetitive behavior at the concrete action level.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model with OCI-modulated Stage 2 Perseveration.
    
    Hypothesis: OCI drives compulsive repetition (stickiness) in the second stage 
    (alien choice), causing the participant to repeat the previous motor action 
    (0 or 1) regardless of the current planet state or reward history.
    
    Parameters:
    - learning_rate: [0, 1]
    - beta: [0, 10]
    - w: [0, 1]
    - s2_pers_base: [0, 5] Base perseveration bonus for Stage 2.
    - s2_pers_oci: [0, 5] Modulation by OCI.
    """
    learning_rate, beta, w, s2_pers_base, s2_pers_oci = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate Stage 2 perseveration bonus
    pers_bonus = s2_pers_base + (s2_pers_oci * current_oci)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a2 = -1 # Initialize previous Stage 2 action

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy with Perseveration ---
        logits_2 = beta * q_stage2_mf[s_idx]
        
        # Add stickiness bonus if previous action exists
        if prev_a2 != -1:
            logits_2[prev_a2] += pers_bonus
            
        exp_q2 = np.exp(logits_2 - np.max(logits_2)) # Subtract max for stability
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Update previous action tracker
        prev_a2 = a2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```