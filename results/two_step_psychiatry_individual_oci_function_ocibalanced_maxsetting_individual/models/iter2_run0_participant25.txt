Here are three new cognitive models exploring different mechanisms for how Obsessive-Compulsive Inventory (OCI) scores might modulate reinforcement learning in this two-step task.

### Model 1: Hybrid Model with OCI-Modulated Mixing Weight ($w$)
This model hypothesizes that the balance between Model-Based (planning) and Model-Free (habitual) control is influenced by the participant's OCI score. Specifically, it tests if higher OCI scores lead to a stronger reliance on habitual (Model-Free) strategies, perhaps due to a compulsion to repeat rewarded actions regardless of the transition structure.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based / Model-Free learner where the mixing weight w is
    modulated by the OCI score.
    
    The net Q-value for the first stage is:
    Q_net = w * Q_MB + (1 - w) * Q_MF
    
    where w is derived from a base parameter and the OCI score.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for value updates.
    - beta: [0, 10] Inverse temperature for softmax choice.
    - w_base: [0, 1] Baseline model-based weight.
    - oci_w_scaling: [-1, 1] How strongly OCI affects w. 
                     (Positive = higher OCI -> more MB, Negative = higher OCI -> more MF)
    """
    learning_rate, beta, w_base, oci_w_scaling = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate mixing weight w based on OCI
    # We use a logistic-like clipping or simple linear addition clipped to [0,1]
    w = w_base + (oci_score * oci_w_scaling)
    w = np.clip(w, 0.0, 1.0)

    # Fixed transition matrix (70% common, 30% rare) as per task description
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Q-values
    q_stage1_mf = np.zeros(2)      # Model-free values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for stage 2 (Aliens)

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        
        # 1. Model-Based Value Calculation
        # Max value of next stage states
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        # MB value is expected value of next state given transition probabilities
        q_stage1_mb = transition_matrix @ max_q_stage2

        # 2. Hybrid Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf

        # 3. Action Selection
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]

        # --- Transition ---
        s_idx = int(state[trial]) # 0 or 1 (Planet X or Y)

        # --- Stage 2 Decision ---
        # Standard Model-Free choice at stage 2
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        r = reward[trial]

        # Stage 2 update (TD error)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

        # Stage 1 MF update (TD(1) / Sarsa-like)
        # Using the value of the chosen stage 2 action as the target
        # Note: In standard MB/MF models (e.g., Daw et al., 2011), 
        # MF update for stage 1 often uses TD(1) logic: Q1 += alpha * (r - Q1) 
        # or uses the stage 2 value. Here we use a simple TD update.
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Perseveration (Stickiness) Modulated by OCI
This model investigates whether OCI scores relate to "choice stickiness" or perseveration. Individuals with higher compulsive symptoms might be more prone to simply repeating the previous action regardless of reward history or model-based inference.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Pure Model-Free learner with Choice Stickiness (Perseveration) 
    modulated by OCI.
    
    The value used for decision making includes a bonus for the previously chosen action:
    Q_choice = Q_MF + stickiness_bonus * I(action == prev_action)
    
    Parameters:
    - learning_rate: [0, 1]
    - beta: [0, 10]
    - stick_base: [0, 5] Base stickiness parameter.
    - oci_stick_mod: [-5, 5] Modulation of stickiness by OCI.
    """
    learning_rate, beta, stick_base, oci_stick_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate total stickiness
    stickiness = stick_base + (oci_score * oci_stick_mod)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    prev_a1 = -1 # No previous action for first trial

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Add stickiness bonus to Q-values temporarily for decision
        q_eff = q_stage1.copy()
        if prev_a1 != -1:
            q_eff[prev_a1] += stickiness
            
        exp_q1 = np.exp(beta * q_eff)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # Store for next trial
        prev_a1 = a1

        # --- Transition ---
        s_idx = int(state[trial])

        # --- Stage 2 Choice ---
        # No stickiness modeled for stage 2 in this variant (often stage 1 is focus of stickiness)
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        r = reward[trial]

        # Stage 2 update
        delta_2 = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += learning_rate * delta_2

        # Stage 1 update (TD(0))
        # Update based on the value of the state we landed in (or Q-value of chosen action)
        delta_1 = q_stage2[s_idx, a2] - q_stage1[a1]
        q_stage1[a1] += learning_rate * delta_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Asymmetric Learning Rates Modulated by OCI
This model posits that OCI affects how participants learn from positive versus negative prediction errors. For example, individuals with higher OCI might be hypersensitive to failures (negative prediction errors) or less sensitive to rewards, affecting how quickly they update their value estimates.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner with asymmetric learning rates for positive and negative
    prediction errors, where the negative learning rate is modulated by OCI.
    
    alpha_pos: fixed parameter
    alpha_neg = alpha_neg_base + oci * oci_neg_mod
    
    Parameters:
    - alpha_pos: [0, 1] Learning rate for positive PE (R > Q).
    - alpha_neg_base: [0, 1] Base learning rate for negative PE (R < Q).
    - oci_neg_mod: [-1, 1] Modulation of negative learning rate by OCI.
    - beta: [0, 10] Inverse temperature.
    """
    alpha_pos, alpha_neg_base, oci_neg_mod, beta = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective negative learning rate
    alpha_neg = alpha_neg_base + (oci_score * oci_neg_mod)
    alpha_neg = np.clip(alpha_neg, 0.0, 1.0)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]

        # --- Transition ---
        s_idx = int(state[trial])

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        r = reward[trial]

        # Stage 2 Update
        delta_2 = r - q_stage2[s_idx, a2]
        if delta_2 >= 0:
            q_stage2[s_idx, a2] += alpha_pos * delta_2
        else:
            q_stage2[s_idx, a2] += alpha_neg * delta_2

        # Stage 1 Update
        # Using the updated Q-value of stage 2 as the target for stage 1
        delta_1 = q_stage2[s_idx, a2] - q_stage1[a1]
        
        if delta_1 >= 0:
            q_stage1[a1] += alpha_pos * delta_1
        else:
            q_stage1[a1] += alpha_neg * delta_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```