Here are three new cognitive models based on the two-step task and the participant's OCI score.

### Cognitive Model 1: Sticky-Choice Model with OCI-modulated Perseverance
This model hypothesizes that individuals with higher OCI scores exhibit "stickiness" or perseverance in their choices, regardless of reward outcomes. They might repeat the same first-stage action simply because they chose it before (compulsive repetition). The strength of this perseverance is modulated by their OCI score.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free reinforcement learning with OCI-modulated choice perseverance.
    
    Hypothesis:
    Higher OCI scores lead to higher choice perseverance ('stickiness'), 
    causing the participant to repeat the previous stage-1 action regardless of the outcome.
    
    Parameters:
    - learning_rate: [0, 1] Rate at which Q-values are updated.
    - beta: [0, 10] Inverse temperature for softmax.
    - w: [0, 1] Weighting between Model-Based (1) and Model-Free (0) control.
    - perseverance_factor: [0, 5] Base strength of the tendency to repeat the last choice.
    """
    learning_rate, beta, w, perseverance_factor = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Scale perseverance by OCI score. 
    # A higher OCI score increases the bonus added to the previously chosen action.
    effective_perseverance = perseverance_factor * (1.0 + oci_score)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2) # Model-free Q-values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Model-free Q-values for stage 2 (2 states, 2 actions)
    
    last_action_1 = -1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        if a1 == -1: continue # Skip missing data

        # --- Stage 1 Policy ---
        # 1. Model-Based Value Calculation (Bellman equation using transition matrix)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Hybrid Value Calculation
        q_integrated = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # 3. Add Perseverance Bonus
        # Create a copy to avoid modifying the Q-values permanently for learning
        logits_stage1 = q_integrated.copy()
        if last_action_1 != -1:
            logits_stage1[last_action_1] += effective_perseverance

        # 4. Softmax Choice Probability
        exp_q1 = np.exp(beta * logits_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        s_idx = int(state[trial])
        if s_idx == -1: continue

        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        if a2 == -1: continue
        p_choice_2[trial] = probs_2[a2]

        # --- Learning Updates ---
        r = reward[trial]

        # Stage 2 Update (TD Learning)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 Update (TD Learning)
        # Note: We use the value of the state actually reached (s_idx)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Store action for next trial's perseverance bonus
        last_action_1 = a1

    eps = 1e-10
    valid_trials = (action_1 != -1) & (state != -1) & (action_2 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```

### Cognitive Model 2: OCI-Modulated Learning Rates (Split Positive/Negative)
This model posits that OCI affects how participants learn from feedback. Specifically, it tests if higher OCI scores lead to an asymmetry in learning from positive versus negative prediction errors (e.g., being hyper-sensitive to failure/lack of reward).

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free Q-learning where learning rates for positive and negative prediction errors
    are split and modulated by OCI.
    
    Hypothesis:
    OCI relates to anxiety and error-sensitivity. Higher OCI might amplify learning 
    from negative prediction errors (punishment/lack of reward) relative to positive ones.
    
    Parameters:
    - alpha_pos: [0, 1] Learning rate for positive prediction errors.
    - alpha_neg_base: [0, 1] Base learning rate for negative prediction errors.
    - beta: [0, 10] Inverse temperature.
    - lambda_eligibility: [0, 1] Eligibility trace decay (allow credit assignment to stage 1).
    """
    alpha_pos, alpha_neg_base, beta, lambda_eligibility = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # OCI modulates sensitivity to negative errors
    # If OCI is high, alpha_neg increases.
    alpha_neg = np.clip(alpha_neg_base * (1.0 + oci_score), 0.0, 1.0)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2) 
    q_stage2 = np.zeros((2, 2)) 
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        if a1 == -1: continue

        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        s_idx = int(state[trial])
        if s_idx == -1: continue

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        if a2 == -1: continue
        p_choice_2[trial] = probs_2[a2]

        # --- Learning Updates ---
        r = reward[trial]

        # Stage 2 Prediction Error
        pe_2 = r - q_stage2[s_idx, a2]
        lr_2 = alpha_pos if pe_2 > 0 else alpha_neg
        q_stage2[s_idx, a2] += lr_2 * pe_2
        
        # Stage 1 Prediction Error (SARSA-like update)
        # Using the value of the chosen stage 2 action to update stage 1
        pe_1 = q_stage2[s_idx, a2] - q_stage1[a1]
        lr_1 = alpha_pos if pe_1 > 0 else alpha_neg
        
        # Standard TD update for stage 1
        q_stage1[a1] += lr_1 * pe_1
        
        # Eligibility Trace: Also update Stage 1 based on Stage 2's outcome
        # This connects the final reward back to the first choice directly
        q_stage1[a1] += lambda_eligibility * lr_2 * pe_2

    eps = 1e-10
    valid_trials = (action_1 != -1) & (state != -1) & (action_2 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```

### Cognitive Model 3: OCI-Modulated Model-Based Weighting
This model tests the hypothesis that OCI scores influence the balance between goal-directed (Model-Based) and habitual (Model-Free) control. It proposes that higher OCI scores might correlate with a reliance on rigid habits (Model-Free) or, conversely, an over-thinking planning mode (Model-Based). Here, we parameterize the mixing weight `w` as a function of OCI.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model with OCI-dependent weighting (w) between Model-Based and Model-Free systems.
    
    Hypothesis:
    The balance between goal-directed planning (MB) and habit (MF) is a function of OCI.
    We model 'w' (weight of MB) as a logistic function of OCI.
    
    Parameters:
    - learning_rate: [0, 1]
    - beta: [0, 10]
    - w_slope: [0, 10] Determines how strongly OCI pushes w towards 0 or 1.
    - w_intercept: [0, 1] The baseline bias towards MB or MF when OCI is 0.
    """
    learning_rate, beta, w_slope, w_intercept = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate w based on OCI using a sigmoid-like transformation to keep it in [0,1]
    # If w_slope is positive, higher OCI -> higher w (more Model-Based).
    # If the optimizer finds a negative effective slope (conceptually), it might mean habits.
    # We use a simple linear clamp here for stability within the bounds.
    
    # We model w = w_intercept + slope * OCI, clipped to [0, 1].
    # This allows the model to find if OCI increases or decreases MB control.
    # Note: Since bounds are positive, we interpret w_slope as a magnitude
    # and we can subtract or add. To allow bidirectional search without negative bounds,
    # we can define w = w_intercept +/- (w_slope * OCI). 
    # However, to keep it simple and within the prompt constraints:
    # Let's assume the hypothesis that OCI *decreases* flexibility (increases MF habits).
    
    w_raw = w_intercept - (w_slope * oci_score)
    w = np.clip(w_raw, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        if a1 == -1: continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Weighted mixture
        q_integrated = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_integrated)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        s_idx = int(state[trial])
        if s_idx == -1: continue

        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        if a2 == -1: continue
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        r = reward[trial]

        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    valid_trials = (action_1 != -1) & (state != -1) & (action_2 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```