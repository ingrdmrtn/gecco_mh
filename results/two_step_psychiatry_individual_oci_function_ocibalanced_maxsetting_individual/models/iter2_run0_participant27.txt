Here are the three cognitive models based on the participant's data and OCI score.

### Model 1: Asymmetric Learning with OCI-Modulated Eligibility Trace
**Hypothesis:** This model posits that the participant's obsessive-compulsive symptoms (OCI) influence their "eligibility trace" ($\lambda$). A higher OCI score might lead to stronger credit assignment to the original choice based on the final outcome (reinforcing habits), or conversely, disrupt this link. This model also incorporates asymmetric learning rates for positive and negative prediction errors, given the participant's tendency to stick with choices despite lack of reward (suggesting different sensitivity to gains vs losses).

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Asymmetric learning rates with OCI-modulated Eligibility Trace (Lambda).
    
    Hypothesis: OCI modulates the strength of the eligibility trace (lambda),
    affecting how strongly Stage 2 outcomes reinforce Stage 1 choices.
    
    Parameters:
    lr_pos: [0,1] - Learning rate for positive prediction errors.
    lr_neg: [0,1] - Learning rate for negative prediction errors.
    beta: [0,10] - Inverse temperature (exploration/exploitation).
    w: [0,1] - Model-based weight.
    stickiness: [0,5] - Choice perseveration bonus.
    lam_base: [0,1] - Base eligibility trace, modulated by OCI.
    """
    lr_pos, lr_neg, beta, w, stickiness, lam_base = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # OCI modulation: Higher OCI increases the eligibility trace (habit formation from distal rewards)
    # We clip to ensure it stays within valid bounds [0, 1]
    lam = np.clip(lam_base * (1.0 + oci_score), 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values initialization
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # state x action

    last_action_1 = -1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Handle missing data
        if a1 == -1 or s_idx == -1 or a2 == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # --- Stage 1 Policy ---
        # Model-Based Value
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Value
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Stickiness
        if last_action_1 != -1:
            q_net_1[last_action_1] += stickiness
            
        # Softmax Choice 1
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        # Softmax Choice 2
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Stage 1 MF Update (TD(0))
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        alpha_1 = lr_pos if delta_stage1 > 0 else lr_neg
        q_stage1_mf[a1] += alpha_1 * delta_stage1

        # Stage 2 MF Update
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        alpha_2 = lr_pos if delta_stage2 > 0 else lr_neg
        q_stage2_mf[s_idx, a2] += alpha_2 * delta_stage2

        # Stage 1 Eligibility Trace Update (TD(lambda))
        # Uses the alpha associated with the Stage 2 error
        q_stage1_mf[a1] += alpha_2 * lam * delta_stage2
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Asymmetric Learning with OCI-Modulated Model-Based Weight
**Hypothesis:** This model tests if OCI symptoms specifically impair or alter the balance between model-based (goal-directed) and model-free (habitual) control. It hypothesizes that higher OCI scores lead to a reduction in the model-based weight ($w$), indicating a greater reliance on habitual strategies. It retains asymmetric learning rates to account for the participant's specific response to rewards vs. omissions.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Asymmetric learning rates with OCI-modulated Model-Based Weight (w).
    
    Hypothesis: Higher OCI scores reduce the Model-Based weight (w),
    leading to more habitual (Model-Free) behavior.
    
    Parameters:
    lr_pos: [0,1] - Learning rate for positive prediction errors.
    lr_neg: [0,1] - Learning rate for negative prediction errors.
    beta: [0,10] - Inverse temperature.
    w_base: [0,1] - Base Model-Based weight, modulated by OCI.
    stickiness: [0,5] - Choice perseveration bonus.
    lam: [0,1] - Eligibility trace parameter.
    """
    lr_pos, lr_neg, beta, w_base, stickiness, lam = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # OCI modulation: Higher OCI reduces w (more MF, less MB)
    w = np.clip(w_base * (1.0 - oci_score), 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    last_action_1 = -1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        if a1 == -1 or s_idx == -1 or a2 == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_action_1 != -1:
            q_net_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        alpha_1 = lr_pos if delta_stage1 > 0 else lr_neg
        q_stage1_mf[a1] += alpha_1 * delta_stage1

        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        alpha_2 = lr_pos if delta_stage2 > 0 else lr_neg
        q_stage2_mf[s_idx, a2] += alpha_2 * delta_stage2

        # Eligibility trace update
        q_stage1_mf[a1] += alpha_2 * lam * delta_stage2
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Asymmetric Learning with OCI-Modulated Inverse Temperature
**Hypothesis:** This model tests whether OCI symptoms relate to decision noise or rigidity. It proposes that higher OCI scores increase the inverse temperature parameter ($\beta$), making choices more deterministic (rigid) and less exploratory. This aligns with the clinical presentation of rigidity and intolerance of uncertainty in OCD.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Asymmetric learning rates with OCI-modulated Inverse Temperature (Beta).
    
    Hypothesis: OCI modulates the exploration/exploitation balance (beta).
    Higher OCI leads to higher beta (more deterministic/rigid behavior).
    
    Parameters:
    lr_pos: [0,1] - Learning rate for positive prediction errors.
    lr_neg: [0,1] - Learning rate for negative prediction errors.
    beta_base: [0,10] - Base Inverse temperature, modulated by OCI.
    w: [0,1] - Model-based weight.
    stickiness: [0,5] - Choice perseveration bonus.
    lam: [0,1] - Eligibility trace parameter.
    """
    lr_pos, lr_neg, beta_base, w, stickiness, lam = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # OCI modulation: Higher OCI increases Beta (more rigid/deterministic)
    beta = beta_base * (1.0 + oci_score)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    last_action_1 = -1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        if a1 == -1 or s_idx == -1 or a2 == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_action_1 != -1:
            q_net_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        alpha_1 = lr_pos if delta_stage1 > 0 else lr_neg
        q_stage1_mf[a1] += alpha_1 * delta_stage1

        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        alpha_2 = lr_pos if delta_stage2 > 0 else lr_neg
        q_stage2_mf[s_idx, a2] += alpha_2 * delta_stage2

        # Eligibility trace update
        q_stage1_mf[a1] += alpha_2 * lam * delta_stage2
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```