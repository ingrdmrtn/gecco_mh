Here are three cognitive models designed to explain the participant's behavior, incorporating the OCI score into different mechanisms of the standard hybrid Model-Based/Model-Free reinforcement learning framework.

### Model 1: Hybrid MB/MF with OCI-Modulated Choice Stickiness
This model hypothesizes that the participant's tendency to repeat choices (perseveration) is directly linked to their OCI score. The data shows long blocks of repeating the same spaceship choice (0 or 1) regardless of immediate outcomes, suggesting a "sticky" choice kernel. This model posits that higher OCI scores lead to higher behavioral rigidity (stickiness).

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free RL with OCI-modulated Choice Stickiness.
    
    Hypothesis: OCI modulates the 'stickiness' parameter, which adds a bonus 
    to the value of the previously chosen action, independent of reward history.
    
    Parameters:
    - lr: Learning rate for Q-value updates [0, 1]
    - beta: Inverse temperature [0, 10]
    - w: Mixing weight (0=Pure MF, 1=Pure MB) [0, 1]
    - stick_base: Baseline choice perseveration bonus [0, 5]
    - stick_oci: Effect of OCI on stickiness [-5, 5]
    """
    lr, beta, w, stick_base, stick_oci = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate effective stickiness based on OCI
    stickiness = stick_base + (stick_oci * current_oci)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Handle missing data
        if a1 == -1 or s_idx == -1 or a2 == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        # --- Stage 1 Policy ---
        # Model-Based Value Calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value Calculation
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Add Stickiness Bonus
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        # Softmax Choice 1
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning Updates ---
        # Stage 2 Update (TD Error)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_stage2
        
        # Stage 1 Update (TD Error using Stage 2 Q-value)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid MB/MF with OCI-Modulated Decision Rigidity (Beta)
This model assumes that OCI affects the exploration-exploitation balance. A core feature of compulsive behavior is rigidity. This model tests if the participant's OCI score acts as a modifier on the inverse temperature (`beta`). High OCI might lead to a higher `beta` (making choices more deterministic/greedy based on current values), while low OCI allows for more "softmax noise" or exploration.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF RL with OCI-modulated Decision Rigidity (Inverse Temperature).
    
    Hypothesis: OCI affects the 'beta' parameter. Higher OCI leads to 
    more rigid (deterministic) choices, effectively amplifying the value differences.
    
    Parameters:
    - lr: Learning rate [0, 1]
    - w: Mixing weight (0=Pure MF, 1=Pure MB) [0, 1]
    - beta_base: Baseline inverse temperature [0, 10]
    - beta_oci_slope: How strongly OCI increases/decreases beta [-5, 5]
    - stickiness: Constant choice repetition bonus [0, 5]
    """
    lr, w, beta_base, beta_oci_slope, stickiness = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate effective beta (ensure it stays non-negative)
    beta_effective = max(0.0, beta_base + (beta_oci_slope * current_oci))
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        if a1 == -1 or s_idx == -1 or a2 == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta_effective * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        # Note: Beta effective applies to both stages here
        exp_q2 = np.exp(beta_effective * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning Updates ---
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_stage2
        
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid MB/MF with OCI-Modulated Stage 1 Habit Learning
This model differentiates between the learning rate used for the immediate reward (Stage 2) and the learning rate used to update the "Habit" (Stage 1 Model-Free value). It hypothesizes that OCI specifically modulates the speed of habit formation (Stage 1 MF learning rate). A higher OCI might imply that the agent reinforces the first-stage choice more strongly based on the eventual outcome, solidifying habits faster than the model-based system can adjust.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF RL with OCI-modulated Stage 1 Learning Rate (Habit Strength).
    
    Hypothesis: OCI modulates the specific learning rate used to update 
    Stage 1 Model-Free values. This reflects how quickly 'habits' are stamped 
    in by the final reward, distinct from the learning of the terminal states.
    
    Parameters:
    - lr_stage2: Learning rate for Stage 2 (terminal states) [0, 1]
    - beta: Inverse temperature [0, 10]
    - w: Mixing weight (0=Pure MF, 1=Pure MB) [0, 1]
    - lr_s1_base: Base learning rate for Stage 1 MF [0, 1]
    - lr_s1_oci: Modulation of Stage 1 LR by OCI [-1, 1]
    """
    lr_stage2, beta, w, lr_s1_base, lr_s1_oci = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate Stage 1 specific learning rate
    lr_stage1 = np.clip(lr_s1_base + (lr_s1_oci * current_oci), 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        if a1 == -1 or s_idx == -1 or a2 == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning Updates ---
        # Stage 2 uses the standard parameter
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr_stage2 * delta_stage2
        
        # Stage 1 uses the OCI-modulated rate
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr_stage1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```