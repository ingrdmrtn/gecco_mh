Here are three new cognitive models based on the participant data and OCI score.

### Model 1: Hybrid MB/MF with Eligibility Trace ($\lambda$) Modulated by OCI
This model hypothesizes that the Obsessive-Compulsive Inventory (OCI) score modulates the strength of the eligibility trace ($\lambda$). The eligibility trace controls how strongly the outcome at the second stage (reward) directly reinforces the first-stage choice in the Model-Free system. A higher OCI score might lead to a stronger direct stamping-in of habits (higher $\lambda$) or potentially a decoupling (lower $\lambda$).

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model with Eligibility Trace (lambda) modulated by OCI.
    
    The eligibility trace parameter determines the extent to which the Stage 2 
    prediction error reinforces the Stage 1 choice directly.
    
    Bounds:
    learning_rate: [0,1]
    beta: [0,10]
    w: [0,1] (MB/MF weight)
    p: [0,1] (Choice Stickiness)
    lambda_base: [0,1]
    lambda_oci_mod: [0,1]
    """
    learning_rate, beta, w, p, lambda_base, lambda_oci_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Modulate lambda based on OCI
    # Maps lambda_oci_mod from [0,1] to a shift [-1, 1] scaled by OCI
    lambda_val = lambda_base + (lambda_oci_mod - 0.5) * 2.0 * oci_score
    lambda_val = np.clip(lambda_val, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits = beta * q_net
        if last_action_1 != -1:
            logits[last_action_1] += p * 5.0 # Scale p for meaningful impact
            
        exp_q1 = np.exp(logits - np.max(logits))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Updates
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        # MF update with eligibility trace for Stage 1
        q_stage1_mf[a1] += learning_rate * delta_stage1 + learning_rate * lambda_val * delta_stage2
        
        # Standard MF update for Stage 2
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid MB/MF with Asymmetric Learning Rates Modulated by OCI
This model hypothesizes that OCI affects sensitivity to negative prediction errors (losses or worse-than-expected outcomes). It uses separate learning rates for positive (`lr_pos`) and negative (`lr_neg`) prediction errors. The negative learning rate is modulated by OCI, reflecting potential differences in how compulsive individuals process failure or lack of reward.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model with Asymmetric Learning Rates modulated by OCI.
    
    Differentiates learning from positive vs negative prediction errors.
    The learning rate for negative prediction errors (lr_neg) is modulated by OCI.
    
    Bounds:
    lr_pos: [0,1]
    lr_neg_base: [0,1]
    lr_neg_oci_mod: [0,1]
    beta: [0,10]
    w: [0,1]
    p: [0,1]
    """
    lr_pos, lr_neg_base, lr_neg_oci_mod, beta, w, p = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate negative learning rate based on OCI
    lr_neg = lr_neg_base + (lr_neg_oci_mod - 0.5) * 2.0 * oci_score
    lr_neg = np.clip(lr_neg, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits = beta * q_net
        if last_action_1 != -1:
            logits[last_action_1] += p * 5.0
            
        exp_q1 = np.exp(logits - np.max(logits))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Updates
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        # Apply asymmetric learning rates based on the sign of the RPE
        alpha1 = lr_pos if delta_stage1 >= 0 else lr_neg
        q_stage1_mf[a1] += alpha1 * delta_stage1
        
        alpha2 = lr_pos if delta_stage2 >= 0 else lr_neg
        q_stage2_mf[s_idx, a2] += alpha2 * delta_stage2
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid MB/MF with Value Decay (Forgetting) Modulated by OCI
This model introduces a passive decay (forgetting) for the Q-values of unchosen actions. The rate of this decay is modulated by the OCI score. This tests the hypothesis that compulsivity might relate to the persistence of value representations (low forgetting) or conversely, a rapid turnover of associations (high forgetting).

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model with Forgetting/Decay modulated by OCI.
    
    Unchosen action values decay toward 0 at a rate determined by 'forget_rate'.
    The forgetting rate is modulated by OCI.
    
    Bounds:
    learning_rate: [0,1]
    beta: [0,10]
    w: [0,1]
    p: [0,1]
    forget_base: [0,1]
    forget_oci_mod: [0,1]
    """
    learning_rate, beta, w, p, forget_base, forget_oci_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate forgetting rate based on OCI
    forget_rate = forget_base + (forget_oci_mod - 0.5) * 2.0 * oci_score
    forget_rate = np.clip(forget_rate, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits = beta * q_net
        if last_action_1 != -1:
            logits[last_action_1] += p * 5.0
            
        exp_q1 = np.exp(logits - np.max(logits))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Updates
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        q_stage1_mf[a1] += learning_rate * delta_stage1
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Forgetting for unchosen actions
        # Decay unchosen stage 1 action
        q_stage1_mf[1 - a1] *= (1.0 - forget_rate)
        # Decay unchosen stage 2 action in the current state
        q_stage2_mf[s_idx, 1 - a2] *= (1.0 - forget_rate)
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```