Here are three new cognitive models that explore different mechanisms for how OCI scores might influence decision-making in the two-step task, specifically focusing on the balance between model-based and model-free control, and asymmetric learning rates.

### Model 1: OCI-Modulated Mixing Weight (W)
This model hypothesizes that the balance between Model-Based (planning) and Model-Free (habitual) systems is influenced by the participant's OCI score. Compulsivity is often associated with habit formation. This model tests if higher OCI scores lead to a stronger reliance on the Model-Free system (lower `w`) or the Model-Based system.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free learner where the mixing weight 'w' is modulated by OCI.
    
    The parameter 'w' controls the balance between MB and MF values for Stage 1 choice.
    w = 1 implies pure Model-Based. w = 0 implies pure Model-Free.
    Here, w is a logistic function of a base parameter and the OCI score.
    
    Parameters:
    learning_rate: [0, 1] Learning rate for value updates.
    beta: [0, 10] Inverse temperature for softmax.
    w_logit_base: [0, 1] Base parameter for the mixing weight (before sigmoid).
    oci_w_slope: [0, 1] Slope of OCI effect on mixing weight (scaled to [-5, 5]).
    """
    learning_rate, beta, w_logit_base, oci_w_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Transform parameters to useful ranges
    # Map w_logit_base and oci_w_slope to create a w in [0, 1]
    # We treat w_logit_base as the intercept and oci_w_slope as the slope in logit space
    slope = (oci_w_slope - 0.5) * 10.0 # Range [-5, 5]
    intercept = (w_logit_base - 0.5) * 10.0 # Range [-5, 5]
    
    w_logit = intercept + slope * oci_score
    w = 1.0 / (1.0 + np.exp(-w_logit))
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        # Model-Based Value
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        # Stage 2 RPE
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 RPE (TD(1) style - using the stage 2 value to update stage 1)
        # Note: In standard 2-step MB/MF models, MF update for stage 1 is often driven by 
        # the value of the state reached or the reward. 
        # Here we use the standard TD update: Q1(a1) += alpha * (Q2(s, a2) - Q1(a1)) + alpha * lambda * delta2
        # For simplicity in this template structure, we use a direct update from the stage 2 value.
        
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # We also apply the second stage prediction error to the first stage choice (eligibility trace = 1)
        q_stage1_mf[a1] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Reward Sensitivity
This model posits that OCI symptoms might relate to how strongly an individual reacts to rewards (or lack thereof). Specifically, this model allows the `beta` (inverse temperature) parameter to vary based on the OCI score. A higher beta implies more deterministic exploitation of high-value options, while a lower beta implies more noise or exploration.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Based learner where the inverse temperature (beta) is modulated by OCI.
    
    This tests if higher OCI scores lead to more deterministic (exploitation-heavy) 
    or more stochastic (exploration-heavy) behavior.
    
    Parameters:
    learning_rate: [0, 1] Learning rate.
    beta_base: [0, 10] Baseline inverse temperature.
    oci_beta_mod: [0, 1] Modulation of beta by OCI score (scaled to [-5, 5]).
    w: [0, 1] Fixed mixing weight for MB vs MF control.
    """
    learning_rate, beta_base, oci_beta_mod, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective beta
    # Scale modifier to be meaningful relative to beta range
    mod_scaled = (oci_beta_mod - 0.5) * 10.0 # Range [-5, 5]
    beta_eff = beta_base + (mod_scaled * oci_score)
    
    # Ensure beta stays non-negative
    beta_eff = np.maximum(0.0, beta_eff)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_eff * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta_eff * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        q_stage1_mf[a1] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Asymmetric Learning Rates (Positive vs Negative)
This model investigates if OCI scores correlate with a bias in learning from positive versus negative prediction errors. It implements separate learning rates for positive updates (`alpha_pos`) and negative updates (`alpha_neg`), where the balance between them is shifted by the OCI score. This could reflect a heightened sensitivity to punishment or missed rewards (negative errors) in individuals with higher OCI traits.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Pure Model-Based learner with asymmetric learning rates for positive/negative RPEs,
    where the asymmetry is modulated by OCI.
    
    Parameters:
    lr_base: [0, 1] Baseline learning rate.
    beta: [0, 10] Inverse temperature.
    asymmetry_param: [0, 1] Parameter controlling the split between pos/neg learning rates.
    oci_asym_mod: [0, 1] How much OCI affects the asymmetry (scaled to [-1, 1]).
    """
    lr_base, beta, asymmetry_param, oci_asym_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate asymmetry bias
    # asymmetry_param centers around 0.5. 
    # > 0.5 means higher LR for positive, < 0.5 means higher LR for negative
    base_bias = (asymmetry_param - 0.5) * 2.0 # Range [-1, 1]
    mod_bias = (oci_asym_mod - 0.5) * 2.0 # Range [-1, 1]
    
    total_bias = base_bias + (mod_bias * oci_score)
    total_bias = np.clip(total_bias, -0.99, 0.99) # limit to avoid extreme 0 or 2 multipliers
    
    # Define lr_pos and lr_neg relative to lr_base
    # If bias is 0, both are lr_base.
    # If bias is positive, lr_pos > lr_neg
    lr_pos = lr_base * (1.0 + total_bias)
    lr_neg = lr_base * (1.0 - total_bias)
    
    # Clip to valid range [0, 1]
    lr_pos = np.clip(lr_pos, 0.0, 1.0)
    lr_neg = np.clip(lr_neg, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy (Pure Model-Based) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        if delta_stage2 >= 0:
            q_stage2_mf[s_idx, a2] += lr_pos * delta_stage2
        else:
            q_stage2_mf[s_idx, a2] += lr_neg * delta_stage2
            
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```