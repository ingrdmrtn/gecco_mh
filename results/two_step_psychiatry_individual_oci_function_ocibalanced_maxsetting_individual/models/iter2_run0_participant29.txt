Here are three new cognitive models expressed as Python functions. These models focus on different mechanisms by which Obsessive-Compulsive symptoms (OCI) might influence decision-making, specifically targeting learning rates, reward sensitivity, and uncertainty-driven exploration, distinct from the previous combinations tried.

### Model 1: OCI-Modulated Reward Sensitivity in a Hybrid Model
This model hypothesizes that OCI symptoms affect how intensely rewards are perceived. Higher OCI might lead to a heightened sensitivity to positive outcomes (or failure to dampen them), effectively scaling the reward prediction error. This is implemented by modulating the effective reward value `R` using the OCI score before it enters the Q-learning update.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Reward Sensitivity.
    
    This is a Hybrid MB/MF model where the effective reward magnitude is scaled
    by the OCI score. This tests if OCI symptoms relate to hyper- or hypo-sensitivity 
    to reward outcomes, affecting both Stage 2 and Stage 1 updates.
    
    Parameters:
    - learning_rate: [0, 1] Standard learning rate for value updates.
    - beta: [0, 10] Inverse temperature for softmax choice.
    - w: [0, 1] Weighting parameter (0 = pure MF, 1 = pure MB).
    - reward_sens_base: [0, 2] Base reward sensitivity.
    - reward_sens_oci: [-2, 2] Modulation of reward sensitivity by OCI score.
    """
    learning_rate, beta, w, reward_sens_base, reward_sens_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective reward sensitivity
    # We clip to ensure the multiplier doesn't become negative and invert reward logic nonsensically
    r_sensitivity = max(0.0, reward_sens_base + (reward_sens_oci * oci_score))

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # The reward is scaled by the sensitivity parameter before calculating PE
        effective_reward = r * r_sensitivity
        
        delta_stage2 = effective_reward - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += learning_rate * delta_stage2
        
        # TD(1) style update for stage 1 MF
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Driven "Common Transition" Bias
This model proposes that high OCI scores might lead to an over-reliance on the fixed structure of the task (the transition matrix) and a discounting of recent model-free experience. Specifically, it tests if OCI modulates the mixing weight `w` directly, but instead of a linear slope, it uses a bounded sigmoid-like mapping or simple linear constraint to ensure `w` stays valid, testing if OCI pushes the participant towards rigid Model-Based behavior (high `w`) or habitual Model-Free behavior (low `w`).

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Dependent Mixing Weight (MB/MF Balance).
    
    This model posits that the balance between Model-Based (planning) and 
    Model-Free (habit) systems is determined by the OCI score. 
    It replaces the static 'w' with a function of OCI.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w_intercept: [0, 1] Base mixing weight.
    - w_oci_factor: [-1, 1] How strongly OCI shifts the balance. 
                    Positive = OCI increases Model-Based control.
                    Negative = OCI increases Model-Free control.
    - eligibility: [0, 1] Eligibility trace decay for Stage 1 update.
    """
    learning_rate, beta, w_intercept, w_oci_factor, eligibility = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate w based on OCI, clamping between 0 and 1
    w = w_intercept + (w_oci_factor * oci_score)
    w = np.clip(w, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        delta_stage2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += learning_rate * delta_stage2
        
        # Stage 1 update using eligibility trace logic (lambda)
        # If eligibility=0, it's pure TD(0) for stage 1 (based on stage 2 value)
        # If eligibility=1, it's effectively Monte Carlo (based on outcome)
        delta_stage1 = (q_stage2_mf[s2, a2] - q_stage1_mf[a1]) + eligibility * delta_stage2
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Second-Stage Learning Rate
This model explores the idea that OCI specifically impacts how information is updated at the second stage (the "planet" level). High OCI might be associated with "over-learning" from specific outcomes at the terminal state, or conversely, rigidity and inability to update beliefs about specific aliens. This separates the learning rate into a base component and an OCI-dependent component specifically for the second stage, while keeping the first stage learning rate tied to it or fixed.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Specific Second-Stage Learning Rate.
    
    This model investigates if OCI symptoms specifically distort the learning 
    rate at the second stage (direct reward learning), distinct from general 
    decision noise or strategy balance.
    
    Parameters:
    - lr_base: [0, 1] Base learning rate.
    - lr_oci_mod: [-1, 1] Modulation of learning rate by OCI.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] MB/MF mixing weight.
    - stickiness: [0, 5] General choice stickiness (perseveration).
    """
    lr_base, lr_oci_mod, beta, w, stickiness = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective learning rate
    lr = lr_base + (lr_oci_mod * oci_score)
    lr = np.clip(lr, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Add stickiness to logits
        logits = beta * q_net
        if last_action_1 != -1:
            logits[last_action_1] += stickiness

        exp_q1 = np.exp(logits)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        last_action_1 = a1

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Use the OCI-modulated learning rate for both stages to maintain consistency
        # but the hypothesis is driven by the direct reward impact at stage 2.
        
        delta_stage2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += lr * delta_stage2
        
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```