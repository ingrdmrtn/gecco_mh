Here are 3 new cognitive models that explore different mechanisms for how Obsessive-Compulsive symptoms (OCI) might influence decision-making in this two-step task, specifically focusing on how high OCI scores (like this participant's 0.93) might relate to rigidity, habit formation, or altered learning rates.

### Model 1: OCI-Modulated Model-Based/Model-Free Balance with Separate Learning Rates
This model hypothesizes that OCI levels influence the trade-off between Model-Based (goal-directed) and Model-Free (habitual) control. High OCI might be associated with a reliance on habitual (MF) control, or conversely, rigid goal-directed (MB) control. Additionally, it allows for different learning rates for the two stages, as compulsivity might affect how quickly values are updated at the outcome stage versus the transition stage.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated MB/MF Balance with Separate Learning Rates.
    
    Hypothesis: The balance between Model-Based and Model-Free control (w) is a 
    linear function of the OCI score. Additionally, we separate learning rates 
    for stage 1 (transition structure/value) and stage 2 (reward outcome) to see 
    if OCI relates to a specific deficit in one type of updating.
    
    Bounds:
    lr_1: [0, 1] - Learning rate for stage 1
    lr_2: [0, 1] - Learning rate for stage 2
    beta: [0, 10] - Inverse temperature
    w_base: [0, 1] - Baseline weighting for Model-Based control
    w_oci_mod: [-1, 1] - Modulation of w by OCI score
    """
    lr_1, lr_2, beta, w_base, w_oci_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate w based on OCI, clamping between 0 and 1
    w = w_base + (w_oci_mod * oci_score)
    w = np.clip(w, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        # Model-Based Value: Transition * Max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combined Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        # Note: Handling missing data (-1) is implicit if data is clean, 
        # but here we assume valid indices. If r is -1, usually we skip update.
        if r != -1:
            # Update Stage 2 Q-values (Outcome)
            delta_stage2 = r - q_stage2_mf[s, a2]
            q_stage2_mf[s, a2] += lr_2 * delta_stage2

            # Update Stage 1 Q-values (Transition/TD)
            # Using SARSA-like update or Q-learning update for stage 1
            # Standard two-step often uses Q(s2, a2) as the target for stage 1
            delta_stage1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
            q_stage1_mf[a1] += lr_1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Dependent Decay of Unchosen Options
This model posits that high OCI scores might relate to "hyper-focused" learning or an inability to maintain alternative hypotheses. Specifically, it tests if OCI modulates the decay rate of *unchosen* actions. A high decay rate for unchosen options implies a "winner-take-all" memory or forgetting of alternatives, which could lead to rigid behavior.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Dependent Decay of Unchosen Options.
    
    Hypothesis: OCI modulates how quickly the value of unchosen options decays 
    towards 0 (or a neutral value). High OCI might lead to faster forgetting of 
    paths not taken, reinforcing rigidity.
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    decay_base: [0, 1] - Base decay rate for unchosen options
    decay_oci_slope: [-1, 1] - Effect of OCI on decay rate
    w: [0, 1] - MB/MF balance
    """
    learning_rate, beta, decay_base, decay_oci_slope, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate decay rate based on OCI
    decay_rate = decay_base + (decay_oci_slope * oci_score)
    decay_rate = np.clip(decay_rate, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        if r != -1:
            # --- Update Chosen ---
            # Stage 2
            delta_stage2 = r - q_stage2_mf[s, a2]
            q_stage2_mf[s, a2] += learning_rate * delta_stage2
            
            # Stage 1
            delta_stage1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
            q_stage1_mf[a1] += learning_rate * delta_stage1
            
            # --- Decay Unchosen ---
            # Decay Stage 1 unchosen
            unchosen_a1 = 1 - a1
            q_stage1_mf[unchosen_a1] *= (1 - decay_rate)
            
            # Decay Stage 2 unchosen (in the current state)
            unchosen_a2 = 1 - a2
            q_stage2_mf[s, unchosen_a2] *= (1 - decay_rate)
            # (Optional: Could also decay the unvisited state's values, 
            # but standard models usually just decay unchosen in current context)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Second-Stage "Temperature" (Exploration/Exploitation)
This model investigates if OCI specifically impacts the exploration-exploitation trade-off at the second stage (the final decision of which alien to ask). High OCI might lead to anxious, risk-averse behavior (low temperature, high exploitation) or conversely, erratic checking behavior. This model separates the inverse temperature ($\beta$) for the two stages, with the second stage $\beta$ modulated by OCI.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Second-Stage Temperature.
    
    Hypothesis: OCI scores impact decision noise (beta) specifically at the 
    second stage (choosing the alien). High OCI might correlate with deterministic 
    behavior (high beta) or noisy behavior (low beta) once the planet is reached.
    
    Bounds:
    learning_rate: [0, 1]
    beta_1: [0, 10] - Inverse temperature for Stage 1
    beta_2_base: [0, 10] - Base inverse temperature for Stage 2
    beta_2_oci_mod: [-5, 5] - OCI modulation of Stage 2 beta
    w: [0, 1] - MB/MF balance
    """
    learning_rate, beta_1, beta_2_base, beta_2_oci_mod, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate Beta 2 based on OCI
    # We use exp or softplus to ensure beta stays positive, 
    # or simple clipping if using linear mod.
    # Linear modulation with clipping:
    beta_2 = beta_2_base + (beta_2_oci_mod * oci_score)
    beta_2 = np.clip(beta_2, 0.0, 20.0) # Allow slightly higher ceiling for deterministic behavior

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Choice (Uses beta_1) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Choice (Uses OCI-modulated beta_2) ---
        exp_q2 = np.exp(beta_2 * q_stage2_mf[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        if r != -1:
            delta_stage2 = r - q_stage2_mf[s, a2]
            q_stage2_mf[s, a2] += learning_rate * delta_stage2

            delta_stage1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
            q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```