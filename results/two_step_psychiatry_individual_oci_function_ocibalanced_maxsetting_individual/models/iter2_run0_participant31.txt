Here are 3 new cognitive models for the two-step task, incorporating the participant's high OCI score (0.733) in different mechanistic ways.

### Model 1: OCI-Modulated Choice Stickiness (Perseveration)
This model tests the hypothesis that high OCI leads to "sticky" or repetitive behavior (perseveration), regardless of reward history. It implements a choice autocorrelation parameter (stickiness) that is scaled by the OCI score. Instead of just modulating learning or MB/MF balance, OCI here directly boosts the tendency to repeat the previous Stage 1 action.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Choice Stickiness.
    
    This model incorporates a "stickiness" parameter (perseveration) that biases the 
    participant to repeat their previous Stage 1 choice. The strength of this stickiness 
    is dynamically scaled by the participant's OCI score, hypothesizing that higher 
    compulsivity leads to more repetitive behavior.
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    w: [0, 1] - Mixing weight (0=MF, 1=MB).
    stick_base: [0, 5] - Base stickiness parameter.
    """
    learning_rate, beta, w, stick_base = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Stickiness is the base tendency plus an OCI-dependent boost.
    # Since OCI is [0,1], high OCI amplifies the stickiness.
    stickiness = stick_base * (1.0 + current_oci)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)      
    q_stage2_mf = np.zeros((2, 2)) 
    
    # Track the previous choice for stickiness
    prev_choice = -1

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combine MB and MF values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness bonus to the Q-value of the previously chosen action
        if prev_choice != -1:
            q_net[prev_choice] += stickiness

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)

        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # Update previous choice
        prev_choice = a1

        # --- Stage 2 Policy ---
        s_idx = int(state[trial]) 
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        r = reward[trial]
        
        # TD Error Stage 2
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # TD Error Stage 1
        # Using the Q-value of the chosen stage 2 option as the target
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Dependent Learning Rate Asymmetry (Punishment Sensitivity)
This model posits that individuals with high OCI scores might learn differently from positive versus negative outcomes (e.g., fear of failure or perfectionism). It splits the learning rate into `lr_pos` (for rewards) and `lr_neg` (for non-rewards). The `lr_neg` is modulated by OCI, testing the idea that high OCI increases sensitivity to negative prediction errors (missing a coin).

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Dependent Learning Rate Asymmetry.
    
    This model separates learning rates for positive (reward=1) and negative (reward=0) 
    outcomes. It hypothesizes that OCI specifically modulates sensitivity to negative 
    outcomes (punishment/omission). The negative learning rate is scaled by OCI.
    
    Bounds:
    lr_pos: [0, 1] - Learning rate for rewards.
    lr_neg_base: [0, 1] - Base learning rate for non-rewards.
    beta: [0, 10]
    w: [0, 1]
    """
    lr_pos, lr_neg_base, beta, w = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Negative learning rate increases with OCI (heightened sensitivity to failure)
    # Capped at 1.0 to ensure stability
    lr_neg = min(1.0, lr_neg_base * (1.0 + current_oci))

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)      
    q_stage2_mf = np.zeros((2, 2)) 

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)

        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        s_idx = int(state[trial]) 
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        r = reward[trial]
        
        # Determine which learning rate to use based on REWARD
        if r > 0:
            current_lr = lr_pos
        else:
            current_lr = lr_neg
        
        # TD Error Stage 2
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += current_lr * delta_stage2
        
        # TD Error Stage 1
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += current_lr * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Randomness (Noise)
This model explores the possibility that high OCI is associated with decision noise or anxiety-induced exploration, rather than strategic shifts. Here, the inverse temperature parameter `beta` is modulated by OCI. A base `beta` is adjusted such that higher OCI leads to lower `beta` (more randomness), or potentially higher `beta` (more deterministic/rigid), depending on the fit. This formulation allows the data to decide if OCI correlates with decision rigidity or noise.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Decision Noise (Beta).
    
    This model hypothesizes that OCI affects the exploration-exploitation trade-off 
    (decision noise). The inverse temperature `beta` is a function of a base beta 
    and an OCI scaling factor.
    
    beta_effective = beta_base * (1 + oci_factor * (oci - 0.5))
    
    If oci_factor is positive, high OCI -> higher beta (more rigid/deterministic).
    If oci_factor is negative, high OCI -> lower beta (more random/anxious).
    
    Bounds:
    learning_rate: [0, 1]
    beta_base: [0, 10]
    w: [0, 1]
    oci_factor: [-1, 1] - Direction and strength of OCI influence on noise.
    """
    learning_rate, beta_base, w, oci_factor = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Modulate beta. We center OCI around 0.5 to allow the factor to tilt up or down.
    # We clip beta to be non-negative to avoid computational issues.
    beta_effective = max(0.0, beta_base * (1.0 + oci_factor * (current_oci - 0.5)))

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)      
    q_stage2_mf = np.zeros((2, 2)) 

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_effective * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)

        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        s_idx = int(state[trial]) 
        exp_q2 = np.exp(beta_effective * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        r = reward[trial]
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```