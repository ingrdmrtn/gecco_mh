Here are three new cognitive models for the two-step task, incorporating the OCI score to explain individual variability in decision-making strategies.

### Model 1: OCI-Modulated Win-Stay/Lose-Switch (WSLS)
This model extends the hybrid Model-Based/Model-Free reinforcement learning framework by adding a heuristic "Win-Stay, Lose-Switch" bias to the first-stage choice. The strength of this heuristic is modulated by the participant's OCI score. High OCI scores may correlate with a stronger reliance on rigid response patterns based on immediate prior outcomes (compulsive repetition after success or intolerance of failure).

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Win-Stay/Lose-Switch Model.
    
    Combines MB/MF RL with a heuristic bias.
    If the previous trial was rewarded, the model adds a bonus to repeating the action (Win-Stay).
    If unrewarded, it subtracts that bonus (Lose-Switch).
    The magnitude of this bias is modulated by OCI.
    
    Parameters:
    - learning_rate: [0,1] Value update rate.
    - beta: [0,10] Inverse temperature (exploration/exploitation).
    - w: [0,1] Weighting parameter (0=Pure MF, 1=Pure MB).
    - wsls_base: [0,1] Baseline strength of the WSLS heuristic.
    - wsls_oci: [0,1] Effect of OCI on the WSLS strength.
    """
    learning_rate, beta, w, wsls_base, wsls_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Map parameters to functional ranges
    # WSLS strength can be positive (classic WSLS) or negative.
    # Base contributes [0, 5], OCI contributes [-2.5, 2.5] approx.
    wsls_strength = (wsls_base * 5.0) + ((wsls_oci - 0.5) * 5.0 * oci_score)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Q-values
    q_stage1_mf = np.zeros(2) # Values for Spaceships A(0) and U(1)
    q_stage2_mf = np.zeros((2, 2)) # Values for Aliens (2 planets x 2 aliens)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_a1 = -1
    last_reward = -1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        # Model-Based Value
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Calculate Logits
        logits = beta * q_net
        
        # Apply WSLS Bias
        if last_a1 != -1:
            if last_reward == 1:
                logits[last_a1] += wsls_strength # Win-Stay
            else:
                logits[last_a1] -= wsls_strength # Lose-Switch
        
        # Softmax Choice 1
        logits = logits - np.max(logits) # Numerical stability
        exp_q1 = np.exp(logits)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        q_vals_2 = q_stage2_mf[s_idx]
        logits_2 = beta * q_vals_2
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Stage 1 Update (TD(0))
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Store history
        last_a1 = a1
        last_reward = r

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Dual Inverse Temperatures
This model hypothesizes that OCI affects the exploration-exploitation trade-off differently in the two stages. Specifically, it allows for a separate inverse temperature ($\beta$) for the second stage (choosing an alien), which is modulated by the OCI score. High OCI might lead to more deterministic (rigid) behavior or more chaotic (anxious) behavior specifically when facing the direct reward outcome in Stage 2.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Dual Inverse Temperature Model.
    
    Uses separate beta parameters for Stage 1 (Spaceship) and Stage 2 (Alien).
    The Stage 2 beta is modulated by the OCI score.
    
    Parameters:
    - learning_rate: [0,1] Value update rate.
    - w: [0,1] Weighting parameter (0=Pure MF, 1=Pure MB).
    - beta_1: [0,10] Inverse temperature for Stage 1.
    - beta_2_base: [0,1] Baseline component for Stage 2 beta (mapped to [0, 10]).
    - beta_2_oci: [0,1] OCI modulation component for Stage 2 beta.
    """
    learning_rate, w, beta_1, beta_2_base, beta_2_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Map parameters
    # Stage 1 Beta
    b1 = beta_1 
    
    # Stage 2 Beta: Base + OCI modulation
    # Maps roughly to [0, 10] range, ensuring non-negativity
    b2_val = (beta_2_base * 10.0) + ((beta_2_oci - 0.5) * 10.0 * oci_score)
    b2 = np.maximum(0.0, b2_val)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy (uses b1) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = b1 * q_net
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy (uses b2) ---
        q_vals_2 = q_stage2_mf[s_idx]
        logits_2 = b2 * q_vals_2
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Stage-Specific Learning Rates
This model proposes that OCI symptoms affect how quickly participants learn from direct outcomes (Stage 2) versus how they maintain global value estimates. It implements two separate learning rates: `lr1` for the first stage (updating spaceship values) and `lr2` for the second stage (updating alien values). The second stage learning rate is modulated by the OCI score.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Dual Learning Rate Model.
    
    Uses separate learning rates for Stage 1 updates and Stage 2 updates.
    The Stage 2 learning rate is modulated by OCI.
    High OCI might correlate with over-sensitivity (high LR) or rigidity (low LR) 
    to immediate reward outcomes.
    
    Parameters:
    - beta: [0,10] Inverse temperature.
    - w: [0,1] Weighting parameter.
    - lr1: [0,1] Learning rate for Stage 1.
    - lr2_base: [0,1] Baseline learning rate for Stage 2.
    - lr2_oci: [0,1] OCI modulation for Stage 2 learning rate.
    """
    beta, w, lr1, lr2_base, lr2_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Map Stage 2 Learning Rate
    # Base + OCI modulation, clipped to [0, 1]
    lr2_val = lr2_base + (lr2_oci - 0.5) * 2.0 * oci_score
    lr2 = np.clip(lr2_val, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        q_vals_2 = q_stage2_mf[s_idx]
        logits_2 = beta * q_vals_2
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Update Stage 1 MF using lr1
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr1 * delta_stage1
        
        # Update Stage 2 MF using lr2 (OCI modulated)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr2 * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```