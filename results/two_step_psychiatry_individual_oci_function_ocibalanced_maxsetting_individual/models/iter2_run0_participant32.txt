Here are three cognitive models expressed as Python functions. They incorporate the OCI score to modulate specific mechanisms of decision-making, such as heuristic usage, model-based capacity, and memory decay.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Win-Stay Lose-Shift (WSLS) Model.
    
    Hypothesis: High OCI participants rely on a rigid "Win-Stay, Lose-Shift" heuristic 
    layered on top of standard reinforcement learning. The strength of this heuristic 
    is proportional to their OCI score.
    
    Parameters:
    - learning_rate: [0, 1] Rate of updating Q-values.
    - beta: [0, 10] Inverse temperature for softmax (exploration/exploitation).
    - w: [0, 1] Weight of Model-Based values (0=Pure MF, 1=Pure MB).
    - wsls_strength: [0, 5] Magnitude of the OCI-scaled WSLS bias.
    """
    learning_rate, beta, w, wsls_strength = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    last_reward = -1

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Apply OCI-modulated WSLS bias
        # If previous trial rewarded: bias towards staying (Win-Stay)
        # If previous trial unrewarded: bias away from staying (Lose-Shift)
        if last_action_1 != -1:
            bias = wsls_strength * oci_score
            if last_reward == 1:
                q_net[last_action_1] += bias
            else:
                q_net[last_action_1] -= bias

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]

        # --- Stage 2 Policy ---
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Stage 1 Update (TD)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        last_action_1 = a1
        last_reward = r

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Constrained Model-Based Deficit Model.
    
    Hypothesis: Compulsivity (OCI) is structurally linked to a deficit in Model-Based control.
    The mixing weight 'w' is constrained to decrease as OCI increases.
    Also includes a general stickiness parameter for perseveration.
    
    Parameters:
    - learning_rate: [0, 1] Rate of updating Q-values.
    - beta: [0, 10] Inverse temperature.
    - w_raw: [0, 1] Base Model-Based weight potential.
    - stickiness: [0, 5] General tendency to repeat the previous choice.
    
    Mechanism:
    Effective w = w_raw * (1.0 - oci_score).
    High OCI strictly limits the contribution of the model-based system.
    """
    learning_rate, beta, w_raw, stickiness = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Constrain w based on OCI hypothesis
    w_eff = w_raw * (1.0 - oci_score)
    # Ensure w_eff is non-negative (OCI is usually < 1, but safe clipping)
    w_eff = max(0.0, w_eff)

    last_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w_eff * q_stage1_mb) + ((1 - w_eff) * q_stage1_mf)
        
        # Add stickiness bonus
        stickiness_vec = np.zeros(2)
        if last_action_1 != -1:
            stickiness_vec[last_action_1] = stickiness
            
        exp_q1 = np.exp(beta * (q_net + stickiness_vec))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]

        # --- Stage 2 Policy ---
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Anxiety/Decay Model.
    
    Hypothesis: High OCI is associated with anxiety about uncertainty or unchosen paths.
    This is modeled as a decay of the value of unchosen options, where the rate of decay
    is scaled by the OCI score. "Use it or lose it" logic becomes more severe with higher OCI.
    
    Parameters:
    - learning_rate: [0, 1] Rate of updating Q-values.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weight of Model-Based values.
    - decay_factor: [0, 1] Base decay rate, scaled by OCI.
    - stickiness: [0, 5] General perseveration parameter.
    """
    learning_rate, beta, w, decay_factor, stickiness = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        stickiness_vec = np.zeros(2)
        if last_action_1 != -1:
            stickiness_vec[last_action_1] = stickiness

        exp_q1 = np.exp(beta * (q_net + stickiness_vec))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]

        # --- Stage 2 Policy ---
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Update chosen option (Standard TD)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Decay unchosen option (OCI-modulated)
        unchosen_a1 = 1 - a1
        decay_rate = decay_factor * oci_score
        # Ensure decay doesn't invert sign or go unstable
        decay_rate = np.clip(decay_rate, 0, 1)
        q_stage1_mf[unchosen_a1] *= (1.0 - decay_rate)
        
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```