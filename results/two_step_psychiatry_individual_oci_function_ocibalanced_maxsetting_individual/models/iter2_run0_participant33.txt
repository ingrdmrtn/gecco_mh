Here are the 3 cognitive models based on the participant's data and OCI score.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Forgetting (Decay) Model.
    
    Hypothesis: High OCI scores may be associated with an inability to maintain 
    stable value representations over time, specifically for options that are not 
    currently being chosen. This model introduces a passive decay rate for 
    unchosen Q-values (forgetting), which is modulated by the participant's OCI score.
    
    Parameters:
    - lr: [0,1] Learning rate for chosen options.
    - beta: [0,10] Inverse temperature (softness of choice).
    - w: [0,1] Mixing weight (0=Model-Free, 1=Model-Based).
    - lambd: [0,1] Eligibility trace parameter.
    - decay_base: [0,1] Baseline decay rate for unchosen options.
    - decay_oci: [0,1] Additional decay rate scaled by OCI score.
    """
    lr, beta, w, lambd, decay_base, decay_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective decay rate, clipped to [0, 1]
    decay = decay_base + decay_oci * oci_score
    decay = np.clip(decay, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    log_loss = 0.0
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Handle missing or invalid data
        if a1 < 0 or s2 < 0 or a2 < 0:
            continue

        # --- Stage 1 Choice ---
        # Model-Based Value
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax Stage 1
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / (np.sum(exp_q1) + 1e-10)
        log_loss -= np.log(probs_1[a1] + 1e-10)

        # --- Stage 2 Choice ---
        # Softmax Stage 2
        exp_q2 = np.exp(beta * q_stage2_mf[s2])
        probs_2 = exp_q2 / (np.sum(exp_q2) + 1e-10)
        log_loss -= np.log(probs_2[a2] + 1e-10)

        # --- Learning & Forgetting ---
        # 1. Standard TD Update (Chosen Options)
        delta_1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_1
        
        delta_2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += lr * delta_2
        
        q_stage1_mf[a1] += lr * lambd * delta_2
        
        # 2. Decay (Unchosen Options)
        # Decay unchosen Stage 1 option
        unchosen_a1 = 1 - a1
        q_stage1_mf[unchosen_a1] *= (1.0 - decay)
        
        # Decay unchosen Stage 2 option (for the current state)
        unchosen_a2 = 1 - a2
        q_stage2_mf[s2, unchosen_a2] *= (1.0 - decay)
        
        # Note: We do not decay values for the unvisited state in this model 
        # to keep focus on the active decision path alternatives.

    return log_loss

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Perseveration after Punishment Model.
    
    Hypothesis: Compulsivity (high OCI) is often characterized by the repetition of 
    behavior despite negative consequences. This model distinguishes between 'stickiness' 
    (perseveration) after a reward (Win-Stay) and after a punishment (Lose-Persevere). 
    It hypothesizes that OCI specifically increases the tendency to repeat the Stage 1 
    choice after a failure (Reward=0).
    
    Parameters:
    - lr: [0,1] Learning rate.
    - beta: [0,10] Inverse temperature.
    - w: [0,1] Mixing weight.
    - lambd: [0,1] Eligibility trace.
    - stick_rew: [0,10] Stickiness bonus applied if previous trial was rewarded.
    - stick_pun_base: [0,10] Baseline stickiness bonus if previous trial was unrewarded.
    - stick_pun_oci: [0,10] Additional punishment stickiness scaled by OCI.
    """
    lr, beta, w, lambd, stick_rew, stick_pun_base, stick_pun_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate punishment stickiness
    stick_pun = stick_pun_base + stick_pun_oci * oci_score
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_a1 = -1
    last_r = -1
    
    log_loss = 0.0
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        if a1 < 0 or s2 < 0 or a2 < 0:
            continue

        # --- Stage 1 Choice with Conditional Stickiness ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness bonus to logits (Q * beta + stick)
        logits = beta * q_net
        
        if last_a1 != -1:
            if last_r == 1:
                logits[last_a1] += stick_rew
            else:
                logits[last_a1] += stick_pun
        
        exp_q1 = np.exp(logits)
        probs_1 = exp_q1 / (np.sum(exp_q1) + 1e-10)
        log_loss -= np.log(probs_1[a1] + 1e-10)

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[s2])
        probs_2 = exp_q2 / (np.sum(exp_q2) + 1e-10)
        log_loss -= np.log(probs_2[a2] + 1e-10)

        # --- Learning ---
        delta_1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_1
        
        delta_2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += lr * delta_2
        
        q_stage1_mf[a1] += lr * lambd * delta_2
        
        # Update history
        last_a1 = a1
        last_r = r

    return log_loss

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Stage 2 Inverse Temperature Model.
    
    Hypothesis: OCI-related rigidity or uncertainty might be specific to the 
    immediate, concrete interaction with the aliens (Stage 2) rather than the 
    abstract planning stage (Stage 1). This model allows for different levels 
    of decision noise (beta) at the two stages, with the Stage 2 beta modulated 
    by OCI.
    
    Parameters:
    - lr: [0,1] Learning rate.
    - beta1: [0,10] Inverse temperature for Stage 1.
    - beta2_base: [0,10] Baseline inverse temperature for Stage 2.
    - beta2_oci: [0,10] Increase in Stage 2 inverse temperature scaled by OCI.
    - w: [0,1] Mixing weight.
    - lambd: [0,1] Eligibility trace.
    """
    lr, beta1, beta2_base, beta2_oci, w, lambd = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate Stage 2 beta
    beta2 = beta2_base + beta2_oci * oci_score
    # Ensure beta stays within reasonable bounds if oci is large (though bounds usually handle this)
    beta2 = max(0.0, beta2) 
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    log_loss = 0.0
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        if a1 < 0 or s2 < 0 or a2 < 0:
            continue

        # --- Stage 1 Choice (Uses beta1) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta1 * q_net)
        probs_1 = exp_q1 / (np.sum(exp_q1) + 1e-10)
        log_loss -= np.log(probs_1[a1] + 1e-10)

        # --- Stage 2 Choice (Uses beta2 modulated by OCI) ---
        exp_q2 = np.exp(beta2 * q_stage2_mf[s2])
        probs_2 = exp_q2 / (np.sum(exp_q2) + 1e-10)
        log_loss -= np.log(probs_2[a2] + 1e-10)

        # --- Learning ---
        delta_1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_1
        
        delta_2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += lr * delta_2
        
        q_stage1_mf[a1] += lr * lambd * delta_2

    return log_loss
```