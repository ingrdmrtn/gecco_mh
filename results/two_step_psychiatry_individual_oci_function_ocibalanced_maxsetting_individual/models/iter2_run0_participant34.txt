Here are three new cognitive models exploring different mechanisms for how OCI might influence decision-making in this two-step task.

### Cognitive Model 1: Hybrid Model with OCI-modulated Model-Based Weight
This model tests the hypothesis that high OCI scores are associated with a reliance on habit (Model-Free) over flexible planning (Model-Based). It posits that as OCI increases, the weight `w` given to the model-based system decreases, leading to more rigid, habitual behavior.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL with OCI-Modulated Model-Based Weight.

    Hypothesis: High OCI participants show reduced reliance on model-based (planning) 
    strategies and increased reliance on model-free (habitual) strategies.
    The mixing weight 'w' is dynamically scaled down by the OCI score.

    Parameters:
    learning_rate: [0, 1] Update rate for Q-values.
    beta: [0, 10] Inverse temperature for softmax choice.
    w_base: [0, 1] Baseline weighting for model-based values (0=pure MF, 1=pure MB).
    oci_w_decay: [0, 1] Strength of reduction in 'w' due to OCI.
    """
    learning_rate, beta, w_base, oci_w_decay = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective w: higher OCI reduces w towards 0 (Model-Free/Habitual)
    w_eff = w_base * (1.0 - oci_score * oci_w_decay)
    w_eff = np.clip(w_eff, 0, 1)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # Model-Based Value Calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_hybrid = w_eff * q_stage1_mb + (1 - w_eff) * q_stage1_mf
        
        # Softmax Policy Stage 1
        exp_q1 = np.exp(beta * q_hybrid)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        
        # --- Stage 2 Decision ---
        # Standard Model-Free Choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # --- Learning ---
        # Stage 2 Update (TD)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2
        
        # Stage 1 Update (TD-0 using Stage 2 Q-value)
        # Note: In standard MB/MF hybrids (like Daw et al. 2011), the MF update 
        # uses the value of the state reached (q_stage2_mf[state_idx, chosen_action]).
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2: OCI-Driven Perseverance (Stickiness)
This model focuses on the "stickiness" or perseverance of choice, which is often elevated in compulsive disorders. It proposes that OCI specifically modulates the tendency to repeat the previous Stage 1 action, regardless of reward history.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free RL with OCI-Driven Perseverance (Stickiness).
    
    Hypothesis: High OCI scores correlate with 'stuck' behaviors or repetition compulsion.
    This model adds a stickiness bonus to the Q-value of the previously chosen action,
    where the magnitude of this bonus is directly scaled by the OCI score.
    
    Parameters:
    learning_rate: [0, 1] Update rate for Q-values.
    beta: [0, 10] Inverse temperature.
    stick_base: [0, 5] Baseline tendency to repeat the last choice.
    oci_stick_boost: [0, 5] Additional stickiness multiplier derived from OCI.
    """
    learning_rate, beta, stick_base, oci_stick_boost = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective stickiness
    stickiness_eff = stick_base + (oci_score * oci_stick_boost)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        q_augmented = q_stage1_mf.copy()
        
        # Apply perseverance bonus to the previously chosen action
        if last_action_1 != -1:
            q_augmented[int(last_action_1)] += stickiness_eff
            
        exp_q1 = np.exp(beta * q_augmented)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        last_action_1 = action_1[trial]
        state_idx = int(state[trial])

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # --- Learning ---
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3: OCI-Asymmetric Learning Rates
This model investigates if OCI affects how participants learn from positive versus negative feedback. It hypothesizes that high OCI might be associated with a hypersensitivity to punishment (or lack of reward), leading to different learning rates for positive prediction errors (wins) versus negative prediction errors (losses/omissions).

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free RL with OCI-Modulated Asymmetric Learning Rates.
    
    Hypothesis: OCI affects sensitivity to negative outcomes. High OCI participants 
    may over-learn from negative prediction errors (losses).
    This model splits learning rate into alpha_pos and alpha_neg, where
    alpha_neg is modulated by the OCI score.
    
    Parameters:
    alpha_pos: [0, 1] Learning rate for positive prediction errors (RPE > 0).
    alpha_neg_base: [0, 1] Baseline learning rate for negative prediction errors (RPE < 0).
    oci_neg_sens: [0, 5] Multiplier increasing alpha_neg based on OCI.
    beta: [0, 10] Inverse temperature.
    """
    alpha_pos, alpha_neg_base, oci_neg_sens, beta = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective negative learning rate
    # We clip it to ensure it doesn't exceed 1.0 or go below 0
    alpha_neg_eff = alpha_neg_base * (1.0 + oci_score * oci_neg_sens)
    alpha_neg_eff = np.clip(alpha_neg_eff, 0, 1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # --- Learning ---
        # Stage 2 RPE
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        
        # Asymmetric update for Stage 2
        if delta_stage2 >= 0:
            q_stage2_mf[state_idx, int(action_2[trial])] += alpha_pos * delta_stage2
        else:
            q_stage2_mf[state_idx, int(action_2[trial])] += alpha_neg_eff * delta_stage2
        
        # Stage 1 RPE
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        
        # Asymmetric update for Stage 1
        if delta_stage1 >= 0:
            q_stage1_mf[int(action_1[trial])] += alpha_pos * delta_stage1
        else:
            q_stage1_mf[int(action_1[trial])] += alpha_neg_eff * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```