Here are three cognitive models that explain the participant's behavior, incorporating the Obsessive-Compulsive Inventory (OCI) score to modulate specific parameters.

### Model 1: Hybrid Model with OCI-modulated Inverse Temperature
This model hypothesizes that high OCI scores are associated with "rigidity" or a more deterministic choice policy. In reinforcement learning terms, this corresponds to a higher inverse temperature ($\beta$). A higher $\beta$ makes the participant more likely to exploit the option with the slightly higher value, exploring less often.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free RL with OCI-modulated Inverse Temperature.
    
    Hypothesis: OCI scores correlate with rigidity in decision making. 
    Higher OCI leads to a higher inverse temperature (beta), resulting in 
    more deterministic (rigid) choices and less exploration.
    
    Parameters:
    learning_rate: [0, 1] - Rate of value updating.
    w: [0, 1] - Weighting between Model-Based (1) and Model-Free (0).
    beta_base: [0, 10] - Baseline inverse temperature.
    beta_oci_sens: [0, 10] - Sensitivity of beta to OCI score.
    
    Equation: beta_effective = beta_base + beta_oci_sens * oci
    """
    learning_rate, w, beta_base, beta_oci_sens = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective beta based on OCI
    beta = beta_base + beta_oci_sens * oci_score
    # Cap beta to prevent numerical overflow
    if beta > 20: beta = 20
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values initialization
    q_mf1 = np.zeros(2)      # MF Stage 1
    q_mb1 = np.zeros(2)      # MB Stage 1
    q_mf2 = np.zeros((2, 2)) # MF Stage 2 (State, Action)
    
    for trial in range(n_trials):
        if action_1[trial] == -1:
            # Skip missing trials by setting prob to 1 (log(1)=0, no loss contribution)
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue
            
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Stage 1 Choice ---
        # Model-Based Value Calculation: V_MB = T * max(Q2)
        max_q_mf2 = np.max(q_mf2, axis=1)
        q_mb1 = transition_matrix @ max_q_mf2
        
        # Hybrid Value Integration
        q_net_1 = w * q_mb1 + (1 - w) * q_mf1
        
        # Softmax Stage 1
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Choice ---
        # Softmax Stage 2 (Pure MF)
        exp_q2 = np.exp(beta * q_mf2[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        # Stage 2 RPE
        delta_2 = r - q_mf2[s2, a2]
        q_mf2[s2, a2] += learning_rate * delta_2
        
        # Stage 1 RPE (SARSA-style for MF)
        delta_1 = q_mf2[s2, a2] - q_mf1[a1]
        q_mf1[a1] += learning_rate * delta_1
        
        # Eligibility trace (lambda=1 assumption for standard Hybrid)
        # This propagates the stage 2 outcome back to stage 1
        q_mf1[a1] += learning_rate * delta_2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model with OCI-modulated Asymmetric Learning Rates
This model hypothesizes that OCI symptoms relate to an altered sensitivity to negative outcomes (in this task, receiving 0 coins). It splits the learning rate into positive and negative components, where the negative learning rate is boosted by the OCI score. This captures "fear of failure" or hypersensitivity to missing rewards.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model with OCI-modulated Asymmetric Learning Rates.
    
    Hypothesis: High OCI participants exhibit altered sensitivity to negative outcomes 
    (failure to receive reward). This model splits the learning rate into positive
    and negative components, where the negative learning rate is modulated by OCI.
    
    Parameters:
    lr_base: [0, 1] - Base learning rate (used for positive PEs).
    lr_neg_mod: [0, 1] - Modulation factor for negative learning rate via OCI.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - MB/MF mixing weight.
    
    Equations:
    lr_pos = lr_base
    lr_neg = lr_base * (1 + lr_neg_mod * oci_score * 5) 
    (The factor 5 scales the effect to ensure broad coverage within bounds)
    """
    lr_base, lr_neg_mod, beta, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    lr_pos = lr_base
    # Allow OCI to increase the learning rate for negative PEs significantly
    lr_neg = lr_base * (1.0 + lr_neg_mod * oci_score * 5.0)
    # Clip to 1.0 to ensure valid learning rate
    if lr_neg > 1.0: lr_neg = 1.0
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_mf1 = np.zeros(2)
    q_mf2 = np.zeros((2, 2))
    
    for trial in range(n_trials):
        if action_1[trial] == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue
            
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Stage 1 Policy
        max_q_mf2 = np.max(q_mf2, axis=1)
        q_mb1 = transition_matrix @ max_q_mf2
        q_net_1 = w * q_mb1 + (1 - w) * q_mf1
        
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_mf2[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # Updates with Asymmetric Learning Rates
        # Stage 2 Update
        delta_2 = r - q_mf2[s2, a2]
        alpha_2 = lr_pos if delta_2 >= 0 else lr_neg
        q_mf2[s2, a2] += alpha_2 * delta_2
        
        # Stage 1 Update (TD-error from Stage 2 value)
        delta_1 = q_mf2[s2, a2] - q_mf1[a1]
        # We assume stage 1 transition updates are neutral/positive unless delta is negative
        alpha_1 = lr_pos if delta_1 >= 0 else lr_neg
        q_mf1[a1] += alpha_1 * delta_1
        
        # Eligibility Trace Update (TD-error from Reward)
        # Using the same asymmetric logic for the trace update
        alpha_trace = lr_pos if delta_2 >= 0 else lr_neg
        q_mf1[a1] += alpha_trace * delta_2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model with OCI-modulated Eligibility Trace
This model hypothesizes that OCI affects the "habitual chaining" mechanism, specifically the eligibility trace parameter ($\lambda$). A higher OCI might lead to stronger chaining (higher $\lambda$), implying that the final outcome strongly reinforces the initial choice in a model-free manner, or conversely, a disconnection (lower $\lambda$) where steps are treated independently.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model with OCI-modulated Eligibility Trace (Lambda).
    
    Hypothesis: The degree to which the second-stage outcome reinforces the 
    first-stage choice (eligibility trace) varies with OCI. 
    
    Parameters:
    learning_rate: [0, 1]
    beta: [0, 10]
    w: [0, 1]
    lambda_base: [0, 1] - Base eligibility trace.
    lambda_oci_mod: [0, 1] - Modulation of lambda by OCI.
    
    Equation:
    lambda_val = lambda_base + (1 - lambda_base) * oci * lambda_oci_mod
    (Scales lambda from base towards 1.0 as OCI increases)
    """
    learning_rate, beta, w, lambda_base, lambda_oci_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate lambda
    # We interpolate between lambda_base and 1.0 based on OCI intensity
    lambda_val = lambda_base + (1.0 - lambda_base) * oci_score * lambda_oci_mod
    if lambda_val > 1.0: lambda_val = 1.0
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_mf1 = np.zeros(2)
    q_mf2 = np.zeros((2, 2))
    
    for trial in range(n_trials):
        if action_1[trial] == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue
            
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Stage 1
        max_q_mf2 = np.max(q_mf2, axis=1)
        q_mb1 = transition_matrix @ max_q_mf2
        q_net_1 = w * q_mb1 + (1 - w) * q_mf1
        
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Stage 2
        exp_q2 = np.exp(beta * q_mf2[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # Updates
        delta_2 = r - q_mf2[s2, a2]
        q_mf2[s2, a2] += learning_rate * delta_2
        
        delta_1 = q_mf2[s2, a2] - q_mf1[a1]
        q_mf1[a1] += learning_rate * delta_1
        
        # Modulated Eligibility Trace
        # This determines how much the final reward updates the first stage choice directly
        q_mf1[a1] += learning_rate * lambda_val * delta_2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```