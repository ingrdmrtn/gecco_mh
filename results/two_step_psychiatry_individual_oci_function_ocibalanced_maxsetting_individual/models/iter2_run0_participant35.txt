Here are three new cognitive models for the two-step task, designed to capture individual variability associated with OCI scores.

### Model 1: Asymmetric Learning Rates with OCI-Modulated Punishment Sensitivity
This model hypothesizes that high OCI individuals process negative outcomes (lack of reward) differently than low OCI individuals. While `learning_rate_pos` governs updates after finding gold, `learning_rate_neg` (punishment/omission) is modulated by OCI. A lower learning rate for negative outcomes in high OCI individuals could explain "compulsive" persistence (failure to unlearn/switch) despite losses.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Asymmetric Learning Rate Model with OCI-Modulated Punishment Sensitivity.
    
    Hypothesis: OCI score modulates how participants learn from unrewarded trials (punishment).
    High OCI might lead to reduced learning from failure (persistence) or heightened sensitivity.
    
    Parameters:
    - lr_pos: [0, 1] Learning rate for rewarded trials (Gold).
    - lr_neg_base: [0, 1] Baseline learning rate for unrewarded trials (No Gold).
    - lr_neg_oci: [-1, 1] Modulation of negative learning rate by OCI.
    - beta: [0, 10] Inverse temperature (softmax randomness).
    - w: [0, 1] Model-based weight.
    - stick: [0, 5] General choice stickiness (perseveration) for Stage 1.
    """
    lr_pos, lr_neg_base, lr_neg_oci, beta, w, stick = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate OCI-specific negative learning rate
    lr_neg = lr_neg_base + (lr_neg_oci * oci_score)
    lr_neg = np.clip(lr_neg, 0.0, 1.0) # Ensure valid bounds

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_s1 = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Apply stickiness
        if last_action_1 != -1:
            q_net_s1[last_action_1] += stick
            
        exp_q1 = np.exp(beta * q_net_s1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]

        # --- Stage 2 Policy ---
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # --- Learning Updates ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Select learning rate based on outcome
        current_lr = lr_pos if r == 1.0 else lr_neg
        
        # Update Stage 1 MF (TD(0) style using Stage 2 value)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += current_lr * delta_stage1
        
        # Update Stage 2 MF (using Reward)
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += current_lr * delta_stage2
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Stage 2 Stickiness
Previous models often focus on stickiness in the first stage (spaceship choice). This model tests the hypothesis that OCI-related compulsivity manifests as rigid repetition of the specific instrumental action (alien choice) in the second stage, regardless of the value.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model with OCI-Modulated Stage 2 Stickiness.
    
    Hypothesis: High OCI scores correlate with increased compulsivity in the 
    second-stage choice (Alien selection), leading to 'sticking' to the same 
    alien within a planet regardless of reward history.
    
    Parameters:
    - learning_rate: [0, 1] Value update rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Model-based weight.
    - stick_s1: [0, 5] Stickiness for Stage 1 (Spaceship).
    - stick_s2_base: [0, 5] Baseline stickiness for Stage 2 (Alien).
    - stick_s2_oci: [0, 5] Modulation of Stage 2 stickiness by OCI.
    """
    learning_rate, beta, w, stick_s1, stick_s2_base, stick_s2_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate Stage 2 stickiness
    stick_s2 = stick_s2_base + (stick_s2_oci * oci_score)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    # Track last action taken in each state (Planet X and Planet Y separately)
    last_action_2_per_state = [-1, -1] 

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_s1 = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        if last_action_1 != -1:
            q_net_s1[last_action_1] += stick_s1
            
        exp_q1 = np.exp(beta * q_net_s1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]

        # --- Stage 2 Policy ---
        state_idx = int(state[trial])
        q_net_s2 = q_stage2_mf[state_idx].copy()
        
        # Apply Stage 2 Stickiness (State-Dependent)
        if last_action_2_per_state[state_idx] != -1:
            q_net_s2[last_action_2_per_state[state_idx]] += stick_s2
            
        exp_q2 = np.exp(beta * q_net_s2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # --- Learning Updates ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        last_action_1 = a1
        last_action_2_per_state[state_idx] = a2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Transition-Dependent Stickiness
This model investigates if OCI interacts with the *structure* of the task rather than just the reward. It hypothesizes that high OCI individuals might exhibit different perseveration behavior (freezing or sticking) specifically after "Rare" transitions, which represent violations of the world model or high-surprise events.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Transition-Dependent Stickiness Model.
    
    Hypothesis: Stickiness (choice repetition) depends on whether the previous 
    transition was Common or Rare. High OCI individuals may react to Rare 
    transitions (model violations) with increased stickiness (rigidity).
    
    Parameters:
    - learning_rate: [0, 1] Value update rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Model-based weight.
    - stick_common: [0, 5] Stickiness after a Common transition.
    - stick_rare_base: [0, 5] Baseline stickiness after a Rare transition.
    - stick_rare_oci: [0, 5] Modulation of Rare-transition stickiness by OCI.
    """
    learning_rate, beta, w, stick_common, stick_rare_base, stick_rare_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    last_state = -1

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_s1 = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Apply Transition-Dependent Stickiness
        if last_action_1 != -1:
            # Determine if previous transition was Common or Rare
            # Action 0 -> State 0 is Common
            # Action 1 -> State 1 is Common
            is_common = (last_action_1 == 0 and last_state == 0) or \
                        (last_action_1 == 1 and last_state == 1)
            
            if is_common:
                bonus = stick_common
            else:
                bonus = stick_rare_base + (stick_rare_oci * oci_score)
                
            q_net_s1[last_action_1] += bonus
            
        exp_q1 = np.exp(beta * q_net_s1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]

        # --- Stage 2 Policy ---
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # --- Learning Updates ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        last_action_1 = a1
        last_state = state_idx

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```