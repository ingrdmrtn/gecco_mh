Here are three new cognitive models implemented as Python functions.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Forgetting (Decay).
    Hypothesis: OCI score modulates the decay rate of unchosen action values.
    High OCI might lead to 'stickier' memory (lower decay) or obsessive updating (higher decay).
    This model introduces a decay term for unvisited states and unchosen actions.
    
    Parameters:
    - learning_rate: [0, 1] Standard learning rate for chosen actions.
    - beta: [0, 10] Inverse temperature for softmax choice.
    - w: [0, 1] Weight for Model-Based control (0=MF, 1=MB).
    - decay_base: [0, 1] Base decay rate for unchosen options.
    - oci_decay_mod: [-1, 1] Modification of decay rate by OCI score.
    """
    learning_rate, beta, w, decay_base, oci_decay_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective decay rate
    decay = decay_base + (oci_score * oci_decay_mod)
    # Clip decay to valid range [0, 1]
    if decay < 0: decay = 0
    if decay > 1: decay = 1
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2) 
    q_stage2_mf = np.zeros((2, 2)) 

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based Value: T * max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Value: Hybrid of MB and MF
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # --- Stage 2 Policy ---
        s_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Updates ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Decay unchosen Stage 2 values for the current state
        unchosen_a2 = 1 - a2
        q_stage2_mf[s_idx, unchosen_a2] *= (1 - decay)
        
        # Decay all values for the unvisited state
        unvisited_state = 1 - s_idx
        q_stage2_mf[unvisited_state, :] *= (1 - decay)

        # Stage 2 Update (TD)
        delta_2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_2
        
        # Stage 1 Update (TD)
        delta_1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_1
        
        # Decay unchosen Stage 1 value
        unchosen_a1 = 1 - a1
        q_stage1_mf[unchosen_a1] *= (1 - decay)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Modulated Subjective Transition Beliefs.
    Hypothesis: High OCI participants may hold distorted beliefs about the transition structure.
    They might perceive the 'common' transition as more deterministic than it is 
    (intolerance of uncertainty), leading to stronger MB reactions to 'rare' events.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weight for Model-Based control.
    - p_common_base: [0.5, 1] Base subjective probability of common transition.
    - oci_p_mod: [-0.5, 0.5] Modification of p_common by OCI score.
    """
    learning_rate, beta, w, p_common_base, oci_p_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate subjective transition probability
    p_common = p_common_base + (oci_score * oci_p_mod)
    
    # Clip to valid probability range (assuming common is always > 0.5)
    if p_common < 0.5: p_common = 0.5
    if p_common > 0.99: p_common = 0.99
    
    # Construct subjective transition matrix used for MB planning
    # T[0,0] is common (A->X), T[1,1] is common (U->Y)
    transition_matrix = np.array([[p_common, 1-p_common], [1-p_common, p_common]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        
        # Use subjective matrix for MB planning
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Net Value
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # --- Stage 2 Policy ---
        s_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Updates ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Stage 2 Update
        delta_2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_2
        
        # Stage 1 Update (MF)
        delta_1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Second Stage Learning Rate.
    Hypothesis: OCI specificially modulates how quickly participants update values 
    for the specific aliens (Stage 2), distinct from their habit formation at Stage 1.
    High OCI might lead to hyper-sensitivity (or insensitivity) to immediate alien feedback.
    
    Parameters:
    - lr_s1: [0, 1] Learning rate for Stage 1 (Spaceships).
    - lr_s2_base: [0, 1] Base learning rate for Stage 2 (Aliens).
    - oci_lr2_mod: [-1, 1] Modification of Stage 2 LR by OCI.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weight for Model-Based control.
    """
    lr_s1, lr_s2_base, oci_lr2_mod, beta, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective Stage 2 learning rate
    lr_s2 = lr_s2_base + (oci_score * oci_lr2_mod)
    if lr_s2 < 0: lr_s2 = 0
    if lr_s2 > 1: lr_s2 = 1
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # --- Stage 2 Policy ---
        s_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Updates ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Stage 2 Update (Uses OCI-modulated rate)
        delta_2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr_s2 * delta_2
        
        # Stage 1 Update (Uses standard rate)
        delta_1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr_s1 * delta_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```