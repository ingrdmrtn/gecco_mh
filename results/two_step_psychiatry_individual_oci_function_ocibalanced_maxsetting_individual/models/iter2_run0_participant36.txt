Here are three new cognitive models that incorporate OCI scores to explain the participant's behavior, specifically focusing on the observed "stickiness" or rigidity in their choices.

### Model 1: Dual Learning Rate with OCI-Mediated Stage 1 Suppression
This model hypothesizes that high OCI scores are associated with a specific rigidity in high-level planning (Stage 1 choices), while lower-level associations (Stage 2/Alien choices) remain flexible. The model implements this by scaling down the learning rate for Stage 1 based on the OCI score. A high suppression factor means the participant updates their spaceship preference very slowly, leading to the observed long streaks of identical choices.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Dual Learning Rate model where OCI suppresses Stage 1 learning.
    
    Hypothesis: High OCI participants exhibit rigidity in their high-level plans (Stage 1),
    updating these values slower than the immediate associations (Stage 2).
    
    Parameters:
    - lr_stage2: [0, 1] Base learning rate for Stage 2 (Alien choice).
    - beta: [0, 10] Inverse temperature (global) for softmax.
    - w: [0, 1] Weight for Model-Based values (0=MF, 1=MB).
    - lambda_coeff: [0, 1] Eligibility trace decay.
    - lr_1_suppression: [0, 1] Factor by which OCI reduces Stage 1 learning rate.
      lr_stage1 = lr_stage2 * (1 - oci * lr_1_suppression).
    """
    lr_stage2, beta, w, lambda_coeff, lr_1_suppression = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate Stage 1 learning rate based on OCI
    # High OCI -> Lower lr_stage1 -> Slower updating of spaceship values -> Persistence
    lr_stage1 = lr_stage2 * (1.0 - (oci_score * lr_1_suppression))
    lr_stage1 = max(0.0, lr_stage1)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        logits_1 = beta * q_net_1
        logits_1 = logits_1 - np.max(logits_1)
        probs_1 = np.exp(logits_1) / np.sum(np.exp(logits_1))
        
        a1 = int(action_1[trial])
        if a1 != -1:
            p_choice_1[trial] = probs_1[a1]
        else:
            p_choice_1[trial] = 0.5 # Handle missing data
            
        # --- Stage 2 Policy ---
        s_idx = int(state[trial])
        logits_2 = beta * q_stage2_mf[s_idx]
        logits_2 = logits_2 - np.max(logits_2)
        probs_2 = np.exp(logits_2) / np.sum(np.exp(logits_2))
        
        a2 = int(action_2[trial])
        if a2 != -1:
            p_choice_2[trial] = probs_2[a2]
            
            # --- Updates ---
            # Stage 1 RPE (TD error)
            delta_1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
            # Stage 2 RPE
            delta_2 = reward[trial] - q_stage2_mf[s_idx, a2]
            
            # Update Stage 1 MF using the OCI-suppressed learning rate
            q_stage1_mf[a1] += lr_stage1 * (delta_1 + lambda_coeff * delta_2)
            
            # Update Stage 2 MF using the base learning rate
            q_stage2_mf[s_idx, a2] += lr_stage2 * delta_2
        else:
            p_choice_2[trial] = 0.5

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Split Beta with OCI-Driven Stage 1 Determinism
This model posits that the "stickiness" is due to hyper-decisiveness (exploitation) in the initial choice, rather than a learning deficit. It separates the inverse temperature ($\beta$) for Stage 1 and Stage 2. OCI increases $\beta$ specifically for Stage 1, making the spaceship choice nearly deterministic (hard max) based on current values, while the alien choice remains more probabilistic.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Split Beta Model with OCI-modulated Stage 1 Determinism.
    
    Hypothesis: High OCI participants are hyper-decisive or rigid in their primary choice (Spaceship),
    represented by a higher inverse temperature for Stage 1, while Stage 2 remains flexible.
    
    Parameters:
    - learning_rate: [0, 1] Standard learning rate.
    - w: [0, 1] MB/MF mixing weight.
    - lambda_coeff: [0, 1] Eligibility trace.
    - beta_stage2: [0, 10] Inverse temperature for alien choice (Stage 2).
    - beta_stage1_base: [0, 5] Base inverse temperature for spaceship choice (Stage 1).
    - beta_stage1_oci_slope: [0, 5] Increase in Stage 1 beta per unit OCI.
    """
    learning_rate, w, lambda_coeff, beta_stage2, beta_stage1_base, beta_stage1_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate Stage 1 specific beta
    beta_stage1 = beta_stage1_base + (oci_score * beta_stage1_oci_slope)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        # --- Stage 1 ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_1 = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Use OCI-modulated Beta for Stage 1
        logits_1 = beta_stage1 * q_net_1
        logits_1 = logits_1 - np.max(logits_1)
        probs_1 = np.exp(logits_1) / np.sum(np.exp(logits_1))
        
        a1 = int(action_1[trial])
        if a1 != -1:
            p_choice_1[trial] = probs_1[a1]
        else:
            p_choice_1[trial] = 0.5

        # --- Stage 2 ---
        s_idx = int(state[trial])
        # Use independent Beta for Stage 2
        logits_2 = beta_stage2 * q_stage2_mf[s_idx]
        logits_2 = logits_2 - np.max(logits_2)
        probs_2 = np.exp(logits_2) / np.sum(np.exp(logits_2))
        
        a2 = int(action_2[trial])
        if a2 != -1:
            p_choice_2[trial] = probs_2[a2]
            
            delta_1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
            delta_2 = reward[trial] - q_stage2_mf[s_idx, a2]
            
            q_stage1_mf[a1] += learning_rate * (delta_1 + lambda_coeff * delta_2)
            q_stage2_mf[s_idx, a2] += learning_rate * delta_2
        else:
            p_choice_2[trial] = 0.5

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Counterfactual Decay (Forgetting)
This model proposes that high OCI participants obsessively focus on their chosen path and "forget" or devalue the unchosen option. In each trial, the value of the unchosen spaceship decays towards zero. The rate of this decay is scaled by the OCI score. This creates a widening gap between the chosen (maintained) and unchosen (decayed) options, resulting in strong perseveration.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Counterfactual Decay Model modulated by OCI.
    
    Hypothesis: High OCI leads to an "obsessive" focus on the chosen option and a 
    rapid devaluation (forgetting) of the unchosen option's value, reinforcing the current choice.
    
    Parameters:
    - learning_rate: [0, 1] Standard learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] MB/MF mixing weight.
    - lambda_coeff: [0, 1] Eligibility trace.
    - decay_base: [0, 1] Base decay rate for unchosen options.
    - decay_oci_scale: [0, 1] Additional decay scaled by OCI.
    """
    learning_rate, beta, w, lambda_coeff, decay_base, decay_oci_scale = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate decay rate
    decay = decay_base + (oci_score * decay_oci_scale)
    decay = np.clip(decay, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        # --- Stage 1 ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_1 = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        logits_1 = beta * q_net_1
        logits_1 = logits_1 - np.max(logits_1)
        probs_1 = np.exp(logits_1) / np.sum(np.exp(logits_1))
        
        a1 = int(action_1[trial])
        if a1 != -1:
            p_choice_1[trial] = probs_1[a1]
        else:
            p_choice_1[trial] = 0.5
            
        # --- Stage 2 ---
        s_idx = int(state[trial])
        logits_2 = beta * q_stage2_mf[s_idx]
        logits_2 = logits_2 - np.max(logits_2)
        probs_2 = np.exp(logits_2) / np.sum(np.exp(logits_2))
        
        a2 = int(action_2[trial])
        if a2 != -1:
            p_choice_2[trial] = probs_2[a2]
            
            delta_1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
            delta_2 = reward[trial] - q_stage2_mf[s_idx, a2]
            
            q_stage1_mf[a1] += learning_rate * (delta_1 + lambda_coeff * delta_2)
            q_stage2_mf[s_idx, a2] += learning_rate * delta_2
            
            # Decay the unchosen Stage 1 option towards 0
            unchosen_a1 = 1 - a1
            q_stage1_mf[unchosen_a1] *= (1.0 - decay)
            
        else:
            p_choice_2[trial] = 0.5

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```