Here are three new cognitive models that explore different mechanisms for how Obsessive-Compulsive Inventory (OCI) scores might influence decision-making in the two-step task. These models focus on how high OCI scores (like this participant's 0.733) might manifest as rigid habits, altered learning rates, or specific biases in model-based vs. model-free control.

### Cognitive Model 1: OCI-Modulated Habit Strength (The "Compulsive Habit" Model)
This model posits that high OCI scores lead to stronger habit formation. Instead of just "stickiness" (repeating the last action), this model implements a separate habit strength variable that accumulates over time when actions are repeated, making it harder to switch away from established choices. The OCI score scales the rate at which this habit strength grows.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Habit Strength.
    Hypothesis: High OCI accelerates the formation of habits. 
    A separate 'habit' trace accumulates for chosen actions. The growth rate
    of this habit trace is scaled by the OCI score.
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    w: [0, 1] (Mixing weight)
    habit_growth_rate: [0, 1] (Base rate of habit accumulation)
    """
    learning_rate, beta, w, habit_growth_rate = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Habit growth is faster for higher OCI
    effective_habit_rate = habit_growth_rate * (1.0 + oci_score)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    habit_values = np.zeros(2) # Habit strength for stage 1 actions
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        if a1 == -1: a1 = 0
        s_idx = int(state[trial])
        if s_idx == -1: s_idx = 0
        a2 = int(action_2[trial])
        if a2 == -1: a2 = 0
        r = reward[trial]
        if r == -1: r = 0

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combine Model-Based, Model-Free, and Habit
        # Habit is treated as an additive bias to the value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf + habit_values

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Learning & Updates
        # Standard Q-learning for Stage 2
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Standard Q-learning for Stage 1 MF
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Habit Update: The chosen action's habit strength increases
        # The unchosen action's habit strength decays (simple model)
        habit_values[a1] += effective_habit_rate * (1 - habit_values[a1])
        habit_values[1-a1] *= (1 - effective_habit_rate)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2: OCI-Driven Learning Rate Asymmetry (The "Rigid Beliefs" Model)
This model suggests that high OCI scores create a resistance to unlearning established values. Specifically, the learning rate for negative prediction errors (disappointments) is blunted by high OCI, making the agent "stubborn" or less likely to devalue a choice once it has been established as good.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Driven Learning Rate Asymmetry.
    Hypothesis: High OCI makes beliefs rigid. The agent learns from positive
    outcomes normally, but the learning rate for negative prediction errors 
    (alpha_neg) is reduced as OCI increases, leading to persistence.
    
    Bounds:
    alpha_pos: [0, 1] (Learning rate for positive PE)
    beta: [0, 10]
    w: [0, 1]
    rigidity: [0, 1] (How much OCI dampens negative learning)
    """
    alpha_pos, beta, w, rigidity = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate negative learning rate:
    # If rigidity is high and OCI is high, alpha_neg becomes very small compared to alpha_pos.
    alpha_neg = alpha_pos * (1.0 - (rigidity * oci_score))
    # Ensure non-negative
    if alpha_neg < 0: alpha_neg = 0.0
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        if a1 == -1: a1 = 0
        s_idx = int(state[trial])
        if s_idx == -1: s_idx = 0
        a2 = int(action_2[trial])
        if a2 == -1: a2 = 0
        r = reward[trial]
        if r == -1: r = 0

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Learning Stage 2
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        eff_alpha2 = alpha_pos if delta_stage2 >= 0 else alpha_neg
        q_stage2_mf[s_idx, a2] += eff_alpha2 * delta_stage2
        
        # Learning Stage 1
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        eff_alpha1 = alpha_pos if delta_stage1 >= 0 else alpha_neg
        q_stage1_mf[a1] += eff_alpha1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3: OCI-Based Model-Based Suppression (The "Stress-Induced MF" Model)
This model tests the hypothesis that high OCI (associated with anxiety/stress) reduces the cognitive resources available for Model-Based (planning) control. Instead of `w` being a free parameter directly, `w` is derived from a baseline capacity minus an OCI-dependent penalty.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Based Model-Based Suppression.
    Hypothesis: High OCI impairs model-based control (planning).
    The mixing weight 'w' is not a free parameter but is a function of 
    a baseline capacity and an OCI-dependent penalty.
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    w_max: [0, 1] (Maximum possible model-based weight)
    oci_penalty: [0, 1] (How strongly OCI reduces w)
    """
    learning_rate, beta, w_max, oci_penalty = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective w
    # As OCI increases, w decreases from w_max
    w_effective = w_max * (1.0 - (oci_penalty * oci_score))
    if w_effective < 0: w_effective = 0.0
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        if a1 == -1: a1 = 0
        s_idx = int(state[trial])
        if s_idx == -1: s_idx = 0
        a2 = int(action_2[trial])
        if a2 == -1: a2 = 0
        r = reward[trial]
        if r == -1: r = 0

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w_effective * q_stage1_mb + (1 - w_effective) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Learning
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```