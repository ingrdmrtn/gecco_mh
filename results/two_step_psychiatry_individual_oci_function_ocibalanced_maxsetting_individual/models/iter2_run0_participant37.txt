Here are three cognitive models designed to explain the participant's behavior, incorporating their OCI score into the computational logic.

### Model 1: Hybrid MB/MF with OCI-Modulated Rigidity
This model builds on the insight that the participant exhibits rigid, repetitive choices. It posits a Hybrid Model-Based/Model-Free architecture where the OCI score specifically modulates the **inverse temperature (beta)** of the first-stage choice. High OCI leads to a higher effective beta (lower temperature), resulting in more deterministic (rigid) choices, masking potentially goal-directed (MB) computations.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model where OCI modulates Stage 1 Inverse Temperature (Rigidity).
    
    Hypothesis: High OCI increases rigidity (beta_1) within a Hybrid architecture.
    The OCI score scales the base beta parameter, making choices more deterministic 
    (sticky/compulsive) regardless of the underlying value (MB or MF).
    
    beta_1 = beta_base * (1 + OCI)
    
    Parameters:
    - learning_rate: [0,1] Value update rate.
    - beta_base: [0,10] Base inverse temperature for Stage 1.
    - beta_2: [0,10] Inverse temperature for Stage 2.
    - lam: [0,1] Eligibility trace for MF updates.
    - w: [0,1] Weight for Model-Based values (0=MF, 1=MB).
    - stickiness: [0,10] Choice persistence bonus.
    """
    learning_rate, beta_base, beta_2, lam, w, stickiness = model_parameters
    oci_score = oci[0]
    n_trials = len(action_1)
    
    # OCI modulation: High OCI -> High Beta (Rigid)
    beta_1 = beta_base * (1.0 + oci_score)
    
    # Initialize values
    q_mf_1 = np.zeros(2)
    q_mf_2 = np.zeros((2, 2))
    
    # Transition matrix for MB: 0->X(0.7), 1->Y(0.7)
    trans_probs = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    log_loss = 0.0
    eps = 1e-10
    prev_action_1 = -1
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Skip missing data
        if a1 < 0 or s_idx < 0 or a2 < 0:
            continue
            
        # --- Stage 1 Choice ---
        # Model-Based Values: V(s') = max Q_MF_2(s')
        v_stage2 = np.max(q_mf_2, axis=1)
        q_mb_1 = trans_probs @ v_stage2
        
        # Hybrid Net Value
        q_net_1 = w * q_mb_1 + (1 - w) * q_mf_1
        
        # Stickiness
        if prev_action_1 != -1:
            q_net_1[prev_action_1] += stickiness
            
        # Softmax with OCI-modulated Beta
        exp_q1 = np.exp(beta_1 * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + eps)
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta_2 * q_mf_2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[a2] + eps)
        
        # --- Updates ---
        # Stage 1 PE
        delta_1 = q_mf_2[s_idx, a2] - q_mf_1[a1]
        q_mf_1[a1] += learning_rate * delta_1
        
        # Stage 2 PE
        delta_2 = r - q_mf_2[s_idx, a2]
        q_mf_2[s_idx, a2] += learning_rate * delta_2
        
        # Eligibility Trace
        q_mf_1[a1] += learning_rate * lam * delta_2
        
        prev_action_1 = a1
        
    return log_loss
```

### Model 2: Pure Model-Free with OCI-Modulated Forgetting
This model hypothesizes that compulsivity (high OCI) involves a deficit in updating or maintaining alternative options. Specifically, it introduces a **forgetting/decay rate** for unchosen options that is proportional to OCI. High OCI participants "forget" the value of options they don't choose, causing the unchosen option's value to decay to zero, thereby reinforcing the bias to stick with the currently chosen (and potentially rewarding) option.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Pure Model-Free model with OCI-modulated Forgetting of Unchosen Options.
    
    Hypothesis: High OCI drives a 'pruning' or decay of unchosen options.
    The value of unchosen actions decays at a rate determined by OCI, 
    reinforcing the contrast between the chosen habit and alternatives.
    
    decay = forget_base * OCI
    
    Parameters:
    - learning_rate: [0,1] Update rate for chosen options.
    - beta_1: [0,10] Inverse temperature Stage 1.
    - beta_2: [0,10] Inverse temperature Stage 2.
    - lam: [0,1] Eligibility trace.
    - forget_base: [0,1] Base scaling for forgetting rate.
    - stickiness: [0,10] Choice stickiness.
    """
    learning_rate, beta_1, beta_2, lam, forget_base, stickiness = model_parameters
    oci_score = oci[0]
    n_trials = len(action_1)
    
    # Decay rate depends on OCI
    decay = forget_base * oci_score
    if decay > 1.0: decay = 1.0
    
    q_mf_1 = np.zeros(2)
    q_mf_2 = np.zeros((2, 2))
    
    log_loss = 0.0
    eps = 1e-10
    prev_action_1 = -1
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        if a1 < 0 or s_idx < 0 or a2 < 0:
            continue
            
        # Stage 1 Choice
        q_net_1 = q_mf_1.copy()
        if prev_action_1 != -1:
            q_net_1[prev_action_1] += stickiness
            
        exp_q1 = np.exp(beta_1 * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + eps)
        
        # Stage 2 Choice
        exp_q2 = np.exp(beta_2 * q_mf_2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[a2] + eps)
        
        # Updates
        # Stage 1 Chosen
        delta_1 = q_mf_2[s_idx, a2] - q_mf_1[a1]
        q_mf_1[a1] += learning_rate * delta_1
        
        # Stage 1 Unchosen: Decay
        unchosen_a1 = 1 - a1
        q_mf_1[unchosen_a1] *= (1.0 - decay)
        
        # Stage 2 Chosen
        delta_2 = r - q_mf_2[s_idx, a2]
        q_mf_2[s_idx, a2] += learning_rate * delta_2
        
        # Stage 2 Unchosen: Decay
        unchosen_a2 = 1 - a2
        q_mf_2[s_idx, unchosen_a2] *= (1.0 - decay)
        
        # Lambda update
        q_mf_1[a1] += learning_rate * lam * delta_2
        
        prev_action_1 = a1
        
    return log_loss
```

### Model 3: Hybrid MB/MF with OCI-Enhanced Habit Trace
This model investigates if OCI specifically strengthens the **eligibility trace (lambda)** within a Hybrid architecture. A higher lambda implies that the first-stage choice is more strongly reinforced by the final outcome (Model-Free mechanism), bypassing the state-by-state value estimation. This tests if high OCI participants are more prone to forming direct Stimulus-Response-Outcome chains (habits) even while possessing a Model-Based system.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model where OCI modulates the Eligibility Trace (Lambda).
    
    Hypothesis: High OCI enhances the formation of habits by increasing the 
    eligibility trace (lambda). This makes Stage 1 choices more sensitive to 
    direct Stage 2 outcomes (Model-Free reinforcement) rather than relying 
    solely on the Model-Based structure or value of the second stage state.
    
    lambda = lam_base * (1 + OCI) (clipped at 1.0)
    
    Parameters:
    - learning_rate: [0,1]
    - beta_1: [0,10]
    - beta_2: [0,10]
    - lam_base: [0,1] Base eligibility trace.
    - w: [0,1] MB/MF weight.
    - stickiness: [0,10]
    """
    learning_rate, beta_1, beta_2, lam_base, w, stickiness = model_parameters
    oci_score = oci[0]
    n_trials = len(action_1)
    
    # OCI modulates lambda
    lam = lam_base * (1.0 + oci_score)
    if lam > 1.0: lam = 1.0
    
    q_mf_1 = np.zeros(2)
    q_mf_2 = np.zeros((2, 2))
    trans_probs = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    log_loss = 0.0
    eps = 1e-10
    prev_action_1 = -1
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        if a1 < 0 or s_idx < 0 or a2 < 0:
            continue
            
        # Stage 1 Choice (Hybrid)
        v_stage2 = np.max(q_mf_2, axis=1)
        q_mb_1 = trans_probs @ v_stage2
        
        q_net_1 = w * q_mb_1 + (1 - w) * q_mf_1
        
        if prev_action_1 != -1:
            q_net_1[prev_action_1] += stickiness
            
        exp_q1 = np.exp(beta_1 * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + eps)
        
        # Stage 2 Choice
        exp_q2 = np.exp(beta_2 * q_mf_2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[a2] + eps)
        
        # Updates
        delta_1 = q_mf_2[s_idx, a2] - q_mf_1[a1]
        q_mf_1[a1] += learning_rate * delta_1
        
        delta_2 = r - q_mf_2[s_idx, a2]
        q_mf_2[s_idx, a2] += learning_rate * delta_2
        
        # Lambda update modulated by OCI
        q_mf_1[a1] += learning_rate * lam * delta_2
        
        prev_action_1 = a1
        
    return log_loss
```