Here are three new cognitive models that incorporate the OCI score to explain the participant's decision-making behavior.

### Cognitive Model 1
```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Eligibility Trace (Lambda).
    
    Hypothesis: The OCI score modulates the eligibility trace parameter (lambda),
    which controls the credit assignment between the second-stage outcome and the 
    first-stage choice. High OCI might lead to stronger 'magical thinking' or 
    over-attribution of causality (higher lambda) regardless of the actual transition structure.
    
    Parameters:
    learning_rate: [0, 1]
    beta: [0, 10]
    w: [0, 1] - Mixing weight between Model-Based and Model-Free.
    lambda_base: [0, 1] - Baseline eligibility trace.
    lambda_oci_factor: [0, 1] - Factor scaling the effect of OCI on lambda.
                                Lambda = lambda_base + lambda_oci_factor * oci (clipped to [0,1]).
    """
    learning_rate, beta, w, lambda_base, lambda_oci_factor = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate OCI-specific lambda, clipped to valid range [0, 1]
    lambda_val = lambda_base + lambda_oci_factor * oci_score
    if lambda_val > 1.0:
        lambda_val = 1.0
    elif lambda_val < 0.0:
        lambda_val = 0.0

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # state x action

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Stage 1 Choice Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2 Choice Policy
        q_stage2 = q_stage2_mf[s_idx]
        exp_q2 = np.exp(beta * q_stage2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Updates
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        # Update Stage 1 MF values using the OCI-modulated lambda
        q_stage1_mf[a1] += learning_rate * delta_stage1 + learning_rate * lambda_val * delta_stage2
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    mask = action_1 != -1
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss
```

### Cognitive Model 2
```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Modulated Asymmetric Learning Rates.
    
    Hypothesis: High OCI participants exhibit a bias in learning from negative prediction 
    errors (disappointments) versus positive ones. This model scales the learning rate 
    for negative prediction errors based on the OCI score, reflecting increased sensitivity 
    to negative outcomes or lack of reward.
    
    Parameters:
    learning_rate: [0, 1] - Base learning rate (applied to positive PEs).
    beta: [0, 10]
    w: [0, 1]
    lambda_decay: [0, 1]
    asymmetry_oci: [0, 5] - Factor scaling the learning rate for negative PEs.
                            lr_neg = learning_rate * (1 + asymmetry_oci * oci).
    """
    learning_rate, beta, w, lambda_decay, asymmetry_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Base learning rate is for positive PEs. 
    # Negative PE learning rate is scaled by OCI.
    lr_pos = learning_rate
    lr_neg = learning_rate * (1.0 + asymmetry_oci * oci_score)
    
    # Clip to ensure stability
    if lr_neg > 1.0: lr_neg = 1.0

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Stage 1 Choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2 Choice
        q_stage2 = q_stage2_mf[s_idx]
        exp_q2 = np.exp(beta * q_stage2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Updates
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        # Determine effective learning rate based on sign of prediction error
        lr_1 = lr_pos if delta_stage1 >= 0 else lr_neg
        lr_2 = lr_pos if delta_stage2 >= 0 else lr_neg
        
        q_stage1_mf[a1] += lr_1 * delta_stage1 + lr_2 * lambda_decay * delta_stage2
        q_stage2_mf[s_idx, a2] += lr_2 * delta_stage2

    eps = 1e-10
    mask = action_1 != -1
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss
```

### Cognitive Model 3
```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Model-Based Weight with Constant Stickiness.
    
    Hypothesis: High OCI correlates with a deficit in Model-Based control (reduced w),
    modeled as an exponential decay of w with increasing OCI. Additionally, the model 
    includes a constant stickiness parameter (independent of OCI) to account for 
    general choice perseveration observed in the data.
    
    Parameters:
    learning_rate: [0, 1]
    beta: [0, 10]
    w_max: [0, 1] - Maximum model-based weight (theoretical w at OCI=0).
    w_oci_decay: [0, 5] - Rate at which w decays with OCI. w = w_max * exp(-w_oci_decay * oci).
    lambda_decay: [0, 1]
    stickiness: [0, 5] - Constant bonus added to the value of the previously chosen action.
    """
    learning_rate, beta, w_max, w_oci_decay, lambda_decay, stickiness = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate OCI-modulated Model-Based weight
    w = w_max * np.exp(-w_oci_decay * oci_score)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_a1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Stage 1 Choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Apply constant stickiness bonus
        stick_vec = np.zeros(2)
        if prev_a1 != -1:
            stick_vec[prev_a1] = stickiness
            
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf + stick_vec
        
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        prev_a1 = a1

        # Stage 2 Choice
        q_stage2 = q_stage2_mf[s_idx]
        exp_q2 = np.exp(beta * q_stage2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Updates
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        q_stage1_mf[a1] += learning_rate * delta_stage1 + learning_rate * lambda_decay * delta_stage2
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    mask = action_1 != -1
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss
```