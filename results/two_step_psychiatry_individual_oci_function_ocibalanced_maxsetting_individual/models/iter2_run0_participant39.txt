Here are the three proposed cognitive models.

### Model 1: Model-Based with OCI-Distorted Transition Beliefs
This model assumes that the participant uses a Model-Based strategy (calculating Stage 1 values based on Stage 2 values and a transition matrix), but their belief about the transition structure is distorted by their OCI score. High OCI might lead to "superstitious" beliefs where the world is perceived as more deterministic (or less) than it actually is, affecting how values propagate from the second stage to the first.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Based RL where OCI distorts the perceived transition matrix.
    
    The participant calculates Stage 1 values using a transition matrix T.
    Instead of the fixed real-world probabilities (0.7/0.3), the participant's 
    belief about the common transition probability is modulated by OCI.
    
    Parameters:
    - learning_rate: [0, 1] Update rate for Stage 2 values.
    - beta: [0, 10] Inverse temperature for softmax choice.
    - belief_logit_base: [0, 10] Base logit for the common transition probability.
    - belief_oci_sens: [0, 10] How strongly OCI affects the transition belief.
    """
    learning_rate, beta, belief_logit_base, belief_oci_sens = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]

    # Calculate subjective transition probability (Sigmoid transform)
    # This determines how "common" the common transition is perceived to be.
    logit = belief_logit_base + (participant_oci * belief_oci_sens)
    # Clip logit to prevent overflow
    logit = np.clip(logit, -20, 20)
    p_common_belief = 1.0 / (1.0 + np.exp(-logit))
    
    # Construct the subjective transition matrix
    # Row 0: Space A -> [Planet X, Planet Y]
    # Row 1: Space B -> [Planet X, Planet Y]
    # Assuming A->X and B->Y are common.
    transition_matrix = np.array([
        [p_common_belief, 1.0 - p_common_belief], 
        [1.0 - p_common_belief, p_common_belief]
    ])

    # Initialize values
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Decision (Model-Based) ---
        # Value of a state is the max Q-value available in that state
        v_stage2 = np.max(q_stage2_mf, axis=1)
        
        # Bellman equation: Q(s1, a1) = Sum(P(s2|s1,a1) * V(s2))
        q_stage1_mb = transition_matrix @ v_stage2
        
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Decision (Model-Free) ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning (Stage 2 Only) ---
        # In pure MB, we typically only learn Stage 2 values from reward
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid WSLS-RL with OCI Arbitration
This model posits that decision-making is a mixture of a simple heuristic (Win-Stay, Lose-Shift) and a standard Reinforcement Learning process. The OCI score determines the "mixing weight" between these two strategies. A high OCI score might indicate a more rigid adherence to the heuristic rule (or conversely, the RL value-based system), capturing the balance between compulsive rule-following and flexible learning.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Win-Stay Lose-Shift (WSLS) and Reinforcement Learning model.
    
    The final policy is a weighted mixture of a pure WSLS probability and 
    a Model-Free RL probability. The weight is determined by OCI.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for the RL component.
    - beta: [0, 10] Inverse temperature for the RL component.
    - w_base: [0, 1] Base weight for the WSLS component (0=Pure RL, 1=Pure WSLS).
    - w_oci_mod: [0, 1] Modification of the weight by OCI.
    """
    learning_rate, beta, w_base, w_oci_mod = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]

    # Calculate mixing weight w (bounded 0-1)
    # w is the probability of using the WSLS heuristic
    w_wsls = w_base + (participant_oci * w_oci_mod)
    w_wsls = np.clip(w_wsls, 0.0, 1.0)
    w_rl = 1.0 - w_wsls

    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1
    prev_reward = 0

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- RL Component Probabilities ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_rl_1 = exp_q1 / np.sum(exp_q1)
        
        # --- WSLS Component Probabilities ---
        # Soft WSLS: High prob to stay if win, low if lose
        probs_wsls_1 = np.array([0.5, 0.5]) # Default for first trial
        if prev_a1 != -1:
            if prev_reward == 1:
                # Win-Stay
                probs_wsls_1[prev_a1] = 0.99
                probs_wsls_1[1 - prev_a1] = 0.01
            else:
                # Lose-Shift
                probs_wsls_1[prev_a1] = 0.01
                probs_wsls_1[1 - prev_a1] = 0.99
        
        # --- Mixture ---
        final_probs_1 = (w_wsls * probs_wsls_1) + (w_rl * probs_rl_1)
        p_choice_1[trial] = final_probs_1[a1]

        # --- Stage 2 (Pure RL) ---
        # WSLS usually applies to the top-level choice. Stage 2 is modeled as RL.
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- RL Updating ---
        # Update Stage 2
        delta_stage2 = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += learning_rate * delta_stage2
        
        # Update Stage 1 (TD(0) / MF)
        delta_stage1 = q_stage2[s_idx, a2] - q_stage1[a1]
        q_stage1[a1] += learning_rate * delta_stage1

        prev_a1 = a1
        prev_reward = r

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

```

### Model 3: Stage-Specific Learning Modulation
This model investigates if OCI specifically impacts the credit assignment for the first-stage choice versus the second-stage choice. It implements a standard Model-Free RL but splits the learning rate into two components. The Stage 2 learning rate is the baseline, and OCI modulates the ratio between the Stage 1 learning rate and the Stage 2 learning rate. This tests if high OCI leads to a disconnect between immediate outcomes and initial choices.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free RL with OCI-modulated Stage 1 Learning Rate.
    
    This model allows the learning rate for the first decision (Space Choice)
    to differ from the second decision (Alien Choice), scaled by OCI.
    High OCI might impair the ability to backpropagate value to the start state.
    
    Parameters:
    - lr_stage2: [0, 1] Base learning rate for Stage 2 (Alien choice).
    - beta: [0, 10] Inverse temperature.
    - ratio_base: [0, 2] Base ratio of LR_stage1 / LR_stage2.
    - ratio_oci_sens: [-2, 2] How OCI affects this ratio.
    """
    lr_stage2, beta, ratio_base, ratio_oci_sens = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]

    # Calculate Stage 1 Learning Rate
    # lr1 = lr2 * (base + oci * sens)
    ratio = ratio_base + (participant_oci * ratio_oci_sens)
    # Ensure ratio is non-negative and lr1 doesn't exceed 1 (soft clip)
    ratio = max(0.0, ratio)
    lr_stage1 = min(1.0, lr_stage2 * ratio)

    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Stage 1 Choice
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2 Choice
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Value Updating
        
        # Stage 2 Update (using lr_stage2)
        delta_stage2 = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += lr_stage2 * delta_stage2
        
        # Stage 1 Update (using OCI-modulated lr_stage1)
        # We use the updated Q-value of stage 2 as the target (SARSA-like logic)
        # or just the value of the chosen state-action pair.
        delta_stage1 = q_stage2[s_idx, a2] - q_stage1[a1]
        q_stage1[a1] += lr_stage1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```