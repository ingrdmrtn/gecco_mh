Here are three cognitive models proposed to explain the participant's behavior, incorporating their OCI score into different mechanisms of the decision-making process.

### Model 1: Asymmetric Learning Rates (Valence-Dependent)
This model hypothesizes that the participant's obsessive-compulsive symptoms (OCI) modulate how they learn from negative prediction errors (e.g., omitted rewards) versus positive ones. High OCI might be associated with an altered sensitivity to "failure" or missing out, leading to a distinct learning rate for negative outcomes.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Asymmetric Learning Rates with OCI modulation on Negative Learning Rate.
    
    Hypothesis:
    Participants update their value estimates differently for positive vs negative prediction errors.
    High OCI scores modulate the learning rate specifically for negative outcomes (delta <= 0),
    reflecting altered sensitivity to omitted rewards or punishments.
    
    Parameters:
    beta: [0, 10] - Inverse temperature (softmax randomness).
    w: [0, 1] - Model-Based / Model-Free mixing weight.
    stickiness: [-5, 5] - General choice perseveration bonus.
    alpha_pos: [0, 1] - Learning rate for positive prediction errors (delta > 0).
    alpha_neg_base: [0, 1] - Base learning rate for negative prediction errors.
    alpha_neg_oci: [-2, 2] - Scaling of negative learning rate by OCI score.
    """
    beta, w, stickiness, alpha_pos, alpha_neg_base, alpha_neg_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate OCI-modulated negative learning rate and clip to valid range
    alpha_neg = alpha_neg_base + alpha_neg_oci * oci_score
    alpha_neg = np.clip(alpha_neg, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        last_action_1 = a1
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        # Calculate Prediction Errors
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        # Update Stage 1 (using accumulated error, effectively lambda=1 for MF)
        delta_total = delta_stage1 + delta_stage2
        lr_1 = alpha_pos if delta_total > 0 else alpha_neg
        q_stage1_mf[a1] += lr_1 * delta_total
        
        # Update Stage 2
        lr_2 = alpha_pos if delta_stage2 > 0 else alpha_neg
        q_stage2_mf[s_idx, a2] += lr_2 * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Stage-Dependent Decision Noise
This model proposes that OCI affects the "decisiveness" (inverse temperature $\beta$) of the high-level spaceship choice (Stage 1) differently than the low-level alien choice (Stage 2). High OCI might lead to more rigid, deterministic choices at the planning stage (Stage 1), while the reaction to aliens (Stage 2) remains standard.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Stage-Dependent Beta with OCI Modulation on Stage 1.
    
    Hypothesis:
    OCI affects the exploration/exploitation balance specifically at the spaceship choice level (Stage 1).
    High OCI scores may lead to higher beta (more deterministic/rigid) behavior in the primary choice,
    independent of the learning process.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate for value updates.
    w: [0, 1] - MB/MF mixing weight.
    stickiness: [-5, 5] - General choice perseveration.
    beta_stage2: [0, 10] - Inverse temperature for Stage 2 (Alien choice).
    beta_stage1_base: [0, 10] - Base inverse temperature for Stage 1.
    beta_stage1_oci: [-5, 5] - Modulation of Stage 1 beta by OCI.
    """
    learning_rate, w, stickiness, beta_stage2, beta_stage1_base, beta_stage1_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate Stage 1 Beta and ensure non-negative
    beta_stage1 = beta_stage1_base + beta_stage1_oci * oci_score
    beta_stage1 = np.maximum(0.0, beta_stage1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        # Use Stage 1 specific Beta
        exp_q1 = np.exp(beta_stage1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        last_action_1 = a1
        
        # --- Stage 2 Policy ---
        # Use Stage 2 specific Beta
        exp_q2 = np.exp(beta_stage2 * q_stage2_mf[s_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        # Update MF values (Assuming lambda=1 for Stage 1 as in standard Two-Step MF)
        q_stage1_mf[a1] += learning_rate * (delta_stage1 + delta_stage2)
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Reward-Dependent Stickiness
This model separates "perseveration" into a general habit component and a reward-driven "Win-Stay" component. It hypothesizes that OCI modulates the *reward-dependent* aspect of stickiness. High OCI participants might persist in choices regardless of the outcome (low reward sensitivity in stickiness) or might be hyper-sensitive to confirmation by reward.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Reward-Dependent Stickiness with OCI Modulation.
    
    Hypothesis:
    Stickiness is composed of a general habit (repetition) and a 'Win-Stay' component (repetition if rewarded).
    OCI modulates the strength of the 'Win-Stay' component specifically.
    This distinguishes between 'compulsive' repetition (general stickiness) and 'reinforced' repetition.
    
    Parameters:
    learning_rate: [0, 1]
    beta: [0, 10]
    w: [0, 1]
    st_general: [-5, 5] - Base stickiness regardless of previous outcome.
    st_reward_base: [-5, 5] - Base bonus added to stickiness if previous trial was rewarded.
    st_reward_oci: [-5, 5] - Modulation of the reward-dependent stickiness bonus by OCI.
    """
    learning_rate, beta, w, st_general, st_reward_base, st_reward_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate the reward-dependent component of stickiness modulated by OCI
    st_reward_effect = st_reward_base + st_reward_oci * oci_score
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    last_reward = 0
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_action_1 != -1:
            # Base stickiness
            bonus = st_general
            # Add reward-dependent bonus if last trial was a "Win" (Reward > 0)
            if last_reward > 0:
                bonus += st_reward_effect
            
            q_net[last_action_1] += bonus
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        last_action_1 = a1
        last_reward = r
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        q_stage1_mf[a1] += learning_rate * (delta_stage1 + delta_stage2)
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```