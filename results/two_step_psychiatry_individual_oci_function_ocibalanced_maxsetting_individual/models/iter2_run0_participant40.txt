Here are three new cognitive models designed to explain the participant's behavior, particularly the high degree of perseveration (stickiness) associated with their high OCI score.

### Model 1: Accumulated Stickiness Model
This model hypothesizes that high OCI individuals exhibit "compulsive" perseveration that builds up over time, rather than just repeating the immediately preceding choice. Stickiness is modeled as a decaying memory trace of past choices. The OCI score modulates the **weight** of this accumulated trace.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Accumulated Stickiness Model.
    Stickiness is modeled as a choice trace that accumulates and decays over trials.
    OCI modulates the weight (influence) of this accumulated stickiness on the decision.
    High OCI leads to a stronger 'habit' force that builds up, explaining long streaks.

    Parameters:
    - learning_rate: [0, 1] Rate of value updating.
    - beta: [0, 10] Inverse temperature (softness of choice).
    - w: [0, 1] Mixing weight (0 = pure MF, 1 = pure MB).
    - stick_weight_base: [0, 5] Baseline weight of the stickiness trace.
    - stick_weight_oci: [0, 5] Additional weight scaled by OCI score.
    - stick_decay: [0, 1] Decay rate of the choice trace (0 = 1-back, 1 = perfect memory).
    """
    learning_rate, beta, w, stick_weight_base, stick_weight_oci, stick_decay = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Stickiness weight increases with OCI
    stick_weight = stick_weight_base + stick_weight_oci * current_oci
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Trace of past choices for Stage 1
    choice_trace = np.zeros(2)

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add accumulated stickiness to the logits
        logits = beta * q_net + stick_weight * choice_trace
        
        exp_q1 = np.exp(logits - np.max(logits))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # Update choice trace: Decay existing trace, add 1 to current choice
        choice_trace *= stick_decay
        choice_trace[int(action_1[trial])] += 1.0
        
        state_idx = int(state[trial])
        
        # --- Stage 2 Decision ---
        logits_2 = beta * q_stage2_mf[state_idx, :]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Learning ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Stage 1 Update (TD(0))
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Stage-Specific Learning Rate Modulation
This model hypothesizes that high OCI individuals have difficulty updating value estimates for the specific outcomes (aliens), leading to rigidity. If the learning rate for the second stage is suppressed by OCI, the participant will not "unlearn" a previously good option even after repeated failures, explaining the persistence in the face of zero rewards.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Stage-Specific Learning Rate Modulation.
    OCI specifically dampens the learning rate for the second stage (Aliens).
    High OCI results in a very low Stage 2 learning rate, causing the participant
    to maintain high value estimates for aliens despite lack of rewards, 
    which propagates to Stage 1 and prevents switching.

    Parameters:
    - lr_stage1: [0, 1] Learning rate for Stage 1 (Spaceships).
    - lr_stage2_base: [0, 1] Base learning rate for Stage 2 (Aliens).
    - lr_stage2_damp_oci: [0, 1] Damping factor: lr2 = base * (1 - damp * OCI).
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight.
    - stickiness: [0, 5] Standard 1-back stickiness.
    """
    lr_stage1, lr_stage2_base, lr_stage2_damp_oci, beta, w, stickiness = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate effective Stage 2 learning rate (lower for high OCI)
    lr_stage2 = max(0.0, lr_stage2_base * (1.0 - lr_stage2_damp_oci * current_oci))
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_a1 = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits = beta * q_net
        if prev_a1 != -1:
            logits[prev_a1] += stickiness
            
        exp_q1 = np.exp(logits - np.max(logits))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        
        # --- Stage 2 Decision ---
        logits_2 = beta * q_stage2_mf[state_idx, :]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Learning ---
        # Update Stage 1 using lr_stage1
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr_stage1 * delta_stage1
        
        # Update Stage 2 using OCI-modulated lr_stage2
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += lr_stage2 * delta_stage2
        
        # Note: We omit the eligibility trace (lambda) update to Stage 1 here.
        # This allows the 'frozen' Stage 2 values to sustain the Stage 1 choice
        # without being destabilized by the immediate reward prediction error.
        
        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Distorted Transition Belief Model
This model tests the hypothesis that high OCI is associated with rigid, "black and white" thinking. Specifically, it proposes that high OCI participants distort the transition matrix in their Model-Based system, perceiving common transitions as more deterministic (probability closer to 1.0) than they actually are.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Distorted Transition Belief Model.
    OCI modulates the subjective probability of common transitions in the Model-Based system.
    High OCI participants perceive the environment as more deterministic.
    The common transition probability shifts from 0.7 towards 1.0 based on OCI.

    Parameters:
    - learning_rate: [0, 1] Rate of value updating.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight.
    - stickiness: [0, 5] Standard 1-back stickiness.
    - lambda_eligibility: [0, 1] Eligibility trace decay.
    - trans_distortion: [0, 1] Scaling factor for OCI distortion (0=no distortion, 1=max).
    """
    learning_rate, beta, w, stickiness, lambda_eligibility, trans_distortion = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Distort transition probability: Common prob moves from 0.7 towards 1.0
    # The distortion is proportional to OCI.
    p_common = 0.7 + 0.3 * trans_distortion * current_oci
    # Cap at 0.999 to avoid numerical issues with log(0) if we were doing inverse
    p_common = min(p_common, 0.999)
    
    # Distorted transition matrix for MB calculation
    transition_matrix = np.array([[p_common, 1-p_common], [1-p_common, p_common]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_a1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits = beta * q_net
        if prev_a1 != -1:
            logits[prev_a1] += stickiness
            
        exp_q1 = np.exp(logits - np.max(logits))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        
        # --- Stage 2 Decision ---
        logits_2 = beta * q_stage2_mf[state_idx, :]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Learning ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Standard Hybrid update with eligibility trace
        q_stage1_mf[a1] += learning_rate * lambda_eligibility * delta_stage2
        
        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```