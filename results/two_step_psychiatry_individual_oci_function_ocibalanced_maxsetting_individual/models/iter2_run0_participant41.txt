Here are the three proposed cognitive models.

### Cognitive Model 1: Hybrid MB/MF with OCI-Modulated Transition Belief
This model hypothesizes that OCI symptoms distort the Model-Based system's belief about the environment's transition structure. While the true transition probability is fixed (0.7/0.3), individuals with high OCI might perceive the environment as more deterministic (rigid belief) or more random (uncertainty), affecting their Model-Based value calculation.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model where OCI modulates the Model-Based transition belief.
    High OCI may lead to distorted beliefs about transition probabilities 
    (e.g., believing they are more deterministic or random), affecting MB value calculation.
    
    Parameters:
    learning_rate: [0,1] - Update rate for MF Q-values.
    beta: [0,10] - Inverse temperature for softmax.
    w: [0,1] - Mixing weight (0=MF, 1=MB).
    trans_base: [0,1] - Baseline belief of the common transition probability (scaled 0.5 to 1.0).
    trans_oci: [0,1] - Modulation of transition belief by OCI.
    """
    learning_rate, beta, w, trans_base, trans_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate subjective transition probability
    # Map base and modulation to a probability in [0.5, 1.0]
    # We assume 'Common' is at least 0.5.
    # Belief = Base + OCI_effect.
    # We treat parameters as additive components clipped to [0, 1] before scaling.
    
    raw_belief = trans_base + (trans_oci - 0.5) * 2 * oci_score # Allow pos/neg modulation
    belief_p = 0.5 + 0.5 * np.clip(raw_belief, 0.0, 1.0)
    
    # Subjective Transition Matrix
    # T[0,0] is P(State 0 | Action 0), assuming Action 0 -> State 0 is common
    transition_matrix = np.array([[belief_p, 1.0 - belief_p], 
                                  [1.0 - belief_p, belief_p]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2) # MF values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # MF values for stage 2 (Aliens)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])

        # Skip missing trials
        if a2 == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        # --- Stage 1 Policy ---
        # Model-Based Value Calculation using distorted transition matrix
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Stage 1 MF Update (TD(0))
        delta_stage1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Stage 2 MF Update
        delta_stage2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2: Hybrid MB/MF with OCI-Modulated Asymmetric Learning Rates
This model suggests that OCI relates to an imbalance in processing positive versus negative prediction errors. High OCI might be associated with hypersensitivity to punishment (negative errors) or difficulty unlearning (low negative learning rate).

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model with asymmetric learning rates for positive and negative prediction errors, 
    with the negative rate modulated by OCI.
    High OCI may alter sensitivity to negative outcomes (e.g., ignoring losses or overreacting).
    
    Parameters:
    lr_pos: [0,1] - Learning rate for positive prediction errors.
    lr_neg_base: [0,1] - Baseline learning rate for negative prediction errors.
    lr_neg_oci: [0,1] - Modulation of negative learning rate by OCI.
    beta: [0,10] - Inverse temperature.
    w: [0,1] - Mixing weight (0=MF, 1=MB).
    """
    lr_pos, lr_neg_base, lr_neg_oci, beta, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate OCI-modulated negative learning rate
    # We allow the modulation to increase or decrease the base rate, clipped to [0,1]
    # Here we model it as an additive factor centered around 0.5 input
    lr_neg_eff = lr_neg_base + (lr_neg_oci - 0.5) * 2 * oci_score
    lr_neg = np.clip(lr_neg_eff, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])

        if a2 == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        alpha_1 = lr_pos if delta_stage1 > 0 else lr_neg
        q_stage1_mf[a1] += alpha_1 * delta_stage1
        
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[s, a2]
        alpha_2 = lr_pos if delta_stage2 > 0 else lr_neg
        q_stage2_mf[s, a2] += alpha_2 * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3: Hybrid MB/MF with OCI-Modulated Eligibility Trace
This model incorporates an eligibility trace ($\lambda$) that allows the Stage 2 outcome to directly reinforce the Stage 1 choice. OCI modulates this parameter, testing the hypothesis that high OCI correlates with stronger habit formation (higher $\lambda$, effectively making the MF system more "Model-Free" in the temporal sense).

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model where OCI modulates the eligibility trace (lambda).
    High OCI may increase the reinforcement of Stage 1 choices by Stage 2 rewards 
    (stronger habit formation/credit assignment).
    
    Parameters:
    learning_rate: [0,1] - Base learning rate.
    beta: [0,10] - Inverse temperature.
    w: [0,1] - Mixing weight (0=MF, 1=MB).
    lambda_base: [0,1] - Baseline eligibility trace parameter.
    lambda_oci: [0,1] - Modulation of lambda by OCI.
    """
    learning_rate, beta, w, lambda_base, lambda_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate OCI-modulated lambda
    # lambda = base + (mod - 0.5) * 2 * oci, clipped to [0,1]
    lam_val = lambda_base + (lambda_oci - 0.5) * 2 * oci_score
    lam = np.clip(lam_val, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])
        
        if a2 == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Stage 2 Prediction Error
        delta_stage2 = r - q_stage2_mf[s, a2]
        
        # Stage 1 Prediction Error
        delta_stage1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        
        # Update Stage 2
        q_stage2_mf[s, a2] += learning_rate * delta_stage2
        
        # Update Stage 1 (TD(lambda))
        # Q1 is updated by its own error + lambda * stage 2 error
        q_stage1_mf[a1] += learning_rate * (delta_stage1 + lam * delta_stage2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```