Here are three new cognitive models that explore different mechanisms for how high OCI scores might influence reinforcement learning in this task, specifically focusing on habit formation, model-based vs model-free arbitration, and uncertainty-driven rigidity.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free reinforcement learning with OCI-modulated mixing weight.
    
    Hypothesis: High OCI scores are associated with a deficit in goal-directed (Model-Based) 
    control, leading to a stronger reliance on habitual (Model-Free) systems. 
    The mixing weight `w` (0=MF, 1=MB) is reduced as OCI increases.

    Parameters:
    learning_rate: [0, 1] Learning rate for value updates.
    beta: [0, 10] Inverse temperature for softmax.
    w_intercept: [0, 1] Baseline mixing weight for a participant with OCI=0.
    w_slope: [0, 1] How much the mixing weight decreases per unit of OCI.
    """
    learning_rate, beta, w_intercept, w_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective mixing weight w based on OCI
    # We clip to ensure w stays in [0, 1]. High OCI -> Lower w (more MF).
    w = np.clip(w_intercept - (w_slope * oci_score), 0.0, 1.0)

    # Transition matrix (fixed structure of the task)
    # 0 -> 0 (70%), 0 -> 1 (30%)
    # 1 -> 0 (30%), 1 -> 1 (70%)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_mf = np.zeros(2)         # Stage 1 Model-Free values
    q_stage2 = np.zeros((2, 2)) # Stage 2 values (terminal)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_curr = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        # Model-Based Value: V_MB(s1) = T(s1, s2) * max(Q_stage2(s2))
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_mb = transition_matrix @ max_q_stage2
        
        # Integrated Value: Q_net = w * Q_MB + (1-w) * Q_MF
        q_net = w * q_mb + (1 - w) * q_mf
        
        logits_1 = beta * q_net
        logits_1 = logits_1 - np.max(logits_1) # stability
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        # Standard softmax on terminal values
        logits_2 = beta * q_stage2[s_curr]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        # Update Stage 2 Q-values
        pe_2 = r - q_stage2[s_curr, a2]
        q_stage2[s_curr, a2] += learning_rate * pe_2
        
        # Update Stage 1 Model-Free Q-values (SARSA-style update using Stage 2 value)
        # Note: In standard 2-step models, MF update for stage 1 is often driven by 
        # the value of the state reached (s_curr) or the Q-value of the action taken at stage 2.
        # Here we use the scalar value of the chosen stage 2 action.
        pe_1 = q_stage2[s_curr, a2] - q_mf[a1]
        q_mf[a1] += learning_rate * pe_1
        
        # Additional eligibility trace or direct update from reward for MF stage 1
        # (Simplified: The standard Daw et al. 2011 model includes an eligibility trace lambda,
        # but here we stick to simple TD(0) or TD(1) logic. The line above is effectively TD(0)
        # if we consider the step from stage 1 to stage 2. To capture the full reward effect 
        # on stage 1 MF, we can also add a second update:
        pe_direct = r - q_mf[a1]
        q_mf[a1] += learning_rate * pe_direct # Effectively treating it as TD(1) with lambda=1 for simplicity in this constrained space

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner with OCI-modulated asymmetric learning rates for positive/negative outcomes.
    
    Hypothesis: High OCI is associated with 'hyper-responsibility' or fear of negative outcomes.
    This model posits that OCI amplifies the learning rate from negative prediction errors 
    (disappointment) relative to positive ones, leading to faster avoidance learning.

    Parameters:
    lr_base: [0, 1] Base learning rate for positive prediction errors.
    beta: [0, 10] Inverse temperature.
    neg_lr_scale: [1, 5] Scaling factor for negative learning rates.
    oci_sens: [0, 1] Sensitivity to OCI score for the negative scaling.
    """
    lr_base, beta, neg_lr_scale, oci_sens = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective learning rates
    # lr_pos is the base rate.
    # lr_neg is amplified by OCI: lr_neg = lr_base * (1 + (neg_lr_scale - 1) * oci * oci_sens)
    # If OCI is 0, lr_neg = lr_base. If OCI is 1 and sens is 1, lr_neg = lr_base * neg_lr_scale.
    
    lr_pos = lr_base
    scaling_factor = 1.0 + ((neg_lr_scale - 1.0) * oci_score * oci_sens)
    lr_neg = np.clip(lr_base * scaling_factor, 0.0, 1.0)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_curr = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Stage 1 Choice
        logits_1 = beta * q_stage1
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2 Choice
        logits_2 = beta * q_stage2[s_curr]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Update Stage 2
        pe_2 = r - q_stage2[s_curr, a2]
        eff_lr_2 = lr_pos if pe_2 >= 0 else lr_neg
        q_stage2[s_curr, a2] += eff_lr_2 * pe_2

        # Update Stage 1
        # Using the value of the chosen stage 2 state-action pair as the target
        pe_1 = q_stage2[s_curr, a2] - q_stage1[a1]
        eff_lr_1 = lr_pos if pe_1 >= 0 else lr_neg
        q_stage1[a1] += eff_lr_1 * pe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Based learner with OCI-modulated uncertainty-driven exploration suppression.
    
    Hypothesis: Obsessive-compulsive traits are linked to intolerance of uncertainty.
    Instead of standard softmax exploration, this model assumes high OCI participants 
    penalize actions with high variance (uncertainty) in their value estimates, 
    preferring 'safe' or well-known options.
    
    This is implemented as a Q-value penalty proportional to the uncertainty (variance) 
    of that Q-value, scaled by OCI.

    Parameters:
    learning_rate: [0, 1] Learning rate for Q-values.
    beta: [0, 10] Inverse temperature.
    uncert_lr: [0, 1] Learning rate for tracking uncertainty (variance/surprise).
    penalty_weight: [0, 5] How strongly uncertainty penalizes choice, scaled by OCI.
    """
    learning_rate, beta, uncert_lr, penalty_weight = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Transition matrix for Model-Based evaluation
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values for Stage 2
    q_stage2 = np.zeros((2, 2)) + 0.5 # Initialize at 0.5 (neutral)
    
    # Uncertainty tracking (approximated as running average of squared prediction errors)
    # Initialize with some uncertainty
    uncertainty_stage2 = np.ones((2, 2)) * 0.1 
    uncertainty_stage1 = np.ones(2) * 0.1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_curr = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        # Calculate MB values
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_mb = transition_matrix @ max_q_stage2
        
        # Calculate propagated uncertainty for Stage 1 (simplified expectation)
        # Expected uncertainty of the next state
        # uncert_mb[0] = 0.7*uncert(s=0) + 0.3*uncert(s=1) ... roughly
        # We take the uncertainty of the max action in the next states
        max_uncert_stage2 = np.array([uncertainty_stage2[0, np.argmax(q_stage2[0])],
                                      uncertainty_stage2[1, np.argmax(q_stage2[1])]])
        uncert_mb = transition_matrix @ max_uncert_stage2
        
        # Effective Value = Q_MB - (Penalty * OCI * Uncertainty)
        # High OCI -> Stronger avoidance of uncertain options
        adjusted_q1 = q_mb - (penalty_weight * oci_score * uncert_mb)
        
        logits_1 = beta * adjusted_q1
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        adjusted_q2 = q_stage2[s_curr] - (penalty_weight * oci_score * uncertainty_stage2[s_curr])
        
        logits_2 = beta * adjusted_q2
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        # Update Stage 2
        pe_2 = r - q_stage2[s_curr, a2]
        q_stage2[s_curr, a2] += learning_rate * pe_2
        
        # Update Uncertainty Stage 2 (track squared error)
        # Simple recursive filter for variance/uncertainty
        uncertainty_stage2[s_curr, a2] += uncert_lr * (pe_2**2 - uncertainty_stage2[s_curr, a2])

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```