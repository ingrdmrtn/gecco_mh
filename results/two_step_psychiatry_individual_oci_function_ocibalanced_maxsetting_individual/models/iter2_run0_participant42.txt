Here are three cognitive models that explain participant behavior in the two-step task, incorporating the Obsessive-Compulsive Inventory (OCI) score to modulate decision-making mechanisms.

### Model 1: OCI-Modulated Probability Mixture Model
This model posits that the brain has two competing controllers (Model-Based and Model-Free) that output probability distributions over actions. Instead of integrating their value estimates (Q-values) into a single utility, the final policy is a probabilistic mixture of the two agents' policies. The OCI score determines the weight of the Model-Free (habitual) agent in this mixture. A high OCI score shifts control towards the habitual system, which also includes a fixed perseveration (stickiness) bias.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Probability Mixture Model.
    
    Instead of mixing Q-values, this model mixes the policies (probabilities) of the 
    Model-Based (MB) and Model-Free (MF) systems. The weight of the MF system is 
    determined by the OCI score.
    
    P_choice = (1 - w_mix) * P_MB + w_mix * P_MF
    w_mix = w_scale * OCI
    
    High OCI favors the Model-Free (habitual) probability distribution.
    
    Parameters:
    - learning_rate: [0,1] Update rate for MF values.
    - beta: [0,10] Inverse temperature for both systems.
    - stickiness: [0,5] Choice stickiness bonus added to MF Q-values.
    - w_scale: [0,1] Scaling factor for OCI to determine MF weight.
    """
    learning_rate, beta, stickiness, w_scale = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate mixing weight based on OCI (bounded at 1.0)
    w_mix = w_scale * oci_score
    if w_mix > 1.0: w_mix = 1.0
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track previous choice for stickiness
    prev_a1 = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        
        # 1. Model-Based Estimates
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # MB Probabilities
        exp_q_mb = np.exp(beta * q_stage1_mb)
        p_mb = exp_q_mb / np.sum(exp_q_mb)
        
        # 2. Model-Free Estimates (with Stickiness)
        q_mf_with_stick = q_stage1_mf.copy()
        if prev_a1 != -1:
            q_mf_with_stick[prev_a1] += stickiness
            
        # MF Probabilities
        exp_q_mf = np.exp(beta * q_mf_with_stick)
        p_mf = exp_q_mf / np.sum(exp_q_mf)
        
        # 3. Mixture
        probs_1 = (1.0 - w_mix) * p_mb + w_mix * p_mf
        
        # Store probability of chosen action
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning Updates ---
        # Stage 1 PE (TD(0))
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Stage 2 PE
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 Eligibility Trace Update (Lambda=1 implicitly)
        q_stage1_mf[a1] += learning_rate * delta_stage2
        
        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Unchosen Value Decay
This model suggests that high OCI scores are associated with a rapid decay (forgetting) of the values of unchosen options. This mechanism can explain the "tunnel vision" or obsessive focus on the current choice, as well as the rapid switching when the current choice fails (since the alternative option's value has decayed to neutral).

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Unchosen Value Decay.
    
    This model introduces a decay parameter for the unchosen spaceship's Q-value.
    The rate of decay is modulated by the OCI score. 
    High OCI leads to faster forgetting of the unchosen option's value, 
    potentially reinforcing 'tunnel vision' or staying with the current choice 
    until it fails, then switching and forgetting the previous one.
    
    Parameters:
    - learning_rate: [0,1] Learning rate.
    - beta: [0,10] Inverse temperature.
    - w: [0,1] Weight for Model-Based control (Standard Q-mixing).
    - decay_scale: [0,1] Scaling factor for decay rate: decay = decay_scale * OCI.
    """
    learning_rate, beta, w, decay_scale = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    decay = decay_scale * oci_score
    if decay > 1.0: decay = 1.0
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        # Stage 1 Choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # Stage 2 Choice
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # Updates
        # Apply Decay to unchosen stage 1 action
        unchosen_a1 = 1 - a1
        q_stage1_mf[unchosen_a1] *= (1.0 - decay)
        
        # Standard TD Updates
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        q_stage1_mf[a1] += learning_rate * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Eligibility Trace (Lambda)
This model proposes that the OCI score modulates the eligibility trace parameter ($\lambda$). The eligibility trace determines how much the outcome of the second stage reinforces the choice made in the first stage. A high OCI score leads to a high $\lambda$, implying that the participant's habits are strongly driven by the immediate outcome (reward), rather than just the value of the state they arrived at.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Eligibility Trace (Lambda).
    
    This model modulates the strength of the eligibility trace (lambda) by OCI.
    The eligibility trace controls how much the Stage 2 outcome reinforces the 
    Stage 1 choice. 
    High OCI -> High lambda (Strong habit reinforcement from final reward).
    Low OCI -> Low lambda (Stage 1 only learns from Stage 2 value prediction).
    
    Parameters:
    - learning_rate: [0,1] Learning rate.
    - beta: [0,10] Inverse temperature.
    - w: [0,1] Weight for Model-Based control.
    - lambda_scale: [0,1] Scaling factor: lambda = lambda_scale * OCI.
    """
    learning_rate, beta, w, lambda_scale = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    lambda_param = lambda_scale * oci_score
    if lambda_param > 1.0: lambda_param = 1.0
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        # Stage 1 Choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # Stage 2 Choice
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # Updates
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Eligibility Trace Update (modulated by lambda)
        # q_s1 += alpha * lambda * delta_s2
        q_stage1_mf[a1] += learning_rate * lambda_param * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```