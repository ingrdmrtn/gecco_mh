Here are 3 new cognitive models that explore different mechanisms for how high OCI scores might influence reinforcement learning in this two-step task.

### Model 1: OCI-Modulated Model-Based Weighting
This model tests the hypothesis that high OCI scores (compulsivity) are associated with a rigid reliance on Model-Free (habitual) learning, leading to a reduction in Model-Based (goal-directed) planning. Here, the `w` parameter (mixing weight) is not static but is directly reduced by the OCI score.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Model-Based Weighting.
    Hypothesis: High OCI scores reduce the weight of model-based planning (w),
    biasing the agent towards model-free (habitual) control.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for value updates.
    - beta: [0, 10] Inverse temperature for softmax.
    - w_max: [0, 1] Maximum possible model-based weight (when OCI is 0).
    - oci_penalty: [0, 1] How much OCI score reduces w.
    """
    learning_rate, beta, w_max, oci_penalty = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective w: w decreases as OCI increases
    # We clip to ensure it stays in [0, 1]
    w = np.clip(w_max - (oci_penalty * oci_score), 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        act1 = int(action_1[trial])
        state_idx = int(state[trial])
        act2 = int(action_2[trial])
        r = reward[trial]

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combined value: w determines the balance between MB and MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[act1]

        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[act2]

        # Value Updating
        # Stage 2 update (TD)
        delta_stage2 = r - q_stage2_mf[state_idx, act2]
        q_stage2_mf[state_idx, act2] += learning_rate * delta_stage2

        # Stage 1 update (TD) - Note: This drives the MF component
        delta_stage1 = q_stage2_mf[state_idx, act2] - q_stage1_mf[act1]
        q_stage1_mf[act1] += learning_rate * delta_stage1
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Driven Asymmetric Learning Rates
This model posits that high OCI individuals might exhibit a "negativity bias" or fear of failure, causing them to learn more from negative prediction errors (disappointments) than positive ones. The OCI score scales the learning rate specifically for negative prediction errors.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Driven Asymmetric Learning Rates.
    Hypothesis: High OCI leads to hypersensitivity to negative outcomes.
    The learning rate for negative prediction errors is amplified by OCI.
    
    Parameters:
    - alpha_base: [0, 1] Base learning rate for positive prediction errors.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight between MB and MF.
    - neg_sensitivity: [0, 5] Multiplier for OCI impact on negative learning rate.
    """
    alpha_base, beta, w, neg_sensitivity = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate alpha for negative PEs
    # If PE < 0, we use alpha_neg. We cap it at 1.0.
    alpha_neg = np.clip(alpha_base * (1 + neg_sensitivity * oci_score), 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        act1 = int(action_1[trial])
        state_idx = int(state[trial])
        act2 = int(action_2[trial])
        r = reward[trial]

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[act1]

        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[act2]

        # Value Updating
        # Determine learning rate based on sign of prediction error
        delta_stage2 = r - q_stage2_mf[state_idx, act2]
        lr_2 = alpha_neg if delta_stage2 < 0 else alpha_base
        q_stage2_mf[state_idx, act2] += lr_2 * delta_stage2

        delta_stage1 = q_stage2_mf[state_idx, act2] - q_stage1_mf[act1]
        lr_1 = alpha_neg if delta_stage1 < 0 else alpha_base
        q_stage1_mf[act1] += lr_1 * delta_stage1
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Scaled Eligibility Trace (Lambda)
This model introduces an eligibility trace parameter ($\lambda$) that is modulated by OCI. In reinforcement learning, $\lambda$ controls how much credit is assigned to the first stage choice based on the second stage outcome. High OCI might relate to "over-thinking" or attributing too much significance to the initial choice based on the final outcome, effectively increasing $\lambda$ (making the update look more like Monte Carlo/full outcome learning rather than pure TD(0)).

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Scaled Eligibility Trace (Lambda).
    Hypothesis: High OCI increases the credit assignment to the first action
    based on the final reward (higher lambda), linking Stage 1 directly to the outcome.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight between MB and MF.
    - lambda_sensitivity: [0, 1] How much OCI increases the eligibility trace lambda.
    """
    learning_rate, beta, w, lambda_sensitivity = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Lambda is scaled by OCI, capped at 1.
    # If lambda is 0, it's standard TD(0). If 1, it's TD(1).
    lam = np.clip(lambda_sensitivity * oci_score, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        act1 = int(action_1[trial])
        state_idx = int(state[trial])
        act2 = int(action_2[trial])
        r = reward[trial]

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[act1]

        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[act2]

        # Value Updating with Eligibility Trace
        # Stage 2 PE
        delta_stage2 = r - q_stage2_mf[state_idx, act2]
        q_stage2_mf[state_idx, act2] += learning_rate * delta_stage2

        # Stage 1 update includes a portion of Stage 2's PE based on lambda
        # Standard TD(0) part:
        delta_stage1_td0 = q_stage2_mf[state_idx, act2] - q_stage1_mf[act1]
        
        # Combined update: TD(0) + lambda * (Stage 2 PE)
        # This effectively allows the final reward 'r' to propagate back to stage 1 more strongly
        combined_update = delta_stage1_td0 + (lam * delta_stage2)
        
        q_stage1_mf[act1] += learning_rate * combined_update
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```