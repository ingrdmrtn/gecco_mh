Here are 3 new cognitive models based on the participant's high OCI score (0.867) and the provided data.

### Model 1: Compulsive Safety-Seeking (Loss Aversion)
**Hypothesis:** High OCI individuals may exhibit heightened sensitivity to the absence of rewards (losses), perceiving them as errors to be aggressively corrected. This model implements asymmetric learning rates for positive and negative prediction errors, where the learning rate for negative outcomes is amplified by the OCI score. This reflects a "safety-seeking" behavior where avoiding zero-reward outcomes drives learning more than obtaining rewards.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Asymmetric learning rates modulated by OCI.
    High OCI amplifies learning from negative prediction errors (loss aversion).
    
    Bounds:
    lr_pos: [0, 1] Learning rate for positive prediction errors.
    lr_neg_base: [0, 1] Base learning rate for negative prediction errors.
    oci_neg_amp: [0, 5] Amplification factor for negative learning rate based on OCI.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Model-based vs Model-free weight.
    """
    lr_pos, lr_neg_base, oci_neg_amp, beta, w = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]

    # Calculate effective negative learning rate, capped at 1.0
    lr_neg = min(1.0, lr_neg_base + (oci_neg_amp * participant_oci))

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # Stage 2 Policy
        s_curr = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[s_curr])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # Learning
        r = reward[trial]
        
        # Stage 2 Update
        pe_2 = r - q_stage2_mf[s_curr, a2]
        effective_lr_2 = lr_pos if pe_2 >= 0 else lr_neg
        q_stage2_mf[s_curr, a2] += effective_lr_2 * pe_2
        
        # Stage 1 Update (TD)
        pe_1 = q_stage2_mf[s_curr, a2] - q_stage1_mf[a1]
        effective_lr_1 = lr_pos if pe_1 >= 0 else lr_neg
        q_stage1_mf[a1] += effective_lr_1 * pe_1 
        
        # Eligibility trace update for Stage 1 from Stage 2 outcome
        q_stage1_mf[a1] += effective_lr_2 * pe_2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Rigid Habit Formation (Inverse Temperature Modulation)
**Hypothesis:** Compulsivity is often associated with rigid behavior that becomes less sensitive to value differences over time or is generally more deterministic. Instead of modifying learning, this model posits that high OCI leads to a higher inverse temperature (`beta`) specifically for the Model-Free system. This implies that habits (MF values) exert a sharper, more deterministic influence on choice than goal-directed (MB) plans, leading to "stuck" behaviors even when values are close.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Dual beta model where OCI modulates the Model-Free inverse temperature.
    High OCI leads to more deterministic (rigid) choices based on habit (MF),
    while MB choices remain more stochastic.
    
    Bounds:
    lr: [0, 1] Learning rate.
    beta_mb: [0, 10] Inverse temperature for Model-Based component.
    beta_mf_base: [0, 10] Base inverse temperature for Model-Free component.
    oci_beta_scale: [0, 5] Scaling factor for MF beta based on OCI.
    w: [0, 1] Mixing weight.
    """
    lr, beta_mb, beta_mf_base, oci_beta_scale, w = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]

    # MF beta is enhanced by OCI, leading to more rigid habitual choices
    beta_mf = beta_mf_base + (oci_beta_scale * participant_oci)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # Stage 1 Policy: Separate betas for MB and MF components before mixing
        # Note: Standard hybrid models mix Q values then apply one beta. 
        # Here we mix the *logits* (beta * Q) to allow differential rigidity.
        
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Weighted combination of logits
        logits_1 = w * (beta_mb * q_stage1_mb) + (1 - w) * (beta_mf * q_stage1_mf)
        
        exp_logits_1 = np.exp(logits_1)
        probs_1 = exp_logits_1 / np.sum(exp_logits_1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # Stage 2 Policy (Purely MF, so uses beta_mf)
        s_curr = int(state[trial])
        logits_2 = beta_mf * q_stage2_mf[s_curr]
        exp_logits_2 = np.exp(logits_2)
        probs_2 = exp_logits_2 / np.sum(exp_logits_2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # Learning
        r = reward[trial]
        
        pe_2 = r - q_stage2_mf[s_curr, a2]
        q_stage2_mf[s_curr, a2] += lr * pe_2
        
        pe_1 = q_stage2_mf[s_curr, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * pe_1
        q_stage1_mf[a1] += lr * pe_2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Uncertainty Intolerance (Decay Rate Modulation)
**Hypothesis:** High OCI is linked to intolerance of uncertainty. In a reinforcement learning context, this might manifest as a faster decay of unchosen option values. If an action is not taken, the participant becomes highly uncertain about its value and assumes it has degraded or is no longer safe. This model introduces a passive decay parameter for unchosen actions, where the rate of decay is proportional to the OCI score.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model with passive decay of unchosen options, modulated by OCI.
    High OCI accelerates the 'forgetting' or devaluation of unchosen paths,
    reflecting intolerance of uncertainty about the state of the world.
    
    Bounds:
    lr: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Mixing weight.
    decay_base: [0, 1] Base decay rate for unchosen options (0 = no decay, 1 = full reset).
    oci_decay_mod: [0, 1] OCI modulation of decay.
    """
    lr, beta, w, decay_base, oci_decay_mod = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]

    # Calculate decay rate. Higher OCI -> more decay (values move toward 0)
    decay = min(1.0, decay_base + (oci_decay_mod * participant_oci))

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # Initialized to 0 (neutral expectation)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # Stage 2 Policy
        s_curr = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[s_curr])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # Learning
        r = reward[trial]
        
        # Update chosen Stage 2
        pe_2 = r - q_stage2_mf[s_curr, a2]
        q_stage2_mf[s_curr, a2] += lr * pe_2
        
        # Decay unchosen Stage 2 in current state
        unchosen_a2 = 1 - a2
        q_stage2_mf[s_curr, unchosen_a2] *= (1 - decay)
        
        # Update chosen Stage 1
        pe_1 = q_stage2_mf[s_curr, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * pe_1
        q_stage1_mf[a1] += lr * pe_2
        
        # Decay unchosen Stage 1
        unchosen_a1 = 1 - a1
        q_stage1_mf[unchosen_a1] *= (1 - decay)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```