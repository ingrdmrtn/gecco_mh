Here are the 3 new cognitive models as Python functions.

```python
import numpy as np

def cognitive_model1_oci_forgetting(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: Hybrid Learner with OCI-Modulated Forgetting.
    
    Hypothesis: OCI score impacts the rate at which unchosen action values decay.
    High OCI might be associated with persistent "obsessive" memory (low decay) or 
    difficulty maintaining working memory (high decay).
    The decay rate of the unchosen Stage 1 action value is scaled by the OCI score.
    
    Q(unchosen) = Q(unchosen) * (1 - decay_rate)
    decay_rate = decay_factor * OCI
    
    Parameters:
    - learning_rate: [0, 1] Update rate for chosen actions.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Model-Based weight.
    - decay_factor: [0, 1] Scaling factor for decay rate based on OCI.
    """
    learning_rate, beta, w, decay_factor = model_parameters
    oci_score = oci[0]
    n_trials = len(action_1)
    
    # Calculate OCI-dependent decay rate
    decay_rate = decay_factor * oci_score
    decay_rate = np.clip(decay_rate, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    
    for trial in range(n_trials):
        # Stage 1 Choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Stage 2 Choice
        state_idx = state[trial]
        logits_2 = beta * q_stage2_mf[state_idx]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # Updates
        # MF Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Forgetting step for unchosen Stage 1 action
        unchosen_action = 1 - action_1[trial]
        q_stage1_mf[unchosen_action] *= (1.0 - decay_rate)
        
        # MF Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2_oci_asymmetric_lr(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: Hybrid Learner with OCI-Modulated Asymmetric Learning Rates.
    
    Hypothesis: OCI score modulates the sensitivity to negative outcomes (zero rewards)
    relative to positive outcomes. High OCI might lead to hyper-correction after failure.
    
    alpha_pos = learning_rate
    alpha_neg = learning_rate * asym_factor * OCI
    
    Parameters:
    - learning_rate: [0, 1] Base learning rate for positive rewards.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Model-Based weight.
    - asym_factor: [0, 5] Scaling factor for negative learning rate based on OCI.
      (Values > 1/OCI imply stronger learning from failure).
    """
    learning_rate, beta, w, asym_factor = model_parameters
    oci_score = oci[0]
    n_trials = len(action_1)
    
    # Calculate negative learning rate
    lr_pos = learning_rate
    lr_neg = learning_rate * asym_factor * oci_score
    lr_neg = np.clip(lr_neg, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    
    for trial in range(n_trials):
        # Stage 1 Choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Stage 2 Choice
        state_idx = state[trial]
        logits_2 = beta * q_stage2_mf[state_idx]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # Updates
        # Determine effective learning rate based on reward outcome of the trial
        current_lr = lr_pos if reward[trial] == 1 else lr_neg
        
        # MF Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += current_lr * delta_stage1
        
        # MF Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += current_lr * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3_oci_state_stickiness(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: Hybrid Learner with OCI-Modulated State Stickiness.
    
    Hypothesis: Participants with high OCI may exhibit a compulsion to return to a 
    previously visited state (Planet), rather than just repeating an action.
    This "State Stickiness" biases the Stage 1 choice towards the spaceship that 
    most likely leads to the previous trial's planet.
    
    Bonus = state_stick_factor * OCI * P(Last_State | Action)
    
    Parameters:
    - learning_rate: [0, 1]
    - beta: [0, 10]
    - w: [0, 1]
    - state_stick_factor: [0, 5] Strength of state stickiness scaled by OCI.
    """
    learning_rate, beta, w, state_stick_factor = model_parameters
    oci_score = oci[0]
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    
    last_planet = -1
    
    for trial in range(n_trials):
        # Stage 1 Choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net
        
        # Add State Stickiness Bonus
        if last_planet != -1:
            # Calculate bonus based on transition probability to the last visited planet
            stickiness_bonus = transition_matrix[:, last_planet] * state_stick_factor * oci_score
            logits_1 += stickiness_bonus
            
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_planet = state[trial]
        
        # Stage 2 Choice
        state_idx = state[trial]
        logits_2 = beta * q_stage2_mf[state_idx]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # Updates
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```