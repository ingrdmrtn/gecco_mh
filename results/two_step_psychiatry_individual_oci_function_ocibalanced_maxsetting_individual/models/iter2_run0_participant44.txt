Here are three new cognitive models exploring different mechanisms by which Obsessive-Compulsive symptoms (OCI) might influence decision-making in this task.

### Model 1: OCI-Modulated Model-Based/Model-Free Trade-off
This model hypothesizes that high OCI scores are associated with a deficit in goal-directed (Model-Based) control, leading to a greater reliance on habitual (Model-Free) control. Instead of a fixed mixing weight `w`, the weight is dynamically adjusted downwards based on the participant's OCI score.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated MB/MF Trade-off.
    
    Hypothesis: High OCI scores reduce the contribution of the Model-Based (MB) system,
    leading to greater reliance on the Model-Free (MF) system (habitual control).
    The mixing weight 'w' is calculated as a base weight minus an OCI-dependent penalty.
    
    Parameters:
    lr: [0, 1] Learning rate for value updates.
    beta: [0, 10] Inverse temperature for softmax choice.
    w_base: [0, 1] Baseline Model-Based weighting (before OCI modulation).
    oci_decrement: [0, 1] Strength of reduction in MB weighting due to OCI.
    """
    lr, beta, w_base, oci_decrement = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective mixing weight. High OCI reduces w, pushing towards MF (0).
    # We clip to ensure w stays in [0, 1].
    w = np.clip(w_base - (oci_decrement * oci_score), 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combined value: w determines the balance between MB and MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        s_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        r = reward[trial]
        
        # --- Value Updating ---
        # SARSA-like update for Stage 1 MF
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        # Q-learning update for Stage 2
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Driven "Safety" Learning Asymmetry
This model suggests that individuals with high OCI symptoms are hyper-sensitive to negative outcomes (or the absence of reward) as a signal of error or danger. This is modeled by having separate learning rates for positive (reward = 1) and negative (reward = 0) outcomes, where the learning rate for negative outcomes is amplified by the OCI score.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Driven "Safety" Learning Asymmetry.
    
    Hypothesis: High OCI leads to hyper-sensitivity to negative outcomes (0 reward).
    The learning rate for negative outcomes (lr_neg) is boosted by the OCI score,
    while the learning rate for positive outcomes is fixed. This reflects a 'fear of failure'
    or safety-seeking mechanism.
    
    Parameters:
    lr_pos: [0, 1] Learning rate for positive outcomes (Reward = 1).
    lr_neg_base: [0, 1] Base learning rate for negative outcomes (Reward = 0).
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Mixing weight (MB vs MF).
    oci_neg_scale: [0, 5] Scaling factor for OCI impact on negative learning rate.
    """
    lr_pos, lr_neg_base, beta, w, oci_neg_scale = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective negative learning rate
    lr_neg = np.clip(lr_neg_base + (oci_neg_scale * oci_score), 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        s_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        r = reward[trial]
        
        # Determine which learning rate to use based on outcome
        if r > 0:
            current_lr = lr_pos
        else:
            current_lr = lr_neg
        
        # --- Value Updating ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += current_lr * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += current_lr * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Dependent Inverse Temperature (Rigidity)
This model posits that high OCI scores relate to behavioral rigidity or determinism in choice. Instead of influencing learning or strategy balance directly, OCI acts on the softmax temperature parameter `beta`. Higher OCI scores increase `beta`, making choices more deterministic (exploitative) and less exploratory, effectively "freezing" the participant into their estimated best option.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Dependent Inverse Temperature (Rigidity).
    
    Hypothesis: High OCI is associated with behavioral rigidity.
    The inverse temperature parameter (beta) is not fixed but increases with the OCI score.
    A higher beta means choices are more deterministic (less random exploration),
    reflecting a rigid adherence to the perceived best option.
    
    Parameters:
    lr: [0, 1] Learning rate.
    beta_base: [0, 10] Baseline inverse temperature.
    w: [0, 1] Mixing weight (MB vs MF).
    oci_beta_boost: [0, 5] Additional beta added per unit of OCI.
    """
    lr, beta_base, w, oci_beta_boost = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective beta: Base rigidity + OCI-induced rigidity
    beta_eff = beta_base + (oci_beta_boost * oci_score)
    # Although beta is usually bounded [0, 10] in fitting, the effective beta can go higher theoretically.
    # We will keep it unbounded here as the optimizer bounds apply to parameters, not internal variables.
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Use beta_eff instead of raw beta parameter
        exp_q1 = np.exp(beta_eff * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        s_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta_eff * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        r = reward[trial]
        
        # --- Value Updating ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```