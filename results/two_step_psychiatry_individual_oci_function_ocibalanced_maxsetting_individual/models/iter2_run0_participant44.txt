Here are three cognitive models expressed as Python functions. They explore different mechanisms by which obsessive-compulsive symptoms (OCI) might alter decision-making: **Asymmetric Belief Rigidity**, **Reward Hypersensitivity**, and **Distorted Model-Based Beliefs**.

### Model 1: Asymmetric Belief Rigidity
This model hypothesizes that high OCI scores are associated with "cognitive rigidity," specifically a resistance to "unlearning" established habits when faced with negative outcomes. While positive prediction errors (things going better than expected) are processed normally, negative prediction errors (disappointments) are suppressed by the OCI score, causing the participant to maintain high Q-values for actions even after they stop yielding rewards.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Asymmetric Belief Rigidity Model.
    
    Hypothesis: High OCI participants exhibit cognitive rigidity, modeled here as
    a resistance to updating values downwards. When a prediction error is negative 
    (outcome worse than expected), the learning rate is suppressed by the OCI score.
    They learn from success (positive PE) normally, but ignore failure (negative PE).
    
    Parameters:
    - learning_rate: Base learning rate [0, 1]
    - rigidity: OCI-based suppression of negative prediction errors [0, 1]
    - beta: Inverse temperature for softmax [0, 10]
    - w: Mixing weight (0=MF, 1=MB) [0, 1]
    - lambd: Eligibility trace parameter [0, 1]
    """
    learning_rate, rigidity, beta, w, lambd = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Mix MB and MF values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        state_idx = int(state[trial])

        # policy for the second choice
        qs2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        # Update Stage 1 (SARSA-style)
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        
        # Asymmetric learning: Suppress update if delta is negative (Rigidity)
        lr_s1 = learning_rate
        if delta_stage1 < 0:
            lr_s1 *= (1.0 - (rigidity * oci_score))
            
        q_stage1_mf[int(action_1[trial])] += lr_s1 * delta_stage1
        
        # Update Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        
        # Asymmetric learning: Suppress update if delta is negative
        lr_s2 = learning_rate
        if delta_stage2 < 0:
            lr_s2 *= (1.0 - (rigidity * oci_score))

        q_stage2_mf[state_idx, int(action_2[trial])] += lr_s2 * delta_stage2
        
        # Eligibility trace (propagating stage 2 outcome to stage 1 value)
        # We use the stage 2 effective learning rate for consistency
        q_stage1_mf[int(action_1[trial])] += lr_s2 * lambd * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Reward Hypersensitivity
This model hypothesizes that OCI symptoms correlate with an altered subjective valuation of outcomes (scrupulosity or perfectionism). High OCI participants may perceive the "success" signal (the gold coin) as having a higher magnitude than low OCI participants. This effectively scales up the prediction error for rewards, causing faster/stronger reinforcement of successful actions while leaving the mechanics of learning (alpha) and exploration (beta) intact.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Reward Hypersensitivity Model.
    
    Hypothesis: High OCI scores relate to perfectionism or scrupulosity, increasing 
    the subjective magnitude of the reward signal. A coin is not just 1.0, but 
    1.0 + (sensitivity * OCI). This results in larger prediction errors 
    for successes, leading to stronger habit formation (stamping in) of rewarded paths.
    
    Parameters:
    - learning_rate: Base learning rate [0, 1]
    - beta: Inverse temperature [0, 10]
    - w: Mixing weight [0, 1]
    - lambd: Eligibility trace [0, 1]
    - sensitivity: OCI-based gain on the reward signal [0, 5]
    """
    learning_rate, beta, w, lambd, sensitivity = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        state_idx = int(state[trial])

        # policy for the second choice
        qs2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1
        
        # Subjective Reward Modulation
        # The reward is scaled up by OCI sensitivity
        effective_reward = reward[trial] * (1.0 + (sensitivity * oci_score))
        
        delta_stage2 = effective_reward - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2
        
        # Eligibility trace
        q_stage1_mf[int(action_1[trial])] += learning_rate * lambd * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Distorted Model-Based Beliefs
This model targets the Model-Based (MB) component specifically. It hypothesizes that high OCI participants possess a distorted internal model of the environment's structure. While the true transition probability is 0.7, high OCI (associated with intolerance of uncertainty or overestimation of cause-effect) leads the participant to treat the transitions as more deterministic than they actually are. The model-based system calculates values assuming the transition probability is closer to 1.0 as OCI increases.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Distorted Model-Based Beliefs.
    
    Hypothesis: High OCI participants have a distorted internal model of the task structure.
    They overestimate the reliability of transitions (intolerance of uncertainty), 
    believing the common transition is more certain (closer to 1.0) than reality (0.7).
    This distorts the MB value calculation, making them over-rely on the 'correct' 
    path even when the MF system might suggest otherwise.
    
    Parameters:
    - learning_rate: [0, 1]
    - beta: [0, 10]
    - w: Mixing weight [0, 1]
    - lambd: Eligibility trace [0, 1]
    - distortion: Degree to which OCI pushes belief toward determinism [0, 1]
    """
    learning_rate, beta, w, lambd, distortion = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    # Standard matrix is [0.7, 0.3]. 
    # OCI distorts this belief towards [1.0, 0.0].
    # p_belief = 0.7 + (distortion * OCI * (1.0 - 0.7))
    p_common = 0.7 + (distortion * oci_score * 0.3)
    p_common = np.clip(p_common, 0.7, 1.0) # Ensure bounds
    
    # The participant's internal model (subjective transition matrix)
    transition_matrix = np.array([[p_common, 1.0 - p_common], 
                                  [1.0 - p_common, p_common]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        # MB Calculation uses the DISTORTED transition matrix
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        state_idx = int(state[trial])

        # policy for the second choice
        qs2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2
        
        q_stage1_mf[int(action_1[trial])] += learning_rate * lambd * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```