Here are three new cognitive models exploring different mechanisms for how OCI scores might influence learning and decision-making in this task.

### Model 1: Hybrid Learner with OCI-modulated Model-Based Weight (`w`)
This model hypothesizes that individuals with different OCI scores rely differently on Model-Based (planning) vs. Model-Free (habit) systems. Specifically, we test if the balance parameter `w` (where 1 is fully Model-Based and 0 is fully Model-Free) is a function of the OCI score.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free learner where the weighting parameter 'w'
    is modulated by the OCI score.
    
    Hypothesis: The balance between goal-directed (MB) and habitual (MF) control
    depends on obsessive-compulsive traits.
    
    Parameters:
    learning_rate: [0, 1] Learning rate for value updates.
    beta: [0, 10] Inverse temperature for softmax choice.
    w_base: [0, 1] Baseline weighting for Model-Based system.
    w_oci_factor: [0, 1] How strongly OCI affects the weighting.
                 w = clip(w_base + w_oci_factor * oci, 0, 1)
    """
    learning_rate, beta, w_base, w_oci_factor = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective weight w
    w = w_base + w_oci_factor * oci_score
    if w > 1.0: w = 1.0
    if w < 0.0: w = 0.0
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)      # Model-free values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for stage 2 (aliens)

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        # Model-Based value calculation: V_MB(s1) = T * max(Q_MF(s2))
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid value: Q_net = w * Q_MB + (1-w) * Q_MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        # Stage 2 update (TD)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 update (TD) using the value of the state actually reached
        # Note: In standard hybrid models, MF Q-values at stage 1 are updated via TD(1) or TD(0)
        # Here we use a simple TD(1)-like update based on the reward received
        delta_stage1 = r - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    valid_trials = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```

### Model 2: Reinforcement Sensitivity (Asymmetric Learning Rates)
This model tests whether OCI scores modulate sensitivity to positive vs. negative prediction errors. High OCI might be associated with an increased focus on avoiding errors (loss aversion) or rigid learning from failure.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Based learner with asymmetric learning rates for positive and negative
    prediction errors, where the asymmetry is modulated by OCI.
    
    Hypothesis: OCI scores influence how strongly participants learn from 
    outcomes that are better than expected (positive PE) vs worse than expected (negative PE).
    
    Parameters:
    lr_pos: [0, 1] Learning rate for positive prediction errors.
    lr_neg_base: [0, 1] Base learning rate for negative prediction errors.
    lr_neg_oci_scale: [0, 2] Scaling factor for OCI on negative learning rate.
                   lr_neg = lr_neg_base * (1 + lr_neg_oci_scale * oci)
    beta: [0, 10] Inverse temperature.
    """
    lr_pos, lr_neg_base, lr_neg_oci_scale, beta = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective negative learning rate
    lr_neg = lr_neg_base * (1.0 + lr_neg_oci_scale * oci_score)
    if lr_neg > 1.0: lr_neg = 1.0
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Only tracking stage 2 values for a pure MB agent to simplify
    # (Pure MB agents use learned Stage 2 values to plan Stage 1)
    q_stage2_mf = np.zeros((2, 2)) 

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy (Model-Based) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        # Apply asymmetric learning rates
        if delta_stage2 >= 0:
            q_stage2_mf[s_idx, a2] += lr_pos * delta_stage2
        else:
            q_stage2_mf[s_idx, a2] += lr_neg * delta_stage2

    eps = 1e-10
    valid_trials = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Eligibility Traces (Lambda)
This model introduces an eligibility trace parameter ($\lambda$) that connects the second-stage outcome back to the first-stage choice. This is a "Model-Free" way of handling sequential decisions (like TD($\lambda$)). We hypothesize that OCI affects how effectively credit is assigned to the first action based on the eventual outcome.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    TD(lambda) learner where the eligibility trace decay parameter (lambda)
    is modulated by OCI.
    
    Hypothesis: OCI affects the 'credit assignment' window. A higher lambda
    means the first-stage choice is more strongly updated by the second-stage reward directly.
    
    Parameters:
    learning_rate: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    lambda_base: [0, 1] Base eligibility trace parameter.
    lambda_oci_slope: [-1, 1] How OCI changes lambda.
                      lambda = clip(lambda_base + lambda_oci_slope * oci, 0, 1)
    """
    learning_rate, beta, lambda_base, lambda_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective lambda
    lam = lambda_base + lambda_oci_slope * oci_score
    if lam > 1.0: lam = 1.0
    if lam < 0.0: lam = 0.0
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning (TD(lambda) logic) ---
        # 1. Prediction error at stage 1 (transition from Stage 1 to Stage 2)
        # The value of the state arrived at is q_stage2[s_idx, a2] (SARSA style) 
        # or max(q_stage2[s_idx]) (Q-learning style). 
        # Here we use SARSA-style for the transition to be consistent with standard TD(lambda) implementations in this task.
        delta1 = q_stage2[s_idx, a2] - q_stage1[a1]
        
        # 2. Prediction error at stage 2 (transition from Stage 2 to Reward)
        delta2 = r - q_stage2[s_idx, a2]
        
        # Update Stage 1 Value:
        # It gets the immediate update from the transition (delta1)
        # PLUS the eligibility trace of the second stage error (lambda * delta2)
        q_stage1[a1] += learning_rate * (delta1 + lam * delta2)
        
        # Update Stage 2 Value:
        q_stage2[s_idx, a2] += learning_rate * delta2

    eps = 1e-10
    valid_trials = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```