Here are three new cognitive models for the two-step task, incorporating OCI scores into the decision-making mechanisms.

### Model 1: Post-Error Beta Modulation
This model tests the hypothesis that OCI symptoms regulate the decision noise (or rigidity) specifically following unrewarded trials (errors). High OCI is often associated with anxiety and error-monitoring alterations; this model posits that participants may become more rigid (higher beta) or more erratic (lower beta) in their choices immediately after failing to receive a reward, while their baseline behavior after rewards remains stable.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Post-Error Beta Modulation Model.
    
    Hypothesis: OCI modulates the inverse temperature (beta) specifically after unrewarded trials.
    This captures potential 'post-error rigidity' or anxiety-induced exploration.
    
    Parameters:
    - alpha: [0, 1] Learning rate for value updates.
    - beta_base: [0, 10] Baseline inverse temperature (used after rewarded trials).
    - beta_error_mod: [0, 1] Modulation parameter for beta after unrewarded trials.
        The effective beta after an error is: beta_base + (beta_error_mod - 0.5) * 20 * oci.
        This allows OCI to either increase or decrease beta after errors.
    - w: [0, 1] Weighting between Model-Based (1) and Model-Free (0) strategies.
    - stickiness: [0, 5] Choice perseveration bonus.
    """
    alpha, beta_base, beta_error_mod, w, stickiness = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Map [0,1] parameter to a modulation range (e.g., -10 to +10)
    mod_value = (beta_error_mod - 0.5) * 20.0 
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    prev_reward = 1.0 # Initialize assuming neutral/successful start

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        if a1 < 0 or s_idx < 0 or a2 < 0:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        # Determine current beta based on previous outcome
        if prev_reward > 0:
            current_beta = beta_base
        else:
            # Apply OCI modulation on error trials
            current_beta = beta_base + (mod_value * current_oci)
            
        current_beta = max(0.0, current_beta) # Ensure beta remains non-negative

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        q_augmented = q_net.copy()
        if last_action_1 != -1:
            q_augmented[last_action_1] += stickiness
            
        exp_q1 = np.exp(current_beta * q_augmented)
        # Numerical stability
        if np.any(np.isinf(exp_q1)):
            max_val = np.max(current_beta * q_augmented)
            exp_q1 = np.exp(current_beta * q_augmented - max_val)
            
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Stage 2 Policy
        exp_q2 = np.exp(current_beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # Updates
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        # Standard TD(1) update logic
        q_stage2_mf[s_idx, a2] += alpha * delta_stage2
        q_stage1_mf[a1] += alpha * (delta_stage1 + delta_stage2)
        
        last_action_1 = a1
        prev_reward = r

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Transition Belief
This model investigates if OCI symptoms distort the participant's internal model of the environment structure. Standard Model-Based learning assumes the agent knows the true transition probabilities (0.7/0.3). This model proposes that OCI might lead to an over-belief in determinism (thinking the common transition is >0.7) or uncertainty (thinking it is closer to 0.5), which affects how the Model-Based values are computed.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Transition Belief Model.
    
    Hypothesis: OCI affects the subject's belief about the transition matrix probabilities used in MB planning.
    High OCI might correlate with a distorted causal model (e.g., believing transitions are more deterministic).
    
    Parameters:
    - alpha: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] MB/MF mixing weight.
    - stickiness: [0, 5] Choice perseveration.
    - trans_base: [0.5, 1.0] Baseline belief about the common transition probability.
    - trans_oci_mod: [0, 1] OCI modulation of transition belief.
        subjective_p = trans_base + (trans_oci_mod - 0.5) * oci
    """
    alpha, beta, w, stickiness, trans_base, trans_oci_mod = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate subjective transition probability
    # Modulate around the base belief
    mod = (trans_oci_mod - 0.5) # Range -0.5 to 0.5
    subjective_p = trans_base + (mod * current_oci)
    subjective_p = np.clip(subjective_p, 0.0, 1.0)
    
    # Construct subjective transition matrix
    # Assumes symmetry: P(X|A) = p, P(Y|U) = p
    T_subj = np.array([[subjective_p, 1-subjective_p], [1-subjective_p, subjective_p]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        if a1 < 0 or s_idx < 0 or a2 < 0:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        # Stage 1 Policy (MB uses subjective T)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = T_subj @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        q_augmented = q_net.copy()
        if last_action_1 != -1:
            q_augmented[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_augmented)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # Updates
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        q_stage2_mf[s_idx, a2] += alpha * delta_stage2
        q_stage1_mf[a1] += alpha * (delta_stage1 + delta_stage2)
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Stage-Specific Learning Rate Modulation
This model proposes that OCI differentially affects learning at different hierarchical levels of the task. Specifically, it allows the learning rate for the Stage 1 "Spaceship" values (higher-order planning/transition learning) to be modulated by OCI, distinct from the learning rate for Stage 2 "Alien" values (immediate reward learning).

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Stage-Specific Learning Rate Model.
    
    Hypothesis: OCI differentially affects the learning of higher-order transitions (Stage 1) 
    versus immediate rewards (Stage 2).
    
    Parameters:
    - alpha_2: [0, 1] Learning rate for Stage 2 (Alien values).
    - alpha_1_mod: [0, 1] OCI-dependent modulation for Stage 1 Learning Rate.
        alpha_1 = alpha_2 + (alpha_1_mod - 0.5) * 2 * oci
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] MB/MF mixing weight.
    - stickiness: [0, 5] Choice perseveration.
    """
    alpha_2, alpha_1_mod, beta, w, stickiness = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate Stage 1 LR relative to Stage 2 LR
    # Allows alpha_1 to be higher or lower than alpha_2 based on OCI
    mod = (alpha_1_mod - 0.5) * 2.0
    alpha_1 = alpha_2 + (mod * current_oci)
    alpha_1 = np.clip(alpha_1, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        if a1 < 0 or s_idx < 0 or a2 < 0:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        q_augmented = q_net.copy()
        if last_action_1 != -1:
            q_augmented[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_augmented)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # Updates
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        # Update Stage 2 with alpha_2
        q_stage2_mf[s_idx, a2] += alpha_2 * delta_stage2
        
        # Update Stage 1 with alpha_1 (using accumulated error)
        q_stage1_mf[a1] += alpha_1 * (delta_stage1 + delta_stage2)
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```