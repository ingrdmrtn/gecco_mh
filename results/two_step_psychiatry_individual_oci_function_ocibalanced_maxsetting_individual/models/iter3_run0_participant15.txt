Here are three new cognitive models that incorporate the OCI score to explain the participant's decision-making behavior, specifically addressing the observed "stickiness" and learning patterns.

### Model 1: OCI-Modulated Decay of Unchosen Options
This model posits that individuals with higher OCI scores exhibit a "tunnel vision" effect where the value of unchosen options decays more rapidly. This leads to perseveration on the chosen option because the alternative option's value degrades over time, making it less attractive to switch even if the current option yields mixed rewards.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: MF/MB Hybrid with OCI-modulated decay of unchosen Stage 1 values.
    High OCI leads to faster forgetting of unchosen paths, reinforcing the current choice.

    Parameters:
    lr: [0, 1] Learning rate for value updates.
    beta: [0, 10] Inverse temperature (exploration/exploitation).
    w: [0, 1] Weight for Model-Based control (0=Pure MF, 1=Pure MB).
    stick: [0, 5] General choice stickiness/perseveration.
    decay_oci: [0, 1] Decay rate for unchosen Stage 1 actions, scaled by OCI.
    """
    lr, beta, w, stick, decay_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Transition matrix (fixed for this task)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2) 
    q_stage2_mf = np.zeros((2, 2)) 
    
    last_action_1 = -1

    for trial in range(n_trials):
        # Skip invalid trials (timeouts)
        if action_1[trial] == -1 or action_2[trial] == -1:
            continue

        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2

        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness to the previously chosen action
        if last_action_1 != -1:
            q_net[last_action_1] += stick

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # 1. Update Stage 1 MF value for the CHOSEN action
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        # 2. Decay Stage 1 MF value for the UNCHOSEN action
        # This mechanism drives the tunnel vision effect
        unchosen_a1 = 1 - a1
        decay_rate = decay_oci * oci_score
        # Ensure decay doesn't invert sign arbitrarily, just shrinks towards 0
        q_stage1_mf[unchosen_a1] *= (1.0 - decay_rate)

        # 3. Update Stage 2 MF value
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_stage2
        
        last_action_1 = a1

    eps = 1e-10
    # Sum log loss only for valid trials where probability was recorded
    valid_mask = (p_choice_1 > 0) & (p_choice_2 > 0)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Stage 1 Learning Suppression
This model hypothesizes that high OCI scores are associated with rigidity in high-level choices (Stage 1). The model implements this by splitting the learning rate: a base rate for Stage 2 (direct outcome learning) and a suppressed rate for Stage 1 (planning/choice), where the suppression is proportional to the OCI score. This makes the first-stage choice harder to update/unlearn.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: Dual Learning Rate model where OCI suppresses Stage 1 learning.
    High OCI reduces the learning rate for the spaceship choice (Stage 1), 
    leading to rigid habits, while alien learning (Stage 2) remains normal.

    Parameters:
    lr_base: [0, 1] Base learning rate (applied to Stage 2).
    beta: [0, 10] Inverse temperature.
    w: [0, 1] MB/MF weight.
    stick: [0, 5] General stickiness.
    lr1_impair_oci: [0, 1] Factor by which OCI reduces Stage 1 learning rate.
    """
    lr_base, beta, w, stick, lr1_impair_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    
    # Calculate the impaired learning rate for Stage 1
    # If OCI is high, lr_stage1 becomes smaller than lr_base
    lr_stage1 = lr_base * (1.0 - (lr1_impair_oci * oci_score))
    # Ensure it doesn't go below 0
    if lr_stage1 < 0: lr_stage1 = 0.0
    
    lr_stage2 = lr_base

    for trial in range(n_trials):
        if action_1[trial] == -1 or action_2[trial] == -1:
            continue

        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_action_1 != -1:
            q_net[last_action_1] += stick

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Update Stage 1 using the suppressed learning rate
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr_stage1 * delta_stage1

        # Update Stage 2 using the base learning rate
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr_stage2 * delta_stage2
        
        last_action_1 = a1

    eps = 1e-10
    valid_mask = (p_choice_1 > 0) & (p_choice_2 > 0)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Lose-Stay Stickiness
This model investigates pathological persistence. While normal behavior involves "Win-Stay, Lose-Shift", compulsive behavior might involve "Lose-Stay" (persisting despite negative feedback). Here, OCI adds a specific "stickiness bonus" to the previously chosen option *specifically* after a loss (or no reward), counteracting the normal tendency to switch.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: MF/MB Hybrid with OCI-modulated 'Lose-Stay' stickiness.
    High OCI adds a perseveration bonus specifically after unrewarded trials,
    mimicking compulsive repetition despite lack of positive feedback.

    Parameters:
    lr: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] MB/MF weight.
    stick_base: [0, 5] Baseline stickiness (applied after any trial).
    stick_loss_oci: [0, 5] Additional stickiness after a LOSS, scaled by OCI.
    """
    lr, beta, w, stick_base, stick_loss_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    last_reward = 0

    for trial in range(n_trials):
        if action_1[trial] == -1 or action_2[trial] == -1:
            continue

        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Apply Stickiness
        if last_action_1 != -1:
            # Base stickiness applied always
            total_stick = stick_base
            
            # Additional 'compulsive' stickiness if the last outcome was a loss (<=0)
            if last_reward <= 0:
                total_stick += (stick_loss_oci * oci_score)
            
            q_net[last_action_1] += total_stick

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1

        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_stage2
        
        last_action_1 = a1
        last_reward = r

    eps = 1e-10
    valid_mask = (p_choice_1 > 0) & (p_choice_2 > 0)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss
```