Here are three cognitive models based on the participant's data and OCI score.

### Model 1: Hybrid Learner with OCI-Modulated Learning Rate
This model posits that the participant uses a hybrid Model-Based (planning) and Model-Free (habit) strategy, but their general rate of learning (updating values based on prediction errors) is modulated by their OCI score. This tests the hypothesis that obsessive-compulsive symptoms correlate with either hyper-plasticity (rapid updating) or rigidity (slow updating) in value estimation.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based / Model-Free learner where the learning rate is modulated by OCI.
    Includes a fixed stickiness parameter.
    
    Parameters:
    - lr_base: Base logit for learning rate.
    - lr_oci_mod: Effect of OCI on learning rate (logit scale).
    - beta: Inverse temperature [0, 10].
    - w: Mixing weight for MB vs MF [0, 1].
    - stickiness: Choice perseveration bonus.
    """
    lr_base, lr_oci_mod, beta, w, stickiness = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    # Sigmoid transformation to keep learning rate in [0, 1]
    # lr = 1 / (1 + exp(-(base + mod * oci)))
    lr_logit = lr_base + (lr_oci_mod * current_oci)
    # Clip logit to avoid overflow
    lr_logit = np.clip(lr_logit, -20, 20)
    learning_rate = 1.0 / (1.0 + np.exp(-lr_logit))

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # 2 planets, 2 aliens
    
    prev_a1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # Model-Based Value: T * max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Value: w * MB + (1-w) * MF
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add Stickiness
        if prev_a1 != -1:
            q_net_1[prev_a1] += stickiness
            
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        if action_1[trial] != -1:
            a1 = int(action_1[trial])
            p_choice_1[trial] = probs_1[a1]
            prev_a1 = a1
        else:
            p_choice_1[trial] = 1.0
            prev_a1 = -1
            continue # Skip learning for missing trials

        s_idx = int(state[trial])

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        if action_2[trial] != -1:
            a2 = int(action_2[trial])
            p_choice_2[trial] = probs_2[a2]
            r = reward[trial]

            # --- Learning ---
            # Update Stage 2 Q-values (MF only)
            pe_2 = r - q_stage2_mf[s_idx, a2]
            q_stage2_mf[s_idx, a2] += learning_rate * pe_2
            
            # Update Stage 1 Q-values (MF only)
            # TD(0) update: target is Q_stage2 of chosen option
            pe_1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
            q_stage1_mf[a1] += learning_rate * pe_1
            
        else:
            p_choice_2[trial] = 1.0

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Model-Free with Asymmetric Learning Rates (OCI-Modulated Negative Learning)
This model assumes a pure Model-Free strategy (habitual) but investigates "Perfectionism/Punishment Sensitivity." It proposes that the learning rate for negative outcomes (0 coins) is modulated by OCI, while the learning rate for positive outcomes (1 coin) is fixed. High OCI might lead to over-correction after failure.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner with asymmetric learning rates.
    The negative learning rate (response to 0 reward) is modulated by OCI.
    
    Parameters:
    - lr_pos: Learning rate for positive prediction errors [0, 1].
    - lr_neg_base: Base logit for negative learning rate.
    - lr_neg_oci: Effect of OCI on negative learning rate.
    - beta: Inverse temperature [0, 10].
    - stickiness: Choice perseveration bonus.
    """
    lr_pos, lr_neg_base, lr_neg_oci, beta, stickiness = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    # Calculate negative learning rate via sigmoid
    lr_neg_logit = lr_neg_base + (lr_neg_oci * current_oci)
    lr_neg_logit = np.clip(lr_neg_logit, -20, 20)
    lr_neg = 1.0 / (1.0 + np.exp(-lr_neg_logit))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    prev_a1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        q_net_1 = q_stage1.copy()
        
        if prev_a1 != -1:
            q_net_1[prev_a1] += stickiness
            
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        if action_1[trial] != -1:
            a1 = int(action_1[trial])
            p_choice_1[trial] = probs_1[a1]
            prev_a1 = a1
        else:
            p_choice_1[trial] = 1.0
            prev_a1 = -1
            continue

        s_idx = int(state[trial])

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        if action_2[trial] != -1:
            a2 = int(action_2[trial])
            p_choice_2[trial] = probs_2[a2]
            r = reward[trial]

            # --- Learning with Asymmetry ---
            # Stage 2 Update
            pe_2 = r - q_stage2[s_idx, a2]
            alpha_2 = lr_pos if pe_2 >= 0 else lr_neg
            q_stage2[s_idx, a2] += alpha_2 * pe_2
            
            # Stage 1 Update
            pe_1 = q_stage2[s_idx, a2] - q_stage1[a1]
            alpha_1 = lr_pos if pe_1 >= 0 else lr_neg
            q_stage1[a1] += alpha_1 * pe_1
            
        else:
            p_choice_2[trial] = 1.0

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Learner with OCI-Modulated Strategy Weight (w)
This model tests the core "Compulsivity" hypothesis: that OCI symptoms shift the balance between Goal-Directed (Model-Based) and Habitual (Model-Free) control. Unlike previous attempts, this model includes a fixed *stickiness* parameter, which is crucial for model fit, allowing the OCI modulation to specifically target the strategic balance ($w$) rather than perseveration.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based / Model-Free learner where the mixing weight (w) is modulated by OCI.
    Includes a fixed stickiness parameter.
    
    Parameters:
    - learning_rate: Fixed learning rate [0, 1].
    - beta: Inverse temperature [0, 10].
    - stickiness: Choice perseveration bonus.
    - w_base: Base logit for mixing weight w.
    - w_oci_mod: Effect of OCI on mixing weight w.
    """
    learning_rate, beta, stickiness, w_base, w_oci_mod = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    # Calculate w (weight of Model-Based) via sigmoid
    w_logit = w_base + (w_oci_mod * current_oci)
    w_logit = np.clip(w_logit, -20, 20)
    w = 1.0 / (1.0 + np.exp(-w_logit))

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_a1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combined Value
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if prev_a1 != -1:
            q_net_1[prev_a1] += stickiness
            
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        if action_1[trial] != -1:
            a1 = int(action_1[trial])
            p_choice_1[trial] = probs_1[a1]
            prev_a1 = a1
        else:
            p_choice_1[trial] = 1.0
            prev_a1 = -1
            continue

        s_idx = int(state[trial])

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        if action_2[trial] != -1:
            a2 = int(action_2[trial])
            p_choice_2[trial] = probs_2[a2]
            r = reward[trial]

            # --- Learning ---
            pe_2 = r - q_stage2_mf[s_idx, a2]
            q_stage2_mf[s_idx, a2] += learning_rate * pe_2
            
            pe_1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
            q_stage1_mf[a1] += learning_rate * pe_1
            
        else:
            p_choice_2[trial] = 1.0

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```