Here are the three proposed cognitive models.

### Model 1: Asymmetric Learning Rates with OCI-Modulated Strategy Weight
This model hypothesizes that the participant processes positive and negative prediction errors differently (asymmetric learning rates), a feature often linked to dopaminergic signaling. Furthermore, the OCI score is proposed to modulate the balance between Model-Based (goal-directed) and Model-Free (habitual) control (`w`), reflecting the theory that compulsivity involves a shift towards habitual control.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Asymmetric Learning Rates with OCI-Modulated Strategy Weight.
    
    Hypothesis: The participant has different sensitivities to positive and negative
    prediction errors (lr_pos, lr_neg). The OCI score modulates the mixing weight 'w'
    between Model-Based and Model-Free strategies, testing if compulsivity relates
    to a deficit in Model-Based control.
    
    Parameters:
    lr_pos: [0, 1] - Learning rate for positive prediction errors.
    lr_neg: [0, 1] - Learning rate for negative prediction errors.
    beta: [0, 10] - Inverse temperature (softmax noise).
    stickiness: [-5, 5] - Choice perseveration bonus.
    w_base: [0, 1] - Baseline mixing weight (1=MB, 0=MF).
    w_oci: [-1, 1] - Modulation of w by OCI score.
    """
    lr_pos, lr_neg, beta, stickiness, w_base, w_oci = model_parameters
    n_trials = len(action_1)
    oci_val = oci[0]
    
    # Calculate OCI-modulated mixing weight
    w = w_base + (w_oci * oci_val)
    w = np.clip(w, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net
        if last_action_1 != -1:
            logits_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        last_action_1 = int(action_1[trial])
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        
        # --- Stage 2 Choice ---
        logits_2 = beta * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning Updates ---
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        lr_1 = lr_pos if delta_stage1 > 0 else lr_neg
        q_stage1_mf[a1] += lr_1 * delta_stage1
        
        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        lr_2 = lr_pos if delta_stage2 > 0 else lr_neg
        q_stage2_mf[state_idx, a2] += lr_2 * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Stickiness Decay
This model introduces a "Stickiness Trace" that decays over time, rather than a simple one-step repetition bonus. The OCI score modulates the *decay rate* of this trace. High OCI might be associated with a slower decay of past action representations (stronger habits/perseveration), while low OCI might imply faster forgetting of past choices.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Stickiness Decay.
    
    Hypothesis: Stickiness is not just a 1-trial repetition bonus but a decaying trace 
    of past actions. OCI scores modulate the decay rate of this trace, reflecting 
    how persistent the urge to repeat actions is (compulsivity/habit strength).
    
    Parameters:
    learning_rate: [0, 1] - Learning rate for Q-values.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - MB/MF mixing weight.
    stick_mag: [-5, 5] - Magnitude of the stickiness influence.
    decay_base: [0, 1] - Baseline decay rate for the stickiness trace.
    decay_oci: [-1, 1] - Modulation of decay rate by OCI.
    """
    learning_rate, beta, w, stick_mag, decay_base, decay_oci = model_parameters
    n_trials = len(action_1)
    oci_val = oci[0]
    
    # Calculate OCI-modulated decay rate
    decay = decay_base + (decay_oci * oci_val)
    decay = np.clip(decay, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Stickiness trace for Stage 1 actions [Action A, Action U]
    stick_trace = np.zeros(2)
    
    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness based on the accumulated trace
        logits_1 = beta * q_net + stick_mag * stick_trace
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # Update stickiness trace: Decay then reinforce chosen action
        stick_trace *= decay
        stick_trace[int(action_1[trial])] += 1.0
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        
        # --- Stage 2 Choice ---
        logits_2 = beta * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning Updates ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Transition Belief
This model tests the hypothesis that OCI scores correlate with a distorted internal model of the environment's transition probabilities. High OCI (often associated with intolerance of uncertainty) might lead a participant to overestimate the determinism of the transitions (believing the probability is closer to 1.0 or closer to 0.5) compared to the true 0.7 probability. This distorted matrix is then used for the Model-Based value calculation.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Transition Belief.
    
    Hypothesis: OCI scores distort the perceived transition probabilities used in 
    Model-Based planning. For example, high OCI might correlate with an overestimation 
    of transition determinism (intolerance of uncertainty) or underestimation.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate for Q-values.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - MB/MF mixing weight.
    stickiness: [-5, 5] - Choice perseveration bonus.
    trans_base: [0, 1] - Baseline belief of the common transition probability.
    trans_oci: [-1, 1] - Modulation of transition belief by OCI.
    """
    learning_rate, beta, w, stickiness, trans_base, trans_oci = model_parameters
    n_trials = len(action_1)
    oci_val = oci[0]
    
    # Calculate OCI-modulated transition belief
    p_common = trans_base + (trans_oci * oci_val)
    p_common = np.clip(p_common, 0.0, 1.0)
    
    # Construct the subjective transition matrix
    # Row 0: Action 0 (A) -> [Prob(State 0), Prob(State 1)]
    # Row 1: Action 1 (U) -> [Prob(State 0), Prob(State 1)]
    # Assuming symmetry: P(S0|A0) = p, P(S1|A1) = p
    transition_matrix = np.array([[p_common, 1.0 - p_common], 
                                  [1.0 - p_common, p_common]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        # Use the subjective transition matrix for MB calculation
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net
        if last_action_1 != -1:
            logits_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        last_action_1 = int(action_1[trial])
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        
        # --- Stage 2 Choice ---
        logits_2 = beta * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning Updates ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```