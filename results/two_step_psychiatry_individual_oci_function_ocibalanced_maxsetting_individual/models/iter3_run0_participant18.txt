Here are three cognitive models that incorporate the OCI score into the decision-making process for the two-step task.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Model-Based Weight (Habit Hypothesis).
    Hypothesis: Obsessive-Compulsive symptoms are associated with a deficit in goal-directed (Model-Based) control.
    This model assumes that the balance between Model-Based (MB) and Model-Free (MF) control, denoted by 'w',
    is negatively modulated by the OCI score. Higher OCI scores reduce 'w', leading to more habitual behavior.

    Parameters:
    - lr: Learning rate [0,1]
    - beta: Inverse temperature (softmax) [0,10]
    - w_base: Baseline Model-Based weight for a participant with OCI=0 [0,1]
    - lambda_val: Eligibility trace decay parameter [0,1]
    - stickiness: Choice perseveration parameter [0,5]
    - oci_penalty: Sensitivity of w to OCI score [0,1]. 
                   Effective w = w_base * (1 - oci_penalty * OCI).
    """
    lr, beta, w_base, lambda_val, stickiness, oci_penalty = model_parameters
    oci_score = oci[0]
    n_trials = len(action_1)
    
    # Calculate effective MB weight based on OCI
    w_eff = w_base * (1.0 - oci_penalty * oci_score)
    w_eff = np.clip(w_eff, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Initialize Q-values
    # Stage 1: 2 actions
    q_mf_s1 = np.zeros(2) 
    # Stage 2: 2 states x 2 actions
    q_mf_s2 = np.zeros((2, 2)) 
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1
    
    for t in range(n_trials):
        # Handle missing data
        if action_1[t] == -1:
            p_choice_1[t] = 1.0
            p_choice_2[t] = 1.0
            continue
            
        a1 = int(action_1[t])
        s2 = int(state[t])
        a2 = int(action_2[t])
        r = reward[t]

        # --- Stage 1 Policy ---
        # Model-Based value: Expectation over transition matrix * max Q_MF(s2)
        max_q_s2 = np.max(q_mf_s2, axis=1)
        q_mb_s1 = transition_matrix @ max_q_s2
        
        # Net value: mixture of MB and MF
        q_net = w_eff * q_mb_s1 + (1 - w_eff) * q_mf_s1
        logits_1 = beta * q_net
        
        # Add stickiness to the previously chosen action
        if prev_a1 != -1:
            logits_1[prev_a1] += stickiness
            
        # Softmax for Choice 1
        exp_1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_1 / np.sum(exp_1)
        p_choice_1[t] = probs_1[a1]

        # --- Stage 2 Policy ---
        # Standard Model-Free choice
        logits_2 = beta * q_mf_s2[s2]
        exp_2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_2 / np.sum(exp_2)
        p_choice_2[t] = probs_2[a2]

        # --- Updates ---
        # Stage 1 TD Error (SARSA-like update for Stage 1 MF value)
        delta_1 = q_mf_s2[s2, a2] - q_mf_s1[a1]
        q_mf_s1[a1] += lr * delta_1
        
        # Stage 2 TD Error (Reward prediction error)
        delta_2 = r - q_mf_s2[s2, a2]
        q_mf_s2[s2, a2] += lr * delta_2
        
        # Eligibility Trace Update: Propagate Stage 2 RPE back to Stage 1 choice
        q_mf_s1[a1] += lr * lambda_val * delta_2
        
        prev_a1 = a1

    eps = 1e-10
    valid_mask = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Enhanced Perseveration (Compulsion Hypothesis).
    Hypothesis: OCI is linked to compulsive, repetitive behaviors ("stickiness").
    This model posits that the stickiness parameter (tendency to repeat the previous Stage 1 choice)
    increases linearly with the OCI score.
    
    Parameters:
    - lr: Learning rate [0,1]
    - beta: Inverse temperature [0,10]
    - w: Model-Based weight [0,1]
    - lambda_val: Eligibility trace decay [0,1]
    - stick_base: Baseline stickiness for OCI=0 [0,5]
    - stick_oci_sens: Sensitivity of stickiness to OCI [0,5]. 
                      Effective stickiness = stick_base + stick_oci_sens * OCI.
    """
    lr, beta, w, lambda_val, stick_base, stick_oci_sens = model_parameters
    oci_score = oci[0]
    n_trials = len(action_1)
    
    # Calculate effective stickiness modulated by OCI
    stickiness = stick_base + stick_oci_sens * oci_score
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_mf_s1 = np.zeros(2) 
    q_mf_s2 = np.zeros((2, 2)) 
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1
    
    for t in range(n_trials):
        if action_1[t] == -1:
            p_choice_1[t] = 1.0
            p_choice_2[t] = 1.0
            continue
            
        a1 = int(action_1[t])
        s2 = int(state[t])
        a2 = int(action_2[t])
        r = reward[t]

        # Stage 1 Policy
        max_q_s2 = np.max(q_mf_s2, axis=1)
        q_mb_s1 = transition_matrix @ max_q_s2
        
        q_net = w * q_mb_s1 + (1 - w) * q_mf_s1
        logits_1 = beta * q_net
        
        # Apply OCI-modulated stickiness
        if prev_a1 != -1:
            logits_1[prev_a1] += stickiness
            
        exp_1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_1 / np.sum(exp_1)
        p_choice_1[t] = probs_1[a1]

        # Stage 2 Policy
        logits_2 = beta * q_mf_s2[s2]
        exp_2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_2 / np.sum(exp_2)
        p_choice_2[t] = probs_2[a2]

        # Updates
        delta_1 = q_mf_s2[s2, a2] - q_mf_s1[a1]
        q_mf_s1[a1] += lr * delta_1
        
        delta_2 = r - q_mf_s2[s2, a2]
        q_mf_s2[s2, a2] += lr * delta_2
        
        q_mf_s1[a1] += lr * lambda_val * delta_2
        
        prev_a1 = a1

    eps = 1e-10
    valid_mask = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Error Sensitivity (Anxiety/Perfectionism Hypothesis).
    Hypothesis: Individuals with higher OCI scores are hypersensitive to errors (negative prediction errors).
    This model increases the learning rate specifically when the outcome is worse than expected (negative RPE),
    reflecting a "fear of failure" or perfectionistic checking mechanism.
    
    Parameters:
    - lr: Base learning rate for positive or neutral outcomes [0,1]
    - beta: Inverse temperature [0,10]
    - w: Model-Based weight [0,1]
    - lambda_val: Eligibility trace decay [0,1]
    - stickiness: Choice perseveration [0,5]
    - error_sens: Sensitivity factor for negative RPEs [0,5].
                  If RPE < 0: lr_eff = lr * (1 + error_sens * OCI).
    """
    lr, beta, w, lambda_val, stickiness, error_sens = model_parameters
    oci_score = oci[0]
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_mf_s1 = np.zeros(2) 
    q_mf_s2 = np.zeros((2, 2)) 
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1
    
    for t in range(n_trials):
        if action_1[t] == -1:
            p_choice_1[t] = 1.0
            p_choice_2[t] = 1.0
            continue
            
        a1 = int(action_1[t])
        s2 = int(state[t])
        a2 = int(action_2[t])
        r = reward[t]

        # Stage 1 Policy
        max_q_s2 = np.max(q_mf_s2, axis=1)
        q_mb_s1 = transition_matrix @ max_q_s2
        
        q_net = w * q_mb_s1 + (1 - w) * q_mf_s1
        logits_1 = beta * q_net
        
        if prev_a1 != -1:
            logits_1[prev_a1] += stickiness
            
        exp_1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_1 / np.sum(exp_1)
        p_choice_1[t] = probs_1[a1]

        # Stage 2 Policy
        logits_2 = beta * q_mf_s2[s2]
        exp_2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_2 / np.sum(exp_2)
        p_choice_2[t] = probs_2[a2]

        # Updates
        delta_1 = q_mf_s2[s2, a2] - q_mf_s1[a1]
        
        # Standard update for Stage 1 (based on expected value of S2)
        q_mf_s1[a1] += lr * delta_1
        
        delta_2 = r - q_mf_s2[s2, a2]
        
        # Determine Learning Rate for Stage 2 outcome update
        if delta_2 < 0:
            # Negative RPE: Boost learning rate based on OCI
            current_lr = lr * (1.0 + error_sens * oci_score)
            current_lr = min(current_lr, 1.0) # Clip to max 1.0
        else:
            current_lr = lr
            
        q_mf_s2[s2, a2] += current_lr * delta_2
        
        # Eligibility Trace Update for Stage 1 using the outcome-dependent LR
        q_mf_s1[a1] += current_lr * lambda_val * delta_2
        
        prev_a1 = a1

    eps = 1e-10
    valid_mask = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss
```