Here are three new cognitive models. These models focus on different mechanisms where OCI symptoms might influence decision-making: the balance between model-based and model-free control, asymmetric learning from positive/negative outcomes, and the impact of uncertainty (exploration).

### Model 1: OCI-Modulated Model-Based/Model-Free Balance
This model hypothesizes that OCI symptoms affect the balance between goal-directed (Model-Based) and habitual (Model-Free) control. Specifically, it tests if higher OCI scores lead to a stronger reliance on habitual, repetitive behaviors (Model-Free) at the expense of flexible planning (Model-Based).

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Model-Based/Model-Free Balance.
    
    This model assumes a hybrid learner (MB + MF). The mixing weight 'w' determines
    the balance between MB and MF systems.
    Hypothesis: OCI score modulates 'w', shifting the balance towards MF (habitual) control.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for value updates.
    - beta: [0, 10] Inverse temperature (softmax sensitivity).
    - w_base: [0, 1] Baseline mixing weight (0 = pure MF, 1 = pure MB).
    - w_oci_mod: [-1, 1] Modulation of mixing weight by OCI score.
    """
    learning_rate, beta, w_base, w_oci_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective mixing weight, clamped between 0 and 1
    # If w_oci_mod is negative, high OCI reduces w (more MF).
    w = w_base + (w_oci_mod * oci_score)
    w = np.clip(w, 0.0, 1.0)

    # Transition matrix (fixed for this task structure: A->X commonly, U->Y commonly)
    # Indices: 0->X, 1->Y. A=0, U=1.
    # P(State|Action)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_mf = np.zeros((2, 2))  # Stage 1 MF values (2 actions)
    q_stage2 = np.zeros((2, 2)) # Stage 2 values (2 states x 2 actions)
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])
        
        if a1 == -1: # Skip invalid trials
            continue

        # --- Stage 1 Policy ---
        # Model-Based Value: V_MB(s1) = Sum(P(s2|s1,a1) * max(Q_stage2(s2, :)))
        max_q_stage2 = np.max(q_stage2, axis=1) # Max value for each state (X, Y)
        q_mb = transition_matrix @ max_q_stage2 # Expected value based on transition probs
        
        # Integrated Value: Q_net = w * Q_MB + (1-w) * Q_MF
        q_net = w * q_mb + (1 - w) * q_mf[0] # q_mf[0] represents stage 1 actions
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        # Stage 2 Update (TD)
        delta_stage2 = r - q_stage2[s2, a2]
        q_stage2[s2, a2] += learning_rate * delta_stage2
        
        # Stage 1 Update (TD-0 / SARSA-like for MF)
        # Note: In standard hybrid models, MF is updated by TD error from stage 2 transition
        # delta_1 = Q_stage2(s2, a2) - Q_MF(s1, a1)
        # However, simpler implementations often just update stage 1 based on final reward or max stage 2
        # Here we use the standard TD(1) lambda=1 approximation often used in 2-step tasks:
        # direct reinforcement of stage 1 choice by the final reward.
        
        delta_stage1 = r - q_mf[0, a1]
        q_mf[0, a1] += learning_rate * delta_stage1

    eps = 1e-10
    valid_trials = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```

### Model 2: OCI-Specific Loss Aversion (Asymmetric Learning)
This model investigates whether high OCI scores are associated with an increased sensitivity to negative outcomes (absence of reward). It splits the learning rate into separate components for positive prediction errors (receiving gold) and negative prediction errors (receiving nothing), with OCI modulating the negative learning rate.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Specific Loss Aversion (Asymmetric Learning).
    
    This is a pure Model-Free learner that updates values differently depending on
    whether the outcome was better or worse than expected.
    Hypothesis: High OCI individuals over-learn from failures (0 reward).
    
    Parameters:
    - lr_pos: [0, 1] Learning rate for positive prediction errors (Reward > Expectation).
    - lr_neg_base: [0, 1] Base learning rate for negative prediction errors.
    - lr_neg_oci: [0, 1] Additional sensitivity to negative errors scaling with OCI.
    - beta: [0, 10] Inverse temperature.
    - decay: [0, 1] Decay rate for unchosen options (forgetting).
    """
    lr_pos, lr_neg_base, lr_neg_oci, beta, decay = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective negative learning rate
    lr_neg = lr_neg_base + (lr_neg_oci * oci_score)
    lr_neg = np.clip(lr_neg, 0.0, 1.0)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values for Stage 1 (2 actions) and Stage 2 (2 states x 2 actions)
    q_stage1 = np.zeros(2) 
    q_stage2 = np.zeros((2, 2))
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])
        
        if a1 == -1:
            continue

        # --- Choice 1 ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Choice 2 ---
        exp_q2 = np.exp(beta * q_stage2[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updating Stage 2 ---
        pe_2 = r - q_stage2[s2, a2]
        if pe_2 >= 0:
            q_stage2[s2, a2] += lr_pos * pe_2
        else:
            q_stage2[s2, a2] += lr_neg * pe_2
            
        # Passive decay for unchosen stage 2 alien in the current state
        unchosen_a2 = 1 - a2
        q_stage2[s2, unchosen_a2] *= (1 - decay)

        # --- Updating Stage 1 ---
        # Using simple TD(1) logic: Stage 1 is updated by the final reward
        pe_1 = r - q_stage1[a1]
        if pe_1 >= 0:
            q_stage1[a1] += lr_pos * pe_1
        else:
            q_stage1[a1] += lr_neg * pe_1
            
        # Passive decay for unchosen stage 1 spaceship
        unchosen_a1 = 1 - a1
        q_stage1[unchosen_a1] *= (1 - decay)

    eps = 1e-10
    valid_trials = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```

### Model 3: OCI-Driven Exploration Suppression
This model posits that OCI symptoms relate to intolerance of uncertainty, leading to reduced exploration. It uses a hybrid architecture where the "inverse temperature" (beta)—which controls the randomness of choice—is modulated by the OCI score. Higher OCI leads to higher beta (more deterministic/exploitation) and less random exploration.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Driven Exploration Suppression.
    
    This model assumes a standard Hybrid learner (fixed mixing weight) but allows
    the exploration/exploitation trade-off (beta) to vary by OCI.
    Hypothesis: High OCI leads to rigid exploitation (high beta), low OCI leads to more exploration.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta_base: [0, 10] Baseline inverse temperature.
    - beta_oci_slope: [0, 5] Increase in beta per unit of OCI (reduced exploration).
    - w: [0, 1] Fixed mixing weight between MB and MF.
    - stickiness: [0, 3] General choice perseveration (unrelated to OCI).
    """
    learning_rate, beta_base, beta_oci_slope, w, stickiness = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective beta
    # Higher OCI -> Higher Beta -> Sharper probabilities (less exploration)
    beta_eff = beta_base + (beta_oci_slope * oci_score)
    
    # Transition matrix
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_mf = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    last_a1 = -1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])
        
        if a1 == -1:
            last_a1 = -1
            continue
            
        # --- Stage 1 Policy ---
        # MB Value
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_mb = transition_matrix @ max_q_stage2
        
        # Net Value
        q_net = w * q_mb + (1 - w) * q_mf
        
        # Add stickiness to the Q-value of the previously chosen action
        if last_a1 != -1:
            q_net[last_a1] += stickiness
            
        exp_q1 = np.exp(beta_eff * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta_eff * q_stage2[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        # Stage 2
        delta_stage2 = r - q_stage2[s2, a2]
        q_stage2[s2, a2] += learning_rate * delta_stage2
        
        # Stage 1 (MF update via direct reinforcement)
        delta_stage1 = r - q_mf[a1]
        q_mf[a1] += learning_rate * delta_stage1
        
        last_a1 = a1

    eps = 1e-10
    valid_trials = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```