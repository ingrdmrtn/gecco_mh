Here are the three cognitive models in Python functions.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Stage 2 Stickiness.
    
    This model posits that OCI symptoms manifest as motor perseveration specifically 
    at the second stage (choosing the alien), rather than the first stage. 
    It adds a 'stickiness' bonus to the Q-values of the previously chosen alien, 
    with the magnitude of this bonus scaled by the OCI score.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for Q-value updates.
    - beta: [0, 10] Inverse temperature parameter for softmax.
    - w: [0, 1] Weighting parameter (0 = Model-Free, 1 = Model-Based).
    - st2_stick_oci: [0, 5] OCI scaling factor for Stage 2 stickiness.
    """
    learning_rate, beta, w, st2_stick_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Stickiness bonus proportional to OCI score
    stickiness_bonus = st2_stick_oci * oci_score
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_2 = -1 

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_s1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_s1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        curr_state = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Stage 2 Policy ---
        q_net_s2 = q_stage2_mf[curr_state, :].copy()
        
        # Apply stickiness to the previously chosen alien (action index)
        if last_action_2 != -1:
            q_net_s2[last_action_2] += stickiness_bonus
            
        exp_q2 = np.exp(beta * q_net_s2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Value Updates ---
        # TD(0) update for Stage 1
        delta_stage1 = q_stage2_mf[curr_state, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Update Stage 2
        delta_stage2 = r - q_stage2_mf[curr_state, a2]
        q_stage2_mf[curr_state, a2] += learning_rate * delta_stage2
        
        last_action_2 = a2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Scaled Stage 1 Learning Rate.
    
    This model hypothesizes that high OCI scores correlate with faster habit formation.
    The learning rate for the first stage (habitual choice) is boosted by the OCI score,
    while the second stage learning rate remains at the base level.
    
    Parameters:
    - learning_rate: [0, 1] Base learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Model-Based weight.
    - oci_lr_boost: [0, 5] Scaling factor for Stage 1 learning rate boost.
    """
    learning_rate, beta, w, oci_lr_boost = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate Stage 1 specific learning rate
    # Boosted by OCI, capped at 1.0 to ensure stability
    lr_s1 = learning_rate * (1 + oci_lr_boost * oci_score)
    lr_s1 = min(lr_s1, 1.0)
    lr_s2 = learning_rate # Stage 2 uses base rate

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_s1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_s1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        curr_state = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[curr_state, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Value Updates ---
        # Update Stage 1 MF using boosted LR
        delta_stage1 = q_stage2_mf[curr_state, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr_s1 * delta_stage1
        
        # Update Stage 2 MF using base LR
        delta_stage2 = r - q_stage2_mf[curr_state, a2]
        q_stage2_mf[curr_state, a2] += lr_s2 * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Stage 2 Precision.
    
    This model proposes that OCI affects the determinism (inverse temperature beta) of choices
    specifically at the second stage (alien choice). This reflects a rigid response style 
    to proximal rewards where high OCI leads to more deterministic (less noisy) selection 
    of the best alien.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta_base: [0, 10] Base inverse temperature.
    - w: [0, 1] Model-Based weight.
    - oci_beta2_scale: [0, 5] Scaling factor for Stage 2 beta.
    """
    learning_rate, beta_base, w, oci_beta2_scale = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Stage 1 uses base beta
    beta_s1 = beta_base
    # Stage 2 beta is scaled by OCI (High OCI = Higher Beta = More Deterministic)
    beta_s2 = beta_base * (1 + oci_beta2_scale * oci_score)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_s1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_s1 * q_net_s1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        curr_state = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Stage 2 Policy ---
        # Uses OCI-scaled beta
        exp_q2 = np.exp(beta_s2 * q_stage2_mf[curr_state, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Value Updates ---
        delta_stage1 = q_stage2_mf[curr_state, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[curr_state, a2]
        q_stage2_mf[curr_state, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```