Here are 3 new cognitive models that explore different mechanisms for how OCI scores might influence decision-making in the two-step task.

### Model 1: OCI-Modulated Mixing Weight (w) with Sigmoid Transformation
This model tests the hypothesis that higher OCI scores lead to a shift in the balance between model-based (goal-directed) and model-free (habitual) control. Instead of a linear scaling which is unbounded, this model uses a sigmoid transformation to keep the mixing weight $w$ strictly between 0 and 1, where the OCI score shifts the bias of this weight. This reflects literature suggesting compulsion might relate to an over-reliance on habit (Model-Free) or rigid rule-following (Model-Based), depending on the specific pathology.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Mixing Weight (w) via Sigmoid.
    
    Hypothesis: The balance between Model-Based (MB) and Model-Free (MF) control 
    is determined by a baseline bias plus an OCI-dependent shift. We use a sigmoid 
    function to ensure the resulting weight 'w' stays validly in [0,1].
    
    w = 1 / (1 + exp(-(w_bias + w_oci_slope * oci)))
    
    Parameters:
    learning_rate: [0,1] - Update rate for Q-values.
    beta: [0,10] - Inverse temperature for softmax.
    w_bias: [-5, 5] - Baseline logit bias for the weighting parameter.
    w_oci_slope: [-5, 5] - How strongly OCI affects the logit of w.
    """
    learning_rate, beta, w_bias, w_oci_slope = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]

    # Calculate mixing weight w using a logistic function
    logit_w = w_bias + w_oci_slope * participant_oci
    w = 1.0 / (1.0 + np.exp(-logit_w))

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2) 
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # Handle missing data
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        # Model-Based value calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax choice 1
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        # Softmax choice 2
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        # Stage 2 update (TD)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

        # Stage 1 update (TD)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Dependent Learning Rate Asymmetry
This model investigates if OCI scores relate to a differential sensitivity to positive versus negative prediction errors. Individuals with higher compulsivity might be more sensitive to errors (negative outcomes) or more rigid in maintaining values despite new evidence. Here, we split the learning rate into a base rate and an OCI-modulated rate specifically for negative prediction errors (when outcomes are worse than expected).

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Dependent Learning Rate Asymmetry.
    
    Hypothesis: OCI affects how participants learn from negative prediction errors relative 
    to positive ones. We define a base learning rate, and an additional modifier that 
    scales with OCI only when the prediction error is negative.
    
    If delta < 0: effective_lr = lr_base + (lr_neg_oci_scale * oci)
    Else:         effective_lr = lr_base
    
    Parameters:
    lr_base: [0,1] - Base learning rate.
    beta: [0,10] - Inverse temperature.
    w: [0,1] - Mixing weight (MB vs MF).
    lr_neg_oci_scale: [-1, 1] - How much OCI modifies learning from negative errors.
    """
    lr_base, beta, w, lr_neg_oci_scale = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2) 
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        # Stage 2 update
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        # Determine effective learning rate based on sign of error and OCI
        current_lr = lr_base
        if delta_stage2 < 0:
            current_lr = lr_base + (lr_neg_oci_scale * participant_oci)
            # Clip to ensure stability [0, 1]
            current_lr = np.clip(current_lr, 0.0, 1.0)
            
        q_stage2_mf[s_idx, a2] += current_lr * delta_stage2

        # Stage 1 update
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        
        # Apply same logic to stage 1 error
        current_lr_s1 = lr_base
        if delta_stage1 < 0:
            current_lr_s1 = lr_base + (lr_neg_oci_scale * participant_oci)
            current_lr_s1 = np.clip(current_lr_s1, 0.0, 1.0)

        q_stage1_mf[a1] += current_lr_s1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Driven Inverse Temperature (Exploration/Exploitation)
This model posits that OCI scores influence the randomness of choice (exploration vs exploitation). High OCI might be associated with more rigid, deterministic responding (higher beta), or conversely, high anxiety might lead to more erratic behavior (lower beta). We model Beta as a linear function of OCI.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Driven Inverse Temperature (Beta).
    
    Hypothesis: OCI determines the level of decision noise (exploration/exploitation trade-off).
    A base beta is modified by the OCI score.
    
    beta_effective = beta_base + (beta_oci_slope * oci)
    
    Parameters:
    learning_rate: [0,1] - Update rate for Q-values.
    beta_base: [0,10] - Baseline inverse temperature.
    w: [0,1] - Mixing weight (MB vs MF).
    beta_oci_slope: [-5, 5] - How much OCI scales the beta parameter.
    """
    learning_rate, beta_base, w, beta_oci_slope = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]

    # Calculate effective beta, ensuring it stays non-negative
    beta_eff = beta_base + (beta_oci_slope * participant_oci)
    beta_eff = np.maximum(0.0, beta_eff) # Beta cannot be negative

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2) 
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Use effective beta
        exp_q1 = np.exp(beta_eff * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta_eff * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```