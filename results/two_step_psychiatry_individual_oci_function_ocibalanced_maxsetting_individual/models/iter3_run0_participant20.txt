Here are three cognitive models expressed as Python functions that capture different hypothesized mechanisms for how OCI symptoms might modulate decision-making in this task.

### Model 1: OCI-Modulated Forgetting
This model hypothesizes that individuals with higher OCI scores may exhibit altered "forgetting" of unchosen options. Compulsive symptoms might be linked to a tunnel vision where the value of unchosen paths decays more rapidly (or slowly), affecting the balance between exploration and exploitation.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Forgetting Model.
    
    Hypothesis: OCI modulates the rate at which the values of unchosen actions decay.
    Standard Q-learning only updates the chosen action. This model decays the 
    unchosen action's value towards 0, with the rate dependent on OCI.
    
    Parameters:
    - lr: [0, 1] Learning rate for chosen actions.
    - beta: [0, 10] Inverse temperature (softmax sensitivity).
    - w: [0, 1] Mixing weight (0 = pure Model-Free, 1 = pure Model-Based).
    - stick: [0, 5] Choice perseveration bonus (stickiness).
    - forget_base: [0, 1] Baseline forgetting rate for unchosen actions.
    - forget_oci_sens: [0, 5] Sensitivity of forgetting rate to OCI.
      forget_rate = forget_base * (1 + forget_oci_sens * oci), clipped to [0, 1].
    
    Args:
        action_1, state, action_2, reward, oci, model_parameters
        
    Returns:
        float: Negative log-likelihood.
    """
    lr, beta, w, stick, forget_base, forget_oci_sens = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate forgetting rate
    forget_rate = forget_base * (1.0 + forget_oci_sens * oci_score)
    if forget_rate > 1.0: forget_rate = 1.0
    if forget_rate < 0.0: forget_rate = 0.0

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # state x action
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_choice_1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness
        logits_1 = beta * q_net
        if last_choice_1 != -1:
            logits_1[last_choice_1] += stick
            
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        logits_2 = beta * q_stage2_mf[s2]
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        # Stage 1 Forgetting (Unchosen)
        unchosen_a1 = 1 - a1
        q_stage1_mf[unchosen_a1] *= (1.0 - forget_rate)

        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += lr * delta_stage2
        
        # Stage 2 Forgetting (Unchosen in current state)
        unchosen_a2 = 1 - a2
        q_stage2_mf[s2, unchosen_a2] *= (1.0 - forget_rate)
        
        last_choice_1 = a1

    mask = (action_1 != -1)
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss
```

### Model 2: Stage-Specific Learning Rate Modulation
This model proposes that OCI specifically impacts the learning rate for the second stage (direct reward probability learning) differently from the first stage. This reflects the hypothesis that compulsive individuals might over-attend to immediate outcomes (Stage 2) or be more rigid in their probability estimates, distinct from their learning of the task structure (Stage 1).

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Stage-Specific Learning Rate Model.
    
    Hypothesis: OCI modulates the learning rate for Stage 2 (aliens -> reward) 
    differently than Stage 1. This allows the model to capture if OCI is associated 
    with hyper-learning or rigidity specifically regarding reward probabilities, 
    independent of the structural learning in Stage 1.
    
    Parameters:
    - lr_s1: [0, 1] Learning rate for Stage 1 (Spaceships).
    - lr_s2_base: [0, 1] Base learning rate for Stage 2 (Aliens).
    - lr_s2_oci_sens: [0, 5] Sensitivity of Stage 2 learning rate to OCI.
      lr_s2 = lr_s2_base * (1 + lr_s2_oci_sens * oci), clipped to [0, 1].
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] MB/MF weight.
    - stick: [0, 5] Choice perseveration bonus.
    
    Args:
        action_1, state, action_2, reward, oci, model_parameters
        
    Returns:
        float: Negative log-likelihood.
    """
    lr_s1, lr_s2_base, lr_s2_oci_sens, beta, w, stick = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate Stage 2 learning rate
    lr_s2 = lr_s2_base * (1.0 + lr_s2_oci_sens * oci_score)
    if lr_s2 > 1.0: lr_s2 = 1.0
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_choice_1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net
        if last_choice_1 != -1:
            logits_1[last_choice_1] += stick
            
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        logits_2 = beta * q_stage2_mf[s2]
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Stage 1 Update (uses lr_s1)
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr_s1 * delta_stage1
        
        # Stage 2 Update (uses OCI-modulated lr_s2)
        delta_stage2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += lr_s2 * delta_stage2
        
        last_choice_1 = a1

    mask = (action_1 != -1)
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss
```

### Model 3: Post-Loss Rigidity (Dynamic Beta)
This model posits that OCI symptoms manifest as a reaction to failure (0 reward). High OCI participants might become more "rigid" (higher inverse temperature $\beta$) immediately following a loss, attempting to regain control or strictly exploit the best available option, whereas low OCI participants might maintain exploration.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Post-Loss Rigidity Model.
    
    Hypothesis: OCI modulates the inverse temperature (beta) specifically after 
    unrewarded trials (losses). Higher OCI is linked to increased rigidity 
    (higher beta) following a failure to obtain reward.
    
    Parameters:
    - lr: [0, 1] Learning rate.
    - beta_base: [0, 10] Baseline inverse temperature (after rewards).
    - beta_loss_oci: [0, 5] Increase in beta after a loss, scaled by OCI.
      If prev_reward == 0: beta = beta_base + beta_loss_oci * oci.
    - w: [0, 1] MB/MF weight.
    - stick: [0, 5] Choice perseveration bonus.
    
    Args:
        action_1, state, action_2, reward, oci, model_parameters
        
    Returns:
        float: Negative log-likelihood.
    """
    lr, beta_base, beta_loss_oci, w, stick = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_choice_1 = -1
    last_reward = 1 # Initialize as if previous trial was rewarded (neutral start)

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])

        # Determine current beta based on previous outcome
        current_beta = beta_base
        if last_reward == 0:
            current_beta += beta_loss_oci * oci_score
            # Cap beta to prevent numerical instability, though bounds usually handle this
            if current_beta > 20.0: current_beta = 20.0 

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = current_beta * q_net
        if last_choice_1 != -1:
            logits_1[last_choice_1] += stick
            
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        logits_2 = current_beta * q_stage2_mf[s2]
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += lr * delta_stage2
        
        last_choice_1 = a1
        last_reward = r

    mask = (action_1 != -1)
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss
```