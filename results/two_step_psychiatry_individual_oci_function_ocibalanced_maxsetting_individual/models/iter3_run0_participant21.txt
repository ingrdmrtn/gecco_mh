Here are three new cognitive models exploring different mechanisms for how OCI scores might influence learning and decision-making in this two-step task.

### Model 1: Hybrid Learner with OCI-Modulated Model-Based Weight
This model tests the hypothesis that higher OCI scores relate to a reliance on habit (model-free) over goal-directed (model-based) planning. It implements a classic hybrid reinforcement learning model where the mixing parameter $w$ (weight of model-based control) is inversely modulated by the OCI score. A higher OCI score reduces the influence of the model-based system.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free Learner with OCI-Modulated Weighting.
    
    Hypothesis: Higher OCI scores lead to reduced model-based (goal-directed) control
    and increased reliance on model-free (habitual) learning.
    
    Parameters:
    lr : [0, 1] Learning rate for value updates.
    beta : [0, 10] Inverse temperature for softmax choice.
    w_base : [0, 1] Baseline weight for model-based control.
    w_oci_penalty : [0, 1] Reduction in model-based weight proportional to OCI score.
    """
    lr, beta, w_base, w_oci_penalty = model_parameters
    n_trials = len(action_1)
    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    
    current_oci = oci[0]
    
    # Calculate the mixing weight w based on OCI
    # Ensure w stays within [0, 1]
    # Higher OCI -> Lower w (less model-based)
    w = w_base - (w_oci_penalty * current_oci)
    w = np.clip(w, 0.0, 1.0)
    
    # Fixed transition matrix for the task structure
    # A (0) -> X (0) mostly (0.7), U (1) -> Y (1) mostly (0.7)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2) # Model-free Q-values for stage 1
    q_stage2 = np.zeros((2, 2)) # Q-values for stage 2 (shared MF/MB)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        
        # --- Stage 1 Choice ---
        # Model-Based Value Calculation: V_MB(s1, a) = T(s1, a, s2) * max(Q(s2, :))
        max_q_stage2 = np.max(q_stage2, axis=1) # Max value of each state (X, Y)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value: Q_net = w * Q_MB + (1-w) * Q_MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Choice ---
        state_idx = state[trial]
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning ---
        # Update Stage 2 Q-values
        # RPE_2 = Reward - Q_stage2(s, a)
        rpe_2 = reward[trial] - q_stage2[state_idx, action_2[trial]]
        q_stage2[state_idx, action_2[trial]] += lr * rpe_2
        
        # Update Stage 1 Model-Free Q-values (TD(0))
        # Note: In pure hybrid models, stage 1 MF is often updated by the value of the state reached
        # or the reward directly if using eligibility traces. Here we use simple TD(0) from stage 2 value.
        # RPE_1 = Q_stage2(s_actual, a_chosen_in_s2) - Q_stage1_mf(a)
        # Often simplified to just the value of the state reached: V(s')
        
        # Using the value of the chosen action in stage 2 as the target
        target_stage1 = q_stage2[state_idx, action_2[trial]]
        rpe_1 = target_stage1 - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr * rpe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Loss Aversion Learner with OCI-Modulated Sensitivity
This model focuses on how rewards and lack of rewards (losses) are processed. It hypothesizes that individuals with higher OCI scores might be more sensitive to "losses" (getting 0 coins), effectively treating them as punishments rather than just neutral outcomes. The learning rate for negative outcomes is scaled by OCI.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free Learner with OCI-Modulated Loss Sensitivity.
    
    Hypothesis: OCI scores correlate with a heightened sensitivity to negative outcomes (0 coins).
    Instead of a single learning rate, there are separate rates for positive (reward=1)
    and negative (reward=0) outcomes. The negative learning rate is boosted by OCI.
    
    Parameters:
    lr_pos : [0, 1] Learning rate for positive rewards (1 coin).
    lr_neg_base : [0, 1] Base learning rate for negative outcomes (0 coins).
    lr_neg_oci_boost : [0, 1] Additional learning rate for negative outcomes scaled by OCI.
    beta : [0, 10] Inverse temperature.
    """
    lr_pos, lr_neg_base, lr_neg_oci_boost, beta = model_parameters
    n_trials = len(action_1)
    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    
    current_oci = oci[0]
    
    # Calculate effective negative learning rate
    # Bounded at 1.0
    lr_neg = lr_neg_base + (lr_neg_oci_boost * current_oci)
    lr_neg = np.clip(lr_neg, 0.0, 1.0)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Choice ---
        state_idx = state[trial]
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning ---
        # Determine which learning rate to use based on reward
        if reward[trial] == 1:
            curr_lr = lr_pos
        else:
            curr_lr = lr_neg
            
        # Update Stage 2
        rpe_2 = reward[trial] - q_stage2[state_idx, action_2[trial]]
        q_stage2[state_idx, action_2[trial]] += curr_lr * rpe_2
        
        # Update Stage 1 (TD(0) using Stage 2 value)
        # Note: We use the same lr logic for propagation, assuming the 'shock' of the outcome
        # drives the update magnitude for the whole chain.
        target_stage1 = q_stage2[state_idx, action_2[trial]]
        rpe_1 = target_stage1 - q_stage1[action_1[trial]]
        q_stage1[action_1[trial]] += curr_lr * rpe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Uncertainty-Driven Exploration with OCI-Dampening
This model investigates exploration strategies. It proposes that while standard agents might explore options with high uncertainty (uncertainty bonus), higher OCI scores lead to "uncertainty avoidance" or rigidity, effectively dampening this exploration bonus.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Learner with Uncertainty Bonus and OCI-Modulated Dampening.
    
    Hypothesis: Agents usually get an exploration bonus for actions not taken recently (uncertainty).
    However, high OCI is associated with rigidity and uncertainty intolerance, which might
    manifest as a reduction (dampening) of this exploration bonus, leading to more repetitive behavior.
    
    Parameters:
    lr : [0, 1] Learning rate.
    beta : [0, 10] Inverse temperature.
    unc_bonus_base : [0, 5] Base exploration bonus for uncertainty (time since last chosen).
    oci_dampening : [0, 1] Factor by which OCI reduces the uncertainty bonus.
    """
    lr, beta, unc_bonus_base, oci_dampening = model_parameters
    n_trials = len(action_1)
    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    
    current_oci = oci[0]
    
    # Calculate effective uncertainty bonus weight
    # High OCI reduces the bonus. 
    # Formula: effective_bonus = base * (1 - oci * dampening_factor)
    # If oci * dampening >= 1, bonus becomes 0 (pure exploitation/rigidity)
    dampening_mult = np.clip(1.0 - (current_oci * oci_dampening), 0.0, 1.0)
    w_unc = unc_bonus_base * dampening_mult
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    # Track counters for uncertainty (time since last selection)
    # Initialize with 0
    counts_stage1 = np.zeros(2)
    counts_stage2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        
        # --- Stage 1 Choice ---
        # Add uncertainty bonus: Q_aug = Q + w_unc * sqrt(log(trial)/count)
        # Simplified: just proportional to time since last picked (counter)
        # Using a simple counter approach: bonus scales with how long it's been neglected.
        
        # Values for decision include the bonus
        # To avoid runaway values, we can use a simpler "unchosen bonus" logic:
        # 1.0 for unchosen, 0 for chosen. Or just accumulated counts.
        # Let's use simple counters representing "trials since last chosen".
        
        q_net_1 = q_stage1 + (w_unc * counts_stage1)
        
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Update counters for stage 1
        counts_stage1 += 1 # Increment all
        counts_stage1[action_1[trial]] = 0 # Reset chosen
        
        # --- Stage 2 Choice ---
        state_idx = state[trial]
        
        q_net_2 = q_stage2[state_idx] + (w_unc * counts_stage2[state_idx])
        
        exp_q2 = np.exp(beta * q_net_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # Update counters for stage 2 (specific state)
        counts_stage2[state_idx] += 1
        counts_stage2[state_idx, action_2[trial]] = 0
        
        # --- Learning ---
        # Standard Q-learning updates (on the base Q-values, not the augmented ones)
        
        # Stage 2
        rpe_2 = reward[trial] - q_stage2[state_idx, action_2[trial]]
        q_stage2[state_idx, action_2[trial]] += lr * rpe_2
        
        # Stage 1
        target_stage1 = q_stage2[state_idx, action_2[trial]]
        rpe_1 = target_stage1 - q_stage1[action_1[trial]]
        q_stage1[action_1[trial]] += lr * rpe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```