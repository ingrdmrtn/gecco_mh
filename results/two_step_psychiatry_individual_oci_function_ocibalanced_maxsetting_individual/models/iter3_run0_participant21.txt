Here are the three proposed cognitive models.

### Model 1: Forgetting of Unchosen Options Modulated by OCI
This model hypothesizes that OCI symptoms relate to how the participant manages value representations for options they *did not* choose. High OCI might be associated with "tunnel vision" (rapid forgetting of unchosen paths) or conversely, rumination (slower forgetting). The model implements a passive decay for unchosen action values in both stages.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid model with Forgetting of Unchosen Options modulated by OCI.
    
    Hypothesis: OCI score predicts the rate at which the participant 'forgets' or devalues 
    options that were not chosen (passive decay). High OCI might lead to faster forgetting 
    (tunnel vision) or slower forgetting.
    
    Parameters:
    lr: [0, 1] Learning rate for chosen options.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Weight for Model-Based values (0=MF, 1=MB).
    pers: [-2, 2] General perseveration bonus.
    forget_base: [0, 1] Base forgetting rate for unchosen options.
    forget_oci_slope: [-1, 1] Effect of OCI on forgetting rate.
    """
    lr, beta, w, pers, forget_base, forget_oci_slope = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate forgetting rate bounded in [0, 1]
    forget_rate = forget_base + forget_oci_slope * current_oci
    forget_rate = np.clip(forget_rate, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # state x action
    
    prev_action_1 = -1

    for trial in range(n_trials):
        # Stage 1 Choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness
        if prev_action_1 != -1:
            q_net[prev_action_1] += pers
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # Stage 2 Choice
        s_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # Updates
        r = reward[trial]
        
        # Stage 1 MF Update (TD(0))
        delta_s1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_s1
        
        # Forgetting for unchosen Stage 1
        unchosen_s1 = 1 - a1
        q_stage1_mf[unchosen_s1] *= (1.0 - forget_rate)
        
        # Stage 2 MF Update
        delta_s2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_s2
        
        # Forgetting for unchosen Stage 2 (in current state)
        unchosen_s2 = 1 - a2
        q_stage2_mf[s_idx, unchosen_s2] *= (1.0 - forget_rate)
        
        prev_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Loss-Dependent Stickiness Modulated by OCI
This model tests the hypothesis that OCI is specifically linked to "compulsive" repetition in the face of failure (chasing losses). It separates perseveration into a general component and a loss-specific component, with OCI modulating the tendency to repeat a choice that resulted in zero reward.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid model with Loss-Dependent Stickiness modulated by OCI.
    
    Hypothesis: OCI scores relate to 'compulsive' repetition specifically after negative outcomes 
    (chasing losses or inability to switch after failure).
    
    Parameters:
    lr: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Weight for Model-Based values.
    pers_general: [-2, 2] General perseveration bonus (regardless of outcome).
    pers_loss_base: [-2, 2] Additional stickiness applied only if previous reward was 0.
    pers_loss_oci: [-2, 2] Modulation of loss-stickiness by OCI.
    """
    lr, beta, w, pers_general, pers_loss_base, pers_loss_oci = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate loss-specific stickiness
    pers_loss_val = pers_loss_base + pers_loss_oci * current_oci
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1
    prev_reward = 0 # Default for first trial (no effect)

    for trial in range(n_trials):
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if prev_action_1 != -1:
            bonus = pers_general
            if prev_reward == 0.0:
                bonus += pers_loss_val
            q_net[prev_action_1] += bonus
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        s_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        r = reward[trial]
        
        delta_s1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_s1
        
        delta_s2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_s2
        
        prev_action_1 = a1
        prev_reward = r

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Habit Trace Decay Modulated by OCI
This model generalizes "stickiness" to an accumulated "habit trace" that decays over time. It hypothesizes that OCI affects the *persistence* of this habit. A slower decay (associated with high OCI) would result in longer "blocks" of repeated choices, consistent with the participant's data.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid model with Habit Trace Decay modulated by OCI.
    
    Hypothesis: 'Stickiness' is not just based on the last choice, but an accumulated 
    habit trace. OCI modulates how quickly this habit trace decays. 
    Slower decay (higher OCI?) leads to longer blocks of repetition.
    
    Parameters:
    lr: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Weight for Model-Based values.
    trace_weight: [-2, 2] Weight of the habit trace in decision making.
    decay_base: [0, 1] Base decay rate of the habit trace (gamma).
    decay_oci: [-1, 1] Modulation of trace decay by OCI.
    """
    lr, beta, w, trace_weight, decay_base, decay_oci = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate trace decay (gamma)
    # gamma = 0: trace clears immediately (only last choice matters if added before decay)
    # gamma = 1: infinite accumulation
    trace_decay_gamma = decay_base + decay_oci * current_oci
    trace_decay_gamma = np.clip(trace_decay_gamma, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Choice trace for Stage 1 actions
    trace = np.zeros(2)

    for trial in range(n_trials):
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add habit trace influence
        q_net += trace_weight * trace
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # Update Trace
        # Decay existing trace
        trace *= trace_decay_gamma
        # Reinforce chosen action
        trace[a1] += 1.0
        
        s_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        r = reward[trial]
        
        delta_s1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_s1
        
        delta_s2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_s2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```