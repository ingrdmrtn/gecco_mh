To explain the decision-making behavior of the participant, I propose three cognitive models that incorporate the OCI-R score to modulate specific mechanisms of reinforcement learning.

### Model 1: OCI-Distorted Transition Belief
This model hypothesizes that OCI symptoms correlate with a distortion in the internal model of the environment. While the true transition probability is 0.7, participants with higher OCI scores may perceive this probability as higher (over-confidence/rigidity) or lower (uncertainty/worry) when calculating Model-Based values. This "distorted belief" alters how they project values from the second stage back to the first stage.

### Model 2: OCI-Modulated Choice Kernel (Long-Term Habit)
This model tests the hypothesis that OCI symptoms drive the formation of long-term habits (perseveration over many trials) rather than just immediate repetition (1-back stickiness). It uses a "Choice Kernel" that integrates choice history with a learning rate. The influence of this kernel on choice (the weight $\phi$) is modulated by the OCI score.

### Model 3: OCI-Modulated Post-Error Perseveration
This model hypothesizes that OCI symptoms specifically affect the ability to switch away from an option after a lack of reward (loss). While standard behavior ("Win-Stay, Lose-Shift") implies low stickiness after a loss, high OCI might lead to "Lose-Stay" behavior. Here, the stickiness parameter is split into `stick_win` and `stick_loss`, with the OCI score specifically modulating `stick_loss`.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Distorted Transition Belief Model.
    
    Hypothesis: OCI score distorts the participant's internal model of the transition 
    probabilities used in Model-Based planning. High OCI may lead to an over- or 
    under-estimation of the common transition probability (0.7), affecting how 
    Stage 2 values are integrated into Stage 1 choices.
    
    Parameters:
    - learning_rate: [0, 1] Value updating rate for Model-Free Q-values.
    - beta: [0, 10] Inverse temperature (exploration/exploitation).
    - w: [0, 1] Mixing weight (0=MF, 1=MB).
    - stickiness: [0, 5] Perseveration bonus for the previously chosen spaceship.
    - belief_distortion: [-0.5, 0.5] Adjustment to the perceived transition prob (0.7) scaled by OCI.
    """
    learning_rate, beta, w, stickiness, belief_distortion = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate perceived transition probability
    # Base is 0.7. Distortion is scaled by OCI. 
    # Clamped between 0 and 1 to ensure validity.
    perceived_p = 0.7 + (belief_distortion * oci_score)
    perceived_p = np.clip(perceived_p, 0.0, 1.0)
    
    # Internal model based on perceived probability
    transition_matrix = np.array([[perceived_p, 1 - perceived_p], 
                                  [1 - perceived_p, perceived_p]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)      # MF values for Stage 1 (2 spaceships)
    q_stage2_mf = np.zeros((2, 2)) # MF values for Stage 2 (2 planets x 2 aliens)
    
    prev_action_1 = -1

    for trial in range(n_trials):
        # Skip invalid trials
        if action_1[trial] == -1:
            continue

        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        # Model-Based Value Calculation using distorted transition matrix
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2

        # Integrated Net Q-value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf

        # Stickiness
        stick_bonus = np.zeros(2)
        if prev_action_1 != -1:
            stick_bonus[prev_action_1] = stickiness

        # Softmax Choice 1
        logits_1 = beta * q_net + stick_bonus
        logits_1 = logits_1 - np.max(logits_1) # stability
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        # Standard Softmax on Stage 2 MF values
        logits_2 = beta * q_stage2_mf[s_idx]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Stage 1 MF Update (SARSA-like using Stage 2 value)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

        # Stage 2 MF Update (Reward Prediction Error)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

        prev_action_1 = a1

    eps = 1e-10
    # Filter valid trials for likelihood calculation
    valid_mask = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_mask] + eps)))
    
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Choice Kernel Model.
    
    Hypothesis: OCI score modulates the reliance on a "Choice Kernel" (long-term habit)
    rather than just immediate stickiness. A choice kernel integrates past choices 
    over time. High OCI creates a stronger influence (phi) of this habit history.
    
    Parameters:
    - learning_rate: [0, 1] Value updating rate.
    - lr_kernel: [0, 1] Decay/Update rate for the choice kernel (habit memory).
    - beta: [0, 10] Inverse temperature for value.
    - w: [0, 1] Mixing weight (MB vs MF).
    - phi_base: [0, 5] Baseline weight of the choice kernel.
    - phi_oci: [-5, 5] Modulation of kernel weight by OCI score.
    """
    learning_rate, lr_kernel, beta, w, phi_base, phi_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Choice Kernel: Tracks frequency/history of selecting A vs U
    choice_kernel = np.zeros(2) 

    # Calculate OCI-modulated kernel weight
    phi = phi_base + (phi_oci * oci_score)
    # Ensure phi doesn't become negative if that implies avoidance of habit (optional, but standard interpretation is positive)
    # However, allowing negative could imply "anti-habit", so we leave it, or softplus. 
    # We'll just assume the optimizer handles bounds or interpretation.

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue

        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf

        # Policy combines Value (beta) and Habit (phi)
        logits_1 = beta * q_net + phi * choice_kernel
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        logits_2 = beta * q_stage2_mf[s_idx]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Value Updates
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

        # Choice Kernel Update
        # Increases for chosen action, decays for both (or unchosen decays).
        # Standard implementation: CK(a) = CK(a) + lr_k * (1 - CK(a)) for chosen
        #                          CK(a) = CK(a) + lr_k * (0 - CK(a)) for unchosen
        # Which simplifies to: CK_new = (1-lr_k)*CK_old + lr_k*Indicator
        mask = np.zeros(2)
        mask[a1] = 1.0
        choice_kernel = (1 - lr_kernel) * choice_kernel + lr_kernel * mask

    eps = 1e-10
    valid_mask = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_mask] + eps)))
    
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Post-Error Perseveration Model.
    
    Hypothesis: OCI symptoms are associated with an inability to switch behavior 
    after negative feedback (loss). This model splits stickiness into 'Win-Stickiness' 
    and 'Loss-Stickiness'. OCI specifically modulates the stickiness applied 
    after a 0-reward trial.
    
    Parameters:
    - learning_rate: [0, 1] Value updating rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight (MB vs MF).
    - stick_win: [0, 5] Stickiness after receiving a reward.
    - stick_loss_base: [0, 5] Baseline stickiness after no reward.
    - stick_loss_oci: [-5, 5] Modulation of post-loss stickiness by OCI.
    """
    learning_rate, beta, w, stick_win, stick_loss_base, stick_loss_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1
    prev_reward = -1

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue

        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf

        # Determine Stickiness based on previous outcome
        stick_bonus = np.zeros(2)
        if prev_action_1 != -1 and prev_reward != -1:
            if prev_reward == 1:
                # Win-Stay (Standard Stickiness)
                current_stick = stick_win
            else:
                # Lose-Stay? (Modulated by OCI)
                current_stick = stick_loss_base + (stick_loss_oci * oci_score)
            
            stick_bonus[prev_action_1] = current_stick

        logits_1 = beta * q_net + stick_bonus
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        logits_2 = beta * q_stage2_mf[s_idx]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

        prev_action_1 = a1
        prev_reward = r

    eps = 1e-10
    valid_mask = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_mask] + eps)))
    
    return log_loss
```