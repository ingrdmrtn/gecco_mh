def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free RL with OCI-modulated Inverse Temperature (Beta).
    
    This model assumes that OCI symptoms influence the 'rigidity' or 'noise' of decision-making.
    A higher OCI score modulates the inverse temperature (beta), making choices either more 
    deterministic (rigid) or more random (anxious/noisy), while keeping learning and stickiness constant.
    
    Parameters:
    lr: [0, 1] Learning rate for Q-value updates.
    beta_base: [0, 10] Baseline inverse temperature.
    beta_oci_slope: [-10, 10] Slope of OCI effect on beta.
    stickiness: [0, 5] Fixed choice persistence bonus.
    """
    lr, beta_base, beta_oci_slope, stickiness = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective beta. Ensure it stays non-negative.
    beta = beta_base + (beta_oci_slope * oci_score)
    if beta < 0: beta = 0.0
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values initialized to 0.5 (midpoint of reward 0 and 1)
    q_stage1 = np.ones(2) * 0.5
    q_stage2 = np.ones((2, 2)) * 0.5
    
    last_action_1 = -1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        if a1 == -1: # Skip missing trials
            last_action_1 = -1
            continue
            
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Stage 1 Choice
        # Add stickiness bonus to Q-value of the previously chosen action
        q_net_1 = q_stage1.copy()
        if last_action_1 != -1:
            q_net_1[last_action_1] += stickiness

        exp_q1 = np.exp(beta * q_net_1)
        if np.sum(exp_q1) == 0: exp_q1 = np.ones(2) # Prevent div by zero
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Stage 2 Choice
        exp_q2 = np.exp(beta * q_stage2[s])
        if np.sum(exp_q2) == 0: exp_q2 = np.ones(2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # Updates (TD-0 / SARSA-like for Stage 1)
        # Stage 2 update
        delta2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += lr * delta2
        
        # Stage 1 update
        # Using the value of the state reached (q_stage2[s, a2]) as the target
        delta1 = q_stage2[s, a2] - q_stage1[a1]
        q_stage1[a1] += lr * delta1
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free RL with OCI-modulated Reward Sensitivity.
    
    This model posits that OCI affects the subjective valuation of rewards relative to habits (stickiness).
    The 'Reward Sensitivity' scales the Q-values before they enter the softmax.
    If sensitivity is low (due to OCI), decisions are driven primarily by stickiness (habit).
    If sensitivity is high, decisions are driven by expected value.
    
    Parameters:
    lr: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature (scaling for both Q and stickiness).
    stickiness: [0, 5] Base stickiness bonus.
    rew_sens_base: [0, 5] Base multiplier for Q-values (subjective utility of coins).
    rew_sens_oci_slope: [-5, 5] Modulation of reward sensitivity by OCI.
    """
    lr, beta, stickiness, rew_sens_base, rew_sens_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective reward sensitivity
    rew_sensitivity = rew_sens_base + (rew_sens_oci_slope * oci_score)
    if rew_sensitivity < 0: rew_sensitivity = 0.0

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.ones(2) * 0.5
    q_stage2 = np.ones((2, 2)) * 0.5
    
    last_action_1 = -1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        if a1 == -1:
            last_action_1 = -1
            continue
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Stage 1 Policy: Softmax(beta * (Sensitivity * Q + Stickiness))
        # Note: Sensitivity scales Q relative to Stickiness.
        logits_1 = (q_stage1 * rew_sensitivity)
        if last_action_1 != -1:
            logits_1[last_action_1] += stickiness
        
        exp_q1 = np.exp(beta * logits_1)
        if np.sum(exp_q1) == 0: exp_q1 = np.ones(2)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2 Policy
        logits_2 = q_stage2[s] * rew_sensitivity
        exp_q2 = np.exp(beta * logits_2)
        if np.sum(exp_q2) == 0: exp_q2 = np.ones(2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Standard Q-learning updates
        delta2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += lr * delta2
        
        delta1 = q_stage2[s, a2] - q_stage1[a1]
        q_stage1[a1] += lr * delta1
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Based RL with OCI-modulated Transition Learning Rate.
    
    This model assumes the participant is Model-Based (planning using a transition matrix),
    but they do not assume the transition probabilities are fixed. Instead, they learn 
    the transition matrix over time. OCI modulates how quickly they update their 
    beliefs about the spaceship-planet transitions (structural flexibility).
    
    Parameters:
    lr_reward: [0, 1] Learning rate for the values of aliens (Stage 2).
    lr_trans_base: [0, 1] Base learning rate for transition probabilities.
    lr_trans_oci: [-1, 1] Modulation of transition learning rate by OCI.
    beta: [0, 10] Inverse temperature.
    stickiness: [0, 5] Choice stickiness bonus (added to MB Q-values).
    """
    lr_reward, lr_trans_base, lr_trans_oci, beta, stickiness = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Effective transition learning rate
    lr_trans = lr_trans_base + (lr_trans_oci * oci_score)
    if lr_trans < 0: lr_trans = 0.0
    if lr_trans > 1: lr_trans = 1.0
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Initialize estimated transition matrix: T[action, next_state]
    # Start with uniform uncertainty (0.5)
    trans_probs = np.ones((2, 2)) * 0.5
    
    # Stage 2 values (Alien values)
    q_stage2 = np.ones((2, 2)) * 0.5
    
    last_action_1 = -1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        if a1 == -1:
            last_action_1 = -1
            continue
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Model-Based Calculation for Stage 1
        # Q_MB(a1) = sum_s' T(s'|a1) * max_a2 Q_stage2(s', a2)
        max_q_s2 = np.max(q_stage2, axis=1) # Max value for each planet
        q_mb = np.zeros(2)
        for act in range(2):
            q_mb[act] = np.dot(trans_probs[act], max_q_s2)
            
        # Add stickiness
        if last_action_1 != -1:
            q_mb[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_mb)
        if np.sum(exp_q1) == 0: exp_q1 = np.ones(2)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2 Choice
        exp_q2 = np.exp(beta * q_stage2[s])
        if np.sum(exp_q2) == 0: exp_q2 = np.ones(2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # Update Stage 2 Values (Model-Free update for the aliens)
        delta2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += lr_reward * delta2
        
        # Update Transition Matrix (Model Learning)
        # We observed transition a1 -> s
        # Prediction error for the transition probability
        # Target is 1 for the observed state, 0 for the other
        
        # Update P(s | a1)
        t_err = 1.0 - trans_probs[a1, s]
        trans_probs[a1, s] += lr_trans * t_err
        
        # Ensure probabilities sum to 1
        trans_probs[a1, 1-s] = 1.0 - trans_probs[a1, s]
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss