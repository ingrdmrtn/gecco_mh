Here are three new cognitive models for the two-step task, incorporating the OCI score into different computational mechanisms.

### Model 1: Subjective Transition Distortion Model
This model hypothesizes that OCI symptoms relate to a distortion in how transition probabilities are perceived (e.g., intolerance of uncertainty or "magical thinking"). The model modulates the transition matrix used in the Model-Based calculation. A "distortion exponent" $\gamma$ warps the objective probabilities: $\gamma > 1$ sharpens them (perceiving the world as more deterministic), while $\gamma < 1$ flattens them (perceiving more randomness/entropy).

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Subjective Transition Distortion Model.
    
    Hypothesis: OCI modulates the subjective perception of transition probabilities 
    used in Model-Based planning. High OCI may lead to distorted risk assessment 
    (sharpening or flattening the transition matrix).
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for value updates.
    - beta: [0, 10] Inverse temperature for softmax choice.
    - w: [0, 1] Weighting between Model-Based (1) and Model-Free (0) systems.
    - gamma_base: [0, 5] Baseline distortion exponent for transition probabilities.
    - gamma_oci: [-2, 2] Slope of OCI modulation on gamma.
      gamma = gamma_base + gamma_oci * oci.
      P_subj = P_obj^gamma / sum(P_obj^gamma).
    """
    learning_rate, beta, w, gamma_base, gamma_oci = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    # Calculate subjective distortion exponent
    gamma = gamma_base + (gamma_oci * current_oci)
    gamma = np.maximum(0.01, gamma) # Ensure strictly positive

    # Distort the standard transition matrix [[0.7, 0.3], [0.3, 0.7]]
    p_common_obj = 0.7
    p_rare_obj = 0.3
    
    # Apply softmax-like distortion
    num_common = p_common_obj ** gamma
    num_rare = p_rare_obj ** gamma
    denom = num_common + num_rare
    
    p_common_subj = num_common / denom
    p_rare_subj = num_rare / denom
    
    # Subjective transition matrix used for MB planning
    transition_matrix_subj = np.array([
        [p_common_subj, p_rare_subj],
        [p_rare_subj, p_common_subj]
    ])

    # Initialize Q-values
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])

        # --- Stage 1 Choice ---
        # Model-Based Value: Uses distorted transition matrix
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix_subj @ max_q_stage2
        
        # Hybrid Value
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning Updates ---
        # Stage 2 Update (TD error)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 Update (TD(0): Update towards Stage 2 value)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Unchosen Value Decay Model
This model hypothesizes that OCI affects "cognitive flexibility" or the ability to let go of old information. It introduces a decay parameter for **unchosen** actions. High OCI might lead to lower decay (rigidity/hoarding of value estimates) or higher decay (anxiety-driven forgetting), affecting how quickly the participant switches behavior when a path is not taken.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Unchosen Value Decay Model.
    
    Hypothesis: OCI modulates the decay rate of Q-values for unchosen actions.
    This reflects the persistence of memory or the inability to 'let go' of 
    options not currently pursued (compulsive rigidity).
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for chosen actions.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] MB/MF weight.
    - decay_base: [0, 1] Baseline decay rate for unchosen actions.
    - decay_oci: [-1, 1] Slope of OCI modulation on decay.
      Effective decay = clip(decay_base + decay_oci * oci, 0, 1).
      Q(unchosen) = Q(unchosen) * (1 - decay).
    """
    learning_rate, beta, w, decay_base, decay_oci = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    # Calculate decay rate
    decay = decay_base + (decay_oci * current_oci)
    decay = np.clip(decay, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])

        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Update chosen Stage 2
        q_stage2_mf[s_idx, a2] += learning_rate * (r - q_stage2_mf[s_idx, a2])
        # Decay unchosen Stage 2
        unchosen_a2 = 1 - a2
        q_stage2_mf[s_idx, unchosen_a2] *= (1.0 - decay)

        # Update chosen Stage 1
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        # Decay unchosen Stage 1
        unchosen_a1 = 1 - a1
        q_stage1_mf[unchosen_a1] *= (1.0 - decay)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Stage-Specific Beta Model
This model hypothesizes that OCI affects the exploration/exploitation balance differently for high-level planning (Stage 1) versus immediate response (Stage 2). OCI participants might be decisive in simple contexts (Stage 2) but exhibit high uncertainty or noise (low beta) or extreme rigidity (high beta) in the complex, ambiguous first stage.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Stage-Specific Beta Model.
    
    Hypothesis: OCI differentially affects decision noise (inverse temperature) 
    at Stage 1 (planning/spaceship choice) vs Stage 2 (harvesting/alien choice).
    OCI modulates Beta 1, while Beta 2 is fixed/independent.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - w: [0, 1] MB/MF weight.
    - beta2: [0, 10] Inverse temperature for Stage 2.
    - beta1_base: [0, 10] Baseline inverse temperature for Stage 1.
    - beta1_oci: [-5, 5] Slope of OCI modulation on Beta 1.
      beta1 = clip(beta1_base + beta1_oci * oci, 0, 10).
    """
    learning_rate, w, beta2, beta1_base, beta1_oci = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    # Calculate Stage 1 Beta
    beta1 = beta1_base + (beta1_oci * current_oci)
    beta1 = np.clip(beta1, 0.0, 10.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])

        # --- Stage 1 Choice (Uses Beta 1) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta1 * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Choice (Uses Beta 2) ---
        exp_q2 = np.exp(beta2 * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Stage 2
        q_stage2_mf[s_idx, a2] += learning_rate * (r - q_stage2_mf[s_idx, a2])
        
        # Stage 1
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```