Here are the three cognitive models based on the participant's data and OCI score.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Reward-Dependent Stickiness (Win-Stay).
    
    Hypothesis:
    OCI scores correlate with compulsivity and a tendency to repeat behaviors 
    that have been recently reinforced. This model implements a 'Win-Stay' bias 
    where the stickiness (perseverance) to the previous choice is added only 
    if that choice resulted in a reward. The magnitude of this bias is scaled 
    by the OCI score.
    
    Parameters:
    - lr: [0, 1] Learning rate for reward value updates.
    - beta: [0, 10] Inverse temperature (softness of softmax).
    - w: [0, 1] Weighting parameter (0 = pure MF, 1 = pure MB).
    - stick_param: [0, 5] Scaling factor for OCI-driven win-stay stickiness.
      Effective stickiness = stick_param * oci.
    """
    lr, beta, w, stick_param = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Fixed transition matrix for MB calculation (Common: 0.7, Rare: 0.3)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Initialize Q-values
    q_stage1_mf = np.zeros(2) 
    q_stage2_mf = np.zeros((2, 2)) 
    
    log_likelihood = 0.0
    eps = 1e-10
    
    # Track previous action and reward for stickiness
    last_a1 = -1
    last_r = 0
    
    for trial in range(n_trials):
        # Handle missing data
        if action_1[trial] == -1:
            last_a1 = -1
            continue
            
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Q-value mixing MB and MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Calculate base logits
        logits_stage1 = beta * q_net
        
        # Add Stickiness Bonus: Only if previous trial existed and resulted in reward (Win-Stay)
        if last_a1 != -1 and last_r == 1:
            logits_stage1[last_a1] += stick_param * oci_score
            
        # Softmax for Stage 1
        max_logit = np.max(logits_stage1)
        exp_q1 = np.exp(logits_stage1 - max_logit)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        log_likelihood += np.log(probs_1[a1] + eps)
        
        # --- Stage 2 Policy ---
        logits_stage2 = beta * q_stage2_mf[s2, :]
        max_logit2 = np.max(logits_stage2)
        exp_q2 = np.exp(logits_stage2 - max_logit2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        log_likelihood += np.log(probs_2[a2] + eps)
        
        # --- Value Updates ---
        # Stage 1 MF update (TD)
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        # Stage 2 MF update
        delta_stage2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += lr * delta_stage2
        
        # Update history
        last_a1 = a1
        last_r = r

    return -log_likelihood


def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Modulated Memory Decay.
    
    Hypothesis:
    High OCI scores may relate to altered retention of value associations, 
    potentially due to obsessive focus preventing the refreshing of alternative 
    options, or anxiety leading to faster devaluation of unvisited states.
    This model introduces a passive decay for unchosen actions, where the 
    decay rate is determined by the OCI score.
    
    Parameters:
    - lr: [0, 1] Learning rate for chosen actions.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] MB/MF weight.
    - decay_param: [0, 1] Base factor for memory decay.
      Effective decay_rate = decay_param * oci.
    """
    lr, beta, w, decay_param = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective decay rate based on OCI
    decay_rate = decay_param * oci_score
    decay_rate = np.clip(decay_rate, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    log_likelihood = 0.0
    eps = 1e-10
    
    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_likelihood += np.log(probs_1[a1] + eps)
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[s2, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_likelihood += np.log(probs_2[a2] + eps)
        
        # --- Updates ---
        # Update Chosen Stage 1
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        # Decay Unchosen Stage 1
        unchosen_a1 = 1 - a1
        q_stage1_mf[unchosen_a1] *= (1.0 - decay_rate)
        
        # Update Chosen Stage 2
        delta_stage2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += lr * delta_stage2
        
        # Decay Unchosen Stage 2 (in the visited state)
        unchosen_a2 = 1 - a2
        q_stage2_mf[s2, unchosen_a2] *= (1.0 - decay_rate)

    return -log_likelihood


def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: Dynamic Transition Learning with OCI-Modulated Update Rate.
    
    Hypothesis:
    Participants with higher OCI scores may be less confident in the static 
    structure of the task or more sensitive to transition surprises (uncertainty 
    intolerance). This model allows the Model-Based system to update its 
    internal transition matrix based on experienced transitions, with the 
    learning rate for this structure learning scaled by OCI.
    
    Parameters:
    - lr_reward: [0, 1] Learning rate for value updates (MF).
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] MB/MF weight.
    - lr_trans_param: [0, 1] Scaling factor for transition learning rate.
      Effective lr_trans = lr_trans_param * oci.
    """
    lr_reward, beta, w, lr_trans_param = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Initialize transition matrix (belief) with standard instructions
    trans_probs = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Effective transition learning rate
    lr_trans = lr_trans_param * oci_score
    lr_trans = np.clip(lr_trans, 0.0, 1.0)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    log_likelihood = 0.0
    eps = 1e-10
    
    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Stage 1 Policy (MB + MF) ---
        # Use current dynamic transition matrix for MB calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = trans_probs @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_likelihood += np.log(probs_1[a1] + eps)
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s2, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_likelihood += np.log(probs_2[a2] + eps)
        
        # --- Value Updates ---
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr_reward * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += lr_reward * delta_stage2
        
        # --- Transition Matrix Update ---
        # Update belief about the transition from a1 to s2
        outcome_vec = np.zeros(2)
        outcome_vec[s2] = 1.0
        
        # Update row a1: P_new = P_old + lr_trans * (Outcome - P_old)
        trans_probs[a1, :] += lr_trans * (outcome_vec - trans_probs[a1, :])
        
        # Ensure normalization
        trans_probs[a1, :] /= np.sum(trans_probs[a1, :])

    return -log_likelihood
```