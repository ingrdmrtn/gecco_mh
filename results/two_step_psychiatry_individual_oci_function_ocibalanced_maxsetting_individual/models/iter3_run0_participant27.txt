Here are 3 new cognitive models exploring different mechanisms for how OCI might influence learning and decision-making in the two-step task.

### Model 1: Hybrid Learner with OCI-Driven Model-Based Weighting
This model hypothesizes that OCI levels specifically modulate the balance between Model-Based (MB) and Model-Free (MF) control. High OCI is often associated with rigid habits (MF), but sometimes also with over-thinking or compensatory control (MB). Given the participant's "medium" score (0.35) and highly repetitive behavior (choosing 0.0 almost exclusively), this model tests if OCI shifts the mixing weight `w` towards habit (MF) or planning (MB).

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free learner where the mixing weight 'w' is 
    a logistic function of the OCI score.
    
    Hypothesis: OCI score determines the reliance on Model-Based vs Model-Free strategies.
    
    Parameters:
    - learning_rate: Learning rate for value updates [0, 1]
    - beta: Inverse temperature for softmax choice [0, 10]
    - w_base: Baseline mixing weight (pre-sigmoid) [-5, 5]
    - w_oci_mod: Sensitivity of mixing weight to OCI [-5, 5]
    """
    learning_rate, beta, w_base, w_oci_mod = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate mixing weight w based on OCI
    # Sigmoid transform to keep w in [0, 1]
    # w = 1 -> Pure Model-Based, w = 0 -> Pure Model-Free
    w_logits = w_base + (w_oci_mod * current_oci)
    w = 1 / (1 + np.exp(-w_logits))
    
    # Fixed transition matrix for MB (Task structure)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_mf_stage1 = np.zeros(2)
    q_mf_stage2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Missing data handling
        if a1 == -1 or s_idx == -1 or a2 == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue
            
        # --- Stage 1 Choice ---
        # Model-Based Value: V_MB(s1) = T * max(Q_MF_s2)
        max_q_stage2 = np.max(q_mf_stage2, axis=1)
        q_mb_stage1 = transition_matrix @ max_q_stage2
        
        # Hybrid Value: Q_net = w * Q_MB + (1-w) * Q_MF
        q_net_stage1 = w * q_mb_stage1 + (1 - w) * q_mf_stage1
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Choice ---
        # Pure Model-Free at stage 2
        exp_q2 = np.exp(beta * q_mf_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        # Stage 2 update (TD)
        pe2 = r - q_mf_stage2[s_idx, a2]
        q_mf_stage2[s_idx, a2] += learning_rate * pe2
        
        # Stage 1 update (TD)
        # Note: In standard hybrid models, MF stage 1 is updated by the stage 2 value
        pe1 = q_mf_stage2[s_idx, a2] - q_mf_stage1[a1]
        q_mf_stage1[a1] += learning_rate * pe1
        
        # Eligibility trace (lambda=1 assumption for simple hybrid)
        # The reward prediction error also updates stage 1
        q_mf_stage1[a1] += learning_rate * pe2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Choice Stickiness (Perseveration)
This model focuses on the extreme repetitiveness seen in the data (choosing spaceship 0.0 for over 100 trials). It hypothesizes that OCI relates to "stickiness" or perseverationâ€”the tendency to repeat the previous choice regardless of reward.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner with choice stickiness modulated by OCI.
    
    Hypothesis: OCI increases the 'stickiness' parameter, making the agent 
    more likely to repeat the previous Stage 1 action regardless of value.
    
    Parameters:
    - learning_rate: Learning rate [0, 1]
    - beta: Inverse temperature [0, 10]
    - stick_base: Base level of choice perseveration [-2, 2]
    - stick_oci_slope: How strongly OCI increases stickiness [-5, 5]
    """
    learning_rate, beta, stick_base, stick_oci_slope = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate effective stickiness
    stickiness = stick_base + (stick_oci_slope * current_oci)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Track previous choice for stickiness (initialize with -1)
    last_a1 = -1
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        if a1 == -1 or s_idx == -1 or a2 == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue
            
        # --- Stage 1 Choice ---
        # Add stickiness bonus to the Q-value of the previously chosen action
        q_stage1_effective = q_stage1.copy()
        if last_a1 != -1:
            q_stage1_effective[last_a1] += stickiness
            
        exp_q1 = np.exp(beta * q_stage1_effective)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Update last choice
        last_a1 = a1
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning (SARSA / TD) ---
        # Stage 2 update
        pe2 = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += learning_rate * pe2
        
        # Stage 1 update
        # Using the value of the state reached (V(s') approx max Q(s', a'))
        v_next = np.max(q_stage2[s_idx])
        pe1 = v_next - q_stage1[a1]
        q_stage1[a1] += learning_rate * pe1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Dependent Reward Sensitivity (Inverse Temperature Modulation)
This model posits that OCI affects the exploration-exploitation balance directly. Instead of changing how values are learned, OCI changes how deterministically the agent acts on those values. A high OCI might lead to more rigid exploitation (higher beta), or high anxiety might lead to more noise (lower beta).

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner where the inverse temperature (beta) is a function of OCI.
    
    Hypothesis: OCI affects decision noise. High OCI might lead to rigid 
    exploitation (high beta) or erratic behavior (low beta).
    
    Parameters:
    - learning_rate: Learning rate [0, 1]
    - beta_base: Baseline inverse temperature [0, 10]
    - beta_oci_factor: Multiplicative factor for OCI's effect on beta [-5, 5]
    - decay: Forgetting rate for unchosen options [0, 1]
    """
    learning_rate, beta_base, beta_oci_factor, decay = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate effective beta
    # Ensure beta stays positive. We use an exponential transform to modulate it.
    # If beta_oci_factor is positive, OCI increases precision (less noise).
    effective_beta = beta_base * np.exp(beta_oci_factor * current_oci)
    # Cap beta to prevent overflow
    effective_beta = np.clip(effective_beta, 0.0, 20.0)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        if a1 == -1 or s_idx == -1 or a2 == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue
            
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(effective_beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(effective_beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        # Stage 2 update
        pe2 = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += learning_rate * pe2
        
        # Decay unchosen stage 2
        unchosen_a2 = 1 - a2
        q_stage2[s_idx, unchosen_a2] *= (1 - decay)
        
        # Stage 1 update
        # Using max Q of next stage as target
        target = np.max(q_stage2[s_idx])
        pe1 = target - q_stage1[a1]
        q_stage1[a1] += learning_rate * pe1
        
        # Decay unchosen stage 1
        unchosen_a1 = 1 - a1
        q_stage1[unchosen_a1] *= (1 - decay)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```