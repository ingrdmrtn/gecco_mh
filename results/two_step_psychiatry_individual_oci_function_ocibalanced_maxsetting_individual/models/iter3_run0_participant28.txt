Here are the 3 cognitive models based on the provided participant data and OCI scores.

### Model 1: Asymmetric Learning Rate Modulation
This model hypothesizes that OCI symptoms affect how strongly participants learn from positive outcomes (Reward Prediction Errors) versus negative ones. Specifically, the learning rate for positive prediction errors (`lr_pos`) is modulated by the OCI score, while the learning rate for negative prediction errors (`lr_neg`) remains fixed. This captures potential hypersensitivity to rewards (or relief) in maintaining compulsive behaviors.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Two-step model with Asymmetric Learning Rates modulated by OCI.
    
    The learning rate for positive prediction errors (lr_pos) is modulated by OCI,
    while the learning rate for negative prediction errors (lr_neg) is fixed.
    
    Bounds:
    lr_pos_base: [0,1]
    lr_pos_oci_mod: [0,1]
    lr_neg: [0,1]
    beta: [0,10]
    w: [0,1]
    p: [0,1] (Scaled by 5 internally)
    lambda_val: [0,1]
    """
    lr_pos_base, lr_pos_oci_mod, lr_neg, beta, w, p, lambda_val = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Modulate positive learning rate based on OCI
    # Map modifier from [0,1] to [-0.5, 0.5] to allow increase or decrease
    lr_pos = lr_pos_base + (lr_pos_oci_mod - 0.5) * oci_score
    lr_pos = np.clip(lr_pos, 0.0, 1.0)
    
    # Fixed transition matrix (Task structure)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    
    last_action_1 = -1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        # Model-Based Value
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Calculate logits with stickiness
        logits_1 = beta * q_net
        if last_action_1 != -1:
            logits_1[last_action_1] += p * 5.0
            
        # Softmax Stage 1
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        # Softmax Stage 2
        logits_2 = beta * q_stage2_mf[s_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Prediction Errors
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        # Determine learning rates based on sign of PE
        alpha_1 = lr_pos if delta_stage1 > 0 else lr_neg
        alpha_2 = lr_pos if delta_stage2 > 0 else lr_neg
        
        # Update Stage 1 (TD(lambda))
        # Note: alpha_2 is applied to the eligibility trace part
        q_stage1_mf[a1] += alpha_1 * delta_stage1 + alpha_2 * lambda_val * delta_stage2
        
        # Update Stage 2
        q_stage2_mf[s_idx, a2] += alpha_2 * delta_stage2
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Stage-Specific Inverse Temperature Modulation
This model proposes that OCI symptoms differentially affect the precision (inverse temperature $\beta$) of choices at the second stage (concrete goal selection) versus the first stage. A separate $\beta$ is used for Stage 2, and this parameter is modulated by OCI. This captures the possibility that individuals with higher OCI scores might be more deterministic (or erratic) in immediate, concrete choices compared to abstract planning.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Two-step model with separate Stage 2 Beta modulated by OCI.
    
    Allows for different exploration/exploitation trade-offs at the two stages,
    with the second stage precision modulated by OCI.
    
    Bounds:
    learning_rate: [0,1]
    beta_1: [0,10]
    beta_2_base: [0,10]
    beta_2_oci_mod: [0,1]
    w: [0,1]
    p: [0,1] (Scaled by 5 internally)
    lambda_val: [0,1]
    """
    learning_rate, beta_1, beta_2_base, beta_2_oci_mod, w, p, lambda_val = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Modulate Stage 2 Beta
    # Map modifier to a scaling factor or additive shift. 
    # Here we allow shifting beta_2_base by +/- 5.0 based on OCI.
    beta_2 = beta_2_base + (beta_2_oci_mod - 0.5) * 10.0 * oci_score
    beta_2 = np.clip(beta_2, 0.0, 20.0) # Allow slightly wider range if driven by OCI
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy (uses beta_1) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta_1 * q_net
        if last_action_1 != -1:
            logits_1[last_action_1] += p * 5.0
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy (uses beta_2) ---
        logits_2 = beta_2 * q_stage2_mf[s_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        q_stage1_mf[a1] += learning_rate * delta_stage1 + learning_rate * lambda_val * delta_stage2
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Reinforced Stickiness Modulation
This model distinguishes between general perseverance (repeating a choice regardless of outcome) and "Reinforced Stickiness" (Win-Stay). The parameter `p_rew` adds a bonus to the previous choice *only if* it was rewarded. This specific "compulsive" adherence to rewarded actions is modulated by the OCI score, separating it from general habit or motor perseveration.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Two-step model with Reinforced Stickiness modulated by OCI.
    
    Separates stickiness into general perseverance (p_gen) and 
    reward-dependent perseverance (p_rew). OCI modulates the reward-dependent component.
    
    Bounds:
    learning_rate: [0,1]
    beta: [0,10]
    w: [0,1]
    p_gen: [0,1] (General stickiness, scaled by 5)
    p_rew_base: [0,1] (Base reinforced stickiness, scaled by 5)
    p_rew_oci_mod: [0,1]
    lambda_val: [0,1]
    """
    learning_rate, beta, w, p_gen, p_rew_base, p_rew_oci_mod, lambda_val = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Modulate Reinforced Stickiness
    # p_rew represents the tendency to repeat a choice specifically after a reward
    p_rew = p_rew_base + (p_rew_oci_mod - 0.5) * oci_score
    p_rew = np.clip(p_rew, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    last_reward = 0

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net
        
        # Apply Stickiness
        if last_action_1 != -1:
            # General Stickiness
            stickiness_bonus = p_gen * 5.0
            
            # Reinforced Stickiness (Win-Stay)
            if last_reward == 1.0:
                stickiness_bonus += p_rew * 5.0
            
            logits_1[last_action_1] += stickiness_bonus
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        logits_2 = beta * q_stage2_mf[s_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        q_stage1_mf[a1] += learning_rate * delta_stage1 + learning_rate * lambda_val * delta_stage2
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        last_action_1 = a1
        last_reward = r

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```