Here are three new cognitive models exploring different mechanisms for how OCI symptoms might influence reinforcement learning in this two-step task.

### Model 1: OCI-Modulated Eligibility Trace (Lambda)
This model hypothesizes that OCI symptoms affect the decay of eligibility traces ($\lambda$). A higher eligibility trace implies that the second-stage outcome has a stronger, more direct impact on the first-stage values (similar to Model-Free learning but with a longer memory window). This tests if individuals with higher OCI scores bind the two stages more tightly (or loosely) in their credit assignment.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner with Eligibility Traces (TD(lambda)), where lambda is modulated by OCI.
    
    This tests if OCI symptoms affect how credit is assigned from the second stage outcome 
    back to the first stage choice.
    
    Parameters:
    learning_rate: [0, 1] Learning rate for value updates.
    beta: [0, 10] Inverse temperature for softmax choice.
    lambda_base: [0, 1] Baseline eligibility trace decay parameter.
    oci_lambda_mod: [0, 1] Modulation of lambda by OCI (centered and scaled).
    """
    learning_rate, beta, lambda_base, oci_lambda_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Map oci_lambda_mod to range [-1, 1] to allow increase or decrease
    mod_val = (oci_lambda_mod - 0.5) * 2.0 
    
    # Calculate effective lambda, clamped between 0 and 1
    lambda_eff = lambda_base + (mod_val * oci_score)
    lambda_eff = np.clip(lambda_eff, 0.0, 1.0)

    # Initialize Q-values
    # Stage 1: 2 actions
    q_stage1 = np.zeros(2)
    # Stage 2: 2 states * 2 actions
    q_stage2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        # Prediction error at stage 1 (transition to stage 2)
        # Note: In standard TD(0), V(s') is used. Here we use Q(s', a') (SARSA-like) 
        # or max Q(s', a') (Q-learning). Given the task structure, we often treat 
        # the transition to stage 2 state as the immediate reward for stage 1 is 0.
        # But standard analysis often updates Q1 based on Q2.
        
        # TD error 1: Difference between value of state 2 and value of action 1
        delta1 = q_stage2[s_idx, a2] - q_stage1[a1]
        
        # TD error 2: Difference between reward and value of action 2
        delta2 = r - q_stage2[s_idx, a2]
        
        # Update Stage 2 value
        q_stage2[s_idx, a2] += learning_rate * delta2
        
        # Update Stage 1 value
        # Direct update from transition
        q_stage1[a1] += learning_rate * delta1
        # Eligibility trace update: Stage 1 also learns from Stage 2's RPE, scaled by lambda
        q_stage1[a1] += learning_rate * lambda_eff * delta2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Choice Stickiness (Perseveration)
This model posits that OCI symptoms relate to "stickiness" or perseverationâ€”the tendency to repeat the previous choice regardless of reward. While similar concepts exist, this model separates the modulation specifically onto a perseveration parameter $P$ added to the Q-values, rather than modulating learning rates or mixing weights.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid learner with OCI-modulated Choice Stickiness (Perseveration).
    
    OCI modulates how much the participant tends to repeat the last Stage 1 action,
    independent of the reward history.
    
    Parameters:
    learning_rate: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Weight for Model-Based vs Model-Free.
    persev_base: [0, 5] Baseline perseveration bonus.
    oci_persev_mod: [0, 1] Modulation of perseveration by OCI.
    """
    learning_rate, beta, w, persev_base, oci_persev_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Map modulation to a scaling factor centered at 0
    # If oci_persev_mod > 0.5, higher OCI increases stickiness.
    # If < 0.5, higher OCI decreases stickiness.
    mod_factor = (oci_persev_mod - 0.5) * 4.0 
    
    # Effective perseveration parameter
    persev_eff = persev_base + (mod_factor * oci_score)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track the previous stage 1 action
    last_action_1 = -1 
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add perseveration bonus to the previously chosen action
        q_net_persev = q_net.copy()
        if last_action_1 != -1:
            q_net_persev[last_action_1] += persev_eff
            
        exp_q1 = np.exp(beta * q_net_persev)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        delta2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta2
        
        delta1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta1
        q_stage1_mf[a1] += learning_rate * delta2 # TD(1) assumption common in this task
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Mixing Weight (w) via Logistic Function
This model assumes that the balance between Model-Based (goal-directed) and Model-Free (habitual) control is determined by OCI, but uses a logistic transformation to ensure the weight $w$ stays strictly within bounds $[0, 1]$ while allowing the OCI score to push the system towards one extreme.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid learner where mixing weight w is a logistic function of OCI.
    
    This tests if OCI determines the probability of using a Model-Based strategy 
    vs a Model-Free strategy.
    
    Parameters:
    learning_rate: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    w_intercept: [0, 1] Intercept for the logit of w (mapped to -5 to 5).
    w_slope: [0, 1] Slope for the effect of OCI on logit of w (mapped to -10 to 10).
    """
    learning_rate, beta, w_intercept_param, w_slope_param = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Map parameters to logit space
    # intercept: centered at 0 (w=0.5), range approx -5 to 5
    b0 = (w_intercept_param - 0.5) * 10.0
    # slope: range -10 to 10
    b1 = (w_slope_param - 0.5) * 20.0
    
    # Calculate w using logistic function: 1 / (1 + exp(-(b0 + b1*OCI)))
    logit = b0 + b1 * oci_score
    w = 1.0 / (1.0 + np.exp(-logit))
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        delta2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta2
        
        delta1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta1
        q_stage1_mf[a1] += learning_rate * delta2 

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```