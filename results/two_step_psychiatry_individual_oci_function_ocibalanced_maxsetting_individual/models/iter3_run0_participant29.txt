Here are three cognitive models that incorporate the participant's OCI score to explain their decision-making behavior in the two-step task.

### Model 1: OCI-Modulated Eligibility Trace (Cognitive Model 1)
This model hypothesizes that obsessive-compulsive traits affect "credit assignment"â€”specifically, how much the outcome of the second stage is attributed to the choice made at the first stage. A higher OCI score increases the eligibility trace parameter ($\lambda$), implying that individuals with higher symptoms form stronger causal links between the final reward and the initial spaceship choice, potentially reinforcing habits (Model-Free values) more strongly.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-modulated Eligibility Trace Model.
    Hypothesis: OCI scores correlate with the strength of the eligibility trace (lambda).
    High OCI participants may link outcomes to initial choices more strongly (magical thinking/credit assignment),
    updating the Model-Free value of the first stage more aggressively based on Stage 2 outcomes.
    
    Parameters:
    learning_rate: [0, 1] Standard learning rate for value updates.
    beta: [0, 10] Inverse temperature for softmax choice.
    w: [0, 1] Mixing weight between Model-Based and Model-Free values (0=MF, 1=MB).
    stickiness: [0, 5] Choice perseveration bonus for repeating the last action.
    lambda_scale: [0, 1] Scaling factor for OCI to determine lambda. Lambda = lambda_scale * OCI.
    """
    learning_rate, beta, w, stickiness, lambda_scale = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate lambda based on OCI
    lambda_val = np.clip(lambda_scale * oci_score, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_a1 = -1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        state_idx = int(state[trial])
        a2 = int(action_2[trial])

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Apply stickiness to the previously chosen action
        if prev_a1 != -1:
            q_net[prev_a1] += stickiness
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Stage 1 TD error
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Stage 2 TD error
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Eligibility trace update: Stage 2 error propagates to Stage 1 value scaled by lambda
        q_stage1_mf[a1] += learning_rate * lambda_val * delta_stage2
        
        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Reward Sensitivity (Cognitive Model 2)
This model posits that higher OCI scores are associated with reduced sensitivity to rewards (anhedonia or dampening). By scaling down the effective reward magnitude based on the OCI score, the relative influence of the "stickiness" parameter increases. This explains why a participant might rigidly persist in a choice (compulsivity) even when rewards are low or devalued.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-modulated Reward Sensitivity Model.
    Hypothesis: OCI scores relate to reduced reward sensitivity (anhedonia or dampening).
    A higher OCI reduces the effective reward experienced, making the fixed 'stickiness' parameter 
    relatively more dominant, leading to rigid behavior despite outcomes.
    
    Parameters:
    learning_rate: [0, 1] Learning rate for value updates.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Mixing weight.
    stickiness: [0, 5] Choice perseveration.
    r_damp: [0, 1] Factor by which OCI dampens reward perception. Eff_Reward = Reward * (1 - r_damp * OCI).
    """
    learning_rate, beta, w, stickiness, r_damp = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Effective reward is dampened by OCI
    r_scaling = 1.0 - (r_damp * oci_score)
    # Ensure scaling doesn't go negative
    if r_scaling < 0: r_scaling = 0

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_a1 = -1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        state_idx = int(state[trial])
        a2 = int(action_2[trial])

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if prev_a1 != -1:
            q_net[prev_a1] += stickiness
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Calculate effective reward
        r_eff = reward[trial] * r_scaling

        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r_eff - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Stickiness Decay (Cognitive Model 3)
Instead of a simple "repeat last choice" stickiness, this model implements a "choice trace" that accumulates and decays over time. The decay rate is modulated by the OCI score. Higher OCI leads to a slower decay of the choice trace, meaning past choices influence current decisions for longer. This models the "difficulty switching" and behavioral rigidity characteristic of OCD.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-modulated Stickiness Decay Model.
    Hypothesis: High OCI participants have "stickier" choice traces that decay slower,
    making it harder to switch away from previous choices (rigidity/persistence).
    Standard stickiness is 1-back; this is an exponential trace.
    
    Parameters:
    learning_rate: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Mixing weight.
    stick_weight: [0, 5] Weight of the accumulated choice trace in the decision.
    decay_scale: [0, 1] Determines decay rate based on OCI. Decay = decay_scale * OCI.
    """
    learning_rate, beta, w, stick_weight, decay_scale = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Decay rate depends on OCI. If OCI is high, decay is high (trace persists).
    # If OCI is 0, decay is 0 (trace resets immediately -> standard 1-back stickiness).
    decay = np.clip(decay_scale * oci_score, 0.0, 0.99)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Choice trace for Stage 1 actions (A or U)
    choice_trace = np.zeros(2)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        state_idx = int(state[trial])
        a2 = int(action_2[trial])

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combine MB, MF, and the accumulated Choice Trace
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf + stick_weight * choice_trace
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Update Trace: Unchosen options decay; Chosen option resets to 1 (or accumulates)
        # Here we use a "set to 1, decay others" logic which generalizes 1-back stickiness
        choice_trace[a1] = 1.0
        choice_trace[1 - a1] *= decay

        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```