Here are 3 new cognitive models that introduce specific mechanisms relating OCI scores to decision-making parameters, distinct from the previous attempts.

### Cognitive Model 1: OCI-Modulated Transition Learning Rate
This model hypothesizes that high OCI individuals might be more rigid in their beliefs about the world structure (the transition matrix) or, conversely, hyper-sensitive to changes in it. Instead of assuming a fixed transition matrix (0.7/0.3), this model learns the transition matrix trial-by-trial, and the *learning rate* for this structure learning is modulated by OCI.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Structure Learning.
    
    Hypothesis: While the standard model assumes a fixed transition matrix, 
    participants may actually learn it. High OCI might correlate with 
    hyper-updating of structural beliefs (interpreting rare transitions as 
    structural changes) or rigidity. This model allows the transition 
    matrix to be updated, with the rate modulated by OCI.
    
    Bounds:
    lr_value: [0, 1] - Learning rate for reward values (MF)
    lr_struct_base: [0, 1] - Base learning rate for transition matrix
    lr_struct_oci: [-1, 1] - Modulation of structure learning by OCI
    beta: [0, 10] - Inverse temperature
    w: [0, 1] - Mixing weight
    """
    lr_value, lr_struct_base, lr_struct_oci, beta, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate structure learning rate
    lr_struct = lr_struct_base + (lr_struct_oci * oci_score)
    lr_struct = np.clip(lr_struct, 0.0, 1.0)

    # Initialize transition matrix (belief state)
    # Rows: Action 1 (0 or 1), Cols: State (0 or 1)
    # Start with flat priors or the common instruction
    transition_counts = np.array([[0.7, 0.3], [0.3, 0.7]]) 
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # 1. Calculate Model-Based Values using current transition belief
        # Normalize counts to get probabilities
        row_sums = transition_counts.sum(axis=1, keepdims=True)
        tm_probs = transition_counts / row_sums
        
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = tm_probs @ max_q_stage2
        
        # 2. Net Value for Stage 1
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # 3. Stage 2 Choice
        exp_q2 = np.exp(beta * q_stage2_mf[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        if r != -1: # Valid trial
            # Update Value Estimates (MF)
            delta_stage2 = r - q_stage2_mf[s, a2]
            q_stage2_mf[s, a2] += lr_value * delta_stage2

            delta_stage1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
            q_stage1_mf[a1] += lr_value * delta_stage1
            
            # Update Structure Beliefs (Transition Matrix)
            # We treat the transition observation as a "reward" of 1 for the observed state
            # and 0 for the unobserved state, updating the row corresponding to a1.
            # This is a delta-rule update on probability estimates.
            
            # Observed transition vector (one-hot)
            obs_vec = np.zeros(2)
            obs_vec[s] = 1.0
            
            # Current belief for this action
            current_belief = tm_probs[a1]
            
            # Update counts effectively by moving probability mass
            # We implement this as a direct update to the probability matrix for simplicity in this constrained format
            # Re-using the transition_counts variable to store probabilities directly to fit the loop structure
            # (Note: In a pure Bayesian model we'd update counts, here we use Rescorla-Wagner on the probs)
            
            tm_probs[a1] += lr_struct * (obs_vec - current_belief)
            
            # Ensure valid probabilities
            tm_probs[a1] = np.clip(tm_probs[a1], 0.0, 1.0)
            tm_probs[a1] /= np.sum(tm_probs[a1])
            
            # Store back into the variable used for calculation
            transition_counts = tm_probs 

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2: OCI-Dependent Choice Perseveration (Stickiness)
This model posits that OCI affects the tendency to repeat the previous choice regardless of reward (perseveration or "stickiness"). High OCI is often associated with repetitive behaviors. This model separates the learning process from a raw repetition bias modulated by the OCI score.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Dependent Choice Perseveration.
    
    Hypothesis: High OCI scores correlate with a higher tendency to repeat 
    previous actions (perseveration), independent of the reward history. 
    This is modeled as a "stickiness" bonus added to the Q-value of the 
    previously chosen action.
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    w: [0, 1]
    stick_base: [-5, 5] - Base tendency to repeat (or switch if negative)
    stick_oci_slope: [-5, 5] - How OCI changes the stickiness magnitude
    """
    learning_rate, beta, w, stick_base, stick_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate stickiness parameter
    stickiness = stick_base + (stick_oci_slope * oci_score)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_a1 = -1 # Initialize previous action

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # 1. Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness bonus to the previously chosen action
        q_net_sticky = q_net.copy()
        if last_a1 != -1:
            q_net_sticky[last_a1] += stickiness
        
        exp_q1 = np.exp(beta * q_net_sticky)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # 2. Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        if r != -1:
            # Update Values
            delta_stage2 = r - q_stage2_mf[s, a2]
            q_stage2_mf[s, a2] += learning_rate * delta_stage2

            delta_stage1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
            q_stage1_mf[a1] += learning_rate * delta_stage1
            
            last_a1 = a1 # Update history

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3: OCI-Modulated Second-Stage Learning Rate
This model tests if OCI specifically impacts how feedback is processed at the second stage (the direct reward receipt). It hypothesizes that high OCI individuals might over-weight or under-weight the immediate deterministic feedback from the aliens compared to the probabilistic transitions. This separates the learning rate into two: one for the first stage (transition prediction error) and one for the second stage (reward prediction error), with the latter modulated by OCI.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Second-Stage Learning Rate.
    
    Hypothesis: The core deficit or variance lies in how reward feedback 
    at the second stage is integrated. High OCI might lead to 'over-learning' 
    from specific outcomes (high lr_2) or insensitivity (low lr_2). 
    Stage 1 learning (transition structure) is kept separate.
    
    Bounds:
    lr_1: [0, 1] - Learning rate for Stage 1 (TD(1))
    lr_2_base: [0, 1] - Base learning rate for Stage 2 (Reward)
    lr_2_oci: [-1, 1] - OCI modulation on Stage 2 learning
    beta: [0, 10]
    w: [0, 1]
    """
    lr_1, lr_2_base, lr_2_oci, beta, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate stage 2 specific learning rate
    lr_2 = lr_2_base + (lr_2_oci * oci_score)
    lr_2 = np.clip(lr_2, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # 1. Stage 1 Choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # 2. Stage 2 Choice
        exp_q2 = np.exp(beta * q_stage2_mf[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        if r != -1:
            # Update Stage 2 Values (Reward Prediction Error)
            # Uses the OCI-modulated rate
            delta_stage2 = r - q_stage2_mf[s, a2]
            q_stage2_mf[s, a2] += lr_2 * delta_stage2

            # Update Stage 1 Values (Temporal Difference Error)
            # Uses the fixed Stage 1 rate
            # Note: We use q_stage2_mf[s, a2] as the target, effectively TD(1) logic often used in this task
            delta_stage1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
            q_stage1_mf[a1] += lr_1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```