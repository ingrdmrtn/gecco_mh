Here are three new cognitive models exploring different mechanisms for how OCI might influence decision-making in the two-step task, specifically focusing on perseveration and learning rate asymmetries, which are theoretically relevant to compulsivity.

### Model 1: OCI-Modulated Perseveration (Stickiness)
This model hypothesizes that high OCI leads to "sticky" behaviorâ€”a tendency to repeat the previous choice regardless of reward history. This is a common feature in compulsivity, representing a "habitual" repetition. We modulate a stickiness parameter by the OCI score.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Perseveration (Stickiness).
    
    This model introduces a 'stickiness' parameter that biases the subject to repeat 
    their previous first-stage choice. The magnitude of this stickiness is scaled 
    by the participant's OCI score, testing the hypothesis that high OCI leads to 
    more repetitive, compulsive behavior.
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    w: [0, 1] - Weighting between MB and MF (0=Pure MF, 1=Pure MB)
    stick_base: [0, 5] - Base tendency to repeat previous choice.
    stick_oci_sens: [0, 5] - Sensitivity of stickiness to OCI score.
    """
    learning_rate, beta, w, stick_base, stick_oci_sens = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Stickiness increases with OCI
    stickiness = stick_base + (stick_oci_sens * current_oci)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track previous choice for stickiness (initialize with -1 or handle first trial)
    last_action_1 = -1

    for trial in range(n_trials):

        # Policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness bonus to the previously chosen action
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        state_idx = int(state[trial])

        # Policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        r = reward[trial]
  
        # Update values
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Dependent Learning Rate Asymmetry
This model tests if OCI affects how participants learn from positive versus negative prediction errors. It is hypothesized that compulsive individuals might be hyper-sensitive to negative outcomes (avoidance) or fail to integrate positive feedback effectively. We split the learning rate into positive (`lr_pos`) and negative (`lr_neg`) components, where the negative learning rate is modulated by OCI.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Dependent Learning Rate Asymmetry.
    
    This model allows for different learning rates for positive and negative 
    prediction errors. The learning rate for negative prediction errors is 
    modulated by the OCI score, testing if high OCI subjects over-weight 
    failures/punishments.
    
    Bounds:
    lr_pos: [0, 1] - Learning rate for positive prediction errors.
    lr_neg_base: [0, 1] - Base learning rate for negative prediction errors.
    lr_oci_mod: [0, 1] - Modulation of negative learning rate by OCI.
    beta: [0, 10]
    w: [0, 1]
    """
    lr_pos, lr_neg_base, lr_oci_mod, beta, w = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate effective negative learning rate, bounded [0, 1]
    # We hypothesize OCI increases sensitivity to negative errors
    lr_neg = lr_neg_base + (lr_oci_mod * current_oci)
    if lr_neg > 1.0: lr_neg = 1.0

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # Policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        state_idx = int(state[trial])

        # Policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        r = reward[trial]
  
        # Update Stage 2
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        alpha2 = lr_pos if delta_stage2 > 0 else lr_neg
        q_stage2_mf[state_idx, a2] += alpha2 * delta_stage2
        
        # Update Stage 1
        # Note: Standard TD uses the updated value of the next state, or the reward + value of next state
        # Here we use the standard update: Q1(a1) += alpha * (Q2(s, a2) - Q1(a1))
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        alpha1 = lr_pos if delta_stage1 > 0 else lr_neg
        q_stage1_mf[a1] += alpha1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Randomness (Temperature)
This model posits that high OCI might be associated with more decision noise (or conversely, more rigid determinism). Instead of affecting the *values* or *weights*, OCI modulates the inverse temperature parameter `beta`. A lower beta implies more random exploration, while a higher beta implies stricter exploitation.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Inverse Temperature (Beta).
    
    This model assumes that the consistency of choices (exploration/exploitation balance)
    is influenced by OCI. The inverse temperature `beta` is a function of the OCI score.
    Beta = beta_base * (1 + oci_beta_scale * OCI). 
    If oci_beta_scale is positive, high OCI leads to more deterministic choices (rigidness).
    If negative (handled by bounds or interpretation), it would lead to more noise.
    
    Bounds:
    learning_rate: [0, 1]
    beta_base: [0, 10] - Baseline inverse temperature.
    oci_beta_scale: [0, 5] - Scaling factor for OCI's effect on beta.
    w: [0, 1]
    """
    learning_rate, beta_base, oci_beta_scale, w = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate effective beta
    # We assume high OCI might lead to higher rigidity (higher beta)
    beta = beta_base * (1.0 + oci_beta_scale * current_oci)
    
    # Cap beta to prevent overflow issues, though bounds usually handle this
    if beta > 20: beta = 20

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # Policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        state_idx = int(state[trial])

        # Policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        r = reward[trial]
  
        # Update values
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```