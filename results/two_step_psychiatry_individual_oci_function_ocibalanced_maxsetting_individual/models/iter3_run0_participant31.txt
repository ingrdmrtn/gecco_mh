Here are the three proposed cognitive models.

### Model 1: Asymmetric Learning Rate Model
This model hypothesizes that OCI symptoms affect how participants learn from negative outcomes (omission of reward) versus positive outcomes. High OCI might be associated with hypersensitivity to failure (anxiety) or hyposensitivity (habitual rigidity). The model splits the learning rate into `lr_pos` (for rewards) and `lr_neg` (for omissions), with the latter modulated by the OCI score.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Asymmetric Learning Rate Model.
    
    OCI modulates the learning rate specifically for negative prediction errors (omissions).
    This captures potential hypersensitivity or hyposensitivity to failure associated with OCI.
    
    Parameters:
    - lr_pos: [0,1] Learning rate for positive prediction errors (Reward=1).
    - lr_neg_base: [0,1] Baseline learning rate for negative prediction errors (Reward=0).
    - lr_neg_oci: [0,1] Modulation of negative learning rate by OCI (mapped to [-1, 1] scaling).
    - beta: [0,10] Inverse temperature.
    - w: [0,1] Weight for Model-Based control.
    """
    lr_pos, lr_neg_base, lr_neg_oci, beta, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate negative learning rate modulated by OCI
    # Map lr_neg_oci from [0,1] to [-1, 1] scaling factor
    lr_neg = lr_neg_base + (lr_neg_oci * 2.0 - 1.0) * oci_score
    lr_neg = np.clip(lr_neg, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # state x action
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * (q_net_1 - np.max(q_net_1))
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        q_vals_2 = q_stage2_mf[s_idx]
        logits_2 = beta * (q_vals_2 - np.max(q_vals_2))
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates with Asymmetric LR ---
        
        # Stage 1 Update (TD(0))
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        # Use lr_pos if value increased (positive surprise), lr_neg if decreased
        lr_1 = lr_pos if delta_stage1 >= 0 else lr_neg
        q_stage1_mf[a1] += lr_1 * delta_stage1
        
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        lr_2 = lr_pos if delta_stage2 >= 0 else lr_neg
        q_stage2_mf[s_idx, a2] += lr_2 * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Subjective Transition Belief
This model proposes that high OCI individuals may hold distorted beliefs about the environmental structure. While the true transition probability is 0.7, participants might perceive it as more deterministic (closer to 1.0) or more random (closer to 0.5) due to uncertainty or rigid thinking. This model modulates the `p` parameter in the Model-Based calculation based on OCI.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Subjective Transition Belief Model.
    
    The participant's belief about the transition matrix (Model-Based component) is modulated by OCI.
    High OCI may lead to rigid beliefs (overestimating commonality) or uncertainty (underestimating).
    
    Parameters:
    - learning_rate: [0,1]
    - beta: [0,10]
    - w: [0,1]
    - p_base: [0,1] Baseline belief of the 'common' transition probability.
    - p_oci: [0,1] Modulation of belief by OCI.
    """
    learning_rate, beta, w, p_base, p_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate subjective transition probability
    # Map p_oci to a shift factor. 
    # We allow OCI to shift the belief up or down from the baseline.
    shift = (p_oci * 2.0 - 1.0) * 0.5 * oci_score 
    p_belief = p_base + shift
    p_belief = np.clip(p_belief, 0.0, 1.0)
    
    # Construct the Subjective Transition Matrix
    transition_matrix = np.array([[p_belief, 1.0 - p_belief], [1.0 - p_belief, p_belief]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        # Use subjective matrix for MB calculation
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * (q_net_1 - np.max(q_net_1))
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        q_vals_2 = q_stage2_mf[s_idx]
        logits_2 = beta * (q_vals_2 - np.max(q_vals_2))
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Outcome-Specific Stickiness Model
This model refines the "stickiness" concept by distinguishing between "Win-Stay" and "Lose-Stay" behavior. Compulsivity (high OCI) is often characterized by the repetition of actions despite negative outcomes (Lose-Stay). This model allows OCI to specifically modulate the tendency to repeat an action after receiving no reward (0 coins).

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Outcome-Specific Stickiness Model.
    
    Distinguishes between sticking to an action after a win (Win-Stay) and after a loss (Lose-Stay).
    OCI modulates the 'Lose-Stay' tendency, reflecting compulsive repetition of unrewarded actions.
    
    Parameters:
    - learning_rate: [0,1]
    - beta: [0,10]
    - w: [0,1]
    - stick_win: [0,1] Stickiness bonus after a reward (mapped to [0, 5]).
    - stick_loss_base: [0,1] Baseline stickiness after no reward (mapped to [0, 5]).
    - stick_loss_oci: [0,1] Modulation of loss-stickiness by OCI.
    """
    learning_rate, beta, w, stick_win, stick_loss_base, stick_loss_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Map parameters to stickiness magnitude
    stick_w = stick_win * 5.0
    # Modulate loss-stickiness: OCI can increase or decrease the tendency to stick after loss
    stick_l = (stick_loss_base * 5.0) + ((stick_loss_oci * 2.0 - 1.0) * 5.0 * oci_score)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_a1 = -1
    last_r = -1
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net_1
        
        # Apply outcome-specific stickiness to the previously chosen action
        if last_a1 != -1:
            if last_r == 1:
                logits_1[last_a1] += stick_w
            else:
                logits_1[last_a1] += stick_l
                
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        last_a1 = a1
        
        # --- Stage 2 Policy ---
        q_vals_2 = q_stage2_mf[s_idx]
        logits_2 = beta * (q_vals_2 - np.max(q_vals_2))
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        last_r = r 

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```