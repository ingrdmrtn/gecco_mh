Here are 3 new cognitive models inspired by the participant's high OCI score (0.833) and the two-step task structure.

### Model 1: Hybrid Model with OCI-Modulated Model-Based Weight (`w`)
This model combines Model-Free (MF) and Model-Based (MB) reinforcement learning. The core hypothesis is that high OCI (compulsivity) might relate to a rigid reliance on habits (MF) over goal-directed planning (MB), or conversely, an over-thinking (MB) tendency. Here, we test if the balance parameter `w` is a function of the OCI score. Specifically, we model `w` as a logistic function of OCI to keep it bounded between 0 and 1, allowing the OCI score to shift the participant along the MF-MB spectrum.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL (MB/MF) where the weighting parameter 'w' is determined by OCI via a sigmoid function.
    
    Bounds:
    learning_rate: [0,1]
    beta: [0,10]
    w_logit_intercept: [-5, 5] (Unconstrained in theory, but bounded for optimizer)
    w_logit_slope: [-5, 5]
    """
    learning_rate, beta, w_logit_intercept, w_logit_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate w based on OCI using a sigmoid transform
    # w = 1 / (1 + exp(-(intercept + slope * oci)))
    # If slope is negative, higher OCI -> lower w (more Model-Free/Habitual)
    # If slope is positive, higher OCI -> higher w (more Model-Based)
    logit_w = w_logit_intercept + w_logit_slope * oci_score
    w = 1.0 / (1.0 + np.exp(-logit_w))
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_mf = np.zeros(2)         # Stage 1 Model-Free values
    q_mb = np.zeros(2)         # Stage 1 Model-Based values
    q_stage2 = np.zeros((2, 2)) # Stage 2 values (common to MB and MF)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Choice ---
        # Calculate Model-Based values for Stage 1: V_MB(s1, a1) = sum(T(s1, a1, s2) * max(Q_stage2(s2)))
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_mb = transition_matrix @ max_q_stage2
        
        # Hybrid value: Q_net = w * Q_MB + (1-w) * Q_MF
        q_net = w * q_mb + (1 - w) * q_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        # Stage 2 update (Standard Q-learning)
        # Prediction error at stage 2
        delta_stage2 = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 MF update (TD(1) logic usually, or simple SARSA-like)
        # Here we use the value of the chosen stage 2 state to update stage 1 MF
        # Standard in 2-step literature: update Q_MF(a1) using Q_stage2(s2, a2)
        delta_stage1 = q_stage2[s_idx, a2] - q_mf[a1]
        q_mf[a1] += learning_rate * delta_stage1
        
        # Note: We also often include an eligibility trace where the stage 2 RPE updates stage 1,
        # but for this simplified hybrid model, we stick to the basic TD structure.
        q_mf[a1] += learning_rate * delta_stage2 # Eligibility trace update

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Perseveration (Stickiness) Modulated by OCI
This model focuses on the "stickiness" or choice perseveration phenomenon. High OCI individuals might exhibit repetitive behaviors or difficulty switching sets. This model implements a Model-Free RL agent where the tendency to repeat the previous Stage 1 action (perseveration) is directly scaled by the OCI score. We separate the base stickiness from the OCI-dependent component.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free RL with Choice Stickiness (Perseveration) modulated by OCI.
    Stickiness is added to the Q-values of the previously chosen action.
    
    Bounds:
    learning_rate: [0,1]
    beta: [0,10]
    stick_base: [-2, 2] (Base tendency to repeat/switch)
    stick_oci_sens: [-5, 5] (Sensitivity of stickiness to OCI score)
    """
    learning_rate, beta, stick_base, stick_oci_sens = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate total stickiness bonus
    # If stick_total > 0, participant tends to repeat. If < 0, tends to switch.
    stickiness_total = stick_base + (stick_oci_sens * oci_score)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    prev_a1 = -1 # Initialize previous action

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Choice ---
        # Add stickiness bonus to Q-values for decision making only (not learning)
        q_stage1_modified = q_stage1.copy()
        if prev_a1 != -1:
            q_stage1_modified[prev_a1] += stickiness_total
            
        exp_q1 = np.exp(beta * q_stage1_modified)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Update previous action
        prev_a1 = a1

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        # Stage 2 update
        delta_stage2 = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 update (TD)
        # Using the value of the state we landed in (max Q of that state)
        val_stage2 = np.max(q_stage2[s_idx])
        delta_stage1 = val_stage2 - q_stage1[a1]
        q_stage1[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Separate Learning Rates for Rare vs. Common Transitions Modulated by OCI
This model hypothesizes that OCI affects how surprising events (rare transitions) are processed compared to expected events (common transitions). A pure Model-Based agent handles transitions via the probability matrix, but humans often show a "Model-Free" like update that is affected by the transition type. Here, we allow the learning rate for Stage 1 updates to differ based on whether the transition was Common or Rare, and we scale the *Rare* learning rate based on OCI. This captures the idea that high OCI might lead to over-interpreting or under-interpreting "glitches" in the environment structure.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free RL where the learning rate for Stage 1 updates depends on transition type (Common/Rare).
    The learning rate for Rare transitions is specifically modulated by OCI.
    
    Bounds:
    lr_common: [0,1]
    lr_rare_base: [0,1]
    lr_rare_oci_mod: [-1, 1]
    beta: [0,10]
    """
    lr_common, lr_rare_base, lr_rare_oci_mod, beta = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective learning rate for rare transitions
    # Bounded between 0 and 1
    lr_rare = lr_rare_base + (lr_rare_oci_mod * oci_score)
    lr_rare = np.clip(lr_rare, 0.001, 0.999)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    # Define common transitions: 0->0 (A->X) and 1->1 (U->Y) are common (idx 0->0, 1->1)
    # 0->1 and 1->0 are rare.
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        # Determine if transition was common or rare
        is_common = (a1 == s_idx) # 0->0 or 1->1
        
        current_lr = lr_common if is_common else lr_rare

        # Stage 2 update (Standard)
        delta_stage2 = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += current_lr * delta_stage2 # Using the same LR for simplicity, or could use fixed
        
        # Stage 1 update
        # We update Stage 1 based on the value of the state reached
        val_stage2 = np.max(q_stage2[s_idx])
        delta_stage1 = val_stage2 - q_stage1[a1]
        
        q_stage1[a1] += current_lr * delta_stage1
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```