Here are the three proposed cognitive models.

### Model 1: OCI-Modulated MB/MF Weighting
This model hypothesizes that the balance between Model-Based (goal-directed) and Model-Free (habitual) control is directly a function of the OCI score. Specifically, it tests if higher obsessive-compulsive symptoms lead to a reduction in Model-Based weighting ($w$), shifting behavior towards Model-Free habits.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model with OCI-modulated Model-Based Weighting.
    
    Hypothesis: OCI score determines the balance between goal-directed (MB) and habitual (MF) control.
    High OCI scores reduce the weight 'w', leading to more habitual behavior.
    
    Parameters:
    - learning_rate: [0, 1] Rate of updating Q-values.
    - beta: [0, 10] Inverse temperature for softmax.
    - w_base: [0, 1] Baseline weight for Model-Based values (at OCI=0).
    - w_oci_sens: [0, 1] Sensitivity of w to OCI. w reduces as OCI increases.
      w_eff = clip(w_base - (w_oci_sens * oci), 0, 1).
    """
    learning_rate, beta, w_base, w_oci_sens = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)      
    q_stage2_mf = np.zeros((2, 2)) 

    # Calculate effective w based on OCI
    w_effective = w_base - (w_oci_sens * oci_score)
    if w_effective < 0:
        w_effective = 0.0
    elif w_effective > 1:
        w_effective = 1.0

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2

        q_net = (w_effective * q_stage1_mb) + ((1 - w_effective) * q_stage1_mf)

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]

        # --- Stage 2 Policy ---
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Eligibility Traces
This model hypothesizes that high OCI scores increase the strength of "eligibility traces" ($\lambda$) in the Model-Free system. This means that for high OCI participants, the outcome at the second stage (reward) directly reinforces the first-stage choice, bypassing the step-by-step chain of values. This mechanism promotes the formation of strong stimulus-response habits over the full sequence.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model with OCI-modulated Eligibility Traces (TD-Lambda).
    
    Hypothesis: OCI increases the 'caching' of second-stage outcomes to first-stage choices.
    High OCI leads to a higher lambda, meaning outcomes directly reinforce the initial choice.
    
    Parameters:
    - learning_rate: [0, 1] Rate of updating Q-values.
    - beta: [0, 10] Inverse temperature for softmax.
    - w: [0, 1] Weight of Model-Based values.
    - lambda_scale: [0, 1] Scaling factor for eligibility trace based on OCI.
      lambda_eff = lambda_scale * oci.
    """
    learning_rate, beta, w, lambda_scale = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)      
    q_stage2_mf = np.zeros((2, 2)) 

    # Calculate effective lambda based on OCI
    lambda_eff = lambda_scale * oci_score
    if lambda_eff > 1.0: lambda_eff = 1.0

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2

        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]

        # --- Stage 2 Policy ---
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Calculate RPEs
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[state_idx, a2] # Calculated early for trace update
        
        # Update Stage 1 with eligibility trace
        # Q1 += alpha * (RPE1 + lambda * RPE2)
        q_stage1_mf[a1] += learning_rate * (delta_stage1 + lambda_eff * delta_stage2)

        # Update Stage 2
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Belief Rigidity
This model hypothesizes that high OCI scores are associated with cognitive rigidity, modeled here as a suppression of the learning rate. Participants with high OCI are assumed to update their beliefs more slowly in response to new information, making their behavior more resistant to change once established.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model with OCI-modulated Learning Rate Suppression (Rigidity).
    
    Hypothesis: High OCI participants have rigid beliefs and update Q-values more slowly.
    The effective learning rate is attenuated by the OCI score.
    
    Parameters:
    - lr_base: [0, 1] Base learning rate (when OCI=0).
    - beta: [0, 10] Inverse temperature for softmax.
    - w: [0, 1] Weight of Model-Based values.
    - rigidity: [0, 1] Factor by which OCI suppresses learning.
      lr_eff = lr_base / (1 + rigidity * oci * 10).
    """
    lr_base, beta, w, rigidity = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)      
    q_stage2_mf = np.zeros((2, 2)) 

    # Calculate effective learning rate (Rigidity Hypothesis)
    # Scaling rigidity by 10 to allow strong suppression within parameter bounds
    effective_lr = lr_base / (1.0 + (rigidity * oci_score * 10.0))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2

        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]

        # --- Stage 2 Policy ---
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += effective_lr * delta_stage1

        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += effective_lr * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```