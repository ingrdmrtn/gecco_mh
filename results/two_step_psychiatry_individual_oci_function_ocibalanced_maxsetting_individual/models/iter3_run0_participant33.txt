Here are three new cognitive models that incorporate the OCI score to explain the participant's behavior in the two-step task.

### Model 1: Hybrid Model-Based/Model-Free with OCI-Modulated Mixing
This model hypothesizes that the balance between Model-Based (planning) and Model-Free (habit) systems is determined by the OCI score. High OCI scores are often associated with a reliance on habit (Model-Free) over goal-directed planning (Model-Based).

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free reinforcement learning where the mixing weight (w)
    is modulated by the OCI score.
    
    Hypothesis: Higher OCI scores shift the strategy towards Model-Free (habitual) control.
    w = w_base * (1 - oci_score)  --> High OCI reduces w (dominance of MF).
    
    Parameters:
    learning_rate: [0, 1] - Learning rate for value updates.
    beta: [0, 10] - Inverse temperature for softmax.
    w_base: [0, 1] - Baseline weight for Model-Based control (0=Pure MF, 1=Pure MB).
    """
    learning_rate, beta, w_base = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # OCI modulation: High OCI reduces the model-based weight (w), making it more model-free.
    # We clamp it between 0 and 1.
    w = w_base * (1.0 - oci_score)
    w = np.clip(w, 0.0, 1.0)

    # Fixed transition matrix for the task (MB component)
    # A->X (0->0) is 0.7, A->Y (0->1) is 0.3
    # U->Y (1->1) is 0.7, U->X (1->0) is 0.3
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Q-values
    q_mf_stage1 = np.zeros(2)      # Model-free values for stage 1
    q_mf_stage2 = np.zeros((2, 2)) # Model-free values for stage 2 (states X, Y; actions L, R)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Handle missing data
        if a1 == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        # --- Stage 1 Policy ---
        # 1. Calculate Model-Based values: V_MB(s1) = T * max(Q_MF(s2))
        # We use the max of the stage 2 MF values as the estimated value of the state
        max_q_stage2 = np.max(q_mf_stage2, axis=1) # [max(Q(X)), max(Q(Y))]
        q_mb_stage1 = transition_matrix @ max_q_stage2

        # 2. Mix MB and MF values
        q_net_stage1 = w * q_mb_stage1 + (1 - w) * q_mf_stage1

        # 3. Softmax choice probability
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        # Standard Model-Free choice at stage 2
        exp_q2 = np.exp(beta * q_mf_stage2[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning Updates ---
        # Update Stage 2 Q-values (TD error)
        delta_stage2 = r - q_mf_stage2[s, a2]
        q_mf_stage2[s, a2] += learning_rate * delta_stage2

        # Update Stage 1 MF Q-values (TD(1) or similar - using Q(s2) to update Q(s1))
        # Here we use the standard SARSA-like update: Q(s1,a1) <~ Q(s2,a2)
        delta_stage1 = q_mf_stage2[s, a2] - q_mf_stage1[a1]
        q_mf_stage1[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Driven Learning Rate Asymmetry
This model posits that individuals with high OCI scores might learn differently from positive versus negative prediction errors. Specifically, compulsivity might be linked to over-learning from negative outcomes (avoidance) or positive outcomes (habit formation). Here we test if OCI scales the learning rate specifically for negative prediction errors.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free RL with asymmetric learning rates modulated by OCI.
    
    Hypothesis: High OCI participants have a heightened sensitivity to prediction errors,
    specifically scaling the learning rate based on the OCI score.
    lr_effective = lr_base * (1 + oci_factor * oci)
    
    Parameters:
    lr_base: [0, 1] - Base learning rate.
    beta: [0, 10] - Inverse temperature.
    oci_sens: [0, 5] - Sensitivity factor. Determines how much OCI boosts the learning rate.
    """
    lr_base, beta, oci_sens = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Effective learning rate is boosted by OCI score
    lr_effective = lr_base * (1.0 + oci_sens * oci_score)
    lr_effective = np.clip(lr_effective, 0.0, 1.0)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Pure Model-Free Q-values
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        if a1 == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        # Stage 1 Choice
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2 Choice
        exp_q2 = np.exp(beta * q_stage2[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Updates
        # Stage 2 update
        delta_stage2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += lr_effective * delta_stage2

        # Stage 1 update (Direct reinforcement from stage 2 Q-value)
        delta_stage1 = q_stage2[s, a2] - q_stage1[a1]
        q_stage1[a1] += lr_effective * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Dependent Decay of Unchosen Options
This model investigates whether high OCI leads to a faster "forgetting" or decay of the values of actions not taken. Compulsive behavior often involves a narrowing of focus or rigidity. If unchosen options decay rapidly, the agent becomes "stuck" on the chosen path because alternatives appear increasingly less valuable (or revert to neutral 0) compared to a reinforced habit.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Reinforcement Learning with decay of unchosen options, where the decay rate
    is modulated by OCI.
    
    Hypothesis: High OCI leads to faster decay of unchosen action values, 
    reinforcing rigidity/compulsivity.
    decay_rate = decay_base * oci_score
    
    Parameters:
    learning_rate: [0, 1] - Update rate for chosen actions.
    beta: [0, 10] - Inverse temperature.
    decay_base: [0, 1] - Base decay rate for unchosen actions.
    """
    learning_rate, beta, decay_base = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Decay rate scales with OCI. Higher OCI -> Stronger decay of alternatives.
    decay_rate = decay_base * oci_score
    decay_rate = np.clip(decay_rate, 0.0, 1.0)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        if a1 == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        # Stage 1 Choice
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2 Choice
        exp_q2 = np.exp(beta * q_stage2[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Updates
        
        # Stage 2: Update chosen, decay unchosen
        delta_stage2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += learning_rate * delta_stage2
        
        # Decay unchosen action in the current state
        unchosen_a2 = 1 - a2
        q_stage2[s, unchosen_a2] *= (1.0 - decay_rate)

        # Stage 1: Update chosen, decay unchosen
        delta_stage1 = q_stage2[s, a2] - q_stage1[a1]
        q_stage1[a1] += learning_rate * delta_stage1
        
        unchosen_a1 = 1 - a1
        q_stage1[unchosen_a1] *= (1.0 - decay_rate)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```