Here are three new cognitive models exploring different mechanisms for how OCI might influence decision-making in this task.

### Model 1: Hybrid MB/MF with OCI-Driven Model-Based Suppression
This model hypothesizes that high OCI symptoms, often associated with habitual or rigid behaviors, might suppress the flexible, computationally expensive Model-Based system in favor of the simpler Model-Free system. Instead of OCI affecting temperature or learning rates directly, here it acts as a weight on the mixture parameter `w`, specifically pulling it towards 0 (pure Model-Free).

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF with OCI-Driven Model-Based Suppression.
    
    Hypothesis: High OCI scores are associated with habitual, rigid behavior. 
    This model posits that higher OCI suppresses the contribution of the 
    flexible Model-Based (MB) system, pushing the agent towards a pure 
    Model-Free (MF) strategy.
    
    Parameters:
    learning_rate: [0, 1] Update rate for Q-values.
    beta: [0, 10] Inverse temperature for softmax.
    w_max: [0, 1] The maximum possible weight for the MB system (when OCI is 0).
    oci_suppression: [0, 1] How strongly OCI reduces the MB weight.
    
    The effective weight `w_eff` is calculated as w_max * (1 - oci_suppression * oci).
    """
    learning_rate, beta, w_max, oci_suppression = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective mixing weight. High OCI reduces w_eff towards 0 (Model-Free).
    w_eff = w_max * (1.0 - oci_suppression * oci_score)
    w_eff = np.clip(w_eff, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based Value: V(s') = max(Q_stage2(s', a'))
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = w_eff * q_stage1_mb + (1 - w_eff) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        
        # --- Stage 2 Choice ---
        # Stage 2 is purely Model-Free (terminal step)
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Learning ---
        # Update Stage 2 Q-values (TD(0))
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2
        
        # Update Stage 1 MF Q-values (TD(1) / SARSA-like update using stage 2 value)
        # Note: In standard Daw task, Stage 1 MF is often updated via Q(s1,a1) += alpha * (Q(s2,a2) - Q(s1,a1))
        # or using the reward directly if lambda=1. Here we use the standard TD chain.
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Dual Learning Rates with OCI-Enhanced Punishment Sensitivity
This model explores the idea that OCI is related to anxiety and error-monitoring. High OCI individuals might be hyper-sensitive to "failures" (receiving 0 coins). This model splits the learning rate into `alpha_pos` (for rewards) and `alpha_neg` (for omissions), and hypothesizes that OCI specifically boosts `alpha_neg`, causing the agent to unlearn options rapidly after a loss.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Dual Learning Rates with OCI-Enhanced Punishment Sensitivity.
    
    Hypothesis: High OCI correlates with anxiety and over-sensitivity to errors.
    This model assumes separate learning rates for positive outcomes (reward=1)
    and negative outcomes (reward=0). OCI specifically amplifies the learning rate
    for negative outcomes, making the agent abandon unrewarded paths faster.
    
    Parameters:
    alpha_pos: [0, 1] Learning rate for positive rewards (1 coin).
    alpha_neg_base: [0, 1] Baseline learning rate for zero rewards (0 coins).
    oci_neg_boost: [0, 5] Multiplier for alpha_neg based on OCI.
    beta: [0, 10] Inverse temperature.
    
    alpha_neg_eff = alpha_neg_base * (1 + oci * oci_neg_boost) (clipped to 1).
    """
    alpha_pos, alpha_neg_base, oci_neg_boost, beta = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective negative learning rate
    alpha_neg_eff = alpha_neg_base * (1.0 + oci_score * oci_neg_boost)
    alpha_neg_eff = np.clip(alpha_neg_eff, 0.0, 1.0)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Pure Model-Free formulation for simplicity to isolate learning rate effects
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Learning ---
        r = reward[trial]
        
        # Determine which learning rate to use
        if r > 0.5: # Reward received
            curr_alpha = alpha_pos
        else: # No reward
            curr_alpha = alpha_neg_eff
            
        # Update Stage 2
        delta_stage2 = r - q_stage2[state_idx, int(action_2[trial])]
        q_stage2[state_idx, int(action_2[trial])] += curr_alpha * delta_stage2
        
        # Update Stage 1 (TD update using Stage 2 Q-value)
        delta_stage1 = q_stage2[state_idx, int(action_2[trial])] - q_stage1[int(action_1[trial])]
        q_stage1[int(action_1[trial])] += curr_alpha * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Perseveration (Stickiness) Modulated by OCI
This model focuses on the compulsive aspect of OCI. Compulsivity is often characterized by repetition of actions regardless of outcome. Here, we model this as "stickiness" or perseveration—a tendency to repeat the previous Stage 1 choice. We hypothesize that OCI directly scales the magnitude of this stickiness bonus.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Perseveration (Stickiness) Modulated by OCI.
    
    Hypothesis: The compulsive component of OCI leads to "stickiness" or 
    perseveration—repeating the same Stage 1 spaceship choice regardless of 
    reward history. This model adds a bonus to the Q-value of the previously 
    chosen action, where the magnitude of the bonus is determined by OCI.
    
    Parameters:
    learning_rate: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] MB/MF mixing weight.
    stickiness_oci_factor: [0, 5] How much OCI contributes to the stickiness bonus.
    
    Stickiness bonus = oci * stickiness_oci_factor.
    """
    learning_rate, beta, w, stickiness_oci_factor = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    stickiness_bonus = oci_score * stickiness_oci_factor
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Apply stickiness to the previously chosen action
        if last_action_1 != -1:
            q_net[int(last_action_1)] += stickiness_bonus
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        last_action_1 = action_1[trial]
        state_idx = int(state[trial])
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Learning ---
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```