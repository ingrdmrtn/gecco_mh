Here are the 3 proposed cognitive models.

### Model 1: OCI-Modulated Value Forgetting
This model hypothesizes that high OCI scores correlate with an altered rate of "forgetting" for unchosen options. While standard Q-learning only updates the chosen action, this model decays the values of unchosen actions (aliens) at each step. High OCI might be associated with "sticky" mental representations (low forgetting) or conversely, a rapid flushing of working memory (high forgetting).

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Q-Learning with OCI-Modulated Forgetting (Value Decay).
    
    Hypothesis: OCI score modulates the decay rate of unchosen action values in the second stage.
    This tests if obsessive symptoms relate to the persistence (or lack thereof) of value representations.
    
    Parameters:
    - learning_rate: [0, 1] Update rate for chosen actions.
    - beta: [0, 10] Inverse temperature for softmax choice.
    - w: [0, 1] Weight for Model-Based control (0=MF, 1=MB).
    - stickiness: [0, 5] General choice perseveration bonus.
    - forget_base: [0, 1] Baseline forgetting rate for unchosen options.
    - forget_oci: [-1, 1] Modulation of forgetting rate by OCI.
    """
    learning_rate, beta, w, stickiness, forget_base, forget_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective forgetting rate constrained to [0, 1]
    forget_rate = forget_base + (forget_oci * oci_score)
    forget_rate = np.clip(forget_rate, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # 2 planets, 2 aliens each
    
    last_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based Value Calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Value
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Apply Stickiness
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        # Softmax Policy Stage 1
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]

        # --- Stage 2 Choice ---
        state_idx = int(state[trial])
        
        # Softmax Policy Stage 2
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # --- Learning & Forgetting ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Stage 1 Update (TD-learning)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Stage 2 Update (Reward Prediction Error)
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Forgetting for unchosen alien in the current state
        unchosen_a2 = 1 - a2
        q_stage2_mf[state_idx, unchosen_a2] *= (1.0 - forget_rate)
        
        # Note: We could also decay values for the unvisited state, 
        # but standard decay usually applies to available but unchosen options.
        # Here we apply decay to the unchosen option in the visited state.
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Subjective Transition Belief
This model hypothesizes that high OCI individuals may form a distorted internal model of the environment's structure. While the true transition probability is 0.7, high OCI participants might perceive the "Common" transition as more deterministic (closer to 1.0) or less reliable (closer to 0.5), affecting their Model-Based value calculations.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model with OCI-Distorted Transition Belief.
    
    Hypothesis: OCI score modulates the subjective probability of the 'Common' transition 
    used in the Model-Based calculation. High OCI might lead to over-confidence (rigidity) 
    or under-confidence (distrust) in the transition structure.
    
    Parameters:
    - learning_rate: [0, 1] Value update rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Model-Based weight.
    - stickiness: [0, 5] Choice perseveration.
    - belief_base: [0, 1] Baseline subjective probability of common transition.
    - belief_oci: [-1, 1] Modulation of belief by OCI.
    """
    learning_rate, beta, w, stickiness, belief_base, belief_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate subjective transition probability
    # True value is 0.7. The subject might deviate from this.
    p_common_subjective = belief_base + (belief_oci * oci_score)
    p_common_subjective = np.clip(p_common_subjective, 0.0, 1.0)
    p_rare_subjective = 1.0 - p_common_subjective
    
    # Subjective Transition Matrix used for MB calculation
    # Row 0: transitions from Space A (Common->X, Rare->Y)
    # Row 1: transitions from Space U (Rare->X, Common->Y)
    # Assuming standard mapping: A->X is common, U->Y is common.
    subjective_tm = np.array([
        [p_common_subjective, p_rare_subjective], 
        [p_rare_subjective, p_common_subjective]
    ])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based Value Calculation using SUBJECTIVE matrix
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = subjective_tm @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]

        # --- Stage 2 Choice ---
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # --- Learning ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Stage-Specific Learning Rates
This model tests if OCI differentially affects the learning of high-level choices (Stage 1) versus low-level reward associations (Stage 2). High OCI is often linked to deficits in flexibility; this might manifest as a specific rigidity (low learning rate) or volatility (high learning rate) in updating the value of specific aliens, distinct from the structural learning of the spaceships.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model with OCI-Modulated Stage 2 Learning Rate.
    
    Hypothesis: The deficit in OCI lies in the update of reward probabilities (Stage 2),
    while Stage 1 learning remains distinct. OCI modulates the Stage 2 learning rate.
    
    Parameters:
    - lr_s1: [0, 1] Learning rate for Stage 1 (Spaceships).
    - lr_s2_base: [0, 1] Baseline learning rate for Stage 2 (Aliens).
    - lr_s2_oci: [-1, 1] Modulation of Stage 2 learning rate by OCI.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Model-Based weight.
    - stickiness: [0, 5] Choice perseveration.
    """
    lr_s1, lr_s2_base, lr_s2_oci, beta, w, stickiness = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate Stage 2 specific learning rate
    lr_s2 = lr_s2_base + (lr_s2_oci * oci_score)
    lr_s2 = np.clip(lr_s2, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]

        # --- Stage 2 Choice ---
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # --- Learning ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Update Stage 1 with its specific learning rate
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr_s1 * delta_stage1
        
        # Update Stage 2 with OCI-modulated learning rate
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += lr_s2 * delta_stage2
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```