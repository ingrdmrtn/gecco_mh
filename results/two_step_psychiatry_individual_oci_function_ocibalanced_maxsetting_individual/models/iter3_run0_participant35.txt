Here are 3 new cognitive models that introduce different mechanisms for how OCI scores might influence reinforcement learning in this two-step task, distinct from previous iterations.

### Model 1: OCI-Modulated "Stickiness" (Perseveration)
This model hypothesizes that high OCI scores lead to increased behavioral rigidity or "stickiness" (perseveration), meaning the participant is more likely to repeat their previous choice regardless of the reward outcome. This is implemented as an autocorrelation term added to the Q-values, where the strength of this stickiness scales with the OCI score.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free RL with OCI-modulated choice perseverance (stickiness).
    
    Hypothesis: Participants with higher OCI scores exhibit higher behavioral rigidity
    (stickiness), making them more likely to repeat the previous first-stage choice 
    regardless of value.
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    w: [0, 1] (Mixing weight: 1=MB, 0=MF)
    stickiness_mag: [0, 5] (Magnitude of the stickiness effect)
    """
    learning_rate, beta, w, stickiness_mag = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective stickiness based on OCI
    # Higher OCI -> higher stickiness
    stickiness = stickiness_mag * oci_score

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_mf1 = np.zeros(2)      
    q_mf2 = np.zeros((2, 2)) 
    
    prev_a1 = -1 # Track previous action

    for trial in range(n_trials):
        # Handle missing data
        if action_1[trial] == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            prev_a1 = -1
            continue

        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_mf2 = np.max(q_mf2, axis=1)
        q_mb1 = transition_matrix @ max_q_mf2
        
        q_net = w * q_mb1 + (1 - w) * q_mf1
        
        # Add stickiness bonus to the previously chosen action
        q_net_sticky = q_net.copy()
        if prev_a1 != -1:
            q_net_sticky[prev_a1] += stickiness
        
        exp_q1 = np.exp(beta * q_net_sticky)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_mf2[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        # Update Stage 2
        delta_2 = r - q_mf2[s2, a2]
        q_mf2[s2, a2] += learning_rate * delta_2

        # Update Stage 1 (Model-Free) - SARSA-like
        delta_1 = q_mf2[s2, a2] - q_mf1[a1]
        q_mf1[a1] += learning_rate * delta_1
        
        # Eligibility trace (lambda=1) for stage 1
        q_mf1[a1] += learning_rate * delta_2
        
        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Second-Stage Learning Rate
This model posits that OCI affects how aggressively participants update their values based on direct outcomes (second-stage rewards). Specifically, it tests if higher OCI is associated with "over-learning" or hypersensitivity to prediction errors at the second stage, potentially driving compulsive checking behaviors. The learning rate is a base rate plus an OCI-dependent boost.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL with OCI-modulated learning rate.
    
    Hypothesis: Participants with high OCI have a heightened sensitivity to prediction
    errors (outcomes), resulting in a higher effective learning rate. They 'over-update'
    their beliefs based on recent outcomes.
    
    Bounds:
    lr_base: [0, 0.5] (Base learning rate)
    lr_oci_gain: [0, 0.5] (Additional learning rate scaled by OCI)
    beta: [0, 10]
    w: [0, 1]
    """
    lr_base, lr_oci_gain, beta, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Effective learning rate increases with OCI
    # We clip to 1.0 to ensure stability
    learning_rate = min(1.0, lr_base + (lr_oci_gain * oci_score))

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_mf1 = np.zeros(2)      
    q_mf2 = np.zeros((2, 2))

    for trial in range(n_trials):
        if action_1[trial] == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_mf2 = np.max(q_mf2, axis=1)
        q_mb1 = transition_matrix @ max_q_mf2
        
        q_net = w * q_mb1 + (1 - w) * q_mf1
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_mf2[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        # Update Stage 2
        delta_2 = r - q_mf2[s2, a2]
        q_mf2[s2, a2] += learning_rate * delta_2

        # Update Stage 1 (Model-Free)
        delta_1 = q_mf2[s2, a2] - q_mf1[a1]
        q_mf1[a1] += learning_rate * delta_1
        
        # Eligibility trace
        q_mf1[a1] += learning_rate * delta_2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Inverse Temperature (Exploration/Exploitation)
This model suggests that OCI affects the decision noise or the exploration-exploitation balance. Higher OCI scores might correlate with more deterministic (exploitative) behavior, reducing random exploration. Here, the inverse temperature `beta` is modeled as a base value modulated by the OCI score.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL with OCI-modulated inverse temperature (beta).
    
    Hypothesis: High OCI is associated with reduced exploration (higher beta).
    Anxious or compulsive individuals may be less willing to explore randomly 
    and stick more rigidly to what they perceive as the high-value option.
    
    Bounds:
    learning_rate: [0, 1]
    beta_base: [0, 5]
    beta_oci_slope: [0, 5] (Increase in beta per unit of OCI)
    w: [0, 1]
    """
    learning_rate, beta_base, beta_oci_slope, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective beta
    # Higher OCI -> higher beta -> more deterministic/less exploration
    beta = beta_base + (beta_oci_slope * oci_score)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_mf1 = np.zeros(2)      
    q_mf2 = np.zeros((2, 2))

    for trial in range(n_trials):
        if action_1[trial] == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_mf2 = np.max(q_mf2, axis=1)
        q_mb1 = transition_matrix @ max_q_mf2
        
        q_net = w * q_mb1 + (1 - w) * q_mf1
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_mf2[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        delta_2 = r - q_mf2[s2, a2]
        q_mf2[s2, a2] += learning_rate * delta_2

        delta_1 = q_mf2[s2, a2] - q_mf1[a1]
        q_mf1[a1] += learning_rate * delta_1
        
        q_mf1[a1] += learning_rate * delta_2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```