Here are three new cognitive models that explore different mechanisms by which OCI scores might influence decision-making in the two-step task, specifically focusing on mixing model-based/model-free strategies, learning rate asymmetries, and eligibility traces.

### Model 1: OCI-Modulated Model-Based Weighting
This model tests the hypothesis that high OCI scores are associated with a reliance on habit (model-free) over goal-directed (model-based) planning. It implements a standard hybrid reinforcement learning model where the weighting parameter `w` (which balances model-based and model-free control) is a function of the OCI score. A higher OCI score is hypothesized to reduce `w`, pushing the agent towards model-free behavior.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Model-Based Weighting.
    Hypothesis: High OCI reduces the contribution of the model-based system (goal-directed),
    leading to more habitual (model-free) behavior. The weighting parameter 'w' is
    down-modulated by the OCI score.
    
    Parameters:
    - learning_rate: [0, 1] Update rate for Q-values.
    - beta: [0, 10] Inverse temperature for softmax choice.
    - w_base: [0, 1] Base weight for model-based values (0=pure MF, 1=pure MB).
    - oci_w_damp: [0, 1] Strength of OCI-driven reduction in model-based weighting.
    """
    learning_rate, beta, w_base, oci_w_damp = model_parameters
    n_trials = len(action_1)
    # Ensure inputs are integers
    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    
    # Calculate effective w based on OCI
    # High OCI reduces w, making behavior more model-free
    # We clip to ensure w stays in [0, 1]
    w = w_base * (1.0 - (oci[0] * oci_w_damp))
    w = np.clip(w, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Q-values
    q_stage1_mf = np.zeros(2) # Model-free stage 1 values
    q_stage2_mf = np.zeros((2, 2)) # Model-free stage 2 values (also used for MB calculation)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-based Value: V(s') = max_a Q(s', a)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        # Q_MB(s, a) = T(s, a, s') * V(s')
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value: Q_net = w * Q_MB + (1-w) * Q_MF
        q_net_s1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax Policy
        exp_q1 = np.exp(beta * q_net_s1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        s_idx = state[trial]
        
        # --- Stage 2 Choice ---
        # Standard model-free choice at stage 2
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]
        
        # --- Learning ---
        # Stage 2 Update (TD)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 Update (TD)
        # Note: In simple hybrid models, Stage 1 MF Q-values are updated using the 
        # value of the state actually reached (SARSA-like) or the best value (Q-learning).
        # Here we use the value of the chosen stage 2 action as the target.
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Enhanced Eligibility Trace (Lambda)
This model hypothesizes that high OCI is linked to how credit is assigned to past actions. Specifically, it tests if OCI modulates the eligibility trace parameter ($\lambda$). A higher $\lambda$ means the first-stage action gets more direct credit for the final reward (ignoring the transition structure), which is a signature of model-free learning. If OCI increases $\lambda$, it suggests a stronger "direct reinforcement" of the initial choice, bypassing the causal structure of the task.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Enhanced Eligibility Trace (Lambda).
    Hypothesis: OCI modulates the eligibility trace parameter (lambda). 
    Higher lambda means stage 1 choices are reinforced more directly by the final outcome,
    characteristic of model-free logic.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - lambda_base: [0, 1] Base eligibility trace decay.
    - oci_lambda_gain: [0, 1] Increase in lambda per unit of OCI.
    """
    learning_rate, beta, lambda_base, oci_lambda_gain = model_parameters
    n_trials = len(action_1)
    
    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)

    # Calculate lambda based on OCI
    # Ensure lambda stays in [0, 1]
    lambda_param = lambda_base + (oci[0] * oci_lambda_gain)
    lambda_param = np.clip(lambda_param, 0.0, 1.0)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        s_idx = state[trial]
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]
        
        # --- Learning ---
        # Prediction error at stage 2
        delta_2 = r - q_stage2[s_idx, a2]
        
        # Prediction error at stage 1
        # The value of state 2 is estimated by the Q-value of the chosen action
        delta_1 = q_stage2[s_idx, a2] - q_stage1[a1]
        
        # Update Stage 2
        q_stage2[s_idx, a2] += learning_rate * delta_2
        
        # Update Stage 1
        # Standard TD(1) or TD(lambda) update:
        # Stage 1 is updated by its own error (delta_1) PLUS the eligibility trace of delta_2
        q_stage1[a1] += learning_rate * (delta_1 + lambda_param * delta_2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Asymmetric Learning Rates (Pos/Neg Bias)
This model investigates whether OCI scores relate to an imbalance in learning from positive versus negative prediction errors. It posits that OCI might amplify learning from negative outcomes (avoidance) or dampen learning from positive outcomes. We define a base learning rate and an asymmetry parameter that is modulated by OCI.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Asymmetric Learning Rates.
    Hypothesis: High OCI individuals might learn differently from positive (reward) 
    vs negative (omission) prediction errors.
    This model uses separate learning rates for positive and negative deltas.
    The ratio/balance is determined by OCI.
    
    Parameters:
    - lr_base: [0, 1] Base learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Model-based weight (fixed for this model to isolate learning rate effects).
    - oci_neg_bias: [0, 1] How much OCI adds to the learning rate for negative prediction errors.
    """
    lr_base, beta, w, oci_neg_bias = model_parameters
    n_trials = len(action_1)
    
    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    
    # Define learning rates
    # lr_pos is the base rate
    # lr_neg is boosted by OCI
    lr_pos = lr_base
    lr_neg = lr_base + (oci[0] * oci_neg_bias)
    lr_neg = np.clip(lr_neg, 0.0, 1.0) # Ensure bound

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        s_idx = state[trial]
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]
        
        # --- Learning ---
        # Stage 2
        delta_2 = r - q_stage2_mf[s_idx, a2]
        if delta_2 >= 0:
            q_stage2_mf[s_idx, a2] += lr_pos * delta_2
        else:
            q_stage2_mf[s_idx, a2] += lr_neg * delta_2
            
        # Stage 1
        delta_1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        if delta_1 >= 0:
            q_stage1_mf[a1] += lr_pos * delta_1
        else:
            q_stage1_mf[a1] += lr_neg * delta_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```