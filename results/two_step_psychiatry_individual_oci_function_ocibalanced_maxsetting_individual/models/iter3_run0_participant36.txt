Here are the three proposed cognitive models.

### Model 1: Asymmetric Learning Rates Modulated by OCI
This model hypothesizes that OCI symptoms affect how participants learn from positive versus negative prediction errors (asymmetric learning). Specifically, it proposes that high OCI scores might amplify learning from negative outcomes (hypersensitivity to failure/punishment) or alter learning from positive outcomes. The model splits the learning rate into `lr_pos` (for positive prediction errors) and `lr_neg` (for negative prediction errors), with `lr_neg` being modulated by the OCI score.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Asymmetric Learning Rate Model.
    
    Hypothesis: Participants update their value estimates differently depending on whether 
    the outcome was better (positive prediction error) or worse (negative prediction error) 
    than expected. OCI score modulates the learning rate for negative prediction errors 
    (e.g., increased sensitivity to 'failure').
    
    Parameters:
    - lr_pos: [0, 1] Learning rate for positive prediction errors.
    - lr_neg_base: [0, 1] Base learning rate for negative prediction errors.
    - lr_neg_oci_scale: [-1, 1] Modulation of negative learning rate by OCI.
    - beta: [0, 10] Inverse temperature (softmax sensitivity).
    - w: [0, 1] Weighting between Model-Based (1) and Model-Free (0) control.
    - lambda_coeff: [0, 1] Eligibility trace decay (coupling stage 2 outcome to stage 1).
    """
    lr_pos, lr_neg_base, lr_neg_oci_scale, beta, w, lambda_coeff = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate OCI-modulated negative learning rate and clip to valid range
    lr_neg = lr_neg_base + (lr_neg_oci_scale * oci_score)
    lr_neg = np.clip(lr_neg, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # Model-Based Value
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Value
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Softmax
        logits = beta * q_net
        logits = logits - np.max(logits)
        probs_1 = np.exp(logits) / np.sum(np.exp(logits))
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        
        # --- Stage 2 Decision ---
        logits_2 = beta * q_stage2_mf[state_idx]
        logits_2 = logits_2 - np.max(logits_2)
        probs_2 = np.exp(logits_2) / np.sum(np.exp(logits_2))
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Learning Updates ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Prediction Errors
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        
        # Update Stage 2
        alpha_2 = lr_pos if delta_stage2 >= 0 else lr_neg
        q_stage2_mf[state_idx, a2] += alpha_2 * delta_stage2
        
        # Update Stage 1 (SARSA(lambda))
        total_delta_1 = delta_stage1 + lambda_coeff * delta_stage2
        alpha_1 = lr_pos if total_delta_1 >= 0 else lr_neg
        q_stage1_mf[a1] += alpha_1 * total_delta_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Accumulating Habit Trace Modulated by OCI
This model distinguishes between simple 1-step repetition (stickiness) and a growing "habit strength" that accumulates over repeated choices. It tracks a `trace` for each spaceship that decays over time but increases when chosen. The influence of this habit trace on the decision is modulated by OCI. This captures the idea that compulsive behavior might involve stronger habit formation or difficulty disengaging from a repeated action.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Accumulating Habit Trace Model.
    
    Hypothesis: OCI modulates the weight of a 'habit trace' that accumulates over 
    repeated choices. Unlike simple stickiness (which only looks at the previous trial), 
    this trace builds up if the same option is chosen repeatedly, modeling compulsive 
    repetition.
    
    Parameters:
    - learning_rate: [0, 1]
    - beta: [0, 10]
    - w: [0, 1] MB/MF balance.
    - lambda_coeff: [0, 1] Eligibility trace.
    - trace_decay: [0, 1] Rate at which the choice trace decays (1 = instant decay, 0 = no decay).
    - habit_w_base: [0, 5] Base weight of the habit trace in the decision.
    - habit_w_oci_scale: [0, 5] OCI modulation of the habit weight.
    """
    learning_rate, beta, w, lambda_coeff, trace_decay, habit_w_base, habit_w_oci_scale = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    habit_weight = habit_w_base + (habit_w_oci_scale * oci_score)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Trace for stage 1 choices (Spaceship A vs U)
    choice_trace = np.zeros(2)
    
    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Add habit trace bonus to logits
        logits = beta * q_net + (habit_weight * choice_trace)
        
        logits = logits - np.max(logits)
        probs_1 = np.exp(logits) / np.sum(np.exp(logits))
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # Update Choice Trace
        # trace[a] <- trace[a] * (1-decay) + 1 * decay if chosen
        # trace[not_a] <- trace[not_a] * (1-decay)
        # Simplified: decay towards 1 if chosen, towards 0 if not.
        # Standard implementation: T(a) = T(a) + decay * (1 - T(a)) if chosen
        #                          T(a') = T(a') + decay * (0 - T(a')) if not chosen
        chosen = int(action_1[trial])
        choice_trace[chosen] += trace_decay * (1.0 - choice_trace[chosen])
        choice_trace[1 - chosen] += trace_decay * (0.0 - choice_trace[1 - chosen])
        
        state_idx = int(state[trial])
        
        # --- Stage 2 Decision ---
        logits_2 = beta * q_stage2_mf[state_idx]
        logits_2 = logits_2 - np.max(logits_2)
        probs_2 = np.exp(logits_2) / np.sum(np.exp(logits_2))
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Learning ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        
        q_stage1_mf[a1] += learning_rate * delta_stage1 + learning_rate * lambda_coeff * delta_stage2
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Outcome-Dependent Perseveration Modulated by OCI
This model refines the concept of "stickiness" by differentiating between "Win-Stay" and "Lose-Stay". It hypothesizes that high OCI scores might specifically drive "Lose-Stay" behavior (perseveration even after a lack of reward), possibly due to rigidity or checking compulsions. The model has a base perseveration for wins, and an OCI-modulated perseveration parameter specifically for losses (outcomes with 0 reward).

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Outcome-Dependent Perseveration Model.
    
    Hypothesis: Perseveration (stickiness) is not uniform but depends on the previous outcome.
    High OCI might lead to 'stubbornness' or 'Lose-Stay' behavior, where the participant 
    repeats a choice even after receiving no reward (0), potentially reflecting compulsive 
    rigidity.
    
    Parameters:
    - learning_rate: [0, 1]
    - beta: [0, 10]
    - w: [0, 1]
    - lambda_coeff: [0, 1]
    - p_win: [0, 5] Perseveration bonus after a reward (Win-Stay).
    - p_loss_base: [0, 5] Base perseveration bonus after no reward (Lose-Stay).
    - p_loss_oci_scale: [0, 5] OCI modulation of Lose-Stay perseveration.
    """
    learning_rate, beta, w, lambda_coeff, p_win, p_loss_base, p_loss_oci_scale = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    p_loss = p_loss_base + (p_loss_oci_scale * oci_score)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1
    prev_reward = -1.0
    
    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        logits = beta * q_net
        
        # Apply outcome-dependent stickiness
        if prev_action_1 != -1:
            if prev_reward == 1.0:
                logits[int(prev_action_1)] += p_win
            else:
                # prev_reward == 0.0 or other
                logits[int(prev_action_1)] += p_loss
        
        logits = logits - np.max(logits)
        probs_1 = np.exp(logits) / np.sum(np.exp(logits))
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # Store for next trial
        prev_action_1 = action_1[trial]
        # Note: We use the reward obtained at the END of this trial for the NEXT trial's stickiness
        # But here we need to store it after we process the current reward.
        # Wait, strictly, stickiness applies based on the *previous* trial's outcome.
        # So we update prev_reward at the end of the loop.
        
        state_idx = int(state[trial])
        
        # --- Stage 2 Decision ---
        logits_2 = beta * q_stage2_mf[state_idx]
        logits_2 = logits_2 - np.max(logits_2)
        probs_2 = np.exp(logits_2) / np.sum(np.exp(logits_2))
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Learning ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        
        q_stage1_mf[a1] += learning_rate * delta_stage1 + learning_rate * lambda_coeff * delta_stage2
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        prev_reward = r

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```