Here are 3 new cognitive models exploring different mechanisms for how OCI scores might influence decision-making in this two-step task.

### Cognitive Model 1: OCI-Modulated Model-Based/Model-Free Trade-off
This model hypothesizes that high OCI scores are associated with a deficit in goal-directed (Model-Based) control, leading to a stronger reliance on habitual (Model-Free) systems. Instead of a fixed mixing weight `w`, the weight is dynamically adjusted based on the OCI score. Higher OCI reduces `w` (the contribution of the Model-Based system).

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated MB/MF Trade-off.
    Hypothesis: Higher OCI scores correlate with reduced goal-directed (Model-Based) control.
    The mixing weight 'w' is derived from a base parameter and reduced by the OCI score.
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    w_base: [0, 1] (Baseline Model-Based weight for OCI=0)
    oci_impact: [0, 1] (How strongly OCI reduces MB control)
    """
    learning_rate, beta, w_base, oci_impact = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective w. High OCI reduces w (shift towards Model-Free).
    # We clip to ensure w stays in [0, 1].
    w = w_base * (1.0 - (oci_score * oci_impact))
    w = np.clip(w, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        if a1 == -1: a1 = 0
        s_idx = int(state[trial])
        if s_idx == -1: s_idx = 0
        a2 = int(action_2[trial])
        if a2 == -1: a2 = 0
        r = reward[trial]
        if r == -1: r = 0

        # Model-Based Value Calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Stage 1 Policy
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Stage 2 Update (TD)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 Update (TD)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2: OCI-Dependent Learning Rate Asymmetry
This model investigates if OCI affects how participants learn from positive versus negative outcomes. Specifically, it posits that high OCI individuals might be hypersensitive to prediction errors (either positive or negative). Here, we model a splitting of the learning rate into positive (`alpha_pos`) and negative (`alpha_neg`) components, where the balance is shifted by the OCI score.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Dependent Learning Rate Asymmetry.
    Hypothesis: OCI affects the balance between learning from positive vs negative prediction errors.
    The model uses a base learning rate, but OCI modifies the rate for negative errors specifically,
    potentially reflecting increased sensitivity to failure or missed rewards (perfectionism).
    
    Bounds:
    alpha_base: [0, 1]
    beta: [0, 10]
    w: [0, 1]
    oci_neg_bias: [0, 2] (Multiplier for learning rate on negative PE, scaled by OCI)
    """
    alpha_base, beta, w, oci_neg_bias = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Alpha for positive PEs is the base.
    # Alpha for negative PEs is modified by OCI.
    # If oci_neg_bias > 1, high OCI learns faster from negative outcomes.
    alpha_pos = alpha_base
    alpha_neg = alpha_base * (1.0 + oci_score * oci_neg_bias)
    alpha_neg = np.clip(alpha_neg, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        if a1 == -1: a1 = 0
        s_idx = int(state[trial])
        if s_idx == -1: s_idx = 0
        a2 = int(action_2[trial])
        if a2 == -1: a2 = 0
        r = reward[trial]
        if r == -1: r = 0

        # Stage 1 Choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2 Choice
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        eff_alpha_2 = alpha_pos if delta_stage2 >= 0 else alpha_neg
        q_stage2_mf[s_idx, a2] += eff_alpha_2 * delta_stage2
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        eff_alpha_1 = alpha_pos if delta_stage1 >= 0 else alpha_neg
        q_stage1_mf[a1] += eff_alpha_1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3: Inverse Temperature (Exploration) Modulation
This model proposes that OCI symptoms manifest as rigidity or reduced exploration (uncertainty intolerance). Instead of affecting learning or strategy weighting directly, OCI scales the inverse temperature parameter `beta`. A higher beta means more deterministic (greedy) choices and less exploration. This model tests if high OCI participants are "stiffer" in their selection policy.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Inverse Temperature (Rigidity).
    Hypothesis: High OCI leads to more deterministic/rigid choice behavior (reduced exploration).
    The inverse temperature 'beta' is scaled up by the OCI score.
    
    Bounds:
    learning_rate: [0, 1]
    beta_base: [0, 10]
    w: [0, 1]
    oci_rigidity: [0, 5] (Scales beta based on OCI)
    """
    learning_rate, beta_base, w, oci_rigidity = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Effective beta increases with OCI score
    beta_eff = beta_base * (1.0 + oci_score * oci_rigidity)
    # Cap beta to prevent numerical overflows, though bounds usually handle this
    beta_eff = min(beta_eff, 20.0) 
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        if a1 == -1: a1 = 0
        s_idx = int(state[trial])
        if s_idx == -1: s_idx = 0
        a2 = int(action_2[trial])
        if a2 == -1: a2 = 0
        r = reward[trial]
        if r == -1: r = 0

        # Stage 1 Choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_eff * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2 Choice
        exp_q2 = np.exp(beta_eff * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```