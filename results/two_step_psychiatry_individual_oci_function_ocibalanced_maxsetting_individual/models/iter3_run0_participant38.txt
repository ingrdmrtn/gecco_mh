Here are three cognitive models designed to explain the participant's behavior, incorporating their high OCI score (0.783) as a key modulator of decision-making mechanisms.

### Model 1: Dynamic MB/MF Balance (Stress-Induced Habit)
This model tests the hypothesis that high OCI participants retreat to habitual (Model-Free) control strategies specifically following negative outcomes. While they may use Model-Based planning normally, a failure (receiving 0 coins) acts as a stressor that degrades their Model-Based weight ($w$) for the subsequent trial, making them more prone to repetition or simple temporal difference learning.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Dynamic MB/MF Balance Model (Stress-Induced Habit).
    
    Hypothesis: High OCI participants retreat to Model-Free (habitual) control 
    following a negative outcome (loss). The mixing weight 'w' decreases 
    in trials immediately following a reward of 0, proportional to OCI severity.
    
    Parameters:
    learning_rate: [0, 1] - Update rate for Q-values.
    beta: [0, 10] - Inverse temperature (softmax).
    w_base: [0, 1] - Baseline mixing weight (1=Full MB, 0=Full MF).
    w_loss_k: [0, 1] - Reduction in 'w' after a loss, scaled by OCI.
    lambda_decay: [0, 1] - Eligibility trace decay.
    """
    learning_rate, beta, w_base, w_loss_k, lambda_decay = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_reward = 1.0 # Assume neutral/success start to not trigger penalty immediately

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Dynamic w calculation
        # If previous reward was 0, reduce w (shift to MF)
        # w_eff = w_base * (1 - penalty_factor)
        if prev_reward == 0.0:
            w_eff = w_base * (1.0 - w_loss_k * oci_score)
        else:
            w_eff = w_base
        
        # Clip w to [0,1]
        w_eff = np.clip(w_eff, 0.0, 1.0)

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w_eff * q_stage1_mb + (1 - w_eff) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Stage 2 Policy
        qs_2 = q_stage2_mf[s_idx]
        exp_q2 = np.exp(beta * qs_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # Updates
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        q_stage1_mf[a1] += learning_rate * delta_stage1 + learning_rate * lambda_decay * delta_stage2
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        prev_reward = r

    eps = 1e-10
    mask = action_1 != -1
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss
```

### Model 2: Subjective Punishment Model (Loss Aversion)
This model posits that high OCI leads to a distorted perception of reward. Specifically, the absence of a coin (0 reward) is not treated as a neutral event but as a punishment (negative utility). This "fear of failure" drives the agent to avoid unrewarded options more aggressively than a standard agent would.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Subjective Punishment Model (Loss Aversion).
    
    Hypothesis: High OCI participants perceive the lack of reward (0 coins) 
    as an active punishment/negative utility rather than a neutral zero. 
    The magnitude of this negative utility is scaled by the OCI score.
    
    Parameters:
    learning_rate: [0, 1]
    beta: [0, 10]
    w: [0, 1]
    lambda_decay: [0, 1]
    loss_magnitude: [0, 5] - Scaling factor for negative reward perception (R=0 -> R = -k*OCI).
    """
    learning_rate, beta, w, lambda_decay, loss_magnitude = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Subjective Reward Transformation
        # If reward is 0, treat it as negative based on OCI
        if r == 0.0:
            r_eff = -1.0 * loss_magnitude * oci_score
        else:
            r_eff = r

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Stage 2 Policy
        qs_2 = q_stage2_mf[s_idx]
        exp_q2 = np.exp(beta * qs_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # Updates using effective reward
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r_eff - q_stage2_mf[s_idx, a2]
        
        q_stage1_mf[a1] += learning_rate * delta_stage1 + learning_rate * lambda_decay * delta_stage2
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    mask = action_1 != -1
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss
```

### Model 3: Dual Stickiness Model (State-Dependent Compulsivity)
This model differentiates between general choice inertia (repeating the spaceship choice) and compulsive checking behavior at the terminal state (repeating the alien choice). It hypothesizes that OCI specifically exacerbates the tendency to repeat actions at Stage 2 (within the same planet), reflecting a "checking" compulsion.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Dual Stickiness Model (State-Dependent Compulsivity).
    
    Hypothesis: OCI drives compulsive repetition specifically at the terminal 
    stage (checking the same alien), distinct from general choice inertia at Stage 1.
    Includes a base stickiness for Stage 1 and an OCI-modulated stickiness for Stage 2.
    
    Parameters:
    learning_rate: [0, 1]
    beta: [0, 10]
    w: [0, 1]
    lambda_decay: [0, 1]
    stick_1: [0, 5] - General stickiness for Stage 1 choice.
    stick_2_oci: [0, 5] - OCI-scaled stickiness for Stage 2 choice within state.
    """
    learning_rate, beta, w, lambda_decay, stick_1, stick_2_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_a1 = -1
    # Track prev action for state 0 and state 1 separately for stage 2 stickiness
    prev_a2_by_state = np.array([-1, -1]) 

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Stage 1 Policy with Base Stickiness
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        stick_vec_1 = np.zeros(2)
        if prev_a1 != -1:
            stick_vec_1[prev_a1] = stick_1
            
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf + stick_vec_1
        
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Stage 2 Policy with OCI-modulated Stickiness (State-dependent)
        qs_2 = q_stage2_mf[s_idx]
        
        stick_vec_2 = np.zeros(2)
        last_a2_in_this_state = prev_a2_by_state[s_idx]
        if last_a2_in_this_state != -1:
            stick_vec_2[last_a2_in_this_state] = stick_2_oci * oci_score
            
        q_net_2 = qs_2 + stick_vec_2
        
        exp_q2 = np.exp(beta * q_net_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # Updates
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        q_stage1_mf[a1] += learning_rate * delta_stage1 + learning_rate * lambda_decay * delta_stage2
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Update history
        prev_a1 = a1
        prev_a2_by_state[s_idx] = a2

    eps = 1e-10
    mask = action_1 != -1
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss
```