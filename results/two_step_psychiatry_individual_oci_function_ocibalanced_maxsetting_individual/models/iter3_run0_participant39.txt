Here are the three proposed cognitive models.

### Cognitive Model 1: Value Decay Modulated by OCI
This model hypothesizes that OCI symptoms relate to the persistence of value representations (rigidity vs. volatility). It introduces a **memory decay** parameter for unchosen options. High OCI scores may modulate this decay rate, potentially leading to "stickier" habits (low decay) or an inability to sustain value representations (high decay).

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Value Decay Modulated by OCI.
    
    Hypothesis:
    OCI scores relate to the persistence of value representations.
    This model introduces a memory decay parameter for unchosen actions.
    High OCI might lead to lower decay (stickier habits) or modified forgetting rates.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate for value updates.
    beta: [0, 10] - Inverse temperature for softmax (shared across stages).
    w: [0, 1] - MB/MF mixing weight.
    stickiness: [-5, 5] - Choice perseveration bonus.
    decay_base: [0, 1] - Base decay rate for unchosen Q-values.
    decay_oci: [-1, 1] - Modulation of decay rate by OCI.
    """
    learning_rate, beta, w, stickiness, decay_base, decay_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate decay and clip to [0, 1]
    decay = decay_base + decay_oci * oci_score
    decay = np.clip(decay, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2) # MF values for stage 1 (Spaceships)
    q_stage2_mf = np.zeros((2, 2)) # MF values for stage 2 (Aliens)
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        # Data extraction
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Handling invalid trials (e.g. missing action_2)
        if a2 == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue
        
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        last_action_1 = a1
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning & Decay ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        # Update Chosen Values (assuming lambda=1 for stage 1 update)
        q_stage1_mf[a1] += learning_rate * (delta_stage1 + delta_stage2)
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Decay Unchosen Values
        # Stage 1 unchosen
        unchosen_a1 = 1 - a1
        q_stage1_mf[unchosen_a1] *= (1 - decay)
        
        # Stage 2 unchosen (in the current state)
        unchosen_a2 = 1 - a2
        q_stage2_mf[s_idx, unchosen_a2] *= (1 - decay)
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2: Stage-Specific Learning Rates (Stage 1 Modulated)
This model posits that OCI affects the plasticity of high-level choices (Stage 1, spaceship choice) differently than low-level associations (Stage 2, alien choice). It separates the learning rates for the two stages and allows OCI to modulate the **Stage 1 Learning Rate**. This tests if OCI-related rigidity is specific to the "planning" stage.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Stage-Specific Learning Rates with OCI Modulation on Stage 1.
    
    Hypothesis:
    OCI affects the plasticity of high-level choices (Stage 1) differently than low-level associations (Stage 2).
    A separate learning rate for Stage 1, modulated by OCI, captures this.
    
    Parameters:
    lr_stage2: [0, 1] - Learning rate for Stage 2 (Alien values).
    lr_stage1_base: [0, 1] - Base learning rate for Stage 1 (Spaceship values).
    lr_stage1_oci: [-1, 1] - Modulation of Stage 1 learning rate by OCI.
    beta: [0, 10] - Inverse temperature (shared).
    w: [0, 1] - MB/MF mixing weight.
    stickiness: [-5, 5] - Choice perseveration.
    """
    lr_stage2, lr_stage1_base, lr_stage1_oci, beta, w, stickiness = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate Stage 1 Learning Rate
    lr_stage1 = lr_stage1_base + lr_stage1_oci * oci_score
    lr_stage1 = np.clip(lr_stage1, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        if a2 == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue
            
        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        last_action_1 = a1
        
        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # Updates
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        # Stage 1 update uses lr_stage1
        q_stage1_mf[a1] += lr_stage1 * (delta_stage1 + delta_stage2)
        
        # Stage 2 update uses lr_stage2
        q_stage2_mf[s_idx, a2] += lr_stage2 * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3: OCI-Modulated MB/MF Weight with Separate Betas
This model assumes that OCI primarily impacts the balance between Model-Based and Model-Free control (`w`). However, unlike previous attempts, it includes **separate inverse temperature (beta) parameters for Stage 1 and Stage 2**. This structural separation allows the model to account for different levels of baseline noise in the two stages, isolating the OCI effect on the strategy parameter `w` more cleanly.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated MB/MF Weight with Separate Betas.
    
    Hypothesis:
    OCI primarily influences the balance between Goal-Directed (MB) and Habitual (MF) control (w).
    However, decision noise differs between the abstract spaceship choice (Stage 1) and the concrete alien choice (Stage 2).
    This model allows separate betas for each stage, with OCI modulating 'w'.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate.
    w_base: [0, 1] - Base mixing weight.
    w_oci: [-1, 1] - Modulation of w by OCI.
    stickiness: [-5, 5] - Choice perseveration.
    beta_stage1: [0, 10] - Inverse temperature for Stage 1.
    beta_stage2: [0, 10] - Inverse temperature for Stage 2.
    """
    learning_rate, w_base, w_oci, stickiness, beta_stage1, beta_stage2 = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate w
    w = w_base + w_oci * oci_score
    w = np.clip(w, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        if a2 == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # Stage 1
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta_stage1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        last_action_1 = a1
        
        # Stage 2
        exp_q2 = np.exp(beta_stage2 * q_stage2_mf[s_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # Updates
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        q_stage1_mf[a1] += learning_rate * (delta_stage1 + delta_stage2)
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```