Here are three new cognitive models exploring different mechanisms for how OCI scores might influence decision-making in this two-step task, specifically focusing on the balance between model-based and model-free control, learning rates, and exploration.

### Cognitive Model 1: Hybrid Learner with OCI-Modulated Model-Based Weight
This model hypothesizes that individuals with higher OCI scores rely more heavily on habit-based (Model-Free) learning and less on goal-directed (Model-Based) planning. The parameter `w` (mixing weight) determines the balance between MF and MB values, and this balance is linearly modulated by the OCI score.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free learner where the mixing weight is modulated by OCI.
    
    Hypothesis: Higher OCI scores lead to reduced Model-Based control (lower w),
    relying more on Model-Free habits.
    
    Bounds:
    learning_rate: [0,1]
    beta: [0,10]
    w_intercept: [0,1] (Base MB weight for OCI=0)
    w_slope: [-1,1] (How OCI affects MB weight)
    """
    learning_rate, beta, w_intercept, w_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Cast inputs to integers
    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    reward = reward.astype(int)

    # Fixed transition matrix (common transitions)
    # A (0) -> X (0) w/ 0.7, U (1) -> Y (1) w/ 0.7
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    # Initialize Q-values
    q_mf = np.zeros((2, 2))  # Stage 1 MF values (Action 1 -> Value) is usually just size 2, but here we treat it as Q(s1, a1)
                             # Actually, standard MF for stage 1 is just Q(a1). Let's stick to standard hybrid.
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State (planet) x Action (alien)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Calculate the mixing weight w based on OCI
    # We clip w to be between 0 and 1
    w = w_intercept + w_slope * oci_score
    w = np.clip(w, 0.0, 1.0)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based Value Calculation
        # V_MB(a1) = sum(P(s2|a1) * max_a2 Q_stage2(s2, a2))
        max_q_stage2 = np.max(q_stage2_mf, axis=1) # Max value for each planet
        q_stage1_mb = transition_matrix @ max_q_stage2

        # Hybrid Value
        q_hybrid = w * q_stage1_mb + (1 - w) * q_stage1_mf

        # Softmax Policy Stage 1
        logits_1 = beta * q_hybrid
        logits_1 = logits_1 - np.max(logits_1) # Numerical stability
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        p_choice_1[trial] = probs_1[action_1[trial]]

        # --- Stage 2 Choice ---
        state_idx = state[trial]
        
        # Standard MF for Stage 2
        logits_2 = beta * q_stage2_mf[state_idx]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning ---
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]

        # Update Stage 2 MF
        # Q2(s,a) = Q2(s,a) + alpha * (r - Q2(s,a))
        pe_2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * pe_2

        # Update Stage 1 MF (TD-0 or TD-1)
        # Often in hybrid models, Stage 1 MF is updated via TD(1) using the reward directly, 
        # or TD(0) using the value of the next state. Let's use TD(1) for simplicity in this task context.
        pe_1 = r - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * pe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2: Dual Learning Rates (Positive/Negative) Modulated by OCI
This model investigates if OCI affects how participants learn from positive versus negative feedback. It proposes that high OCI might be associated with a "safety bias" or hyper-sensitivity to punishment (or lack of reward), leading to different learning rates for prediction errors > 0 vs < 0.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner with separate learning rates for positive and negative prediction errors.
    The negative learning rate is scaled by the OCI score.
    
    Hypothesis: High OCI individuals are more sensitive to negative outcomes (missing gold),
    so their learning rate for negative PE is amplified by their OCI score.
    
    Bounds:
    alpha_pos: [0,1] (Learning rate for positive PE)
    alpha_neg_base: [0,1] (Base learning rate for negative PE)
    beta: [0,10]
    oci_sens: [0, 5] (Multiplier for OCI effect on negative learning)
    """
    alpha_pos, alpha_neg_base, beta, oci_sens = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    reward = reward.astype(int)

    # Calculate effective negative learning rate
    # We constrain it to be within [0, 1]
    alpha_neg = alpha_neg_base * (1 + oci_sens * oci_score)
    alpha_neg = np.clip(alpha_neg, 0.0, 1.0)

    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Stage 1 Policy
        logits_1 = beta * q_stage1
        logits_1 = logits_1 - np.max(logits_1)
        probs_1 = np.exp(logits_1) / np.sum(np.exp(logits_1))
        p_choice_1[t] = probs_1[action_1[t]]

        # Stage 2 Policy
        s_idx = state[t]
        logits_2 = beta * q_stage2[s_idx]
        logits_2 = logits_2 - np.max(logits_2)
        probs_2 = np.exp(logits_2) / np.sum(np.exp(logits_2))
        p_choice_2[t] = probs_2[action_2[t]]

        # Learning
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Stage 2 Update
        pe_2 = r - q_stage2[s_idx, a2]
        if pe_2 >= 0:
            q_stage2[s_idx, a2] += alpha_pos * pe_2
        else:
            q_stage2[s_idx, a2] += alpha_neg * pe_2

        # Stage 1 Update (using reward directly for pure MF)
        pe_1 = r - q_stage1[a1]
        if pe_1 >= 0:
            q_stage1[a1] += alpha_pos * pe_1
        else:
            q_stage1[a1] += alpha_neg * pe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3: OCI-Driven Inverse Temperature (Exploration/Exploitation)
This model posits that OCI affects the decision noise or exploration parameter (beta). Specifically, it tests if higher OCI leads to more rigid, deterministic behavior (high beta) or more anxious, erratic behavior (low beta) specifically in the second stage where the reward is immediate.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner where the inverse temperature (beta) is a function of OCI.
    
    Hypothesis: OCI score modulates the exploration-exploitation balance. 
    High OCI might lead to more rigid exploitation (higher beta).
    
    Bounds:
    learning_rate: [0,1]
    beta_base: [0,10] (Baseline inverse temp)
    beta_oci_coeff: [-5, 5] (How OCI changes beta)
    """
    learning_rate, beta_base, beta_oci_coeff = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    reward = reward.astype(int)

    # Calculate effective beta
    # Ensure beta stays non-negative
    beta_eff = beta_base + beta_oci_coeff * oci_score
    beta_eff = np.maximum(beta_eff, 0.0)

    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Stage 1 Policy
        logits_1 = beta_eff * q_stage1
        logits_1 = logits_1 - np.max(logits_1)
        probs_1 = np.exp(logits_1) / np.sum(np.exp(logits_1))
        p_choice_1[t] = probs_1[action_1[t]]

        # Stage 2 Policy
        s_idx = state[t]
        logits_2 = beta_eff * q_stage2[s_idx]
        logits_2 = logits_2 - np.max(logits_2)
        probs_2 = np.exp(logits_2) / np.sum(np.exp(logits_2))
        p_choice_2[t] = probs_2[action_2[t]]

        # Learning
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Stage 2 Update
        pe_2 = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += learning_rate * pe_2

        # Stage 1 Update
        pe_1 = r - q_stage1[a1]
        q_stage1[a1] += learning_rate * pe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```