Here are three new cognitive models that hypothesize different mechanisms for how Obsessive-Compulsive symptoms (OCI) influence decision-making, specifically focusing on the extreme perseveration observed in the participant data.

### Cognitive Model 1: Asymmetric Learning Rate (OCI-Dampened Negative Learning)
This model proposes that high OCI participants exhibit a "learning deficit" specifically for negative outcomes. They update their values normally when rewarded, but their learning rate for negative prediction errors (failures) is suppressed by their OCI score. This effectively prevents the value of a chosen option from decreasing despite repeated lack of reward, leading to "stuck" behavior.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Asymmetric Learning Rate Model with OCI-Dampened Negative Learning.
    
    This model posits that participants with high OCI scores have a reduced ability 
    to unlearn associations when faced with negative prediction errors (outcomes worse than expected).
    This leads to 'stuck' behavior where they persist in a choice despite lack of reward,
    as the value of the chosen option does not decrease effectively.
    
    Parameters:
    - lr_pos: [0, 1] Learning rate for positive prediction errors.
    - lr_neg_base: [0, 1] Baseline learning rate for negative prediction errors.
    - lr_neg_damp_oci: [0, 1] Factor by which OCI dampens the negative learning rate. 
                             lr_neg = lr_neg_base * (1 - oci * lr_neg_damp_oci).
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weighting between Model-Based (1) and Model-Free (0).
    - stick_weight: [0, 5] Weight of the choice stickiness trace.
    - stick_decay: [0, 1] Decay rate of the stickiness trace.
    """
    lr_pos, lr_neg_base, lr_neg_damp_oci, beta, w, stick_weight, stick_decay = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate OCI-modulated negative learning rate
    # We clip the dampening factor to ensure lr_neg doesn't go negative
    dampening = np.clip(current_oci * lr_neg_damp_oci, 0, 1)
    lr_neg = lr_neg_base * (1.0 - dampening)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    choice_trace = np.zeros(2)
    
    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # Model-Based Value
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add Stickiness
        logits = beta * q_net + stick_weight * choice_trace
        
        # Softmax Stage 1
        exp_q1 = np.exp(logits - np.max(logits))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # Update Stickiness Trace
        choice_trace *= stick_decay
        choice_trace[int(action_1[trial])] += 1.0
        
        # --- Stage 2 Decision ---
        state_idx = int(state[trial])
        logits_2 = beta * q_stage2_mf[state_idx, :]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Learning ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        lr_1 = lr_pos if delta_stage1 >= 0 else lr_neg
        q_stage1_mf[a1] += lr_1 * delta_stage1
        
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        lr_2 = lr_pos if delta_stage2 >= 0 else lr_neg
        q_stage2_mf[state_idx, a2] += lr_2 * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2: Outcome-Dependent Stickiness (Compulsive Habit)
This model differentiates between "Win-Stay" and "Lose-Stay" tendencies. It hypothesizes that high OCI drives **compulsive perseveration specifically after failure**. While normal stickiness might be driven by reward, this model allows the weight of the stickiness trace to be higher after a loss for high-OCI individuals, explaining why they repeat choices that yield zero coins.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Outcome-Dependent Stickiness Model.
    
    This model differentiates between 'winning stickiness' (staying after reward) and 
    'losing stickiness' (staying after no reward). OCI modulates the 'losing stickiness',
    hypothesizing that high OCI leads to compulsive repetition of choices even when they 
    fail to yield rewards (perseveration).
    
    Parameters:
    - learning_rate: [0, 1] Standard learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] MB/MF weight.
    - stick_weight_win: [0, 5] Weight of stickiness trace if previous trial was rewarded.
    - stick_weight_loss_base: [0, 5] Base weight of stickiness trace if previous trial was unrewarded.
    - stick_weight_loss_oci: [0, 5] OCI scaling for losing stickiness. 
                             stick_loss = base + oci * stick_weight_loss_oci.
    - stick_decay: [0, 1] Decay rate of stickiness trace.
    """
    learning_rate, beta, w, stick_weight_win, stick_weight_loss_base, stick_weight_loss_oci, stick_decay = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    stick_weight_loss = stick_weight_loss_base + stick_weight_loss_oci * current_oci
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    choice_trace = np.zeros(2)
    
    last_reward = 1.0 # Initialize as if previous was a win to avoid bias on trial 1
    
    for trial in range(n_trials):
        # Determine current stickiness weight based on previous outcome
        if last_reward > 0.5: # Win
            current_stick_weight = stick_weight_win
        else: # Loss
            current_stick_weight = stick_weight_loss
            
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits = beta * q_net + current_stick_weight * choice_trace
        
        exp_q1 = np.exp(logits - np.max(logits))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # Update Trace
        choice_trace *= stick_decay
        choice_trace[int(action_1[trial])] += 1.0
        
        # --- Stage 2 Decision ---
        state_idx = int(state[trial])
        logits_2 = beta * q_stage2_mf[state_idx, :]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Learning ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        last_reward = r

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3: OCI-Modulated Stickiness Decay (Persistent Memory)
Instead of modulating the *weight* of the habit (strength), this model modulates the *persistence* (memory) of the habit. High OCI leads to a slower decay of the choice trace. This means past choices exert influence for much longer, allowing the habit trace to accumulate to very high levels, explaining the extremely long streaks (e.g., >100 trials) seen in the data.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Stickiness Decay Model.
    
    Instead of modulating the weight of the habit, this model modulates the persistence (memory)
    of the habit. High OCI leads to a slower decay of the choice trace, meaning past choices
    exert influence for much longer, explaining long streaks of perseveration.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] MB/MF weight.
    - stick_weight: [0, 5] Weight of the stickiness trace.
    - decay_base: [0, 1] Baseline decay rate.
    - decay_oci_slope: [0, 1] OCI influence on decay. 
                      decay = decay_base + decay_oci_slope * oci.
    - lambda_eligibility: [0, 1] Eligibility trace for stage 1 update.
    """
    learning_rate, beta, w, stick_weight, decay_base, decay_oci_slope, lambda_eligibility = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate decay, clipped to [0, 0.99] to prevent explosion
    stick_decay = decay_base + decay_oci_slope * current_oci
    stick_decay = np.clip(stick_decay, 0.0, 0.99)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    choice_trace = np.zeros(2)
    
    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits = beta * q_net + stick_weight * choice_trace
        
        exp_q1 = np.exp(logits - np.max(logits))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # Update Trace
        choice_trace *= stick_decay
        choice_trace[int(action_1[trial])] += 1.0
        
        # --- Stage 2 Decision ---
        state_idx = int(state[trial])
        logits_2 = beta * q_stage2_mf[state_idx, :]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Learning ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 update with eligibility trace from Stage 2
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1 + learning_rate * lambda_eligibility * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```