Here are three new cognitive models based on the participant's data and OCI score.

### Analysis of Participant Behavior
The participant has a maximum OCI score of 1.0 (High Obsessive-Compulsive symptoms). Looking at the data:
1.  **Stage 1 Behavior:** The participant shows long runs of repeating the same spaceship choice (e.g., trials 3-15 are all spaceship 0, trials 24-28 are all spaceship 1). This suggests strong perseveration or habit formation, which aligns with compulsivity.
2.  **Reaction to Outcomes:** Even after receiving 0 coins multiple times (e.g., trials 12, 15, 27, 28), they often stick with the same spaceship. This suggests a resistance to negative feedback or a "stiffness" in updating values, potentially characteristic of high OCI.
3.  **Model-Based vs Model-Free:** High OCI is often associated with deficits in goal-directed (Model-Based) control and reliance on habitual (Model-Free) control.

Based on this, the models below explore:
1.  **Model 1:** A "Hybrid" model (MB/MF) where the balance `w` is shifted towards Model-Free by the OCI score.
2.  **Model 2:** A Model-Free model where the learning rate for *negative* prediction errors is dampened by the OCI score (ignoring failure).
3.  **Model 3:** A Model-Based model where the transition matrix learning is "rigid" or biased by OCI, assuming transitions are more deterministic than they are.

### Model 1: OCI-Modulated Model-Based/Model-Free Hybrid
This model tests the hypothesis that high OCI participants rely less on the Model-Based system (planning) and more on the Model-Free system (habit). The mixing parameter `w` (where 1 is fully MB and 0 is fully MF) is down-weighted by the OCI score.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based / Model-Free learner where OCI reduces Model-Based control.
    
    Hypothesis: High OCI scores correlate with reduced goal-directed (MB) planning 
    and increased habitual (MF) control. The weighting parameter `w` is 
    scaled down by OCI.
    
    Parameters:
    learning_rate: [0, 1] Learning rate for value updates.
    beta: [0, 10] Inverse temperature for softmax.
    w_max: [0, 1] Maximum weight for Model-Based system (at OCI=0).
    """
    learning_rate, beta, w_max = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective w. Higher OCI -> lower w (more MF).
    # If OCI=1, w is close to 0. If OCI=0, w is w_max.
    w = w_max * (1.0 - oci_score)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2) # Model-free values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Values for stage 2 (same for MB and MF)

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # Model-Based Value: Transition * Max(Stage 2 Values)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Decision ---
        s_curr = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[s_curr])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        r = reward[trial]
        
        # Stage 2 Update (TD)
        # Q2(s, a2) = Q2(s, a2) + lr * (r - Q2(s, a2))
        delta_stage2 = r - q_stage2_mf[s_curr, a2]
        q_stage2_mf[s_curr, a2] += learning_rate * delta_stage2
        
        # Stage 1 Update (TD)
        # Q1(a1) = Q1(a1) + lr * (Q2(s, a2) - Q1(a1))
        # Note: Using SARSA-style update common in 2-step tasks for MF
        delta_stage1 = q_stage2_mf[s_curr, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Asymmetric Learning Rate (Negativity Resistance)
The participant often repeats choices despite getting 0 coins. This model posits that high OCI individuals are resistant to "unlearning" habits. The learning rate for negative prediction errors (disappointment) is scaled down by the OCI score.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner with asymmetric learning rates modulated by OCI.
    
    Hypothesis: High OCI participants are rigid/compulsive and ignore negative feedback.
    The learning rate for negative prediction errors (alpha_neg) is reduced
    as OCI increases.
    
    Parameters:
    lr_pos: [0, 1] Learning rate for positive prediction errors.
    lr_neg_base: [0, 1] Base learning rate for negative prediction errors.
    beta: [0, 10] Inverse temperature.
    """
    lr_pos, lr_neg_base, beta = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective negative learning rate.
    # High OCI reduces ability to learn from failure.
    lr_neg = lr_neg_base * (1.0 - (0.8 * oci_score)) 
    # (0.8 factor ensures it doesn't go completely to 0, keeping some plasticity)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Decision ---
        s_curr = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2[s_curr])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        r = reward[trial]
        
        # Stage 2 Update
        pe_2 = r - q_stage2[s_curr, a2]
        alpha_2 = lr_pos if pe_2 >= 0 else lr_neg
        q_stage2[s_curr, a2] += alpha_2 * pe_2
        
        # Stage 1 Update
        pe_1 = q_stage2[s_curr, a2] - q_stage1[a1]
        alpha_1 = lr_pos if pe_1 >= 0 else lr_neg
        q_stage1[a1] += alpha_1 * pe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Driven Inverse Temperature (Exploration Suppression)
This model assumes the core deficit is not in the learning mechanism itself (MB vs MF), but in the action selection policy. High OCI is modeled as extremely low exploration (high `beta`), making behavior deterministic and "stuck". The OCI score amplifies the base beta parameter.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner where OCI amplifies the inverse temperature (beta).
    
    Hypothesis: High OCI relates to anxiety-driven rigidity and intolerance of uncertainty,
    leading to very low exploration (high exploitation). OCI acts as a multiplier
    on the base beta.
    
    Parameters:
    learning_rate: [0, 1] Learning rate.
    beta_base: [0, 10] Base inverse temperature.
    oci_amp: [0, 5] Amplification factor for OCI on beta.
    """
    learning_rate, beta_base, oci_amp = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Effective beta increases with OCI
    # beta = beta_base + (oci_amp * oci_score)
    beta = beta_base * (1.0 + oci_amp * oci_score)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # Normalize Q values to prevent overflow with high beta
        q1_norm = q_stage1 - np.max(q_stage1)
        exp_q1 = np.exp(beta * q1_norm)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Decision ---
        s_curr = int(state[trial])
        q2_norm = q_stage2[s_curr] - np.max(q_stage2[s_curr])
        exp_q2 = np.exp(beta * q2_norm)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        r = reward[trial]
        
        # Standard TD Learning
        pe_2 = r - q_stage2[s_curr, a2]
        q_stage2[s_curr, a2] += learning_rate * pe_2
        
        pe_1 = q_stage2[s_curr, a2] - q_stage1[a1]
        q_stage1[a1] += learning_rate * pe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```