To explain the decision-making behavior of the participant with high OCI symptoms, I propose three cognitive models. These models hypothesize that OCI affects reward sensitivity, habit formation (perseveration), or memory retention (decay).

### Model 1: Hybrid MB/MF with OCI-Modulated Loss Aversion
This model assumes that individuals with high OCI symptoms may perceive a lack of reward (0 coins) not just as a neutral outcome, but as a negative outcome (loss). This "loss aversion" is scaled by their OCI score. The model uses a hybrid Model-Based (MB) and Model-Free (MF) architecture.
- **Mechanism**: The effective reward signal $R_{eff}$ is equal to the observed reward if positive. If the reward is 0, $R_{eff} = -1 \times \text{loss\_sens} \times \text{OCI}$. This shifts the asymptotic value of actions leading to failure, potentially driving stronger avoidance.
- **Parameters**: `learning_rate`, `beta`, `w` (mixing weight), `loss_sens`.

### Model 2: Model-Free with OCI-Modulated Cumulative Stickiness
This model builds on the observation that the participant exhibits "sticky" behavior (repeating choices). It hypothesizes that this stickiness is not constant but "cumulative"â€”the more they repeat a choice, the harder it is to switch. The rate at which this habit strength builds up is modulated by the OCI score.
- **Mechanism**: A stickiness bonus is added to the Q-value of the previously chosen action. This bonus increases linearly with the number of consecutive times that action has been chosen: $Stick = \text{base} + \text{slope} \times \text{OCI} \times (\text{consecutive\_counts})$.
- **Parameters**: `learning_rate`, `beta`, `stick_base`, `stick_slope_oci`.

### Model 3: Hybrid MB/MF with OCI-Modulated Memory Decay
This model hypothesizes that high OCI scores might relate to altered memory retention for action values, specifically "forgetting" the value of options that are not currently being chosen. This could reflect a narrow focus or an inability to maintain representations of unvisited states.
- **Mechanism**: On each trial, the Q-values of the *unchosen* Stage 1 options decay towards zero: $Q_{unchosen} \leftarrow Q_{unchosen} \times (1 - \text{decay} \times \text{OCI})$.
- **Parameters**: `learning_rate`, `beta`, `w`, `decay_oci`.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model with OCI-modulated Loss Aversion.
    A reward of 0 is perceived as a negative loss, scaled by the OCI score.
    
    Parameters:
    learning_rate: [0,1] - Update rate for Q-values.
    beta: [0,10] - Inverse temperature for softmax.
    w: [0,1] - Weight for Model-Based values (0=MF, 1=MB).
    loss_sens: [0,5] - Sensitivity to '0' reward outcomes, scaled by OCI.
    """
    learning_rate, beta, w, loss_sens = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Transition matrix (fixed)
    # 0 -> [0.7, 0.3], 1 -> [0.3, 0.7]
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # state x action
    
    # Calculate effective punishment for 0 reward based on OCI
    loss_penalty = loss_sens * oci_score

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])
        
        # Handle missing data
        if a2 == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # --- Stage 1 Policy ---
        # Model-Based Value calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        # Determine effective reward
        # If reward is 1, use 1. If 0, use negative penalty.
        r_eff = r if r == 1 else -loss_penalty
        
        # Stage 2 Update (TD)
        delta_stage2 = r_eff - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += learning_rate * delta_stage2
        
        # Stage 1 Update (TD)
        # Note: In standard hybrid, MF Q1 updates towards Q2
        delta_stage1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Eligibility trace (lambda=1 implicitly for direct reinforcement in simple hybrid)
        # Or standard Q-learning: Q1 updates towards Q2, and Q2 updates towards R.
        # Often an additional eligibility trace connects R to Q1. 
        # Here we use standard one-step update for simplicity unless lambda is specified.
        # However, to propagate the "loss" effectively, we also add the stage 2 error to stage 1
        # which mimics lambda=1 behavior common in these tasks.
        q_stage1_mf[a1] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free RL with OCI-modulated Cumulative Stickiness.
    Stickiness bonus increases with consecutive choices of the same option.
    
    Parameters:
    learning_rate: [0,1] - Update rate.
    beta: [0,10] - Inverse temperature.
    stick_base: [0,5] - Base stickiness parameter.
    stick_slope_oci: [0,5] - Rate at which stickiness grows with repetition, scaled by OCI.
    """
    learning_rate, beta, stick_base, stick_slope_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action = -1
    consecutive_count = 0
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])
        
        if a2 == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            # Reset continuity on missing data to be safe, or keep it. 
            # We keep it but don't increment.
            continue

        # Calculate stickiness
        stickiness_bonus = 0.0
        if last_action != -1:
            # Base stickiness + cumulative component
            # Note: consecutive_count tracks how many times we've done this BEFORE current trial
            stickiness_bonus = stick_base + (stick_slope_oci * oci_score * consecutive_count)
        
        # --- Stage 1 Policy ---
        q_net = q_stage1_mf.copy()
        if last_action != -1:
            q_net[last_action] += stickiness_bonus
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Update consecutive count for next trial
        if a1 == last_action:
            consecutive_count += 1
        else:
            consecutive_count = 0 # Reset if switched
            # Note: The bonus applied *this* trial was based on history. 
            # Now we update history for *next* trial.
            
        last_action = a1
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        # Standard TD Updates
        delta_stage2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Eligibility trace (lambda=1)
        q_stage1_mf[a1] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model with OCI-modulated Memory Decay.
    Unchosen options in Stage 1 decay towards 0, with rate dependent on OCI.
    
    Parameters:
    learning_rate: [0,1] - Update rate.
    beta: [0,10] - Inverse temperature.
    w: [0,1] - Mixing weight (0=MF, 1=MB).
    decay_oci: [0,1] - Decay rate for unchosen options, scaled by OCI.
    """
    learning_rate, beta, w, decay_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Calculate effective decay rate
    decay_rate = decay_oci * oci_score
    # Clamp decay to [0, 1] to ensure stability
    if decay_rate > 1.0: decay_rate = 1.0
    if decay_rate < 0.0: decay_rate = 0.0

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])
        
        if a2 == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        # 1. Decay unchosen Stage 1 MF value
        unchosen_a1 = 1 - a1
        q_stage1_mf[unchosen_a1] *= (1.0 - decay_rate)
        
        # 2. Standard Updates
        delta_stage2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Eligibility trace (lambda=1)
        q_stage1_mf[a1] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```