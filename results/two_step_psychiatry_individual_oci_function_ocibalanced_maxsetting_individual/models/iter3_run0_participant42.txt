Here are the three cognitive models derived from the analysis of the participant's high OCI score and behavioral patterns.

### Model 1: OCI-Modulated Asymmetric Learning
This model tests the hypothesis that high OCI individuals exhibit **anxiety-driven learning asymmetries**. Specifically, it proposes that OCI scales the learning rate for negative prediction errors (punishments or reward omissions) more than positive ones, leading to rapid avoidance switching or hyper-sensitivity to failure.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Asymmetric Learning Model.
    
    This model posits that individuals with high OCI scores exhibit asymmetric learning 
    from prediction errors. Specifically, the learning rate for negative prediction errors 
    (RPE < 0) is scaled by OCI, reflecting increased sensitivity to punishment or 
    reward omission.
    
    Parameters:
    - learning_rate: [0,1] Base learning rate for positive prediction errors.
    - beta: [0,10] Inverse temperature (choice consistency).
    - w: [0,1] Model-based weight (fixed for this model).
    - asymmetry: [0,10] Scaling factor for negative learning rate.
                 lr_neg = lr_pos * (1 + asymmetry * oci).
    """
    learning_rate, beta, w, asymmetry = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate Asymmetric Learning Rates
    # lr_pos is the base rate. lr_neg is boosted by OCI.
    lr_pos = learning_rate
    lr_neg = learning_rate * (1.0 + asymmetry * oci_score)
    # Clamp lr_neg to 1.0 to ensure stability
    if lr_neg > 1.0:
        lr_neg = 1.0

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Stage 1 Update (TD(0))
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        # Use asymmetric learning rate based on sign of delta
        current_lr = lr_pos if delta_stage1 >= 0 else lr_neg
        q_stage1_mf[a1] += current_lr * delta_stage1
        
        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        current_lr = lr_pos if delta_stage2 >= 0 else lr_neg
        q_stage2_mf[state_idx, a2] += current_lr * delta_stage2
        
        # Direct reinforcement of Stage 1 choice (Eligibility Trace)
        q_stage1_mf[a1] += current_lr * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Dual Beta Rigidity
This model suggests that high OCI manifests as **rigidity in the planning phase** (Stage 1) specifically. It separates the inverse temperature (beta) for Stage 1 and Stage 2, hypothesizing that OCI increases `beta_1` (making spaceship choice more deterministic/rigid) while `beta_2` (alien choice) remains a standard bandit parameter.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Dual Beta Rigidity Model.
    
    This model separates decision noise (beta) for the planning stage (Stage 1) and 
    the bandit stage (Stage 2). High OCI is hypothesized to increase rigidity (higher beta)
    specifically in the first stage choice, reflecting compulsive adherence to a route.
    
    Parameters:
    - learning_rate: [0,1] Value update rate.
    - beta_1_base: [0,10] Base inverse temperature for Stage 1.
    - beta_1_stiffness: [0,10] OCI modulation for Stage 1 beta.
                         beta_1 = beta_1_base * (1 + stiffness * oci).
    - beta_2: [0,10] Inverse temperature for Stage 2 (independent of OCI).
    - w: [0,1] Model-based weight.
    """
    learning_rate, beta_1_base, beta_1_stiffness, beta_2, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate Stage 1 Beta modulated by OCI
    beta_1 = beta_1_base * (1.0 + beta_1_stiffness * oci_score)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy (Modulated Beta) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])

        # --- Stage 2 Policy (Fixed Beta) ---
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Direct reinforcement
        q_stage1_mf[a1] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Linked Habit and Stickiness
This model integrates two common phenotypes of compulsion: **habit reliance** and **perseveration**. It links the OCI score to *both* the Model-Based weight `w` (decreasing it) and a choice Stickiness parameter (increasing it). This assumes that the pathology drives a coupled shift towards repetitive, model-free behavior.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Linked Habit and Stickiness Model.
    
    This model links high OCI scores to both an increase in habitual control (lower Model-Based weight)
    AND an increase in choice perseveration (stickiness). This captures the dual nature of 
    compulsivity: reliance on habit and repetitive behavior.
    
    Parameters:
    - learning_rate: [0,1]
    - beta: [0,10]
    - w_max: [0,1] Maximum MB weight. Effective w decreases with OCI.
             w = w_max * (1 - oci).
    - stick_base: [0,5] Base stickiness.
    - stick_oci: [0,5] OCI-dependent stickiness increase.
                 stickiness = stick_base + stick_oci * oci.
    """
    learning_rate, beta, w_max, stick_base, stick_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # OCI simultaneously reduces MB weight and increases Stickiness
    w = w_max * (1.0 - oci_score)
    stickiness = stick_base + stick_oci * oci_score
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_a1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Policy with Stickiness ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Calculate logits and add stickiness bonus to previous choice
        logits = beta * q_net
        if prev_a1 != -1:
            logits[prev_a1] += stickiness
            
        exp_q1 = np.exp(logits)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        prev_a1 = a1

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Direct reinforcement
        q_stage1_mf[a1] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```