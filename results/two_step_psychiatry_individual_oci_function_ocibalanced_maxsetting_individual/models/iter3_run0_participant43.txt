Here are the three proposed cognitive models.

### Model 1: Hybrid Learner with OCI-Modulated Model-Based Suppression
This model hypothesizes that high OCI scores are associated with a functional deficit in Model-Based (goal-directed) control. Instead of a fixed mixing weight $w$, the effective weight given to the Model-Based system is suppressed as OCI increases, leading to a dominance of Model-Free (habitual) control. This explains the participant's tendency to repeat actions (habits) even when the environment structure suggests otherwise.

```python
def cognitive_model1_oci_mb_suppression(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: Hybrid Learner with OCI-Modulated Model-Based Suppression.
    
    Hypothesis: High OCI scores suppress the contribution of the Model-Based (MB) system,
    leading to a dominance of Model-Free (MF) control.
    
    w_effective = w_base / (1 + suppression_k * OCI)
    
    Parameters:
    - learning_rate: [0, 1] Update rate for MF values.
    - beta: [0, 10] Inverse temperature (softness of choice).
    - w_base: [0, 1] Baseline weight of Model-Based control (when OCI is 0).
    - suppression_k: [0, 10] Scaling factor for OCI-driven suppression of MB control.
    """
    learning_rate, beta, w_base, suppression_k = model_parameters
    oci_score = oci[0]
    n_trials = len(action_1)
    
    # Calculate effective MB weight based on OCI
    w_effective = w_base / (1.0 + suppression_k * oci_score)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based Value Calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2

        # Hybrid Value
        q_net = w_effective * q_stage1_mb + (1 - w_effective) * q_stage1_mf

        # Softmax Choice 1
        logits_1 = beta * q_net
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        # --- Stage 2 Policy ---
        state_idx = state[trial]
        logits_2 = beta * q_stage2_mf[state_idx]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        # Stage 1 Update (SARSA-style)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1

        # Stage 2 Update (Reward Prediction Error)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Asymmetric Learning (Punishment Insensitivity)
This model hypothesizes that high OCI scores are associated with a resistance to "unlearning" habits when outcomes are negative (0 reward). The learning rate for negative prediction errors is scaled down by OCI, causing the participant to persist with a choice despite a lack of reward. This fits the data pattern where the participant sticks to a spaceship even during unrewarded streaks.

```python
def cognitive_model2_oci_asymmetric_learning(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Modulated Asymmetric Learning (Punishment Insensitivity).
    
    Hypothesis: High OCI participants have a reduced learning rate for negative prediction errors
    (punishment/omission of reward), leading to persistence/compulsivity.
    
    alpha_neg = alpha_pos / (1 + resistance * OCI)
    
    Parameters:
    - learning_rate_pos: [0, 1] Learning rate for positive prediction errors (Reward > Expectation).
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weight of Model-Based control.
    - resistance: [0, 10] OCI-dependent resistance to updating from negative outcomes.
    """
    learning_rate_pos, beta, w, resistance = model_parameters
    oci_score = oci[0]
    n_trials = len(action_1)
    
    # Define negative learning rate based on OCI
    learning_rate_neg = learning_rate_pos / (1.0 + resistance * oci_score)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf

        logits_1 = beta * q_net
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        # --- Stage 2 Policy ---
        state_idx = state[trial]
        logits_2 = beta * q_stage2_mf[state_idx]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        # Stage 1
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        lr_1 = learning_rate_pos if delta_stage1 >= 0 else learning_rate_neg
        q_stage1_mf[action_1[trial]] += lr_1 * delta_stage1

        # Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        lr_2 = learning_rate_pos if delta_stage2 >= 0 else learning_rate_neg
        q_stage2_mf[state_idx, action_2[trial]] += lr_2 * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Choice Kernel (Cumulative Habit)
This model extends the concept of perseveration by using a "Choice Kernel" that tracks the history of past choices (exponentially weighted). High OCI scores amplify the influence of this choice history on current decisions, leading to strong habit formation that builds up over time and is resistant to change.

```python
def cognitive_model3_oci_choice_kernel(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Choice Kernel (Cumulative Habit).
    
    Hypothesis: OCI modulates the strength of a 'Choice Kernel' (CK), which tracks 
    the frequency/history of past choices. High OCI leads to a stronger bias 
    to repeat frequently chosen actions (compulsion).
    
    Logits = beta * Q_net + ck_weight * OCI * CK
    
    Parameters:
    - learning_rate: [0, 1] Update rate for MF values.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weight of Model-Based control.
    - ck_lr: [0, 1] Learning rate (decay rate) of the choice kernel.
    - ck_weight: [0, 10] Weight of the choice kernel bias, scaled by OCI.
    """
    learning_rate, beta, w, ck_lr, ck_weight = model_parameters
    oci_score = oci[0]
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    choice_kernel = np.zeros(2) # Tracks choice history for Stage 1
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf

        logits_1 = beta * q_net
        # Add OCI-scaled Choice Kernel bias
        logits_1 += ck_weight * oci_score * choice_kernel

        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        # --- Choice Kernel Update ---
        # CK accumulates for chosen, decays for unchosen
        # CK[a] <- CK[a] + lr * (1 - CK[a]) if chosen
        # CK[a] <- CK[a] + lr * (0 - CK[a]) if not chosen
        chosen = action_1[trial]
        choice_kernel[chosen] += ck_lr * (1.0 - choice_kernel[chosen])
        choice_kernel[1 - chosen] += ck_lr * (0.0 - choice_kernel[1 - chosen])

        # --- Stage 2 Policy ---
        state_idx = state[trial]
        logits_2 = beta * q_stage2_mf[state_idx]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1

        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```