Here are three cognitive models formulated as Python functions, designed to capture the specific behavioral patterns of a high-OCI participant (score 0.7) in the two-step task.

### Model 1: OCI-Mediated Reward Magnification
**Hypothesis:** High OCI scores are often associated with an exaggerated sense of relief or "rightness" upon successful task completion. This model posits that the participant subjectively values the reward (gold coin) higher than its objective value of 1.0. The `reward_magnification` parameter scales with OCI, making the effective reward $>1.0$. This leads to higher Q-values for rewarded actions and consequently more deterministic (stiffer) selection of those actions via the softmax function, mimicking the "stickiness" seen in the data without a separate perseveration parameter.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Mediated Reward Magnification Model.
    
    Hypothesis: High OCI participants experience exaggerated subjective value 
    from rewards (relief/completeness), effectively scaling the reward signal 
    based on their OCI score. This drives stronger reinforcement and 
    more rigid adherence to rewarded paths.

    Parameters:
    - learning_rate: Base learning rate [0, 1]
    - beta: Inverse temperature for softmax [0, 10]
    - w: Mixing weight (0=MF, 1=MB) [0, 1]
    - lambd: Eligibility trace parameter [0, 1]
    - reward_mag: Magnification factor for reward perception [0, 5]
    """
    learning_rate, beta, w, lambd, reward_mag = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # Policy for choice 1
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        state_idx = int(state[trial])

        # Policy for choice 2
        qs2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # Calculate Effective Reward (Subjective Magnification)
        # If OCI is high, the subjective value of the coin is > 1.0
        effective_reward = reward[trial] * (1.0 + (reward_mag * oci_score))

        # Updates
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1

        delta_stage2 = effective_reward - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2

        # Eligibility trace update for Stage 1
        q_stage1_mf[int(action_1[trial])] += learning_rate * lambd * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Obsessive Value Retention (Inverse Decay)
**Hypothesis:** Standard reinforcement learning assumes values persist until updated. However, "forgetting" or value decay is a natural component of flexible behavior. This model hypothesizes that high OCI is characterized by *obsessive maintenance* of value representationsâ€”an inability to "let go" or forget unchosen options. Low OCI individuals have a natural decay rate for unchosen actions (flexibility), while high OCI individuals (like this participant) exhibit reduced decay (rigidity), keeping old values active longer.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Obsessive Value Retention Model (Inverse Decay).
    
    Hypothesis: Flexibility requires the passive decay of unchosen option values.
    High OCI participants exhibit 'obsessive retention', where the decay rate 
    of unchosen options is suppressed by their OCI score, causing them to 
    hold onto outdated value estimates rigidly.

    Parameters:
    - learning_rate: Base learning rate [0, 1]
    - beta: Inverse temperature [0, 10]
    - w: Mixing weight [0, 1]
    - lambd: Eligibility trace [0, 1]
    - decay_base: Base decay rate for unchosen options [0, 1]
    """
    learning_rate, beta, w, lambd, decay_base = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    # Calculate effective decay rate: High OCI -> Low Decay (High Retention)
    effective_decay = decay_base * (1.0 - oci_score)

    for trial in range(n_trials):

        # Policy 1
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        state_idx = int(state[trial])

        # Policy 2
        qs2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # Standard Updates
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1

        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2
        q_stage1_mf[int(action_1[trial])] += learning_rate * lambd * delta_stage2

        # Decay Mechanism for Unchosen Stage 1 Option
        # The unchosen option fades towards 0.0 unless OCI prevents it.
        unchosen_1 = 1 - int(action_1[trial])
        q_stage1_mf[unchosen_1] *= (1.0 - effective_decay)
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Proximal Learning Hypersensitivity
**Hypothesis:** This model suggests that high OCI individuals disproportionately focus on immediate, proximal cause-effect relationships (Stage 2: Alien $\to$ Coin) over distal ones (Stage 1: Spaceship $\to$ Planet). This is modeled by splitting the learning rate: the Stage 2 learning rate is boosted by the OCI score, while Stage 1 remains at the base rate. This explains strong reactions to the final outcome while potentially maintaining a disconnect from the transition structure.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Proximal Learning Hypersensitivity Model.
    
    Hypothesis: High OCI participants show a learning bias towards proximal 
    outcomes (Stage 2) compared to distal choices (Stage 1). They update 
    values for the Aliens (immediate reward source) much more aggressively 
    than for the Spaceships.

    Parameters:
    - learning_rate: Base learning rate for Stage 1 [0, 1]
    - beta: Inverse temperature [0, 10]
    - w: Mixing weight [0, 1]
    - lambd: Eligibility trace [0, 1]
    - prox_gain: OCI-based multiplier for Stage 2 learning rate [0, 5]
    """
    learning_rate, beta, w, lambd, prox_gain = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    # Calculate Stage 2 specific learning rate
    # High OCI -> Higher learning rate for the immediate outcome
    lr_stage2 = learning_rate * (1.0 + (proximal_gain_scale := prox_gain) * oci_score)
    # Clamp to max 1.0 to maintain stability
    if lr_stage2 > 1.0: lr_stage2 = 1.0

    for trial in range(n_trials):

        # Policy 1
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        state_idx = int(state[trial])

        # Policy 2
        qs2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # Updates
        # Stage 1 uses base learning rate
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1

        # Stage 2 uses OCI-boosted learning rate (Proximal Focus)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += lr_stage2 * delta_stage2

        # Eligibility trace uses base learning rate (distal connection)
        q_stage1_mf[int(action_1[trial])] += learning_rate * lambd * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```