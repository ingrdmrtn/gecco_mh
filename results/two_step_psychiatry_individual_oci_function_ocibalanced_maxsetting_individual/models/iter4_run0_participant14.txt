Here are three new cognitive models exploring different mechanisms for how OCI scores might modulate decision-making in this task.

### Model 1: Hybrid Learner with OCI-modulated Weighting (w)
This model hypothesizes that the balance between Model-Based (MB) and Model-Free (MF) control is shifted by the participant's OCI score. Specifically, we test if higher OCI scores lead to a stronger reliance on habit (Model-Free) or planning (Model-Based). Given the participant's low OCI (0.3), this model allows the data to determine if `w` (the weight of MB) is a simple linear function of OCI.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free learner where the weighting parameter 'w'
    is a linear function of the OCI score.
    
    The balance between planning (MB) and habit (MF) is determined by:
    w = w_base + w_oci_slope * oci
    Sigmoid is applied to keep w in [0, 1].

    Parameters:
    learning_rate: [0, 1] Learning rate for value updates.
    beta: [0, 10] Inverse temperature for softmax choice.
    w_base: [-5, 5] Base logit for model-based weighting.
    w_oci_slope: [-5, 5] How OCI score changes the MB weighting.
    """
    learning_rate, beta, w_base, w_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate w based on OCI
    w_logit = w_base + w_oci_slope * oci_score
    w = 1.0 / (1.0 + np.exp(-w_logit))
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Stage 1 Policy ---
        # Model-Based Value: Transition * Max(Stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        # Stage 2 update (TD error)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 update (TD error using Stage 2 value)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    valid_trials = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```

### Model 2: Pure Model-Based with OCI-modulated Forgetting
This model posits that while the participant uses a Model-Based strategy (planning based on transitions), their ability to maintain accurate value estimates of the second-stage aliens decays over time. We hypothesize that OCI might relate to memory retention or "stickiness" of negative outcomes, but here we model it as a decay rate. Higher OCI might correlate with either rigid maintenance (low decay) or anxiety-driven volatility (high decay).

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Pure Model-Based learner with a decay parameter on unchosen options.
    The decay rate is modulated by OCI.
    
    decay = decay_base * (1 + decay_oci_factor * oci)
    Decay is clamped between 0 and 1.
    
    Parameters:
    learning_rate: [0, 1] Learning rate for chosen options.
    beta: [0, 10] Inverse temperature.
    decay_base: [0, 1] Base decay rate for unchosen options (0 = no forgetting, 1 = instant reset).
    decay_oci_factor: [-2, 2] How OCI modulates the decay rate.
    """
    learning_rate, beta, decay_base, decay_oci_factor = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective decay
    decay_raw = decay_base * (1.0 + decay_oci_factor * oci_score)
    decay = np.clip(decay_raw, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values for the 4 aliens (2 per planet)
    q_stage2 = np.zeros((2, 2)) + 0.5 # Initialize at 0.5
    
    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Stage 1 Policy (Model-Based) ---
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        # Update chosen alien
        q_stage2[s_idx, a2] += learning_rate * (r - q_stage2[s_idx, a2])
        
        # Decay unchosen aliens towards 0.5 (uncertainty/forgetting)
        # We iterate over all aliens, if not chosen, decay
        for s in range(2):
            for a in range(2):
                if not (s == s_idx and a == a2):
                    q_stage2[s, a] += decay * (0.5 - q_stage2[s, a])

    eps = 1e-10
    valid_trials = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```

### Model 3: Model-Free with OCI-modulated Eligibility Trace (Lambda)
This model assumes a standard Model-Free TD(lambda) learning algorithm, but tests if the "eligibility trace" (lambda)—how much the Stage 1 choice is credited for the Stage 2 outcome—is dependent on OCI. A higher lambda implies treating the two steps as a single event sequence, while a lower lambda treats them more independently.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free TD(lambda) learner where the eligibility trace parameter (lambda)
    is modulated by the OCI score.
    
    lambda = lambda_base + lambda_oci_slope * oci
    Lambda is clamped between 0 and 1.
    
    Parameters:
    learning_rate: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    lambda_base: [0, 1] Base eligibility trace.
    lambda_oci_slope: [-2, 2] Slope of OCI effect on lambda.
    """
    learning_rate, beta, lambda_base, lambda_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective lambda
    lam_raw = lambda_base + lambda_oci_slope * oci_score
    lam = np.clip(lam_raw, 0.0, 1.0)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Stage 1 Policy ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning with Eligibility Trace ---
        # Stage 1 prediction error (TD error based on Stage 2 Q-value)
        delta1 = q_stage2[s_idx, a2] - q_stage1[a1]
        q_stage1[a1] += learning_rate * delta1
        
        # Stage 2 prediction error (Reward based)
        delta2 = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += learning_rate * delta2
        
        # Eligibility trace update: Stage 1 value is also updated by Stage 2 error
        # scaled by lambda
        q_stage1[a1] += learning_rate * lam * delta2

    eps = 1e-10
    valid_trials = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```