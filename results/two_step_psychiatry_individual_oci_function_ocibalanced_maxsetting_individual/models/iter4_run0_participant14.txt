Here are three cognitive models represented as Python functions. They all incorporate the OCI score to modulate specific mechanisms within the decision-making process, based on the hypothesis that obsessive-compulsive traits influence reinforcement learning and perseveration dynamics.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Loss Sensitivity Model.
    
    Hypothesis: Participants with higher OCI scores may exhibit 'loss aversion' or 
    'fear of failure', perceiving the absence of reward (0 coins) as a punishment 
    rather than a neutral outcome. This model scales the negative value of a 
    non-reward outcome by the OCI score.
    
    Reward Function:
    R_effective = 1.0 if reward == 1
    R_effective = -(loss_sensitivity * OCI) if reward == 0
    
    Parameters:
    - learning_rate: [0, 1] Update rate for Q-values.
    - beta: [0, 10] Inverse temperature for softmax choice.
    - w: [0, 1] Weighting parameter (0 = pure Model-Free, 1 = pure Model-Based).
    - loss_sensitivity: [0, 10] Scaling factor that converts OCI into a penalty for non-rewards.
    """
    learning_rate, beta, w, loss_sensitivity = model_parameters
    oci_val = oci[0]
    n_trials = len(action_1)
    
    # Transition matrix (fixed structure: A->X (0.7), U->Y (0.7))
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values initialization
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State (Planet) x Action (Alien)
    
    for trial in range(n_trials):
        # Cast inputs to integers for indexing
        a1 = int(action_1[trial])
        if a1 == -1: 
            continue # Skip missing trials
            
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r_raw = reward[trial]
        
        # Effective reward transformation
        if r_raw == 1.0:
            r_eff = 1.0
        else:
            # 0 reward is treated as a penalty proportional to OCI
            r_eff = -(loss_sensitivity * oci_val)
            
        # --- Stage 1 Policy (Hybrid) ---
        # Model-Based Value: Transition * Max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax Choice 1
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy (Model-Free) ---
        exp_q2 = np.exp(beta * q_stage2_mf[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        # Stage 1 MF update (TD-like)
        delta_1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_1
        
        # Stage 2 update
        delta_2 = r_eff - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += learning_rate * delta_2
        
        # Propagate Stage 2 RPE to Stage 1 (Eligibility Trace)
        q_stage1_mf[a1] += learning_rate * delta_2
        
    eps = 1e-10
    mask = (action_1 != -1)
    # Negative Log Likelihood
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Goal Precision Model.
    
    Hypothesis: OCI modulates the balance of exploration/exploitation differently 
    between the planning phase (Stage 1) and the harvesting phase (Stage 2). 
    Higher OCI scores are associated with increased rigidity (higher beta) 
    in the second stage relative to the first.
    
    beta_stage2 = beta_stage1 * (1 + oci_beta_scale * OCI)
    
    Parameters:
    - learning_rate: [0, 1]
    - beta_1: [0, 10] Inverse temperature for Stage 1 choices.
    - w: [0, 1] Weight for Model-Based values.
    - oci_beta_scale: [0, 10] Factor scaling the increase of Beta for Stage 2 based on OCI.
    """
    learning_rate, beta_1, w, oci_beta_scale = model_parameters
    oci_val = oci[0]
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Calculate Stage 2 Beta based on OCI
    beta_2 = beta_1 * (1.0 + oci_beta_scale * oci_val)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        if a1 == -1: continue
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Stage 1 Choice (Using Beta 1) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Choice (Using Beta 2) ---
        exp_q2 = np.exp(beta_2 * q_stage2_mf[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        delta_1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_1
        
        delta_2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += learning_rate * delta_2
        
        q_stage1_mf[a1] += learning_rate * delta_2
        
    eps = 1e-10
    mask = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Stickiness Retention Model.
    
    Hypothesis: OCI modulates the temporal dynamics of perseveration. 
    Rather than just a static stickiness bonus, this model assumes choice 'traces' 
    decay over time. Higher OCI leads to higher retention (slower decay) of these traces, 
    causing a cumulative build-up of compulsion to repeat actions.
    
    retention_rate = retention_sensitivity * OCI (clipped at 0.95)
    choice_trace[t] = choice_trace[t-1] * retention_rate + I(action)
    
    Parameters:
    - learning_rate: [0, 1]
    - beta: [0, 10]
    - w: [0, 1]
    - stick_weight: [0, 5] The magnitude of the bonus added to Q-values per unit of trace.
    - retention_sensitivity: [0, 5] Controls how strongly OCI increases the memory retention of past choices.
    """
    learning_rate, beta, w, stick_weight, retention_sensitivity = model_parameters
    oci_val = oci[0]
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Calculate retention rate bounded [0, 0.95] to prevent explosion
    retention = retention_sensitivity * oci_val
    if retention > 0.95: retention = 0.95
    if retention < 0.0: retention = 0.0
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Trace of previous choices
    choice_trace = np.zeros(2)
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        if a1 == -1: continue
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add cumulative stickiness bonus
        q_net_stick = q_net + stick_weight * choice_trace
        
        exp_q1 = np.exp(beta * q_net_stick)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        delta_1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_1
        
        delta_2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += learning_rate * delta_2
        
        q_stage1_mf[a1] += learning_rate * delta_2
        
        # --- Update Choice Trace ---
        # Decay the existing trace based on OCI-modulated retention
        choice_trace *= retention
        # Increment the trace for the chosen action
        choice_trace[a1] += 1.0
        
    eps = 1e-10
    mask = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss
```