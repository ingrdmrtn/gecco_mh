Here are the three proposed cognitive models.

### Model 1: Outcome-Dependent Stickiness Model
This model hypothesizes that OCI symptoms specifically modulate the tendency to repeat a choice after an unrewarded outcome ("compulsive" repetition or "chasing"), distinct from the tendency to repeat after a reward.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Outcome-Dependent Stickiness Model.
    
    Hypothesis: OCI modulates stickiness (perseveration) specifically after unrewarded trials.
    High OCI might lead to 'chasing' or compulsive repetition after failure (Lose-Stay),
    distinct from sticking after success (Win-Stay).
    
    Parameters:
    - lr: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] MB/MF weighting (0=MF, 1=MB).
    - stick_rew: [0, 5] Stickiness bonus applied after a rewarded trial.
    - stick_unrew_base: [0, 5] Base stickiness bonus applied after an unrewarded trial.
    - stick_unrew_oci: [-5, 5] OCI modulation of unrewarded stickiness.
      stick_unrew = stick_unrew_base + stick_unrew_oci * oci
    """
    lr, beta, w, stick_rew, stick_unrew_base, stick_unrew_oci = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate OCI-modulated stickiness for unrewarded trials
    stick_unrew = stick_unrew_base + stick_unrew_oci * current_oci
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    
    last_action_1 = -1
    last_reward = 0
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Handle missing data
        if a1 < 0 or s_idx < 0 or a2 < 0:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue
            
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Apply outcome-dependent stickiness
        q_augmented = q_net.copy()
        if last_action_1 != -1:
            if last_reward > 0:
                q_augmented[last_action_1] += stick_rew
            else:
                q_augmented[last_action_1] += stick_unrew
                
        exp_q1 = np.exp(beta * q_augmented)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        q_stage2_mf[s_idx, a2] += lr * delta_stage2
        q_stage1_mf[a1] += lr * (delta_stage1 + delta_stage2)
        
        last_action_1 = a1
        last_reward = r
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Stage-Specific Exploration Model
This model hypothesizes that OCI affects decision noise (beta) differently in the second stage (the bandit task) compared to the first stage. High OCI might be associated with increased rigidity or uncertainty specifically when selecting the final outcome.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Stage-Specific Exploration Model.
    
    Hypothesis: OCI affects the exploration-exploitation trade-off (beta) differently 
    in the second stage (bandit task) compared to the first stage.
    High OCI may be associated with altered decision noise specifically 
    when selecting the final outcome.
    
    Parameters:
    - lr: [0, 1] Learning rate.
    - beta_1: [0, 10] Inverse temperature for Stage 1 choice.
    - beta_2_base: [0, 10] Base inverse temperature for Stage 2 choice.
    - beta_2_oci: [-5, 5] OCI modulation of Stage 2 beta.
        beta_2 = beta_2_base + beta_2_oci * oci
    - w: [0, 1] MB/MF weighting.
    - stickiness: [0, 5] Stickiness bonus for Stage 1.
    """
    lr, beta_1, beta_2_base, beta_2_oci, w, stickiness = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate Stage 2 beta
    beta_2 = beta_2_base + beta_2_oci * current_oci
    beta_2 = np.clip(beta_2, 0.0, 20.0) # Ensure non-negative, allow reasonable range
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        if a1 < 0 or s_idx < 0 or a2 < 0:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue
            
        # --- Stage 1 Choice (Uses beta_1) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        q_augmented = q_net.copy()
        if last_action_1 != -1:
            q_augmented[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta_1 * q_augmented)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Choice (Uses beta_2) ---
        exp_q2 = np.exp(beta_2 * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        q_stage2_mf[s_idx, a2] += lr * delta_stage2
        q_stage1_mf[a1] += lr * (delta_stage1 + delta_stage2)
        
        last_action_1 = a1
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Stage 2 Stickiness Model
This model introduces stickiness for the second stage choice (the alien) and hypothesizes that OCI modulates this specific form of motor or stimulus perseveration (e.g., repeatedly choosing the "left" alien regardless of the planet).

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Stage 2 Stickiness Model.
    
    Hypothesis: OCI modulates perseveration (habit) in the second stage choice.
    While Stage 1 stickiness captures spaceship repetition, Stage 2 stickiness captures 
    repetitive selection of the alien (e.g., left vs right) regardless of context,
    which may be heightened in high OCI.
    
    Parameters:
    - lr: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] MB/MF weighting.
    - stick_1: [0, 5] Stickiness for Stage 1 (Spaceship).
    - stick_2_base: [0, 5] Base stickiness for Stage 2 (Alien).
    - stick_2_oci: [-5, 5] OCI modulation of Stage 2 stickiness.
        stick_2 = stick_2_base + stick_2_oci * oci
    """
    lr, beta, w, stick_1, stick_2_base, stick_2_oci = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate Stage 2 stickiness
    stick_2 = stick_2_base + stick_2_oci * current_oci
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    last_action_2 = -1
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        if a1 < 0 or s_idx < 0 or a2 < 0:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue
            
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        q_aug_1 = q_net.copy()
        if last_action_1 != -1:
            q_aug_1[last_action_1] += stick_1
            
        exp_q1 = np.exp(beta * q_aug_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Choice ---
        q_stage2_curr = q_stage2_mf[s_idx].copy()
        
        # Apply Stage 2 stickiness (Global motor stickiness: repeats a2 regardless of state)
        if last_action_2 != -1:
            q_stage2_curr[last_action_2] += stick_2
            
        exp_q2 = np.exp(beta * q_stage2_curr)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        q_stage2_mf[s_idx, a2] += lr * delta_stage2
        q_stage1_mf[a1] += lr * (delta_stage1 + delta_stage2)
        
        last_action_1 = a1
        last_action_2 = a2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```