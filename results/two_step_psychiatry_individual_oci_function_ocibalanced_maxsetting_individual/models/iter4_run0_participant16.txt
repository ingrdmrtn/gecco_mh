Here are three new cognitive models that explore different mechanisms for how obsessive-compulsive symptoms (OCI) might influence decision-making in this two-step task. These models focus on alterations in model-based control, punishment sensitivity, and habit formation.

### Model 1: Hybrid Learner with OCI-Modulated Mixing Weight (w)
This model assumes that the balance between Model-Based (planning) and Model-Free (habitual) systems is disrupted by OCI symptoms. Specifically, higher OCI scores might lead to a reliance on habitual (Model-Free) control or a deficit in goal-directed (Model-Based) planning. The parameter `w` controls this trade-off, and here it is a linear function of the OCI score.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free learner where the mixing weight 'w' is modulated by OCI.
    w = 1 implies pure Model-Based, w = 0 implies pure Model-Free.
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    w_intercept: [0, 1]
    w_slope: [-1, 1]
    """
    learning_rate, beta, w_intercept, w_slope = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    # Calculate mixing weight w based on OCI, clamped between 0 and 1
    w = w_intercept + (w_slope * current_oci)
    w = np.clip(w, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2) # Model-Free Q-values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Model-Free Q-values for stage 2 (also used for MB calculation)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based Value: V_MB(s1) = T * max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value: Q_net = w * Q_MB + (1-w) * Q_MF
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        if action_1[trial] != -1:
            a1 = int(action_1[trial])
            p_choice_1[trial] = probs_1[a1]
        else:
            p_choice_1[trial] = 1.0
            continue # Skip update if data missing

        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        # Stage 2 is purely model-free in this standard formulation
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        if action_2[trial] != -1:
            a2 = int(action_2[trial])
            p_choice_2[trial] = probs_2[a2]
            r = reward[trial]

            # --- Updates ---
            # 1. Stage 2 TD Error (Model-Free)
            delta_stage2 = r - q_stage2_mf[state_idx, a2]
            q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
            
            # 2. Stage 1 TD Error (Model-Free)
            # SARSA-style update: Q(s1, a1) <-- Q(s1, a1) + lr * (Q(s2, a2) - Q(s1, a1))
            delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
            q_stage1_mf[a1] += learning_rate * delta_stage1
            
        else:
            p_choice_2[trial] = 1.0

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Asymmetric Learning Rates Modulated by OCI
This model hypothesizes that individuals with higher OCI scores process positive and negative feedback differently. Obsessive-compulsive traits are often associated with harm avoidance or altered error processing. Here, we define separate learning rates for positive prediction errors (better than expected) and negative prediction errors (worse than expected). The OCI score specifically modulates the learning rate for *negative* prediction errors (`alpha_neg`), potentially leading to "over-learning" from failure or missed rewards.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner with asymmetric learning rates for positive and negative prediction errors.
    The negative learning rate is modulated by OCI.
    
    Bounds:
    alpha_pos: [0, 1]
    alpha_neg_base: [0, 1]
    alpha_neg_oci_sens: [-1, 1]
    beta: [0, 10]
    """
    alpha_pos, alpha_neg_base, alpha_neg_oci_sens, beta = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    # Calculate OCI-dependent negative learning rate
    alpha_neg = alpha_neg_base + (alpha_neg_oci_sens * current_oci)
    alpha_neg = np.clip(alpha_neg, 0.0, 1.0)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):

        # Stage 1 Choice
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        if action_1[trial] != -1:
            a1 = int(action_1[trial])
            p_choice_1[trial] = probs_1[a1]
        else:
            p_choice_1[trial] = 1.0
            continue

        state_idx = int(state[trial])

        # Stage 2 Choice
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        if action_2[trial] != -1:
            a2 = int(action_2[trial])
            p_choice_2[trial] = probs_2[a2]
            r = reward[trial]

            # --- Stage 2 Update ---
            pe2 = r - q_stage2[state_idx, a2]
            lr2 = alpha_pos if pe2 >= 0 else alpha_neg
            q_stage2[state_idx, a2] += lr2 * pe2

            # --- Stage 1 Update ---
            # Using the value of the chosen stage 2 state as the target
            pe1 = q_stage2[state_idx, a2] - q_stage1[a1]
            lr1 = alpha_pos if pe1 >= 0 else alpha_neg
            q_stage1[a1] += lr1 * pe1
            
        else:
            p_choice_2[trial] = 1.0

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Inverse Temperature (Beta) Modulated by OCI
This model proposes that the OCI score affects the exploration-exploitation balance, represented by the softmax inverse temperature parameter `beta`. High OCI might be associated with more rigid, deterministic responding (high beta) or, conversely, more uncertainty-driven exploration (low beta). This model tests if the overall consistency of choice is a function of the OCI score, utilizing a pure Model-Free architecture to isolate this effect.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Standard Model-Free learner where the inverse temperature (beta) is modulated by OCI.
    
    Bounds:
    learning_rate: [0, 1]
    beta_base: [0, 10]
    beta_oci_slope: [-5, 5]
    eligibility_lambda: [0, 1]
    """
    learning_rate, beta_base, beta_oci_slope, eligibility_lambda = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    # Modulate beta based on OCI. 
    # Beta must be positive, so we clip at 0.
    beta = beta_base + (beta_oci_slope * current_oci)
    beta = np.maximum(beta, 0.0)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        
        # Stage 1 Choice
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        if action_1[trial] != -1:
            a1 = int(action_1[trial])
            p_choice_1[trial] = probs_1[a1]
        else:
            p_choice_1[trial] = 1.0
            continue

        state_idx = int(state[trial])

        # Stage 2 Choice
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        if action_2[trial] != -1:
            a2 = int(action_2[trial])
            p_choice_2[trial] = probs_2[a2]
            r = reward[trial]

            # --- Updates with Eligibility Trace (Lambda) ---
            # 1. Stage 2 Update
            delta_2 = r - q_stage2[state_idx, a2]
            q_stage2[state_idx, a2] += learning_rate * delta_2
            
            # 2. Stage 1 Update
            # TD(lambda) update for stage 1
            # The error at stage 1 is (Q(s2,a2) - Q(s1,a1))
            # PLUS the eligibility trace carries back the stage 2 error (delta_2) scaled by lambda
            delta_1 = q_stage2[state_idx, a2] - q_stage1[a1]
            q_stage1[a1] += learning_rate * delta_1 + (learning_rate * eligibility_lambda * delta_2)
            
        else:
            p_choice_2[trial] = 1.0

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```