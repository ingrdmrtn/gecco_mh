Here are the three proposed cognitive models.

### Model 1: Reward-Dependent Stickiness
This model hypothesizes that OCI severity specifically modulates "compulsive" persistence—the tendency to repeat a choice even after receiving no reward (loss)—while "win-stay" behavior remains stable.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Reward-Dependent Stickiness Model.
    
    Hypothesis: OCI severity correlates with 'compulsive' persistence, defined here as 
    stickiness to a choice specifically after a negative outcome (0 reward). 
    'Win-stay' stickiness is modeled separately.
    
    Parameters:
    - lr: [0, 1] Learning rate for both stages.
    - beta: [0, 10] Inverse temperature (softness of softmax).
    - w: [0, 1] Mixing weight (0 = Model-Free, 1 = Model-Based).
    - stick_win: [0, 5] Stickiness bonus applied after a reward (1).
    - stick_loss_low: [0, 5] Stickiness bonus applied after no reward (0) for Low OCI.
    - stick_loss_high: [0, 5] Stickiness bonus applied after no reward (0) for High OCI.
    """
    lr, beta, w, stick_win, stick_loss_low, stick_loss_high = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Interpolate stickiness for loss trials based on OCI
    stick_loss = stick_loss_low * (1 - current_oci) + stick_loss_high * current_oci
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # state x action
    
    last_action_1 = -1
    last_reward = -1
    
    log_likelihood = 0.0
    eps = 1e-10

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Handle missing/invalid data
        if a1 < 0 or s_idx < 0 or a2 < 0:
            continue

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Apply reward-dependent stickiness
        if last_action_1 != -1:
            if last_reward == 1:
                q_net[last_action_1] += stick_win
            else:
                q_net[last_action_1] += stick_loss
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_likelihood += np.log(probs_1[a1] + eps)

        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_likelihood += np.log(probs_2[a2] + eps)

        # Updates
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_stage2

        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        last_action_1 = a1
        last_reward = r

    return -log_likelihood
```

### Model 2: Subjective Transition Belief
This model suggests that high OCI participants may distrust the true transition structure (70/30) due to doubt or uncertainty, using a distorted "subjective" transition matrix for their Model-Based calculations.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Subjective Transition Belief Model.
    
    Hypothesis: OCI modulates the subjective probability of the 'common' transition 
    used in the Model-Based calculation. High OCI may lead to uncertainty (probability ~0.5) 
    or rigidity (probability ~1.0) regarding the transition structure.
    
    Parameters:
    - lr: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight.
    - stickiness: [0, 5] General choice persistence for Stage 1.
    - trans_prob_low: [0.5, 1.0] Subjective common transition probability at OCI=0.
    - trans_prob_high: [0.5, 1.0] Subjective common transition probability at OCI=1.
    """
    lr, beta, w, stickiness, trans_prob_low, trans_prob_high = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Interpolate subjective transition probability
    p_sub = trans_prob_low * (1 - current_oci) + trans_prob_high * current_oci
    
    # Construct subjective transition matrix used for MB planning
    # T[0,0] is prob of Spaceship A (0) -> Planet X (0)
    subjective_T = np.array([[p_sub, 1-p_sub], [1-p_sub, p_sub]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    log_likelihood = 0.0
    eps = 1e-10

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        if a1 < 0 or s_idx < 0 or a2 < 0:
            continue

        # Stage 1 Policy: MB calculation uses subjective T
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = subjective_T @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_likelihood += np.log(probs_1[a1] + eps)

        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_likelihood += np.log(probs_2[a2] + eps)

        # Updates
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_stage2

        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        last_action_1 = a1

    return -log_likelihood
```

### Model 3: Stage 2 Stickiness
This model posits that OCI relates to repetitive "checking" behaviors manifested as stickiness to specific options in the second stage (aliens), distinct from the spaceship choice.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Stage 2 Stickiness Model.
    
    Hypothesis: OCI correlates with repetitive behavior directed at the specific reward sources 
    (Aliens/Stage 2), modeled as a separate stickiness parameter for the second stage choice.
    
    Parameters:
    - lr: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight.
    - stick_s1: [0, 5] Stickiness for Stage 1 (Spaceships).
    - stick_s2_low: [0, 5] Stickiness for Stage 2 (Aliens) at OCI=0.
    - stick_s2_high: [0, 5] Stickiness for Stage 2 (Aliens) at OCI=1.
    """
    lr, beta, w, stick_s1, stick_s2_low, stick_s2_high = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Interpolate Stage 2 stickiness
    stick_s2 = stick_s2_low * (1 - current_oci) + stick_s2_high * current_oci
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    # Track the last alien chosen for each planet separately
    last_action_2_per_state = [-1, -1]
    
    log_likelihood = 0.0
    eps = 1e-10

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        if a1 < 0 or s_idx < 0 or a2 < 0:
            continue

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_action_1 != -1:
            q_net[last_action_1] += stick_s1
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_likelihood += np.log(probs_1[a1] + eps)

        # Stage 2 Policy
        q_s2_eff = q_stage2_mf[s_idx].copy()
        
        # Apply Stage 2 stickiness if we have visited this planet before
        last_a2 = last_action_2_per_state[s_idx]
        if last_a2 != -1:
            q_s2_eff[last_a2] += stick_s2
            
        exp_q2 = np.exp(beta * q_s2_eff)
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_likelihood += np.log(probs_2[a2] + eps)

        # Updates
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_stage2

        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        last_action_1 = a1
        last_action_2_per_state[s_idx] = a2

    return -log_likelihood
```