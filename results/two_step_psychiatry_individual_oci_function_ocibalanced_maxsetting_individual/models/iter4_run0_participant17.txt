Here are the three proposed cognitive models.

### Model 1: Reward-Outcome-Dependent Stickiness
This model hypothesizes that OCI scores modulate "perseveration" (stickiness) differently depending on whether the previous action was rewarded or not. High OCI (compulsivity) might be associated with a tendency to repeat actions even when they were unrewarded (compulsive repetition), whereas low OCI participants might switch after a lack of reward.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model with Reward-Outcome-Dependent Stickiness.
    
    Hypothesis: Stickiness (tendency to repeat the last choice) is split into two components:
    stickiness after a reward, and stickiness after no reward.
    The OCI score modulates the 'unrewarded stickiness', capturing the hypothesis that 
    compulsive individuals may persist in behavior despite a lack of positive feedback.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate for value updates (assumed TD(1) for MF).
    beta: [0, 10] - Inverse temperature for softmax.
    w: [0, 1] - Weighting between Model-Based (1) and Model-Free (0).
    stick_rew: [-5, 5] - Stickiness bonus applied if the previous trial was rewarded.
    stick_unrew_base: [-5, 5] - Baseline stickiness bonus if the previous trial was unrewarded.
    stick_unrew_oci: [-5, 5] - Modulation of unrewarded stickiness by OCI score.
    """
    learning_rate, beta, w, stick_rew, stick_unrew_base, stick_unrew_oci = model_parameters
    n_trials = len(action_1)
    oci_val = oci[0]
    
    # Calculate the effective stickiness for unrewarded trials
    stick_unrew = stick_unrew_base + (stick_unrew_oci * oci_val)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    last_reward = 0

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        logits_1 = beta * q_net
        
        # Apply reward-dependent stickiness
        if last_action_1 != -1:
            if last_reward == 1:
                logits_1[last_action_1] += stick_rew
            else:
                logits_1[last_action_1] += stick_unrew
        
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # Record choice and transition
        a1 = int(action_1[trial])
        last_action_1 = a1
        state_idx = int(state[trial])
        
        # --- Stage 2 Policy ---
        logits_2 = beta * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        r = reward[trial]
        last_reward = r
        
        # TD Errors
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        
        # Update Stage 1 MF (TD(1) equivalent: sum of deltas)
        q_stage1_mf[a1] += learning_rate * (delta_stage1 + delta_stage2)
        
        # Update Stage 2 MF
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Stage-Specific Learning Rates
This model hypothesizes that OCI affects habit formation (Stage 1) differently from goal-directed value learning (Stage 2). It separates the learning rates for the spaceship choice (habitual/MF) and the alien choice (terminal/goal), with OCI specifically modulating the Stage 1 learning rate.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model with Stage-Specific Learning Rates modulated by OCI.
    
    Hypothesis: OCI scores correlate with the rate of habit formation (Stage 1 MF learning).
    We separate the learning rate for Stage 1 (Spaceships) and Stage 2 (Aliens).
    OCI modulates the Stage 1 learning rate, while Stage 2 uses a fixed rate.
    
    Parameters:
    lr_stage2: [0, 1] - Learning rate for the second stage (Aliens).
    lr_stage1_base: [0, 1] - Base learning rate for the first stage (Spaceships).
    lr_stage1_oci: [-1, 1] - Modulation of Stage 1 learning rate by OCI.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - Weighting between Model-Based and Model-Free.
    stickiness: [-5, 5] - General choice perseveration bonus.
    """
    lr_stage2, lr_stage1_base, lr_stage1_oci, beta, w, stickiness = model_parameters
    n_trials = len(action_1)
    oci_val = oci[0]
    
    # Calculate Stage 1 specific learning rate
    lr_stage1 = lr_stage1_base + (lr_stage1_oci * oci_val)
    lr_stage1 = np.clip(lr_stage1, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        logits_1 = beta * q_net
        
        if last_action_1 != -1:
            logits_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        a1 = int(action_1[trial])
        last_action_1 = a1
        state_idx = int(state[trial])
        
        # --- Stage 2 Policy ---
        logits_2 = beta * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        r = reward[trial]
        
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        
        # Update Stage 1 using its specific learning rate (TD(1))
        q_stage1_mf[a1] += lr_stage1 * (delta_stage1 + delta_stage2)
        
        # Update Stage 2 using its specific learning rate
        q_stage2_mf[state_idx, a2] += lr_stage2 * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Forgetting Rate
This model introduces a "forgetting" mechanism for unchosen options, hypothesizing that OCI modulates how quickly participants' value estimates decay for options they do not currently select. This is relevant in a task with drifting reward probabilities.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model with OCI-Modulated Forgetting Rate for Unchosen Options.
    
    Hypothesis: Values of unchosen options decay over time (passive forgetting).
    The rate of this forgetting is modulated by the OCI score. High OCI might
    be associated with lower forgetting (rigidity) or higher forgetting (instability).
    
    Parameters:
    learning_rate: [0, 1] - Learning rate for chosen options.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - Weighting between Model-Based and Model-Free.
    stickiness: [-5, 5] - Choice perseveration bonus.
    forget_base: [0, 1] - Base forgetting rate for unchosen options.
    forget_oci: [-1, 1] - Modulation of forgetting rate by OCI.
    """
    learning_rate, beta, w, stickiness, forget_base, forget_oci = model_parameters
    n_trials = len(action_1)
    oci_val = oci[0]
    
    # Calculate forgetting rate
    forget_rate = forget_base + (forget_oci * oci_val)
    forget_rate = np.clip(forget_rate, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        logits_1 = beta * q_net
        
        if last_action_1 != -1:
            logits_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        a1 = int(action_1[trial])
        last_action_1 = a1
        state_idx = int(state[trial])
        
        # --- Stage 2 Policy ---
        logits_2 = beta * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        r = reward[trial]
        
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        
        # Update chosen Stage 1
        q_stage1_mf[a1] += learning_rate * (delta_stage1 + delta_stage2)
        # Forget unchosen Stage 1
        unchosen_a1 = 1 - a1
        q_stage1_mf[unchosen_a1] *= (1.0 - forget_rate)
        
        # Update chosen Stage 2
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        # Forget unchosen Stage 2 (in the current state)
        unchosen_a2 = 1 - a2
        q_stage2_mf[state_idx, unchosen_a2] *= (1.0 - forget_rate)
        # Note: We do not typically forget values in the unvisited state in this simple model, 
        # but one could apply it there too. Here we apply it to the unchosen action in the visited state.

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```