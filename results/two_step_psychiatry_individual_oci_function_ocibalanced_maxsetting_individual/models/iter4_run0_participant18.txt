Here are three new cognitive models for the two-step task, incorporating the OCI score to explain individual variability.

### Model 1: OCI-Modulated Loss Aversion
This model tests the hypothesis that higher OCI scores are linked to heightened sensitivity to negative outcomes (omission of reward). A reward of 0 is perceived not just as a lack of gain, but as a penalty, the magnitude of which scales with the OCI score.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Loss Aversion.
    Hypothesis: Participants with high OCI scores exhibit 'loss aversion' or heightened sensitivity to 
    negative outcomes (omission of reward). A reward of 0 is perceived not just as neutral, 
    but as a penalty, proportional to their OCI score.
    
    Parameters:
    - lr: Learning rate [0,1]
    - beta: Inverse temperature [0,10]
    - w: Model-Based weight [0,1]
    - lambda_val: Eligibility trace decay [0,1]
    - stickiness: Choice perseveration [0,5]
    - loss_sens: Sensitivity to 0-reward outcomes [0,5]. Effective reward = 0 - (loss_sens * OCI).
    """
    lr, beta, w, lambda_val, stickiness, loss_sens = model_parameters
    oci_score = oci[0]
    n_trials = len(action_1)
    
    # Transition matrix (fixed structure for this model)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Q-values initialization
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1
    
    for t in range(n_trials):
        if action_1[t] == -1:
            continue
            
        a1 = int(action_1[t])
        s2 = int(state[t])
        a2 = int(action_2[t])
        r = reward[t]
        
        # --- Stage 1 Choice ---
        # Model-Based Value
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Value
        q_net_s1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Logits with Stickiness
        logits_1 = beta * q_net_s1
        if prev_a1 != -1:
            logits_1[prev_a1] += stickiness
            
        # Softmax Stage 1
        exp_1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_1 / np.sum(exp_1)
        p_choice_1[t] = probs_1[a1]
        
        # --- Stage 2 Choice ---
        # Logits Stage 2 (Pure MF)
        logits_2 = beta * q_stage2_mf[s2]
        exp_2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_2 / np.sum(exp_2)
        p_choice_2[t] = probs_2[a2]
        
        # --- Learning ---
        # Effective Reward Calculation
        # If r=1, val=1. If r=0, val = -loss_sens * oci
        r_eff = r
        if r == 0:
            r_eff = -(loss_sens * oci_score)
            
        # SARSA / Q-learning updates
        # Stage 1 TD Error (using Stage 2 Q-value)
        delta_1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_1
        
        # Stage 2 TD Error (using Effective Reward)
        delta_2 = r_eff - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += lr * delta_2
        
        # Eligibility Trace Update for Stage 1
        q_stage1_mf[a1] += lr * lambda_val * delta_2
        
        prev_a1 = a1

    eps = 1e-10
    # Filter valid trials for log loss
    valid_mask = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Transition Uncertainty
This model posits that high OCI is associated with doubt about the environmental structure. The agent's internal model of the transition probabilities (usually 0.7/0.3) drifts towards uniformity (0.5/0.5) as OCI increases, reducing the distinctiveness and influence of Model-Based planning.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Modulated Transition Uncertainty.
    Hypothesis: High OCI is associated with doubt about the environmental structure. 
    Instead of trusting the fixed 70/30 transition probabilities, the agent's internal model 
    drifts towards uniformity (50/50) as OCI increases, reducing the distinctiveness 
    of Model-Based values.
    
    Parameters:
    - lr: Learning rate [0,1]
    - beta: Inverse temperature [0,10]
    - w: Model-Based weight [0,1]
    - lambda_val: Eligibility trace decay [0,1]
    - stickiness: Choice perseveration [0,5]
    - confusion: Degree of transition matrix flattening per unit of OCI [0,1].
                 p_trans = 0.7 - (0.2 * confusion * OCI).
    """
    lr, beta, w, lambda_val, stickiness, confusion = model_parameters
    oci_score = oci[0]
    n_trials = len(action_1)
    
    # Adjust Transition Matrix based on OCI
    # Base is 0.7. Max confusion (OCI=1, param=1) reduces it to 0.5.
    # range of shift is 0.0 to 0.2.
    shift = 0.2 * confusion * oci_score
    p_high = 0.7 - shift
    p_low = 1.0 - p_high
    
    transition_matrix = np.array([[p_high, p_low], [p_low, p_high]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1
    
    for t in range(n_trials):
        if action_1[t] == -1:
            continue
            
        a1 = int(action_1[t])
        s2 = int(state[t])
        a2 = int(action_2[t])
        r = reward[t]
        
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        # Use the OCI-modulated transition matrix
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_s1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net_s1
        if prev_a1 != -1:
            logits_1[prev_a1] += stickiness
            
        exp_1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_1 / np.sum(exp_1)
        p_choice_1[t] = probs_1[a1]
        
        # --- Stage 2 Choice ---
        logits_2 = beta * q_stage2_mf[s2]
        exp_2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_2 / np.sum(exp_2)
        p_choice_2[t] = probs_2[a2]
        
        # --- Learning ---
        delta_1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_1
        
        delta_2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += lr * delta_2
        
        q_stage1_mf[a1] += lr * lambda_val * delta_2
        
        prev_a1 = a1

    eps = 1e-10
    valid_mask = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Stage 2 Rigidity
This model hypothesizes that high OCI participants exhibit different levels of exploration/exploitation depending on the decision stage. Specifically, they may become more rigid (higher inverse temperature, beta) at the second stage where the reward is immediate, compared to the first planning stage.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Stage 2 Rigidity.
    Hypothesis: High OCI participants exhibit different levels of exploration/exploitation 
    depending on the decision stage. Specifically, they may become more rigid (higher beta) 
    at the second stage where the reward is immediate, compared to the planning stage.
    
    Parameters:
    - lr: Learning rate [0,1]
    - beta_1: Inverse temperature for Stage 1 [0,10]
    - w: Model-Based weight [0,1]
    - lambda_val: Eligibility trace decay [0,1]
    - stickiness: Choice perseveration [0,5]
    - beta_2_boost: Multiplier for Stage 2 beta based on OCI [0,5].
                    beta_2 = beta_1 * (1 + beta_2_boost * OCI).
    """
    lr, beta_1, w, lambda_val, stickiness, beta_2_boost = model_parameters
    oci_score = oci[0]
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1
    
    # Calculate Stage 2 beta
    beta_2 = beta_1 * (1.0 + beta_2_boost * oci_score)
    
    for t in range(n_trials):
        if action_1[t] == -1:
            continue
            
        a1 = int(action_1[t])
        s2 = int(state[t])
        a2 = int(action_2[t])
        r = reward[t]
        
        # --- Stage 1 Choice (uses beta_1) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_s1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta_1 * q_net_s1
        if prev_a1 != -1:
            logits_1[prev_a1] += stickiness
            
        exp_1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_1 / np.sum(exp_1)
        p_choice_1[t] = probs_1[a1]
        
        # --- Stage 2 Choice (uses beta_2) ---
        logits_2 = beta_2 * q_stage2_mf[s2]
        exp_2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_2 / np.sum(exp_2)
        p_choice_2[t] = probs_2[a2]
        
        # --- Learning ---
        delta_1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_1
        
        delta_2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += lr * delta_2
        
        q_stage1_mf[a1] += lr * lambda_val * delta_2
        
        prev_a1 = a1

    eps = 1e-10
    valid_mask = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss
```