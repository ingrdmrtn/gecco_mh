Here are three new cognitive models based on the participant's data and OCI score.

### Model 1: Stage 1 Learning Rate Modulation
This model hypothesizes that OCI symptoms specifically affect the flexibility of the high-level "planning" stage (Stage 1), while the learning of immediate outcomes (Stage 2) remains standard. High OCI might lead to reduced learning rates at Stage 1, reflecting a rigidity in high-level choice patterns (habit formation) and resistance to changing the overall strategy, even if local outcomes fluctuate.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid model where the learning rate for Stage 1 (Spaceship choice) is modulated by OCI,
    while Stage 2 (Alien choice) uses a fixed learning rate.
    
    Hypothesis: OCI affects the plasticity of high-level choices (Stage 1). 
    High OCI may lead to lower Stage 1 learning rates (rigidity/habits).
    
    Parameters:
    lr_s2: Learning rate for Stage 2 (Aliens) [0, 1]
    beta: Inverse temperature [0, 10]
    w: Mixing weight for Model-Based vs Model-Free [0, 1]
    lr_s1_base: Baseline learning rate for Stage 1 [0, 1]
    lr_s1_oci: Effect of OCI on Stage 1 learning rate [0, 1]
    """
    lr_s2, beta, w, lr_s1_base, lr_s1_oci = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate OCI-modulated Stage 1 learning rate
    # We allow the parameter to scale the base rate, clipped to valid range.
    # Formulation: lr = base * (1 +/- effect) or linear combination.
    # Here we use a linear shift to allow distinct base and OCI sensitivity.
    lr_s1 = lr_s1_base + lr_s1_oci * current_oci
    lr_s1 = np.clip(lr_s1, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Stage 2 update (Fixed LR)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr_s2 * delta_stage2
        
        # Stage 1 update (OCI-modulated LR)
        # Standard TD(0) update for Stage 1 MF
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr_s1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Win-Stay Bonus
This model investigates if OCI drives a specific "Win-Stay" heuristic. Unlike general stickiness (perseveration), which repeats choices regardless of outcome, "Win-Stay" is a safety behavior: if an action was rewarded, repeat it immediately. This model adds a value bonus to the previously chosen spaceship *only* if the last trial was rewarded, with the magnitude of this bonus scaled by OCI.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid model with a 'Win-Stay' bonus modulated by OCI.
    
    Hypothesis: High OCI participants exhibit compulsive repetition of successful actions 
    (safety signals). A bonus is added to the previous choice Q-value ONLY if the 
    previous trial was rewarded.
    
    Parameters:
    lr: Learning rate [0, 1]
    beta: Inverse temperature [0, 10]
    w: Mixing weight [0, 1]
    ws_base: Baseline Win-Stay bonus [0, 1]
    ws_oci: Effect of OCI on the Win-Stay bonus magnitude [0, 1]
    """
    lr, beta, w, ws_base, ws_oci = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate effective Win-Stay bonus
    # We assume the bonus is added to the Q-value (typically 0-1 range).
    # A bonus of 1.0 is very strong.
    win_stay_bonus = ws_base + ws_oci * current_oci
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Apply Win-Stay Bonus
        # Create a temporary Q-values array for decision making so we don't corrupt the learning process
        q_decision = q_net.copy()
        if trial > 0:
            prev_r = reward[trial-1]
            prev_a1 = int(action_1[trial-1])
            if prev_r == 1.0:
                q_decision[prev_a1] += win_stay_bonus
        
        exp_q1 = np.exp(beta * q_decision)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_stage2
        
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Stage 1 Specific Beta Modulation
This model proposes that OCI affects the exploration-exploitation trade-off specifically at the high-level decision stage. It separates the inverse temperature (beta) for Stage 1 and Stage 2. OCI modulates `beta1` (Spaceship choice), reflecting the hypothesis that anxiety or compulsivity drives more deterministic (rigid) selection of the primary strategy, while the second stage remains more flexible or stable.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid model with separate inverse temperatures (beta) for Stage 1 and Stage 2,
    where the Stage 1 beta is modulated by OCI.
    
    Hypothesis: OCI affects decision noise/determinism specifically at the planning stage (Stage 1).
    High OCI may lead to high Beta1 (deterministic, rigid spaceship choice).
    
    Parameters:
    lr: Learning rate [0, 1]
    w: Mixing weight [0, 1]
    beta2: Inverse temperature for Stage 2 (Aliens) [0, 10]
    beta1_base: Baseline inverse temperature for Stage 1 [0, 10]
    beta1_oci: Effect of OCI on Stage 1 inverse temperature [0, 10]
    """
    lr, w, beta2, beta1_base, beta1_oci = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate OCI-modulated Beta for Stage 1
    # We allow beta1 to increase with OCI (more deterministic)
    beta1 = beta1_base + beta1_oci * current_oci
    # Ensure beta stays within reasonable bounds (0-20 is usually enough, template suggests 0-10)
    beta1 = np.clip(beta1, 0.0, 10.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy (Uses Beta 1) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy (Uses Beta 2) ---
        exp_q2 = np.exp(beta2 * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_stage2
        
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```