Here are three cognitive models implemented as Python functions.

### Cognitive Model 1: Compulsive Persistence (Outcome-Dependent Stickiness)
This model hypothesizes that OCI scores correlate with "chasing" behavior or an inability to switch away from an action even after a loss (unrewarded trial). While standard stickiness (perseveration) applies regardless of outcome, this model adds a specific stickiness bonus that activates only when the previous trial resulted in a loss (0 coins), scaled by the participant's OCI score.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Compulsive Persistence Model.
    
    Hypothesis: Participants with higher OCI scores exhibit 'compulsive persistence',
    showing increased stickiness (perseveration) specifically after unrewarded trials (losses),
    reflecting a 'chasing' behavior or inability to switch after failure.
    
    Parameters:
    - lr: [0, 1] Learning rate for Q-value updates.
    - beta: [0, 10] Inverse temperature (softmax sensitivity).
    - w: [0, 1] Mixing weight (0=Model-Free, 1=Model-Based).
    - stick_base: [0, 5] Baseline stickiness to the previous choice (regardless of outcome).
    - stick_loss_oci: [0, 5] Additional stickiness applied only after a LOSS, scaled by OCI.
      Total stickiness after loss = stick_base + stick_loss_oci * oci.
    
    Args:
        action_1, state, action_2, reward, oci, model_parameters
        
    Returns:
        float: Negative log-likelihood of the observed choices.
    """
    lr, beta, w, stick_base, stick_loss_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Transition matrix (fixed world structure)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Initialize Q-values
    q_stage1_mf = np.zeros(2)      # Model-free values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for stage 2 (2 states, 2 actions)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_choice_1 = -1
    last_reward = -1
    
    for trial in range(n_trials):
        # Handle missing data
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])
        
        # --- Stage 1 Policy ---
        # Model-Based Value: T * max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Q-value mixing MB and MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Calculate logits
        logits_1 = beta * q_net
        
        # Apply Stickiness
        if last_choice_1 != -1:
            # Calculate effective stickiness based on previous outcome
            current_stick = stick_base
            if last_reward == 0: # If previous trial was a loss
                current_stick += stick_loss_oci * oci_score
            
            logits_1[last_choice_1] += current_stick
            
        # Softmax for Stage 1
        exp_q1 = np.exp(logits_1 - np.max(logits_1)) # Subtract max for numerical stability
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        logits_2 = beta * q_stage2_mf[s2]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        # SARSA / Q-learning updates
        # Stage 1 MF update (using Stage 2 Q-value as proxy for max)
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        # Stage 2 MF update
        delta_stage2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += lr * delta_stage2
        
        # Update history
        last_choice_1 = a1
        last_reward = r

    # Calculate Negative Log Likelihood
    # Mask out trials where probabilities are 0 (missing data trials)
    mask = (action_1 != -1)
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss
```

### Cognitive Model 2: Subjective Disappointment (Loss Valuation)
This model posits that OCI symptoms are associated with an increased subjective sensitivity to negative outcomes (omission of reward). Instead of treating a 0-coin outcome as a neutral zero, the participant treats it as a punishment (negative value). The magnitude of this negative valuation is scaled by the OCI score.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Subjective Disappointment (Loss Valuation) Model.
    
    Hypothesis: Participants with higher OCI scores perceive the lack of reward (0 coins)
    not as neutral, but as a negative outcome (punishment/disappointment). 
    The model replaces the objective reward R=0 with a subjective effective reward 
    R_eff = -(loss_val_base + loss_val_oci * OCI).
    
    Parameters:
    - lr: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight.
    - stick: [0, 5] General choice stickiness.
    - loss_val_base: [0, 2] Base subjective penalty for 0 reward.
    - loss_val_oci: [0, 5] Additional subjective penalty scaled by OCI.
    
    Args:
        action_1, state, action_2, reward, oci, model_parameters
        
    Returns:
        float: Negative log-likelihood.
    """
    lr, beta, w, stick, loss_val_base, loss_val_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_choice_1 = -1
    
    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])
        
        # Calculate Subjective Reward
        # If reward is 1, use 1. If reward is 0, use negative subjective value.
        if r == 1:
            r_eff = 1.0
        else:
            loss_penalty = loss_val_base + loss_val_oci * oci_score
            r_eff = -loss_penalty
        
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        logits_1 = beta * q_net
        
        if last_choice_1 != -1:
            logits_1[last_choice_1] += stick
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        logits_2 = beta * q_stage2_mf[s2]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        # Note: standard TD uses the value of the next state. 
        # Here we use the standard structure for the task:
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        # Stage 2 Update (using effective reward)
        delta_stage2 = r_eff - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += lr * delta_stage2
        
        last_choice_1 = a1

    mask = (action_1 != -1)
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss
```

### Cognitive Model 3: Distorted World Model (Transition Belief)
This model suggests that OCI affects the internal model of the task structure itself. Specifically, it tests whether participants with higher OCI scores hold a more rigid or deterministic belief about the spaceship transitions (overestimating the probability of the common transition), or conversely, a more chaotic belief. The `belief_oci` parameter modulates the perceived transition probability used in the Model-Based calculation.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Distorted World Model (Transition Belief).
    
    Hypothesis: OCI scores modulate the participant's internal model of the transition probabilities.
    While the true common transition probability is 0.7, high OCI may lead to 'rigidity' 
    (believing transitions are more deterministic) or uncertainty.
    The model-based system uses a subjective transition matrix T' where:
    p_common_subjective = 0.7 + belief_distortion * OCI.
    
    Parameters:
    - lr: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight.
    - stick: [0, 5] Stickiness.
    - belief_distortion: [-0.5, 0.5] Modulation of perceived transition probability by OCI.
      If positive, OCI -> belief in higher determinism (closer to 1.0).
      If negative, OCI -> belief in randomness (closer to 0.5).
    
    Args:
        action_1, state, action_2, reward, oci, model_parameters
        
    Returns:
        float: Negative log-likelihood.
    """
    lr, beta, w, stick, belief_distortion = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate subjective transition probability
    # Base is 0.7. Modifier depends on OCI.
    p_subjective = 0.7 + belief_distortion * oci_score
    
    # Clamp probability to valid range [0, 1] (or slightly inside to avoid log(0) issues in other contexts)
    if p_subjective > 1.0: p_subjective = 1.0
    if p_subjective < 0.0: p_subjective = 0.0
    
    # Construct the subjective transition matrix
    # T[0,0] = p (A->X), T[0,1] = 1-p (A->Y)
    # T[1,0] = 1-p (U->X), T[1,1] = p (U->Y)
    transition_matrix_mb = np.array([[p_subjective, 1.0 - p_subjective], 
                                     [1.0 - p_subjective, p_subjective]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_choice_1 = -1
    
    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])
        
        # --- Stage 1 Policy ---
        # Use the SUBJECTIVE transition matrix for MB calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix_mb @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        logits_1 = beta * q_net
        
        if last_choice_1 != -1:
            logits_1[last_choice_1] += stick
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        logits_2 = beta * q_stage2_mf[s2]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += lr * delta_stage2
        
        last_choice_1 = a1

    mask = (action_1 != -1)
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss
```