Here are the three cognitive models based on the participant's data and OCI score.

### Model 1: Stage-Specific Learning Rates (OCI-Modulated Stage 2)
This model hypothesizes that OCI symptoms differentially affect learning from immediate outcomes (Stage 2, Aliens) versus distal choices (Stage 1, Spaceships). Specifically, the learning rate for the second stage is modulated by the OCI score, allowing the participant to be more or less sensitive to the direct alien rewards compared to the spaceship transitions.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Stage-Specific Learning Rates with OCI modulation on Stage 2.
    
    Hypothesis: OCI affects the learning rate for immediate outcomes (Stage 2) 
    differently than the base learning rate used for Stage 1. This allows the 
    model to capture if high-OCI participants over- or under-update value 
    estimates for the specific aliens compared to the spaceships.

    Parameters:
    lr_1: [0,1] - Fixed learning rate for Stage 1 (Spaceships).
    lr_2_base: [0,1] - Base learning rate for Stage 2 (Aliens).
    lr_2_oci_slope: [-1,1] - How OCI modulates the Stage 2 learning rate.
    beta: [0,10] - Inverse temperature for softmax.
    w: [0,1] - Mixing weight for Model-Based (1) vs Model-Free (0).
    
    Equation:
    lr_2 = lr_2_base + (lr_2_oci_slope * oci)
    (Clipped to [0, 1])
    """
    lr_1, lr_2_base, lr_2_oci_slope, beta, w = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]

    # Calculate effective Stage 2 learning rate
    lr_2 = lr_2_base + (lr_2_oci_slope * participant_oci)
    lr_2 = np.clip(lr_2, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2) 
    q_stage2_mf = np.zeros((2, 2)) # State x Action

    for trial in range(n_trials):
        # Handle missing data
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        # Model-Based Value
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2

        # Integrated Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf

        # Softmax Stage 1
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        # Softmax Stage 2 (Pure Model-Free)
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Update Stage 2 Q-values with OCI-modulated rate (lr_2)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr_2 * delta_stage2

        # Update Stage 1 Q-values with fixed rate (lr_1)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr_1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Memory Decay
This model introduces a forgetting mechanism where the values of unchosen options decay over time. The rate of this decay is determined by the OCI score. This tests the hypothesis that OCI symptoms might correlate with difficulty in maintaining value representations for options that are not currently being engaged with (or conversely, obsessive rigidity preventing normal forgetting).

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Memory Decay (Forgetting) modulated by OCI.
    
    Hypothesis: OCI scores predict the rate at which the participant 'forgets' 
    or discounts the value of unchosen options. High OCI might lead to faster 
    decay (anxiety/interference) or slower decay (obsessive retention).
    
    Parameters:
    learning_rate: [0,1] - Update rate for chosen options.
    beta: [0,10] - Inverse temperature.
    w: [0,1] - Mixing weight.
    decay_base: [0,1] - Base decay rate for unchosen options.
    decay_oci_slope: [-1,1] - OCI modulation of decay rate.
    
    Equation:
    decay = decay_base + (decay_oci_slope * oci)
    (Clipped to [0, 1])
    """
    learning_rate, beta, w, decay_base, decay_oci_slope = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]

    # Calculate effective decay rate
    decay = decay_base + (decay_oci_slope * participant_oci)
    decay = np.clip(decay, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2) 
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2

        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Update chosen Stage 2
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Decay unchosen Stage 2 (on the current planet)
        unchosen_a2 = 1 - a2
        q_stage2_mf[s_idx, unchosen_a2] *= (1.0 - decay)

        # Update chosen Stage 1
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Decay unchosen Stage 1
        unchosen_a1 = 1 - a1
        q_stage1_mf[unchosen_a1] *= (1.0 - decay)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Stage 2 Stickiness (OCI-Modulated)
While previous models explored stickiness at the first decision stage (Spaceships), this model tests for perseveration at the *second* decision stage (Aliens). It proposes that OCI symptoms drive repetitive behavior specifically towards the concrete stimuli (aliens) encountered on the planets, adding a "stickiness" bonus to the previously chosen alien at that location.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Stage 2 Stickiness modulated by OCI.
    
    Hypothesis: Repetitive behavior (stickiness) manifests in the choice of 
    aliens (Stage 2). The magnitude of this perseveration is driven by OCI.
    The model adds a 'choice bonus' to the alien selected the last time 
    the participant visited the current planet.

    Parameters:
    learning_rate: [0,1] - Update rate.
    beta: [0,10] - Inverse temperature.
    w: [0,1] - Mixing weight.
    stick_s2_base: [0,5] - Base stickiness bonus for Stage 2.
    stick_s2_oci: [0,5] - OCI scaling of Stage 2 stickiness.
    
    Equation:
    bonus = stick_s2_base + (stick_s2_oci * oci)
    """
    learning_rate, beta, w, stick_s2_base, stick_s2_oci = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]

    # Calculate stickiness magnitude
    stickiness_mag = stick_s2_base + (stick_s2_oci * participant_oci)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2) 
    q_stage2_mf = np.zeros((2, 2))
    
    # Track last choice made at each planet (initially -1)
    last_choice_at_planet = np.array([-1, -1])

    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        # Note: Stickiness is a choice bias, usually not included in the value 
        # passed to the MB system. MB uses raw Q-values.
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2

        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        # Calculate logits for Stage 2
        logits_stage2 = beta * q_stage2_mf[s_idx].copy()
        
        # Add stickiness bonus if we have visited this planet before
        if last_choice_at_planet[s_idx] != -1:
            prev_a2 = last_choice_at_planet[s_idx]
            logits_stage2[prev_a2] += stickiness_mag
            
        exp_q2 = np.exp(logits_stage2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Update tracking
        last_choice_at_planet[s_idx] = a2

        # --- Updates ---
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```