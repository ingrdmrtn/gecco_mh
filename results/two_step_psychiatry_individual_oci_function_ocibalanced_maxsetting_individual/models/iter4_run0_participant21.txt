Here are three new cognitive models that explore different mechanisms for how Obsessive-Compulsive Inventory (OCI) scores might influence decision-making in the two-step task, specifically focusing on the balance between model-based and model-free control, and asymmetric learning rates.

### Cognitive Model 1: Hybrid Learner with OCI-modulated Mixing Weight
This model posits that the balance between Goal-Directed (Model-Based) and Habitual (Model-Free) systems is directly influenced by the participant's OCI score. Specifically, higher OCI scores might lead to a stronger reliance on the habitual system (lower `w`), reflecting the compulsive nature of habits.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free Learner with OCI-Modulated Mixing Weight.
    
    The mixing weight 'w' determines the balance between Model-Based (MB) and 
    Model-Free (MF) values. Here, 'w' is a function of the OCI score.
    Higher OCI scores are hypothesized to reduce the weight of the MB system 
    (more habitual/compulsive).
    
    Parameters:
    lr : [0, 1] Learning rate for value updates.
    beta : [0, 10] Inverse temperature for softmax choice.
    w_intercept : [0, 1] Base mixing weight (intercept).
    w_slope : [-1, 1] How OCI score adjusts the mixing weight.
                 w = w_intercept + w_slope * oci (clipped to [0,1]).
    """
    lr, beta, w_intercept, w_slope = model_parameters
    n_trials = len(action_1)
    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    
    current_oci = oci[0]
    
    # Calculate mixing weight w based on OCI
    w = w_intercept + w_slope * current_oci
    # Ensure w stays within valid bounds [0, 1]
    w = np.clip(w, 0.0, 1.0)
    
    # Transition matrix (fixed for this task structure)
    # 0 -> 0 (70%), 0 -> 1 (30%)
    # 1 -> 0 (30%), 1 -> 1 (70%)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_mf = np.zeros((2, 2))  # Stage 1 MF values (2 actions) and Stage 2 MF values (2 states x 2 actions) stored separately usually, 
                             # but here we need Q_MF(s1, a1) and Q_MF(s2, a2).
    q1_mf = np.zeros(2)      # Stage 1 MF values
    q2_mf = np.zeros((2, 2)) # Stage 2 MF values
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based Value Calculation
        # V_MB(s1) = sum(P(s2|s1, a1) * max(Q_MF(s2, a')))
        max_q2 = np.max(q2_mf, axis=1) # Max value for each state (planet)
        q1_mb = transition_matrix @ max_q2 # Expected value based on transition probs
        
        # Hybrid Value
        q1_net = w * q1_mb + (1 - w) * q1_mf
        
        # Softmax Policy Stage 1
        exp_q1 = np.exp(beta * q1_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Choice ---
        curr_state = state[trial]
        
        # Softmax Policy Stage 2 (Purely Model-Free usually assumed at 2nd stage)
        exp_q2 = np.exp(beta * q2_mf[curr_state])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning ---
        # Stage 2 RPE
        # delta_2 = r - Q2(s2, a2)
        rpe_2 = reward[trial] - q2_mf[curr_state, action_2[trial]]
        
        # Update Stage 2 MF values
        q2_mf[curr_state, action_2[trial]] += lr * rpe_2
        
        # Stage 1 RPE (SARSA-style TD(0) for MF)
        # delta_1 = Q2(s2, a2) - Q1(s1, a1) 
        # Note: Standard Daw et al. 2011 uses Q(s2, a2) not max Q(s2) for MF update
        rpe_1 = q2_mf[curr_state, action_2[trial]] - q1_mf[action_1[trial]]
        
        # Update Stage 1 MF values
        q1_mf[action_1[trial]] += lr * rpe_1
        
        # Note: MB values don't need explicit updating here as they are derived from Q2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2: Asymmetric Learning Rates Modulated by OCI
This model investigates if OCI scores correlate with a bias in how positive versus negative prediction errors are processed. High OCI might be associated with hypersensitivity to punishment (negative RPEs) or failure, leading to different learning dynamics.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free Learner with OCI-Modulated Asymmetric Learning Rates.
    
    This model splits the learning rate into positive (lr_pos) and negative (lr_neg).
    The negative learning rate is specifically modulated by the OCI score, testing
    the hypothesis that OCI is related to altered sensitivity to negative outcomes.
    
    Parameters:
    lr_pos : [0, 1] Learning rate for positive prediction errors.
    lr_neg_base : [0, 1] Base learning rate for negative prediction errors.
    lr_neg_oci_scale : [0, 1] Scaling factor for OCI effect on negative learning rate.
                         lr_neg = lr_neg_base + lr_neg_oci_scale * oci
    beta : [0, 10] Inverse temperature.
    """
    lr_pos, lr_neg_base, lr_neg_oci_scale, beta = model_parameters
    n_trials = len(action_1)
    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    
    current_oci = oci[0]
    
    # Calculate effective negative learning rate
    lr_neg = lr_neg_base + lr_neg_oci_scale * current_oci
    lr_neg = np.clip(lr_neg, 0.0, 1.0)
    
    q1_mf = np.zeros(2)
    q2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Choice ---
        curr_state = state[trial]
        exp_q2 = np.exp(beta * q2_mf[curr_state])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning ---
        # Stage 2 Update
        rpe_2 = reward[trial] - q2_mf[curr_state, action_2[trial]]
        
        if rpe_2 >= 0:
            q2_mf[curr_state, action_2[trial]] += lr_pos * rpe_2
        else:
            q2_mf[curr_state, action_2[trial]] += lr_neg * rpe_2
            
        # Stage 1 Update
        # Using the value of the chosen second stage action for the update
        rpe_1 = q2_mf[curr_state, action_2[trial]] - q1_mf[action_1[trial]]
        
        if rpe_1 >= 0:
            q1_mf[action_1[trial]] += lr_pos * rpe_1
        else:
            q1_mf[action_1[trial]] += lr_neg * rpe_1
            
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3: OCI-Dependent Eligibility Trace (Lambda)
This model proposes that OCI affects the "credit assignment" process across the two stages. The eligibility trace parameter ($\lambda$) controls how much the second-stage reward updates the first-stage choice directly. A higher $\lambda$ links outcomes more strongly to the initial choice. High OCI might involve rigid, long-range credit assignment (high $\lambda$) or disconnected steps (low $\lambda$).

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free Learner with OCI-Dependent Eligibility Trace (Lambda).
    
    The eligibility trace parameter lambda determines how much the reward received
    at the second stage reinforces the first stage choice directly.
    Lambda is modeled as a function of the OCI score.
    
    Parameters:
    lr : [0, 1] Learning rate.
    beta : [0, 10] Inverse temperature.
    lambda_base : [0, 1] Base eligibility trace parameter.
    lambda_oci_slope : [-1, 1] How OCI modulates lambda.
                       lambda = lambda_base + lambda_oci_slope * oci
    """
    lr, beta, lambda_base, lambda_oci_slope = model_parameters
    n_trials = len(action_1)
    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    
    current_oci = oci[0]
    
    # Calculate effective lambda
    lam = lambda_base + lambda_oci_slope * current_oci
    lam = np.clip(lam, 0.0, 1.0)
    
    q1_mf = np.zeros(2)
    q2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Choice ---
        curr_state = state[trial]
        exp_q2 = np.exp(beta * q2_mf[curr_state])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning (TD(lambda) style logic for 2-step) ---
        
        # 1. Prediction error at stage 2
        # rpe_2 = Reward - Q2(s2, a2)
        rpe_2 = reward[trial] - q2_mf[curr_state, action_2[trial]]
        
        # 2. Prediction error at stage 1
        # rpe_1 = Q2(s2, a2) - Q1(s1, a1)
        rpe_1 = q2_mf[curr_state, action_2[trial]] - q1_mf[action_1[trial]]
        
        # Update Stage 2
        q2_mf[curr_state, action_2[trial]] += lr * rpe_2
        
        # Update Stage 1
        # Q1 is updated by its own prediction error (rpe_1) 
        # PLUS the discounted prediction error from stage 2 (lam * rpe_2)
        q1_mf[action_1[trial]] += lr * (rpe_1 + lam * rpe_2)
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```