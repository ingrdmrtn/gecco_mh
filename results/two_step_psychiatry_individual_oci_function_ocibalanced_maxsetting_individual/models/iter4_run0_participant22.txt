Here are three new cognitive models that explore different mechanisms by which Obsessive-Compulsive symptoms (OCI) might influence decision-making in this two-step task, specifically focusing on how habits and model-based control interact.

### Cognitive Model 1: OCI-Modulated Eligibility Trace (Habit Strength)
This model hypothesizes that individuals with higher OCI scores form stronger "habits" or associations between the first-stage choice and the final reward, bypassing the second-stage state structure. This is implemented via an eligibility trace parameter ($\lambda$) that is modulated by OCI. A higher $\lambda$ means the first-stage value is updated more strongly by the second-stage reward directly, reinforcing simple stimulus-response habits over the model-based transition structure.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Eligibility Trace.
    
    Hypothesis: Higher OCI leads to stronger habit formation, represented by a higher
    eligibility trace (lambda). This means the first-stage choice is more directly
    reinforced by the final outcome, ignoring the transition structure (Model-Free).
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    lambda_base: [0, 1]
    lambda_oci_sens: [0, 1]
    """
    learning_rate, beta, lambda_base, lambda_oci_sens = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    # Eligibility trace lambda increases with OCI (more habit-like)
    # We clip to ensure it stays within valid bounds [0, 1]
    eligibility_lambda = np.clip(lambda_base + lambda_oci_sens * current_oci, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
             p_choice_1[trial] = 0.5
             p_choice_2[trial] = 0.5
             continue

        # Stage 1 Policy (Purely Model-Free in this formulation, but with trace)
        # Note: Standard 2-step models often mix MB/MF. Here we focus on how the MF
        # component itself is updated via lambda. To keep it comparable to standard
        # hybrid models, we can assume a fixed weighting or just use MF values here
        # to isolate the trace effect. For simplicity and to fit the template structure
        # which implies a mix, we calculate MB values but let the decision be driven
        # by the MF values which are heavily influenced by the trace.
        
        # However, to make the lambda meaningful, we usually need a hybrid model.
        # Let's assume a fixed mixing weight w=0.5 to allow the trace effect to shine,
        # or better yet, let the decision be purely MF to see habit strength directly.
        # Given the template asks to fill in logic, let's implement a standard hybrid
        # where the MF update is the key innovation.
        
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Fixed mixing for this model to isolate lambda effect
        w = 0.5 
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        act1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[act1]
        state_idx = int(state[trial])

        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        act2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[act2]

        # Updates
        # Stage 2 RPE
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, act2]
        q_stage2_mf[state_idx, act2] += learning_rate * delta_stage2
        
        # Stage 1 RPE
        # The immediate reward at stage 1 is 0.
        delta_stage1 = q_stage2_mf[state_idx, act2] - q_stage1_mf[act1]
        
        # The crucial part: The Stage 1 value is updated by both the transition prediction error
        # AND the stage 2 prediction error, scaled by lambda.
        # Q1(s1, a1) <- Q1(s1, a1) + alpha * (delta_stage1 + lambda * delta_stage2)
        q_stage1_mf[act1] += learning_rate * (delta_stage1 + eligibility_lambda * delta_stage2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2: OCI-Driven "Slip" or Lapse Rate
This model posits that OCI is related to attentional lapses or "slips" in action execution, rather than just value learning. Even if the participant knows the optimal action (high Beta), they might randomly pick a different action due to intrusive thoughts or compulsions that disengage them from the task structure. We model this as a mixture of the softmax policy and a random uniform policy, where the weight of the random policy ($\epsilon$) scales with OCI.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Driven Lapse Rate.
    
    Hypothesis: High OCI contributes to decision noise or 'lapses' (epsilon-greedy-like behavior)
    on top of a standard hybrid learning process. The probability of making a random choice 
    increases with OCI score.
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    w: [0, 1]
    epsilon_scale: [0, 0.5] (Scaling factor for random noise)
    """
    learning_rate, beta, w, epsilon_scale = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    # Epsilon (lapse rate) scales with OCI. 
    # If OCI=0.6 and scale=0.2, epsilon=0.12.
    # We cap epsilon at 0.5 (random chance).
    epsilon = np.clip(epsilon_scale * current_oci, 0.0, 0.5)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
             p_choice_1[trial] = 0.5
             p_choice_2[trial] = 0.5
             continue

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf

        exp_q1 = np.exp(beta * q_net)
        softmax_probs_1 = exp_q1 / np.sum(exp_q1)
        
        # Mixture: (1-eps) * Softmax + eps * Random(0.5)
        probs_1 = (1 - epsilon) * softmax_probs_1 + epsilon * 0.5
        
        act1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[act1]
        state_idx = int(state[trial])

        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        softmax_probs_2 = exp_q2 / np.sum(exp_q2)
        
        # Apply lapse to second stage as well
        probs_2 = (1 - epsilon) * softmax_probs_2 + epsilon * 0.5
        
        act2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[act2]

        # Updates (Standard Hybrid)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, act2]
        q_stage2_mf[state_idx, act2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[state_idx, act2] - q_stage1_mf[act1]
        q_stage1_mf[act1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3: OCI-Dependent Learning Rate Asymmetry
This model tests the idea that OCI is associated with an over-sensitivity to negative outcomes (or lack of reward) compared to positive ones. Often, compulsive behavior is driven by fear of negative consequences. Here, we split the learning rate into two: one for positive prediction errors and one for negative prediction errors. The ratio or balance between these is modulated by the OCI score.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Dependent Learning Rate Asymmetry.
    
    Hypothesis: OCI modulates the balance between learning from positive vs. negative prediction errors.
    High OCI might lead to higher learning rates for negative outcomes (avoidance learning).
    
    Bounds:
    lr_base: [0, 1]
    beta: [0, 10]
    w: [0, 1]
    neg_bias_oci: [0, 5] (Multiplier for negative learning rate based on OCI)
    """
    lr_base, beta, w, neg_bias_oci = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
             p_choice_1[trial] = 0.5
             p_choice_2[trial] = 0.5
             continue

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        act1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[act1]
        state_idx = int(state[trial])

        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        act2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[act2]

        # Determine Learning Rates
        # alpha_pos = base
        # alpha_neg = base * (1 + bias * oci)
        # If bias is high, negative PEs cause larger updates.
        lr_pos = lr_base
        lr_neg = np.clip(lr_base * (1.0 + neg_bias_oci * current_oci), 0.0, 1.0)

        # Updates
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, act2]
        
        if delta_stage2 >= 0:
            q_stage2_mf[state_idx, act2] += lr_pos * delta_stage2
        else:
            q_stage2_mf[state_idx, act2] += lr_neg * delta_stage2
        
        delta_stage1 = q_stage2_mf[state_idx, act2] - q_stage1_mf[act1]
        
        if delta_stage1 >= 0:
            q_stage1_mf[act1] += lr_pos * delta_stage1
        else:
            q_stage1_mf[act1] += lr_neg * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```