def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based / Model-Free RL with OCI-modulated Inverse Temperature.
    
    This model assumes a mixture of Goal-Directed (MB) and Habitual (MF) control.
    The OCI score modulates the 'beta' (inverse temperature), testing if obsessive-compulsive
    symptoms lead to more deterministic (rigid) or more stochastic (anxious) choice behavior
    within a hybrid framework.
    
    Parameters:
    lr: [0, 1] Learning rate for MF updates.
    beta_base: [0, 10] Baseline inverse temperature.
    beta_oci_slope: [-10, 10] Effect of OCI on beta.
    w: [0, 1] Mixing weight (0=MF, 1=MB).
    stickiness: [0, 5] Choice persistence bonus.
    """
    lr, beta_base, beta_oci_slope, w, stickiness = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate OCI-modulated beta
    beta = beta_base + (beta_oci_slope * oci_score)
    if beta < 0: beta = 0.0
    
    # Fixed transition matrix for MB planning (0.7 common, 0.3 rare)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_mf_stage1 = np.ones(2) * 0.5
    q_mf_stage2 = np.ones((2, 2)) * 0.5
    
    last_action_1 = -1
    log_loss = 0.0
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        if a1 == -1:
            last_action_1 = -1
            continue
            
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Stage 1 Choice ---
        # Model-Based Value: max Q2 weighted by transition probs
        max_q_stage2 = np.max(q_mf_stage2, axis=1)
        q_mb_stage1 = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net_1 = w * q_mb_stage1 + (1 - w) * q_mf_stage1
        
        # Add stickiness
        if last_action_1 != -1:
            q_net_1[last_action_1] += stickiness
            
        # Softmax
        exp_q1 = np.exp(beta * q_net_1)
        if np.sum(exp_q1) == 0: exp_q1 = np.ones(2)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        log_loss -= np.log(probs_1[a1] + 1e-10)
        
        # --- Stage 2 Choice ---
        # Pure Model-Free choice at stage 2
        exp_q2 = np.exp(beta * q_mf_stage2[s])
        if np.sum(exp_q2) == 0: exp_q2 = np.ones(2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        log_loss -= np.log(probs_2[a2] + 1e-10)
        
        # --- Updates ---
        # Stage 2 MF update
        delta2 = r - q_mf_stage2[s, a2]
        q_mf_stage2[s, a2] += lr * delta2
        
        # Stage 1 MF update (SARSA-like connection to stage 2 value)
        delta1 = q_mf_stage2[s, a2] - q_mf_stage1[a1]
        q_mf_stage1[a1] += lr * delta1
        
        last_action_1 = a1
        
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based / Model-Free RL with OCI-modulated Learning Rate.
    
    This model assumes OCI symptoms affect the plasticity of the system.
    High OCI might lead to over-updating (chasing noise) or under-updating (rigidity).
    The learning rate is modulated linearly by the OCI score.
    
    Parameters:
    lr_base: [0, 1] Baseline learning rate.
    lr_oci_slope: [-1, 1] Effect of OCI on learning rate.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Mixing weight (0=MF, 1=MB).
    stickiness: [0, 5] Choice persistence bonus.
    """
    lr_base, lr_oci_slope, beta, w, stickiness = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate OCI-modulated learning rate
    lr = lr_base + (lr_oci_slope * oci_score)
    # Clip to valid range [0, 1]
    if lr < 0: lr = 0.0
    if lr > 1: lr = 1.0
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_mf_stage1 = np.ones(2) * 0.5
    q_mf_stage2 = np.ones((2, 2)) * 0.5
    
    last_action_1 = -1
    log_loss = 0.0
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        if a1 == -1:
            last_action_1 = -1
            continue
            
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_mf_stage2, axis=1)
        q_mb_stage1 = transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_mb_stage1 + (1 - w) * q_mf_stage1
        
        if last_action_1 != -1:
            q_net_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_net_1)
        if np.sum(exp_q1) == 0: exp_q1 = np.ones(2)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        log_loss -= np.log(probs_1[a1] + 1e-10)
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_mf_stage2[s])
        if np.sum(exp_q2) == 0: exp_q2 = np.ones(2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        log_loss -= np.log(probs_2[a2] + 1e-10)
        
        # --- Updates ---
        # Use the modulated learning rate 'lr'
        delta2 = r - q_mf_stage2[s, a2]
        q_mf_stage2[s, a2] += lr * delta2
        
        delta1 = q_mf_stage2[s, a2] - q_mf_stage1[a1]
        q_mf_stage1[a1] += lr * delta1
        
        last_action_1 = a1
        
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based / Model-Free RL with OCI-modulated Transition Belief.
    
    This model assumes that OCI symptoms correlate with 'doubt' or uncertainty about
    the environmental structure. While the true transition probability is 0.7,
    high OCI participants might perceive the world as more random (probability closer to 0.5).
    This degrades the accuracy of the Model-Based planner.
    
    Parameters:
    lr: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Mixing weight (0=MF, 1=MB).
    stickiness: [0, 5] Choice persistence bonus.
    uncertainty_slope: [0, 1] How much OCI increases transition uncertainty.
    """
    lr, beta, w, stickiness, uncertainty_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate perceived transition probability
    # True p is 0.7. Uncertainty pushes it towards 0.5.
    # p = 0.7 - (0.2 * effect) where effect is in [0, 1]
    uncertainty_effect = uncertainty_slope * oci_score
    if uncertainty_effect > 1.0: uncertainty_effect = 1.0
    if uncertainty_effect < 0.0: uncertainty_effect = 0.0
    
    p_common = 0.7 - (0.2 * uncertainty_effect)
    p_rare = 1.0 - p_common
    
    # Perceived transition matrix
    transition_matrix = np.array([[p_common, p_rare], [p_rare, p_common]])
    
    q_mf_stage1 = np.ones(2) * 0.5
    q_mf_stage2 = np.ones((2, 2)) * 0.5
    
    last_action_1 = -1
    log_loss = 0.0
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        if a1 == -1:
            last_action_1 = -1
            continue
            
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_mf_stage2, axis=1)
        # MB planning uses the OCI-distorted transition matrix
        q_mb_stage1 = transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_mb_stage1 + (1 - w) * q_mf_stage1
        
        if last_action_1 != -1:
            q_net_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_net_1)
        if np.sum(exp_q1) == 0: exp_q1 = np.ones(2)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        log_loss -= np.log(probs_1[a1] + 1e-10)
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_mf_stage2[s])
        if np.sum(exp_q2) == 0: exp_q2 = np.ones(2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        log_loss -= np.log(probs_2[a2] + 1e-10)
        
        # --- Updates ---
        delta2 = r - q_mf_stage2[s, a2]
        q_mf_stage2[s, a2] += lr * delta2
        
        delta1 = q_mf_stage2[s, a2] - q_mf_stage1[a1]
        q_mf_stage1[a1] += lr * delta1
        
        last_action_1 = a1
        
    return log_loss