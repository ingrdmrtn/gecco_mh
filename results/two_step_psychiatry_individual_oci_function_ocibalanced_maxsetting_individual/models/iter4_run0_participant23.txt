Here are the 3 new cognitive models, expressed as Python functions.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Pessimistic Planning Model.
    
    Hypothesis: OCI symptoms correlate with anxiety and risk aversion. In the Model-Based 
    planning step, instead of assuming the agent will take the best action at the second 
    stage (max Q), high OCI participants might consider the worst-case scenario (min Q) 
    or a mixture, reflecting a 'pessimistic' evaluation of future states.
    
    Parameters:
    - learning_rate: [0, 1] Value updating rate.
    - beta: [0, 10] Inverse temperature (exploration/exploitation).
    - w: [0, 1] Mixing weight (0=MF, 1=MB).
    - stickiness: [0, 5] Choice stickiness.
    - pessimism_base: [0, 1] Baseline pessimism (weight on min_Q).
    - pessimism_oci: [-2, 2] Modulation of pessimism by OCI score.
    """
    learning_rate, beta, w, stickiness, pessimism_base, pessimism_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1
    
    for trial in range(n_trials):
        if action_1[trial] == -1:
            p_choice_1[trial] = 1.0 # Placeholder for invalid trials
            p_choice_2[trial] = 1.0
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Calculate Pessimism
        pessimism = pessimism_base + pessimism_oci * oci_score
        pessimism = np.clip(pessimism, 0.0, 1.0)
        
        # Model-Based Value Calculation with Pessimism
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        min_q_stage2 = np.min(q_stage2_mf, axis=1)
        
        # Weighted average of best and worst case
        val_stage2 = (1 - pessimism) * max_q_stage2 + pessimism * min_q_stage2
        q_stage1_mb = transition_matrix @ val_stage2
        
        # Net Q-value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Stickiness
        stick_bonus = np.zeros(2)
        if prev_action_1 != -1:
            stick_bonus[prev_action_1] = stickiness
            
        # Stage 1 Choice Probabilities
        logits_1 = beta * q_net + stick_bonus
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Stage 2 Choice Probabilities
        logits_2 = beta * q_stage2_mf[s_idx]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # Learning
        # Update Stage 1 MF
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Update Stage 2 MF
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        prev_action_1 = a1
        
    eps = 1e-10
    valid_mask = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Stage 1 Decision Noise Model.
    
    Hypothesis: OCI symptoms specifically affect the decision noise (inverse temperature) 
    at the first stage of the task. High OCI might lead to more rigid (high beta) or 
    more erratic (low beta) spaceship choices, independent of the learning rate or 
    second-stage behavior.
    
    Parameters:
    - learning_rate: [0, 1] Value updating rate.
    - w: [0, 1] Mixing weight (MB vs MF).
    - stickiness: [0, 5] Choice stickiness.
    - beta_2: [0, 10] Inverse temperature for Stage 2 (Alien choice).
    - beta_1_base: [0, 10] Baseline inverse temperature for Stage 1.
    - beta_1_oci: [-5, 5] Modulation of Stage 1 beta by OCI.
    """
    learning_rate, w, stickiness, beta_2, beta_1_base, beta_1_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1
    
    # Calculate OCI-modulated Beta 1
    beta_1 = beta_1_base + beta_1_oci * oci_score
    beta_1 = np.clip(beta_1, 0.0, 20.0) # Allow some range flexibility, clip to non-negative
    
    for trial in range(n_trials):
        if action_1[trial] == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        stick_bonus = np.zeros(2)
        if prev_action_1 != -1:
            stick_bonus[prev_action_1] = stickiness
            
        # Stage 1 Choice (Modulated Beta 1)
        logits_1 = beta_1 * q_net + stick_bonus
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Stage 2 Choice (Fixed Beta 2)
        logits_2 = beta_2 * q_stage2_mf[s_idx]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        prev_action_1 = a1

    eps = 1e-10
    valid_mask = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Positive Learning Rate Model.
    
    Hypothesis: OCI symptoms modulate the learning rate specifically for positive outcomes 
    (reward = 1), reflecting altered sensitivity to success or safety. High OCI might 
    cause participants to over-learn from success (relief) or under-learn (anhedonia).
    
    Parameters:
    - lr_neg: [0, 1] Learning rate for non-rewarded trials (loss/omission).
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight (MB vs MF).
    - stickiness: [0, 5] Choice stickiness.
    - lr_pos_base: [0, 1] Baseline learning rate for rewarded trials.
    - lr_pos_oci: [-1, 1] Modulation of positive learning rate by OCI.
    """
    lr_neg, beta, w, stickiness, lr_pos_base, lr_pos_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1
    
    for trial in range(n_trials):
        if action_1[trial] == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        stick_bonus = np.zeros(2)
        if prev_action_1 != -1:
            stick_bonus[prev_action_1] = stickiness
            
        logits_1 = beta * q_net + stick_bonus
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        logits_2 = beta * q_stage2_mf[s_idx]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # Determine Learning Rate based on Outcome
        if r == 1:
            lr = lr_pos_base + lr_pos_oci * oci_score
        else:
            lr = lr_neg
        
        lr = np.clip(lr, 0.0, 1.0)
        
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_stage2
        
        prev_action_1 = a1

    eps = 1e-10
    valid_mask = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss
```