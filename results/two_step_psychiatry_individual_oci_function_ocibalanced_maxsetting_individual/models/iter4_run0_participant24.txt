Here are three new cognitive models, expressed as Python functions, that hypothesize different mechanisms for how OCI symptoms might influence decision-making in this task.

### Model 1: Eligibility Trace Modulation
This model tests the hypothesis that OCI modulates the **eligibility trace parameter ($\lambda$)**. An eligibility trace controls how much the outcome of the second stage (reward) directly updates the value of the first-stage choice, bypassing the step-by-step chain. High OCI might lead to a higher reliance on direct outcome-response associations (higher $\lambda$) or a more compartmentalized learning style (lower $\lambda$).

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free learner where OCI modulates the eligibility trace (lambda).
    
    Hypothesis: OCI scores relate to the efficiency of credit assignment. 
    A higher eligibility trace (lambda) allows the second-stage outcome to directly 
    reinforce the first-stage choice.
    
    Parameters:
    - learning_rate: [0, 1] Value learning rate.
    - beta: [0, 10] Inverse temperature (exploration/exploitation).
    - w: [0, 1] Weight for Model-Based (1) vs Model-Free (0).
    - lambda_base: [0, 1] Base eligibility trace.
    - lambda_oci_slope: [-1, 1] Modulation of lambda by OCI.
    """
    learning_rate, beta, w, lambda_base, lambda_oci_slope = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]
    
    # Calculate lambda and clip to valid range [0, 1]
    lambda_param = lambda_base + lambda_oci_slope * participant_oci
    lambda_param = np.clip(lambda_param, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)      # Model-free values for Stage 1
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for Stage 2
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        if action_1[trial] == -1 or action_2[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        # Stage 1 Prediction Error (TD(0))
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Stage 2 Prediction Error
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Eligibility Trace Update for Stage 1 (TD(lambda))
        # The trace allows the stage 2 error to propagate back to stage 1
        q_stage1_mf[a1] += learning_rate * lambda_param * delta_stage2
        
    eps = 1e-10
    # Filter out 0 probabilities to avoid log(0)
    log_loss = -(np.sum(np.log(p_choice_1[p_choice_1 > 0] + eps)) + 
                 np.sum(np.log(p_choice_2[p_choice_2 > 0] + eps)))
    return log_loss
```

### Model 2: Subjective Penalty for Omission
This model tests the hypothesis that OCI is related to perfectionism or an increased sensitivity to negative outcomes (omission of reward). Instead of treating 0 coins as a neutral outcome, the model assigns a **subjective penalty** (negative reward) to 0 outcomes, the magnitude of which scales with OCI.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid learner where OCI modulates subjective punishment for omission of reward.
    
    Hypothesis: High OCI is associated with perfectionism or fear of failure.
    Receiving 0 coins is perceived not just as neutral, but as a penalty (negative reward),
    altering the value landscape.
    
    Parameters:
    - learning_rate: [0, 1]
    - beta: [0, 10]
    - w: [0, 1]
    - penalty_base: [0, 5] Base magnitude of penalty for 0 reward.
    - penalty_oci_slope: [-5, 5] Effect of OCI on penalty magnitude.
    """
    learning_rate, beta, w, penalty_base, penalty_oci_slope = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]
    
    # Calculate penalty magnitude and ensure it is non-negative
    penalty_mag = penalty_base + penalty_oci_slope * participant_oci
    penalty_mag = np.maximum(penalty_mag, 0.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        if action_1[trial] == -1 or action_2[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Effective reward calculation:
        # If reward is 0, the agent perceives it as -penalty_mag
        if r == 1:
            r_eff = 1.0
        else:
            r_eff = -penalty_mag
        
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r_eff - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1[p_choice_1 > 0] + eps)) + 
                 np.sum(np.log(p_choice_2[p_choice_2 > 0] + eps)))
    return log_loss
```

### Model 3: Dynamic Transition Learning
This model tests the hypothesis that OCI affects the **rigidity of beliefs** about the environment. While the standard model assumes a fixed transition matrix, this model assumes the participant learns the transition probabilities over time. OCI modulates the learning rate for these transitions, determining whether beliefs are rigid (low learning rate) or volatile (high learning rate).

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid learner where OCI modulates the learning rate of the transition matrix.
    
    Hypothesis: OCI affects the rigidity of beliefs about environmental structure.
    The participant updates their belief about spaceship-planet transition probabilities 
    based on experience, with the rate of update modulated by OCI.
    
    Parameters:
    - learning_rate: [0, 1] Value learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] MB/MF mixing weight.
    - lr_trans_base: [0, 1] Base learning rate for transition matrix.
    - lr_trans_oci_slope: [-1, 1] Effect of OCI on transition learning rate.
    """
    learning_rate, beta, w, lr_trans_base, lr_trans_oci_slope = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]
    
    # Calculate transition learning rate and clip
    lr_trans = lr_trans_base + lr_trans_oci_slope * participant_oci
    lr_trans = np.clip(lr_trans, 0.0, 1.0)
    
    # Initialize transition matrix (rows: actions, cols: states)
    # Start with the common assumption: A(0)->X(0) 0.7, U(1)->Y(1) 0.7
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        if action_1[trial] == -1 or action_2[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Stage 1 Policy (using dynamic transition matrix) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Value Updates ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # --- Transition Matrix Update ---
        # Update belief about where action a1 leads
        # Construct target vector: [1, 0] if state 0 observed, [0, 1] if state 1 observed
        target = np.zeros(2)
        target[s_idx] = 1.0
        
        # Move transition probabilities towards the observed outcome
        transition_matrix[a1] += lr_trans * (target - transition_matrix[a1])
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1[p_choice_1 > 0] + eps)) + 
                 np.sum(np.log(p_choice_2[p_choice_2 > 0] + eps)))
    return log_loss
```