Here are three new cognitive models exploring different mechanisms for how OCI scores might influence decision-making in this two-step task, specifically focusing on the balance between model-based and model-free control, and sensitivity to rewards versus punishments.

### Cognitive Model 1: OCI-Modulated Model-Based/Model-Free Hybrid
This model hypothesizes that individuals with higher OCI scores might rely more on habitual (model-free) control rather than goal-directed (model-based) planning, perhaps due to cognitive rigidity or a reliance on "safe" routines. The weighting parameter `w` shifts based on the OCI score.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free RL with OCI-modulated weighting.
    
    Hypothesis: OCI score influences the balance (w) between Model-Based (planning)
    and Model-Free (habitual) systems. Higher OCI might lead to more Model-Free 
    reliance (lower w).
    
    Parameters:
    learning_rate: [0, 1] - Learning rate for value updates.
    beta: [0, 10] - Inverse temperature for softmax choice.
    w_max: [0, 1] - Maximum weight for Model-Based control (at OCI=0).
    oci_w_decay: [0, 5] - Rate at which MB weight decays as OCI increases.
    """
    learning_rate, beta, w_max, oci_w_decay = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate mixing weight w based on OCI
    # As OCI increases, w decreases (more Model-Free)
    w = w_max / (1.0 + oci_w_decay * oci_score)
    w = np.clip(w, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2) # Model-Free Stage 1
    q_stage2_mf = np.zeros((2, 2)) # Model-Free Stage 2 (common for both MB and MF)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        
        # --- Stage 1 Policy ---
        # Model-Based Value: V_MB(s1) = T(s1, s2) * max(Q_stage2(s2, :))
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Value: Q_net = w * Q_MB + (1-w) * Q_MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        if a1 != -1:
            p_choice_1[trial] = probs_1[a1]
        else:
            p_choice_1[trial] = 1.0

        # --- Stage 2 Policy ---
        s_idx = int(state[trial])
        if s_idx != -1:
            exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
            probs_2 = exp_q2 / np.sum(exp_q2)
            
            a2 = int(action_2[trial])
            if a2 != -1:
                p_choice_2[trial] = probs_2[a2]
                
                r = reward[trial]
                if r == -1: r = 0 # Handle missing reward coding if present

                # --- Updates ---
                # Stage 2 TD Error
                delta_stage2 = r - q_stage2_mf[s_idx, a2]
                q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
                
                # Stage 1 TD Error (Model-Free update)
                if a1 != -1:
                    delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
                    q_stage1_mf[a1] += learning_rate * delta_stage1
            else:
                p_choice_2[trial] = 1.0
        else:
            p_choice_2[trial] = 1.0

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2: OCI-Driven Learning Rate Asymmetry
This model focuses on how OCI symptoms might relate to an altered sensitivity to prediction errors. Specifically, it tests if higher OCI scores lead to different learning rates for positive versus negative outcomes (confirmatory vs. disconfirmatory evidence), potentially reflecting a bias towards learning from mistakes or failures.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free RL with OCI-modulated asymmetric learning rates.
    
    Hypothesis: OCI score modulates the ratio between learning from positive prediction errors
    (better than expected) vs negative prediction errors (worse than expected).
    
    Parameters:
    lr_base: [0, 1] - Baseline learning rate.
    beta: [0, 10] - Inverse temperature.
    asymmetry_factor: [0, 5] - How much OCI biases learning towards negative PEs.
    """
    lr_base, beta, asymmetry_factor = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Define effective learning rates
    # If asymmetry_factor * oci > 0, lr_neg becomes larger than lr_pos
    # This simulates hypersensitivity to negative outcomes/failures common in anxiety/OCD
    lr_pos = lr_base
    lr_neg = np.clip(lr_base * (1.0 + asymmetry_factor * oci_score), 0.0, 1.0)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        
        # Stage 1 Choice
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        if a1 != -1:
            p_choice_1[trial] = probs_1[a1]
        else:
            p_choice_1[trial] = 1.0
            
        # Stage 2 Choice
        s_idx = int(state[trial])
        if s_idx != -1:
            exp_q2 = np.exp(beta * q_stage2[s_idx])
            probs_2 = exp_q2 / np.sum(exp_q2)
            
            a2 = int(action_2[trial])
            if a2 != -1:
                p_choice_2[trial] = probs_2[a2]
                
                r = reward[trial]
                if r == -1: r = 0 

                # Stage 2 Update
                delta2 = r - q_stage2[s_idx, a2]
                alpha2 = lr_pos if delta2 >= 0 else lr_neg
                q_stage2[s_idx, a2] += alpha2 * delta2
                
                # Stage 1 Update (TD-0)
                if a1 != -1:
                    delta1 = q_stage2[s_idx, a2] - q_stage1[a1]
                    alpha1 = lr_pos if delta1 >= 0 else lr_neg
                    q_stage1[a1] += alpha1 * delta1
            else:
                p_choice_2[trial] = 1.0
        else:
            p_choice_2[trial] = 1.0

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3: OCI-Scaled Inverse Temperature (Exploration/Exploitation)
This model posits that OCI affects the decision noise or exploration-exploitation trade-off directly. Individuals with higher OCI might exhibit more deterministic behavior (higher beta) to reduce uncertainty, or conversely, more erratic behavior (lower beta) if they are unable to settle on a strategy.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free RL with OCI-dependent Inverse Temperature (Beta).
    
    Hypothesis: OCI score fundamentally alters the exploration-exploitation balance.
    High OCI might lead to higher beta (sticking rigidly to perceived best options)
    or lower beta (indecisiveness). Here we model it as an additive effect.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate.
    beta_intercept: [0, 10] - Baseline inverse temperature.
    oci_beta_slope: [-5, 5] - Change in beta per unit of OCI.
    """
    learning_rate, beta_intercept, oci_beta_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective beta
    # Can increase or decrease with OCI depending on sign of slope
    beta_eff = beta_intercept + (oci_beta_slope * oci_score)
    # Ensure beta stays within reasonable positive bounds
    beta_eff = np.clip(beta_eff, 0.0, 20.0) 

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        
        # Stage 1
        exp_q1 = np.exp(beta_eff * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        if a1 != -1:
            p_choice_1[trial] = probs_1[a1]
        else:
            p_choice_1[trial] = 1.0
            
        # Stage 2
        s_idx = int(state[trial])
        if s_idx != -1:
            exp_q2 = np.exp(beta_eff * q_stage2[s_idx])
            probs_2 = exp_q2 / np.sum(exp_q2)
            
            a2 = int(action_2[trial])
            if a2 != -1:
                p_choice_2[trial] = probs_2[a2]
                
                r = reward[trial]
                if r == -1: r = 0

                # Update Stage 2
                delta2 = r - q_stage2[s_idx, a2]
                q_stage2[s_idx, a2] += learning_rate * delta2
                
                # Update Stage 1
                if a1 != -1:
                    delta1 = q_stage2[s_idx, a2] - q_stage1[a1]
                    q_stage1[a1] += learning_rate * delta1
            else:
                p_choice_2[trial] = 1.0
        else:
            p_choice_2[trial] = 1.0

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```