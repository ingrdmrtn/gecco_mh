Here are the 3 new cognitive models based on the participant's data and OCI score.

### Cognitive Model 1: Hybrid Model with OCI-Modulated Mixing Weight (w)
This model posits that the balance between Model-Based (planning) and Model-Free (habit) strategies is influenced by the participant's OCI score. Specifically, it tests the hypothesis that OCI symptoms might drive a reliance on more habitual (Model-Free) or more deliberative (Model-Based) control. The mixing weight `w` determines the contribution of the Model-Based system to the final value estimation.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free learner where the mixing weight (w) is a logistic
    function of the OCI score.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for value updates.
    - beta: [0, 10] Inverse temperature for softmax choice.
    - w_intercept: [-5, 5] Intercept for the logistic function determining w.
    - w_slope: [-5, 5] Slope for the logistic function determining w based on OCI.
    """
    learning_rate, beta, w_intercept, w_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate mixing weight w using a logistic function to keep it in [0, 1]
    # w = 1 means fully Model-Based, w = 0 means fully Model-Free
    w_logit = w_intercept + w_slope * oci_score
    w = 1.0 / (1.0 + np.exp(-w_logit))

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)      # Model-Free Q-values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Model-Free Q-values for stage 2 (same as MB terminal values)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based value calculation: expected max value of next stage
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid value: weighted sum of MB and MF values
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Choice ---
        s_idx = int(state[trial])
        
        # Standard Model-Free choice at stage 2
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        r = reward[trial]
        
        # Update Stage 2 values (SARSA / Q-learning equivalent here since terminal)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Update Stage 1 MF values (TD(0))
        # Note: In pure hybrid models, Stage 1 MF is often updated via TD error using Stage 2 value
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2: Perseveration (Stickiness) Modulated by OCI
This model investigates if the participant's OCI score predicts a tendency to repeat previous choices ("stickiness"), regardless of reward outcomes. This is a common feature in compulsive behaviors. The model adds a "stickiness" bonus to the Q-values of the previously chosen action, with the magnitude of this bonus scaled by the OCI score.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner with choice perseveration (stickiness) where the 
    stickiness parameter is modulated by OCI.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - stick_base: [-2, 2] Base level of choice perseveration.
    - oci_stick_mod: [-2, 2] How much OCI score modulates the stickiness.
    """
    learning_rate, beta, stick_base, oci_stick_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective stickiness
    stickiness = stick_base + (oci_score * oci_stick_mod)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    # Track previous choice for stage 1 (initialized to -1 for none)
    prev_a1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Add stickiness bonus to the Q-values before softmax
        q_stage1_effective = q_stage1.copy()
        if prev_a1 != -1:
            q_stage1_effective[prev_a1] += stickiness
            
        exp_q1 = np.exp(beta * q_stage1_effective)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # Update previous choice
        prev_a1 = a1

        # --- Stage 2 Choice ---
        s_idx = int(state[trial])
        
        # No stickiness implemented for stage 2 in this simple variant
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        r = reward[trial]
        
        # Standard TD(1) / SARSA updates
        delta_2 = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += learning_rate * delta_2
        
        # Using a simple TD(1) approach for stage 1 update based on reward
        # (Assuming lambda=1 for direct reward sensitivity)
        delta_1 = r - q_stage1[a1] 
        q_stage1[a1] += learning_rate * delta_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3: Asymmetric Learning Rates Modulated by OCI
This model tests if OCI symptoms relate to an imbalance in learning from positive versus negative prediction errors. For example, individuals with higher compulsion might be more sensitive to punishments (loss of reward) or less sensitive to gains. Here, the learning rate for negative prediction errors (`alpha_neg`) is derived from a base rate plus an OCI-dependent modulation, while the positive learning rate (`alpha_pos`) is fixed.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner with asymmetric learning rates for positive and negative
    prediction errors. The negative learning rate is modulated by OCI.
    
    Parameters:
    - alpha_pos: [0, 1] Learning rate for positive prediction errors.
    - alpha_neg_base: [0, 1] Base learning rate for negative prediction errors.
    - oci_neg_mod: [-1, 1] Modulation of negative learning rate by OCI.
    - beta: [0, 10] Inverse temperature.
    """
    alpha_pos, alpha_neg_base, oci_neg_mod, beta = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective negative learning rate
    alpha_neg = alpha_neg_base + (oci_score * oci_neg_mod)
    alpha_neg = np.clip(alpha_neg, 0.0, 1.0)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Choice ---
        s_idx = int(state[trial])
        
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        r = reward[trial]
        
        # Stage 2 Update
        delta_2 = r - q_stage2[s_idx, a2]
        lr_2 = alpha_pos if delta_2 > 0 else alpha_neg
        q_stage2[s_idx, a2] += lr_2 * delta_2
        
        # Stage 1 Update (using TD(0) logic: update towards Stage 2 value)
        delta_1 = q_stage2[s_idx, a2] - q_stage1[a1]
        lr_1 = alpha_pos if delta_1 > 0 else alpha_neg
        q_stage1[a1] += lr_1 * delta_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```