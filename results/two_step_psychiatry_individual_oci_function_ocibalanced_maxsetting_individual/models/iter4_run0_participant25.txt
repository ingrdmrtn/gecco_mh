Here are three new cognitive models for the two-step task, incorporating OCI scores to explain individual variability.

### Model 1: Asymmetric Perseveration (Win-Stay vs. Lose-Stay)
This model hypothesizes that OCI symptoms affect "stickiness" (perseveration) differently depending on the previous outcome. While standard perseveration assumes a constant bias to repeat the last action, this model allows for distinct biases after rewards (Win-Stay) versus omissions (Lose-Stay). High OCI might lead to "compulsive" repetition specifically after losses (inability to switch) or heightened adherence after wins.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: Asymmetric Perseveration (Win-Stay vs. Lose-Stay) with OCI Modulation.
    
    Hypothesis: OCI affects the tendency to repeat an action specifically after a loss 
    (compulsive repetition) or win, distinct from value-based learning.
    
    Parameters:
    - learning_rate: [0, 1] Value learning rate.
    - beta: [0, 10] Inverse temperature (softness of choice).
    - w: [0, 1] Mixing weight (0=MF, 1=MB).
    - stick_win: [0, 5] Perseveration bonus added to logits after a rewarded trial.
    - stick_loss_base: [0, 5] Baseline perseveration bonus after an unrewarded trial.
    - stick_loss_oci: [-5, 5] OCI modulation of stickiness after loss.
      Effective stick_loss = stick_loss_base + stick_loss_oci * oci.
    """
    learning_rate, beta, w, stick_win, stick_loss_base, stick_loss_oci = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate effective stickiness for loss trials based on OCI
    stick_loss = stick_loss_base + (stick_loss_oci * current_oci)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1
    prev_r = -1
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])
        
        # --- Stage 1 Choice ---
        # Model-Based Value
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Value
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Calculate Logits
        logits_1 = beta * q_net_1
        
        # Add Stickiness Bonus
        if prev_a1 != -1:
            if prev_r == 1:
                logits_1[prev_a1] += stick_win
            else:
                logits_1[prev_a1] += stick_loss
        
        # Softmax
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Choice ---
        logits_2 = beta * q_stage2_mf[s_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning Updates ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        # TD(1) update for Stage 1 (assuming lambda=1 for parsimony)
        q_stage1_mf[a1] += learning_rate * (delta_stage1 + delta_stage2)
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        prev_a1 = a1
        prev_r = r

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Split Stage Learning Rates
This model proposes that OCI affects learning rates for the two stages differently. Specifically, it tests if OCI modulates the learning rate for the distal choice (Stage 1, Spaceships) while the immediate association learning (Stage 2, Aliens) remains baseline. This could reflect a dissociation between learning immediate outcomes versus learning the value of actions leading to those states.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: Split Stage Learning Rates with OCI Modulation on Stage 1.
    
    Hypothesis: OCI affects the learning rate for the high-level choice (Stage 1) 
    differently than the immediate reward association (Stage 2). Anxious individuals 
    might maintain a different plasticity for the distal predictor compared to the immediate outcome.
    
    Parameters:
    - lr_s2: [0, 1] Learning rate for Stage 2 (Aliens).
    - lr_s1_base: [0, 1] Baseline learning rate for Stage 1 (Spaceships).
    - lr_s1_oci: [-1, 1] OCI modulation on Stage 1 learning rate.
      Effective lr_s1 = clip(lr_s1_base + lr_s1_oci * oci, 0, 1).
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight.
    """
    lr_s2, lr_s1_base, lr_s1_oci, beta, w = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate effective Stage 1 learning rate
    lr_s1 = lr_s1_base + (lr_s1_oci * current_oci)
    lr_s1 = np.clip(lr_s1, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])
        
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net_1
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Choice ---
        logits_2 = beta * q_stage2_mf[s_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning Updates ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        # Update Stage 2 with its specific LR
        q_stage2_mf[s_idx, a2] += lr_s2 * delta_stage2
        
        # Update Stage 1 with its specific modulated LR
        q_stage1_mf[a1] += lr_s1 * (delta_stage1 + delta_stage2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Transition Confidence
This model hypothesizes that OCI affects the *confidence* the Model-Based system has in the transition structure. While the true structure is 70/30, high OCI (often associated with doubt or uncertainty) might lead the agent to "flatten" this probability towards 50/50, reducing the decisiveness of the model-based planning component without explicitly changing the mixing weight `w`.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Model-Based Transition Confidence.
    
    Hypothesis: OCI affects the confidence in the transition structure used by the Model-Based system.
    High OCI might correlate with doubt, flattening the transition matrix towards uniform (0.5/0.5),
    effectively reducing the precision of MB planning.
    
    Parameters:
    - learning_rate: [0, 1]
    - beta: [0, 10]
    - w: [0, 1] Mixing weight.
    - trans_conf_base: [0, 1] Baseline confidence in the 0.7 transition probability.
    - trans_conf_oci: [-1, 1] OCI modulation of confidence.
      Effective prob = 0.5 + 0.2 * clip(base + slope*oci, 0, 1).
      If conf=1, p=0.7. If conf=0, p=0.5.
    """
    learning_rate, beta, w, trans_conf_base, trans_conf_oci = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate effective confidence
    conf = trans_conf_base + (trans_conf_oci * current_oci)
    conf = np.clip(conf, 0.0, 1.0)
    
    # Map confidence [0, 1] to transition probability [0.5, 0.7]
    prob_common = 0.5 + (0.2 * conf)
    prob_rare = 1.0 - prob_common
    
    # Dynamic transition matrix based on OCI
    transition_matrix = np.array([[prob_common, prob_rare], [prob_rare, prob_common]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])
        
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        # Use the OCI-modulated transition matrix for planning
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net_1
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Choice ---
        logits_2 = beta * q_stage2_mf[s_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning Updates ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        q_stage1_mf[a1] += learning_rate * (delta_stage1 + delta_stage2)
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```