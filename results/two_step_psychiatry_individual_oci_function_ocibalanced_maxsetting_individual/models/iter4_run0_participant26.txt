Here are the three cognitive models as Python functions.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Loss Sensitivity (Asymmetric Learning Rates).
    
    Hypothesis:
    Participants with higher OCI scores may exhibit altered sensitivity to negative 
    prediction errors (omission of reward), driven by avoidance or fear of failure.
    This model scales the learning rate for negative prediction errors based on OCI.
    
    Parameters:
    - lr_base: [0, 1] Baseline learning rate for positive prediction errors.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weighting between MB and MF.
    - oci_loss_scale: [0, 10] Scaling factor for learning rate when PE is negative.
      alpha_neg = alpha_pos * (1 + oci_loss_scale * OCI).
    """
    lr_base, beta, w, oci_loss_scale = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Transition probabilities (fixed)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Q-values
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # state x action
    
    log_likelihood = 0.0
    eps = 1e-10

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Stage 1 Decision ---
        # Model-Based Value
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax Stage 1
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_likelihood += np.log(probs_1[a1] + eps)
        
        # --- Stage 2 Decision ---
        # Softmax Stage 2
        exp_q2 = np.exp(beta * q_stage2_mf[s2, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_likelihood += np.log(probs_2[a2] + eps)
        
        # --- Learning ---
        # Stage 1 Update (TD)
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        lr_1 = lr_base
        if delta_stage1 < 0:
            lr_1 = lr_base * (1.0 + oci_loss_scale * oci_score)
        lr_1 = np.clip(lr_1, 0.0, 1.0)
        
        q_stage1_mf[a1] += lr_1 * delta_stage1
        
        # Stage 2 Update (Direct Reinforcement)
        delta_stage2 = r - q_stage2_mf[s2, a2]
        lr_2 = lr_base
        if delta_stage2 < 0:
            lr_2 = lr_base * (1.0 + oci_loss_scale * oci_score)
        lr_2 = np.clip(lr_2, 0.0, 1.0)
        
        q_stage2_mf[s2, a2] += lr_2 * delta_stage2

    return -log_likelihood

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Modulated Stage 2 Plasticity.
    
    Hypothesis:
    OCI scores correlate with a heightened sensitivity to immediate outcomes (Stage 2)
    relative to the planning stage (Stage 1). This model allows the learning rate for 
    the second stage (direct reward) to be scaled by OCI, potentially making it 
    higher than the Stage 1 learning rate.
    
    Parameters:
    - lr_stage1: [0, 1] Learning rate for Stage 1.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weighting between MB and MF.
    - lr_2_oci_scale: [0, 10] Multiplier for Stage 2 learning rate based on OCI.
      lr_stage2 = lr_stage1 * (1 + lr_2_oci_scale * OCI).
    """
    lr_stage1, beta, w, lr_2_oci_scale = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    log_likelihood = 0.0
    eps = 1e-10
    
    # Calculate Stage 2 specific learning rate
    lr_stage2 = lr_stage1 * (1.0 + lr_2_oci_scale * oci_score)
    lr_stage2 = np.clip(lr_stage2, 0.0, 1.0)

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Stage 1 Choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_likelihood += np.log(probs_1[a1] + eps)
        
        # Stage 2 Choice
        exp_q2 = np.exp(beta * q_stage2_mf[s2, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_likelihood += np.log(probs_2[a2] + eps)
        
        # Updates
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr_stage1 * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += lr_stage2 * delta_stage2

    return -log_likelihood

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: Dual-Beta Model with OCI-Modulated Habit Precision.
    
    Hypothesis:
    Instead of a mixing weight 'w', this model posits that the Model-Based and 
    Model-Free systems have independent inverse temperatures (precisions). 
    Crucially, the precision of the Model-Free (Habit) system is modulated by OCI,
    testing the theory that compulsivity reflects a hyper-precise or rigid habit system.
    
    Parameters:
    - lr: [0, 1] Learning rate.
    - beta_mb: [0, 10] Inverse temperature for the Model-Based system.
    - mf_oci_amp: [0, 10] Amplification of Model-Free beta by OCI.
      beta_mf = beta_mb * (1 + mf_oci_amp * OCI).
    """
    lr, beta_mb, mf_oci_amp = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    log_likelihood = 0.0
    eps = 1e-10
    
    # Define MF beta based on OCI
    beta_mf = beta_mb * (1.0 + mf_oci_amp * oci_score)

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Stage 1 Choice: Sum of weighted Q-values
        # Here we add logits: beta_mb * Q_MB + beta_mf * Q_MF
        # This replaces the standard w parameter.
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        logits_1 = (beta_mb * q_stage1_mb) + (beta_mf * q_stage1_mf)
        
        # Softmax Stage 1
        # Subtract max for numerical stability
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_likelihood += np.log(probs_1[a1] + eps)
        
        # Stage 2 Choice: Purely MF driven (habit/bandit)
        # We use beta_mf here to represent the "habitual" precision
        logits_2 = beta_mf * q_stage2_mf[s2, :]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_likelihood += np.log(probs_2[a2] + eps)
        
        # Updates
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += lr * delta_stage2

    return -log_likelihood
```