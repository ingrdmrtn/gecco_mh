Here are three new cognitive models exploring different mechanisms for how OCI might influence decision-making in this two-step task, specifically focusing on the observed "stickiness" or perseveration and the balance between model-based and model-free control.

### Model 1: Hybrid Learner with OCI-Modulated Perseveration
This model hypothesizes that OCI symptoms specifically drive "stickiness" or choice perseveration (repeating the previous action regardless of reward). It uses a standard hybrid reinforcement learning framework (mixing Model-Based and Model-Free values) but adds a perseveration bonus to the Q-values that is scaled by the participant's OCI score.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL model where OCI score modulates choice perseveration (stickiness).
    
    Hypothesis: Higher OCI scores lead to stronger repetitive behaviors (stickiness),
    making it harder to switch away from the previously chosen spaceship.
    
    Parameters:
    - learning_rate: [0, 1] Standard learning rate for value updates.
    - beta: [0, 10] Inverse temperature for softmax.
    - w: [0, 1] Weighting parameter (0 = pure Model-Free, 1 = pure Model-Based).
    - stick_sensitivity: [0, 5] How strongly OCI amplifies the tendency to repeat the last choice.
    """
    learning_rate, beta, w, stick_sensitivity = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate effective stickiness based on OCI
    # Higher OCI -> Higher bonus for repeating the previous action
    stickiness_bonus = stick_sensitivity * current_oci

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Q-values
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1 # Track previous choice for stickiness

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Handle missing data
        if a1 == -1 or s_idx == -1 or a2 == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            prev_a1 = -1
            continue

        # --- Stage 1 Policy ---
        # Model-Based Value Calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Value (Hybrid)
        q_integrated = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add Stickiness Bonus
        q_final_stage1 = q_integrated.copy()
        if prev_a1 != -1:
            q_final_stage1[prev_a1] += stickiness_bonus
            
        exp_q1 = np.exp(beta * q_final_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Stage 2 TD Error
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 TD Error (SARSA-style update using stage 2 value)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Driven Sensitivity to Rare Transitions (Model-Based Reliability)
This model posits that OCI affects how much a participant trusts the transition structure of the task. Instead of a fixed mixing weight `w`, the weight `w` is dynamic. High OCI might lead to "doubt" about the model structure when rare transitions occur, causing a retreat to Model-Free strategies. Conversely, it might lead to over-reliance on the structure (rigidity). Here, we model `w` as a function of OCI.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL where the Model-Based weight (w) is a logistic function of OCI.
    
    Hypothesis: OCI determines the reliance on Model-Based (planning) vs Model-Free (habit) systems.
    Instead of learning 'w', we define it by the OCI score relative to a crossover point.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for Q-values.
    - beta: [0, 10] Inverse temperature.
    - w_max: [0, 1] Maximum possible model-based weight.
    - oci_crossover: [0, 1] The OCI score where the strategy shifts (inflection point).
    - slope: [0, 10] How sharply the strategy changes around the crossover.
    """
    learning_rate, beta, w_max, oci_crossover, slope = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate w based on sigmoid function of OCI
    # If slope is positive: Higher OCI -> Higher w (More Model-Based/Rigid)
    # If slope is negative (conceptually): Higher OCI -> Lower w (More Habitual)
    # We use a standard logistic curve scaled by w_max
    w = w_max / (1 + np.exp(-slope * (current_oci - oci_crossover)))

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        if a1 == -1 or s_idx == -1 or a2 == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        # Stage 2
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Reward Sensitivity (Outcome Valuation)
This model investigates if OCI affects how *rewards* vs *non-rewards* are perceived. Perhaps individuals with specific OCI profiles are hyper-sensitive to failure (zero coins) or less sensitive to gains. This model scales the reward input `r` based on OCI before it enters the prediction error calculation.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free RL where OCI modulates the subjective utility of the reward.
    
    Hypothesis: OCI affects the magnitude of the reward signal. 
    For example, high OCI might dampen the perceived value of a win, 
    or amplify the salience of a loss (though loss is 0 here, relative value changes).
    
    Parameters:
    - learning_rate: [0, 1]
    - beta: [0, 10]
    - reward_sensitivity_base: [0, 2] Base scaling factor for reward.
    - oci_dampening: [0, 1] How much OCI reduces reward sensitivity.
    - persistence: [0, 1] A simple decay factor for unchosen options (forgetting).
    """
    learning_rate, beta, reward_sensitivity_base, oci_dampening, persistence = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Subjective reward calculation
    # Effective reward = r * (Base - OCI * Dampening)
    # If OCI is high, effective reward might be smaller, leading to slower learning from positive outcomes.
    sensitivity = reward_sensitivity_base * (1.0 - (current_oci * oci_dampening))
    sensitivity = max(0.1, sensitivity) # Ensure non-negative/non-zero
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        if a1 == -1 or s_idx == -1 or a2 == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # Stage 1 Choice
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2 Choice
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Updates
        # Scale reward by sensitivity
        subjective_r = r * sensitivity
        
        # Stage 2 Update
        delta2 = subjective_r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += learning_rate * delta2
        
        # Decay unchosen stage 2
        q_stage2[s_idx, 1-a2] *= persistence

        # Stage 1 Update
        delta1 = q_stage2[s_idx, a2] - q_stage1[a1]
        q_stage1[a1] += learning_rate * delta1
        
        # Decay unchosen stage 1
        q_stage1[1-a1] *= persistence

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```