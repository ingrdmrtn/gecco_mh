Here are three cognitive models proposed to explain the participant's behavior, incorporating their OCI score.

### Model 1: OCI-Modulated Model-Based Weight
This model hypothesizes that the Obsessive-Compulsive Inventory (OCI) score modulates the balance between Model-Based (goal-directed) and Model-Free (habitual) control. Specifically, higher OCI scores decrease the weight ($w$) of the Model-Based system, leading to more reliance on Model-Free values. It also includes an eligibility trace parameter ($\lambda$) to scale the influence of the second-stage outcome on the first-stage choice.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model where OCI modulates the balance parameter (w).
    
    Hypothesis: High OCI scores are associated with reduced goal-directed (Model-Based) control,
    shifting behavior towards Model-Free habits.
    w = w_base / (1 + OCI)
    
    Parameters:
    lr: [0,1] - Learning rate for MF values.
    beta: [0,10] - Inverse temperature (exploration/exploitation).
    w_base: [0,1] - Base mixing weight (0=MF, 1=MB).
    lam: [0,1] - Eligibility trace parameter (0=TD(0), 1=TD(1)).
    stickiness: [0,5] - Choice perseveration bonus.
    """
    lr, beta, w_base, lam, stickiness = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # OCI modulation: Higher OCI reduces w (less MB, more MF)
    w = w_base / (1.0 + oci_score)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Handle missing data
        if a1 == -1 or s_idx == -1 or a2 == -1 or r == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue
            
        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness
        if last_action_1 != -1:
            q_net_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # Updates
        # Stage 1 PE
        delta_1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_1
        
        # Stage 2 PE
        delta_2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_2
        
        # Eligibility trace update for Stage 1
        q_stage1_mf[a1] += lr * lam * delta_2
        
        last_action_1 = a1
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Asymmetric Learning Rates
This model suggests that OCI relates to error sensitivity. Participants with higher OCI scores may weigh negative prediction errors (punishments or omissions) more heavily than positive ones. The OCI score scales the learning rate specifically for negative prediction errors.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid model with OCI-modulated asymmetric learning rates.
    
    Hypothesis: OCI is associated with increased sensitivity to negative prediction errors (punishment/omission).
    Negative PEs are weighted more heavily for higher OCI scores.
    
    Parameters:
    lr: [0,1] - Base learning rate (for positive PEs).
    beta: [0,10] - Inverse temperature.
    w: [0,1] - Mixing weight.
    lam: [0,1] - Eligibility trace.
    stickiness: [0,5] - Perseveration.
    """
    lr, beta, w, lam, stickiness = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Define learning rates
    lr_pos = lr
    lr_neg = lr * (1.0 + oci_score) # Amplify learning from negative errors
    if lr_neg > 1.0: lr_neg = 1.0

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        if a1 == -1 or s_idx == -1 or a2 == -1 or r == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue
            
        # Stage 1 Choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_action_1 != -1:
            q_net_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Stage 2 Choice
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # Updates with Asymmetry
        
        # Stage 1 PE
        delta_1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        alpha_1 = lr_pos if delta_1 >= 0 else lr_neg
        q_stage1_mf[a1] += alpha_1 * delta_1
        
        # Stage 2 PE
        delta_2 = r - q_stage2_mf[s_idx, a2]
        alpha_2 = lr_pos if delta_2 >= 0 else lr_neg
        q_stage2_mf[s_idx, a2] += alpha_2 * delta_2
        
        # Eligibility Trace
        # Use the alpha appropriate for the sign of delta_2
        alpha_trace = lr_pos if delta_2 >= 0 else lr_neg
        q_stage1_mf[a1] += alpha_trace * lam * delta_2
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Rigidity (Beta and Stickiness)
This model builds on previous findings that separate learning rates for Stage 1 and Stage 2 improve fit. It hypothesizes that OCI drives "rigidity" in decision-making, increasing both the exploitation of values (via Beta) and the tendency to repeat choices (Stickiness).

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid model with dual learning rates and OCI-modulated Beta and Stickiness.
    
    Hypothesis: OCI affects the 'rigidity' of behavior.
    Higher OCI -> Higher Beta (more deterministic choices) AND Higher Stickiness (repetition).
    
    Parameters:
    lr_s1: [0,1] - Learning rate for Stage 1.
    lr_s2: [0,1] - Learning rate for Stage 2.
    beta_base: [0,10] - Base inverse temperature.
    w: [0,1] - Mixing weight.
    stick_base: [0,5] - Base stickiness.
    """
    lr_s1, lr_s2, beta_base, w, stick_base = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # OCI Modulations
    beta = beta_base * (1.0 + oci_score)
    stickiness = stick_base * (1.0 + oci_score)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        if a1 == -1 or s_idx == -1 or a2 == -1 or r == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue
            
        # Stage 1
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_action_1 != -1:
            q_net_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Stage 2
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # Updates
        delta_1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr_s1 * delta_1
        
        delta_2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr_s2 * delta_2
        
        # Implicit lambda=1 for stage 1 update from stage 2 error, using lr_s1
        q_stage1_mf[a1] += lr_s1 * delta_2
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```