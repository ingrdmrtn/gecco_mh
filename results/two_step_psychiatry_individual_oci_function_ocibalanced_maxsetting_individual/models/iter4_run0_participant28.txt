Here are the 3 new cognitive models based on your requirements.

### Model 1: Punishment Stickiness Modulated by OCI
This model hypothesizes that the participant's obsessive-compulsive traits specifically modulate their tendency to repeat actions that led to **no reward** (punishment stickiness), distinct from their tendency to repeat rewarded actions (reward stickiness). This captures the "compulsive" aspect of persisting in behavior despite negative or neutral outcomes.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Two-step model with separate Reward and Punishment Stickiness.
    OCI modulates Punishment Stickiness (tendency to repeat unrewarded choices).
    
    Parameters:
    learning_rate: [0,1]
    beta: [0,10]
    w: [0,1]
    p_rew: [0,1] (Stickiness after reward, scaled by 5)
    p_pun_base: [0,1] (Base stickiness after no reward, scaled by 5)
    p_pun_oci_mod: [0,1] (Modulation of punishment stickiness by OCI)
    lambda_val: [0,1]
    """
    learning_rate, beta, w, p_rew, p_pun_base, p_pun_oci_mod, lambda_val = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate punishment stickiness based on OCI
    # Allows OCI to increase or decrease the tendency to stay after a loss
    p_pun = p_pun_base + (p_pun_oci_mod - 0.5) * oci_score
    p_pun = np.clip(p_pun, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    last_reward = 0

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        logits_1 = beta * q_net

        # Apply stickiness based on previous outcome
        if last_action_1 != -1:
            stickiness_bonus = 0.0
            if last_reward == 1.0:
                stickiness_bonus = p_rew * 5.0
            else:
                stickiness_bonus = p_pun * 5.0
            
            logits_1[last_action_1] += stickiness_bonus
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2 Policy
        logits_2 = beta * q_stage2_mf[s_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Updates
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        q_stage1_mf[a1] += learning_rate * delta_stage1 + learning_rate * lambda_val * delta_stage2
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        last_action_1 = a1
        last_reward = r

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Stage 1 Beta Modulated by OCI
This model explores the idea that OCI affects the "decision noise" or rigidity specifically at the strategic (first) stage of the task, while the second stage (bandit task) remains standard. It differentiates between the inverse temperature for Stage 1 (`beta_1`) and Stage 2 (`beta_2`).

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Two-step model with distinct inverse temperatures for Stage 1 and Stage 2.
    OCI modulates Beta 1 (Stage 1 rigidity/exploration).
    
    Parameters:
    learning_rate: [0,1]
    beta_1_base: [0,10] (Base Stage 1 inverse temperature)
    beta_1_oci_mod: [0,1] (Modulation of Stage 1 beta by OCI)
    beta_2: [0,10] (Stage 2 inverse temperature)
    w: [0,1]
    p: [0,1] (General stickiness, scaled by 5)
    lambda_val: [0,1]
    """
    learning_rate, beta_1_base, beta_1_oci_mod, beta_2, w, p, lambda_val = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate Beta 1 based on OCI
    # Map modulation to a reasonable range shift, e.g., +/- 5
    beta_1 = beta_1_base + (beta_1_oci_mod - 0.5) * 10.0 * oci_score
    beta_1 = np.clip(beta_1, 0.0, 10.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Stage 1 Policy using Beta 1
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        logits_1 = beta_1 * q_net

        if last_action_1 != -1:
            logits_1[last_action_1] += p * 5.0
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2 Policy using Beta 2
        logits_2 = beta_2 * q_stage2_mf[s_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Updates
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        q_stage1_mf[a1] += learning_rate * delta_stage1 + learning_rate * lambda_val * delta_stage2
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Transition Learning Rate Modulated by OCI
This model allows the participant to learn the transition structure of the environment (the probabilities of Spaceships traveling to Planets) rather than assuming it is fixed. OCI modulates the rate at which this structural learning occurs, testing the hypothesis that OCI relates to the flexibility or rigidity of internal models of the environment structure.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Two-step model with dynamic Transition Learning.
    The agent updates the transition matrix based on observed transitions.
    OCI modulates the Transition Learning Rate.
    
    Parameters:
    lr_val: [0,1] (Learning rate for Q-values)
    lr_trans_base: [0,1] (Base learning rate for transition probabilities)
    lr_trans_oci_mod: [0,1] (Modulation of transition learning rate by OCI)
    beta: [0,10]
    w: [0,1]
    p: [0,1] (Stickiness, scaled by 5)
    lambda_val: [0,1]
    """
    lr_val, lr_trans_base, lr_trans_oci_mod, beta, w, p, lambda_val = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate Transition Learning Rate based on OCI
    lr_trans = lr_trans_base + (lr_trans_oci_mod - 0.5) * oci_score
    lr_trans = np.clip(lr_trans, 0.0, 1.0)
    
    # Initialize transition beliefs: Row 0 is Action 0 (A), Row 1 is Action 1 (U)
    # Belief is P(Planet 0 | Action) and P(Planet 1 | Action)
    # Initialized to the true stats or neutral instructions: 0.7/0.3
    trans_probs = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial]) # Planet reached: 0 or 1
        a2 = int(action_2[trial])
        r = reward[trial]

        # Stage 1 Policy using dynamic transition matrix
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = trans_probs @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        logits_1 = beta * q_net

        if last_action_1 != -1:
            logits_1[last_action_1] += p * 5.0
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2 Policy
        logits_2 = beta * q_stage2_mf[s_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Value Updates
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        q_stage1_mf[a1] += lr_val * delta_stage1 + lr_val * lambda_val * delta_stage2
        q_stage2_mf[s_idx, a2] += lr_val * delta_stage2
        
        # Transition Probability Update
        # Update the row corresponding to the chosen action a1
        # The target is a 1-hot vector for the state s_idx reached
        # P(s'|a) <- P(s'|a) + lr_trans * (Indicator(s') - P(s'|a))
        
        # We only need to update the probability of reaching state 0 (Planet X) for the chosen action,
        # as P(Planet Y) = 1 - P(Planet X).
        is_planet_0 = 1.0 if s_idx == 0 else 0.0
        curr_prob_0 = trans_probs[a1, 0]
        
        new_prob_0 = curr_prob_0 + lr_trans * (is_planet_0 - curr_prob_0)
        
        # Ensure probabilities stay valid
        new_prob_0 = np.clip(new_prob_0, 0.0, 1.0)
        
        trans_probs[a1, 0] = new_prob_0
        trans_probs[a1, 1] = 1.0 - new_prob_0
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```