Here are 3 new cognitive models exploring different mechanisms for how Obsessive-Compulsive symptoms (OCI) might influence decision-making in the two-step task, specifically focusing on mixing model-based/model-free strategies, perseveration, and learning rates.

### Model 1: OCI-Modulated Model-Based Weighting (Logistic)
This model hypothesizes that the balance between Model-Based (planning) and Model-Free (habitual) control is determined by the OCI score. Instead of a linear modulation, it uses a logistic function to map the OCI score to the weighting parameter `w`, allowing for non-linear shifts in strategy based on symptom severity.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free learner where the weighting parameter 'w'
    is a logistic function of the OCI score.
    
    This tests if OCI symptoms drive a non-linear shift between goal-directed (MB)
    and habitual (MF) control.
    
    Parameters:
    learning_rate: [0, 1] Learning rate for value updates.
    beta: [0, 10] Inverse temperature for softmax choice.
    w_m: [0, 10] Slope of the logistic function for OCI modulation.
    w_b: [0, 10] Intercept/Bias of the logistic function (shifted to be centered).
    """
    learning_rate, beta, w_m, w_b = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Logistic mapping of OCI to weight w
    # We center the logistic input somewhat to make the parameters more orthogonal
    # w_b is essentially the threshold.
    logit_val = w_m * (oci_score - 0.5) + (w_b - 5.0) 
    w = 1.0 / (1.0 + np.exp(-logit_val))
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        # Model-Based Value calculation
        # Max Q-value for each state in stage 2
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        # Bellman equation using transition matrix
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # SARSA-like update for Stage 1 MF
        # Note: Standard two-step usually updates Q_MF1 based on Q_MF2 of the chosen state/action
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Q-learning update for Stage 2 MF
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Additional eligibility trace update for Stage 1 MF from Stage 2 RPE is often included in full models,
        # but here we stick to the basic hybrid structure defined by w.
        # To strictly follow the template logic requested:
        # [FILL IN ACTION VALUE UPDATING FOR CHOICE 1] -> Done above
        # [FILL IN ACTION VALUE UPDATING FOR CHOICE 2] -> Done above

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Perseveration (Sticky Choice)
This model introduces a perseveration parameter (stickiness) to the first-stage choice. It hypothesizes that higher OCI scores lead to repetitive behavior (compulsivity) regardless of reward history. The degree of stickiness is linearly modulated by the OCI score.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner with OCI-modulated perseveration (stickiness).
    
    This tests if OCI symptoms increase the tendency to repeat the previous 
    Stage 1 choice regardless of reward (compulsivity).
    
    Parameters:
    learning_rate: [0, 1] Learning rate for value updates.
    beta: [0, 10] Inverse temperature for softmax choice.
    persev_base: [0, 5] Baseline perseveration bonus.
    oci_persev_slope: [0, 5] Increase in perseveration per unit of OCI.
    """
    learning_rate, beta, persev_base, oci_persev_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective perseveration parameter
    # We clip to ensure it stays positive and reasonable
    persev = persev_base + (oci_persev_slope * oci_score)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1 # Indicator for previous choice

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        # Add perseveration bonus to Q-values before softmax
        q_stage1_net = q_stage1_mf.copy()
        if last_action_1 != -1:
            q_stage1_net[last_action_1] += persev
            
        exp_q1 = np.exp(beta * q_stage1_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Update history
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Learning Rate Asymmetry (Positive vs Negative)
This model posits that OCI affects how participants learn from positive versus negative prediction errors. Specifically, it allows for separate learning rates for positive and negative RPEs, where the *ratio* or balance between them is modulated by the OCI score. This could reflect a hypersensitivity to failure or success.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner with asymmetric learning rates for positive and negative RPEs,
    modulated by OCI.
    
    This tests if OCI symptoms bias learning from errors (e.g., over-weighting negative outcomes).
    
    Parameters:
    lr_base: [0, 1] Baseline learning rate.
    beta: [0, 10] Inverse temperature.
    bias_param: [0, 1] Determines the split between pos/neg learning rates.
    oci_bias_mod: [0, 1] Modulates the bias based on OCI.
    """
    lr_base, beta, bias_param, oci_bias_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective bias
    # If effective_bias > 0.5, learns more from positive. < 0.5, learns more from negative.
    # The modulation shifts this balance.
    raw_bias = bias_param + (oci_bias_mod - 0.5) * oci_score 
    effective_bias = np.clip(raw_bias, 0.0, 1.0)
    
    # Define separate learning rates derived from base and bias
    # We maintain the magnitude of lr_base but split it.
    # Often modeled as alpha+ and alpha-. Here we couple them to keep params low.
    lr_pos = lr_base * (2 * effective_bias)
    lr_neg = lr_base * (2 * (1 - effective_bias))
    
    # Clip to valid range [0, 1]
    lr_pos = np.clip(lr_pos, 0.0, 1.0)
    lr_neg = np.clip(lr_neg, 0.0, 1.0)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        lr_1 = lr_pos if delta_stage1 > 0 else lr_neg
        q_stage1_mf[a1] += lr_1 * delta_stage1
        
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        lr_2 = lr_pos if delta_stage2 > 0 else lr_neg
        q_stage2_mf[s_idx, a2] += lr_2 * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```