Here are three new cognitive models based on the participant's data and OCI score.

### Cognitive Model 1: Separate Stage Stickiness with OCI Modulation
This model hypothesizes that OCI-related compulsivity specifically drives **rigid perseveration in the high-level choices (Stage 1)**, while the immediate reactions to aliens (Stage 2) may remain more flexible or governed by different dynamics. The participant data shows extreme streaks in Spaceship choice (Stage 1) but more switching in Alien choice (Stage 2).

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Separate Stage Stickiness model with OCI modulation on Stage 1.
    Hypothesis: OCI specifically drives rigid perseveration in the first-stage choice (Spaceship),
    while second-stage (Alien) choices are modeled with a separate, potentially lower stickiness.
    
    Parameters:
    lr: [0,1] Learning rate.
    beta: [0,10] Inverse temperature.
    w: [0,1] Mixing weight (0=MF, 1=MB).
    stick1_base: [0,5] Base stickiness for stage 1.
    stick1_oci: [0,5] Sensitivity of stage 1 stickiness to OCI (additive).
    stick2: [0,5] Stickiness for stage 2.
    """
    lr, beta, w, stick1_base, stick1_oci, stick2 = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate Stage 1 stickiness modulated by OCI
    stick1 = stick1_base + stick1_oci * oci_score
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2) 
    q_stage2_mf = np.zeros((2, 2)) 
    
    prev_a1 = -1
    # Track last choice made in each state for Stage 2 stickiness
    last_choice_s2 = np.full(2, -1) 

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        state_idx = int(state[trial])
        a2 = int(action_2[trial])

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Apply OCI-modulated stickiness to the previous Stage 1 action
        if prev_a1 != -1:
            q_net_1[prev_a1] += stick1
            
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        q_stage2_net = q_stage2_mf[state_idx].copy()
        
        # Apply separate stickiness to the previous action taken in this specific state
        if last_choice_s2[state_idx] != -1:
             q_stage2_net[last_choice_s2[state_idx]] += stick2
             
        exp_q2 = np.exp(beta * q_stage2_net)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += lr * delta_stage2
        
        prev_a1 = a1
        last_choice_s2[state_idx] = a2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2: OCI-Modulated Choice Trace Decay
This model implements a **"Choice Trace"** mechanism (accumulating stickiness) rather than simple 1-step stickiness. It hypothesizes that OCI affects the **decay rate** of this trace. Higher OCI leads to slower decay (higher $\lambda$), causing habits to persist longer and forming the long streaks observed in the data.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Choice Trace Decay model modulated by OCI.
    Hypothesis: OCI affects the persistence of habits (choice traces). 
    Higher OCI leads to slower decay of the choice trace (higher decay_lambda), 
    resulting in stronger, longer-lasting perseveration.
    
    Parameters:
    lr: [0,1] Learning rate.
    beta: [0,10] Inverse temperature.
    w: [0,1] Mixing weight.
    stick_mag: [0,5] Magnitude of the stickiness effect per unit of trace.
    decay_base: [0,1] Base decay rate for choice trace.
    decay_oci: [0,1] Additive modulation of decay rate by OCI.
    """
    lr, beta, w, stick_mag, decay_base, decay_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Decay rate lambda: Higher means slower decay (memory persists). 
    # Clamped to [0, 1].
    decay_lambda = decay_base + decay_oci * oci_score
    if decay_lambda > 1.0: decay_lambda = 1.0
    if decay_lambda < 0.0: decay_lambda = 0.0
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Trace vector for Stage 1 choices
    trace_1 = np.zeros(2)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        state_idx = int(state[trial])
        a2 = int(action_2[trial])

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add trace-based stickiness
        q_net_1 += stick_mag * trace_1
            
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        # No stickiness modeled for Stage 2 to isolate the trace effect on Stage 1 streaks
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += lr * delta_stage2
        
        # --- Update Choice Trace ---
        # Decay the trace for all actions
        trace_1 *= decay_lambda
        # Increment the trace for the chosen action
        trace_1[a1] += 1.0
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3: OCI-Modulated Learning Rate Ratio
This model hypothesizes that OCI affects the **balance of plasticity** between high-level (Stage 1) and low-level (Stage 2) learning. Specifically, it proposes that higher OCI leads to rigidity in the high-level policy (Stage 1) by reducing its learning rate relative to Stage 2.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Learning Rate Ratio model.
    Hypothesis: OCI affects the balance of plasticity between high-level (Stage 1) and low-level (Stage 2) choices.
    High OCI reduces the learning rate of Stage 1 relative to Stage 2, causing rigidity in spaceship choice
    despite updates in alien values.
    
    Parameters:
    lr_2: [0,1] Learning rate for Stage 2 (Base LR).
    ratio_base: [0,5] Base ratio of (LR1 / LR2).
    ratio_oci: [0,5] OCI dampening factor for the ratio.
    beta: [0,10] Inverse temperature.
    w: [0,1] Mixing weight.
    stickiness: [0,5] Choice perseveration (applied to both stages).
    """
    lr_2, ratio_base, ratio_oci, beta, w, stickiness = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate Stage 1 Learning Rate
    # High OCI increases the denominator, lowering lr_1 (Rigidity hypothesis)
    lr_1 = lr_2 * ratio_base / (1.0 + ratio_oci * oci_score)
    
    # Constrain lr_1 to [0, 1]
    if lr_1 > 1.0: lr_1 = 1.0
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_a1 = -1
    last_choice_s2 = np.full(2, -1)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        state_idx = int(state[trial])
        a2 = int(action_2[trial])

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if prev_a1 != -1:
            q_net_1[prev_a1] += stickiness
            
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        q_stage2_net = q_stage2_mf[state_idx].copy()
        if last_choice_s2[state_idx] != -1:
             q_stage2_net[last_choice_s2[state_idx]] += stickiness
             
        exp_q2 = np.exp(beta * q_stage2_net)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates with separate LRs ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr_1 * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += lr_2 * delta_stage2
        
        prev_a1 = a1
        last_choice_s2[state_idx] = a2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```