Here are three new cognitive models that explore different mechanisms by which Obsessive-Compulsive Inventory (OCI) scores might influence decision-making in the two-step task, specifically focusing on learning rate asymmetries, inverse temperature modulation, and forgetting rates.

### Model 1: OCI-Modulated Loss Aversion in Learning Rates
This model hypothesizes that individuals with higher OCI scores might be differentially sensitive to positive versus negative prediction errors. OCI is often associated with anxiety and harm avoidance. This model tests if OCI scales the learning rate specifically for negative prediction errors (when outcomes are worse than expected), effectively creating an "anxiety-driven" learning asymmetry.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Asymmetric Learning Rates.
    
    This model posits that OCI scores influence how strongly participants learn from
    negative prediction errors compared to positive ones. High OCI might lead to
    over-learning from disappointments (loss aversion in learning).
    
    Parameters:
    - lr_pos: [0, 1] Learning rate for positive prediction errors.
    - lr_neg_base: [0, 1] Base learning rate for negative prediction errors.
    - lr_neg_oci: [0, 1] Additional scaling of negative learning rate by OCI.
    - beta: [0, 10] Inverse temperature (softmax sensitivity).
    - w: [0, 1] Mixing weight (0=MF, 1=MB).
    """
    lr_pos, lr_neg_base, lr_neg_oci, beta, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective negative learning rate, bounded at 1.0
    lr_neg = min(1.0, lr_neg_base + (lr_neg_oci * oci_score))

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Stage 2 update
        delta_stage2 = r - q_stage2_mf[s2, a2]
        effective_alpha_2 = lr_pos if delta_stage2 >= 0 else lr_neg
        q_stage2_mf[s2, a2] += effective_alpha_2 * delta_stage2
        
        # Stage 1 update
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        effective_alpha_1 = lr_pos if delta_stage1 >= 0 else lr_neg
        q_stage1_mf[a1] += effective_alpha_1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Dependent Exploration/Exploitation Balance
This model suggests that OCI symptoms relate to the rigidity of behavior. Instead of affecting learning, OCI here modifies the `beta` (inverse temperature) parameter. A higher OCI score might lead to a higher `beta`, indicating more deterministic, rigid choices (exploitation) and less random exploration, reflecting the compulsive nature of the disorder.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Dependent Inverse Temperature (Rigidity).
    
    This model assumes that OCI scores correlate with behavioral rigidity.
    Participants with higher OCI scores have a higher beta, making their
    choices more deterministic (steeper softmax curve) and less exploratory.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for value updates.
    - beta_base: [0, 10] Baseline inverse temperature.
    - beta_oci: [0, 5] Increase in beta per unit of OCI score.
    - w: [0, 1] Mixing weight (0=MF, 1=MB).
    - eligibility: [0, 1] Eligibility trace for stage 1 update.
    """
    learning_rate, beta_base, beta_oci, w, eligibility = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Beta scales with OCI: higher OCI -> higher beta -> more rigid/deterministic
    beta_effective = beta_base + (beta_oci * oci_score)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta_effective * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta_effective * q_stage2_mf[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Stage 2 update
        delta_stage2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += learning_rate * delta_stage2
        
        # Stage 1 update (using eligibility trace logic often found in these tasks)
        # Note: In standard TD(lambda), the stage 1 update is driven by stage 2 value
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1 + (learning_rate * eligibility * delta_stage2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Value Decay (Forgetting)
This model introduces a memory decay parameter. It hypothesizes that OCI relates to "rumination" or the inability to let go of past values. Specifically, higher OCI scores might be associated with *lower* forgetting rates (higher retention), meaning these participants hold onto learned values longer than controls, making them slower to adapt to the slowly changing reward probabilities.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Value Decay.
    
    This model introduces a decay parameter where unchosen options lose value over time.
    The rate of decay is modulated by OCI. High OCI (compulsivity/rumination) might 
    lead to lower decay (holding onto old values longer), whereas low OCI allows 
    faster forgetting of outdated information.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for chosen options.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight.
    - decay_base: [0, 1] Base decay rate (1 = no decay, 0 = instant forgetting).
    - decay_oci: [0, 1] Modification of decay rate by OCI.
    """
    learning_rate, beta, w, decay_base, decay_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate decay factor. 
    # We model it such that OCI reduces decay (preserves memory).
    # Decay factor is multiplied by unchosen values.
    decay_factor = min(1.0, max(0.0, decay_base + (decay_oci * oci_score)))

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Decay unchosen stage 2 values
        unchosen_a2 = 1 - a2
        q_stage2_mf[s2, unchosen_a2] *= decay_factor
        
        # Update chosen stage 2
        delta_stage2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += learning_rate * delta_stage2
        
        # Decay unchosen stage 1 values
        unchosen_a1 = 1 - a1
        q_stage1_mf[unchosen_a1] *= decay_factor
        
        # Update chosen stage 1
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```