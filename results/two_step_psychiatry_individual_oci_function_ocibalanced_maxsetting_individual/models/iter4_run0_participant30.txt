Here are three new cognitive models exploring different mechanisms for how Obsessive-Compulsive symptoms (OCI) might influence reinforcement learning in this two-step task.

### Model 1: OCI-Modulated Transition Learning (State Prediction Error Sensitivity)
This model hypothesizes that individuals with high OCI scores might be hyper-sensitive to "surprising" transitions (rare transitions). Instead of assuming a fixed transition matrix (0.7/0.3), this model learns the transition probabilities dynamically. The learning rate for this transition structure is modulated by OCI. High OCI might lead to over-updating the internal model of the world based on single rare events (instability in the mental model).

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Transition Learning.
    
    Hypothesis: The participant learns the transition matrix (Spaceship -> Planet)
    dynamically rather than using a fixed one. The rate at which they update 
    their belief about transition probabilities is modulated by OCI. 
    High OCI may lead to over-sensitivity to rare transitions (high structural learning rate).

    Bounds:
    lr_value: [0, 1] - Learning rate for reward values (Q-values)
    beta: [0, 10] - Inverse temperature
    w: [0, 1] - Mixing weight (0=MF, 1=MB)
    lr_struct_base: [0, 1] - Base learning rate for transition matrix
    lr_struct_oci_slope: [-1, 1] - Modulation of structural learning rate by OCI
    """
    lr_value, beta, w, lr_struct_base, lr_struct_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate structural learning rate based on OCI
    lr_struct = lr_struct_base + (lr_struct_oci_slope * oci_score)
    lr_struct = np.clip(lr_struct, 0.0, 1.0)

    # Initialize transition matrix estimate (start with uniform or slight bias)
    # T[action, state] = probability of going to state given action
    # Initial belief is 0.5/0.5 for both, representing uncertainty
    trans_probs = np.full((2, 2), 0.5) 
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Model-Based Value Calculation using learned transition matrix
        # Q_MB(a1) = T(a1, s1) * max(Q2(s1)) + T(a1, s2) * max(Q2(s2))
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = np.zeros(2)
        for act in range(2):
            q_stage1_mb[act] = trans_probs[act, 0] * max_q_stage2[0] + trans_probs[act, 1] * max_q_stage2[1]

        # Net Q-value mixing MB and MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Stage 1 Choice Probability
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2 Choice Probability
        exp_q2 = np.exp(beta * q_stage2_mf[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        if r != -1: # Valid trial
            # 1. Update Transition Matrix Belief
            # Increase prob of the observed transition (a1 -> s)
            # Decrease prob of the unobserved transition (a1 -> other_s)
            # Simple delta rule: P(s|a1) <- P(s|a1) + alpha * (1 - P(s|a1))
            trans_probs[a1, s] += lr_struct * (1 - trans_probs[a1, s])
            trans_probs[a1, 1-s] = 1.0 - trans_probs[a1, s] # Ensure sum to 1

            # 2. Update Value Estimates (MF)
            delta_stage2 = r - q_stage2_mf[s, a2]
            q_stage2_mf[s, a2] += lr_value * delta_stage2

            delta_stage1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
            q_stage1_mf[a1] += lr_value * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Driven "Perseveration" (Choice Stickiness)
This model posits that OCI relates to a compulsion to repeat previous actions, regardless of reward outcome. While standard models often include a "stickiness" parameter, this model specifically links the magnitude of this stickiness to the OCI score. A high OCI participant might exhibit high behavioral rigidity (stickiness) in the first stage choice.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Driven Perseveration (Stickiness).
    
    Hypothesis: High OCI scores correlate with a higher tendency to repeat the 
    previous Stage 1 choice (perseveration), independent of reward history.
    This is implemented as a 'stickiness' bonus added to the Q-value of the 
    previously chosen action.

    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    w: [0, 1]
    stick_base: [0, 5] - Base stickiness bonus
    stick_oci_slope: [-5, 5] - Modulation of stickiness by OCI
    """
    learning_rate, beta, w, stick_base, stick_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate stickiness bonus
    stickiness = stick_base + (stick_oci_slope * oci_score)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Calculate MB values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combine MB and MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness to the net Q-values for decision making (not learning)
        q_net_choice = q_net.copy()
        if last_action_1 != -1:
            q_net_choice[last_action_1] += stickiness

        # Stage 1 Policy
        exp_q1 = np.exp(beta * q_net_choice)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        if r != -1:
            # Update Stage 2 MF
            delta_stage2 = r - q_stage2_mf[s, a2]
            q_stage2_mf[s, a2] += learning_rate * delta_stage2

            # Update Stage 1 MF
            delta_stage1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
            q_stage1_mf[a1] += learning_rate * delta_stage1
            
            last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Reward Sensitivity (Subjective Utility)
This model suggests that OCI affects how rewards are perceived. Instead of treating the reward as exactly 0 or 1, the effective reward used for updating Q-values is scaled. High OCI might lead to a dampened or heightened sensitivity to positive outcomes (or conversely, hypersensitivity to the lack of reward). This is implemented by scaling the reward `r` by an OCI-dependent factor before the prediction error calculation.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Reward Sensitivity.
    
    Hypothesis: OCI scores influence the subjective utility of the reward. 
    The effective reward signal (R_eff) entering the prediction error calculation 
    is R * sensitivity. This captures potential anhedonia or reward hypersensitivity 
    associated with compulsive symptoms.

    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    w: [0, 1]
    rew_sens_base: [0, 2] - Base reward sensitivity (multiplier)
    rew_sens_oci_slope: [-1, 1] - Modulation of reward sensitivity by OCI
    """
    learning_rate, beta, w, rew_sens_base, rew_sens_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective reward sensitivity
    # We clip to ensure it doesn't become negative (inverting reward) unless intended
    reward_sensitivity = rew_sens_base + (rew_sens_oci_slope * oci_score)
    reward_sensitivity = np.maximum(reward_sensitivity, 0.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        if r != -1:
            # Apply OCI-modulated sensitivity to the reward
            effective_reward = r * reward_sensitivity

            # Update Stage 2 MF
            delta_stage2 = effective_reward - q_stage2_mf[s, a2]
            q_stage2_mf[s, a2] += learning_rate * delta_stage2

            # Update Stage 1 MF
            # Note: Standard TD uses the max Q of next state, but here we use 
            # the updated value of the chosen state-action pair to propagate back.
            # Often in these tasks, it's simplified to Q(s,a2).
            delta_stage1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
            q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```