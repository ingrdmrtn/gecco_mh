Here are the three proposed cognitive models.

### Model 1: Pure Model-Free with OCI-Scaled Stickiness and Eligibility
This model tests the hypothesis that high OCI scores reflect a shift towards a purely habitual (Model-Free) control system, characterized by perseveration (stickiness) and strong eligibility traces (direct reinforcement of Stage 1 choices by Stage 2 outcomes).
- **Mechanism**: The model-based weight `w` is fixed to 0. The stickiness (choice repetition) and eligibility trace parameter (`lambda`) are directly proportional to the OCI score.
- **Hypothesis**: Symptom severity drives the strength of habits and the tendency to repeat choices, bypassing the goal-directed system.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Pure Model-Free Learning with OCI-driven Stickiness and Eligibility.
    
    Hypothesis: High OCI participants operate under a pure habit system (w=0).
    Their tendency to repeat choices (stickiness) and the strength of the 
    eligibility trace (lambda) connecting outcomes to stage 1 choices are 
    proportional to their symptom severity.
    
    Parameters:
    - learning_rate: [0, 1] Update rate for Q-values.
    - beta: [0, 10] Inverse temperature (exploration/exploitation).
    - lambda_scale: [0, 1] Scaling factor for eligibility trace (lambda = lambda_scale * oci).
    - stick_scale: [0, 5] Scaling factor for choice stickiness (stickiness = stick_scale * oci).
    """
    learning_rate, beta, lambda_scale, stick_scale = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # OCI-modulated parameters
    lambda_val = lambda_scale * oci_score
    stickiness = stick_scale * oci_score
    
    # Initialize Q-values (Model-Free only)
    q_stage1_mf = np.zeros(2) + 0.5
    q_stage2_mf = np.zeros((2, 2)) + 0.5
    
    last_choice = -1
    log_loss = 0.0
    eps = 1e-10

    for trial in range(n_trials):
        # Skip missing data
        if action_1[trial] == -1:
            last_choice = -1
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Stage 1 Policy (Pure MF + Stickiness)
        q_net_stage1 = q_stage1_mf.copy()
        if last_choice != -1:
            q_net_stage1[last_choice] += stickiness
            
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + eps)

        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[a2] + eps)

        # Learning
        # Stage 2 PE
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 PE (TD(lambda))
        # Note: In pure MF TD(1) or TD(lambda), Stage 1 value updates towards Stage 2 value
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        
        # Update Stage 1: Standard TD + Eligibility Trace from outcome
        # Q1(a1) <- Q1(a1) + lr * ( (V2 - Q1) + lambda * (r - V2) )
        # Simplified: Q1 updates from Stage 2 value prediction (TD) AND the final RPE (Eligibility)
        q_stage1_mf[a1] += learning_rate * delta_stage1 + learning_rate * lambda_val * delta_stage2
        
        last_choice = a1

    return log_loss
```

### Model 2: Hybrid Model with OCI-Modulated Memory Decay
This model tests the hypothesis that high OCI is associated with "hoarding" of value information or an inability to forget/disengage from unchosen options.
- **Mechanism**: Standard hybrid model (MB+MF), but unchosen Q-values in Stage 1 decay over time. The rate of decay is inversely proportional to OCI.
- **Hypothesis**: Low OCI participants decay (forget) unchosen option values quickly, while high OCI participants retain them (low decay), leading to older information interfering with current choices.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model with OCI-Modulated Memory Decay.
    
    Hypothesis: High OCI participants exhibit reduced forgetting (low decay) 
    of unchosen options, 'hoarding' outdated value information. 
    Decay rate is inversely related to OCI.
    
    Parameters:
    - learning_rate: [0, 1] Base learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight (0=MF, 1=MB).
    - decay_max: [0, 1] Maximum decay rate. 
      Actual decay = decay_max * (1 - oci_score).
    """
    learning_rate, beta, w, decay_max = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Decay is low for high OCI (1-OCI is small)
    decay = decay_max * (1.0 - oci_score)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2) + 0.5
    q_stage2_mf = np.zeros((2, 2)) + 0.5
    
    log_loss = 0.0
    eps = 1e-10

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Stage 1 Policy (Hybrid)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + eps)

        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[a2] + eps)

        # Update Stage 2
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Update Stage 1 MF
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Decay unchosen Stage 1 option
        unchosen_a1 = 1 - a1
        q_stage1_mf[unchosen_a1] *= (1.0 - decay)

    return log_loss
```

### Model 3: Hybrid Model with OCI-Modulated Transition Rigidity
This model tests the hypothesis that high OCI participants hold distorted, rigid beliefs about the environment structure (the transition matrix), perceiving probabilistic transitions as more deterministic than they are.
- **Mechanism**: The transition probability `p` used in the Model-Based calculation is not fixed at 0.7. Instead, it interpolates between the true probability (0.7) and certainty (approx 1.0) based on the OCI score.
- **Hypothesis**: High OCI leads to "black-and-white" thinking, causing the Model-Based system to overestimate the reliability of transitions (treating "Common" as "Certain").

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model with OCI-Modulated Transition Rigidity.
    
    Hypothesis: High OCI participants perceive the probabilistic transitions 
    as more deterministic (rigid thinking). The MB system uses a subjective 
    transition matrix distorted by OCI towards p=1.0.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight.
    - rigidity: [0, 1] Degree of distortion. 
      Subjective P = 0.7 * (1 - mix) + 0.99 * mix, where mix = rigidity * oci.
    """
    learning_rate, beta, w, rigidity = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate subjective transition probability
    # If rigidity*oci is 0, p=0.7 (True). If 1, p=0.99 (Deterministic).
    mix = rigidity * oci_score
    # Clamp mix to [0, 1] just in case
    if mix > 1.0: mix = 1.0
    if mix < 0.0: mix = 0.0
    
    p_common = 0.7 * (1.0 - mix) + 0.99 * mix
    # Subjective transition matrix
    subjective_tm = np.array([[p_common, 1.0 - p_common], 
                              [1.0 - p_common, p_common]])

    q_stage1_mf = np.zeros(2) + 0.5
    q_stage2_mf = np.zeros((2, 2)) + 0.5
    
    log_loss = 0.0
    eps = 1e-10

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Stage 1 Policy (Hybrid with Subjective TM)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        # Use subjective matrix for MB valuation
        q_stage1_mb = subjective_tm @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + eps)

        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[a2] + eps)

        # Updates
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    return log_loss
```