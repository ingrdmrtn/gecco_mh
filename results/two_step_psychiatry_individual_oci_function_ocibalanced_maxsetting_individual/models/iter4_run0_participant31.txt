Here are three new cognitive models that explore different mechanisms for how OCI scores might influence decision-making in this two-step task. These models focus on how high obsessive-compulsive symptoms might alter reward sensitivity, learning rates, or the balance between model-based and model-free control in distinct ways.

### Model 1: OCI-Modulated Reward Sensitivity (Outcome Valuation)
This model tests the hypothesis that high OCI scores amplify the subjective value of rewards (or punishments). Individuals with high OC traits might be hyper-sensitive to outcomes, effectively scaling the reward signal before it enters the prediction error calculation. This is implemented by scaling the reward `r` by an OCI-dependent factor.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Reward Sensitivity.
    
    This model posits that the OCI score modulates the subjective valuation of the reward.
    A sensitivity parameter `reward_sens` scales the effective reward based on the OCI score.
    High OCI might lead to perceived hyper-valuation of outcomes (or failures).
    
    Effective Reward = reward * (1 + reward_sens * oci)
    
    Parameters:
    learning_rate: [0, 1] - Rate at which Q-values are updated.
    beta: [0, 10] - Inverse temperature for softmax choice.
    w: [0, 1] - Weighting parameter between Model-Based (1) and Model-Free (0) control.
    reward_sens: [0, 5] - Sensitivity scaling factor for OCI modulation of reward.
    """
    learning_rate, beta, w, reward_sens = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Static transition matrix
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)      # Model-free values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for stage 2 (Aliens)
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based value calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax choice 1
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        s_idx = int(state[trial]) # 0 for X, 1 for Y
        
        # Softmax choice 2
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        # OCI modulates the effective reward magnitude
        r_effective = reward[trial] * (1.0 + reward_sens * current_oci)
        
        # Standard TD updates with effective reward
        delta_stage2 = r_effective - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # TD(1) update for stage 1 (using stage 2 prediction error)
        # Note: Often in these models, stage 1 is updated by both stage 1 PE and stage 2 PE (lambda).
        # Here we follow the template's simple structure but ensure consistency.
        # Standard simple SARSA-like update for stage 1 using Stage 2 Q-value as target
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Dependent Perseveration (Sticky Choice)
This model investigates if high OCI scores lead to increased "stickiness" or perseveration in choices, regardless of reward. This reflects the compulsive aspect of OCI, where individuals might repeat actions habitually. The stickiness parameter is added to the Q-values of the previously chosen action, and its magnitude is scaled by the OCI score.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Dependent Perseveration.
    
    This model adds a 'stickiness' bonus to the action chosen in the previous trial.
    The magnitude of this stickiness is determined by a base stickiness plus an 
    OCI-dependent component. High OCI leads to higher repetition of previous choices.
    
    Stickiness = stick_base + stick_oci * oci
    
    Parameters:
    learning_rate: [0, 1] - Learning rate.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - MB/MF weighting.
    stick_base: [0, 5] - Baseline choice perseveration.
    stick_oci: [0, 5] - Additional perseveration scaled by OCI.
    """
    learning_rate, beta, w, stick_base, stick_oci = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track previous choice for stickiness (initially None or -1)
    prev_a1 = -1
    
    # Calculate total stickiness parameter
    stickiness_param = stick_base + (stick_oci * current_oci)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness bonus to Q-values before softmax
        q_net_stick = q_net.copy()
        if prev_a1 != -1:
            q_net_stick[prev_a1] += stickiness_param
            
        exp_q1 = np.exp(beta * q_net_stick)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # Update previous choice
        prev_a1 = a1
        
        # --- Stage 2 Policy ---
        s_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        r = reward[trial]
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Specific Learning Rate for Negative Outcomes
This model hypothesizes that individuals with high OCI scores learn differently from negative outcomes (omission of reward) compared to positive outcomes. It splits the learning rate into a positive learning rate and a negative learning rate, where the negative learning rate is specifically modulated by OCI. This reflects a potential "fear of failure" or hyper-correction mechanism in OCD.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Specific Learning Rate for Negative Outcomes.
    
    This model separates learning from positive (reward=1) and negative (reward=0) outcomes.
    The learning rate for negative outcomes (lr_neg) is modulated by OCI, while the 
    positive learning rate (lr_pos) is constant.
    
    lr_neg = lr_neg_base * (1 + oci_scale * oci)
    
    Parameters:
    lr_pos: [0, 1] - Learning rate for rewarded trials.
    lr_neg_base: [0, 1] - Base learning rate for unrewarded trials.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - MB/MF weight.
    oci_scale: [0, 5] - Scaling factor for OCI effect on negative learning rate.
    """
    lr_pos, lr_neg_base, beta, w, oci_scale = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate effective negative learning rate, capped at 1.0
    lr_neg_eff = lr_neg_base * (1.0 + oci_scale * current_oci)
    if lr_neg_eff > 1.0:
        lr_neg_eff = 1.0
        
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        s_idx = int(state[trial])
        
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        r = reward[trial]
        
        # Select learning rate based on outcome
        if r > 0:
            current_lr = lr_pos
        else:
            current_lr = lr_neg_eff
            
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += current_lr * delta_stage2
        
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += current_lr * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```