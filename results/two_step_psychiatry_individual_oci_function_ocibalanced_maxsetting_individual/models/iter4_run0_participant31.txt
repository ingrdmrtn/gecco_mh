Here are the three proposed cognitive models.

### Model 1: OCI-Modulated Time-Dependent Novelty Bonus
This model hypothesizes that high OCI scores might relate to an altered drive for exploration or "novelty seeking" (or conversely, avoidance). It adds a bonus to the value of actions based on how long ago they were last chosen. A positive bonus promotes switching (novelty seeking), while a negative bonus promotes perseveration (familiarity preference).

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Time-Dependent Novelty Bonus Model.
    
    Adds a cumulative bonus to actions based on the number of trials since they were 
    last chosen. The magnitude (and sign) of this bonus is modulated by the OCI score.
    
    Parameters:
    - learning_rate: [0,1] Update rate for Q-values.
    - beta: [0,10] Inverse temperature parameter.
    - w: [0,1] Weighting between Model-Based and Model-Free values.
    - nov_base: [0,1] Baseline novelty weight (mapped to [0, 1]).
    - nov_oci: [0,1] OCI modulation of novelty weight (mapped to [-1, 1]).
    """
    learning_rate, beta, w, nov_base, nov_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Map parameters
    # nov_weight determines the bonus per trial unchosen.
    # We map it to roughly [-0.5, 0.5] * 2 + base to allow for strong effects.
    nov_weight = (nov_base * 0.5) + ((2.0 * nov_oci - 1.0) * 0.5 * oci_score)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track trials since each action was chosen
    trials_since_chosen = np.zeros(2) 

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add novelty bonus to the Q-values
        # Bonus = weight * counts
        q_net_1_mod = q_net_1 + nov_weight * trials_since_chosen
        
        logits_1 = beta * (q_net_1_mod - np.max(q_net_1_mod))
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Update novelty tracker
        trials_since_chosen += 1.0
        trials_since_chosen[a1] = 0.0

        # Stage 2 Policy
        q_vals_2 = q_stage2_mf[s_idx]
        logits_2 = beta * (q_vals_2 - np.max(q_vals_2))
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Updates
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Subjective Transition Belief
This model suggests that OCI affects the participant's internal model of the environment's structure. While the true transition probability is 0.7, high OCI individuals might perceive this relationship as more deterministic (closer to 1.0) or more random (closer to 0.5), affecting their Model-Based value calculations.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Subjective Transition Belief Model.
    
    The participant's internal model of transition probabilities (A->X, B->Y)
    is modulated by their OCI score, deviating from the true 0.7 probability.
    
    Parameters:
    - learning_rate: [0,1] Update rate.
    - beta: [0,10] Inverse temperature.
    - w: [0,1] Model-based weight.
    - trans_base: [0,1] Baseline transition probability (mapped to [0.5, 1.0]).
    - trans_oci: [0,1] OCI modulation of transition belief (mapped to [-0.3, 0.3]).
    """
    learning_rate, beta, w, trans_base, trans_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Map transition belief parameters
    # We assume the belief stays between 0.5 (random) and ~1.0 (deterministic)
    base_t = 0.5 + 0.5 * trans_base
    mod_t = (2.0 * trans_oci - 1.0) * 0.3 * oci_score 
    
    t_p = np.clip(base_t + mod_t, 0.5, 0.99)
    
    # Construct the subjective transition matrix
    transition_matrix = np.array([[t_p, 1-t_p], [1-t_p, t_p]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Stage 1
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        # Use subjective transition matrix for MB calculation
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * (q_net_1 - np.max(q_net_1))
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2
        q_vals_2 = q_stage2_mf[s_idx]
        logits_2 = beta * (q_vals_2 - np.max(q_vals_2))
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Updates
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Stage 1 Beta
This model separates the exploration/exploitation trade-off (inverse temperature $\beta$) for the two stages. It hypothesizes that OCI specifically impacts the decision noise at the higher-level planning stage (Stage 1), potentially leading to more erratic (low beta) or rigid (high beta) choices of spaceships, while the reaction to aliens (Stage 2) remains distinct.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Stage 1 Beta Model.
    
    Allows for different exploration rates (inverse temperatures) at Stage 1 and Stage 2.
    The Stage 1 beta is modulated by the OCI score.
    
    Parameters:
    - learning_rate: [0,1] Update rate.
    - beta_2: [0,10] Stage 2 inverse temperature (fixed across OCI).
    - w: [0,1] Model-based weight.
    - beta_1_base: [0,1] Baseline Stage 1 beta (mapped to [0, 10]).
    - beta_1_oci: [0,1] OCI modulation of Stage 1 beta (mapped to [-5, 5]).
    """
    learning_rate, beta_2, w, beta_1_base, beta_1_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Map Beta 1
    # Base is scaled to 0-10. Modulation adds +/- 5 scaled by OCI.
    b1_val = beta_1_base * 10.0
    b1_mod = (2.0 * beta_1_oci - 1.0) * 5.0 * oci_score
    beta_1 = np.clip(b1_val + b1_mod, 0.0, 10.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Stage 1
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Use OCI-modulated Beta 1
        logits_1 = beta_1 * (q_net_1 - np.max(q_net_1))
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2
        q_vals_2 = q_stage2_mf[s_idx]
        # Use standard Beta 2
        logits_2 = beta_2 * (q_vals_2 - np.max(q_vals_2))
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Updates
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```