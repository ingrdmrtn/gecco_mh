Here are 3 new cognitive models based on the participant's data and OCI score.

### Cognitive Model 1: OCI-Modulated Exponential Choice Trace
**Hypothesis:** This model posits that the participant's "stickiness" is not just a repetition of the very last choice (1-back), but a longer-term "motor habit" or choice trace that decays exponentially over time. The influence of this accumulated trace on the current decision is scaled by the participant's OCI score, reflecting the hypothesis that high OCI leads to stronger entrenchment of habitual motor patterns.

**Mechanism:**
- A `choice_trace` vector tracks the history of choices.
- On each trial, the trace for the chosen action increases, while both traces decay: $Trace_{t+1} = \gamma \cdot Trace_t + \mathbb{I}(Action_t)$.
- This trace is added to the Q-values in the softmax step, weighted by `trace_weight * oci`.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Exponential Choice Trace Model.
    
    Parameters:
    - learning_rate: [0, 1] Rate of updating Q-values.
    - beta: [0, 10] Inverse temperature for softmax.
    - w: [0, 1] Weight of Model-Based values (0=Pure MF, 1=Pure MB).
    - decay_rate: [0, 1] Decay factor for the choice trace (0=instant decay, 1=perfect memory).
    - trace_weight: [0, 5] Scaling factor for how OCI amplifies the influence of the choice trace.
    """
    learning_rate, beta, w, decay_rate, trace_weight = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Initialize choice trace (perseveration memory)
    choice_trace = np.zeros(2)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Add choice trace bonus scaled by OCI
        # High OCI -> Stronger influence of past motor history
        trace_bonus = trace_weight * oci_score * choice_trace
        
        exp_q1 = np.exp(beta * (q_net + trace_bonus))
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # Update Choice Trace
        # Decay existing trace and increment selected action
        choice_trace = choice_trace * decay_rate
        choice_trace[a1] += 1.0

        state_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2: OCI-Dampened Negative Learning
**Hypothesis:** High OCI scores are associated with a rigidity or resistance to "unlearning" established habits. This model proposes that OCI modulates the learning rate specifically for negative prediction errors (failures/omissions).
**Mechanism:**
- The model uses a standard learning rate `lr` for positive prediction errors.
- For negative prediction errors (when outcome is worse than expected), the learning rate is scaled down by OCI: $\alpha_{neg} = \alpha \times (1 - \text{oci} \times \text{dampening})$.
- This explains "sticky" behavior where the participant persists with a choice despite receiving 0 coins (e.g., trials 23-24).

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Dampened Negative Learning Model.
    
    Hypothesis: High OCI reduces learning from negative prediction errors, leading to 
    persistence in choice despite lack of reward.
    
    Parameters:
    - learning_rate: [0, 1] Base learning rate (used for positive PE).
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Model-Based weight.
    - stickiness: [0, 5] Basic 1-back choice stickiness.
    - neg_lr_dampening: [0, 1] Factor by which OCI reduces learning from negative PE.
                           alpha_neg = learning_rate * (1 - oci * dampening)
    """
    learning_rate, beta, w, stickiness, neg_lr_dampening = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Basic stickiness
        stick_bonus = np.zeros(2)
        if last_action_1 != -1:
            stick_bonus[int(last_action_1)] = stickiness
        
        exp_q1 = np.exp(beta * (q_net + stick_bonus))
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        last_action_1 = a1
        
        state_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates with Asymmetric Learning Rate ---
        
        # Calculate effective learning rate for Stage 1
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        if delta_stage1 < 0:
            # Dampen learning from negative PE based on OCI
            alpha_eff_1 = learning_rate * (1.0 - (oci_score * neg_lr_dampening))
            # Ensure non-negative
            alpha_eff_1 = max(0.0, alpha_eff_1)
        else:
            alpha_eff_1 = learning_rate
            
        q_stage1_mf[a1] += alpha_eff_1 * delta_stage1
        
        # Calculate effective learning rate for Stage 2
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        if delta_stage2 < 0:
            alpha_eff_2 = learning_rate * (1.0 - (oci_score * neg_lr_dampening))
            alpha_eff_2 = max(0.0, alpha_eff_2)
        else:
            alpha_eff_2 = learning_rate
            
        q_stage2_mf[state_idx, a2] += alpha_eff_2 * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3: OCI-Modulated Loss Sensitivity
**Hypothesis:** This model suggests that OCI alters the subjective valuation of outcomes, specifically making the absence of a reward (0 coins) feel like a punishment (loss).
**Mechanism:**
- Standard Q-learning assumes reward $r \in \{0, 1\}$.
- This model modifies the reward function: if $r=0$, the effective reward is $r_{eff} = -1 \times \text{oci} \times \text{sensitivity}$.
- This creates "active avoidance" of 0-reward states rather than just extinction, potentially driving the switching behavior observed after blocks of failures.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Loss Sensitivity Model.
    
    Hypothesis: Participants with high OCI perceive the lack of reward (0 coins) 
    as an explicit loss/punishment, rather than neutral feedback.
    
    Parameters:
    - learning_rate: [0, 1] Rate of updating Q-values.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Model-Based weight.
    - stickiness: [0, 5] Choice stickiness.
    - loss_sensitivity: [0, 5] Magnitude of subjective punishment for 0-reward outcomes, scaled by OCI.
    """
    learning_rate, beta, w, stickiness, loss_sensitivity = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        stick_bonus = np.zeros(2)
        if last_action_1 != -1:
            stick_bonus[int(last_action_1)] = stickiness
            
        exp_q1 = np.exp(beta * (q_net + stick_bonus))
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        last_action_1 = a1
        
        state_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Calculate Subjective Reward
        # If reward is 0, treat it as a loss proportional to OCI
        if r == 0.0:
            r_effective = -1.0 * oci_score * loss_sensitivity
        else:
            r_effective = r

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Update Stage 2 using r_effective
        delta_stage2 = r_effective - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```