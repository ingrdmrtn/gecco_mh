Here are three new cognitive models that explore different mechanisms by which OCI scores might influence decision-making in this two-step task. These models focus on how high-compulsivity traits (high OCI) might alter the balance between model-based and model-free control, or how they might affect reward sensitivity and learning rates.

### Model 1: Hybrid RL with OCI-modulated Model-Based Weight
This model tests the hypothesis that higher OCI scores lead to a deficit in goal-directed (model-based) control, resulting in a stronger reliance on habitual (model-free) strategies. The mixing weight `w` between the two systems is a logistic function of the OCI score.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based / Model-Free RL.
    The balance between MB and MF control (w) is modulated by OCI score via a logistic function.
    High OCI is hypothesized to reduce MB control (lower w).
    
    Bounds:
    learning_rate: [0,1]
    beta: [0,10]
    w_inflection: [0, 1]  # The OCI score where w transitions
    w_slope: [-10, 10]    # Steepness and direction of the OCI effect on w
    """
    learning_rate, beta, w_inflection, w_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate mixing weight w based on OCI
    # Logistic function to keep w in [0, 1]
    # If w_slope is negative, higher OCI -> lower w (less Model-Based)
    w = 1.0 / (1.0 + np.exp(-w_slope * (oci_score - w_inflection)))
    
    # Transition matrix (fixed for this task structure)
    # 0->0 (70%), 0->1 (30%), 1->0 (30%), 1->1 (70%)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_mf = np.zeros((2, 2))  # Stage 1 MF values (2 actions) and Stage 2 MF values (2 states, 2 actions)
                             # Actually standard implementation separates them usually.
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        # Model-Based Value Calculation
        # Max Q-value at stage 2 for each state
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        # Bellman equation using transition matrix
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        # Stage 2 Update (TD error)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 Update (TD error using stage 2 value)
        # Using the value of the chosen state-action pair at stage 2 as the target
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1 # SARSA-like update for MF stage 1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Model-Free RL with OCI-modulated Reward Sensitivity (Rho)
This model hypothesizes that high OCI scores affect how rewards are subjectively valued. Rather than changing the learning rate or the strategy (MB vs MF), OCI scales the effective reward `r`. This could represent heightened sensitivity to outcomes (hyper-responsibility) or blunted sensitivity.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free RL with OCI-modulated Reward Sensitivity.
    The effective reward (rho) is scaled by OCI.
    
    Bounds:
    learning_rate: [0,1]
    beta: [0,10]
    rho_base: [0, 5]     # Base reward sensitivity
    rho_oci_coeff: [-5, 5] # How OCI modifies reward sensitivity
    """
    learning_rate, beta, rho_base, rho_oci_coeff = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective reward scalar
    rho = rho_base + (rho_oci_coeff * oci_score)
    # Ensure rho is non-negative to avoid inverting reward valence
    rho = np.maximum(rho, 0.0)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        # Scale reward by sensitivity parameter rho
        effective_r = r * rho
        
        # Stage 2 Update
        delta_stage2 = effective_r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 Update
        # Using simple TD(0) to the value of the state reached
        # Value of state reached is approximated by the Q-value of the chosen action in that state
        delta_stage1 = q_stage2[s_idx, a2] - q_stage1[a1]
        q_stage1[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Asymmetric Learning Rates Modulated by OCI
This model investigates if OCI is associated with asymmetric learning from positive versus negative prediction errors. Obsessive-compulsive traits might involve a "negative bias" or fear of failure, potentially leading to stronger learning from negative prediction errors (disappointments) compared to positive ones.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free RL with Asymmetric Learning Rates (Positive vs Negative PE).
    The asymmetry is modulated by OCI.
    
    Bounds:
    alpha_pos: [0,1]       # Learning rate for positive PE
    alpha_neg_base: [0,1]  # Base learning rate for negative PE
    alpha_neg_oci_mod: [-1,1] # OCI modulation on negative learning rate
    beta: [0,10]
    """
    alpha_pos, alpha_neg_base, alpha_neg_oci_mod, beta = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate negative learning rate based on OCI
    alpha_neg = alpha_neg_base + (alpha_neg_oci_mod * oci_score)
    alpha_neg = np.clip(alpha_neg, 0.001, 0.999) # Ensure bounds

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        
        # Stage 2 Update
        delta_stage2 = r - q_stage2[s_idx, a2]
        lr_2 = alpha_pos if delta_stage2 > 0 else alpha_neg
        q_stage2[s_idx, a2] += lr_2 * delta_stage2
        
        # Stage 1 Update
        delta_stage1 = q_stage2[s_idx, a2] - q_stage1[a1]
        lr_1 = alpha_pos if delta_stage1 > 0 else alpha_neg
        q_stage1[a1] += lr_1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```