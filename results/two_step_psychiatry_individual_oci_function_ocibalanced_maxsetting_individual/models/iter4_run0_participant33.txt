Here are the three proposed cognitive models.

### Model 1: OCI-Modulated Stage 1 Learning Rate
This model hypothesizes that high OCI scores are associated with altered habit formation specifically at the first stage of the decision process (choosing a spaceship), distinct from the learning that occurs at the second stage (choosing an alien). OCI may drive over-updating (chasing noise) or rigidity in the high-level choice, modeled by a separate, OCI-modulated learning rate for Stage 1.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Stage 1 Learning Rate Model.
    
    Hypothesis: OCI affects the learning rate specifically for the first-stage 
    decision (spaceships), reflecting altered habit formation or cached value 
    updating, while the second-stage (direct reward) learning remains standard.
    
    Parameters:
    - lr1_base: [0,1] Baseline learning rate for Stage 1 (Spaceship choice).
    - lr1_oci: [0,1] OCI-dependent modulation of Stage 1 learning rate.
    - lr2: [0,1] Learning rate for Stage 2 (Alien choice).
    - beta: [0,10] Inverse temperature (softness of choice).
    - w: [0,1] Mixing weight (0=Model-Free, 1=Model-Based).
    - lambd: [0,1] Eligibility trace parameter.
    """
    lr1_base, lr1_oci, lr2, beta, w, lambd = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate modulated Stage 1 learning rate
    lr1 = lr1_base + lr1_oci * oci_score
    lr1 = np.clip(lr1, 0.0, 1.0)
    
    # Fixed transition matrix (as per task description)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    log_loss = 0.0
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Skip invalid data
        if a1 < 0 or s2 < 0 or a2 < 0:
            continue

        # --- Stage 1 Policy ---
        # Model-Based Value: Expected value of next state (max Q2) weighted by transitions
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2

        # Net Value: Mix of MB and MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf

        # Softmax Choice 1
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / (np.sum(exp_q1) + 1e-10)
        log_loss -= np.log(probs_1[a1] + 1e-10)

        # --- Stage 2 Policy ---
        # Standard Softmax on Stage 2 Q-values
        exp_q2 = np.exp(beta * q_stage2_mf[s2])
        probs_2 = exp_q2 / (np.sum(exp_q2) + 1e-10)
        log_loss -= np.log(probs_2[a2] + 1e-10)

        # --- Learning Updates ---
        
        # Stage 1 PE
        delta_1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        # Update Stage 1 MF value using OCI-modulated lr1
        q_stage1_mf[a1] += lr1 * delta_1
        
        # Stage 2 PE
        delta_2 = r - q_stage2_mf[s2, a2]
        # Update Stage 2 MF value using standard lr2
        q_stage2_mf[s2, a2] += lr2 * delta_2
        
        # Eligibility Trace: Update Stage 1 based on Stage 2 outcome
        # Uses lr1 because it updates the Stage 1 value
        q_stage1_mf[a1] += lr1 * lambd * delta_2

    return log_loss
```

### Model 2: OCI-Modulated Transition Belief
This model suggests that OCI symptoms correlate with a distorted belief about the stability of the environment (the transition probabilities). High OCI might lead participants to underestimate the reliability of the "common" transition (distrusting the structure) or overestimate it (rigid determinism), affecting their Model-Based planning.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Transition Belief Model.
    
    Hypothesis: OCI scores modulate the participant's subjective belief about the 
    transition probabilities (the structure of the task) used in Model-Based planning.
    Deviations from the true 0.7 probability reflect uncertainty or rigidity.
    
    Parameters:
    - lr: [0,1] Learning rate.
    - beta: [0,10] Inverse temperature.
    - w: [0,1] Mixing weight (0=Model-Free, 1=Model-Based).
    - lambd: [0,1] Eligibility trace parameter.
    - tb_base: [0,1] Baseline belief in the 'common' transition probability.
    - tb_oci: [0,1] Modulation of transition belief by OCI.
    """
    lr, beta, w, lambd, tb_base, tb_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate subjective transition probability
    # Represents belief that Spacehip A -> Planet X (and U -> Y)
    p_common = tb_base + tb_oci * oci_score
    p_common = np.clip(p_common, 0.0, 1.0)
    
    # Construct subjective transition matrix
    # [[P(X|A), P(Y|A)], [P(X|U), P(Y|U)]]
    # Assuming symmetry in belief
    subj_transition_matrix = np.array([[p_common, 1.0 - p_common], 
                                       [1.0 - p_common, p_common]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    log_loss = 0.0
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        if a1 < 0 or s2 < 0 or a2 < 0:
            continue

        # --- Stage 1 Policy ---
        # MB Planning uses the SUBJECTIVE transition matrix
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = subj_transition_matrix @ max_q_stage2

        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / (np.sum(exp_q1) + 1e-10)
        log_loss -= np.log(probs_1[a1] + 1e-10)

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s2])
        probs_2 = exp_q2 / (np.sum(exp_q2) + 1e-10)
        log_loss -= np.log(probs_2[a2] + 1e-10)

        # --- Learning Updates ---
        delta_1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_1
        
        delta_2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += lr * delta_2
        
        q_stage1_mf[a1] += lr * lambd * delta_2

    return log_loss
```

### Model 3: OCI-Modulated Random Lapse (Epsilon)
This model posits that high OCI scores are associated with increased "mental noise" or intrusive thoughts, leading to attentional lapses. This is modeled as an epsilon-greedy mixture, where the probability of making a random choice (unrelated to value) increases with the OCI score.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Random Lapse (Epsilon) Model.
    
    Hypothesis: High OCI is associated with lapses in attention or intrusive thoughts, 
    leading to random choices (epsilon-greedy noise) rather than value-based choices.
    The rate of random exploration (epsilon) is modulated by the OCI score.
    
    Parameters:
    - lr: [0,1] Learning rate.
    - beta: [0,10] Inverse temperature.
    - w: [0,1] Mixing weight (0=Model-Free, 1=Model-Based).
    - lambd: [0,1] Eligibility trace parameter.
    - eps_base: [0,1] Baseline lapse rate (epsilon).
    - eps_oci: [0,1] Modulation of lapse rate by OCI.
    """
    lr, beta, w, lambd, eps_base, eps_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate OCI-modulated epsilon (lapse rate)
    epsilon = eps_base + eps_oci * oci_score
    epsilon = np.clip(epsilon, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    log_loss = 0.0
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        if a1 < 0 or s2 < 0 or a2 < 0:
            continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf

        exp_q1 = np.exp(beta * q_net)
        softmax_probs_1 = exp_q1 / (np.sum(exp_q1) + 1e-10)
        
        # Mixture of Softmax and Uniform Randomness
        # Probability = (1 - epsilon) * Softmax + epsilon * 0.5
        final_probs_1 = (1.0 - epsilon) * softmax_probs_1 + epsilon * 0.5
        log_loss -= np.log(final_probs_1[a1] + 1e-10)

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s2])
        softmax_probs_2 = exp_q2 / (np.sum(exp_q2) + 1e-10)
        
        # Apply lapse to second stage as well
        final_probs_2 = (1.0 - epsilon) * softmax_probs_2 + epsilon * 0.5
        log_loss -= np.log(final_probs_2[a2] + 1e-10)

        # --- Learning Updates ---
        delta_1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_1
        
        delta_2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += lr * delta_2
        
        q_stage1_mf[a1] += lr * lambd * delta_2

    return log_loss
```