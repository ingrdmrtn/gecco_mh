Here are three new cognitive models that incorporate the OCI score to explain the participant's behavior in the two-step task.

### Model 1: Model-Based/Model-Free Hybrid with OCI-Driven Imbalance
This model tests the hypothesis that high OCI scores disrupt the balance between goal-directed (Model-Based) and habitual (Model-Free) control. Specifically, it posits that high OCI individuals rely less on the Model-Based system, which calculates values based on the transition structure, and more on the simpler Model-Free system.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free Reinforcement Learning with OCI-modulated Weighting.
    
    Hypothesis: The balance between model-based (MB) and model-free (MF) control is 
    modulated by OCI. Higher OCI scores reduce the weight (w) of the model-based 
    component, leading to more habitual behavior.
    
    Parameters:
    learning_rate: [0, 1] - Update rate for Q-values.
    beta: [0, 10] - Inverse temperature for softmax.
    w_max: [0, 1] - The maximum possible weight for the Model-Based system. 
                    The actual weight is reduced by the OCI score.
    """
    learning_rate, beta, w_max = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate the mixing weight 'w'. 
    # If OCI is high (near 1), w approaches 0 (pure Model-Free).
    # If OCI is low (near 0), w approaches w_max (stronger Model-Based influence).
    w = w_max * (1.0 - oci_score)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2) # Model-free values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for stage 2 (aliens)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        if a1 == -1: # Handle missing data if any
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # --- Stage 1 Policy ---
        # Model-Based Value Calculation: V(s) = max_a Q_stage2(s, a)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        # Q_MB(a1) = sum P(s|a1) * V(s)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value: w * Q_MB + (1-w) * Q_MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        # Stage 2 Update (TD error)
        delta_stage2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += learning_rate * delta_stage2
        
        # Stage 1 Update (TD error using Stage 2 value)
        # Note: Standard TD(0) update for MF system
        delta_stage1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Loss Aversion Amplified by OCI
This model explores the emotional aspect of decision-making. It hypothesizes that individuals with high OCI scores are hypersensitive to negative outcomes (failure to get a coin). Instead of treating a 0 reward as neutral, they perceive it as a "loss," and this perception is magnified by their OCI score.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Q-Learning with OCI-Amplified Loss Aversion.
    
    Hypothesis: High OCI participants perceive the absence of reward (0 coins) 
    not just as neutral, but as a negative 'loss'. The magnitude of this 
    negative utility is scaled by their OCI score.
    
    Parameters:
    learning_rate: [0, 1] - Update rate.
    beta: [0, 10] - Inverse temperature.
    loss_sensitivity: [0, 5] - Base scaling factor for how bad a 0 outcome feels.
    """
    learning_rate, beta, loss_sensitivity = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    # Calculate effective loss penalty. 
    # If reward is 0, the internal signal is -1 * loss_sensitivity * OCI
    # High OCI -> Stronger negative reaction to 0 reward.
    loss_penalty = loss_sensitivity * oci_score

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        if a1 == -1: 
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # Stage 1 Choice
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Stage 2 Choice
        exp_q2 = np.exp(beta * q_stage2[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # Transform Reward
        # If r=1, utility is 1. If r=0, utility is negative (loss).
        effective_reward = r if r > 0 else -loss_penalty

        # Updates
        delta_stage2 = effective_reward - q_stage2[s, a2]
        q_stage2[s, a2] += learning_rate * delta_stage2
        
        # Simple TD update for Stage 1 based on Stage 2 Q-value
        delta_stage1 = q_stage2[s, a2] - q_stage1[a1]
        q_stage1[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Uncertainty-Driven Exploration Reduction (Anxiety)
This model posits that high OCI correlates with intolerance of uncertainty. In this framework, uncertainty (measured here simply as the variance or inverse confidence in the Q-values, approximated by an exploration penalty) is aversive. High OCI participants will actively avoid options that feel "uncertain" or "risky," preferring to exploit known safe bets, effectively reducing their exploration parameter (inverse beta) or adding a penalty to exploration.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Q-Learning with OCI-Driven Exploration Suppression (Inverse Temperature Modulation).
    
    Hypothesis: High OCI leads to 'intolerance of uncertainty' or anxiety, resulting 
    in rigid behavior. This is modeled as the OCI score increasing the inverse 
    temperature (beta), making choices more deterministic (less exploration).
    
    Parameters:
    learning_rate: [0, 1] - Update rate.
    beta_base: [0, 10] - Baseline inverse temperature (exploration/exploitation balance).
    beta_boost: [0, 10] - Additional stiffness added to beta proportional to OCI.
    """
    learning_rate, beta_base, beta_boost = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Effective Beta: High OCI -> Higher Beta -> More deterministic/rigid choices.
    beta_eff = beta_base + (beta_boost * oci_score)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        if a1 == -1: 
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # Stage 1 Choice using effective beta
        exp_q1 = np.exp(beta_eff * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Stage 2 Choice using effective beta
        exp_q2 = np.exp(beta_eff * q_stage2[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # Updates
        delta_stage2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2[s, a2] - q_stage1[a1]
        q_stage1[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```