Here are three new cognitive models for the two-step decision task, designed to capture the influence of Obsessive-Compulsive symptoms (OCI) on decision-making processes.

### Model 1: OCI-Modulated Punishment Sensitivity (Asymmetric Learning)
This model hypothesizes that individuals with high OCI scores are hypersensitive to negative prediction errors (punishments or lack of rewards). While they learn from positive outcomes at a baseline rate, negative outcomes trigger a stronger update in their model-free value system, leading to rapid devaluation of actions that fail to yield rewards.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model with OCI-modulated punishment sensitivity.
    
    Hypothesis:
    High OCI individuals exhibit asymmetric learning rates. They react more strongly 
    to negative prediction errors (worse than expected outcomes) than low OCI individuals.
    This creates a "fear of failure" dynamic where values drop quickly after non-rewards.
    
    Parameters:
    learning_rate: [0,1] - Base learning rate for positive/neutral updates.
    beta: [0,10] - Inverse temperature (exploitation vs exploration).
    w: [0,1] - Weighting between Model-Based (1) and Model-Free (0).
    lambda_elig: [0,1] - Eligibility trace decay.
    oci_punish_mult: [0,10] - Multiplier for learning rate when Prediction Error is negative.
                              lr_neg = lr * (1 + oci_punish_mult * OCI).
    
    Total parameters: 5
    """
    learning_rate, beta, w, lambda_elig, oci_punish_mult = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Value Updating ---
        # Calculate Prediction Errors
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        
        # Determine Learning Rate for Stage 2 (where reward occurs)
        # If the outcome was worse than expected (delta < 0), amplify learning rate by OCI
        current_lr = learning_rate
        if delta_stage2 < 0:
            current_lr = learning_rate * (1.0 + oci_punish_mult * current_oci)
            # Cap LR at 1.0 to prevent instability
            if current_lr > 1.0:
                current_lr = 1.0
        
        # Update Stage 1 (using eligibility trace)
        # We apply the modulated LR here as well, assuming the 'bad news' propagates back strongly
        q_stage1_mf[a1] += current_lr * (delta_stage1 + lambda_elig * delta_stage2)
        
        # Update Stage 2
        q_stage2_mf[state_idx, a2] += current_lr * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Stage 2 Stickiness (Concrete Perseveration)
This model proposes that OCI-related rigidity manifests specifically as "motor perseveration" or habitual repetition at the second stage (choosing the alien). Unlike previous models that focused on the first stage (spaceship), this model assumes that once a participant is in the concrete "harvesting" phase, high OCI leads to a compulsion to repeat the previously selected action (alien index), regardless of the planet they are on.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model with OCI-modulated stickiness on Stage 2 choices.
    
    Hypothesis:
    OCI symptoms manifest as repetitive checking or motor perseveration specifically 
    at the second stage (choosing the alien). High OCI participants are more likely 
    to repeat the same alien choice index (0 or 1) as the previous trial.
    
    Parameters:
    learning_rate: [0,1] - Value updating rate.
    beta: [0,10] - Inverse temperature.
    w: [0,1] - MB/MF weighting.
    lambda_elig: [0,1] - Eligibility trace.
    stick_s2_oci: [0,5] - Stickiness bonus added to the previously chosen alien index,
                          scaled by OCI.
    
    Total parameters: 5
    """
    learning_rate, beta, w, lambda_elig, stick_s2_oci = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_2 = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        
        # --- Stage 2 Policy ---
        q_net_stage2 = q_stage2_mf[state_idx].copy()
        
        # Apply OCI-modulated stickiness to the last chosen alien index
        if last_action_2 != -1:
            stickiness = stick_s2_oci * current_oci
            q_net_stage2[last_action_2] += stickiness
            
        exp_q2 = np.exp(beta * q_net_stage2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Value Updating ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        
        q_stage1_mf[a1] += learning_rate * (delta_stage1 + lambda_elig * delta_stage2)
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        last_action_2 = a2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Eligibility Trace Amplification
This model posits that high OCI scores correlate with an "obsessive" causal linking mechanism. While low OCI individuals might have a decaying memory trace ($\lambda < 1$) that separates the final reward from the initial choice, high OCI individuals maintain a stronger eligibility trace ($\lambda \approx 1$). This means they assign credit for the final outcome more strongly to the initial spaceship choice, effectively making the Model-Free system behave more like a Monte-Carlo update (learning from the full path).

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model with OCI-modulated Eligibility Trace Amplification.
    
    Hypothesis:
    High OCI individuals maintain a stronger 'eligibility trace' (lambda), linking 
    Stage 2 outcomes more robustly back to Stage 1 choices. This represents an 
    obsessive focus on the causal chain, preventing the decay of credit assignment.
    
    Parameters:
    learning_rate: [0,1] - Value updating rate.
    beta: [0,10] - Inverse temperature.
    w: [0,1] - MB/MF weighting.
    lambda_base: [0,1] - Baseline eligibility trace for OCI=0.
    oci_lambda_boost: [0,1] - Increase in lambda per unit of OCI.
                              lambda = min(1.0, lambda_base + oci_lambda_boost * OCI).
    
    Total parameters: 5
    """
    learning_rate, beta, w, lambda_base, oci_lambda_boost = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate effective lambda
    effective_lambda = lambda_base + oci_lambda_boost * current_oci
    if effective_lambda > 1.0:
        effective_lambda = 1.0
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Value Updating ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        
        # Update Stage 1 using the OCI-boosted eligibility trace
        q_stage1_mf[a1] += learning_rate * (delta_stage1 + effective_lambda * delta_stage2)
        
        # Update Stage 2
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```