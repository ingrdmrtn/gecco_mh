Here are three new cognitive models that explore different mechanisms for how OCI scores might influence decision-making in this two-step task.

### Model 1: Hybrid RL with OCI-Modulated Model-Based Weight
This model tests the hypothesis that high OCI scores are associated with a deficit in goal-directed (Model-Based) control, leading to a reliance on habit (Model-Free) systems. It introduces a mixing parameter `w` that shifts between MB and MF strategies, where the OCI score specifically degrades the contribution of the Model-Based system.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL with OCI-Modulated Model-Based Weight.
    
    Hypothesis: High OCI scores reduce the reliance on Model-Based (planning) strategies,
    shifting control towards Model-Free (habitual) strategies.
    
    Parameters:
    learning_rate: [0, 1] Learning rate for Q-value updates.
    beta: [0, 10] Inverse temperature for softmax choice.
    w_base: [0, 1] Baseline weight for Model-Based control (0=Pure MF, 1=Pure MB).
    oci_w_penalty: [0, 1] Strength of OCI-driven reduction in MB weight.
    """
    learning_rate, beta, w_base, oci_w_penalty = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective w: Higher OCI reduces w (less model-based)
    # We clip to ensure it stays between 0 and 1
    w_eff = w_base * (1.0 - oci_score * oci_w_penalty)
    w_eff = np.clip(w_eff, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    
    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based Value: Transition * Max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Value: w * MB + (1-w) * MF
        q_net = w_eff * q_stage1_mb + (1.0 - w_eff) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        
        # --- Stage 2 Choice ---
        # Only Model-Free at stage 2
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Updates ---
        # Stage 2 Update (TD)
        # Note: In standard hybrid models, Q_stage2 is purely MF
        pe_2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * pe_2
        
        # Stage 1 Update (TD-0 for MF component)
        # Using the value of the state actually reached (SARSA-like or Q-learning)
        # Standard Daw 2011 implementation often uses Q(s2, a2)
        pe_1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += learning_rate * pe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Model-Free RL with OCI-Driven Perseveration (Choice Trace)
This model posits that OCI symptoms manifest as "stickiness" or perseveration. Unlike simple repetition of the *immediately* previous action, this model uses a decaying "choice trace" (an eligibility trace of past actions). High OCI scores amplify the influence of this trace, making it harder for the agent to switch strategies even when rewards change.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free RL with OCI-Driven Choice Trace Perseveration.
    
    Hypothesis: OCI is linked to repetitive behavior. This model tracks a 
    decaying trace of past choices. High OCI increases the weight (strength)
    of this trace bias, making the agent prone to repeating sequences of actions.
    
    Parameters:
    learning_rate: [0, 1] Update rate for Q-values.
    beta: [0, 10] Inverse temperature.
    trace_decay: [0, 1] How fast the memory of past choices fades (0=instant, 1=never).
    oci_trace_weight: [0, 5] How strongly OCI scales the influence of the choice trace.
    """
    learning_rate, beta, trace_decay, oci_trace_weight = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective stickiness weight
    k_eff = oci_score * oci_trace_weight
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Choice traces for stage 1 and stage 2
    trace_1 = np.zeros(2)
    trace_2 = np.zeros((2, 2))
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        
        # --- Stage 1 Choice ---
        # Q_net = Q_value + k * Trace
        q_net_1 = q_stage1_mf + k_eff * trace_1
        
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Update Trace 1: Decay all, then increment chosen
        trace_1 *= trace_decay
        trace_1[a1] += 1.0 
        
        # --- Stage 2 Choice ---
        q_net_2 = q_stage2_mf[s_idx] + k_eff * trace_2[s_idx]
        
        exp_q2 = np.exp(beta * q_net_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # Update Trace 2
        trace_2[s_idx] *= trace_decay
        trace_2[s_idx, a2] += 1.0

        # --- Value Updates ---
        delta_stage2 = reward[trial] - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Model-Free RL with OCI-Dependent Asymmetric Learning Rates
This model suggests that OCI affects how individuals process positive versus negative feedback. Specifically, it hypothesizes that high OCI leads to hyper-sensitivity to punishment (or lack of reward, i.e., 0 coins) compared to reward. The OCI score modulates the ratio between the learning rate for positive outcomes (`alpha_pos`) and negative outcomes (`alpha_neg`).

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free RL with OCI-Dependent Asymmetric Learning Rates.
    
    Hypothesis: High OCI participants are more sensitive to negative outcomes (0 reward)
    than positive outcomes. This model splits the learning rate. The negative learning
    rate is boosted by the OCI score.
    
    Parameters:
    alpha_base: [0, 1] Base learning rate for positive outcomes (reward=1).
    alpha_neg_scale: [0, 5] Multiplier for negative learning rate relative to base.
    oci_neg_boost: [0, 5] Additional boost to negative learning rate driven by OCI.
    beta: [0, 10] Inverse temperature.
    """
    alpha_base, alpha_neg_scale, oci_neg_boost, beta = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate learning rates
    alpha_pos = alpha_base
    # Alpha neg is base scaled, plus an extra boost from OCI
    alpha_neg = alpha_base * (alpha_neg_scale + oci_score * oci_neg_boost)
    
    # Clip to ensure stability
    alpha_neg = np.clip(alpha_neg, 0.0, 1.0)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        # Determine which learning rate to use based on REWARD
        current_alpha = alpha_pos if r > 0 else alpha_neg
        
        # Update Stage 2
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += current_alpha * delta_stage2
        
        # Update Stage 1
        # Note: We use the same alpha logic for stage 1 based on the final outcome
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += current_alpha * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```