Here are three new cognitive models based on the provided data and task structure.

### Model 1: Stickiness Modulation by OCI
This model hypothesizes that high OCI scores relate to "stickiness" or choice perseveration (repeating the previous choice regardless of reward). Instead of modulating the balance between model-based and model-free control, OCI here modulates the tendency to repeat the last action (habitual repetition).

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model with Choice Stickiness modulated by OCI.
    
    Hypothesis: High OCI participants exhibit higher choice stickiness (perseveration).
    The stickiness parameter is a base value plus an OCI-dependent component.
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    w: [0, 1]
    stickiness_base: [0, 5]
    stickiness_oci_slope: [0, 5]
    """
    learning_rate, beta, w, stickiness_base, stickiness_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Stickiness increases with OCI score
    stickiness = stickiness_base + (stickiness_oci_slope * oci_score)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_mf1 = np.zeros(2)      # Model-free values for stage 1
    q_mb1 = np.zeros(2)      # Model-based values for stage 1
    q_mf2 = np.zeros((2, 2)) # Model-free values for stage 2
    
    last_action_1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue

        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        # Calculate Model-Based values
        max_q_mf2 = np.max(q_mf2, axis=1)
        q_mb1 = transition_matrix @ max_q_mf2
        
        # Combine MB and MF values
        q_net = w * q_mb1 + (1 - w) * q_mf1
        
        # Add stickiness bonus to the previously chosen action
        q_net_sticky = q_net.copy()
        if last_action_1 != -1:
            q_net_sticky[last_action_1] += stickiness

        exp_q1 = np.exp(beta * q_net_sticky)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_mf2[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        # Stage 2 update
        delta_2 = r - q_mf2[s2, a2]
        q_mf2[s2, a2] += learning_rate * delta_2
        
        # Stage 1 update (SARSA-like TD(1))
        delta_1 = q_mf2[s2, a2] - q_mf1[a1]
        q_mf1[a1] += learning_rate * delta_1
        q_mf1[a1] += learning_rate * delta_2 
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Asymmetric Learning Rates Modulated by OCI
This model proposes that OCI affects how participants learn from positive versus negative prediction errors. Specifically, high OCI might be associated with a hypersensitivity to negative outcomes (punishment/lack of reward) or rigidity in unlearning values. Here, we split the learning rate into positive (`alpha_pos`) and negative (`alpha_neg`) components, where the negative learning rate is modulated by OCI.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model with Asymmetric Learning Rates.
    
    Hypothesis: OCI affects sensitivity to negative prediction errors (lack of reward).
    alpha_neg is modulated by OCI, while alpha_pos is constant.
    
    Bounds:
    alpha_pos: [0, 1]
    alpha_neg_base: [0, 1]
    alpha_neg_oci_mod: [0, 1] (Modulation strength)
    beta: [0, 10]
    w: [0, 1]
    """
    alpha_pos, alpha_neg_base, alpha_neg_oci_mod, beta, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Negative learning rate increases with OCI (hypersensitivity to failure/omission)
    # We clip to ensure it stays in valid bounds
    alpha_neg = alpha_neg_base + (alpha_neg_oci_mod * oci_score)
    if alpha_neg > 1.0: alpha_neg = 1.0
    if alpha_neg < 0.0: alpha_neg = 0.0

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_mf1 = np.zeros(2)
    q_mb1 = np.zeros(2)
    q_mf2 = np.zeros((2, 2))

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue

        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_mf2 = np.max(q_mf2, axis=1)
        q_mb1 = transition_matrix @ max_q_mf2
        q_net = w * q_mb1 + (1 - w) * q_mf1
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_mf2[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        # Stage 2 update with asymmetric learning rates
        delta_2 = r - q_mf2[s2, a2]
        lr_2 = alpha_pos if delta_2 > 0 else alpha_neg
        q_mf2[s2, a2] += lr_2 * delta_2
        
        # Stage 1 update
        delta_1 = q_mf2[s2, a2] - q_mf1[a1]
        lr_1 = alpha_pos if delta_1 > 0 else alpha_neg
        q_mf1[a1] += lr_1 * delta_1
        
        # Eligibility trace update for stage 1 based on stage 2 outcome
        # Using the stage 2 learning rate logic for consistency
        q_mf1[a1] += lr_2 * delta_2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Inverse Temperature (Beta) Modulation by OCI
This model posits that OCI affects the exploration-exploitation balance directly. Instead of changing *how* values are learned (learning rates) or *which* values are used (mixing weight `w`), OCI might influence the stochasticity of the choice itself. High OCI could lead to more deterministic (rigid) choices (higher beta) or more erratic behavior (lower beta) depending on anxiety levels. Here we model Beta as a function of OCI.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model with Inverse Temperature (Beta) modulated by OCI.
    
    Hypothesis: OCI scores influence the deterministic nature of the choice policy.
    Beta = beta_base + (beta_slope * oci). 
    
    Bounds:
    learning_rate: [0, 1]
    beta_base: [0, 10]
    beta_slope: [-5, 5] (Allows for OCI to either increase or decrease randomness)
    w: [0, 1]
    """
    learning_rate, beta_base, beta_slope, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Beta is modulated by OCI
    # We ensure beta stays non-negative
    beta = beta_base + (beta_slope * oci_score)
    if beta < 0: beta = 0

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_mf1 = np.zeros(2)
    q_mb1 = np.zeros(2)
    q_mf2 = np.zeros((2, 2))

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue

        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_mf2 = np.max(q_mf2, axis=1)
        q_mb1 = transition_matrix @ max_q_mf2
        q_net = w * q_mb1 + (1 - w) * q_mf1
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_mf2[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        delta_2 = r - q_mf2[s2, a2]
        q_mf2[s2, a2] += learning_rate * delta_2
        
        delta_1 = q_mf2[s2, a2] - q_mf1[a1]
        q_mf1[a1] += learning_rate * delta_1
        q_mf1[a1] += learning_rate * delta_2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```